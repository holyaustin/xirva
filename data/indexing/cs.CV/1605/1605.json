[{"id": "1605.00029", "submitter": "Lisa Koch", "authors": "Lisa M.Koch, Martin Rajchl, Wenjia Bai, Christian F. Baumgartner, Tong\n  Tong, Jonathan Passerat-Palmbach, Paul Aljabar, Daniel Rueckert", "title": "Multi-Atlas Segmentation using Partially Annotated Data: Methods and\n  Annotation Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-atlas segmentation is a widely used tool in medical image analysis,\nproviding robust and accurate results by learning from annotated atlas\ndatasets. However, the availability of fully annotated atlas images for\ntraining is limited due to the time required for the labelling task.\nSegmentation methods requiring only a proportion of each atlas image to be\nlabelled could therefore reduce the workload on expert raters tasked with\nannotating atlas images. To address this issue, we first re-examine the\nlabelling problem common in many existing approaches and formulate its solution\nin terms of a Markov Random Field energy minimisation problem on a graph\nconnecting atlases and the target image. This provides a unifying framework for\nmulti-atlas segmentation. We then show how modifications in the graph\nconfiguration of the proposed framework enable the use of partially annotated\natlas images and investigate different partial annotation strategies. The\nproposed method was evaluated on two Magnetic Resonance Imaging (MRI) datasets\nfor hippocampal and cardiac segmentation. Experiments were performed aimed at\n(1) recreating existing segmentation techniques with the proposed framework and\n(2) demonstrating the potential of employing sparsely annotated atlas data for\nmulti-atlas segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 21:34:29 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Koch", "Lisa M.", ""], ["Rajchl", "Martin", ""], ["Bai", "Wenjia", ""], ["Baumgartner", "Christian F.", ""], ["Tong", "Tong", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Aljabar", "Paul", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1605.00031", "submitter": "Thomas Wiatowski", "authors": "Philipp Grohs, Thomas Wiatowski, Helmut B\\\"olcskei", "title": "Deep Convolutional Neural Networks on Cartoon Functions", "comments": "This is a slightly updated version of the paper published in the ISIT\n  proceedings. Specifically, we corrected errors in the arguments on the volume\n  of tubes. Note that this correction does not affect the main statements of\n  the paper", "journal-ref": "Proc. of IEEE International Symposium on Information Theory\n  (ISIT), Barcelona, Spain, pp. 1163-1167, July 2016", "doi": "10.1109/ISIT.2016.7541482", "report-no": null, "categories": "cs.LG cs.CV math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wiatowski and B\\\"olcskei, 2015, proved that deformation stability and\nvertical translation invariance of deep convolutional neural network-based\nfeature extractors are guaranteed by the network structure per se rather than\nthe specific convolution kernels and non-linearities. While the translation\ninvariance result applies to square-integrable functions, the deformation\nstability bound holds for band-limited functions only. Many signals of\npractical relevance (such as natural images) exhibit, however, sharp and curved\ndiscontinuities and are, hence, not band-limited. The main contribution of this\npaper is a deformation stability result that takes these structural properties\ninto account. Specifically, we establish deformation stability bounds for the\nclass of cartoon functions introduced by Donoho, 2001.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 21:40:16 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 13:47:49 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Grohs", "Philipp", ""], ["Wiatowski", "Thomas", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1605.00052", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Liang Zheng, Jingdong Wang, Alan Yuille, Qi Tian", "title": "InterActive: Inter-Layer Activeness Propagation", "comments": "To appear, in CVPR 2016 (10 pages, 3 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of computer vision tasks can be tackled with deep\nfeatures, which are the intermediate outputs of a pre-trained Convolutional\nNeural Network. Despite the astonishing performance, deep features extracted\nfrom low-level neurons are still below satisfaction, arguably because they\ncannot access the spatial context contained in the higher layers. In this\npaper, we present InterActive, a novel algorithm which computes the activeness\nof neurons and network connections. Activeness is propagated through a neural\nnetwork in a top-down manner, carrying high-level context and improving the\ndescriptive power of low-level and mid-level neurons. Visualization indicates\nthat neuron activeness can be interpreted as spatial-weighted neuron responses.\nWe achieve state-of-the-art classification performance on a wide range of image\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 02:28:11 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Xie", "Lingxi", ""], ["Zheng", "Liang", ""], ["Wang", "Jingdong", ""], ["Yuille", "Alan", ""], ["Tian", "Qi", ""]]}, {"id": "1605.00055", "submitter": "Lingxi Xie", "authors": "Lingxi Xie, Jingdong Wang, Zhen Wei, Meng Wang, Qi Tian", "title": "DisturbLabel: Regularizing CNN on the Loss Layer", "comments": "To appear in CVPR 2016 (10 pages, 10 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During a long period of time we are combating over-fitting in the CNN\ntraining process with model regularization, including weight decay, model\naveraging, data augmentation, etc. In this paper, we present DisturbLabel, an\nextremely simple algorithm which randomly replaces a part of labels as\nincorrect values in each iteration. Although it seems weird to intentionally\ngenerate incorrect training labels, we show that DisturbLabel prevents the\nnetwork training from over-fitting by implicitly averaging over exponentially\nmany networks which are trained with different label sets. To the best of our\nknowledge, DisturbLabel serves as the first work which adds noises on the loss\nlayer. Meanwhile, DisturbLabel cooperates well with Dropout to provide\ncomplementary regularization functions. Experiments demonstrate competitive\nrecognition results on several popular image recognition datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 02:44:48 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Xie", "Lingxi", ""], ["Wang", "Jingdong", ""], ["Wei", "Zhen", ""], ["Wang", "Meng", ""], ["Tian", "Qi", ""]]}, {"id": "1605.00075", "submitter": "Zezhou Cheng", "authors": "Zezhou Cheng, Qingxiong Yang, Bin Sheng", "title": "Deep Colorization", "comments": "This is a low-resolution version. Please contact the authors to\n  obtain the high-resolution one if you need it. Preliminary version of this\n  work was published in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates into the colorization problem which converts a\ngrayscale image to a colorful version. This is a very difficult problem and\nnormally requires manual adjustment to achieve artifact-free quality. For\ninstance, it normally requires human-labelled color scribbles on the grayscale\ntarget image or a careful selection of colorful reference images (e.g.,\ncapturing the same scene in the grayscale target image). Unlike the previous\nmethods, this paper aims at a high-quality fully-automatic colorization method.\nWith the assumption of a perfect patch matching technique, the use of an\nextremely large-scale reference database (that contains sufficient color\nimages) is the most reliable solution to the colorization problem. However,\npatch matching noise will increase with respect to the size of the reference\ndatabase in practice. Inspired by the recent success in deep learning\ntechniques which provide amazing modeling of large-scale data, this paper\nre-formulates the colorization problem so that deep learning techniques can be\ndirectly employed. To ensure artifact-free quality, a joint bilateral filtering\nbased post-processing step is proposed. We further develop an adaptive image\nclustering technique to incorporate the global image information. Numerous\nexperiments demonstrate that our method outperforms the state-of-art algorithms\nboth in terms of quality and speed.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 07:58:05 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cheng", "Zezhou", ""], ["Yang", "Qingxiong", ""], ["Sheng", "Bin", ""]]}, {"id": "1605.00129", "submitter": "Xinyu Lin", "authors": "Xinyu Lin, Ce Zhu, Qian Zhang, Yipeng Liu", "title": "3D Keypoint Detection Based on Deep Neural Network with Sparse\n  Autoencoder", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have proposed various methods to extract 3D keypoints from the\nsurface of 3D mesh models over the last decades, but most of them are based on\ngeometric methods, which lack enough flexibility to meet the requirements for\nvarious applications. In this paper, we propose a new method on the basis of\ndeep learning by formulating the 3D keypoint detection as a regression problem\nusing deep neural network (DNN) with sparse autoencoder (SAE) as our regression\nmodel. Both local information and global information of a 3D mesh model in\nmulti-scale space are fully utilized to detect whether a vertex is a keypoint\nor not. SAE can effectively extract the internal structure of these two kinds\nof information and formulate high-level features for them, which is beneficial\nto the regression model. Three SAEs are used to formulate the hidden layers of\nthe DNN and then a logistic regression layer is trained to process the\nhigh-level features produced in the third SAE. Numerical experiments show that\nthe proposed DNN based 3D keypoint detection algorithm outperforms current five\nstate-of-the-art methods for various 3D mesh models.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 15:47:28 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Lin", "Xinyu", ""], ["Zhu", "Ce", ""], ["Zhang", "Qian", ""], ["Liu", "Yipeng", ""]]}, {"id": "1605.00164", "submitter": "Dinesh Jayaraman", "authors": "Dinesh Jayaraman and Kristen Grauman", "title": "Look-ahead before you leap: end-to-end active recognition by forecasting\n  the effect of motion", "comments": "A preliminary version of the material in this document was filed as\n  University of Texas technical report no. UT AI15-06, December, 2015, at:\n  http://apps.cs.utexas.edu/tech_reports/reports/ai/AI-2214.pdf, ECCV 2016", "journal-ref": null, "doi": null, "report-no": "University of Texas Technical Report UT AI 15-06 (December 2015)", "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual recognition systems mounted on autonomous moving agents face the\nchallenge of unconstrained data, but simultaneously have the opportunity to\nimprove their performance by moving to acquire new views of test data. In this\nwork, we first show how a recurrent neural network-based system may be trained\nto perform end-to-end learning of motion policies suited for this \"active\nrecognition\" setting. Further, we hypothesize that active vision requires an\nagent to have the capacity to reason about the effects of its motions on its\nview of the world. To verify this hypothesis, we attempt to induce this\ncapacity in our active recognition pipeline, by simultaneously learning to\nforecast the effects of the agent's motions on its internal representation of\nthe environment conditional on all past views. Results across two challenging\ndatasets confirm both that our end-to-end system successfully learns meaningful\npolicies for active category recognition, and that \"learning to look ahead\"\nfurther boosts recognition performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 20:39:16 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 22:15:48 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Jayaraman", "Dinesh", ""], ["Grauman", "Kristen", ""]]}, {"id": "1605.00170", "submitter": "Fei Han", "authors": "Xue Yang, Fei Han, Hua Wang, Hao Zhang", "title": "Enforcing Template Representability and Temporal Consistency for\n  Adaptive Sparse Tracking", "comments": "8 pages. It has been accepted for publication in 25th International\n  Joint Conference on Artificial Intelligence (IJCAI-16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation has been widely studied in visual tracking, which has\nshown promising tracking performance. Despite a lot of progress, the visual\ntracking problem is still a challenging task due to appearance variations over\ntime. In this paper, we propose a novel sparse tracking algorithm that well\naddresses temporal appearance changes, by enforcing template representability\nand temporal consistency (TRAC). By modeling temporal consistency, our\nalgorithm addresses the issue of drifting away from a tracking target. By\nexploring the templates' long-term-short-term representability, the proposed\nmethod adaptively updates the dictionary using the most descriptive templates,\nwhich significantly improves the robustness to target appearance changes. We\ncompare our TRAC algorithm against the state-of-the-art approaches on 12\nchallenging benchmark image sequences. Both qualitative and quantitative\nresults demonstrate that our algorithm significantly outperforms previous\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Sat, 30 Apr 2016 21:53:23 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Yang", "Xue", ""], ["Han", "Fei", ""], ["Wang", "Hua", ""], ["Zhang", "Hao", ""]]}, {"id": "1605.00286", "submitter": "Song Bai", "authors": "Song Bai and Xiang Bai and Longin Jan Latecki and Qi Tian", "title": "Multidimensional Scaling on Multiple Input Distance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional Scaling (MDS) is a classic technique that seeks vectorial\nrepresentations for data points, given the pairwise distances between them.\nHowever, in recent years, data are usually collected from diverse sources or\nhave multiple heterogeneous representations. How to do multidimensional scaling\non multiple input distance matrices is still unsolved to our best knowledge. In\nthis paper, we first define this new task formally. Then, we propose a new\nalgorithm called Multi-View Multidimensional Scaling (MVMDS) by considering\neach input distance matrix as one view. Our algorithm is able to learn the\nweights of views (i.e., distance matrices) automatically by exploring the\nconsensus information and complementary nature of views. Experimental results\non synthetic as well as real datasets demonstrate the effectiveness of MVMDS.\nWe hope that our work encourages a wider consideration in many domains where\nMDS is needed.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 18:15:22 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 00:20:22 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Bai", "Song", ""], ["Bai", "Xiang", ""], ["Latecki", "Longin Jan", ""], ["Tian", "Qi", ""]]}, {"id": "1605.00287", "submitter": "Xiang Xiang", "authors": "Minh Dao, Xiang Xiang, Bulent Ayhan, Chiman Kwan, Trac D. Tran", "title": "Detecting Burnscar from Hyperspectral Imagery via Sparse Representation\n  with Low-Rank Interference", "comments": "It is not a publishable version at this point as there is no IP\n  coverage at the moment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a burnscar detection model for hyperspectral\nimaging (HSI) data. The proposed model contains two-processing steps in which\nthe first step separate and then suppress the cloud information presenting in\nthe data set using an RPCA algorithm and the second step detect the burnscar\narea in the low-rank component output of the first step. Experiments are\nconducted on the public MODIS dataset available at NASA official website.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 18:18:45 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 23:25:22 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Dao", "Minh", ""], ["Xiang", "Xiang", ""], ["Ayhan", "Bulent", ""], ["Kwan", "Chiman", ""], ["Tran", "Trac D.", ""]]}, {"id": "1605.00324", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka, Masaki Hayashi, Kenji Iwata, Yutaka Satoh,\n  Yoshimitsu Aoki, Slobodan Ilic", "title": "Dominant Codewords Selection with Topic Model for Action Recognition", "comments": "in CVPRW16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework for recognizing human activities that\nuses only in-topic dominant codewords and a mixture of intertopic vectors.\nLatent Dirichlet allocation (LDA) is used to develop approximations of human\nmotion primitives; these are mid-level representations, and they adaptively\nintegrate dominant vectors when classifying human activities. In LDA topic\nmodeling, action videos (documents) are represented by a bag-of-words (input\nfrom a dictionary), and these are based on improved dense trajectories. The\noutput topics correspond to human motion primitives, such as finger moving or\nsubtle leg motion. We eliminate the impurities, such as missed tracking or\nchanging light conditions, in each motion primitive. The assembled vector of\nmotion primitives is an improved representation of the action. We demonstrate\nour method on four different datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 23:58:06 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Hayashi", "Masaki", ""], ["Iwata", "Kenji", ""], ["Satoh", "Yutaka", ""], ["Aoki", "Yoshimitsu", ""], ["Ilic", "Slobodan", ""]]}, {"id": "1605.00366", "submitter": "Pavel Svoboda", "authors": "Pavel Svoboda and Michal Hradis and David Barina and Pavel Zemcik", "title": "Compression Artifacts Removal Using Convolutional Neural Networks", "comments": "To be published in WSCG 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that it is possible to train large and deep convolutional\nneural networks (CNN) for JPEG compression artifacts reduction, and that such\nnetworks can provide significantly better reconstruction quality compared to\npreviously used smaller networks as well as to any other state-of-the-art\nmethods. We were able to train networks with 8 layers in a single step and in\nrelatively short time by combining residual learning, skip architecture, and\nsymmetric weight initialization. We provide further insights into convolution\nnetworks for JPEG artifact reduction by evaluating three different objectives,\ngeneralization with respect to training dataset size, and generalization with\nrespect to JPEG quality level.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 06:40:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Svoboda", "Pavel", ""], ["Hradis", "Michal", ""], ["Barina", "David", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1605.00392", "submitter": "Andrea Zunino", "authors": "Andrea Zunino, Jacopo Cavazza, Vittorio Murino", "title": "Revisiting Human Action Recognition: Personalization vs. Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By thoroughly revisiting the classic human action recognition paradigm, this\npaper aims at proposing a new approach for the design of effective action\nclassification systems. Taking as testbed publicly available three-dimensional\n(MoCap) action/activity datasets, we analyzed and validated different\ntraining/testing strategies. In particular, considering that each human action\nin the datasets is performed several times by different subjects, we were able\nto precisely quantify the effect of inter- and intra-subject variability, so as\nto figure out the impact of several learning approaches in terms of\nclassification performance. The net result is that standard testing strategies\nconsisting in cross-validating the algorithm using typical splits of the data\n(holdout, k-fold, or one-subject-out) is always outperformed by a\n\"personalization\" strategy which learns how a subject is performing an action.\nIn other words, it is advantageous to customize (i.e., personalize) the method\nto learn the actions carried out by each subject, rather than trying to\ngeneralize the actions executions across subjects. Consequently, we finally\npropose an action recognition framework consisting of a two-stage\nclassification approach where, given a test action, the subject is first\nidentified before the actual recognition of the action takes place. Despite the\nbasic, off-the-shelf descriptors and standard classifiers adopted, we noted a\nrelevant increase in performance with respect to standard state-of-the-art\nalgorithms, so motivating the usage of personalized approaches for designing\neffective action recognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 08:46:23 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Zunino", "Andrea", ""], ["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "1605.00420", "submitter": "Ritesh Sarkhel", "authors": "Ritesh Sarkhel, Amit K Saha, Nibaran Das", "title": "An Enhanced Harmony Search Method for Bangla Handwritten Character\n  Recognition Using Region Sampling", "comments": "2nd IEEE International Conference on Recent Trends in Information\n  Systems, 2015", "journal-ref": null, "doi": "10.1109/ReTIS.2015.7232899", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of minimum number of local regions of a handwritten character\nimage, containing well-defined discriminating features which are sufficient for\na minimal but complete description of the character is a challenging task. A\nnew region selection technique based on the idea of an enhanced Harmony Search\nmethodology has been proposed here. The powerful framework of Harmony Search\nhas been utilized to search the region space and detect only the most\ninformative regions for correctly recognizing the handwritten character. The\nproposed method has been tested on handwritten samples of Bangla Basic,\nCompound and mixed (Basic and Compound characters) characters separately with\nSVM based classifier using a longest run based feature-set obtained from the\nimage subregions formed by a CG based quad-tree partitioning approach. Applying\nthis methodology on the above mentioned three types of datasets, respectively\n43.75%, 12.5% and 37.5% gains have been achieved in terms of region reduction\nand 2.3%, 0.6% and 1.2% gains have been achieved in terms of recognition\naccuracy. The results show a sizeable reduction in the minimal number of\ndescriptive regions as well a significant increase in recognition accuracy for\nall the datasets using the proposed technique. Thus the time and cost related\nto feature extraction is decreased without dampening the corresponding\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 10:28:07 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Sarkhel", "Ritesh", ""], ["Saha", "Amit K", ""], ["Das", "Nibaran", ""]]}, {"id": "1605.00452", "submitter": "Gilson Antonio Giraldi", "authors": "Paulo S\\'ergio Silva Rodrigues and Gilson Antonio Giraldi", "title": "Fourier Analysis and q-Gaussian Functions: Analytical and Numerical\n  Results", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "arXiv:1549348", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a consensus in signal processing that the Gaussian kernel and its\npartial derivatives enable the development of robust algorithms for feature\ndetection. Fourier analysis and convolution theory have central role in such\ndevelopment. In this paper we collect theoretical elements to follow this\navenue but using the q-Gaussian kernel that is a nonextensive generalization of\nthe Gaussian one. Firstly, we review some theoretical elements behind the\none-dimensional q-Gaussian and its Fourier transform. Then, we consider the\ntwo-dimensional q-Gaussian and we highlight the issues behind its analytical\nFourier transform computation. We analyze the q-Gaussian kernel in the space\nand Fourier domains using the concepts of space window, cut-off frequency, and\nthe Heisenberg inequality.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 12:08:44 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Rodrigues", "Paulo S\u00e9rgio Silva", ""], ["Giraldi", "Gilson Antonio", ""]]}, {"id": "1605.00459", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Khalil Sima'an, Lucia Specia", "title": "Multi30K: Multilingual English-German Image Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Multi30K dataset to stimulate multilingual multimodal\nresearch. Recent advances in image description have been demonstrated on\nEnglish-language datasets almost exclusively, but image description should not\nbe limited to English. This dataset extends the Flickr30K dataset with i)\nGerman translations created by professional translators over a subset of the\nEnglish descriptions, and ii) descriptions crowdsourced independently of the\noriginal English descriptions. We outline how the data can be used for\nmultilingual image description and multimodal machine translation, but we\nanticipate the data will be useful for a broader range of tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 12:38:03 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Sima'an", "Khalil", ""], ["Specia", "Lucia", ""]]}, {"id": "1605.00475", "submitter": "Yuchao Dai Dr.", "authors": "Yuchao Dai, Hongdong Li and Laurent Kneip", "title": "Rolling Shutter Camera Relative Pose: Generalized Epipolar Geometry", "comments": "Accepted by CVPR 2016, this is an extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of modern consumer-grade cameras employ a rolling shutter\nmechanism. In dynamic geometric computer vision applications such as visual\nSLAM, the so-called rolling shutter effect therefore needs to be properly taken\ninto account. A dedicated relative pose solver appears to be the first problem\nto solve, as it is of eminent importance to bootstrap any derivation of\nmulti-view geometry. However, despite its significance, it has received\ninadequate attention to date.\n  This paper presents a detailed investigation of the geometry of the rolling\nshutter relative pose problem. We introduce the rolling shutter essential\nmatrix, and establish its link to existing models such as the push-broom\ncameras, summarized in a clean hierarchy of multi-perspective cameras. The\ngeneralization of well-established concepts from epipolar geometry is completed\nby a definition of the Sampson distance in the rolling shutter case. The work\nis concluded with a careful investigation of the introduced epipolar geometry\nfor rolling shutter cameras on several dedicated benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 13:31:02 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Dai", "Yuchao", ""], ["Li", "Hongdong", ""], ["Kneip", "Laurent", ""]]}, {"id": "1605.00561", "submitter": "David Barina", "authors": "David Barina and Michal Kula and Pavel Zemcik", "title": "Parallel Wavelet Schemes for Images", "comments": "This is a preprint of the article that appeared in the Journal of\n  Real-Time Image Processing. The final publication is available at Springer\n  via http://doi.org/10.1007/s11554-016-0646-3", "journal-ref": "Real-Time Image Proc (2019) 16: 1365", "doi": "10.1007/s11554-016-0646-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce several new schemes for calculation of discrete\nwavelet transforms of images. These schemes reduce the number of steps and, as\na consequence, allow to reduce the number of synchronizations on parallel\narchitectures. As an additional useful property, the proposed schemes can\nreduce also the number of arithmetic operations. The schemes are primarily\ndemonstrated on CDF 5/3 and CDF 9/7 wavelets employed in JPEG 2000 image\ncompression standard. However, the presented method is general, and it can be\napplied on any wavelet transform. As a result, our scheme requires only two\nmemory barriers for 2-D CDF 5/3 transform compared to four barriers in the\noriginal separable form or three barriers in the non-separable scheme recently\npublished. Our reasoning is supported by exhaustive experiments on high-end\ngraphics cards.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 16:46:35 GMT"}, {"version": "v2", "created": "Mon, 26 Sep 2016 11:34:01 GMT"}, {"version": "v3", "created": "Wed, 12 Oct 2016 14:38:22 GMT"}, {"version": "v4", "created": "Wed, 16 Oct 2019 08:09:45 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Barina", "David", ""], ["Kula", "Michal", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1605.00572", "submitter": "Noranart Vesdapunt", "authors": "Noranart Vesdapunt, Utkarsh Sinha", "title": "Comparison of Optimization Methods in Optical Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is a widely known problem in computer vision\nintroduced by Gibson, J.J(1950) to describe the visual perception of human by\nstimulus objects. Estimation of optical flow model can be achieved by solving\nfor the motion vectors from region of interest in the the different timeline.\nIn this paper, we assumed slightly uniform change of velocity between two\nnearby frames, and solve the optical flow problem by traditional method,\nLucas-Kanade(1981). This method performs minimization of errors between\ntemplate and target frame warped back onto the template. Solving minimization\nsteps requires optimization methods which have diverse convergence rate and\nerror. We explored first and second order optimization methods, and compare\ntheir results with Gauss-Newton method in Lucas-Kanade. We generated 105 videos\nwith 10,500 frames by synthetic objects, and 10 videos with 1,000 frames from\nreal world footage. Our experimental results could be used as tuning parameters\nfor Lucas-Kanade method.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 17:21:22 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Vesdapunt", "Noranart", ""], ["Sinha", "Utkarsh", ""]]}, {"id": "1605.00707", "submitter": "Mikhail Breslav", "authors": "Mikhail Breslav, Tyson L. Hedrick, Stan Sclaroff, Margrit Betke", "title": "Discovering Useful Parts for Pose Estimation in Sparsely Annotated\n  Datasets", "comments": "Accepted at WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work introduces a novel way to increase pose estimation accuracy by\ndiscovering parts from unannotated regions of training images. Discovered parts\nare used to generate more accurate appearance likelihoods for traditional\npart-based models like Pictorial Structures [13] and its derivatives. Our\nexperiments on images of a hawkmoth in flight show that our proposed approach\nsignificantly improves over existing work [27] for this application, while also\nbeing more generally applicable. Our proposed approach localizes landmarks at\nleast twice as accurately as a baseline based on a Mixture of Pictorial\nStructures (MPS) model. Our unique High-Resolution Moth Flight (HRMF) dataset\nis made publicly available with annotations.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 23:37:11 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Breslav", "Mikhail", ""], ["Hedrick", "Tyson L.", ""], ["Sclaroff", "Stan", ""], ["Betke", "Margrit", ""]]}, {"id": "1605.00732", "submitter": "Xiao Chen", "authors": "Xiao Chen, Fazhi He", "title": "A propagation matting method based on the Local Sampling and KNN\n  Classification with adaptive feature space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Closed Form is a propagation based matting algorithm, functioning well on\nimages with good propagation . The deficiency of the Closed Form method is that\nfor complex areas with poor image propagation , such as hole areas or areas of\nlong and narrow structures. The right results are usually hard to get. On these\nareas, if certain flags are provided, it can improve the effects of matting. In\nthis paper, we design a matting algorithm by local sampling and the KNN\nclassifier propagation based matting algorithm. First of all, build the\ncorresponding features space according to the different components of image\ncolors to reduce the influence of overlapping between the foreground and\nbackground, and to improve the classification accuracy of KNN classifier.\nSecond, adaptively use local sampling or using local KNN classifier for\nprocessing based on the pros and cons of the sample performance of unknown\nimage areas. Finally, based on different treatment methods for the unknown\nareas, we will use different weight for augmenting constraints to make the\ntreatment more effective. In this paper, by combining qualitative observation\nand quantitative analysis, we will make evaluation of the experimental results\nthrough online standard set of evaluation tests. It shows that on images with\ngood propagation , this method is as effective as the Closed Form method, while\non images in complex regions, it can perform even better than Closed Form.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 02:09:22 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chen", "Xiao", ""], ["He", "Fazhi", ""]]}, {"id": "1605.00743", "submitter": "Chuang Gan", "authors": "Chuang Gan, Tianbao Yang, Boqing Gong", "title": "Learning Attributes Equals Multi-Source Domain Generalization", "comments": "Accepted by CVPR 2016 as a spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributes possess appealing properties and benefit many computer vision\nproblems, such as object recognition, learning with humans in the loop, and\nimage retrieval. Whereas the existing work mainly pursues utilizing attributes\nfor various computer vision problems, we contend that the most basic\nproblem---how to accurately and robustly detect attributes from images---has\nbeen left under explored. Especially, the existing work rarely explicitly\ntackles the need that attribute detectors should generalize well across\ndifferent categories, including those previously unseen. Noting that this is\nanalogous to the objective of multi-source domain generalization, if we treat\neach category as a domain, we provide a novel perspective to attribute\ndetection and propose to gear the techniques in multi-source domain\ngeneralization for the purpose of learning cross-category generalizable\nattribute detectors. We validate our understanding and approach with extensive\nexperiments on four challenging datasets and three different problems.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 03:09:22 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Gan", "Chuang", ""], ["Yang", "Tianbao", ""], ["Gong", "Boqing", ""]]}, {"id": "1605.00763", "submitter": "Nima Hatami", "authors": "Nima Hatami, Michael Goldbaum", "title": "Automatic Identification of Retinal Arteries and Veins in Fundus Images\n  using Local Binary Patterns", "comments": null, "journal-ref": "Investigative Ophthalmology and Visual Science, 55 (5), 232, 2014", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artery and vein (AV) classification of retinal images is a key to necessary\ntasks, such as automated measurement of arteriolar-to-venular diameter ratio\n(AVR). This paper comprehensively reviews the state-of-the art in AV\nclassification methods. To improve on previous methods, a new Local Bi- nary\nPattern-based method (LBP) is proposed. Beside its simplicity, LBP is robust\nagainst low contrast and low quality fundus images; and it helps the process by\nincluding additional AV texture and shape information. Experimental results\ncompare the performance of the new method with the state-of-the art; and also\nmethods with different feature extraction and classification schemas.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 07:22:49 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Hatami", "Nima", ""], ["Goldbaum", "Michael", ""]]}, {"id": "1605.00775", "submitter": "Shu Kong", "authors": "Shu Kong, Surangi Punyasena, Charless Fowlkes", "title": "Spatially Aware Dictionary Learning and Coding for Fossil Pollen\n  Identification", "comments": "CVMI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust approach for performing automatic species-level\nrecognition of fossil pollen grains in microscopy images that exploits both\nglobal shape and local texture characteristics in a patch-based matching\nmethodology. We introduce a novel criteria for selecting meaningful and\ndiscriminative exemplar patches. We optimize this function during training\nusing a greedy submodular function optimization framework that gives a\nnear-optimal solution with bounded approximation error. We use these selected\nexemplars as a dictionary basis and propose a spatially-aware sparse coding\nmethod to match testing images for identification while maintaining global\nshape correspondence. To accelerate the coding process for fast matching, we\nintroduce a relaxed form that uses spatially-aware soft-thresholding during\ncoding. Finally, we carry out an experimental study that demonstrates the\neffectiveness and efficiency of our exemplar selection and classification\nmechanisms, achieving $86.13\\%$ accuracy on a difficult fine-grained species\nclassification task distinguishing three types of fossil spruce pollen.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 07:54:32 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Kong", "Shu", ""], ["Punyasena", "Surangi", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1605.00855", "submitter": "Xirong Li", "authors": "Xirong Li and Qin Jin", "title": "Improving Image Captioning by Concept-based Sentence Reranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our winning entry in the ImageCLEF 2015 image sentence\ngeneration task. We improve Google's CNN-LSTM model by introducing\nconcept-based sentence reranking, a data-driven approach which exploits the\nlarge amounts of concept-level annotations on Flickr. Different from previous\nusage of concept detection that is tailored to specific image captioning\nmodels, the propose approach reranks predicted sentences in terms of their\nmatches with detected concepts, essentially treating the underlying model as a\nblack box. This property makes the approach applicable to a number of existing\nsolutions. We also experiment with fine tuning on the deep language model,\nwhich improves the performance further. Scoring METEOR of 0.1875 on the\nImageCLEF 2015 test set, our system outperforms the runner-up (METEOR of\n0.1687) with a clear margin.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 12:13:26 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Li", "Xirong", ""], ["Jin", "Qin", ""]]}, {"id": "1605.00894", "submitter": "Xiaopeng Hong", "authors": "Jing Zhou, Xiaopeng Hong, Fei Su, Guoying Zhao", "title": "Recurrent Convolutional Neural Network Regression for Continuous Pain\n  Intensity Estimation in Video", "comments": "This paper is the pre-print technical report of the paper accepted by\n  the IEEE CVPR Workshop of Affect \"in-the-wild\". The final version will be\n  available after the workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pain intensity estimation possesses a significant position in\nhealthcare and medical field. Traditional static methods prefer to extract\nfeatures from frames separately in a video, which would result in unstable\nchanges and peaks among adjacent frames. To overcome this problem, we propose a\nreal-time regression framework based on the recurrent convolutional neural\nnetwork for automatic frame-level pain intensity estimation. Given vector\nsequences of AAM-warped facial images, we used a sliding-window strategy to\nobtain fixed-length input samples for the recurrent network. We then carefully\ndesign the architecture of the recurrent network to output continuous-valued\npain intensity. The proposed end-to-end pain intensity regression framework can\npredict the pain intensity of each frame by considering a sufficiently large\nhistorical frames while limiting the scale of the parameters within the model.\nOur method achieves promising results regarding both accuracy and running speed\non the published UNBC-McMaster Shoulder Pain Expression Archive Database.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 13:17:03 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Zhou", "Jing", ""], ["Hong", "Xiaopeng", ""], ["Su", "Fei", ""], ["Zhao", "Guoying", ""]]}, {"id": "1605.00961", "submitter": "Olivier Guye", "authors": "Olivier Guye", "title": "Hierarchical Modeling of Multidimensional Data in Regularly Decomposed\n  Spaces: Main Principles", "comments": "170 pages, 19 figures, research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The described works have been carried out in the framework of a mid-term\nstudy initiated by the Centre Electronique de l'Armement and led by ADERSA, a\nFrench company of research under contract. The aim was to study the techniques\nof regular dividing of numerical data sets so as to provide tools for problem\nsolving enabling to model multidimensional numerical objects and to be used in\ncomputer-aided design and manufacturing, in robotics, in image analysis and\nsynthesis, in pattern recognition, in decision making, in cartography and\nnumerical data base management. These tools are relying on the principle of\nregular hierarchical decomposition and led to the implementation of a\nmultidimensional generalization of quaternary and octernary trees: the trees of\norder 2**k or 2**k-trees mapped in binary trees. This first tome, dedicated to\nthe hierarchical modeling of multidimensional numerical data, describes the\nprinciples used for building, transforming, analyzing and recognizing patterns\non which is relying the development of the associated algorithms. The whole so\ndeveloped algorithms are detailed in pseudo-code at the end of this document.\nThe present publication especially describes: - a building method adapted\ndisordered and overcrowded data streams ; - its extension in inductive limits ;\n- the computation of the homographic transformation of a tree ; - the attribute\ncalculus based on generalized moments and the provision of Eigen trees ; -\nperception procedures of objects without any covering in affine geometry ; -\nseveral supervised and unsupervised pattern recognition methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 15:58:18 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 09:57:41 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Guye", "Olivier", ""]]}, {"id": "1605.00972", "submitter": "Peter Dugan Dr", "authors": "Peter J. Dugan, Christopher W. Clark, Yann Andr\\'e LeCun, Sofie M. Van\n  Parijs", "title": "Phase 2: DCL System Using Deep Learning Approaches for Land-based or\n  Ship-based Real-Time Recognition and Localization of Marine Mammals - Machine\n  Learning Detection Algorithms", "comments": "National Oceanic Partnership Program (NOPP) sponsored by ONR and\n  NFWF: N000141210585", "journal-ref": null, "doi": null, "report-no": "N000141210585", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overarching goals for this work aim to advance the state of the art for\ndetection, classification and localization (DCL) in the field of bioacoustics.\nThis goal is primarily achieved by building a generic framework for\ndetection-classification (DC) using a fast, efficient and scalable\narchitecture, demonstrating the capabilities of this system using on a variety\nof low-frequency mid-frequency cetacean sounds. Two primary goals are to\ndevelop transferable technologies for detection and classification in, one: the\narea of advanced algorithms, such as deep learning and other methods; and two:\nadvanced systems, capable of real-time and archival processing. For each key\narea, we will focus on producing publications from this work and providing\ntools and software to the community where/when possible. Currently massive\namounts of acoustic data are being collected by various institutions,\ncorporations and national defense agencies. The long-term goal is to provide\ntechnical capability to analyze the data using automatic algorithms for (DC)\nbased on machine intelligence. The goal of the automation is to provide\neffective and efficient mechanisms by which to process large acoustic datasets\nfor understanding the bioacoustic behaviors of marine mammals. This capability\nwill provide insights into the potential ecological impacts and influences of\nanthropogenic ocean sounds. This work focuses on building technologies using a\nmaturity model based on DARPA 6.1 and 6.2 processes, for basic and applied\nresearch, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 16:36:30 GMT"}, {"version": "v2", "created": "Thu, 5 May 2016 18:28:21 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dugan", "Peter J.", ""], ["Clark", "Christopher W.", ""], ["LeCun", "Yann Andr\u00e9", ""], ["Van Parijs", "Sofie M.", ""]]}, {"id": "1605.01014", "submitter": "Xiang Yu", "authors": "Xiang Yu and Feng Zhou and Manmohan Chandraker", "title": "Deep Deformation Network for Object Landmark Localization", "comments": "This work is going to appear at ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel cascaded framework, namely deep deformation network (DDN),\nfor localizing landmarks in non-rigid objects. The hallmarks of DDN are its\nincorporation of geometric constraints within a convolutional neural network\n(CNN) framework, ease and efficiency of training, as well as generality of\napplication. A novel shape basis network (SBN) forms the first stage of the\ncascade, whereby landmarks are initialized by combining the benefits of CNN\nfeatures and a learned shape basis to reduce the complexity of the highly\nnonlinear pose manifold. In the second stage, a point transformer network (PTN)\nestimates local deformation parameterized as thin-plate spline transformation\nfor a finer refinement. Our framework does not incorporate either handcrafted\nfeatures or part connectivity, which enables an end-to-end shape prediction\npipeline during both training and testing. In contrast to prior cascaded\nnetworks for landmark localization that learn a mapping from feature space to\nlandmark locations, we demonstrate that the regularization induced through\ngeometric priors in the DDN makes it easier to train, yet produces superior\nresults. The efficacy and generality of the architecture is demonstrated\nthrough state-of-the-art performances on several benchmarks for multiple tasks\nsuch as facial landmark localization, human body pose estimation and bird part\nlocalization.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:31:12 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 06:46:58 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Yu", "Xiang", ""], ["Zhou", "Feng", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1605.01042", "submitter": "Shayegan Omidshafiei", "authors": "Shayegan Omidshafiei, Brett T. Lopez, Jonathan P. How, John Vian", "title": "Hierarchical Bayesian Noise Inference for Robust Real-time Probabilistic\n  Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust environment perception is essential for decision-making on robots\noperating in complex domains. Principled treatment of uncertainty sources in a\nrobot's observation model is necessary for accurate mapping and object\ndetection. This is important not only for low-level observations (e.g.,\naccelerometer data), but for high-level observations such as semantic object\nlabels as well. This paper presents an approach for filtering sequences of\nobject classification probabilities using online modeling of the noise\ncharacteristics of the classifier outputs. A hierarchical Bayesian approach is\nused to model per-class noise distributions, while simultaneously allowing\nsharing of high-level noise characteristics between classes. The proposed\nfiltering scheme, called Hierarchical Bayesian Noise Inference (HBNI), is shown\nto outperform classification accuracy of existing methods. The paper also\npresents real-time filtered classification hardware experiments running fully\nonboard a moving quadrotor, where the proposed approach is demonstrated to work\nin a challenging domain where noise-agnostic filtering fails.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 19:42:18 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 03:02:14 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Omidshafiei", "Shayegan", ""], ["Lopez", "Brett T.", ""], ["How", "Jonathan P.", ""], ["Vian", "John", ""]]}, {"id": "1605.01101", "submitter": "Sourya Roy", "authors": "Avisek Lahiri, Sourya Roy, Anirban Santara, Pabitra Mitra, Prabir\n  Kumar Biswas", "title": "WEPSAM: Weakly Pre-Learnt Saliency Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency detection tries to mimic human vision psychology which\nconcentrates on sparse, important areas in natural image. Saliency prediction\nresearch has been traditionally based on low level features such as contrast,\nedge, etc. Recent thrust in saliency prediction research is to learn high level\nsemantics using ground truth eye fixation datasets. In this paper we present,\nWEPSAM : Weakly Pre-Learnt Saliency Model as a pioneering effort of using\ndomain specific pre-learing on ImageNet for saliency prediction using a light\nweight CNN architecture. The paper proposes a two step hierarchical learning,\nin which the first step is to develop a framework for weakly pre-training on a\nlarge scale dataset such as ImageNet which is void of human eye fixation maps.\nThe second step refines the pre-trained model on a limited set of ground truth\nfixations. Analysis of loss on iSUN and SALICON datasets reveal that\npre-trained network converges much faster compared to randomly initialized\nnetwork. WEPSAM also outperforms some recent state-of-the-art saliency\nprediction models on the challenging MIT300 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 21:47:33 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Lahiri", "Avisek", ""], ["Roy", "Sourya", ""], ["Santara", "Anirban", ""], ["Mitra", "Pabitra", ""], ["Biswas", "Prabir Kumar", ""]]}, {"id": "1605.01115", "submitter": "Mading Li", "authors": "Mading Li, Jiaying Liu, Zhiwei Xiong, Xiaoyan Sun, Zongming Guo", "title": "MARLow: A Joint Multiplanar Autoregressive and Low-Rank Approach for\n  Image Completion", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel multiplanar autoregressive (AR) model to\nexploit the correlation in cross-dimensional planes of a similar patch group\ncollected in an image, which has long been neglected by previous AR models. On\nthat basis, we then present a joint multiplanar AR and low-rank based approach\n(MARLow) for image completion from random sampling, which exploits the nonlocal\nself-similarity within natural images more effectively. Specifically, the\nmultiplanar AR model constraints the local stationarity in different\ncross-sections of the patch group, while the low-rank minimization captures the\nintrinsic coherence of nonlocal patches. The proposed approach can be readily\nextended to multichannel images (e.g. color images), by simultaneously\nconsidering the correlation in different channels. Experimental results\ndemonstrate that the proposed approach significantly outperforms\nstate-of-the-art methods, even if the pixel missing rate is as high as 90%.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 23:41:57 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 03:55:13 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Li", "Mading", ""], ["Liu", "Jiaying", ""], ["Xiong", "Zhiwei", ""], ["Sun", "Xiaoyan", ""], ["Guo", "Zongming", ""]]}, {"id": "1605.01130", "submitter": "Yaming Wang", "authors": "Yaming Wang, Jonghyun Choi, Vlad I. Morariu, Larry S. Davis", "title": "Mining Discriminative Triplets of Patches for Fine-Grained\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained classification involves distinguishing between similar\nsub-categories based on subtle differences in highly localized regions;\ntherefore, accurate localization of discriminative regions remains a major\nchallenge. We describe a patch-based framework to address this problem. We\nintroduce triplets of patches with geometric constraints to improve the\naccuracy of patch localization, and automatically mine discriminative\ngeometrically-constrained triplets for classification. The resulting approach\nonly requires object bounding boxes. Its effectiveness is demonstrated using\nfour publicly available fine-grained datasets, on which it outperforms or\nachieves comparable performance to the state-of-the-art in classification.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 02:34:18 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Wang", "Yaming", ""], ["Choi", "Jonghyun", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1605.01138", "submitter": "Jiajun Wu", "authors": "Renqiao Zhang, Jiajun Wu, Chengkai Zhang, William T. Freeman, Joshua\n  B. Tenenbaum", "title": "A Comparative Evaluation of Approximate Probabilistic Simulation and\n  Deep Neural Networks as Accounts of Human Physical Scene Understanding", "comments": "Accepted to CogSci 2016 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans demonstrate remarkable abilities to predict physical events in complex\nscenes. Two classes of models for physical scene understanding have recently\nbeen proposed: \"Intuitive Physics Engines\", or IPEs, which posit that people\nmake predictions by running approximate probabilistic simulations in causal\nmental models similar in nature to video-game physics engines, and memory-based\nmodels, which make judgments based on analogies to stored experiences of\npreviously encountered scenes and physical outcomes. Versions of the latter\nhave recently been instantiated in convolutional neural network (CNN)\narchitectures. Here we report four experiments that, to our knowledge, are the\nfirst rigorous comparisons of simulation-based and CNN-based models, where both\napproaches are concretely instantiated in algorithms that can run on raw image\ninputs and produce as outputs physical judgments such as whether a stack of\nblocks will fall. Both approaches can achieve super-human accuracy levels and\ncan quantitatively predict human judgments to a similar degree, but only the\nsimulation-based models generalize to novel situations in ways that people do,\nand are qualitatively consistent with systematic perceptual illusions and\njudgment asymmetries that people show.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 04:26:06 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 01:01:47 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Zhang", "Renqiao", ""], ["Wu", "Jiajun", ""], ["Zhang", "Chengkai", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1605.01141", "submitter": "Gui-Song Xia", "authors": "Gang Liu, Yann Gousseau, Gui-Song Xia", "title": "Texture Synthesis Through Convolutional Neural Networks and Spectrum\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a significant improvement for the synthesis of texture\nimages using convolutional neural networks (CNNs), making use of constraints on\nthe Fourier spectrum of the results. More precisely, the texture synthesis is\nregarded as a constrained optimization problem, with constraints conditioning\nboth the Fourier spectrum and statistical features learned by CNNs. In contrast\nwith existing methods, the presented method inherits from previous CNN\napproaches the ability to depict local structures and fine scale details, and\nat the same time yields coherent large scale structures, even in the case of\nquasi-periodic images. This is done at no extra computational cost. Synthesis\nexperiments on various images show a clear improvement compared to a recent\nstate-of-the art method relying on CNN constraints only.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 04:37:52 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 17:50:49 GMT"}, {"version": "v3", "created": "Thu, 19 May 2016 08:43:00 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Liu", "Gang", ""], ["Gousseau", "Yann", ""], ["Xia", "Gui-Song", ""]]}, {"id": "1605.01156", "submitter": "Yunjie Liu", "authors": "Yunjie Liu, Evan Racah, Prabhat, Joaquin Correa, Amir Khosrowshahi,\n  David Lavers, Kenneth Kunkel, Michael Wehner, William Collins", "title": "Application of Deep Convolutional Neural Networks for Detecting Extreme\n  Weather in Climate Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting extreme events in large datasets is a major challenge in climate\nscience research. Current algorithms for extreme event detection are build upon\nhuman expertise in defining events based on subjective thresholds of relevant\nphysical variables. Often, multiple competing methods produce vastly different\nresults on the same dataset. Accurate characterization of extreme events in\nclimate simulations and observational data archives is critical for\nunderstanding the trends and potential impacts of such events in a climate\nchange content. This study presents the first application of Deep Learning\ntechniques as alternative methodology for climate extreme events detection.\nDeep neural networks are able to learn high-level representations of a broad\nclass of patterns from labeled data. In this work, we developed deep\nConvolutional Neural Network (CNN) classification system and demonstrated the\nusefulness of Deep Learning technique for tackling climate pattern detection\nproblems. Coupled with Bayesian based hyper-parameter optimization scheme, our\ndeep CNN system achieves 89\\%-99\\% of accuracy in detecting extreme events\n(Tropical Cyclones, Atmospheric Rivers and Weather Fronts\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 06:38:19 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Liu", "Yunjie", ""], ["Racah", "Evan", ""], ["Prabhat", "", ""], ["Correa", "Joaquin", ""], ["Khosrowshahi", "Amir", ""], ["Lavers", "David", ""], ["Kunkel", "Kenneth", ""], ["Wehner", "Michael", ""], ["Collins", "William", ""]]}, {"id": "1605.01177", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Abu Sajana Rahmathullah, Lennart\n  Svensson", "title": "A metric on the space of finite sets of trajectories for evaluation of\n  multi-target tracking algorithms", "comments": "Matlab code for the metric is available at\n  https://github.com/Agarciafernandez/MTT", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 68, pp. 3917-3928,\n  2020", "doi": "10.1109/TSP.2020.3005309", "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a metric on the space of finite sets of\ntrajectories for assessing multi-target tracking algorithms in a mathematically\nsound way. The main use of the metric is to compare estimates of trajectories\nfrom different algorithms with the ground truth of trajectories. The proposed\nmetric includes intuitive costs associated to localization error for properly\ndetected targets, missed and false targets and track switches at each time\nstep. The metric computation is based on solving a multi-dimensional assignment\nproblem. We also propose a lower bound for the metric, which is also a metric\nfor sets of trajectories and is computable in polynomial time using linear\nprogramming. We also extend the proposed metrics on sets of trajectories to\nrandom finite sets of trajectories.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 08:17:24 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 08:36:32 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 10:03:02 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 15:14:14 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2020 11:22:32 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Rahmathullah", "Abu Sajana", ""], ["Svensson", "Lennart", ""]]}, {"id": "1605.01189", "submitter": "Sheraz Ahmed Dr.", "authors": "Sheraz Ahmed, Muhammad Imran Malik, Muhammad Zeshan Afzal, Koichi\n  Kise, Masakazu Iwamura, Andreas Dengel, Marcus Liwicki", "title": "A Generic Method for Automatic Ground Truth Generation of\n  Camera-captured Documents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this paper is fourfold. The first contribution is a\nnovel, generic method for automatic ground truth generation of camera-captured\ndocument images (books, magazines, articles, invoices, etc.). It enables us to\nbuild large-scale (i.e., millions of images) labeled camera-captured/scanned\ndocuments datasets, without any human intervention. The method is generic,\nlanguage independent and can be used for generation of labeled documents\ndatasets (both scanned and cameracaptured) in any cursive and non-cursive\nlanguage, e.g., English, Russian, Arabic, Urdu, etc. To assess the\neffectiveness of the presented method, two different datasets in English and\nRussian are generated using the presented method. Evaluation of samples from\nthe two datasets shows that 99:98% of the images were correctly labeled. The\nsecond contribution is a large dataset (called C3Wi) of camera-captured\ncharacters and words images, comprising 1 million word images (10 million\ncharacter images), captured in a real camera-based acquisition. This dataset\ncan be used for training as well as testing of character recognition systems on\ncamera-captured documents. The third contribution is a novel method for the\nrecognition of cameracaptured document images. The proposed method is based on\nLong Short-Term Memory and outperforms the state-of-the-art methods for camera\nbased OCRs. As a fourth contribution, various benchmark tests are performed to\nuncover the behavior of commercial (ABBYY), open source (Tesseract), and the\npresented camera-based OCR using the presented C3Wi dataset. Evaluation results\nreveal that the existing OCRs, which already get very high accuracies on\nscanned documents, have limited performance on camera-captured document images;\nwhere ABBYY has an accuracy of 75%, Tesseract an accuracy of 50.22%, while the\npresented character recognition system has an accuracy of 95.10%.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 09:25:04 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Ahmed", "Sheraz", ""], ["Malik", "Muhammad Imran", ""], ["Afzal", "Muhammad Zeshan", ""], ["Kise", "Koichi", ""], ["Iwamura", "Masakazu", ""], ["Dengel", "Andreas", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1605.01224", "submitter": "Karel Lenc", "authors": "Karel Lenc and Andrea Vedaldi", "title": "Learning Covariant Feature Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local covariant feature detection, namely the problem of extracting viewpoint\ninvariant features from images, has so far largely resisted the application of\nmachine learning techniques. In this paper, we propose the first fully general\nformulation for learning local covariant feature detectors. We propose to cast\ndetection as a regression problem, enabling the use of powerful regressors such\nas deep neural networks. We then derive a covariance constraint that can be\nused to automatically learn which visual structures provide stable anchors for\nlocal feature detection. We support these ideas theoretically, proposing a\nnovel analysis of local features in term of geometric transformations, and we\nshow that all common and many uncommon detectors can be derived in this\nframework. Finally, we present empirical results on translation and rotation\ncovariant detectors on standard feature benchmarks, showing the power and\nflexibility of the framework.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 11:11:07 GMT"}, {"version": "v2", "created": "Fri, 9 Sep 2016 14:44:36 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1605.01242", "submitter": "Olivier Guye", "authors": "Olivier Guye", "title": "Hierarchical Modeling of Multidimensional Data in Regularly Decomposed\n  Spaces: Applications in Image Analysis", "comments": "172 pages, 52 figures, research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This last document is showing the gradual introduction of hierarchical\nmodeling techniques in image analysis. The first chapter is dealing with the\nfirst works carried out in the field of industrial applications of pattern\nrecognition. The second chapter is focusing on the usage of these techniques in\nsatellite imagery and on the development of a satellite data archiving system\nin the aim of using it in digital geography. The third chapter is about face\nrecognition based on planar image analysis and about the recognition of\npartially hidden patterns. The present publication is ending with the\ndescription of a future system of self-descriptive coding of still or moving\npictures in relation with the current video coding standards. As in the\nprevious documents, it will be found in annex algorithms targeted on image\nanalysis according two complementary approaches: - boundary-based approach for\nthe industrial applications of artificial vision; - region-based approach for\nsatellite image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 12:15:45 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Guye", "Olivier", ""]]}, {"id": "1605.01368", "submitter": "Mehran Javanmardi", "authors": "Mehran Javanmardi, Mehdi Sajjadi, Ting Liu and Tolga Tasdizen", "title": "Unsupervised Total Variation Loss for Semi-supervised Deep Learning of\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel unsupervised loss function for learning semantic\nsegmentation with deep convolutional neural nets (ConvNet) when densely labeled\ntraining images are not available. More specifically, the proposed loss\nfunction penalizes the L1-norm of the gradient of the label probability vector\nimage , i.e. total variation, produced by the ConvNet. This can be seen as a\nregularization term that promotes piecewise smoothness of the label probability\nvector image produced by the ConvNet during learning. The unsupervised loss\nfunction is combined with a supervised loss in a semi-supervised setting to\nlearn ConvNets that can achieve high semantic segmentation accuracy even when\nonly a tiny percentage of the pixels in the training images are labeled. We\ndemonstrate significant improvements over the purely supervised setting in the\nWeizmann horse, Stanford background and Sift Flow datasets. Furthermore, we\nshow that using the proposed piecewise smoothness constraint in the learning\nphase significantly outperforms post-processing results from a purely\nsupervised approach with Markov Random Fields (MRF). Finally, we note that the\nframework we introduce is general and can be used to learn to label other types\nof structures such as curvilinear structures by modifying the unsupervised loss\nfunction accordingly.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 18:17:25 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 20:46:35 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 20:21:16 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Javanmardi", "Mehran", ""], ["Sajjadi", "Mehdi", ""], ["Liu", "Ting", ""], ["Tasdizen", "Tolga", ""]]}, {"id": "1605.01369", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Abhinav Vishnu, Chris Ding", "title": "Accelerating Deep Learning with Shrinkage and Recall", "comments": "The 22nd IEEE International Conference on Parallel and Distributed\n  Systems (ICPADS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning is a very powerful machine learning model. Deep Learning trains\na large number of parameters for multiple layers and is very slow when data is\nin large scale and the architecture size is large. Inspired from the shrinking\ntechnique used in accelerating computation of Support Vector Machines (SVM)\nalgorithm and screening technique used in LASSO, we propose a shrinking Deep\nLearning with recall (sDLr) approach to speed up deep learning computation. We\nexperiment shrinking Deep Learning with recall (sDLr) using Deep Neural Network\n(DNN), Deep Belief Network (DBN) and Convolution Neural Network (CNN) on 4 data\nsets. Results show that the speedup using shrinking Deep Learning with recall\n(sDLr) can reach more than 2.0 while still giving competitive classification\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 18:17:37 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 19:27:39 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Zheng", "Shuai", ""], ["Vishnu", "Abhinav", ""], ["Ding", "Chris", ""]]}, {"id": "1605.01379", "submitter": "Xiao Lin", "authors": "Xiao Lin, Devi Parikh", "title": "Leveraging Visual Question Answering for Image-Caption Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is the task of taking as input an image and a\nfree-form natural language question about the image, and producing an accurate\nanswer. In this work we view VQA as a \"feature extraction\" module to extract\nimage and caption representations. We employ these representations for the task\nof image-caption ranking. Each feature dimension captures (imagines) whether a\nfact (question-answer pair) could plausibly be true for the image and caption.\nThis allows the model to interpret images and captions from a wide variety of\nperspectives. We propose score-level and representation-level fusion models to\nincorporate VQA knowledge in an existing state-of-the-art VQA-agnostic\nimage-caption ranking model. We find that incorporating and reasoning about\nconsistency between images and captions significantly improves performance.\nConcretely, our model improves state-of-the-art on caption retrieval by 7.1%\nand on image retrieval by 4.4% on the MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 18:54:09 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 20:14:12 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Lin", "Xiao", ""], ["Parikh", "Devi", ""]]}, {"id": "1605.01397", "submitter": "Noel Codella", "authors": "David Gutman, Noel C. F. Codella, Emre Celebi, Brian Helba, Michael\n  Marchetti, Nabin Mishra, Allan Halpern", "title": "Skin Lesion Analysis toward Melanoma Detection: A Challenge at the\n  International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the\n  International Skin Imaging Collaboration (ISIC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we describe the design and implementation of a publicly\naccessible dermatology image analysis benchmark challenge. The goal of the\nchallenge is to sup- port research and development of algorithms for automated\ndiagnosis of melanoma, a lethal form of skin cancer, from dermoscopic images.\nThe challenge was divided into sub-challenges for each task involved in image\nanalysis, including lesion segmentation, dermoscopic feature detection within a\nlesion, and classification of melanoma. Training data included 900 images. A\nseparate test dataset of 379 images was provided to measure resultant\nperformance of systems developed with the training data. Ground truth for both\ntraining and test sets was generated by a panel of dermoscopic experts. In\ntotal, there were 79 submissions from a group of 38 participants, making this\nthe largest standardized and comparative study for melanoma diagnosis in\ndermoscopic images to date. While the official challenge duration and ranking\nof participants has concluded, the datasets remain available for further\nresearch and development.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:49:17 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Gutman", "David", ""], ["Codella", "Noel C. F.", ""], ["Celebi", "Emre", ""], ["Helba", "Brian", ""], ["Marchetti", "Michael", ""], ["Mishra", "Nabin", ""], ["Halpern", "Allan", ""]]}, {"id": "1605.01569", "submitter": "Matthias Plappert", "authors": "Matthias Plappert", "title": "Classification of Human Whole-Body Motion using Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human motion plays an important role in many fields. Large databases exist\nthat store and make available recordings of human motions. However, annotating\neach motion with multiple labels is a cumbersome and error-prone process. This\nbachelor's thesis presents different approaches to solve the multi-label\nclassification problem using Hidden Markov Models (HMMs). First, different\nfeatures that can be directly obtained from the raw data are introduced. Next,\nadditional features are derived to improve classification performance. These\nfeatures are then used to perform the multi-label classification using two\ndifferent approaches. The first approach simply transforms the multi-label\nproblem into a multi-class problem. The second, novel approach solves the same\nproblem without the need to construct a transformation by predicting the labels\ndirectly from the likelihood scores. The second approach scales linearly with\nthe number of labels whereas the first approach is subject to combinatorial\nexplosion. All aspects of the classification process are evaluated on a data\nset that consists of 454 motions. System 1 achieves an accuracy of 98.02% and\nsystem 2 an accuracy of 93.39% on the test set.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 12:38:18 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Plappert", "Matthias", ""]]}, {"id": "1605.01576", "submitter": "Tao Zhou", "authors": "Tao Zhou, Brian Johnson, Rui Li", "title": "Patch-based Texture Synthesis for Image Inpainting", "comments": "in Computer Science and Applications, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image inpaiting is an important task in image processing and vision. In this\npaper, we develop a general method for patch-based image inpainting by\nsynthesizing new textures from existing one. A novel framework is introduced to\nfind several optimal candidate patches and generate a new texture patch in the\nprocess. We form it as an optimization problem that identifies the potential\npatches for synthesis from an coarse-to-fine manner. We use the texture\ndescriptor as a clue in searching for matching patches from the known region.\nTo ensure the structure faithful to the original image, a geometric constraint\nmetric is formally defined that is applied directly to the patch synthesis\nprocedure. We extensively conducted our experiments on a wide range of testing\nimages on various scenarios and contents by arbitrarily specifying the target\nthe regions for inference followed by using existing evaluation metrics to\nverify its texture coherency and structural consistency. Our results\ndemonstrate the high accuracy and desirable output that can be potentially used\nfor numerous applications: object removal, background subtraction, and image\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 13:04:09 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Zhou", "Tao", ""], ["Johnson", "Brian", ""], ["Li", "Rui", ""]]}, {"id": "1605.01600", "submitter": "Fabien Ringeval", "authors": "Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval,\n  Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy\n  Cowie and Maja Pantic", "title": "AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and\n  Challenge", "comments": "Proceedings of the 6th International Workshop on Audio/Visual Emotion\n  Challenge, AVEC'16, co-located with the 24th ACM International Conference on\n  Multimedia, MM 2016, pages 3-10, Amsterdam, The Netherlands, October 2016.\n  ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood\nand Emotion\" will be the sixth competition event aimed at comparison of\nmultimedia processing and machine learning methods for automatic audio, visual\nand physiological depression and emotion analysis, with all participants\ncompeting under strictly the same conditions. The goal of the Challenge is to\nprovide a common benchmark test set for multi-modal information processing and\nto bring together the depression and emotion recognition communities, as well\nas the audio, video and physiological processing communities, to compare the\nrelative merits of the various approaches to depression and emotion recognition\nunder well-defined and strictly comparable conditions and establish to what\nextent fusion of the approaches is possible and beneficial. This paper presents\nthe challenge guidelines, the common data used, and the performance of the\nbaseline system on the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 14:04:50 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 12:34:51 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 08:02:32 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 15:19:24 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Valstar", "Michel", ""], ["Gratch", "Jonathan", ""], ["Schuller", "Bjorn", ""], ["Ringeval", "Fabien", ""], ["Lalanne", "Denis", ""], ["Torres", "Mercedes Torres", ""], ["Scherer", "Stefan", ""], ["Stratou", "Guiota", ""], ["Cowie", "Roddy", ""], ["Pantic", "Maja", ""]]}, {"id": "1605.01643", "submitter": "Jonathan Bates", "authors": "Jonathan Bates", "title": "The embedding dimension of Laplacian eigenfunction maps", "comments": "16 pages, 2 figures, 3 theorems, and a torus in a pear tree", "journal-ref": "Appl. Comput. Harmon. Anal. 37 (3) (2014) 516-530", "doi": "10.1016/j.acha.2014.03.002", "report-no": null, "categories": "stat.ML cs.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any closed, connected Riemannian manifold $M$ can be smoothly embedded by its\nLaplacian eigenfunction maps into $\\mathbb{R}^m$ for some $m$. We call the\nsmallest such $m$ the maximal embedding dimension of $M$. We show that the\nmaximal embedding dimension of $M$ is bounded from above by a constant\ndepending only on the dimension of $M$, a lower bound for injectivity radius, a\nlower bound for Ricci curvature, and a volume bound. We interpret this result\nfor the case of surfaces isometrically immersed in $\\mathbb{R}^3$, showing that\nthe maximal embedding dimension only depends on bounds for the Gaussian\ncurvature, mean curvature, and surface area. Furthermore, we consider the\nrelevance of these results for shape registration.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:18:51 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Bates", "Jonathan", ""]]}, {"id": "1605.01679", "submitter": "Nicholas Rhinehart", "authors": "Nicholas Rhinehart, Kris M. Kitani", "title": "Learning Action Maps of Large Environments via First-Person Vision", "comments": "To appear at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people observe and interact with physical spaces, they are able to\nassociate functionality to regions in the environment. Our goal is to automate\ndense functional understanding of large spaces by leveraging sparse activity\ndemonstrations recorded from an ego-centric viewpoint. The method we describe\nenables functionality estimation in large scenes where people have behaved, as\nwell as novel scenes where no behaviors are observed. Our method learns and\npredicts \"Action Maps\", which encode the ability for a user to perform\nactivities at various locations. With the usage of an egocentric camera to\nobserve human activities, our method scales with the size of the scene without\nthe need for mounting multiple static surveillance cameras and is well-suited\nto the task of observing activities up-close. We demonstrate that by capturing\nappearance-based attributes of the environment and associating these attributes\nwith activity demonstrations, our proposed mathematical framework allows for\nthe prediction of Action Maps in new environments. Additionally, we offer a\npreliminary glance of the applicability of Action Maps by demonstrating a\nproof-of-concept application in which they are used in concert with activity\ndetections to perform localization.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 18:22:30 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Rhinehart", "Nicholas", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1605.01710", "submitter": "Stanley Chan", "authors": "Stanley H. Chan and Xiran Wang and Omar A. Elgendy", "title": "Plug-and-Play ADMM for Image Restoration: Fixed Point Convergence and\n  Applications", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternating direction method of multiplier (ADMM) is a widely used algorithm\nfor solving constrained optimization problems in image restoration. Among many\nuseful features, one critical feature of the ADMM algorithm is its modular\nstructure which allows one to plug in any off-the-shelf image denoising\nalgorithm for a subproblem in the ADMM algorithm. Because of the plug-in\nnature, this type of ADMM algorithms is coined the name \"Plug-and-Play ADMM\".\nPlug-and-Play ADMM has demonstrated promising empirical results in a number of\nrecent papers. However, it is unclear under what conditions and by using what\ndenoising algorithms would it guarantee convergence. Also, since Plug-and-Play\nADMM uses a specific way to split the variables, it is unclear if fast\nimplementation can be made for common Gaussian and Poissonian image restoration\nproblems.\n  In this paper, we propose a Plug-and-Play ADMM algorithm with provable fixed\npoint convergence. We show that for any denoising algorithm satisfying an\nasymptotic criteria, called bounded denoisers, Plug-and-Play ADMM converges to\na fixed point under a continuation scheme. We also present fast implementations\nfor two image restoration problems on super-resolution and single-photon\nimaging. We compare Plug-and-Play ADMM with state-of-the-art algorithms in each\nproblem type, and demonstrate promising experimental results of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:49:19 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 18:12:48 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Chan", "Stanley H.", ""], ["Wang", "Xiran", ""], ["Elgendy", "Omar A.", ""]]}, {"id": "1605.01713", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, Anshul Kundaje", "title": "Not Just a Black Box: Learning Important Features Through Propagating\n  Activation Differences", "comments": "6 pages, 3 figures, this is an older version; see\n  https://arxiv.org/abs/1704.02685 for the newer version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Note: This paper describes an older version of DeepLIFT. See\nhttps://arxiv.org/abs/1704.02685 for the newer version. Original abstract\nfollows: The purported \"black box\" nature of neural networks is a barrier to\nadoption in applications where interpretability is essential. Here we present\nDeepLIFT (Learning Important FeaTures), an efficient and effective method for\ncomputing importance scores in a neural network. DeepLIFT compares the\nactivation of each neuron to its 'reference activation' and assigns\ncontribution scores according to the difference. We apply DeepLIFT to models\ntrained on natural images and genomic data, and show significant advantages\nover gradient-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 19:52:32 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 21:34:42 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 15:58:48 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Greenside", "Peyton", ""], ["Shcherbina", "Anna", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1605.01775", "submitter": "Andras Rozsa", "authors": "Andras Rozsa, Ethan M. Rudd, and Terrance E. Boult", "title": "Adversarial Diversity and Hard Positive Generation", "comments": "Accepted to CVPR 2016 DeepVision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks suffer from a fundamental problem -\nthey misclassify adversarial examples formed by applying small perturbations to\ninputs. In this paper, we present a new psychometric perceptual adversarial\nsimilarity score (PASS) measure for quantifying adversarial images, introduce\nthe notion of hard positive generation, and use a diverse set of adversarial\nperturbations - not just the closest ones - for data augmentation. We introduce\na novel hot/cold approach for adversarial example generation, which provides\nmultiple possible adversarial perturbations for every single image. The\nperturbations generated by our novel approach often correspond to semantically\nmeaningful image structures, and allow greater flexibility to scale\nperturbation-amplitudes, which yields an increased diversity of adversarial\nimages. We present adversarial images on several network topologies and\ndatasets, including LeNet on the MNIST dataset, and GoogLeNet and ResidualNet\non the ImageNet dataset. Finally, we demonstrate on LeNet and GoogLeNet that\nfine-tuning with a diverse set of hard positives improves the robustness of\nthese networks compared to training with prior methods of generating\nadversarial images.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 22:09:35 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 02:46:39 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Rozsa", "Andras", ""], ["Rudd", "Ethan M.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1605.01790", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald, Edmund Zelnio, Alfred Hero", "title": "Robust SAR STAP via Kronecker Decomposition", "comments": "to appear at IEEE AES. arXiv admin note: text overlap with\n  arXiv:1604.03622, arXiv:1501.07481", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a spatio-temporal decomposition for the detection of\nmoving targets in multiantenna SAR. As a high resolution radar imaging\nmodality, SAR detects and localizes non-moving targets accurately, giving it an\nadvantage over lower resolution GMTI radars. Moving target detection is more\nchallenging due to target smearing and masking by clutter. Space-time adaptive\nprocessing (STAP) is often used to remove the stationary clutter and enhance\nthe moving targets. In this work, it is shown that the performance of STAP can\nbe improved by modeling the clutter covariance as a space vs. time Kronecker\nproduct with low rank factors. Based on this model, a low-rank Kronecker\nproduct covariance estimation algorithm is proposed, and a novel separable\nclutter cancelation filter based on the Kronecker covariance estimate is\nintroduced. The proposed method provides orders of magnitude reduction in the\nrequired number of training samples, as well as improved robustness to\ncorruption of the training data. Simulation results and experiments using the\nGotcha SAR GMTI challenge dataset are presented that confirm the advantages of\nour approach relative to existing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 23:53:32 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Zelnio", "Edmund", ""], ["Hero", "Alfred", ""]]}, {"id": "1605.01813", "submitter": "Sohil Shah", "authors": "Sohil Shah, Tom Goldstein and Christoph Studer", "title": "Estimating Sparse Signals with Smooth Support via Convex Programming and\n  Block Sparsity", "comments": "To appear at CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional algorithms for sparse signal recovery and sparse representation\nrely on $l_1$-norm regularized variational methods. However, when applied to\nthe reconstruction of $\\textit{sparse images}$, i.e., images where only a few\npixels are non-zero, simple $l_1$-norm-based methods ignore potential\ncorrelations in the support between adjacent pixels. In a number of\napplications, one is interested in images that are not only sparse, but also\nhave a support with smooth (or contiguous) boundaries. Existing algorithms that\ntake into account such a support structure mostly rely on non-convex methods\nand---as a consequence---do not scale well to high-dimensional problems and/or\ndo not converge to global optima. In this paper, we explore the use of new\nblock $l_1$-norm regularizers, which enforce image sparsity while\nsimultaneously promoting smooth support structure. By exploiting the convexity\nof our regularizers, we develop new computationally-efficient recovery\nalgorithms that guarantee global optimality. We demonstrate the efficacy of our\nregularizers on a variety of imaging tasks including compressive image\nrecovery, image restoration, and robust PCA.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 03:58:25 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Shah", "Sohil", ""], ["Goldstein", "Tom", ""], ["Studer", "Christoph", ""]]}, {"id": "1605.01825", "submitter": "Jiaolong Yang", "authors": "Jiaolong Yang, Hongdong Li, Yuchao Dai, Robby T. Tan", "title": "Robust Optical Flow Estimation of Double-Layer Images under Transparency\n  or Reflection", "comments": "to appear at CVPR 2016 (IEEE Conference on Computer Vision and\n  Pattern Recognition)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with a challenging, frequently encountered, yet not properly\ninvestigated problem in two-frame optical flow estimation. That is, the input\nframes are compounds of two imaging layers -- one desired background layer of\nthe scene, and one distracting, possibly moving layer due to transparency or\nreflection. In this situation, the conventional brightness constancy constraint\n-- the cornerstone of most existing optical flow methods -- will no longer be\nvalid. In this paper, we propose a robust solution to this problem. The\nproposed method performs both optical flow estimation, and image layer\nseparation. It exploits a generalized double-layer brightness consistency\nconstraint connecting these two tasks, and utilizes the priors for both of\nthem. Experiments on both synthetic data and real images have confirmed the\nefficacy of the proposed method. To the best of our knowledge, this is the\nfirst attempt towards handling generic optical flow fields of two-frame images\ncontaining transparency or reflection.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 05:26:56 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Yang", "Jiaolong", ""], ["Li", "Hongdong", ""], ["Dai", "Yuchao", ""], ["Tan", "Robby T.", ""]]}, {"id": "1605.01839", "submitter": "Gao Zhu", "authors": "Gao Zhu, Fatih Porikli, and Hongdong Li", "title": "Beyond Local Search: Tracking Objects Everywhere with Instance-Specific\n  Proposals", "comments": "CVPR'16. arXiv admin note: text overlap with arXiv:1507.08085", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tracking-by-detection methods employ a local search window around the\npredicted object location in the current frame assuming the previous location\nis accurate, the trajectory is smooth, and the computational capacity permits a\nsearch radius that can accommodate the maximum speed yet small enough to reduce\nmismatches. These, however, may not be valid always, in particular for fast and\nirregularly moving objects. Here, we present an object tracker that is not\nlimited to a local search window and has ability to probe efficiently the\nentire frame. Our method generates a small number of \"high-quality\" proposals\nby a novel instance-specific objectness measure and evaluates them against the\nobject model that can be adopted from an existing tracking-by-detection\napproach as a core tracker. During the tracking process, we update the object\nmodel concentrating on hard false-positives supplied by the proposals, which\nhelp suppressing distractors caused by difficult background clutters, and learn\nhow to re-rank proposals according to the object model. Since we reduce\nsignificantly the number of hypotheses the core tracker evaluates, we can use\nricher object descriptors and stronger detector. Our method outperforms most\nrecent state-of-the-art trackers on popular tracking benchmarks, and provides\nimproved robustness for fast moving objects as well as for ultra low-frame-rate\nvideos.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 06:53:48 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Zhu", "Gao", ""], ["Porikli", "Fatih", ""], ["Li", "Hongdong", ""]]}, {"id": "1605.01843", "submitter": "Shaodi You", "authors": "Shaodi You, Nick Barnes and Janine Walker", "title": "Perceptually Consistent Color-to-Gray Image Conversion", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a color to grayscale image conversion algorithm\n(C2G) that aims to preserve the perceptual properties of the color image as\nmuch as possible. To this end, we propose measures for two perceptual\nproperties based on contemporary research in vision science: brightness and\nmulti-scale contrast. The brightness measurement is based on the idea that the\nbrightness of a grayscale image will affect the perception of the probability\nof color information. The color contrast measurement is based on the idea that\nthe contrast of a given pixel to its surroundings can be measured as a linear\ncombination of color contrast at different scales. Based on these measures we\npropose a graph based optimization framework to balance the brightness and\ncontrast measurements. To solve the optimization, an $\\ell_1$-norm based method\nis provided which converts color discontinuities to brightness discontinuities.\nTo validate our methods, we evaluate against the existing \\cadik and Color250\ndatasets, and against NeoColor, a new dataset that improves over existing C2G\ndatasets. NeoColor contains around 300 images from typical C2G scenarios,\nincluding: commercial photograph, printing, books, magazines, masterpiece\nartworks and computer designed graphics. We show improvements in metrics of\nperformance, and further through a user study, we validate the performance of\nboth the algorithm and the metric.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 07:13:48 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["You", "Shaodi", ""], ["Barnes", "Nick", ""], ["Walker", "Janine", ""]]}, {"id": "1605.01923", "submitter": "Christian Mostegel", "authors": "Christian Mostegel, Markus Rumpler, Friedrich Fraundorfer, Horst\n  Bischof", "title": "UAV-based Autonomous Image Acquisition with Multi-View Stereo Quality\n  Assurance by Confidence Prediction", "comments": "This paper was accepted to the 7th International Workshop on Computer\n  Vision in Vehicle Technology (CVVT 2016) and will appear in IEEE Conference\n  on Computer Vision and Pattern Recognition Workshops (CVPRW), 2016. The\n  copyright was transferred to IEEE (ieee.org). The paper will be available on\n  IEEE Xplore(ieeexplore.ieee.org). This version of the paper also contains the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an autonomous system for acquiring close-range\nhigh-resolution images that maximize the quality of a later-on 3D\nreconstruction with respect to coverage, ground resolution and 3D uncertainty.\nIn contrast to previous work, our system uses the already acquired images to\npredict the confidence in the output of a dense multi-view stereo approach\nwithout executing it. This confidence encodes the likelihood of a successful\nreconstruction with respect to the observed scene and potential camera\nconstellations. Our prediction module runs in real-time and can be trained\nwithout any externally recorded ground truth. We use the confidence prediction\nfor on-site quality assurance and for planning further views that are tailored\nfor a specific multi-view stereo approach with respect to the given scene. We\ndemonstrate the capabilities of our approach with an autonomous Unmanned Aerial\nVehicle (UAV) in a challenging outdoor scenario.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 13:00:57 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Mostegel", "Christian", ""], ["Rumpler", "Markus", ""], ["Fraundorfer", "Friedrich", ""], ["Bischof", "Horst", ""]]}, {"id": "1605.01999", "submitter": "Jian Li DR", "authors": "Jian Li and Martin Levine and Xiangjing An and Xin Xu and Hangen He", "title": "Visual Saliency Based on Scale-Space Analysis in the Frequency Domain", "comments": null, "journal-ref": "Pattern Analysis and Machine Intelligence, IEEE Transactions on\n  35.4 (2013): 996-1010", "doi": "10.1109/TPAMI.2012.147", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of visual saliency from three perspectives. First, we\nconsider saliency detection as a frequency domain analysis problem. Second, we\nachieve this by employing the concept of {\\it non-saliency}. Third, we\nsimultaneously consider the detection of salient regions of different size. The\npaper proposes a new bottom-up paradigm for detecting visual saliency,\ncharacterized by a scale-space analysis of the amplitude spectrum of natural\nimages. We show that the convolution of the {\\it image amplitude spectrum} with\na low-pass Gaussian kernel of an appropriate scale is equivalent to such an\nimage saliency detector. The saliency map is obtained by reconstructing the 2-D\nsignal using the original phase and the amplitude spectrum, filtered at a scale\nselected by minimizing saliency map entropy. A Hypercomplex Fourier Transform\nperforms the analysis in the frequency domain. Using available databases, we\ndemonstrate experimentally that the proposed model can predict human fixation\ndata. We also introduce a new image database and use it to show that the\nsaliency detector can highlight both small and large salient regions, as well\nas inhibit repeated distractors in cluttered images. In addition, we show that\nit is able to predict salient regions on which people focus their attention.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 16:32:33 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Li", "Jian", ""], ["Levine", "Martin", ""], ["An", "Xiangjing", ""], ["Xu", "Xin", ""], ["He", "Hangen", ""]]}, {"id": "1605.02029", "submitter": "Tommaso Mansi", "authors": "Dorin Comaniciu, Klaus Engel, Bogdan Georgescu, Tommaso Mansi", "title": "Shaping the Future through Innovations: From Medical Imaging to\n  Precision Medicine", "comments": "Submitted to Medical Image Analysis, Elsevier, 20th Anniversary\n  Special Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images constitute a source of information essential for disease\ndiagnosis, treatment and follow-up. In addition, due to its patient-specific\nnature, imaging information represents a critical component required for\nadvancing precision medicine into clinical practice. This manuscript describes\nrecently developed technologies for better handling of image information:\nphotorealistic visualization of medical images with Cinematic Rendering,\nartificial agents for in-depth image understanding, support for minimally\ninvasive procedures, and patient-specific computational models with enhanced\npredictive power. Throughout the manuscript we will analyze the capabilities of\nsuch technologies and extrapolate on their potential impact to advance the\nquality of medical care, while reducing its cost.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 20:02:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 21:02:19 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Comaniciu", "Dorin", ""], ["Engel", "Klaus", ""], ["Georgescu", "Bogdan", ""], ["Mansi", "Tommaso", ""]]}, {"id": "1605.02057", "submitter": "Igor Fedorov", "authors": "Igor Fedorov, Ritwik Giri, Bhaskar D. Rao, Truong Q. Nguyen", "title": "Robust Bayesian Method for Simultaneous Block Sparse Signal Recovery\n  with Applications to Face Recognition", "comments": "To appear in ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel Bayesian approach to recover simultaneously\nblock sparse signals in the presence of outliers. The key advantage of our\nproposed method is the ability to handle non-stationary outliers, i.e. outliers\nwhich have time varying support. We validate our approach with empirical\nresults showing the superiority of the proposed method over competing\napproaches in synthetic data experiments as well as the multiple measurement\nface recognition problem.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:41:25 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 20:52:12 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Fedorov", "Igor", ""], ["Giri", "Ritwik", ""], ["Rao", "Bhaskar D.", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1605.02060", "submitter": "Kwame Kutten", "authors": "Kwame S. Kutten, Joshua T. Vogelstein, Nicolas Charon, Li Ye, Karl\n  Deisseroth, Michael I. Miller", "title": "Deformably Registering and Annotating Whole CLARITY Brains to an Atlas\n  via Masked LDDMM", "comments": null, "journal-ref": "Proc. SPIE 9896 Optics, Photonics and Digital Technologies for\n  Imaging Applications IV (2016)", "doi": "10.1117/12.2227444", "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CLARITY method renders brains optically transparent to enable\nhigh-resolution imaging in the structurally intact brain. Anatomically\nannotating CLARITY brains is necessary for discovering which regions contain\nsignals of interest. Manually annotating whole-brain, terabyte CLARITY images\nis difficult, time-consuming, subjective, and error-prone. Automatically\nregistering CLARITY images to a pre-annotated brain atlas offers a solution,\nbut is difficult for several reasons. Removal of the brain from the skull and\nsubsequent storage and processing cause variable non-rigid deformations, thus\ncompounding inter-subject anatomical variability. Additionally, the signal in\nCLARITY images arises from various biochemical contrast agents which only\nsparsely label brain structures. This sparse labeling challenges the most\ncommonly used registration algorithms that need to match image histogram\nstatistics to the more densely labeled histological brain atlases. The standard\nmethod is a multiscale Mutual Information B-spline algorithm that dynamically\ngenerates an average template as an intermediate registration target. We\ndetermined that this method performs poorly when registering CLARITY brains to\nthe Allen Institute's Mouse Reference Atlas (ARA), because the image histogram\nstatistics are poorly matched. Therefore, we developed a method (Mask-LDDMM)\nfor registering CLARITY images, that automatically find the brain boundary and\nlearns the optimal deformation between the brain and atlas masks. Using\nMask-LDDMM without an average template provided better results than the\nstandard approach when registering CLARITY brains to the ARA. The LDDMM\npipelines developed here provide a fast automated way to anatomically annotate\nCLARITY images. Our code is available as open source software at\nhttp://NeuroData.io.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 19:51:27 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Kutten", "Kwame S.", ""], ["Vogelstein", "Joshua T.", ""], ["Charon", "Nicolas", ""], ["Ye", "Li", ""], ["Deisseroth", "Karl", ""], ["Miller", "Michael I.", ""]]}, {"id": "1605.02066", "submitter": "Vage Taamazyan", "authors": "Vage Taamazyan, Achuta Kadambi, Ramesh Raskar", "title": "Shape from Mixed Polarization", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape from Polarization (SfP) estimates surface normals using photos captured\nat different polarizer rotations. Fundamentally, the SfP model assumes that\nlight is reflected either diffusely or specularly. However, this model is not\nvalid for many real-world surfaces exhibiting a mixture of diffuse and specular\nproperties. To address this challenge, previous methods have used a sequential\nsolution: first, use an existing algorithm to separate the scene into diffuse\nand specular components, then apply the appropriate SfP model. In this paper,\nwe propose a new method that jointly uses viewpoint and polarization data to\nholistically separate diffuse and specular components, recover refractive\nindex, and ultimately recover 3D shape. By involving the physics of\npolarization in the separation process, we demonstrate competitive results with\na benchmark method, while recovering additional information (e.g. refractive\nindex).\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 01:03:11 GMT"}, {"version": "v2", "created": "Sat, 11 Jun 2016 11:56:26 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Taamazyan", "Vage", ""], ["Kadambi", "Achuta", ""], ["Raskar", "Ramesh", ""]]}, {"id": "1605.02097", "submitter": "Wojciech Ja\\'skowski", "authors": "Micha{\\l} Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek and\n  Wojciech Ja\\'skowski", "title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement\n  Learning", "comments": null, "journal-ref": "Proceedings of IEEE Conference of Computational Intelligence in\n  Games 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advances in deep neural networks have led to effective\nvision-based reinforcement learning methods that have been employed to obtain\nhuman-level controllers in Atari 2600 games from pixel data. Atari 2600 games,\nhowever, do not resemble real-world tasks since they involve non-realistic 2D\nenvironments and the third-person perspective. Here, we propose a novel\ntest-bed platform for reinforcement learning research from raw visual\ninformation which employs the first-person perspective in a semi-realistic 3D\nworld. The software, called ViZDoom, is based on the classical first-person\nshooter video game, Doom. It allows developing bots that play the game using\nthe screen buffer. ViZDoom is lightweight, fast, and highly customizable via a\nconvenient mechanism of user scenarios. In the experimental part, we test the\nenvironment by trying to learn bots for two scenarios: a basic move-and-shoot\ntask and a more complex maze-navigation problem. Using convolutional deep\nneural networks with Q-learning and experience replay, for both scenarios, we\nwere able to train competent bots, which exhibit human-like behaviors. The\nresults confirm the utility of ViZDoom as an AI research platform and imply\nthat visual reinforcement learning in 3D realistic first-person perspective\nenvironments is feasible.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 20:46:34 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 19:12:49 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Kempka", "Micha\u0142", ""], ["Wydmuch", "Marek", ""], ["Runc", "Grzegorz", ""], ["Toczek", "Jakub", ""], ["Ja\u015bkowski", "Wojciech", ""]]}, {"id": "1605.02112", "submitter": "Seyoung Park", "authors": "Seyoung Park, Bruce Xiaohan Nie, Song-Chun Zhu", "title": "Attribute And-Or Grammar for Joint Parsing of Human Attributes, Part and\n  Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an attribute and-or grammar (A-AOG) model for jointly\ninferring human body pose and human attributes in a parse graph with attributes\naugmented to nodes in the hierarchical representation. In contrast to other\npopular methods in the current literature that train separate classifiers for\nposes and individual attributes, our method explicitly represents the\ndecomposition and articulation of body parts, and account for the correlations\nbetween poses and attributes. The A-AOG model is an amalgamation of three\ntraditional grammar formulations: (i) Phrase structure grammar representing the\nhierarchical decomposition of the human body from whole to parts; (ii)\nDependency grammar modeling the geometric articulation by a kinematic graph of\nthe body pose; and (iii) Attribute grammar accounting for the compatibility\nrelations between different parts in the hierarchy so that their appearances\nfollow a consistent style. The parse graph outputs human detection, pose\nestimation, and attribute prediction simultaneously, which are intuitive and\ninterpretable. We conduct experiments on two tasks on two datasets, and\nexperimental results demonstrate the advantage of joint modeling in comparison\nwith computing poses and attributes independently. Furthermore, our model\nobtains better performance over existing methods for both pose estimation and\nattribute prediction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 22:23:41 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 20:10:52 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Park", "Seyoung", ""], ["Nie", "Bruce Xiaohan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1605.02140", "submitter": "Immanuel Manohar", "authors": "Jacob Chakareski, Immanuel Manohar and Shantanu Rane", "title": "Matrix Factorization-Based Clustering Of Image Features For\n  Bandwidth-Constrained Information Retrieval", "comments": "6 Pages, 7 figures, ICMEWorkshop 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of accurately and efficiently querying a remote\nserver to retrieve information about images captured by a mobile device. In\naddition to reduced transmission overhead and computational complexity, the\nretrieval protocol should be robust to variations in the image acquisition\nprocess, such as translation, rotation, scaling, and sensor-related\ndifferences. We propose to extract scale-invariant image features and then\nperform clustering to reduce the number of features needed for image matching.\nPrincipal Component Analysis (PCA) and Non-negative Matrix Factorization (NMF)\nare investigated as candidate clustering approaches. The image matching\ncomplexity at the database server is quadratic in the (small) number of\nclusters, not in the (very large) number of image features. We employ an\nimage-dependent information content metric to approximate the model order,\ni.e., the number of clusters, needed for accurate matching, which is preferable\nto setting the model order using trial and error. We show how to combine the\nhypotheses provided by PCA and NMF factor loadings, thereby obtaining more\naccurate retrieval than using either approach alone. In experiments on a\ndatabase of urban images, we obtain a top-1 retrieval accuracy of 89% and a\ntop-3 accuracy of 92.5%.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 03:46:22 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Chakareski", "Jacob", ""], ["Manohar", "Immanuel", ""], ["Rane", "Shantanu", ""]]}, {"id": "1605.02164", "submitter": "Kunal Narayan Chaudhury", "authors": "Sanjay Ghosh and Kunal N. Chaudhury", "title": "Fast Bilateral Filtering of Vector-Valued Images", "comments": "To appear in IEEE International Conference on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a natural extension of the edge-preserving\nbilateral filter for vector-valued images. The direct computation of this\nnon-linear filter is slow in practice. We demonstrate how a fast algorithm can\nbe obtained by first approximating the Gaussian kernel of the bilateral filter\nusing raised-cosines, and then using Monte Carlo sampling. We present\nsimulation results on color images to demonstrate the accuracy of the algorithm\nand the speedup over the direct implementation.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 09:57:59 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Ghosh", "Sanjay", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1605.02178", "submitter": "Kunal Narayan Chaudhury", "authors": "Sanjay Ghosh and Kunal N. Chaudhury", "title": "Fast and High-Quality Bilateral Filtering Using Gauss-Chebyshev\n  Approximation", "comments": "To appear in International Conference on Signal Processing and\n  Communications (SPCOM) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bilateral filter is an edge-preserving smoother that has diverse\napplications in image processing, computer vision, computer graphics, and\ncomputational photography. The filter uses a spatial kernel along with a range\nkernel to perform edge-preserving smoothing. In this paper, we consider the\nGaussian bilateral filter where both the kernels are Gaussian. A direct\nimplementation of the Gaussian bilateral filter requires $O(\\sigma_s^2)$\noperations per pixel, where $\\sigma_s$ is the standard deviation of the spatial\nGaussian. In fact, it is well-known that the direct implementation is slow in\npractice. We present an approximation of the Gaussian bilateral filter, whereby\nwe can cut down the number of operations to $O(1)$ per pixel for any arbitrary\n$\\sigma_s$, and yet achieve very high-quality filtering that is almost\nindistinguishable from the output of the original filter. We demonstrate that\nthe proposed approximation is few orders faster in practice compared to the\ndirect implementation. We also demonstrate that the approximation is\ncompetitive with existing fast algorithms in terms of speed and accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 12:14:04 GMT"}, {"version": "v2", "created": "Tue, 10 May 2016 03:19:09 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Ghosh", "Sanjay", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1605.02196", "submitter": "Peter Radecki", "authors": "Peter Radecki, Mark Campbell and Kevin Matzen", "title": "All Weather Perception: Joint Data Association, Tracking, and\n  Classification for Autonomous Ground Vehicles", "comments": "35 pages, 21 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel probabilistic perception algorithm is presented as a real-time joint\nsolution to data association, object tracking, and object classification for an\nautonomous ground vehicle in all-weather conditions. The presented algorithm\nextends a Rao-Blackwellized Particle Filter originally built with a particle\nfilter for data association and a Kalman filter for multi-object tracking\n(Miller et al. 2011a) to now also include multiple model tracking for\nclassification. Additionally a state-of-the-art vision detection algorithm that\nincludes heading information for autonomous ground vehicle (AGV) applications\nwas implemented. Cornell's AGV from the DARPA Urban Challenge was upgraded and\nused to experimentally examine if and how state-of-the-art vision algorithms\ncan complement or replace lidar and radar sensors. Sensor and algorithm\nperformance in adverse weather and lighting conditions is tested. Experimental\nevaluation demonstrates robust all-weather data association, tracking, and\nclassification where camera, lidar, and radar sensors complement each other\ninside the joint probabilistic perception algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 14:36:34 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Radecki", "Peter", ""], ["Campbell", "Mark", ""], ["Matzen", "Kevin", ""]]}, {"id": "1605.02240", "submitter": "Anish Acharya", "authors": "Anish Acharya, Uddipan Mukherjee, Charless Fowlkes", "title": "On Image segmentation using Fractional Gradients-Learning Model\n  Parameters using Approximate Marginal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimates of image gradients play a ubiquitous role in image segmentation and\nclassification problems since gradients directly relate to the boundaries or\nthe edges of a scene. This paper proposes an unified approach to gradient\nestimation based on fractional calculus that is computationally cheap and\nreadily applicable to any existing algorithm that relies on image gradients. We\nshow experiments on edge detection and image segmentation on the Stanford\nBackgrounds Dataset where these improved local gradients outperforms state of\nthe art, achieving a performance of 79.2% average accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 20:12:12 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Acharya", "Anish", ""], ["Mukherjee", "Uddipan", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1605.02260", "submitter": "Saihui Hou", "authors": "Saihui Hou, Zilei Wang, Feng Wu", "title": "Deeply Exploit Depth Information for Object Detection", "comments": "9 pages, 3 figures, and 4 tables. Accepted by CVPR2016 Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue on how to more effectively coordinate the\ndepth with RGB aiming at boosting the performance of RGB-D object detection.\nParticularly, we investigate two primary ideas under the CNN model: property\nderivation and property fusion. Firstly, we propose that the depth can be\nutilized not only as a type of extra information besides RGB but also to derive\nmore visual properties for comprehensively describing the objects of interest.\nSo a two-stage learning framework consisting of property derivation and fusion\nis constructed. Here the properties can be derived either from the provided\ncolor/depth or their pairs (e.g. the geometry contour adopted in this paper).\nSecondly, we explore the fusion method of different properties in feature\nlearning, which is boiled down to, under the CNN model, from which layer the\nproperties should be fused together. The analysis shows that different semantic\nproperties should be learned separately and combined before passing into the\nfinal classifier. Actually, such a detection way is in accordance with the\nmechanism of the primary neural cortex (V1) in brain. We experimentally\nevaluate the proposed method on the challenging dataset, and have achieved\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 01:56:50 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Hou", "Saihui", ""], ["Wang", "Zilei", ""], ["Wu", "Feng", ""]]}, {"id": "1605.02264", "submitter": "Golnaz Ghiasi", "authors": "Golnaz Ghiasi and Charless C. Fowlkes", "title": "Laplacian Pyramid Reconstruction and Refinement for Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNN architectures have terrific recognition performance but rely on spatial\npooling which makes it difficult to adapt them to tasks that require dense,\npixel-accurate labeling. This paper makes two contributions: (1) We demonstrate\nthat while the apparent spatial resolution of convolutional feature maps is\nlow, the high-dimensional feature representation contains significant sub-pixel\nlocalization information. (2) We describe a multi-resolution reconstruction\narchitecture based on a Laplacian pyramid that uses skip connections from\nhigher resolution feature maps and multiplicative gating to successively refine\nsegment boundaries reconstructed from lower-resolution maps. This approach\nyields state-of-the-art semantic segmentation results on the PASCAL VOC and\nCityscapes segmentation benchmarks without resorting to more complex\nrandom-field inference or instance detection driven architectures.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 02:25:12 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 21:21:58 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Ghiasi", "Golnaz", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1605.02266", "submitter": "Michael Iliadis", "authors": "Michael Iliadis, Haohong Wang, Rafael Molina, Aggelos K. Katsaggelos", "title": "Robust and Low-Rank Representation for Fast Face Identification with\n  Occlusions", "comments": "IEEE Transactions on Image Processing (TIP), 2017", "journal-ref": null, "doi": "10.1109/TIP.2017.2675206", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an iterative method to address the face\nidentification problem with block occlusions. Our approach utilizes a robust\nrepresentation based on two characteristics in order to model contiguous errors\n(e.g., block occlusion) effectively. The first fits to the errors a\ndistribution described by a tailored loss function. The second describes the\nerror image as having a specific structure (resulting in low-rank in comparison\nto image size). We will show that this joint characterization is effective for\ndescribing errors with spatial continuity. Our approach is computationally\nefficient due to the utilization of the Alternating Direction Method of\nMultipliers (ADMM). A special case of our fast iterative algorithm leads to the\nrobust representation method which is normally used to handle non-contiguous\nerrors (e.g., pixel corruption). Extensive results on representative face\ndatabases (in constrained and unconstrained environments) document the\neffectiveness of our method over existing robust representation methods with\nrespect to both identification rates and computational time.\n  Code is available at Github, where you can find implementations of the\nF-LR-IRNNLS and F-IRNNLS (fast version of the RRC) :\nhttps://github.com/miliadis/FIRC\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 02:45:33 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 20:38:50 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Iliadis", "Michael", ""], ["Wang", "Haohong", ""], ["Molina", "Rafael", ""], ["Katsaggelos", "Aggelos K.", ""]]}, {"id": "1605.02289", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Songzhi Su, Donglin Cao, Shaozi Li", "title": "Detecting Ground Control Points via Convolutional Neural Network for\n  Stereo Matching", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a novel approach to detect ground control points\n(GCPs) for stereo matching problem. First of all, we train a convolutional\nneural network (CNN) on a large stereo set, and compute the matching confidence\nof each pixel by using the trained CNN model. Secondly, we present a ground\ncontrol points selection scheme according to the maximum matching confidence of\neach pixel. Finally, the selected GCPs are used to refine the matching costs,\nand we apply the new matching costs to perform optimization with semi-global\nmatching algorithm for improving the final disparity maps. We evaluate our\napproach on the KITTI 2012 stereo benchmark dataset. Our experiments show that\nthe proposed approach significantly improves the accuracy of disparity maps.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 07:38:40 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Zhong", "Zhun", ""], ["Su", "Songzhi", ""], ["Cao", "Donglin", ""], ["Li", "Shaozi", ""]]}, {"id": "1605.02305", "submitter": "Chunhua Shen", "authors": "Yuanzhouhan Cao, Zifeng Wu, Chunhua Shen", "title": "Estimating Depth from Monocular Images as Classification Using Deep\n  Fully Convolutional Residual Networks", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation from single monocular images is a key component of scene\nunderstanding and has benefited largely from deep convolutional neural networks\n(CNN) recently. In this article, we take advantage of the recent deep residual\nnetworks and propose a simple yet effective approach to this problem. We\nformulate depth estimation as a pixel-wise classification task. Specifically,\nwe first discretize the continuous depth values into multiple bins and label\nthe bins according to their depth range. Then we train fully convolutional deep\nresidual networks to predict the depth label of each pixel. Performing discrete\ndepth label classification instead of continuous depth value regression allows\nus to predict a confidence in the form of probability distribution. We further\napply fully-connected conditional random fields (CRF) as a post processing step\nto enforce local smoothness interactions, which improves the results. We\nevaluate our approach on both indoor and outdoor datasets and achieve\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 10:38:06 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 15:29:01 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 06:52:36 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Cao", "Yuanzhouhan", ""], ["Wu", "Zifeng", ""], ["Shen", "Chunhua", ""]]}, {"id": "1605.02346", "submitter": "Georgia Gkioxari", "authors": "Georgia Gkioxari, Alexander Toshev, Navdeep Jaitly", "title": "Chained Predictions Using Convolutional Neural Networks", "comments": "in submission to EECV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an adaptation of the sequence-to-sequence model for\nstructured output prediction in vision tasks. In this model the output\nvariables for a given input are predicted sequentially using neural networks.\nThe prediction for each output variable depends not only on the input but also\non the previously predicted output variables. The model is applied to spatial\nlocalization tasks and uses convolutional neural networks (CNNs) for processing\ninput images and a multi-scale deconvolutional architecture for making spatial\npredictions at each time step. We explore the impact of weight sharing with a\nrecurrent connection matrix between consecutive predictions, and compare it to\na formulation where these weights are not tied. Untied weights are particularly\nsuited for problems with a fixed sized structure, where different classes of\noutput are predicted in different steps. We show that chained predictions\nachieve top performing results on human pose estimation from single images and\nvideos.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2016 18:52:24 GMT"}, {"version": "v2", "created": "Sun, 23 Oct 2016 17:08:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Gkioxari", "Georgia", ""], ["Toshev", "Alexander", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1605.02424", "submitter": "Hailin Shi", "authors": "Hailin Shi, Xiangyu Zhu, Zhen Lei, Shengcai Liao, Stan Z. Li", "title": "Learning Discriminative Features with Class Encoder", "comments": "Accepted by CVPR2016 Workshop of Robust Features for Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks usually benefit from unsupervised pre-training, e.g.\nauto-encoders. However, the classifier further needs supervised fine-tuning\nmethods for good discrimination. Besides, due to the limits of full-connection,\nthe application of auto-encoders is usually limited to small, well aligned\nimages. In this paper, we incorporate the supervised information to propose a\nnovel formulation, namely class-encoder, whose training objective is to\nreconstruct a sample from another one of which the labels are identical.\nClass-encoder aims to minimize the intra-class variations in the feature space,\nand to learn a good discriminative manifolds on a class scale. We impose the\nclass-encoder as a constraint into the softmax for better supervised training,\nand extend the reconstruction on feature-level to tackle the parameter size\nissue and translation issue. The experiments show that the class-encoder helps\nto improve the performance on benchmarks of classification and face\nrecognition. This could also be a promising direction for fast training of face\nrecognition models.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 06:04:14 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Shi", "Hailin", ""], ["Zhu", "Xiangyu", ""], ["Lei", "Zhen", ""], ["Liao", "Shengcai", ""], ["Li", "Stan Z.", ""]]}, {"id": "1605.02460", "submitter": "Jiyo Athertya", "authors": "Jiyo.S.Athertya and G.Saravana Kumar", "title": "Fuzzy Clustering Based Segmentation Of Vertebrae in T1-Weighted Spinal\n  MR Images", "comments": null, "journal-ref": null, "doi": "10.5121/ijfls.2016.6202", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Image segmentation in the medical domain is a challenging field owing to poor\nresolution and limited contrast. The predominantly used conventional\nsegmentation techniques and the thresholding methods suffer from limitations\nbecause of heavy dependence on user interactions. Uncertainties prevalent in an\nimage cannot be captured by these techniques. The performance further\ndeteriorates when the images are corrupted by noise, outliers and other\nartifacts. The objective of this paper is to develop an effective robust fuzzy\nC- means clustering for segmenting vertebral body from magnetic resonance image\nowing to its unsupervised form of learning. The motivation for this work is\ndetection of spine geometry and proper localisation and labelling will enhance\nthe diagnostic output of a physician. The method is compared with Otsu\nthresholding and K-means clustering to illustrate the robustness.The reference\nstandard for validation was the annotated images from the radiologist, and the\nDice coefficient and Hausdorff distance measures were used to evaluate the\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 08:09:13 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Athertya", "Jiyo. S.", ""], ["Kumar", "G. Saravana", ""]]}, {"id": "1605.02464", "submitter": "Liqian Ma", "authors": "Liqian Ma, Hong Liu, Liang Hu, Can Wang, Qianru Sun", "title": "Orientation Driven Bag of Appearances for Person Re-identification", "comments": "13 pages, 15 figures, 3 tables, submitted to IEEE Transactions on\n  Circuits and Systems for Video Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) consists of associating individual across\ncamera network, which is valuable for intelligent video surveillance and has\ndrawn wide attention. Although person re-identification research is making\nprogress, it still faces some challenges such as varying poses, illumination\nand viewpoints. For feature representation in re-identification, existing works\nusually use low-level descriptors which do not take full advantage of body\nstructure information, resulting in low representation ability.\n%discrimination. To solve this problem, this paper proposes the mid-level\nbody-structure based feature representation (BSFR) which introduces body\nstructure pyramid for codebook learning and feature pooling in the vertical\ndirection of human body. Besides, varying viewpoints in the horizontal\ndirection of human body usually causes the data missing problem, $i.e.$, the\nappearances obtained in different orientations of the identical person could\nvary significantly. To address this problem, the orientation driven bag of\nappearances (ODBoA) is proposed to utilize person orientation information\nextracted by orientation estimation technic. To properly evaluate the proposed\napproach, we introduce a new re-identification dataset (Market-1203) based on\nthe Market-1501 dataset and propose a new re-identification dataset (PKU-Reid).\nBoth datasets contain multiple images captured in different body orientations\nfor each person. Experimental results on three public datasets and two proposed\ndatasets demonstrate the superiority of the proposed approach, indicating the\neffectiveness of body structure and orientation information for improving\nre-identification performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 08:25:33 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Ma", "Liqian", ""], ["Liu", "Hong", ""], ["Hu", "Liang", ""], ["Wang", "Can", ""], ["Sun", "Qianru", ""]]}, {"id": "1605.02559", "submitter": "Olivier Colliot", "authors": "Linda Marrakchi-Kacem (ARAMIS), Alexandre Vignaud (NEUROSPIN), Julien\n  Sein (CRMBM), Johanne Germain (ARAMIS), Thomas R Henry (CMRR), Cyril Poupon\n  (NEUROSPIN), Lucie Hertz-Pannier, St\\'ephane Leh\\'ericy (CENIR, ICM), Olivier\n  Colliot (ARAMIS, ICM), Pierre-Fran\\c{c}ois Van de Moortele (CMRR), Marie\n  Chupin (ARAMIS, ICM)", "title": "Robust imaging of hippocampal inner structure at 7T: in vivo acquisition\n  protocol and methodological choices", "comments": null, "journal-ref": "Magnetic Resonance Materials in Physics, Biology and Medicine,\n  Springer Verlag, 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OBJECTIVE:Motion-robust multi-slab imaging of hippocampal inner structure in\nvivo at 7T.MATERIALS AND METHODS:Motion is a crucial issue for ultra-high\nresolution imaging, such as can be achieved with 7T MRI. An acquisition\nprotocol was designed for imaging hippocampal inner structure at 7T. It relies\non a compromise between anatomical details visibility and robustness to motion.\nIn order to reduce acquisition time and motion artifacts, the full slab\ncovering the hippocampus was split into separate slabs with lower acquisition\ntime. A robust registration approach was implemented to combine the acquired\nslabs within a final 3D-consistent high-resolution slab covering the whole\nhippocampus. Evaluation was performed on 50 subjects overall, made of three\ngroups of subjects acquired using three acquisition settings; it focused on\nthree issues: visibility of hippocampal inner structure, robustness to motion\nartifacts and registration procedure performance.RESULTS:Overall, T2-weighted\nacquisitions with interleaved slabs proved robust. Multi-slab registration\nyielded high quality datasets in 96 % of the subjects, thus compatible with\nfurther analyses of hippocampal inner structure.CONCLUSION:Multi-slab\nacquisition and registration setting is efficient for reducing acquisition time\nand consequently motion artifacts for ultra-high resolution imaging of the\ninner structure of the hippocampus.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 12:38:44 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Marrakchi-Kacem", "Linda", "", "ARAMIS"], ["Vignaud", "Alexandre", "", "NEUROSPIN"], ["Sein", "Julien", "", "CRMBM"], ["Germain", "Johanne", "", "ARAMIS"], ["Henry", "Thomas R", "", "CMRR"], ["Poupon", "Cyril", "", "NEUROSPIN"], ["Hertz-Pannier", "Lucie", "", "CENIR, ICM"], ["Leh\u00e9ricy", "St\u00e9phane", "", "CENIR, ICM"], ["Colliot", "Olivier", "", "ARAMIS, ICM"], ["Van de Moortele", "Pierre-Fran\u00e7ois", "", "CMRR"], ["Chupin", "Marie", "", "ARAMIS, ICM"]]}, {"id": "1605.02560", "submitter": "Zi Wang", "authors": "Zi Wang, Vyacheslav Karolis, Chiara Nosarti, Giovanni Montana", "title": "Studying the brain from adolescence to adulthood through sparse\n  multi-view matrix factorisations", "comments": "Submitted to the 6th International Workshop on Pattern Recognition in\n  Neuroimaging (PRNI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Men and women differ in specific cognitive abilities and in the expression of\nseveral neuropsychiatric conditions. Such findings could be attributed to sex\nhormones, brain differences, as well as a number of environmental variables.\nExisting research on identifying sex-related differences in brain structure\nhave predominantly used cross-sectional studies to investigate, for instance,\ndifferences in average gray matter volumes (GMVs). In this article we explore\nthe potential of a recently proposed multi-view matrix factorisation (MVMF)\nmethodology to study structural brain changes in men and women that occur from\nadolescence to adulthood. MVMF is a multivariate variance decomposition\ntechnique that extends principal component analysis to \"multi-view\" datasets,\ni.e. where multiple and related groups of observations are available. In this\napplication, each view represents a different age group. MVMF identifies latent\nfactors explaining shared and age-specific contributions to the observed\noverall variability in GMVs over time. These latent factors can be used to\nproduce low-dimensional visualisations of the data that emphasise age-specific\neffects once the shared effects have been accounted for. The analysis of two\ndatasets consisting of individuals born prematurely as well as healthy controls\nprovides evidence to suggest that the separation between males and females\nbecomes increasingly larger as the brain transitions from adolescence to\nadulthood. We report on specific brain regions associated to these variance\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 12:40:22 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Wang", "Zi", ""], ["Karolis", "Vyacheslav", ""], ["Nosarti", "Chiara", ""], ["Montana", "Giovanni", ""]]}, {"id": "1605.02633", "submitter": "Chong You", "authors": "Chong You, Chun-Guang Li, Daniel P. Robinson, Rene Vidal", "title": "Oracle Based Active Set Algorithm for Scalable Elastic Net Subspace\n  Clustering", "comments": "15 pages, 6 figures, accepted to CVPR 2016 for oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art subspace clustering methods are based on expressing each\ndata point as a linear combination of other data points while regularizing the\nmatrix of coefficients with $\\ell_1$, $\\ell_2$ or nuclear norms. $\\ell_1$\nregularization is guaranteed to give a subspace-preserving affinity (i.e.,\nthere are no connections between points from different subspaces) under broad\ntheoretical conditions, but the clusters may not be connected. $\\ell_2$ and\nnuclear norm regularization often improve connectivity, but give a\nsubspace-preserving affinity only for independent subspaces. Mixed $\\ell_1$,\n$\\ell_2$ and nuclear norm regularizations offer a balance between the\nsubspace-preserving and connectedness properties, but this comes at the cost of\nincreased computational complexity. This paper studies the geometry of the\nelastic net regularizer (a mixture of the $\\ell_1$ and $\\ell_2$ norms) and uses\nit to derive a provably correct and scalable active set method for finding the\noptimal coefficients. Our geometric analysis also provides a theoretical\njustification and a geometric interpretation for the balance between the\nconnectedness (due to $\\ell_2$ regularization) and subspace-preserving (due to\n$\\ell_1$ regularization) properties for elastic net subspace clustering. Our\nexperiments show that the proposed active set method not only achieves\nstate-of-the-art clustering performance, but also efficiently handles\nlarge-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 15:49:36 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["You", "Chong", ""], ["Li", "Chun-Guang", ""], ["Robinson", "Daniel P.", ""], ["Vidal", "Rene", ""]]}, {"id": "1605.02677", "submitter": "Quanzeng You", "authors": "Quanzeng You, Jiebo Luo, Hailin Jin, Jianchao Yang", "title": "Building a Large Scale Dataset for Image Emotion Recognition: The Fine\n  Print and The Benchmark", "comments": "7 pages, 7 figures, AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological research results have confirmed that people can have different\nemotional reactions to different visual stimuli. Several papers have been\npublished on the problem of visual emotion analysis. In particular, attempts\nhave been made to analyze and predict people's emotional reaction towards\nimages. To this end, different kinds of hand-tuned features are proposed. The\nresults reported on several carefully selected and labeled small image data\nsets have confirmed the promise of such features. While the recent successes of\nmany computer vision related tasks are due to the adoption of Convolutional\nNeural Networks (CNNs), visual emotion analysis has not achieved the same level\nof success. This may be primarily due to the unavailability of confidently\nlabeled and relatively large image data sets for visual emotion analysis. In\nthis work, we introduce a new data set, which started from 3+ million weakly\nlabeled images of different emotions and ended up 30 times as large as the\ncurrent largest publicly available visual emotion data set. We hope that this\ndata set encourages further research on visual emotion analysis. We also\nperform extensive benchmarking analyses on this large data set using the state\nof the art methods including CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:14:52 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["You", "Quanzeng", ""], ["Luo", "Jiebo", ""], ["Jin", "Hailin", ""], ["Yang", "Jianchao", ""]]}, {"id": "1605.02686", "submitter": "Jun-Cheng Chen", "authors": "Jun-Cheng Chen, Rajeev Ranjan, Swami Sankaranarayanan, Amit Kumar,\n  Ching-Hui Chen, Vishal M. Patel, Carlos D. Castillo, Rama Chellappa", "title": "Unconstrained Still/Video-Based Face Verification with Deep\n  Convolutional Neural Networks", "comments": "accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last five years, methods based on Deep Convolutional Neural Networks\n(DCNNs) have shown impressive performance improvements for object detection and\nrecognition problems. This has been made possible due to the availability of\nlarge annotated datasets, a better understanding of the non-linear mapping\nbetween input images and class labels as well as the affordability of GPUs. In\nthis paper, we present the design details of a deep learning system for\nunconstrained face recognition, including modules for face detection,\nassociation, alignment and face verification. The quantitative performance\nevaluation is conducted using the IARPA Janus Benchmark A (IJB-A), the JANUS\nChallenge Set 2 (JANUS CS2), and the LFW dataset. The IJB-A dataset includes\nreal-world unconstrained faces of 500 subjects with significant pose and\nillumination variations which are much harder than the Labeled Faces in the\nWild (LFW) and Youtube Face (YTF) datasets. JANUS CS2 is the extended version\nof IJB-A which contains not only all the images/frames of IJB-A but also\nincludes the original videos for evaluating the video-based face verification\nsystem. Some open issues regarding DCNNs for face verification problems are\nthen discussed.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 18:28:44 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 15:24:50 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 02:45:38 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Chen", "Jun-Cheng", ""], ["Ranjan", "Rajeev", ""], ["Sankaranarayanan", "Swami", ""], ["Kumar", "Amit", ""], ["Chen", "Ching-Hui", ""], ["Patel", "Vishal M.", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1605.02697", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Marcus Rohrbach and Mario Fritz", "title": "Ask Your Neurons: A Deep Learning Approach to Visual Question Answering", "comments": "Improved version, it also has a final table from the VQA challenge,\n  and more baselines on DAQUAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a question answering task on real-world images that is set up as a\nVisual Turing Test. By combining latest advances in image representation and\nnatural language processing, we propose Ask Your Neurons, a scalable, jointly\ntrained, end-to-end formulation to this problem.\n  In contrast to previous efforts, we are facing a multi-modal problem where\nthe language output (answer) is conditioned on visual and natural language\ninputs (image and question). We provide additional insights into the problem by\nanalyzing how much information is contained only in the language part for which\nwe provide a new human baseline. To study human consensus, which is related to\nthe ambiguities inherent in this challenging task, we propose two novel metrics\nand collect additional answers which extend the original DAQUAR dataset to\nDAQUAR-Consensus.\n  Moreover, we also extend our analysis to VQA, a large-scale question\nanswering about images dataset, where we investigate some particular design\nchoices and show the importance of stronger visual models. At the same time, we\nachieve strong performance of our model that still uses a global image\nrepresentation. Finally, based on such analysis, we refine our Ask Your Neurons\non DAQUAR, which also leads to a better performance on this challenging task.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:04:23 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 10:30:18 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Rohrbach", "Marcus", ""], ["Fritz", "Mario", ""]]}, {"id": "1605.02699", "submitter": "Saikat Basu", "authors": "Saikat Basu, Manohar Karki, Robert DiBiano, Supratik Mukhopadhyay,\n  Sangram Ganguly, Ramakrishna Nemani and Shreekant Gayaka", "title": "A Theoretical Analysis of Deep Neural Networks for Texture\n  Classification", "comments": "Accepted in International Joint Conference on Neural Networks, IJCNN\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of Deep Neural Networks for the classification of\nimage datasets where texture features are important for generating\nclass-conditional discriminative representations. To this end, we first derive\nthe size of the feature space for some standard textural features extracted\nfrom the input dataset and then use the theory of Vapnik-Chervonenkis dimension\nto show that hand-crafted feature extraction creates low-dimensional\nrepresentations which help in reducing the overall excess error rate. As a\ncorollary to this analysis, we derive for the first time upper bounds on the VC\ndimension of Convolutional Neural Network as well as Dropout and Dropconnect\nnetworks and the relation between excess error rate of Dropout and Dropconnect\nnetworks. The concept of intrinsic dimension is used to validate the intuition\nthat texture-based datasets are inherently higher dimensional as compared to\nhandwritten digits or other object recognition datasets and hence more\ndifficult to be shattered by neural networks. We then derive the mean distance\nfrom the centroid to the nearest and farthest sampling points in an\nn-dimensional manifold and show that the Relative Contrast of the sample data\nvanishes as dimensionality of the underlying vector space tends to infinity.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:11:22 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 19:32:06 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Basu", "Saikat", ""], ["Karki", "Manohar", ""], ["DiBiano", "Robert", ""], ["Mukhopadhyay", "Supratik", ""], ["Ganguly", "Sangram", ""], ["Nemani", "Ramakrishna", ""], ["Gayaka", "Shreekant", ""]]}, {"id": "1605.02766", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Chen Zhao, Yezhou Yang, Cornelia Fermuller, Yiannis\n  Aloimonos", "title": "LightNet: A Versatile, Standalone Matlab-based Environment for Deep\n  Learning", "comments": "Accepted to ACM MULTIMEDIA 2016 Open Source Software Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LightNet is a lightweight, versatile and purely Matlab-based deep learning\nframework. The idea underlying its design is to provide an easy-to-understand,\neasy-to-use and efficient computational platform for deep learning research.\nThe implemented framework supports major deep learning architectures such as\nMultilayer Perceptron Networks (MLP), Convolutional Neural Networks (CNN) and\nRecurrent Neural Networks (RNN). The framework also supports both CPU and GPU\ncomputation, and the switch between them is straightforward. Different\napplications in computer vision, natural language processing and robotics are\ndemonstrated as experiments.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 20:33:30 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 00:20:14 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 04:00:30 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ye", "Chengxi", ""], ["Zhao", "Chen", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1605.02775", "submitter": "Diego Sebasti\\'an P\\'erez", "authors": "Diego Sebasti\\'an P\\'erez, Facundo Bromberg, Carlos Ariel Diaz", "title": "Image Classification of Grapevine Buds using Scale-Invariant Features\n  Transform, Bag of Features and Support Vector Machines", "comments": "21 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In viticulture, there are several applications where bud detection in\nvineyard images is a necessary task, susceptible of being automated through the\nuse of computer vision methods. A common and effective family of visual\ndetection algorithms are the scanning-window type, that slide a (usually) fixed\nsize window along the original image, classifying each resulting windowed-patch\nas containing or not containing the target object. The simplicity of these\nalgorithms finds its most challenging aspect in the classification stage.\nInterested in grapevine buds detection in natural field conditions, this paper\npresents a classification method for images of grapevine buds ranging 100 to\n1600 pixels in diameter, captured in outdoor, under natural field conditions,\nin winter (i.e., no grape bunches, very few leaves, and dormant buds), without\nartificial background, and with minimum equipment requirements. The proposed\nmethod uses well-known computer vision technologies: Scale-Invariant Feature\nTransform for calculating low-level features, Bag of Features for building an\nimage descriptor, and Support Vector Machines for training a classifier. When\nevaluated over images containing buds of at least 100 pixels in diameter, the\napproach achieves a recall higher than 0.9 and a precision of 0.86 over all\nwindowed-patches covering the whole bud and down to 60% of it, and scaled up to\nwindow patches containing a proportion of 20%-80% of bud versus background\npixels. This robustness on the position and size of the window demonstrates its\nviability for use as the classification stage in a scanning-window detection\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 20:48:20 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["P\u00e9rez", "Diego Sebasti\u00e1n", ""], ["Bromberg", "Facundo", ""], ["Diaz", "Carlos Ariel", ""]]}, {"id": "1605.02783", "submitter": "Leandro Abraham Eng.", "authors": "Leandro Abraham and Facundo Bromberg and Raymundo Forradellas", "title": "Estimacion de carga muscular mediante imagenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Un problema de gran interes en disciplinas como la ocupacional, ergonomica y\ndeportiva, es la medicion de variables biomecanicas involucradas en el\nmovimiento humano (como las fuerzas musculares internas y torque de\narticulaciones). Actualmente este problema se resuelve en un proceso de dos\npasos. Primero capturando datos con dispositivos poco pr\\'acticos, intrusivos y\ncostosos. Luego estos datos son usados como entrada en modelos complejos para\nobtener las variables biomecanicas como salida. El presente trabajo representa\nuna alternativa automatizada, no intrusiva y economica al primer paso,\nproponiendo la captura de estos datos a traves de imagenes. En trabajos futuros\nla idea es automatizar todo el proceso de calculo de esas variables. En este\ntrabajo elegimos un caso particular de medicion de variables biomecanicas: el\nproblema de estimar el nivel discreto de carga muscular que estan ejerciendo\nlos musculos de un brazo. Para estimar a partir de imagenes estaticas del brazo\nejerciendo la fuerza de sostener la carga, el nivel de la misma, realizamos un\nproceso de clasificacion. Nuestro enfoque utiliza Support Vector Machines para\nclasificacion, combinada con una etapa de pre-procesamiento que extrae\ncaracter{\\i}sticas visuales utilizando variadas tecnicas (Bag of Keypoints,\nLocal Binary Patterns, Histogramas de Color, Momentos de Contornos) En los\nmejores casos (Local Binary Patterns y Momentos de Contornos) obtenemos medidas\nde performance en la clasificacion (Precision, Recall, F-Measure y Accuracy)\nsuperiores al 90 %.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 21:06:30 GMT"}, {"version": "v2", "created": "Thu, 2 Jun 2016 14:13:28 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Abraham", "Leandro", ""], ["Bromberg", "Facundo", ""], ["Forradellas", "Raymundo", ""]]}, {"id": "1605.02827", "submitter": "Yang Feng", "authors": "Yang Feng, Jiebo Luo", "title": "When Do Luxury Cars Hit the Road? Findings by A Big Data Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on studying the appearing time of different kinds of\ncars on the road. This information will enable us to infer the life style of\nthe car owners. The results can further be used to guide marketing towards car\nowners. Conventionally, this kind of study is carried out by sending out\nquestionnaires, which is limited in scale and diversity. To solve this problem,\nwe propose a fully automatic method to carry out this study. Our study is based\non publicly available surveillance camera data. To make the results reliable,\nwe only use the high resolution cameras (i.e. resolution greater than $1280\n\\times 720$). Images from the public cameras are downloaded every minute. After\nobtaining 50,000 images, we apply faster R-CNN (region-based convoluntional\nneural network) to detect the cars in the downloaded images and a fine-tuned\nVGG16 model is used to recognize the car makes. Based on the recognition\nresults, we present a data-driven analysis on the relationship between car\nmakes and their appearing times, with implications on lifestyles.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 02:54:39 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 16:29:38 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Feng", "Yang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1605.02892", "submitter": "Marco Bertini", "authors": "Simone Ercoli, Marco Bertini and Alberto Del Bimbo", "title": "Compact Hash Codes for Efficient Visual Descriptors Retrieval in Large\n  Scale Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an efficient method for visual descriptors retrieval\nbased on compact hash codes computed using a multiple k-means assignment. The\nmethod has been applied to the problem of approximate nearest neighbor (ANN)\nsearch of local and global visual content descriptors, and it has been tested\non different datasets: three large scale public datasets of up to one billion\ndescriptors (BIGANN) and, supported by recent progress in convolutional neural\nnetworks (CNNs), also on the CIFAR-10 and MNIST datasets. Experimental results\nshow that, despite its simplicity, the proposed method obtains a very high\nperformance that makes it superior to more complex state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 08:53:04 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Ercoli", "Simone", ""], ["Bertini", "Marco", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.02914", "submitter": "Vasileios Belagiannis", "authors": "Vasileios Belagiannis and Andrew Zisserman", "title": "Recurrent Human Pose Estimation", "comments": "FG 2017, More Info and Demo:\n  http://www.robots.ox.ac.uk/~vgg/software/keypoint_detection/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel ConvNet model for predicting 2D human body poses in an\nimage. The model regresses a heatmap representation for each body keypoint, and\nis able to learn and represent both the part appearances and the context of the\npart configuration. We make the following three contributions: (i) an\narchitecture combining a feed forward module with a recurrent module, where the\nrecurrent module can be run iteratively to improve the performance, (ii) the\nmodel can be trained end-to-end and from scratch, with auxiliary losses\nincorporated to improve performance, (iii) we investigate whether keypoint\nvisibility can also be predicted. The model is evaluated on two benchmark\ndatasets. The result is a simple architecture that achieves performance on par\nwith the state of the art, but without the complexity of a graphical model\nstage (or layers).\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 09:58:11 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 09:45:32 GMT"}, {"version": "v3", "created": "Sat, 5 Aug 2017 11:56:51 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Belagiannis", "Vasileios", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1605.02964", "submitter": "Abhilash Srikantha", "authors": "Abhilash Srikantha, Juergen Gall", "title": "Weakly Supervised Learning of Affordances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing functional regions of objects or affordances is an important\naspect of scene understanding. In this work, we cast the problem of affordance\nsegmentation as that of semantic image segmentation. In order to explore\nvarious levels of supervision, we introduce a pixel-annotated affordance\ndataset of 3090 images containing 9916 object instances with rich contextual\ninformation in terms of human-object interactions. We use a deep convolutional\nneural network within an expectation maximization framework to take advantage\nof weakly labeled data like image level annotations or keypoint annotations. We\nshow that a further reduction in supervision is possible with a minimal loss in\nperformance when human pose is used as context.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 12:04:07 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 13:46:59 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Srikantha", "Abhilash", ""], ["Gall", "Juergen", ""]]}, {"id": "1605.02971", "submitter": "J\\\"orn-Henrik Jacobsen", "authors": "J\\\"orn-Henrik Jacobsen, Jan van Gemert, Zhongyu Lou, Arnold W. M.\n  Smeulders", "title": "Structured Receptive Fields in CNNs", "comments": "Reason for update: i) Fix Reference for \"Deep roto-translation\n  scattering for object classification\" by Oyallon and Mallat. ii) Fixed two\n  minor typos. iii) Removed implicit assumption in equation (4) where scale is\n  represented with diffusion time and adapted to rest of paper where scale is\n  represented with standard deviation, to avoid possible confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning powerful feature representations with CNNs is hard when training\ndata are limited. Pre-training is one way to overcome this, but it requires\nlarge datasets sufficiently similar to the target domain. Another option is to\ndesign priors into the model, which can range from tuned hyperparameters to\nfully engineered representations like Scattering Networks. We combine these\nideas into structured receptive field networks, a model which has a fixed\nfilter basis and yet retains the flexibility of CNNs. This flexibility is\nachieved by expressing receptive fields in CNNs as a weighted sum over a fixed\nbasis which is similar in spirit to Scattering Networks. The key difference is\nthat we learn arbitrary effective filter sets from the basis rather than\nmodeling the filters. This approach explicitly connects classical multiscale\nimage analysis with general CNNs. With structured receptive field networks, we\nimprove considerably over unstructured CNNs for small and medium dataset\nscenarios as well as over Scattering for large datasets. We validate our\nfindings on ILSVRC2012, Cifar-10, Cifar-100 and MNIST. As a realistic small\ndataset example, we show state-of-the-art classification results on popular 3D\nMRI brain-disease datasets where pre-training is difficult due to a lack of\nlarge public datasets in a similar domain.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 12:18:03 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 11:56:08 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Jacobsen", "J\u00f6rn-Henrik", ""], ["van Gemert", "Jan", ""], ["Lou", "Zhongyu", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1605.03012", "submitter": "Fang Lu", "authors": "Fang Lu and Fa Wu and Peijun Hu and Zhiyi Peng and Dexing Kong", "title": "Automatic 3D liver location and segmentation via convolutional neural\n  networks and graph cut", "comments": "12 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose Segmentation of the liver from abdominal computed tomography (CT)\nimage is an essential step in some computer assisted clinical interventions,\nsuch as surgery planning for living donor liver transplant (LDLT), radiotherapy\nand volume measurement. In this work, we develop a deep learning algorithm with\ngraph cut refinement to automatically segment liver in CT scans. Methods The\nproposed method consists of two main steps: (i) simultaneously liver detection\nand probabilistic segmentation using 3D convolutional neural networks (CNNs);\n(ii) accuracy refinement of initial segmentation with graph cut and the\npreviously learned probability map. Results The proposed approach was validated\non forty CT volumes taken from two public databases MICCAI-Sliver07 and\n3Dircadb. For the MICCAI-Sliver07 test set, the calculated mean ratios of\nvolumetric overlap error (VOE), relative volume difference (RVD), average\nsymmetric surface distance (ASD), root mean square symmetric surface distance\n(RMSD) and maximum symmetric surface distance (MSD) are 5.9%, 2.7%, 0.91%, 1.88\nmm, and 18.94 mm, respectively. In the case of 20 3Dircadb data, the calculated\nmean ratios of VOE, RVD, ASD, RMSD and MSD are 9.36%, 0.97%, 1.89%, 4.15 mm and\n33.14 mm, respectively. Conclusion The proposed method is fully automatic\nwithout any user interaction. Quantitative results reveal that the proposed\napproach is efficient and accurate for hepatic volume estimation in a clinical\nsetup. The high correlation between the automatic and manual references shows\nthat the proposed method can be good enough to replace the time-consuming and\nnon-reproducible manual segmentation method.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 13:42:51 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Lu", "Fang", ""], ["Wu", "Fa", ""], ["Hu", "Peijun", ""], ["Peng", "Zhiyi", ""], ["Kong", "Dexing", ""]]}, {"id": "1605.03124", "submitter": "Ethan Rudd", "authors": "Ethan M. Rudd, Manuel Gunther, and Terrance E. Boult", "title": "PARAPH: Presentation Attack Rejection by Analyzing Polarization\n  Hypotheses", "comments": "Accepted to CVPR 2016 Biometrics Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications such as airport border control, biometric technologies that\ncan process many capture subjects quickly, efficiently, with weak supervision,\nand with minimal discomfort are desirable. Facial recognition is particularly\nappealing because it is minimally invasive yet offers relatively good\nrecognition performance. Unfortunately, the combination of weak supervision and\nminimal invasiveness makes even highly accurate facial recognition systems\nsusceptible to spoofing via presentation attacks. Thus, there is great demand\nfor an effective and low cost system capable of rejecting such attacks.To this\nend we introduce PARAPH -- a novel hardware extension that exploits different\nmeasurements of light polarization to yield an image space in which\npresentation media are readily discernible from Bona Fide facial\ncharacteristics. The PARAPH system is inexpensive with an added cost of less\nthan 10 US dollars. The system makes two polarization measurements in rapid\nsuccession, allowing them to be approximately pixel-aligned, with a frame rate\nlimited by the camera, not the system. There are no moving parts above the\nmolecular level, due to the efficient use of twisted nematic liquid crystals.\nWe present evaluation images using three presentation attack media next to an\nactual face -- high quality photos on glossy and matte paper and a video of the\nface on an LCD. In each case, the actual face in the image generated by PARAPH\nis structurally discernible from the presentations, which appear either as\nnoise (print attacks) or saturated images (replay attacks).\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 17:46:59 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Rudd", "Ethan M.", ""], ["Gunther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1605.03150", "submitter": "Yasamin Alkhorshid", "authors": "Yasamin Alkhorshid, Kamelia Aryafar, Sven Bauer, and Gerd Wanielik", "title": "Road Detection through Supervised Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is a rapidly evolving technology. Autonomous vehicles are\ncapable of sensing their environment and navigating without human input through\nsensory information such as radar, lidar, GNSS, vehicle odometry, and computer\nvision. This sensory input provides a rich dataset that can be used in\ncombination with machine learning models to tackle multiple problems in\nsupervised settings. In this paper we focus on road detection through\ngray-scale images as the sole sensory input. Our contributions are twofold:\nfirst, we introduce an annotated dataset of urban roads for machine learning\ntasks; second, we introduce a road detection framework on this dataset through\nsupervised classification and hand-crafted feature vectors.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 18:53:09 GMT"}], "update_date": "2016-05-11", "authors_parsed": [["Alkhorshid", "Yasamin", ""], ["Aryafar", "Kamelia", ""], ["Bauer", "Sven", ""], ["Wanielik", "Gerd", ""]]}, {"id": "1605.03170", "submitter": "Leonid Pishchulin", "authors": "Eldar Insafutdinov, Leonid Pishchulin, Bjoern Andres, Mykhaylo\n  Andriluka, and Bernt Schiele", "title": "DeeperCut: A Deeper, Stronger, and Faster Multi-Person Pose Estimation\n  Model", "comments": "ECCV'16. High-res version at\n  https://www.d2.mpi-inf.mpg.de/sites/default/files/insafutdinov16arxiv.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to advance the state-of-the-art of articulated pose\nestimation in scenes with multiple people. To that end we contribute on three\nfronts. We propose (1) improved body part detectors that generate effective\nbottom-up proposals for body parts; (2) novel image-conditioned pairwise terms\nthat allow to assemble the proposals into a variable number of consistent body\npart configurations; and (3) an incremental optimization strategy that explores\nthe search space more efficiently thus leading both to better performance and\nsignificant speed-up factors. Evaluation is done on two single-person and two\nmulti-person pose estimation benchmarks. The proposed approach significantly\noutperforms best known multi-person pose estimation results while demonstrating\ncompetitive performance on the task of single person pose estimation. Models\nand code available at http://pose.mpi-inf.mpg.de\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 19:49:40 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 15:39:24 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 19:03:17 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Insafutdinov", "Eldar", ""], ["Pishchulin", "Leonid", ""], ["Andres", "Bjoern", ""], ["Andriluka", "Mykhaylo", ""], ["Schiele", "Bernt", ""]]}, {"id": "1605.03222", "submitter": "Alvaro Soto", "authors": "Anali Alfaro, Domingo Mery, Alvaro Soto", "title": "Action Recognition in Video Using Sparse Coding and Relative Features", "comments": "Accepted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an approach to category-based action recognition in video\nusing sparse coding techniques. The proposed approach includes two main\ncontributions: i) A new method to handle intra-class variations by decomposing\neach video into a reduced set of representative atomic action acts or\nkey-sequences, and ii) A new video descriptor, ITRA: Inter-Temporal Relational\nAct Descriptor, that exploits the power of comparative reasoning to capture\nrelative similarity relations among key-sequences. In terms of the method to\nobtain key-sequences, we introduce a loss function that, for each video, leads\nto the identification of a sparse set of representative key-frames capturing\nboth, relevant particularities arising in the input video, as well as relevant\ngeneralities arising in the complete class collection. In terms of the method\nto obtain the ITRA descriptor, we introduce a novel scheme to quantify relative\nintra and inter-class similarities among local temporal patterns arising in the\nvideos. The resulting ITRA descriptor demonstrates to be highly effective to\ndiscriminate among action categories. As a result, the proposed approach\nreaches remarkable action recognition performance on several popular benchmark\ndatasets, outperforming alternative state-of-the-art techniques by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 21:52:25 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Alfaro", "Anali", ""], ["Mery", "Domingo", ""], ["Soto", "Alvaro", ""]]}, {"id": "1605.03234", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong, Jurgen Seiler, Andre Kaup, Soren Forchhammer, Nikos\n  Deligiannis", "title": "Measurement Bounds for Sparse Signal Reconstruction with Multiple Side\n  Information", "comments": "submitted to a journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of compressed sensing (CS), this paper considers the problem\nof reconstructing sparse signals with the aid of other given correlated sources\nas multiple side information. To address this problem, we theoretically study a\ngeneric \\textcolor{black}{weighted $n$-$\\ell_{1}$ minimization} framework and\npropose a reconstruction algorithm that leverages multiple side information\nsignals (RAMSI). The proposed RAMSI algorithm computes adaptively optimal\nweights among the side information signals at every reconstruction iteration.\nIn addition, we establish theoretical bounds on the number of measurements that\nare required to successfully reconstruct the sparse source by using\n\\textcolor{black}{weighted $n$-$\\ell_{1}$ minimization}. The analysis of the\nestablished bounds reveal that \\textcolor{black}{weighted $n$-$\\ell_{1}$\nminimization} can achieve sharper bounds and significant performance\nimprovements compared to classical CS. We evaluate experimentally the proposed\nRAMSI algorithm and the established bounds using synthetic sparse signals as\nwell as correlated feature histograms, extracted from a multiview image\ndatabase for object recognition. The obtained results show clearly that the\nproposed algorithm outperforms state-of-the-art\nalgorithms---\\textcolor{black}{including classical CS, $\\ell_1\\text{-}\\ell_1$\nminimization, Modified-CS, regularized Modified-CS, and weighted $\\ell_1$\nminimization}---in terms of both the theoretical bounds and the practical\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2016 22:48:14 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 10:20:35 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Van Luong", "Huynh", ""], ["Seiler", "Jurgen", ""], ["Kaup", "Andre", ""], ["Forchhammer", "Soren", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1605.03259", "submitter": "Chi Su", "authors": "Chi Su, Shiliang Zhang, Junliang Xing, Wen Gao and Qi Tian", "title": "Deep Attributes Driven Multi-Camera Person Re-identification", "comments": "Person Re-identification; 17 pages; 5 figures; In IEEE ECCV 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46475-6_30", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual appearance of a person is easily affected by many factors like\npose variations, viewpoint changes and camera parameter differences. This makes\nperson Re-Identification (ReID) among multiple cameras a very challenging task.\nThis work is motivated to learn mid-level human attributes which are robust to\nsuch visual appearance variations. And we propose a semi-supervised attribute\nlearning framework which progressively boosts the accuracy of attributes only\nusing a limited number of labeled data. Specifically, this framework involves a\nthree-stage training. A deep Convolutional Neural Network (dCNN) is first\ntrained on an independent dataset labeled with attributes. Then it is\nfine-tuned on another dataset only labeled with person IDs using our defined\ntriplet loss. Finally, the updated dCNN predicts attribute labels for the\ntarget dataset, which is combined with the independent dataset for the final\nround of fine-tuning. The predicted attributes, namely \\emph{deep attributes}\nexhibit superior generalization ability across different datasets. By directly\nusing the deep attributes with simple Cosine distance, we have obtained\nsurprisingly good accuracy on four person ReID datasets. Experiments also show\nthat a simple metric learning modular further boosts our method, making it\nsignificantly outperform many recent works.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 02:05:22 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 05:58:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Su", "Chi", ""], ["Zhang", "Shiliang", ""], ["Xing", "Junliang", ""], ["Gao", "Wen", ""], ["Tian", "Qi", ""]]}, {"id": "1605.03324", "submitter": "Ozan Sener", "authors": "Ozan Sener and Amir Roshan Zamir and Chenxia Wu and Silvio Savarese\n  and Ashutosh Saxena", "title": "Unsupervised Semantic Action Discovery from Video Collections", "comments": "First version of this paper arXiv:1506.08438 appeared in ICCV 2015.\n  This extended version has more details on the learning algorithm and\n  hierarchical clustering with full derivation, additional analysis on the\n  robustness to the subtitle noise, and a novel application on robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human communication takes many forms, including speech, text and\ninstructional videos. It typically has an underlying structure, with a starting\npoint, ending, and certain objective steps between them. In this paper, we\nconsider instructional videos where there are tens of millions of them on the\nInternet.\n  We propose a method for parsing a video into such semantic steps in an\nunsupervised way. Our method is capable of providing a semantic \"storyline\" of\nthe video composed of its objective steps. We accomplish this using both visual\nand language cues in a joint generative model. Our method can also provide a\ntextual description for each of the identified semantic steps and video\nsegments. We evaluate our method on a large number of complex YouTube videos\nand show that our method discovers semantically correct instructions for a\nvariety of tasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:22:06 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Sener", "Ozan", ""], ["Zamir", "Amir Roshan", ""], ["Wu", "Chenxia", ""], ["Savarese", "Silvio", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1605.03328", "submitter": "Magnus Andersson", "authors": "Alvaro Rodriguez, Hanqing Zhang, Krister Wiklund, Tomas Brodin,\n  Jonatan Klaminder, Patrik Andersson, Magnus Andersson", "title": "A robust particle detection algorithm based on symmetry", "comments": "Manuscript including supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle tracking is common in many biophysical, ecological, and\nmicro-fluidic applications. Reliable tracking information is heavily dependent\non of the system under study and algorithms that correctly determines particle\nposition between images. However, in a real environmental context with the\npresence of noise including particular or dissolved matter in water, and low\nand fluctuating light conditions, many algorithms fail to obtain reliable\ninformation. We propose a new algorithm, the Circular Symmetry algorithm\n(C-Sym), for detecting the position of a circular particle with high accuracy\nand precision in noisy conditions. The algorithm takes advantage of the spatial\nsymmetry of the particle allowing for subpixel accuracy. We compare the\nproposed algorithm with four different methods using both synthetic and\nexperimental datasets. The results show that C-Sym is the most accurate and\nprecise algorithm when tracking micro-particles in all tested conditions and it\nhas the potential for use in applications including tracking biota in their\nenvironment.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 08:38:32 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Rodriguez", "Alvaro", ""], ["Zhang", "Hanqing", ""], ["Wiklund", "Krister", ""], ["Brodin", "Tomas", ""], ["Klaminder", "Jonatan", ""], ["Andersson", "Patrik", ""], ["Andersson", "Magnus", ""]]}, {"id": "1605.03344", "submitter": "Jiaolong Yang", "authors": "Jiaolong Yang, Hongdong Li, Dylan Campbell, Yunde Jia", "title": "Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration", "comments": "to appear in T-PAMI 2016 (IEEE Transactions on Pattern Analysis and\n  Machine Intelligence)", "journal-ref": null, "doi": "10.1109/TPAMI.2015.2513405", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Iterative Closest Point (ICP) algorithm is one of the most widely used\nmethods for point-set registration. However, being based on local iterative\noptimization, ICP is known to be susceptible to local minima. Its performance\ncritically relies on the quality of the initialization and only local\noptimality is guaranteed. This paper presents the first globally optimal\nalgorithm, named Go-ICP, for Euclidean (rigid) registration of two 3D\npoint-sets under the L2 error metric defined in ICP. The Go-ICP method is based\non a branch-and-bound (BnB) scheme that searches the entire 3D motion space\nSE(3). By exploiting the special structure of SE(3) geometry, we derive novel\nupper and lower bounds for the registration error function. Local ICP is\nintegrated into the BnB scheme, which speeds up the new method while\nguaranteeing global optimality. We also discuss extensions, addressing the\nissue of outlier robustness. The evaluation demonstrates that the proposed\nmethod is able to produce reliable registration results regardless of the\ninitialization. Go-ICP can be applied in scenarios where an optimal solution is\ndesirable or where a good initialization is not always available.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 09:15:11 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Yang", "Jiaolong", ""], ["Li", "Hongdong", ""], ["Campbell", "Dylan", ""], ["Jia", "Yunde", ""]]}, {"id": "1605.03389", "submitter": "Markus Oberweger", "authors": "Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit", "title": "Efficiently Creating 3D Training Data for Fine Hand Pose Estimation", "comments": "added link to source https://github.com/moberweger/semi-auto-anno.\n  Appears in Proc. of CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many recent hand pose estimation methods critically rely on a training\nset of labelled frames, the creation of such a dataset is a challenging task\nthat has been overlooked so far. As a result, existing datasets are limited to\na few sequences and individuals, with limited accuracy, and this prevents these\nmethods from delivering their full potential. We propose a semi-automated\nmethod for efficiently and accurately labeling each frame of a hand depth video\nwith the corresponding 3D locations of the joints: The user is asked to provide\nonly an estimate of the 2D reprojections of the visible joints in some\nreference frames, which are automatically selected to minimize the labeling\nwork by efficiently optimizing a sub-modular loss function. We then exploit\nspatial, temporal, and appearance constraints to retrieve the full 3D poses of\nthe hand over the complete sequence. We show that this data can be used to\ntrain a recent state-of-the-art hand pose estimation method, leading to\nincreased accuracy. The code and dataset can be found on our website\nhttps://cvarlab.icg.tugraz.at/projects/hand_detection/\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 11:40:27 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:45:38 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Oberweger", "Markus", ""], ["Riegler", "Gernot", ""], ["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1605.03428", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Luc Van Gool", "title": "Image-level Classification in Hyperspectral Images using Feature\n  Descriptors, with Application to Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we proposed a novel pipeline for image-level classification in\nthe hyperspectral images. By doing this, we show that the discriminative\nspectral information at image-level features lead to significantly improved\nperformance in a face recognition task. We also explored the potential of\ntraditional feature descriptors in the hyperspectral images. From our\nevaluations, we observe that SIFT features outperform the state-of-the-art\nhyperspectral face recognition methods, and also the other descriptors. With\nthe increasing deployment of hyperspectral sensors in a multitude of\napplications, we believe that our approach can effectively exploit the spectral\ninformation in hyperspectral images, thus beneficial to more accurate\nclassification.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 13:18:22 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Sharma", "Vivek", ""], ["Van Gool", "Luc", ""]]}, {"id": "1605.03477", "submitter": "Marc Masana Castrillo", "authors": "Marc Masana and Joost van de Weijer and Andrew D. Bagdanov", "title": "On-the-fly Network Pruning for Object Detection", "comments": "Accepted at ICLR 2016 workshop track as a poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection with deep neural networks is often performed by passing a\nfew thousand candidate bounding boxes through a deep neural network for each\nimage. These bounding boxes are highly correlated since they originate from the\nsame image. In this paper we investigate how to exploit feature occurrence at\nthe image scale to prune the neural network which is subsequently applied to\nall bounding boxes. We show that removing units which have near-zero activation\nin the image allows us to significantly reduce the number of parameters in the\nnetwork. Results on the PASCAL 2007 Object Detection Challenge demonstrate that\nup to 40% of units in some fully-connected layers can be entirely eliminated\nwith little change in the detection result.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:27:43 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Masana", "Marc", ""], ["van de Weijer", "Joost", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1605.03483", "submitter": "Menglong Ye", "authors": "Menglong Ye, Lin Zhang, Stamatia Giannarou, Guang-Zhong Yang", "title": "Real-time 3D Tracking of Articulated Tools for Robotic Surgery", "comments": "This paper was presented in MICCAI 2016 conference, and a DOI was\n  linked to the publisher's version", "journal-ref": null, "doi": "10.1007/978-3-319-46720-7_45", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotic surgery, tool tracking is important for providing safe tool-tissue\ninteraction and facilitating surgical skills assessment. Despite recent\nadvances in tool tracking, existing approaches are faced with major\ndifficulties in real-time tracking of articulated tools. Most algorithms are\ntailored for offline processing with pre-recorded videos. In this paper, we\npropose a real-time 3D tracking method for articulated tools in robotic\nsurgery. The proposed method is based on the CAD model of the tools as well as\nrobot kinematics to generate online part-based templates for efficient 2D\nmatching and 3D pose estimation. A robust verification approach is incorporated\nto reject outliers in 2D detections, which is then followed by fusing inliers\nwith robot kinematic readings for 3D pose estimation of the tool. The proposed\nmethod has been validated with phantom data, as well as ex vivo and in vivo\nexperiments. The results derived clearly demonstrate the performance advantage\nof the proposed method when compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:35:23 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 09:10:39 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 12:14:02 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ye", "Menglong", ""], ["Zhang", "Lin", ""], ["Giannarou", "Stamatia", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1605.03498", "submitter": "Micael Carvalho", "authors": "Micael Carvalho, Matthieu Cord, Sandra Avila, Nicolas Thome, Eduardo\n  Valle", "title": "Deep Neural Networks Under Stress", "comments": "This article corresponds to the accepted version at IEEE ICIP 2016.\n  We will link the DOI as soon as it is available", "journal-ref": null, "doi": "10.1109/ICIP.2016.7533200", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep architectures have been used for transfer learning with\nstate-of-the-art performance in many datasets. The properties of their features\nremain, however, largely unstudied under the transfer perspective. In this\nwork, we present an extensive analysis of the resiliency of feature vectors\nextracted from deep models, with special focus on the trade-off between\nperformance and compression rate. By introducing perturbations to image\ndescriptions extracted from a deep convolutional neural network, we change\ntheir precision and number of dimensions, measuring how it affects the final\nscore. We show that deep features are more robust to these disturbances when\ncompared to classical approaches, achieving a compression rate of 98.4%, while\nlosing only 0.88% of their original score for Pascal VOC 2007.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 16:22:23 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 08:34:50 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Carvalho", "Micael", ""], ["Cord", "Matthieu", ""], ["Avila", "Sandra", ""], ["Thome", "Nicolas", ""], ["Valle", "Eduardo", ""]]}, {"id": "1605.03557", "submitter": "Tinghui Zhou", "authors": "Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, Alexei A.\n  Efros", "title": "View Synthesis by Appearance Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of novel view synthesis: given an input image,\nsynthesizing new images of the same object or scene observed from arbitrary\nviewpoints. We approach this as a learning task but, critically, instead of\nlearning to synthesize pixels from scratch, we learn to copy them from the\ninput image. Our approach exploits the observation that the visual appearance\nof different views of the same instance is highly correlated, and such\ncorrelation could be explicitly learned by training a convolutional neural\nnetwork (CNN) to predict appearance flows -- 2-D coordinate vectors specifying\nwhich pixels in the input view could be used to reconstruct the target view.\nFurthermore, the proposed framework easily generalizes to multiple input views\nby learning how to optimally combine single-view predictions. We show that for\nboth objects and scenes, our approach is able to synthesize novel views of\nhigher perceptual quality than previous CNN-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 19:16:24 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 06:03:03 GMT"}, {"version": "v3", "created": "Sat, 11 Feb 2017 20:33:50 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zhou", "Tinghui", ""], ["Tulsiani", "Shubham", ""], ["Sun", "Weilun", ""], ["Malik", "Jitendra", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1605.03621", "submitter": "Suren Jayasuriya", "authors": "Huaijin Chen, Suren Jayasuriya, Jiyue Yang, Judy Stephen, Sriram\n  Sivaramakrishnan, Ashok Veeraraghavan, Alyosha Molnar", "title": "ASP Vision: Optically Computing the First Layer of Convolutional Neural\n  Networks using Angle Sensitive Pixels", "comments": "Presented in CVPR 2016 (oral), 10 pages, 12 figures. This new version\n  corrects the comparison between imaging power for ASPs and a regular image\n  sensor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning using convolutional neural networks (CNNs) is quickly becoming\nthe state-of-the-art for challenging computer vision applications. However,\ndeep learning's power consumption and bandwidth requirements currently limit\nits application in embedded and mobile systems with tight energy budgets. In\nthis paper, we explore the energy savings of optically computing the first\nlayer of CNNs. To do so, we utilize bio-inspired Angle Sensitive Pixels (ASPs),\ncustom CMOS diffractive image sensors which act similar to Gabor filter banks\nin the V1 layer of the human visual cortex. ASPs replace both image sensing and\nthe first layer of a conventional CNN by directly performing optical edge\nfiltering, saving sensing energy, data bandwidth, and CNN FLOPS to compute. Our\nexperimental results (both on synthetic data and a hardware prototype) for a\nvariety of vision tasks such as digit recognition, object recognition, and face\nidentification demonstrate using ASPs while achieving similar performance\ncompared to traditional deep learning pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 21:11:30 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 18:21:56 GMT"}, {"version": "v3", "created": "Wed, 16 Nov 2016 19:58:35 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chen", "Huaijin", ""], ["Jayasuriya", "Suren", ""], ["Yang", "Jiyue", ""], ["Stephen", "Judy", ""], ["Sivaramakrishnan", "Sriram", ""], ["Veeraraghavan", "Ashok", ""], ["Molnar", "Alyosha", ""]]}, {"id": "1605.03624", "submitter": "Aamir Adam", "authors": "A. M. Adam, R. M. Farouk, M. E. Abd El-aziz", "title": "Blind image separation based on exponentiated transmuted Weibull\n  distribution", "comments": "14 pages, 12 figures, 4 tables. International Journal of Computer\n  Science and Information Security (IJCSIS),Vol. 14, No. 3, March 2016 (pp.\n  423-433)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years the processing of blind image separation has been\ninvestigated. As a result, a number of feature extraction algorithms for direct\napplication of such image structures have been developed. For example,\nseparation of mixed fingerprints found in any crime scene, in which a mixture\nof two or more fingerprints may be obtained, for identification, we have to\nseparate them. In this paper, we have proposed a new technique for separating a\nmultiple mixed images based on exponentiated transmuted Weibull distribution.\nTo adaptively estimate the parameters of such score functions, an efficient\nmethod based on maximum likelihood and genetic algorithm will be used. We also\ncalculate the accuracy of this proposed distribution and compare the\nalgorithmic performance using the efficient approach with other previous\ngeneralized distributions. We find from the numerical results that the proposed\ndistribution has flexibility and an efficient result\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 21:26:50 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Adam", "A. M.", ""], ["Farouk", "R. M.", ""], ["El-aziz", "M. E. Abd", ""]]}, {"id": "1605.03639", "submitter": "Ali Mollahosseini", "authors": "Ali Mollahosseini, Behzad Hassani, Michelle J. Salvador, Hojjat\n  Abdollahi, David Chan, and Mohammad H. Mahoor", "title": "Facial Expression Recognition from World Wild Web", "comments": null, "journal-ref": null, "doi": "10.1109/CVPRW.2016.188", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing facial expression in a wild setting has remained a challenging\ntask in computer vision. The World Wide Web is a good source of facial images\nwhich most of them are captured in uncontrolled conditions. In fact, the\nInternet is a Word Wild Web of facial images with expressions. This paper\npresents the results of a new study on collecting, annotating, and analyzing\nwild facial expressions from the web. Three search engines were queried using\n1250 emotion related keywords in six different languages and the retrieved\nimages were mapped by two annotators to six basic expressions and neutral. Deep\nneural networks and noise modeling were used in three different training\nscenarios to find how accurately facial expressions can be recognized when\ntrained on noisy images collected from the web using query terms (e.g. happy\nface, laughing man, etc)? The results of our experiments show that deep neural\nnetworks can recognize wild facial expressions with an accuracy of 82.12%.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 23:45:00 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 04:38:42 GMT"}, {"version": "v3", "created": "Thu, 5 Jan 2017 18:07:46 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Mollahosseini", "Ali", ""], ["Hassani", "Behzad", ""], ["Salvador", "Michelle J.", ""], ["Abdollahi", "Hojjat", ""], ["Chan", "David", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1605.03663", "submitter": "Stephen Zakrewsky", "authors": "Stephen Zakrewsky, Kamelia Aryafar, and Ali Shokoufandeh", "title": "Item Popularity Prediction in E-commerce Using Image Quality Feature\n  Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online retail is a visual experience- Shoppers often use images as first\norder information to decide if an item matches their personal style. Image\ncharacteristics such as color, simplicity, scene composition, texture, style,\naesthetics and overall quality play a crucial role in making a purchase\ndecision, clicking on or liking a product listing. In this paper we use a set\nof image features that indicate quality to predict product listing popularity\non a major e-commerce website, Etsy. We first define listing popularity through\nsearch clicks, favoriting and purchase activity. Next, we infer listing quality\nfrom the pixel-level information of listed images as quality features. We then\ncompare our findings to text-only models for popularity prediction. Our initial\nresults indicate that a combined image and text modeling of product listings\noutperforms text-only models in popularity prediction.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 03:10:10 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Zakrewsky", "Stephen", ""], ["Aryafar", "Kamelia", ""], ["Shokoufandeh", "Ali", ""]]}, {"id": "1605.03688", "submitter": "Minghuang Ma", "authors": "Minghuang Ma, Haoqi Fan, Kris M. Kitani", "title": "Going Deeper into First-Person Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We bring together ideas from recent work on feature design for egocentric\naction recognition under one framework by exploring the use of deep\nconvolutional neural networks (CNN). Recent work has shown that features such\nas hand appearance, object attributes, local hand motion and camera ego-motion\nare important for characterizing first-person actions. To integrate these ideas\nunder one framework, we propose a twin stream network architecture, where one\nstream analyzes appearance information and the other stream analyzes motion\ninformation. Our appearance stream encodes prior knowledge of the egocentric\nparadigm by explicitly training the network to segment hands and localize\nobjects. By visualizing certain neuron activation of our network, we show that\nour proposed architecture naturally learns features that capture object\nattributes and hand-object configurations. Our extensive experiments on\nbenchmark egocentric action datasets show that our deep architecture enables\nrecognition rates that significantly outperform state-of-the-art techniques --\nan average $6.6\\%$ increase in accuracy over all datasets. Furthermore, by\nlearning to recognize objects, actions and activities jointly, the performance\nof individual recognition tasks also increase by $30\\%$ (actions) and $14\\%$\n(objects). We also include the results of extensive ablative analysis to\nhighlight the importance of network design decisions..\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 05:59:50 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Ma", "Minghuang", ""], ["Fan", "Haoqi", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1605.03689", "submitter": "Liu Liu", "authors": "Liu Liu, Hongdong Li and Yuchao Dai", "title": "Robust and Efficient Relative Pose with a Multi-camera System for\n  Autonomous Vehicle in Highly Dynamic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the relative pose problem for autonomous vehicle driving\nin highly dynamic and possibly cluttered environments. This is a challenging\nscenario due to the existence of multiple, large, and independently moving\nobjects in the environment, which often leads to excessive portion of outliers\nand results in erroneous motion estimation. Existing algorithms cannot cope\nwith such situations well. This paper proposes a new algorithm for relative\npose using a multi-camera system with multiple non-overlapping individual\ncameras. The method works robustly even when the numbers of outliers are\noverwhelming. By exploiting specific prior knowledge of driving scene we have\ndeveloped an efficient 4-point algorithm for multi-camera relative pose, which\nadmits analytic solutions by solving a polynomial root-finding equation, and\nruns extremely fast (at about 0.5$u$s per root). When the solver is used in\ncombination with RANSAC, we are able to quickly prune unpromising hypotheses,\nsignificantly improve the chance of finding inliers. Experiments on synthetic\ndata have validated the performance of the proposed algorithm. Tests on real\ndata further confirm the method's practical relevance.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 06:00:05 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Liu", "Liu", ""], ["Li", "Hongdong", ""], ["Dai", "Yuchao", ""]]}, {"id": "1605.03705", "submitter": "Marcus Rohrbach", "authors": "Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon,\n  Christopher Pal, Hugo Larochelle, Aaron Courville, Bernt Schiele", "title": "Movie Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audio Description (AD) provides linguistic descriptions of movies and allows\nvisually impaired people to follow a movie along with their peers. Such\ndescriptions are by design mainly visual and thus naturally form an interesting\ndata source for computer vision and computational linguistics. In this work we\npropose a novel dataset which contains transcribed ADs, which are temporally\naligned to full length movies. In addition we also collected and aligned movie\nscripts used in prior work and compare the two sources of descriptions. In\ntotal the Large Scale Movie Description Challenge (LSMDC) contains a parallel\ncorpus of 118,114 sentences and video clips from 202 movies. First we\ncharacterize the dataset by benchmarking different approaches for generating\nvideo descriptions. Comparing ADs to scripts, we find that ADs are indeed more\nvisual and describe precisely what is shown rather than what should happen\naccording to the scripts created prior to movie production. Furthermore, we\npresent and compare the results of several teams who participated in a\nchallenge organized in the context of the workshop \"Describing and\nUnderstanding Video & The Large Scale Movie Description Challenge (LSMDC)\", at\nICCV 2015.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 07:34:08 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Rohrbach", "Anna", ""], ["Torabi", "Atousa", ""], ["Rohrbach", "Marcus", ""], ["Tandon", "Niket", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""], ["Schiele", "Bernt", ""]]}, {"id": "1605.03718", "submitter": "Anna Khoreva", "authors": "Anna Khoreva, Rodrigo Benenson, Fabio Galasso, Matthias Hein, Bernt\n  Schiele", "title": "Improved Image Boundaries for Better Video Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based video segmentation methods rely on superpixels as starting point.\nWhile most previous work has focused on the construction of the graph edges and\nweights as well as solving the graph partitioning problem, this paper focuses\non better superpixels for video segmentation. We demonstrate by a comparative\nanalysis that superpixels extracted from boundaries perform best, and show that\nboundary estimation can be significantly improved via image and time domain\ncues. With superpixels generated from our better boundaries we observe\nconsistent improvement for two video segmentation methods in two different\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 08:14:00 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 10:25:47 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Khoreva", "Anna", ""], ["Benenson", "Rodrigo", ""], ["Galasso", "Fabio", ""], ["Hein", "Matthias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1605.03720", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Luka \\v{C}ehovin, Matej Kristan", "title": "Deformable Parts Correlation Filters for Robust Visual Tracking", "comments": "14 pages, first submission to jurnal: 9.11.2015, re-submission on\n  11.5.2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable parts models show a great potential in tracking by principally\naddressing non-rigid object deformations and self occlusions, but according to\nrecent benchmarks, they often lag behind the holistic approaches. The reason is\nthat potentially large number of degrees of freedom have to be estimated for\nobject localization and simplifications of the constellation topology are often\nassumed to make the inference tractable. We present a new formulation of the\nconstellation model with correlation filters that treats the geometric and\nvisual constraints within a single convex cost function and derive a highly\nefficient optimization for MAP inference of a fully-connected constellation. We\npropose a tracker that models the object at two levels of detail. The coarse\nlevel corresponds a root correlation filter and a novel color model for\napproximate object localization, while the mid-level representation is composed\nof the new deformable constellation of correlation filters that refine the\nobject location. The resulting tracker is rigorously analyzed on a highly\nchallenging OTB, VOT2014 and VOT2015 benchmarks, exhibits a state-of-the-art\nperformance and runs in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 08:22:46 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["\u010cehovin", "Luka", ""], ["Kristan", "Matej", ""]]}, {"id": "1605.03730", "submitter": "Tae-Hyun Oh", "authors": "Kyungdon Joo, Tae-Hyun Oh, Junsik Kim and In So Kweon", "title": "Robust and Globally Optimal Manhattan Frame Estimation in Near Real Time", "comments": "To appear in TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most man-made environments, such as urban and indoor scenes, consist of a set\nof parallel and orthogonal planar structures. These structures are approximated\nby the Manhattan world assumption, in which notion can be represented as a\nManhattan frame (MF). Given a set of inputs such as surface normals or\nvanishing points, we pose an MF estimation problem as a consensus set\nmaximization that maximizes the number of inliers over the rotation search\nspace. Conventionally, this problem can be solved by a branch-and-bound\nframework, which mathematically guarantees global optimality. However, the\ncomputational time of the conventional branch-and-bound algorithms is rather\nfar from real-time. In this paper, we propose a novel bound computation method\non an efficient measurement domain for MF estimation, i.e., the extended\nGaussian image (EGI). By relaxing the original problem, we can compute the\nbound with a constant complexity, while preserving global optimality.\nFurthermore, we quantitatively and qualitatively demonstrate the performance of\nthe proposed method for various synthetic and real-world data. We also show the\nversatility of our approach through three different applications: extension to\nmultiple MF estimation, 3D rotation based video stabilization, and vanishing\npoint estimation (line clustering).\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 08:55:06 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 16:23:37 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Joo", "Kyungdon", ""], ["Oh", "Tae-Hyun", ""], ["Kim", "Junsik", ""], ["Kweon", "In So", ""]]}, {"id": "1605.03746", "submitter": "Stefano Rosa", "authors": "Giorgio Toscana, Stefano Rosa", "title": "Fast Graph-Based Object Segmentation for RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation is an important capability for robotic systems, in\nparticular for grasping. We present a graph- based approach for the\nsegmentation of simple objects from RGB-D images. We are interested in\nsegmenting objects with large variety in appearance, from lack of texture to\nstrong textures, for the task of robotic grasping. The algorithm does not rely\non image features or machine learning. We propose a modified Canny edge\ndetector for extracting robust edges by using depth information and two simple\ncost functions for combining color and depth cues. The cost functions are used\nto build an undirected graph, which is partitioned using the concept of\ninternal and external differences between graph regions. The partitioning is\nfast with O(NlogN) complexity. We also discuss ways to deal with missing depth\ninformation. We test the approach on different publicly available RGB-D object\ndatasets, such as the Rutgers APC RGB-D dataset and the RGB-D Object Dataset,\nand compare the results with other existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 10:29:14 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Toscana", "Giorgio", ""], ["Rosa", "Stefano", ""]]}, {"id": "1605.03804", "submitter": "Sandra Avila", "authors": "Carlos Caetano and Sandra Avila and William Robson Schwartz and Silvio\n  Jamil F. Guimar\\~aes and Arnaldo de A. Ara\\'ujo", "title": "A Mid-level Video Representation based on Binary Descriptors: A Case\n  Study for Pornography Detection", "comments": "Manuscript accepted at Elsevier Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2016.03.099", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing amount of inappropriate content on the Internet, such as\npornography, arises the need to detect and filter such material. The reason for\nthis is given by the fact that such content is often prohibited in certain\nenvironments (e.g., schools and workplaces) or for certain publics (e.g.,\nchildren). In recent years, many works have been mainly focused on detecting\npornographic images and videos based on visual content, particularly on the\ndetection of skin color. Although these approaches provide good results, they\ngenerally have the disadvantage of a high false positive rate since not all\nimages with large areas of skin exposure are necessarily pornographic images,\nsuch as people wearing swimsuits or images related to sports. Local feature\nbased approaches with Bag-of-Words models (BoW) have been successfully applied\nto visual recognition tasks in the context of pornography detection. Even\nthough existing methods provide promising results, they use local feature\ndescriptors that require a high computational processing time yielding\nhigh-dimensional vectors. In this work, we propose an approach for pornography\ndetection based on local binary feature extraction and BossaNova image\nrepresentation, a BoW model extension that preserves more richly the visual\ninformation. Moreover, we propose two approaches for video description based on\nthe combination of mid-level representations namely BossaNova Video Descriptor\n(BNVD) and BoW Video Descriptor (BoW-VD). The proposed techniques are\npromising, achieving an accuracy of 92.40%, thus reducing the classification\nerror by 16% over the current state-of-the-art local features approach on the\nPornography dataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 13:27:12 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Caetano", "Carlos", ""], ["Avila", "Sandra", ""], ["Schwartz", "William Robson", ""], ["Guimar\u00e3es", "Silvio Jamil F.", ""], ["Ara\u00fajo", "Arnaldo de A.", ""]]}, {"id": "1605.03821", "submitter": "Jian Wang", "authors": "Liqing Gao, Yanzhang Wang, Xin Ye and Jian Wang", "title": "Crowd Counting Considering Network Flow Constraints in Videos", "comments": "20pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of the number of people in the monitoring scene may increase the\nprobability of security threat, which makes crowd counting more and more\nimportant. Most of the existing approaches estimate the number of pedestrians\nwithin one frame, which results in inconsistent predictions in terms of time.\nThis paper, for the first time, introduces a quadratic programming model with\nthe network flow constraints to improve the accuracy of crowd counting.\nFirstly, the foreground of each frame is segmented into groups, each of which\ncontains several pedestrians. Then, a regression-based map is developed in\naccordance with the relationship between low-level features of each group and\nthe number of people in it. Secondly, a directed graph is constructed to\nsimulate constraints on people's flow, whose vertices represent groups of each\nframe and arcs represent people moving from one group to another. Then, the\npeople flow can be viewed as an integer flow in the constructed digraph.\nFinally, by solving a quadratic programming problem with network flow\nconstraints in the directed graph, we obtain consistency in people counting.\nThe experimental results show that the proposed method can reduce the crowd\ncounting errors and improve the accuracy. Moreover, this method can also be\napplied to any ultramodern group-based regression counting approach to get\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 14:12:21 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 14:22:55 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Gao", "Liqing", ""], ["Wang", "Yanzhang", ""], ["Ye", "Xin", ""], ["Wang", "Jian", ""]]}, {"id": "1605.03865", "submitter": "Fengfu Li", "authors": "Fengfu Li, Xiayuan Huang, Hong Qiao and Bo Zhang", "title": "A New Manifold Distance Measure for Visual Object Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold distances are very effective tools for visual object recognition.\nHowever, most of the traditional manifold distances between images are based on\nthe pixel-level comparison and thus easily affected by image rotations and\ntranslations. In this paper, we propose a new manifold distance to model the\ndissimilarities between visual objects based on the Complex Wavelet Structural\nSimilarity (CW-SSIM) index. The proposed distance is more robust to rotations\nand translations of images than the traditional manifold distance and the\nCW-SSIM index based distance. In addition, the proposed distance is combined\nwith the $k$-medoids clustering method to derive a new clustering method for\nvisual object categorization. Experiments on Coil-20, Coil-100 and Olivetti\nFace Databases show that the proposed distance measure is better for visual\nobject categorization than both the traditional manifold distances and the\nCW-SSIM index based distances.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 15:51:39 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Li", "Fengfu", ""], ["Huang", "Xiayuan", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""]]}, {"id": "1605.04006", "submitter": "Ruoqiao Zhang", "authors": "Ruoqiao Zhang, Dong Hye Ye, Debashish Pal, Jean-Baptiste Thibault, Ken\n  D. Sauer, Charles A. Bouman", "title": "A Gaussian Mixture MRF for Model-Based Iterative Reconstruction with\n  Applications to Low-Dose X-ray CT", "comments": "accepted by IEEE Transactions on Computed Imaging", "journal-ref": null, "doi": "10.1109/TCI.2016.2582042", "report-no": null, "categories": "cs.CV math.OC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields (MRFs) have been widely used as prior models in various\ninverse problems such as tomographic reconstruction. While MRFs provide a\nsimple and often effective way to model the spatial dependencies in images,\nthey suffer from the fact that parameter estimation is difficult. In practice,\nthis means that MRFs typically have very simple structure that cannot\ncompletely capture the subtle characteristics of complex images.\n  In this paper, we present a novel Gaussian mixture Markov random field model\n(GM-MRF) that can be used as a very expressive prior model for inverse problems\nsuch as denoising and reconstruction. The GM-MRF forms a global image model by\nmerging together individual Gaussian-mixture models (GMMs) for image patches.\nIn addition, we present a novel analytical framework for computing MAP\nestimates using the GM-MRF prior model through the construction of surrogate\nfunctions that result in a sequence of quadratic optimizations. We also\nintroduce a simple but effective method to adjust the GM-MRF so as to control\nthe sharpness in low- and high-contrast regions of the reconstruction\nseparately. We demonstrate the value of the model with experiments including\nimage denoising and low-dose CT reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 22:56:12 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 21:33:07 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Zhang", "Ruoqiao", ""], ["Ye", "Dong Hye", ""], ["Pal", "Debashish", ""], ["Thibault", "Jean-Baptiste", ""], ["Sauer", "Ken D.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1605.04039", "submitter": "Guangrun Wang", "authors": "Liang Lin, Guangrun Wang, Wangmeng Zuo, Xiangchu Feng, and Lei Zhang", "title": "Cross-Domain Visual Matching via Generalized Similarity Measure and\n  Feature Learning", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI), 2016", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2567386", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain visual data matching is one of the fundamental problems in many\nreal-world vision tasks, e.g., matching persons across ID photos and\nsurveillance videos. Conventional approaches to this problem usually involves\ntwo steps: i) projecting samples from different domains into a common space,\nand ii) computing (dis-)similarity in this space based on a certain distance.\nIn this paper, we present a novel pairwise similarity measure that advances\nexisting models by i) expanding traditional linear projections into affine\ntransformations and ii) fusing affine Mahalanobis distance and Cosine\nsimilarity by a data-driven combination. Moreover, we unify our similarity\nmeasure with feature representation learning via deep convolutional neural\nnetworks. Specifically, we incorporate the similarity measure matrix into the\ndeep architecture, enabling an end-to-end way of model optimization. We\nextensively evaluate our generalized similarity model in several challenging\ncross-domain matching tasks: person re-identification under different views and\nface verification over different modalities (i.e., faces from still images and\nvideos, older and younger faces, and sketch and photo portraits). The\nexperimental results demonstrate superior performance of our model over other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 03:35:14 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lin", "Liang", ""], ["Wang", "Guangrun", ""], ["Zuo", "Wangmeng", ""], ["Feng", "Xiangchu", ""], ["Zhang", "Lei", ""]]}, {"id": "1605.04046", "submitter": "George Stamatescu", "authors": "George Stamatescu, Langford B White and Riley Bruce-Doust", "title": "Track Extraction with Hidden Reciprocal Chain Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops Bayesian track extraction algorithms for targets modelled\nas hidden reciprocal chains (HRC). HRC are a class of finite-state random\nprocess models that generalise the familiar hidden Markov chains (HMC). HRC are\nable to model the \"intention\" of a target to proceed from a given origin to a\ndestination, behaviour which cannot be properly captured by a HMC. While\nBayesian estimation problems for HRC have previously been studied, this paper\nfocusses principally on the problem of track extraction, of which the primary\ntask is confirming target existence in a set of detections obtained from\nthresholding sensor measurements. Simulation examples are presented which show\nthat the additional model information contained in a HRC improves detection\nperformance when compared to HMC models.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 05:29:33 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Stamatescu", "George", ""], ["White", "Langford B", ""], ["Bruce-Doust", "Riley", ""]]}, {"id": "1605.04068", "submitter": "Falong Shen", "authors": "Falong Shen and Gang Zeng", "title": "Fast Semantic Image Segmentation with High Order Context and Guided\n  Filtering", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fast and accurate semantic image segmentation approach\nthat encodes not only the discriminative features from deep neural networks,\nbut also the high-order context compatibility among adjacent objects as well as\nlow level image features. We formulate the underlying problem as the\nconditional random field that embeds local feature extraction, clique potential\nconstruction, and guided filtering within the same framework, and provide an\nefficient coarse-to-fine solver. At the coarse level, we combine local feature\nrepresentation and context interaction using a deep convolutional network, and\ndirectly learn the interaction from high order cliques with a message passing\nroutine, avoiding time-consuming explicit graph inference for joint probability\ndistribution. At the fine level, we introduce a guided filtering interpretation\nfor the mean field algorithm, and achieve accurate object boundaries with 100+\nfaster than classic learning methods. The two parts are connected and jointly\ntrained in an end-to-end fashion. Experimental results on Pascal VOC 2012\ndataset have shown that the proposed algorithm outperforms the\nstate-of-the-art, and that it achieves the rank 1 performance at the time of\nsubmission, both of which prove the effectiveness of this unified framework for\nsemantic image segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 07:21:37 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Shen", "Falong", ""], ["Zeng", "Gang", ""]]}, {"id": "1605.04129", "submitter": "Maedeh Aghaei", "authors": "Maedeh Aghaei, Mariella Dimiccoli, Petia Radeva", "title": "With Whom Do I Interact? Detecting Social Interactions in Egocentric\n  Photo-streams", "comments": "6 pages, 9 figures, accepted and presented in International\n  Conference on Pattern Recognition (ICPR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a user wearing a low frame rate wearable camera during a day, this work\naims to automatically detect the moments when the user gets engaged into a\nsocial interaction solely by reviewing the automatically captured photos by the\nworn camera. The proposed method, inspired by the sociological concept of\nF-formation, exploits distance and orientation of the appearing individuals\n-with respect to the user- in the scene from a bird-view perspective. As a\nresult, the interaction pattern over the sequence can be understood as a\ntwo-dimensional time series that corresponds to the temporal evolution of the\ndistance and orientation features over time. A Long-Short Term Memory-based\nRecurrent Neural Network is then trained to classify each time series.\nExperimental evaluation over a dataset of 30.000 images has shown promising\nresults on the proposed method for social interaction detection in egocentric\nphoto-streams.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 11:04:28 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 11:27:50 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Aghaei", "Maedeh", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1605.04243", "submitter": "Henryk Blasinski", "authors": "Henryk Blasinski, Joyce Farrell and Brian Wandell", "title": "Simultaneous Surface Reflectance and Fluorescence Spectra Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is widespread interest in estimating the fluorescence properties of\nnatural materials in an image. However, the separation between reflected and\nfluoresced components is difficult, because it is impossible to distinguish\nreflected and fluoresced photons without controlling the illuminant spectrum.\nWe show how to jointly estimate the reflectance and fluorescence from a single\nset of images acquired under multiple illuminants. We present a framework based\non a linear approximation to the physical equations describing image formation\nin terms of surface spectral reflectance and fluorescence due to multiple\nfluorophores. We relax the non-convex, inverse estimation problem in order to\njointly estimate the reflectance and fluorescence properties in a single\noptimization step and we use the Alternating Direction Method of Multipliers\n(ADMM) approach to efficiently find a solution. We provide a software\nimplementation of the solver for our method and prior methods. We evaluate the\naccuracy and reliability of the method using both simulations and experimental\ndata. To acquire data to test the methods, we built a custom imaging system\nusing a monochrome camera, a filter wheel with bandpass transmissive filters\nand a small number of light emitting diodes. We compared the system and\nalgorithm performance with the ground truth as well as with prior methods. Our\napproach produces lower errors compared to earlier algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 16:36:09 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Blasinski", "Henryk", ""], ["Farrell", "Joyce", ""], ["Wandell", "Brian", ""]]}, {"id": "1605.04250", "submitter": "Han Gong", "authors": "Graham D. Finlayson, Han Gong, Robert B. Fisher", "title": "Color Homography", "comments": "Accepted by Progress in Colour Studies 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show the surprising result that colors across a change in viewing\ncondition (changing light color, shading and camera) are related by a\nhomography. Our homography color correction application delivers improved color\nfidelity compared with the linear least-square.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 16:56:10 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 13:25:06 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Finlayson", "Graham D.", ""], ["Gong", "Han", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1605.04253", "submitter": "Soravit Changpinyo", "authors": "Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, Fei Sha", "title": "An Empirical Study and Analysis of Generalized Zero-Shot Learning for\n  Object Recognition in the Wild", "comments": "ECCV2016 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) methods have been studied in the unrealistic setting\nwhere test data are assumed to come from unseen classes only. In this paper, we\nadvocate studying the problem of generalized zero-shot learning (GZSL) where\nthe test data's class memberships are unconstrained. We show empirically that\nnaively using the classifiers constructed by ZSL approaches does not perform\nwell in the generalized setting. Motivated by this, we propose a simple but\neffective calibration method that can be used to balance two conflicting\nforces: recognizing data from seen classes versus those from unseen ones. We\ndevelop a performance metric to characterize such a trade-off and examine the\nutility of this metric in evaluating various ZSL approaches. Our analysis\nfurther shows that there is a large gap between the performance of existing\napproaches and an upper bound established via idealized semantic embeddings,\nsuggesting that improving class semantic embeddings is vital to GZSL.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:05:01 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 08:33:25 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Chao", "Wei-Lun", ""], ["Changpinyo", "Soravit", ""], ["Gong", "Boqing", ""], ["Sha", "Fei", ""]]}, {"id": "1605.04369", "submitter": "Ragav Venkatesan", "authors": "Ragav Venkatesan and Vijetha Gattupalli and Baoxin Li", "title": "Neural Dataset Generality", "comments": "Long version of the paper accepted at IEEE International Conference\n  on Image Processing 2016", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532315", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the filters learned by Convolutional Neural Networks (CNNs) from\ndifferent datasets appear similar. This is prominent in the first few layers.\nThis similarity of filters is being exploited for the purposes of transfer\nlearning and some studies have been made to analyse such transferability of\nfeatures. This is also being used as an initialization technique for different\ntasks in the same dataset or for the same task in similar datasets.\nOff-the-shelf CNN features have capitalized on this idea to promote their\nnetworks as best transferable and most general and are used in a cavalier\nmanner in day-to-day computer vision tasks.\n  It is curious that while the filters learned by these CNNs are related to the\natomic structures of the images from which they are learnt, all datasets learn\nsimilar looking low-level filters. With the understanding that a dataset that\ncontains many such atomic structures learn general filters and are therefore\nuseful to initialize other networks with, we propose a way to analyse and\nquantify generality among datasets from their accuracies on transferred\nfilters. We applied this metric on several popular character recognition,\nnatural image and a medical image dataset, and arrived at some interesting\nconclusions. On further experimentation we also discovered that particular\nclasses in a dataset themselves are more general than others.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 03:17:15 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Venkatesan", "Ragav", ""], ["Gattupalli", "Vijetha", ""], ["Li", "Baoxin", ""]]}, {"id": "1605.04478", "submitter": "Hamid Tizhoosh", "authors": "Mina Nouredanesh, Hamid R. Tizhoosh, Ershad Banijamali", "title": "Gabor Barcodes for Medical Image Retrieval", "comments": "To appear in proceedings of The 2016 IEEE International Conference on\n  Image Processing (ICIP 2016), Sep 25-28, 2016, Phoenix, Arizona, USA", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532807", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, advances in medical imaging have led to the emergence of\nmassive databases, containing images from a diverse range of modalities. This\nhas significantly heightened the need for automated annotation of the images on\none side, and fast and memory-efficient content-based image retrieval systems\non the other side. Binary descriptors have recently gained more attention as a\npotential vehicle to achieve these goals. One of the recently introduced binary\ndescriptors for tagging of medical images are Radon barcodes (RBCs) that are\ndriven from Radon transform via local thresholding. Gabor transform is also a\npowerful transform to extract texture-based information. Gabor features have\nexhibited robustness against rotation, scale, and also photometric\ndisturbances, such as illumination changes and image noise in many\napplications. This paper introduces Gabor Barcodes (GBCs), as a novel framework\nfor the image annotation. To find the most discriminative GBC for a given query\nimage, the effects of employing Gabor filters with different parameters, i.e.,\ndifferent sets of scales and orientations, are investigated, resulting in\ndifferent barcode lengths and retrieval performances. The proposed method has\nbeen evaluated on the IRMA dataset with 193 classes comprising of 12,677 x-ray\nimages for indexing, and 1,733 x-rays images for testing. A total error score\nas low as $351$ ($\\approx 80\\%$ accuracy for the first hit) was achieved.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2016 22:39:29 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Nouredanesh", "Mina", ""], ["Tizhoosh", "Hamid R.", ""], ["Banijamali", "Ershad", ""]]}, {"id": "1605.04502", "submitter": "Bing Wang", "authors": "Bing Wang, Li Wang, Bing Shuai, Zhen Zuo, Ting Liu, Kap Luk Chan, Gang\n  Wang", "title": "Joint Learning of Siamese CNNs and Temporally Constrained Metrics for\n  Tracklet Association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the challenging problem of multi-object tracking in a\ncomplex scene captured by a single camera. Different from the existing tracklet\nassociation-based tracking methods, we propose a novel and efficient way to\nobtain discriminative appearance-based tracklet affinity models. Our proposed\nmethod jointly learns the convolutional neural networks (CNNs) and temporally\nconstrained metrics. In our method, a Siamese convolutional neural network\n(CNN) is first pre-trained on the auxiliary data. Then the Siamese CNN and\ntemporally constrained metrics are jointly learned online to construct the\nappearance-based tracklet affinity models. The proposed method can jointly\nlearn the hierarchical deep features and temporally constrained segment-wise\nmetrics under a unified framework. For reliable association between tracklets,\na novel loss function incorporating temporally constrained multi-task learning\nmechanism is proposed. By employing the proposed method, tracklet association\ncan be accomplished even in challenging situations. Moreover, a new dataset\nwith 40 fully annotated sequences is created to facilitate the tracking\nevaluation. Experimental results on five public datasets and the new\nlarge-scale dataset show that our method outperforms several state-of-the-art\napproaches in multi-object tracking.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 07:09:28 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2016 09:58:32 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Wang", "Bing", ""], ["Wang", "Li", ""], ["Shuai", "Bing", ""], ["Zuo", "Zhen", ""], ["Liu", "Ting", ""], ["Chan", "Kap Luk", ""], ["Wang", "Gang", ""]]}, {"id": "1605.04603", "submitter": "Roman Novak", "authors": "Roman Novak and Yaroslav Nikulin", "title": "Improving the Neural Algorithm of Artistic Style", "comments": "A short class project report (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate different avenues of improving the Neural\nAlgorithm of Artistic Style (by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge, arXiv:1508.06576).\n  While showing great results when transferring homogeneous and repetitive\npatterns, the original style representation often fails to capture more complex\nproperties, like having separate styles of foreground and background. This\nleads to visual artifacts and undesirable textures appearing in unexpected\nregions when performing style transfer.\n  We tackle this issue with a variety of approaches, mostly by modifying the\nstyle representation in order for it to capture more information and impose a\ntighter constraint on the style transfer result.\n  In our experiments, we subjectively evaluate our best method as producing\nfrom barely noticeable to significant improvements in the quality of style\ntransfer.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 20:11:27 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Novak", "Roman", ""], ["Nikulin", "Yaroslav", ""]]}, {"id": "1605.04634", "submitter": "Changzhe Jiao", "authors": "Changzhe Jiao, Princess Lyons, Alina Zare, Licet Rosales, Marjorie\n  Skubic", "title": "Heart Beat Characterization from Ballistocardiogram Signals using\n  Extended Functions of Multiple Instances", "comments": "IEEE EMBC 2016, pp. 1-5", "journal-ref": "IEEE EMBC 2016, pp. 1-5", "doi": "10.1109/EMBC.2016.7590812", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple instance learning (MIL) method, extended Function of Multiple\nInstances ($e$FUMI), is applied to ballistocardiogram (BCG) signals produced by\na hydraulic bed sensor. The goal of this approach is to learn a personalized\nheartbeat \"concept\" for an individual. This heartbeat concept is a prototype\n(or \"signature\") that characterizes the heartbeat pattern for an individual in\nballistocardiogram data. The $e$FUMI method models the problem of learning a\nheartbeat concept from a BCG signal as a MIL problem. This approach elegantly\naddresses the uncertainty inherent in a BCG signal e. g., misalignment between\ntraining data and ground truth, mis-collection of heartbeat by some\ntransducers, etc. Given a BCG training signal coupled with a ground truth\nsignal (e.g., a pulse finger sensor), training \"bags\" labeled with only binary\nlabels denoting if a training bag contains a heartbeat signal or not can be\ngenerated. Then, using these bags, $e$FUMI learns a personalized concept of\nheartbeat for a subject as well as several non-heartbeat background concepts.\nAfter learning the heartbeat concept, heartbeat detection and heart rate\nestimation can be applied to test data. Experimental results show that the\nestimated heartbeat concept found by $e$FUMI is more representative and a more\ndiscriminative prototype of the heartbeat signals than those found by\ncomparison MIL methods in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 02:30:28 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jiao", "Changzhe", ""], ["Lyons", "Princess", ""], ["Zare", "Alina", ""], ["Rosales", "Licet", ""], ["Skubic", "Marjorie", ""]]}, {"id": "1605.04711", "submitter": "Fengfu Li", "authors": "Fengfu Li, Bo Zhang and Bin Liu", "title": "Ternary Weight Networks", "comments": "5 pages, 3 fitures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ternary weight networks (TWNs) - neural networks with weights\nconstrained to +1, 0 and -1. The Euclidian distance between full (float or\ndouble) precision weights and the ternary weights along with a scaling factor\nis minimized. Besides, a threshold-based ternary function is optimized to get\nan approximated solution which can be fast and easily computed. TWNs have\nstronger expressive abilities than the recently proposed binary precision\ncounterparts and are thus more effective than the latter. Meanwhile, TWNs\nachieve up to 16$\\times$ or 32$\\times$ model compression rate and need fewer\nmultiplications compared with the full precision counterparts. Benchmarks on\nMNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of\nTWNs is only slightly worse than the full precision counterparts but\noutperforms the analogous binary precision counterparts a lot.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 10:21:25 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 02:23:03 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Li", "Fengfu", ""], ["Zhang", "Bo", ""], ["Liu", "Bin", ""]]}, {"id": "1605.04731", "submitter": "Xianye Liang", "authors": "Xianye Liang, Bocheng Zhuo, Peijie Li, Liangju He", "title": "CNN based texture synthesize with Semantic segment", "comments": "7 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1505.07376, arXiv:1604.04339, arXiv:1602.07188 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithm display powerful ability in Computer Vision area, in\nrecent year, the CNN has been applied to solve problems in the subarea of\nImage-generating, which has been widely applied in areas such as photo editing,\nimage design, computer animation, real-time rendering for large scale of scenes\nand for visual effects in movies. However in the texture synthesize procedure.\nThe state-of-art CNN can not capture the spatial location of texture in image,\nlead to significant distortion after texture synthesize, we propose a new way\nto generating-image by adding the semantic segment step with deep learning\nalgorithm as Pre-Processing and analyze the outcome.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 11:24:03 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Liang", "Xianye", ""], ["Zhuo", "Bocheng", ""], ["Li", "Peijie", ""], ["He", "Liangju", ""]]}, {"id": "1605.04770", "submitter": "Lamberto Ballan", "authors": "Tiberio Uricchio, Lamberto Ballan, Lorenzo Seidenari, Alberto Del\n  Bimbo", "title": "Automatic Image Annotation via Label Transfer in the Semantic Space", "comments": "To appear in Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2017.05.019", "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation is among the fundamental problems in computer\nvision and pattern recognition, and it is becoming increasingly important in\norder to develop algorithms that are able to search and browse large-scale\nimage collections. In this paper, we propose a label propagation framework\nbased on Kernel Canonical Correlation Analysis (KCCA), which builds a latent\nsemantic space where correlation of visual and textual features are well\npreserved into a semantic embedding. The proposed approach is robust and can\nwork either when the training set is well annotated by experts, as well as when\nit is noisy such as in the case of user-generated tags in social media. We\nreport extensive results on four popular datasets. Our results show that our\nKCCA-based framework can be applied to several state-of-the-art label transfer\nmethods to obtain significant improvements. Our approach works even with the\nnoisy tags of social users, provided that appropriate denoising is performed.\nExperiments on a large scale setting show that our method can provide some\nbenefits even when the semantic space is estimated on a subset of training\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 13:45:15 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:24:00 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 13:21:02 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Uricchio", "Tiberio", ""], ["Ballan", "Lamberto", ""], ["Seidenari", "Lorenzo", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1605.04785", "submitter": "Fran\\c{c}ois Piti\\'e", "authors": "Fran\\c{c}ois Piti\\'e", "title": "An Alternative Matting Laplacian", "comments": "ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting out and object and estimate its transparency mask is a key task in\nmany applications. We take on the work on closed-form matting by Levin et al.,\nthat is used at the core of many matting techniques, and propose an alternative\nformulation that offers more flexible controls over the matting priors. We also\nshow that this new approach is efficient at upscaling transparency maps from\ncoarse estimates.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 14:42:34 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "1605.04806", "submitter": "Mohammad Hamed Mozaffari", "authors": "Mohammad Hamed Mozaffari, Won-Sook Lee", "title": "Multilevel Thresholding Segmentation of T2 weighted Brain MRI images\n  using Convergent Heterogeneous Particle Swarm Optimization", "comments": "Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new image thresholding segmentation approach using the\nheuristic method, Convergent Heterogeneous Particle Swarm Optimization\nalgorithm. The proposed algorithm incorporates a new strategy of searching the\nproblem space by dividing the swarm into subswarms. Each subswarm particles\nsearch for better solution separately lead to better exploitation while they\ncooperate with each other to find the best global position. The consequence of\nthe aforementioned cooperation is better exploration, convergence and it able\nthe algorithm to jump from local optimal solution to the better spots. A\npractical application of this method is demonstrated for the problem of medical\nimage thresholding segmentation. We considered two classical thresholding\ntechniques of Otsu and Kapur separately as the objective function for the\noptimization method and applied on a set of brain MR images. Comparative\nexperimental results reveal that the proposed method outperforms another state\nof the art method from the literature in terms of accuracy, computation time\nand stable results.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 15:30:05 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Mozaffari", "Mohammad Hamed", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1605.04850", "submitter": "Michael Gygli", "authors": "Michael Gygli and Yale Song and Liangliang Cao", "title": "Video2GIF: Automatic Generation of Animated GIFs from Video", "comments": "Accepted to CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel problem of automatically generating animated GIFs from\nvideo. GIFs are short looping video with no sound, and a perfect combination\nbetween image and video that really capture our attention. GIFs tell a story,\nexpress emotion, turn events into humorous moments, and are the new wave of\nphotojournalism. We pose the question: Can we automate the entirely manual and\nelaborate process of GIF creation by leveraging the plethora of user generated\nGIF content? We propose a Robust Deep RankNet that, given a video, generates a\nranked list of its segments according to their suitability as GIF. We train our\nmodel to learn what visual content is often selected for GIFs by using over\n100K user generated GIFs and their corresponding video sources. We effectively\ndeal with the noisy web data by proposing a novel adaptive Huber loss in the\nranking formulation. We show that our approach is robust to outliers and picks\nup several patterns that are frequently present in popular animated GIFs. On\nour new large-scale benchmark dataset, we show the advantage of our approach\nover several state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 17:44:31 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Gygli", "Michael", ""], ["Song", "Yale", ""], ["Cao", "Liangliang", ""]]}, {"id": "1605.04874", "submitter": "Amir Hosein Zamanian", "authors": "Amir Hosein Zamanian, Abdolreza Ohadi", "title": "Gearbox Fault Detection through PSO Exact Wavelet Analysis and SVM\n  Classifier", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.1.4983.3442", "report-no": "ISME2010-3820", "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-frequency methods for vibration-based gearbox faults detection have been\nconsidered the most efficient method. Among these methods, continuous wavelet\ntransform (CWT) as one of the best time-frequency method has been used for both\nstationary and transitory signals. Some deficiencies of CWT are problem of\noverlapping and distortion ofsignals. In this condition, a large amount of\nredundant information exists so that it may cause false alarm or\nmisinterpretation of the operator. In this paper a modified method called Exact\nWavelet Analysis is used to minimize the effects of overlapping and distortion\nin case of gearbox faults. To implement exact wavelet analysis, Particle Swarm\nOptimization (PSO) algorithm has been used for this purpose. This method have\nbeen implemented for the acceleration signals from 2D acceleration sensor\nacquired by Advantech PCI-1710 card from a gearbox test setup in Amirkabir\nUniversity of Technology. Gearbox has been considered in both healthy and\nchipped tooth gears conditions. Kernelized Support Vector Machine (SVM) with\nradial basis functions has used the extracted features from exact wavelet\nanalysis for classification. The efficiency of this classifier is then\nevaluated with the other signals acquired from the setup test. The results show\nthat in comparison of CWT, PSO Exact Wavelet Transform has better ability in\nfeature extraction in price of more computational effort. In addition, PSO\nexact wavelet has better speed comparing to Genetic Algorithm (GA) exact\nwavelet in condition of equal population because of factoring mutation and\ncrossover in PSO algorithm. SVM classifier with the extracted features in\ngearbox shows very good results and its ability has been proved.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 23:29:29 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Zamanian", "Amir Hosein", ""], ["Ohadi", "Abdolreza", ""]]}, {"id": "1605.04932", "submitter": "Frosti Palsson", "authors": "Magnus O. Ulfarsson, Frosti Palsson, Jakob Sigurdsson and Johannes R.\n  Sveinsson", "title": "Classification of Big Data with Application to Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data applications, such as medical imaging and genetics, typically\ngenerate datasets that consist of few observations n on many more variables p,\na scenario that we denote as p>>n. Traditional data processing methods are\noften insufficient for extracting information out of big data. This calls for\nthe development of new algorithms that can deal with the size, complexity, and\nthe special structure of such datasets. In this paper, we consider the problem\nof classifying p>>n data and propose a classification method based on linear\ndiscriminant analysis (LDA). Traditional LDA depends on the covariance estimate\nof the data, but when p>>n the sample covariance estimate is singular. The\nproposed method estimates the covariance by using a sparse version of noisy\nprincipal component analysis (nPCA). The use of sparsity in this setting aims\nat automatically selecting variables that are relevant for classification. In\nexperiments, the new method is compared to state-of-the art methods for big\ndata problems using both simulated datasets and imaging genetics datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 20:16:29 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Ulfarsson", "Magnus O.", ""], ["Palsson", "Frosti", ""], ["Sigurdsson", "Jakob", ""], ["Sveinsson", "Johannes R.", ""]]}, {"id": "1605.04951", "submitter": "Po-Shen Lee", "authors": "Po-shen Lee, Jevin D. West, and Bill Howe", "title": "Viziometrics: Analyzing Visual Information in the Scientific Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.DL cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientific results are communicated visually in the literature through\ndiagrams, visualizations, and photographs. These information-dense objects have\nbeen largely ignored in bibliometrics and scientometrics studies when compared\nto citations and text. In this paper, we use techniques from computer vision\nand machine learning to classify more than 8 million figures from PubMed into 5\nfigure types and study the resulting patterns of visual information as they\nrelate to impact. We find that the distribution of figures and figure types in\nthe literature has remained relatively constant over time, but can vary widely\nacross field and topic. Remarkably, we find a significant correlation between\nscientific impact and the use of visual information, where higher impact papers\ntend to include more diagrams, and to a lesser extent more plots and\nphotographs. To explore these results and other ways of extracting this visual\ninformation, we have built a visual browser to illustrate the concept and\nexplore design alternatives for supporting viziometric analysis and organizing\nvisual information. We use these results to articulate a new research agenda --\nviziometrics -- to study the organization and presentation of visual\ninformation in the scientific literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 21:03:57 GMT"}, {"version": "v2", "created": "Fri, 27 May 2016 17:26:22 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Lee", "Po-shen", ""], ["West", "Jevin D.", ""], ["Howe", "Bill", ""]]}, {"id": "1605.04988", "submitter": "Mehrtash Harandi", "authors": "Samitha Herath, Mehrtash Harandi, Fatih Porikli", "title": "Going Deeper into Action Recognition: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human actions in visual data is tied to advances in\ncomplementary research areas including object recognition, human dynamics,\ndomain adaptation and semantic segmentation. Over the last decade, human action\nanalysis evolved from earlier schemes that are often limited to controlled\nenvironments to nowadays advanced solutions that can learn from millions of\nvideos and apply to almost all daily activities. Given the broad range of\napplications from video surveillance to human-computer interaction, scientific\nmilestones in action recognition are achieved more rapidly, eventually leading\nto the demise of what used to be good in a short time. This motivated us to\nprovide a comprehensive review of the notable steps taken towards recognizing\nhuman actions. To this end, we start our discussion with the pioneering methods\nthat use handcrafted representations, and then, navigate into the realm of deep\nlearning based approaches. We aim to remain objective throughout this survey,\ntouching upon encouraging improvements as well as inevitable fallbacks, in the\nhope of raising fresh questions and motivating new research directions for the\nreader.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 23:59:10 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 03:27:52 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Herath", "Samitha", ""], ["Harandi", "Mehrtash", ""], ["Porikli", "Fatih", ""]]}, {"id": "1605.04996", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Fuyong Xing, Xiaoshuang Shi, Lin Yang", "title": "SemiContour: A Semi-supervised Learning Approach for Contour Detection", "comments": "Accepted by Computer Vision and Pattern Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised contour detection methods usually require many labeled training\nimages to obtain satisfactory performance. However, a large set of annotated\ndata might be unavailable or extremely labor intensive. In this paper, we\ninvestigate the usage of semi-supervised learning (SSL) to obtain competitive\ndetection accuracy with very limited training data (three labeled images).\nSpecifically, we propose a semi-supervised structured ensemble learning\napproach for contour detection built on structured random forests (SRF). To\nallow SRF to be applicable to unlabeled data, we present an effective sparse\nrepresentation approach to capture inherent structure in image patches by\nfinding a compact and discriminative low-dimensional subspace representation in\nan unsupervised manner, enabling the incorporation of abundant unlabeled\npatches with their estimated structured labels to help SRF perform better node\nsplitting. We re-examine the role of sparsity and propose a novel and fast\nsparse coding algorithm to boost the overall learning efficiency. To the best\nof our knowledge, this is the first attempt to apply SSL for contour detection.\nExtensive experiments on the BSDS500 segmentation dataset and the NYU Depth\ndataset demonstrate the superiority of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 01:33:20 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Zhang", "Zizhao", ""], ["Xing", "Fuyong", ""], ["Shi", "Xiaoshuang", ""], ["Yang", "Lin", ""]]}, {"id": "1605.05019", "submitter": "Tianzhu Xiang", "authors": "Tianzhu Xiang, Gui-Song Xia, Liangpei Zhang", "title": "Image stitching with perspective-preserving warping", "comments": "ISPRS 2016 - XXIII ISPRS Congress: Prague, Czech Republic, 2016", "journal-ref": null, "doi": "10.5194/isprs-annals-III-3-287-2016", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image stitching algorithms often adopt the global transformation, such as\nhomography, and work well for planar scenes or parallax free camera motions.\nHowever, these conditions are easily violated in practice. With casual camera\nmotions, variable taken views, large depth change, or complex structures, it is\na challenging task for stitching these images. The global transformation model\noften provides dreadful stitching results, such as misalignments or projective\ndistortions, especially perspective distortion. To this end, we suggest a\nperspective-preserving warping for image stitching, which spatially combines\nlocal projective transformations and similarity transformation. By weighted\ncombination scheme, our approach gradually extrapolates the local projective\ntransformations of the overlapping regions into the non-overlapping regions,\nand thus the final warping can smoothly change from projective to similarity.\nThe proposed method can provide satisfactory alignment accuracy as well as\nreduce the projective distortions and maintain the multi-perspective view.\nExperiments on a variety of challenging images confirm the efficiency of the\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 05:07:51 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xiang", "Tianzhu", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1605.05034", "submitter": "Xiaojie Guo", "authors": "Xiaojie Guo", "title": "LIME: A Method for Low-light IMage Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When one captures images in low-light conditions, the images often suffer\nfrom low visibility. This poor quality may significantly degrade the\nperformance of many computer vision and multimedia algorithms that are\nprimarily designed for high-quality inputs. In this paper, we propose a very\nsimple and effective method, named as LIME, to enhance low-light images. More\nconcretely, the illumination of each pixel is first estimated individually by\nfinding the maximum value in R, G and B channels. Further, we refine the\ninitial illumination map by imposing a structure prior on it, as the final\nillumination map. Having the well-constructed illumination map, the enhancement\ncan be achieved accordingly. Experiments on a number of challenging real-world\nlow-light images are present to reveal the efficacy of our LIME and show its\nsuperiority over several state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 06:50:55 GMT"}, {"version": "v2", "created": "Sun, 29 May 2016 11:51:48 GMT"}, {"version": "v3", "created": "Sun, 24 Jul 2016 07:43:28 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Guo", "Xiaojie", ""]]}, {"id": "1605.05045", "submitter": "Raffaello Camoriano", "authors": "Raffaello Camoriano, Giulia Pasquale, Carlo Ciliberto, Lorenzo Natale,\n  Lorenzo Rosasco, Giorgio Metta", "title": "Incremental Robot Learning of New Objects with Fixed Update Time", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider object recognition in the context of lifelong learning, where a\nrobotic agent learns to discriminate between a growing number of object classes\nas it accumulates experience about the environment. We propose an incremental\nvariant of the Regularized Least Squares for Classification (RLSC) algorithm,\nand exploit its structure to seamlessly add new classes to the learned model.\nThe presented algorithm addresses the problem of having an unbalanced\nproportion of training examples per class, which occurs when new objects are\npresented to the system for the first time.\n  We evaluate our algorithm on both a machine learning benchmark dataset and\ntwo challenging object recognition tasks in a robotic setting. Empirical\nevidence shows that our approach achieves comparable or higher classification\nperformance than its batch counterpart when classes are unbalanced, while being\nsignificantly faster.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 07:50:58 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 20:50:38 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 16:53:19 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Camoriano", "Raffaello", ""], ["Pasquale", "Giulia", ""], ["Ciliberto", "Carlo", ""], ["Natale", "Lorenzo", ""], ["Rosasco", "Lorenzo", ""], ["Metta", "Giorgio", ""]]}, {"id": "1605.05054", "submitter": "Minseok Park", "authors": "Minseok Park, Hanxiang Li, Junmo Kim", "title": "HARRISON: A Benchmark on HAshtag Recommendation for Real-world Images in\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple, short, and compact hashtags cover a wide range of information on\nsocial networks. Although many works in the field of natural language\nprocessing (NLP) have demonstrated the importance of hashtag recommendation,\nhashtag recommendation for images has barely been studied. In this paper, we\nintroduce the HARRISON dataset, a benchmark on hashtag recommendation for real\nworld images in social networks. The HARRISON dataset is a realistic dataset,\ncomposed of 57,383 photos from Instagram and an average of 4.5 associated\nhashtags for each photo. To evaluate our dataset, we design a baseline\nframework consisting of visual feature extractor based on convolutional neural\nnetwork (CNN) and multi-label classifier based on neural network. Based on this\nframework, two single feature-based models, object-based and scene-based model,\nand an integrated model of them are evaluated on the HARRISON dataset. Our\ndataset shows that hashtag recommendation task requires a wide and contextual\nunderstanding of the situation conveyed in the image. As far as we know, this\nwork is the first vision-only attempt at hashtag recommendation for real world\nimages in social networks. We expect this benchmark to accelerate the\nadvancement of hashtag recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 08:21:07 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Park", "Minseok", ""], ["Li", "Hanxiang", ""], ["Kim", "Junmo", ""]]}, {"id": "1605.05106", "submitter": "Kaelon Lloyd", "authors": "Kaelon Lloyd, David Marshall, Simon C. Moore, Paul L. Rosin", "title": "Detecting Violent and Abnormal Crowd activity using Temporal Analysis of\n  Grey Level Co-occurrence Matrix (GLCM) Based Texture Measures", "comments": "Published under open access, 9 pages, 12 Figures", "journal-ref": "Machine Vision and Applications (2017)", "doi": "10.1007/s00138-017-0830-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The severity of sustained injury resulting from assault-related violence can\nbe minimised by reducing detection time. However, it has been shown that human\noperators perform poorly at detecting events found in video footage when\npresented with simultaneous feeds. We utilise computer vision techniques to\ndevelop an automated method of abnormal crowd detection that can aid a human\noperator in the detection of violent behaviour. We observed that behaviour in\ncity centre environments often occur in crowded areas, resulting in individual\nactions being occluded by other crowd members. We propose a real-time\ndescriptor that models crowd dynamics by encoding changes in crowd texture\nusing temporal summaries of Grey Level Co-Occurrence Matrix (GLCM) features. We\nintroduce a measure of inter-frame uniformity (IFU) and demonstrate that the\nappearance of violent behaviour changes in a less uniform manner when compared\nto other types of crowd behaviour. Our proposed method is computationally cheap\nand offers real-time description. Evaluating our method using a privately held\nCCTV dataset and the publicly available Violent Flows, UCF Web Abnormality, and\nUMN Abnormal Crowd datasets, we report a receiver operating characteristic\nscore of 0.9782, 0.9403, 0.8218 and 0.9956 respectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 10:53:07 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 10:39:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Lloyd", "Kaelon", ""], ["Marshall", "David", ""], ["Moore", "Simon C.", ""], ["Rosin", "Paul L.", ""]]}, {"id": "1605.05157", "submitter": "Li Yu", "authors": "Li Yu, Cyril Joly, Guillaume Bresson, Fabien Moutarde", "title": "Monocular Urban Localization using Street View", "comments": "6 pages, 6 figures, submitted to ICARCV2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a metric global localization in the urban environment\nonly with a monocular camera and the Google Street View database. We fully\nleverage the abundant sources from the Street View and benefits from its\ntopo-metric structure to build a coarse-to-fine positioning, namely a\ntopological place recognition process and then a metric pose estimation by\nlocal bundle adjustment. Our method is tested on a 3 km urban environment and\ndemonstrates both sub-meter accuracy and robustness to viewpoint changes,\nillumination and occlusion. To our knowledge, this is the first work that\nstudies the global urban localization simply with a single camera and Street\nView.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 13:33:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 11:47:33 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Yu", "Li", ""], ["Joly", "Cyril", ""], ["Bresson", "Guillaume", ""], ["Moutarde", "Fabien", ""]]}, {"id": "1605.05180", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Isinsu Katircioglu, Mathieu Salzmann, Vincent Lepetit,\n  Pascal Fua", "title": "Structured Prediction of 3D Human Pose with Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent approaches to monocular 3D pose estimation rely on Deep Learning.\nThey either train a Convolutional Neural Network to directly regress from image\nto 3D pose, which ignores the dependencies between human joints, or model these\ndependencies via a max-margin structured learning framework, which involves a\nhigh computational cost at inference time.\n  In this paper, we introduce a Deep Learning regression architecture for\nstructured prediction of 3D human pose from monocular images that relies on an\novercomplete auto-encoder to learn a high-dimensional latent pose\nrepresentation and account for joint dependencies. We demonstrate that our\napproach outperforms state-of-the-art ones both in terms of structure\npreservation and prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 14:26:14 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Tekin", "Bugra", ""], ["Katircioglu", "Isinsu", ""], ["Salzmann", "Mathieu", ""], ["Lepetit", "Vincent", ""], ["Fua", "Pascal", ""]]}, {"id": "1605.05197", "submitter": "Philippe Weinzaepfel", "authors": "Philippe Weinzaepfel, Xavier Martin, Cordelia Schmid", "title": "Human Action Localization with Sparse Spatial Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach for spatio-temporal human action localization using\nsparse spatial supervision. Our method leverages the large amount of annotated\nhumans available today and extracts human tubes by combining a state-of-the-art\nhuman detector with a tracking-by-detection approach. Given these high-quality\nhuman tubes and temporal supervision, we select positive and negative tubes\nwith very sparse spatial supervision, i.e., only one spatially annotated frame\nper instance. The selected tubes allow us to effectively learn a\nspatio-temporal action detector based on dense trajectories or CNNs. We conduct\nexperiments on existing action localization benchmarks: UCF-Sports, J-HMDB and\nUCF-101. Our results show that our approach, despite using sparse spatial\nsupervision, performs on par with methods using full supervision, i.e., one\nbounding box annotation per frame. To further validate our method, we introduce\nDALY (Daily Action Localization in YouTube), a dataset for realistic action\nlocalization in space and time. It contains high quality temporal and spatial\nannotations for 3.6k instances of 10 actions in 31 hours of videos (3.3M\nframes). It is an order of magnitude larger than existing datasets, with more\ndiversity in appearance and long untrimmed videos.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 14:55:03 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 19:19:23 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Weinzaepfel", "Philippe", ""], ["Martin", "Xavier", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1605.05212", "submitter": "Youngjune Gwon", "authors": "Youngjune Gwon and William Campbell and Kevin Brady and Douglas Sturim\n  and Miriam Cha and H.T. Kung", "title": "Multimodal Sparse Coding for Event Detection", "comments": "Multimodal Machine Learning Workshop at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised feature learning methods have proven effective for\nclassification tasks based on a single modality. We present multimodal sparse\ncoding for learning feature representations shared across multiple modalities.\nThe shared representations are applied to multimedia event detection (MED) and\nevaluated in comparison to unimodal counterparts, as well as other feature\nlearning methods such as GMM supervectors and sparse RBM. We report the\ncross-validated classification accuracy and mean average precision of the MED\nsystem trained on features learned from our unimodal and multimodal settings\nfor a subset of the TRECVID MED 2014 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 15:37:19 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Gwon", "Youngjune", ""], ["Campbell", "William", ""], ["Brady", "Kevin", ""], ["Sturim", "Douglas", ""], ["Cha", "Miriam", ""], ["Kung", "H. T.", ""]]}, {"id": "1605.05258", "submitter": "Anjith George", "authors": "Anjith George and Aurobinda Routray", "title": "Real-time Eye Gaze Direction Classification Using Convolutional Neural\n  Network", "comments": "5 pages, To appear in IEEE International Conference on Signal\n  Processing and Communication, SPCOM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation eye gaze direction is useful in various human-computer interaction\ntasks. Knowledge of gaze direction can give valuable information regarding\nusers point of attention. Certain patterns of eye movements known as eye\naccessing cues are reported to be related to the cognitive processes in the\nhuman brain. We propose a real-time framework for the classification of eye\ngaze direction and estimation of eye accessing cues. In the first stage, the\nalgorithm detects faces using a modified version of the Viola-Jones algorithm.\nA rough eye region is obtained using geometric relations and facial landmarks.\nThe eye region obtained is used in the subsequent stage to classify the eye\ngaze direction. A convolutional neural network is employed in this work for the\nclassification of eye gaze direction. The proposed algorithm was tested on Eye\nChimera database and found to outperform state of the art methods. The\ncomputational complexity of the algorithm is very less in the testing phase.\nThe algorithm achieved an average frame rate of 24 fps in the desktop\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 17:33:18 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1605.05272", "submitter": "Anjith George", "authors": "Anjith George and Aurobinda Routray", "title": "Fast and Accurate Algorithm for Eye Localization for Gaze Tracking in\n  Low Resolution Images", "comments": "12 pages, 10 figures, IET Computer Vision, 2016", "journal-ref": null, "doi": "10.1049/iet-cvi.2015.0316", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris centre localization in low-resolution visible images is a challenging\nproblem in computer vision community due to noise, shadows, occlusions, pose\nvariations, eye blinks, etc. This paper proposes an efficient method for\ndetermining iris centre in low-resolution images in the visible spectrum. Even\nlow-cost consumer-grade webcams can be used for gaze tracking without any\nadditional hardware. A two-stage algorithm is proposed for iris centre\nlocalization. The proposed method uses geometrical characteristics of the eye.\nIn the first stage, a fast convolution based approach is used for obtaining the\ncoarse location of iris centre (IC). The IC location is further refined in the\nsecond stage using boundary tracing and ellipse fitting. The algorithm has been\nevaluated in public databases like BioID, Gi4E and is found to outperform the\nstate of the art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 18:10:43 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1605.05359", "submitter": "Aravind Srinivas", "authors": "Aravind Srinivas, Ramnandan Krishnamurthy, Peeyush Kumar and Balaraman\n  Ravindran", "title": "Option Discovery in Hierarchical Reinforcement Learning using\n  Spatio-Temporal Clustering", "comments": "Revised version of ICML 16 Abstraction in Reinforcement Learning\n  workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an automated skill acquisition framework in\nreinforcement learning which involves identifying a hierarchical description of\nthe given task in terms of abstract states and extended actions between\nabstract states. Identifying such structures present in the task provides ways\nto simplify and speed up reinforcement learning algorithms. These structures\nalso help to generalize such algorithms over multiple tasks without relearning\npolicies from scratch. We use ideas from dynamical systems to find metastable\nregions in the state space and associate them with abstract states. The\nspectral clustering algorithm PCCA+ is used to identify suitable abstractions\naligned to the underlying structure. Skills are defined in terms of the\nsequence of actions that lead to transitions between such abstract states. The\nconnectivity information from PCCA+ is used to generate these skills or\noptions. These skills are independent of the learning task and can be\nefficiently reused across a variety of tasks defined over the same model. This\napproach works well even without the exact model of the environment by using\nsample trajectories to construct an approximate estimate. We also present our\napproach to scaling the skill acquisition framework to complex tasks with large\nstate spaces for which we perform state aggregation using the representation\nlearned from an action conditional video prediction network and use the skill\nacquisition framework on the aggregated state space.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 20:44:19 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 19:14:20 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 22:18:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Srinivas", "Aravind", ""], ["Krishnamurthy", "Ramnandan", ""], ["Kumar", "Peeyush", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1605.05368", "submitter": "Kin Gwn Lore", "authors": "Kin Gwn Lore, Daniel Stoecklein, Michael Davies, Baskar\n  Ganapathysubramanian, Soumik Sarkar", "title": "Deep Action Sequence Learning for Causal Shape Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning became the method of choice in recent year for solving a wide\nvariety of predictive analytics tasks. For sequence prediction, recurrent\nneural networks (RNN) are often the go-to architecture for exploiting\nsequential information where the output is dependent on previous computation.\nHowever, the dependencies of the computation lie in the latent domain which may\nnot be suitable for certain applications involving the prediction of a\nstep-wise transformation sequence that is dependent on the previous computation\nonly in the visible domain. We propose that a hybrid architecture of\nconvolution neural networks (CNN) and stacked autoencoders (SAE) is sufficient\nto learn a sequence of actions that nonlinearly transforms an input shape or\ndistribution into a target shape or distribution with the same support. While\nsuch a framework can be useful in a variety of problems such as robotic path\nplanning, sequential decision-making in games, and identifying material\nprocessing pathways to achieve desired microstructures, the application of the\nframework is exemplified by the control of fluid deformations in a microfluidic\nchannel by deliberately placing a sequence of pillars. Learning of a multistep\ntopological transform has significant implications for rapid advances in\nmaterial science and biomedical applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 21:07:18 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 02:01:37 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 20:48:47 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Lore", "Kin Gwn", ""], ["Stoecklein", "Daniel", ""], ["Davies", "Michael", ""], ["Ganapathysubramanian", "Baskar", ""], ["Sarkar", "Soumik", ""]]}, {"id": "1605.05395", "submitter": "Scott Reed", "authors": "Scott Reed, Zeynep Akata, Bernt Schiele, Honglak Lee", "title": "Learning Deep Representations of Fine-grained Visual Descriptions", "comments": "CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for zero-shot visual recognition formulate learning\nas a joint embedding problem of images and side information. In these\nformulations the current best complement to visual features are attributes:\nmanually encoded vectors describing shared characteristics among categories.\nDespite good performance, attributes have limitations: (1) finer-grained\nrecognition requires commensurately more attributes, and (2) attributes do not\nprovide a natural language interface. We propose to overcome these limitations\nby training neural language models from scratch; i.e. without pre-training and\nonly consuming words and characters. Our proposed models train end-to-end to\nalign with the fine-grained and category-specific content of images. Natural\nlanguage provides a flexible and compact way of encoding only the salient\nvisual aspects for distinguishing categories. By training on raw text, our\nmodel can do inference on raw text as well, providing humans a familiar mode\nboth for annotation and retrieval. Our model achieves strong performance on\nzero-shot text-based image retrieval and significantly outperforms the\nattribute-based state-of-the-art for zero-shot classification on the Caltech\nUCSD Birds 200-2011 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 23:08:46 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Reed", "Scott", ""], ["Akata", "Zeynep", ""], ["Schiele", "Bernt", ""], ["Lee", "Honglak", ""]]}, {"id": "1605.05396", "submitter": "Scott Reed", "authors": "Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt\n  Schiele, Honglak Lee", "title": "Generative Adversarial Text to Image Synthesis", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic synthesis of realistic images from text would be interesting and\nuseful, but current AI systems are still far from this goal. However, in recent\nyears generic and powerful recurrent neural network architectures have been\ndeveloped to learn discriminative text feature representations. Meanwhile, deep\nconvolutional generative adversarial networks (GANs) have begun to generate\nhighly compelling images of specific categories, such as faces, album covers,\nand room interiors. In this work, we develop a novel deep architecture and GAN\nformulation to effectively bridge these advances in text and image model- ing,\ntranslating visual concepts from characters to pixels. We demonstrate the\ncapability of our model to generate plausible images of birds and flowers from\ndetailed text descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 23:09:15 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 13:39:27 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Reed", "Scott", ""], ["Akata", "Zeynep", ""], ["Yan", "Xinchen", ""], ["Logeswaran", "Lajanugen", ""], ["Schiele", "Bernt", ""], ["Lee", "Honglak", ""]]}, {"id": "1605.05411", "submitter": "Ethan Rudd", "authors": "Andras Rozsa, Manuel G\\\"unther, Ethan M. Rudd, and Terrance E. Boult", "title": "Are Facial Attributes Adversarially Robust?", "comments": "Pre-print of article accepted to the International Conference on\n  Pattern Recognition (ICPR) 2016. 7 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes are emerging soft biometrics that have the potential to\nreject non-matches, for example, based on mismatching gender. To be usable in\nstand-alone systems, facial attributes must be extracted from images\nautomatically and reliably. In this paper, we propose a simple yet effective\nsolution for automatic facial attribute extraction by training a deep\nconvolutional neural network (DCNN) for each facial attribute separately,\nwithout using any pre-training or dataset augmentation, and we obtain new\nstate-of-the-art facial attribute classification results on the CelebA\nbenchmark. To test the stability of the networks, we generated adversarial\nimages -- formed by adding imperceptible non-random perturbations to original\ninputs which result in classification errors -- via a novel fast flipping\nattribute (FFA) technique. We show that FFA generates more adversarial examples\nthan other related algorithms, and that DCNNs for certain attributes are\ngenerally robust to adversarial inputs, while DCNNs for other attributes are\nnot. This result is surprising because no DCNNs tested to date have exhibited\nrobustness to adversarial images without explicit augmentation in the training\nprocedure to account for adversarial examples. Finally, we introduce the\nconcept of natural adversarial samples, i.e., images that are misclassified but\ncan be easily turned into correctly classified images by applying small\nperturbations. We demonstrate that natural adversarial samples commonly occur,\neven within the training set, and show that many of these images remain\nmisclassified even with additional training epochs. This phenomenon is\nsurprising because correcting the misclassification, particularly when guided\nby training data, should require only a small adjustment to the DCNN\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 01:13:09 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 18:44:50 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 21:49:14 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Rudd", "Ethan M.", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1605.05415", "submitter": "Ke Yang", "authors": "Ke Yang, Yong Dou, Shaohe Lv, Fei Zhang, Qi Lv", "title": "Relative distance features for gait recognition with Kinect", "comments": "28 pages,10 figures, under 2nd round review of JVIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait and static body measurement are important biometric technologies for\npassive human recognition. Many previous works argue that recognition\nperformance based completely on the gait feature is limited. The reason for\nthis limited performance remains unclear. This study focuses on human\nrecognition with gait feature obtained by Kinect and shows that gait feature\ncan effectively distinguish from different human beings through a novel\nrepresentation -- relative distance-based gait features. Experimental results\nshow that the recognition accuracy with relative distance features reaches up\nto 85%, which is comparable with that of anthropometric features. The\ncombination of relative distance features and anthropometric features can\nprovide an accuracy of more than 95%. Results indicate that the relative\ndistance feature is quite effective and worthy of further study in more general\nscenarios (e.g., without Kinect).\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 01:38:37 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Yang", "Ke", ""], ["Dou", "Yong", ""], ["Lv", "Shaohe", ""], ["Zhang", "Fei", ""], ["Lv", "Qi", ""]]}, {"id": "1605.05440", "submitter": "Andrew Shin", "authors": "Andrew Shin, Katsunori Ohnishi, Tatsuya Harada", "title": "Beyond Caption To Narrative: Video Captioning With Multiple Sentences", "comments": "accepted to ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in image captioning task have led to increasing interests in\nvideo captioning task. However, most works on video captioning are focused on\ngenerating single input of aggregated features, which hardly deviates from\nimage captioning process and does not fully take advantage of dynamic contents\npresent in videos. We attempt to generate video captions that convey richer\ncontents by temporally segmenting the video with action localization,\ngenerating multiple captions from multiple frames, and connecting them with\nnatural language processing techniques, in order to generate a story-like\ncaption. We show that our proposed method can generate captions that are richer\nin contents and can compete with state-of-the-art method without explicitly\nusing video-level features as input.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 05:00:12 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Shin", "Andrew", ""], ["Ohnishi", "Katsunori", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1605.05462", "submitter": "Marius Leordeanu", "authors": "Alina Marcu and Marius Leordeanu", "title": "Dual Local-Global Contextual Pathways for Recognition in Aerial Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual context is important in object recognition and it is still an open\nproblem in computer vision. Along with the advent of deep convolutional neural\nnetworks (CNN), using contextual information with such systems starts to\nreceive attention in the literature. At the same time, aerial imagery is\ngaining momentum. While advances in deep learning make good progress in aerial\nimage analysis, this problem still poses many great challenges. Aerial images\nare often taken under poor lighting conditions and contain low resolution\nobjects, many times occluded by trees or taller buildings. In this domain, in\nparticular, visual context could be of great help, but there are still very few\npapers that consider context in aerial image understanding. Here we introduce\ncontext as a complementary way of recognizing objects. We propose a dual-stream\ndeep neural network model that processes information along two independent\npathways, one for local and another for global visual reasoning. The two are\nlater combined in the final layers of processing. Our model learns to combine\nlocal object appearance as well as information from the larger scene at the\nsame time and in a complementary way, such that together they form a powerful\nclassifier. We test our dual-stream network on the task of segmentation of\nbuildings and roads in aerial images and obtain state-of-the-art results on the\nMassachusetts Buildings Dataset. We also introduce two new datasets, for\nbuildings and road segmentation, respectively, and study the relative\nimportance of local appearance vs. the larger scene, as well as their\nperformance in combination. While our local-global model could also be useful\nin general recognition tasks, we clearly demonstrate the effectiveness of\nvisual context in conjunction with deep nets for aerial image understanding.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 07:37:22 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Marcu", "Alina", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1605.05466", "submitter": "Jeremiah Deng", "authors": "Xianbin Gu, Jeremiah D. Deng, Martin K. Purvis", "title": "Image segmentation with superpixel-based covariance descriptors in\n  low-rank representation", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of image segmentation using superpixels.\nWe propose two approaches to enhance the discriminative ability of the\nsuperpixel's covariance descriptors. In the first one, we employ the\nLog-Euclidean distance as the metric on the covariance manifolds, and then use\nthe RBF kernel to measure the similarities between covariance descriptors. The\nsecond method is focused on extracting the subspace structure of the set of\ncovariance descriptors by extending a low rank representation algorithm on to\nthe covariance manifolds. Experiments are carried out with the Berkly\nSegmentation Dataset, and compared with the state-of-the-art segmentation\nalgorithms, both methods are competitive.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 07:44:38 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Gu", "Xianbin", ""], ["Deng", "Jeremiah D.", ""], ["Purvis", "Martin K.", ""]]}, {"id": "1605.05538", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov and Christoph H. Lampert", "title": "Improving Weakly-Supervised Object Localization By Micro-Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised object localization methods tend to fail for object classes\nthat consistently co-occur with the same background elements, e.g. trains on\ntracks. We propose a method to overcome these failures by adding a very small\namount of model-specific additional annotation. The main idea is to cluster a\ndeep network's mid-level representations and assign object or distractor labels\nto each cluster. Experiments show substantially improved localization results\non the challenging ILSVC2014 dataset for bounding box detection and the PASCAL\nVOC2012 dataset for semantic segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:06:35 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1605.05543", "submitter": "Youdong Mao", "authors": "Yanan Zhu, Qi Ouyang, Youdong Mao", "title": "A deep convolutional neural network approach to single-particle\n  recognition in cryo-electron microscopy", "comments": "26 pages, 6 figures, 1 table", "journal-ref": "BMC Bioinformatics 18, 348 (2017)", "doi": "10.1186/s12859-017-1757-y", "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Single-particle cryo-electron microscopy (cryo-EM) has become a\npopular tool for structural determination of biological macromolecular\ncomplexes. High-resolution cryo-EM reconstruction often requires hundreds of\nthousands of single-particle images. Particle extraction from experimental\nmicrographs thus can be laborious and presents a major practical bottleneck in\ncryo-EM structural determination. Existing computational methods of particle\npicking often use low-resolution templates as inputs for particle matching,\nmaking it possible to cause reference-dependent bias. It is critical to develop\na highly efficient template-free method to automatically recognize particle\nimages from cryo-EM micrographs. Results: We developed a deep learning-based\nalgorithmic framework, DeepEM, for single-particle recognition from noisy\ncryo-EM micrographs, enabling automated particle picking, selection and\nverification in an integrated fashion. The kernel of DeepEM is built upon a\nconvolutional neural network (CNN) of eight layers, which can be recursively\ntrained to be highly \"knowledgeable\". Our approach exhibits improved\nperformance and high precision when tested on the standard KLH dataset.\nApplication of DeepEM to several challenging experimental cryo-EM datasets\ndemonstrates its capability in avoiding selection of un-wanted particles and\nnon-particles even when true particles contain fewer features. Conclusions: The\nDeepEM method derived from a deep CNN allows automated particle extraction from\nraw cryo-EM micrographs in the absence of templates, which demonstrated\nimproved performance, objectivity and accuracy. Application of this novel\napproach is expected to free the labor involved in single-particle\nverification, thus promoting the efficiency of cryo-EM data processing.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:15:10 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 07:22:21 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Zhu", "Yanan", ""], ["Ouyang", "Qi", ""], ["Mao", "Youdong", ""]]}, {"id": "1605.05579", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Nathanael Perraudin, Pierre Vandergheynst", "title": "Low-Rank Matrices on Graphs: Generalized Recovery & Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world datasets subsume a linear or non-linear low-rank structure in\na very low-dimensional space. Unfortunately, one often has very little or no\ninformation about the geometry of the space, resulting in a highly\nunder-determined recovery problem. Under certain circumstances,\nstate-of-the-art algorithms provide an exact recovery for linear low-rank\nstructures but at the expense of highly inscalable algorithms which use nuclear\nnorm. However, the case of non-linear structures remains unresolved. We revisit\nthe problem of low-rank recovery from a totally different perspective,\ninvolving graphs which encode pairwise similarity between the data samples and\nfeatures. Surprisingly, our analysis confirms that it is possible to recover\nmany approximate linear and non-linear low-rank structures with recovery\nguarantees with a set of highly scalable and efficient algorithms. We call such\ndata matrices as \\textit{Low-Rank matrices on graphs} and show that many real\nworld datasets satisfy this assumption approximately due to underlying\nstationarity. Our detailed theoretical and experimental analysis unveils the\npower of the simple, yet very novel recovery framework \\textit{Fast Robust PCA\non Graphs}\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 13:50:04 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 07:37:35 GMT"}, {"version": "v3", "created": "Wed, 25 May 2016 20:50:42 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Shahid", "Nauman", ""], ["Perraudin", "Nathanael", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1605.05652", "submitter": "Wei Zhu", "authors": "Wei Zhu, Zuoqiang Shi, Stanley Osher", "title": "Scalable low dimensional manifold model in the reconstruction of noisy\n  and incomplete hyperspectral images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scalable low dimensional manifold model for the reconstruction\nof noisy and incomplete hyperspectral images. The model is based on the\nobservation that the spatial-spectral blocks of a hyperspectral image typically\nlie close to a collection of low dimensional manifolds. To emphasize this, the\ndimension of the manifold is directly used as a regularizer in a variational\nfunctional, which is solved efficiently by alternating direction of\nminimization and weighted nonlocal Laplacian. Unlike general 3D images, the\nsame similarity matrix can be shared across all spectral bands for a\nhyperspectral image, therefore the resulting algorithm is much more scalable\nthan that for general 3D data. Numerical experiments on the reconstruction of\nhyperspectral images from sparse and noisy sampling demonstrate the superiority\nof our proposed algorithm in terms of both speed and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 16:47:53 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 19:16:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zhu", "Wei", ""], ["Shi", "Zuoqiang", ""], ["Osher", "Stanley", ""]]}, {"id": "1605.05757", "submitter": "Menglong Ye", "authors": "Menglong Ye, Edward Johns, Benjamin Walter, Alexander Meining,\n  Guang-Zhong Yang", "title": "Robust Image Descriptors for Real-Time Inter-Examination Retargeting in\n  Gastrointestinal Endoscopy", "comments": "This paper was presented in MICCAI 2016 conference, and a DOI was\n  linked to the publisher's version", "journal-ref": null, "doi": "10.1007/978-3-319-46720-7_52", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For early diagnosis of malignancies in the gastrointestinal tract,\nsurveillance endoscopy is increasingly used to monitor abnormal tissue changes\nin serial examinations of the same patient. Despite successes with optical\nbiopsy for in vivo and in situ tissue characterisation, biopsy retargeting for\nserial examinations is challenging because tissue may change in appearance\nbetween examinations. In this paper, we propose an inter-examination\nretargeting framework for optical biopsy, based on an image descriptor designed\nfor matching between endoscopic scenes over significant time intervals. Each\nscene is described by a hierarchy of regional intensity comparisons at various\nscales, offering tolerance to long-term change in tissue appearance whilst\nremaining discriminative. Binary coding is then used to compress the descriptor\nvia a novel random forests approach, providing fast comparisons in Hamming\nspace and real-time retargeting. Extensive validation conducted on 13 in vivo\ngastrointestinal videos, collected from six patients, show that our approach\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 21:07:35 GMT"}, {"version": "v2", "created": "Sun, 30 Oct 2016 12:19:12 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ye", "Menglong", ""], ["Johns", "Edward", ""], ["Walter", "Benjamin", ""], ["Meining", "Alexander", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1605.05791", "submitter": "Shoaib Ehsan", "authors": "Shoaib Ehsan, Adrian F. Clark, Ales Leonardis, Naveed ur Rehman and\n  Klaus D. McDonald-Maier", "title": "A Generic Framework for Assessing the Performance Bounds of Image\n  Feature Detectors", "comments": "Journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since local feature detection has been one of the most active research areas\nin computer vision during the last decade, a large number of detectors have\nbeen proposed. The interest in feature-based applications continues to grow and\nhas thus rendered the task of characterizing the performance of various feature\ndetection methods an important issue in vision research. Inspired by the good\npractices of electronic system design, a generic framework based on the\nrepeatability measure is presented in this paper that allows assessment of the\nupper and lower bounds of detector performance and finds statistically\nsignificant performance differences between detectors as a function of image\ntransformation amount by introducing a new variant of McNemars test in an\neffort to design more reliable and effective vision systems. The proposed\nframework is then employed to establish operating and guarantee regions for\nseveral state-of-the-art detectors and to identify their statistical\nperformance differences for three specific image transformations: JPEG\ncompression, uniform light changes and blurring. The results are obtained using\na newly acquired, large image database (20482) images with 539 different\nscenes. These results provide new insights into the behaviour of detectors and\nare also useful from the vision systems design perspective.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 01:02:22 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Ehsan", "Shoaib", ""], ["Clark", "Adrian F.", ""], ["Leonardis", "Ales", ""], ["Rehman", "Naveed ur", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1605.05815", "submitter": "Abdul Kayom Md Khairuzzaman", "authors": "Abdul kayom Md Khairuzzaman", "title": "Bacterial foraging optimization based brain magnetic resonance image\n  segmentation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Segmentation partitions an image into its constituent parts. It is\nessentially the pre-processing stage of image analysis and computer vision. In\nthis work, T1 and T2 weighted brain magnetic resonance images are segmented\nusing multilevel thresholding and bacterial foraging optimization (BFO)\nalgorithm. The thresholds are obtained by maximizing the between class variance\n(multilevel Otsu method) of the image. The BFO algorithm is used to optimize\nthe threshold searching process. The edges are then obtained from the\nthresholded image by comparing the intensity of each pixel with its eight\nconnected neighbourhood. Post processing is performed to remove spurious\nresponses in the segmented image. The proposed segmentation technique is\nevaluated using edge detector evaluation parameters such as figure of merit,\nRand Index and variation of information. The proposed brain MR image\nsegmentation technique outperforms the traditional edge detectors such as canny\nand sobel.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 05:39:13 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Khairuzzaman", "Abdul kayom Md", ""]]}, {"id": "1605.05829", "submitter": "Jie Liang", "authors": "Jie Liang, Jun Zhou, Yuntao Qian, Lian Wen, Xiao Bai, Yongsheng Gao", "title": "On the Sampling Strategy for Evaluation of Spectral-spatial Methods in\n  Hyperspectral Image Classification", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2016.2616489", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral-spatial processing has been increasingly explored in remote sensing\nhyperspectral image classification. While extensive studies have focused on\ndeveloping methods to improve the classification accuracy, experimental setting\nand design for method evaluation have drawn little attention. In the scope of\nsupervised classification, we find that traditional experimental designs for\nspectral processing are often improperly used in the spectral-spatial\nprocessing context, leading to unfair or biased performance evaluation. This is\nespecially the case when training and testing samples are randomly drawn from\nthe same image - a practice that has been commonly adopted in the experiments.\nUnder such setting, the dependence caused by overlap between the training and\ntesting samples may be artificially enhanced by some spatial information\nprocessing methods such as spatial filtering and morphological operation. Such\ninteraction between training and testing sets has violated data independence\nassumption that is abided by supervised learning theory and performance\nevaluation mechanism. Therefore, the widely adopted pixel-based random sampling\nstrategy is not always suitable to evaluate spectral-spatial classification\nalgorithms because it is difficult to determine whether the improvement of\nclassification accuracy is caused by incorporating spatial information into\nclassifier or by increasing the overlap between training and testing samples.\nTo partially solve this problem, we propose a novel controlled random sampling\nstrategy for spectral-spatial methods. It can greatly reduce the overlap\nbetween training and testing samples and provides more objective and accurate\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 06:59:03 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Liang", "Jie", ""], ["Zhou", "Jun", ""], ["Qian", "Yuntao", ""], ["Wen", "Lian", ""], ["Bai", "Xiao", ""], ["Gao", "Yongsheng", ""]]}, {"id": "1605.05863", "submitter": "Ran Tao", "authors": "Ran Tao, Efstratios Gavves, Arnold W.M. Smeulders", "title": "Siamese Instance Search for Tracking", "comments": "This paper is accepted to the IEEE Conference on Computer Vision and\n  Pattern Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a tracker, which is radically different from\nstate-of-the-art trackers: we apply no model updating, no occlusion detection,\nno combination of trackers, no geometric matching, and still deliver\nstate-of-the-art tracking performance, as demonstrated on the popular online\ntracking benchmark (OTB) and six very challenging YouTube videos. The presented\ntracker simply matches the initial patch of the target in the first frame with\ncandidates in a new frame and returns the most similar patch by a learned\nmatching function. The strength of the matching function comes from being\nextensively trained generically, i.e., without any data of the target, using a\nSiamese deep neural network, which we design for tracking. Once learned, the\nmatching function is used as is, without any adapting, to track previously\nunseen targets. It turns out that the learned matching function is so powerful\nthat a simple tracker built upon it, coined Siamese INstance search Tracker,\nSINT, which only uses the original observation of the target from the first\nframe, suffices to reach state-of-the-art performance. Further, we show the\nproposed tracker even allows for target re-identification after the target was\nabsent for a complete video shot.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 09:24:40 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Tao", "Ran", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "1605.05904", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Mingyi Lei, Shaozi Li, Jianping Fan", "title": "Re-ranking Object Proposals for Object Detection in Automatic Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection often suffers from a plenty of bootless proposals, selecting\nhigh quality proposals remains a great challenge. In this paper, we propose a\nsemantic, class-specific approach to re-rank object proposals, which can\nconsistently improve the recall performance even with less proposals. We first\nextract features for each proposal including semantic segmentation, stereo\ninformation, contextual information, CNN-based objectness and low-level cue,\nand then score them using class-specific weights learnt by Structured SVM. The\nadvantages of the proposed model are twofold: 1) it can be easily merged to\nexisting generators with few computational costs, and 2) it can achieve high\nrecall rate uner strict critical even using less proposals. Experimental\nevaluation on the KITTI benchmark demonstrates that our approach significantly\nimproves existing popular generators on recall performance. Moreover, in the\nexperiment conducted for object detection, even with 1,500 proposals, our\napproach can still have higher average precision (AP) than baselines with 5,000\nproposals.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 11:56:55 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 01:45:43 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Zhong", "Zhun", ""], ["Lei", "Mingyi", ""], ["Li", "Shaozi", ""], ["Fan", "Jianping", ""]]}, {"id": "1605.05912", "submitter": "Kele Xu", "authors": "Aurore Jaumard-Hakoun, Kele Xu, Pierre Roussel-Ragot, G\\'erard\n  Dreyfus, Bruce Denby", "title": "Tongue contour extraction from ultrasound images based on deep neural\n  network", "comments": "5 pages, 3 figures, published in The International Congress of\n  Phonetic Sciences, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying tongue motion during speech using ultrasound is a standard\nprocedure, but automatic ultrasound image labelling remains a challenge, as\nstandard tongue shape extraction methods typically require human intervention.\nThis article presents a method based on deep neural networks to automatically\nextract tongue contour from ultrasound images on a speech dataset. We use a\ndeep autoencoder trained to learn the relationship between an image and its\nrelated contour, so that the model is able to automatically reconstruct\ncontours from the ultrasound image alone. In this paper, we use an automatic\nlabelling algorithm instead of time-consuming hand-labelling during the\ntraining process, and estimate the performances of both automatic labelling and\ncontour extraction as compared to hand-labelling. Observed results show quality\nscores comparable to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:20:40 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Jaumard-Hakoun", "Aurore", ""], ["Xu", "Kele", ""], ["Roussel-Ragot", "Pierre", ""], ["Dreyfus", "G\u00e9rard", ""], ["Denby", "Bruce", ""]]}, {"id": "1605.05923", "submitter": "Praveen Krishnan", "authors": "Praveen Krishnan and C.V. Jawahar", "title": "Matching Handwritten Document Images", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting similarity between a pair of handwritten\ndocument images written by different individuals. This has applications related\nto matching and mining in image collections containing handwritten content. A\nsimilarity score is computed by detecting patterns of text re-usages between\ndocument images irrespective of the minor variations in word morphology, word\nordering, layout and paraphrasing of the content. Our method does not depend on\nan accurate segmentation of words and lines. We formulate the document matching\nproblem as a structured comparison of the word distributions across two\ndocument images. To match two word images, we propose a convolutional neural\nnetwork (CNN) based feature descriptor. Performance of this representation\nsurpasses the state-of-the-art on handwritten word spotting. Finally, we\ndemonstrate the applicability of our method on a practical problem of matching\nhandwritten assignments.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 12:50:10 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Krishnan", "Praveen", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1605.05937", "submitter": "Imanol Luengo", "authors": "Imanol Luengo, Mark Basham and Andrew P. French", "title": "Hierarchical Piecewise-Constant Super-regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications in computer vision have come to heavily rely on\nsuperpixel over-segmentation as a pre-processing step for higher level vision\ntasks, such as object recognition, image labelling or image segmentation. Here\nwe present a new superpixel algorithm called Hierarchical Piecewise-Constant\nSuper-regions (HPCS), which not only obtains superpixels comparable to the\nstate-of-the-art, but can also be applied hierarchically to form what we call\nn-th order super-regions. In essence, a Markov Random Field (MRF)-based\nanisotropic denoising formulation over the quantized feature space is adopted\nto form piecewise-constant image regions, which are then combined with a\ngraph-based split & merge post-processing step to form superpixels. The graph\nand quantized feature based formulation of the problem allows us to generalize\nit hierarchically to preserve boundary adherence with fewer superpixels.\nExperimental results show that, despite the simplicity of our framework, it is\nable to provide high quality superpixels, and to hierarchically apply them to\nform layers of over-segmentation, each with a decreasing number of superpixels,\nwhile maintaining the same desired properties (such as adherence to strong\nimage edges). The algorithm is also memory efficient and has a low\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 13:14:13 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Luengo", "Imanol", ""], ["Basham", "Mark", ""], ["French", "Andrew P.", ""]]}, {"id": "1605.05967", "submitter": "Kele Xu", "authors": "Kele Xu (SIGMA Laboratory, ESPCI ParisTech), Yin Yang, Cl\\'emence\n  Leboullenger (SIGMA Laboratory, ESPCI ParisTech), Pierre Roussel (SIGMA\n  Laboratory, ESPCI ParisTech), Bruce Denby (UPMC)", "title": "Contour-based 3d tongue motion visualization using ultrasound image\n  sequences", "comments": "ICASSP 2016, Mar 2016, SHANGHAI, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a contour-based 3D tongue deformation visualization\nframework using B-mode ultrasound image sequences. A robust, automatic tracking\nalgorithm characterizes tongue motion via a contour, which is then used to\ndrive a generic 3D Finite Element Model (FEM). A novel contour-based 3D dynamic\nmodeling method is presented. Modal reduction and modal warping techniques are\napplied to model the deformation of the tongue physically and efficiently. This\nwork can be helpful in a variety of fields, such as speech production, silent\nspeech recognition, articulation training, speech disorder study, etc.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 14:17:46 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Xu", "Kele", "", "SIGMA Laboratory, ESPCI ParisTech"], ["Yang", "Yin", "", "SIGMA Laboratory, ESPCI ParisTech"], ["Leboullenger", "Cl\u00e9mence", "", "SIGMA Laboratory, ESPCI ParisTech"], ["Roussel", "Pierre", "", "SIGMA\n  Laboratory, ESPCI ParisTech"], ["Denby", "Bruce", "", "UPMC"]]}, {"id": "1605.05977", "submitter": "Freddie {\\AA}str\\\"om", "authors": "Freddie {\\AA}str\\\"om and Christoph Schn\\\"orr", "title": "A Geometric Approach to Color Image Regularization", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new vectorial total variation method that addresses the problem\nof color consistent image filtering. Our approach is inspired from the\ndouble-opponent cell representation in the human visual cortex. Existing\nmethods of vectorial total variation regularizers have insufficient (or no)\ncoupling between the color channels and thus may introduce color artifacts. We\naddress this problem by introducing a novel coupling between the color channels\nrelated to a pullback-metric from the opponent space to the data (RGB color)\nspace. Our energy is a non-convex, non-smooth higher-order vectorial total\nvariation approach and promotes color consistent image filtering via a coupling\nterm. For a convex variant, we show well-posedness and existence of a solution\nin the space of vectorial bounded variation. For the higher-order scheme we\nemploy a half-quadratic strategy, which model the non-convex energy terms as\nthe infimum of a sequence of quadratic functions. In experiments, we elaborate\non traditional image restoration applications of inpainting, deblurring and\ndenoising. Regarding the latter, we demonstrate state of the art restoration\nquality with respect to structure coherence and color consistency.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 14:38:59 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["\u00c5str\u00f6m", "Freddie", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1605.06052", "submitter": "Jason Grant", "authors": "Jason Grant and Patrick Flynn", "title": "Hierarchical Clustering in Face Similarity Score Space", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity scores in face recognition represent the proximity between pairs\nof images as computed by a matching algorithm. Given a large set of images and\nthe proximities between all pairs, a similarity score space is defined. Cluster\nanalysis was applied to the similarity score space to develop various\ntaxonomies. Given the number of subjects in the dataset, we used hierarchical\nmethods to aggregate images of the same subject. We also explored the hierarchy\nabove and below the subject level, including clusters that reflect gender and\nethnicity. Evidence supports the existence of clustering by race, gender,\nsubject, and illumination condition.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 17:08:16 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Grant", "Jason", ""], ["Flynn", "Patrick", ""]]}, {"id": "1605.06083", "submitter": "Emiel van Miltenburg", "authors": "Emiel van Miltenburg", "title": "Stereotyping and Bias in the Flickr30K Dataset", "comments": "In: Proceedings of the Workshop on Multimodal Corpora (MMC-2016),\n  pages 1-4. Editors: Jens Edlund, Dirk Heylen and Patrizia Paggio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An untested assumption behind the crowdsourced descriptions of the images in\nthe Flickr30K dataset (Young et al., 2014) is that they \"focus only on the\ninformation that can be obtained from the image alone\" (Hodosh et al., 2013, p.\n859). This paper presents some evidence against this assumption, and provides a\nlist of biases and unwarranted inferences that can be found in the Flickr30K\ndataset. Finally, it considers methods to find examples of these, and discusses\nhow we should deal with stereotype-driven descriptions in future applications.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:17:23 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["van Miltenburg", "Emiel", ""]]}, {"id": "1605.06094", "submitter": "Bruno Ferrarini", "authors": "Bruno Ferrarini, Shoaib Ehsan, Naveed Ur Rehman, Ales Leonardis, Klaus\n  D. McDonald-Maier", "title": "Automatic Selection of the Optimal Local Feature Detector", "comments": "pre-print version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of different feature detectors has been proposed so far. Any\nexisting approach presents strengths and weaknesses, which make a detector\noptimal only for a limited range of applications. A tool capable of selecting\nthe optimal feature detector in relation to the operating conditions is\npresented in this paper. The input images are quickly analyzed to determine\nwhat type of image transformation is applied to them and at which amount.\nFinally, the detector that is expected to obtain the highest repeatability\nunder such conditions, is chosen to extract features from the input images. The\nefficiency and the good accuracy in determining the optimal feature detector\nfor any operating condition, make the proposed tool suitable to be utilized in\nreal visual applications. %A large number of different feature detectors has\nbeen proposed so far. Any existing approach presents strengths and weaknesses,\nwhich make a detector optimal only for a limited range of applications. A large\nnumber of different local feature detectors have been proposed in the last few\nyears. However, each feature detector has its own strengths ad weaknesses that\nlimit its use to a specific range of applications. In this paper is presented a\ntool capable of quickly analysing input images to determine which type and\namount of transformation is applied to them and then selecting the optimal\nfeature detector, which is expected to perform the best. The results show that\nthe performance and the fast execution time render the proposed tool suitable\nfor real-world vision applications.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:42:42 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Ferrarini", "Bruno", ""], ["Ehsan", "Shoaib", ""], ["Rehman", "Naveed Ur", ""], ["Leonardis", "Ales", ""], ["McDonald-Maier", "Klaus D.", ""]]}, {"id": "1605.06106", "submitter": "Kele Xu", "authors": "Kele Xu, Yin Yang, Aurore Jaumard-Hakoun, Clemence Leboullenger,\n  Gerard Dreyfus, Pierre Roussel, Maureen Stone, Bruce Denby", "title": "Development of a 3D tongue motion visualization platform based on\n  ultrasound image sequences", "comments": "5 Pages, 5 figures, published in 18th International Congress of\n  Phonetic Sciences, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the development of a platform designed to visualize\nthe 3D motion of the tongue using ultrasound image sequences. An overview of\nthe system design is given and promising results are presented. Compared to the\nanalysis of motion in 2D image sequences, such a system can provide additional\nvisual information and a quantitative description of the tongue 3D motion. The\nplatform can be useful in a variety of fields, such as speech production,\narticulation training, etc.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:56:38 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Xu", "Kele", ""], ["Yang", "Yin", ""], ["Jaumard-Hakoun", "Aurore", ""], ["Leboullenger", "Clemence", ""], ["Dreyfus", "Gerard", ""], ["Roussel", "Pierre", ""], ["Stone", "Maureen", ""], ["Denby", "Bruce", ""]]}, {"id": "1605.06155", "submitter": "Cheng Zhang", "authors": "Cheng Zhang and Hedvig Kjellstrom and Carl Henrik Ek", "title": "Inter-Battery Topic Representation Learning", "comments": "ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Inter-Battery Topic Model (IBTM). Our approach\nextends traditional topic models by learning a factorized latent variable\nrepresentation. The structured representation leads to a model that marries\nbenefits traditionally associated with a discriminative approach, such as\nfeature selection, with those of a generative model, such as principled\nregularization and ability to handle missing data. The factorization is\nprovided by representing data in terms of aligned pairs of observations as\ndifferent views. This provides means for selecting a representation that\nseparately models topics that exist in both views from the topics that are\nunique to a single view. This structured consolidation allows for efficient and\nrobust inference and provides a compact and efficient representation. Learning\nis performed in a Bayesian fashion by maximizing a rigorous bound on the\nlog-likelihood. Firstly, we illustrate the benefits of the model on a synthetic\ndataset,. The model is then evaluated in both uni- and multi-modality settings\non two different classification tasks with off-the-shelf convolutional neural\nnetwork (CNN) features which generate state-of-the-art results with extremely\ncompact representations.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 21:44:12 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 10:08:40 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Zhang", "Cheng", ""], ["Kjellstrom", "Hedvig", ""], ["Ek", "Carl Henrik", ""]]}, {"id": "1605.06177", "submitter": "David Hall", "authors": "David Hall and Pietro Perona", "title": "Fine-Grained Classification of Pedestrians in Video: Benchmark and State\n  of the Art", "comments": "CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video dataset that is designed to study fine-grained categorisation of\npedestrians is introduced. Pedestrians were recorded \"in-the-wild\" from a\nmoving vehicle. Annotations include bounding boxes, tracks, 14 keypoints with\nocclusion information and the fine-grained categories of age (5 classes), sex\n(2 classes), weight (3 classes) and clothing style (4 classes). There are a\ntotal of 27,454 bounding box and pose labels across 4222 tracks. This dataset\nis designed to train and test algorithms for fine-grained categorisation of\npeople, it is also useful for benchmarking tracking, detection and pose\nestimation of pedestrians. State-of-the-art algorithms for fine-grained\nclassification and pose estimation were tested using the dataset and the\nresults are reported as a useful performance baseline.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 00:03:42 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Hall", "David", ""], ["Perona", "Pietro", ""]]}, {"id": "1605.06182", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi, Mathieu Salzmann, Richard Hartley", "title": "Dimensionality Reduction on SPD Manifolds: The Emergence of\n  Geometry-Aware Methods", "comments": "arXiv admin note: text overlap with arXiv:1407.1120", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing images and videos with Symmetric Positive Definite (SPD)\nmatrices, and considering the Riemannian geometry of the resulting space, has\nbeen shown to yield high discriminative power in many visual recognition tasks.\nUnfortunately, computation on the Riemannian manifold of SPD matrices\n-especially of high-dimensional ones- comes at a high cost that limits the\napplicability of existing techniques. In this paper, we introduce algorithms\nable to handle high-dimensional SPD matrices by constructing a\nlower-dimensional SPD manifold. To this end, we propose to model the mapping\nfrom the high-dimensional SPD manifold to the low-dimensional one with an\northonormal projection. This lets us formulate dimensionality reduction as the\nproblem of finding a projection that yields a low-dimensional manifold either\nwith maximum discriminative power in the supervised scenario, or with maximum\nvariance of the data in the unsupervised one. We show that learning can be\nexpressed as an optimization problem on a Grassmann manifold and discuss fast\nsolutions for special cases. Our evaluation on several classification tasks\nevidences that our approach leads to a significant accuracy gain over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 00:33:48 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Salzmann", "Mathieu", ""], ["Hartley", "Richard", ""]]}, {"id": "1605.06211", "submitter": "Evan Shelhamer", "authors": "Evan Shelhamer, Jonathan Long, Trevor Darrell", "title": "Fully Convolutional Networks for Semantic Segmentation", "comments": "to appear in PAMI (accepted May, 2016); journal edition of\n  arXiv:1411.4038", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional networks are powerful visual models that yield hierarchies of\nfeatures. We show that convolutional networks by themselves, trained\nend-to-end, pixels-to-pixels, improve on the previous best result in semantic\nsegmentation. Our key insight is to build \"fully convolutional\" networks that\ntake input of arbitrary size and produce correspondingly-sized output with\nefficient inference and learning. We define and detail the space of fully\nconvolutional networks, explain their application to spatially dense prediction\ntasks, and draw connections to prior models. We adapt contemporary\nclassification networks (AlexNet, the VGG net, and GoogLeNet) into fully\nconvolutional networks and transfer their learned representations by\nfine-tuning to the segmentation task. We then define a skip architecture that\ncombines semantic information from a deep, coarse layer with appearance\ninformation from a shallow, fine layer to produce accurate and detailed\nsegmentations. Our fully convolutional network achieves improved segmentation\nof PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT\nFlow, and PASCAL-Context, while inference takes one tenth of a second for a\ntypical image.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 04:30:16 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Shelhamer", "Evan", ""], ["Long", "Jonathan", ""], ["Darrell", "Trevor", ""]]}, {"id": "1605.06215", "submitter": "Gary Pui-Tung Choi", "authors": "Chun Pang Yung, Gary P. T. Choi, Ke Chen, Lok Ming Lui", "title": "Efficient Feature-based Image Registration by Mapping Sparsified\n  Surfaces", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation 55,\n  561-571 (2018)", "doi": "10.1016/j.jvcir.2018.07.005", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, conventional registration\napproaches usually require long computational time for high resolution images\nand video frames. This hinders the application of the registration approaches\nin the modern industries. In this work, we first propose a new image\nrepresentation method to accelerate the registration process by triangulating\nthe images effectively. For each high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, we apply a surface registration algorithm to obtain a\nregistration map which is used to compute the registration of the high\nresolution image. Experimental results suggest that our overall algorithm is\nefficient and capable to achieve a high compression rate while the accuracy of\nthe registration is well retained when compared with the conventional\ngrid-based approach. Also, the computational time of the registration is\nsignificantly reduced using our triangulation-based approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 05:42:12 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 03:32:25 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yung", "Chun Pang", ""], ["Choi", "Gary P. T.", ""], ["Chen", "Ke", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1605.06217", "submitter": "Xiao Liu", "authors": "Xiao Liu, Jiang Wang, Shilei Wen, Errui Ding, Yuanqing Lin", "title": "Localizing by Describing: Attribute-Guided Attention Localization for\n  Fine-Grained Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in fine-grained recognition is how to find and represent\ndiscriminative local regions. Recent attention models are capable of learning\ndiscriminative region localizers only from category labels with reinforcement\nlearning. However, not utilizing any explicit part information, they are not\nable to accurately find multiple distinctive regions. In this work, we\nintroduce an attribute-guided attention localization scheme where the local\nregion localizers are learned under the guidance of part attribute\ndescriptions. By designing a novel reward strategy, we are able to learn to\nlocate regions that are spatially and semantically distinctive with\nreinforcement learning algorithm. The attribute labeling requirement of the\nscheme is more amenable than the accurate part location annotation required by\ntraditional part-based fine-grained recognition methods. Experimental results\non the CUB-200-2011 dataset demonstrate the superiority of the proposed scheme\non both fine-grained recognition and attribute recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 05:54:54 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 03:37:54 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Liu", "Xiao", ""], ["Wang", "Jiang", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1605.06240", "submitter": "Yangyan Li", "authors": "Yangyan Li and Soeren Pirk and Hao Su and Charles R. Qi and Leonidas\n  J. Guibas", "title": "FPNN: Field Probing Neural Networks for 3D Data", "comments": "To appear in NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Building discriminative representations for 3D data has been an important\ntask in computer graphics and computer vision research. Convolutional Neural\nNetworks (CNNs) have shown to operate on 2D images with great success for a\nvariety of tasks. Lifting convolution operators to 3D (3DCNNs) seems like a\nplausible and promising next step. Unfortunately, the computational complexity\nof 3D CNNs grows cubically with respect to voxel resolution. Moreover, since\nmost 3D geometry representations are boundary based, occupied regions do not\nincrease proportionately with the size of the discretization, resulting in\nwasted computation. In this work, we represent 3D spaces as volumetric fields,\nand propose a novel design that employs field probing filters to efficiently\nextract features from them. Each field probing filter is a set of probing\npoints --- sensors that perceive the space. Our learning algorithm optimizes\nnot only the weights associated with the probing points, but also their\nlocations, which deforms the shape of the probing filters and adaptively\ndistributes them in 3D space. The optimized probing points sense the 3D space\n\"intelligently\", rather than operating blindly over the entire domain. We show\nthat field probing is significantly more efficient than 3DCNNs, while providing\nstate-of-the-art performance, on classification tasks for 3D object recognition\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 08:15:57 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 07:34:49 GMT"}, {"version": "v3", "created": "Tue, 25 Oct 2016 03:59:16 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Li", "Yangyan", ""], ["Pirk", "Soeren", ""], ["Su", "Hao", ""], ["Qi", "Charles R.", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "1605.06265", "submitter": "Julien Mairal", "authors": "Julien Mairal", "title": "End-to-End Kernel Learning with Supervised Convolutional Kernel Networks", "comments": "to appear in Advances in Neural Information Processing Systems (NIPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new image representation based on a multilayer\nkernel machine. Unlike traditional kernel methods where data representation is\ndecoupled from the prediction task, we learn how to shape the kernel with\nsupervision. We proceed by first proposing improvements of the\nrecently-introduced convolutional kernel networks (CKNs) in the context of\nunsupervised learning; then, we derive backpropagation rules to take advantage\nof labeled training data. The resulting model is a new type of convolutional\nneural network, where optimizing the filters at each layer is equivalent to\nlearning a linear subspace in a reproducing kernel Hilbert space (RKHS). We\nshow that our method achieves reasonably competitive performance for image\nclassification on some standard \"deep learning\" datasets such as CIFAR-10 and\nSVHN, and also for image super-resolution, demonstrating the applicability of\nour approach to a large variety of image-related tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 09:52:14 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 12:52:50 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Mairal", "Julien", ""]]}, {"id": "1605.06311", "submitter": "Karl Granstr\\\"om", "authors": "Karl Granstrom, Maryam Fatemi, Lennart Svensson", "title": "Poisson multi-Bernoulli conjugate prior for multiple extended object\n  filtering", "comments": null, "journal-ref": null, "doi": "10.1109/TAES.2019.2920220", "report-no": null, "categories": "stat.CO cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Poisson multi-Bernoulli mixture (PMBM) conjugate prior\nfor multiple extended object filtering. A Poisson point process is used to\ndescribe the existence of yet undetected targets, while a multi-Bernoulli\nmixture describes the distribution of the targets that have been detected. The\nprediction and update equations are presented for the standard transition\ndensity and measurement likelihood. Both the prediction and the update preserve\nthe PMBM form of the density, and in this sense the PMBM density is a conjugate\nprior. However, the unknown data associations lead to an intractably large\nnumber of terms in the PMBM density, and approximations are necessary for\ntractability. A gamma Gaussian inverse Wishart implementation is presented,\nalong with methods to handle the data association problem. A simulation study\nshows that the extended target PMBM filter performs well in comparison to the\nextended target d-GLMB and LMB filters. An experiment with Lidar data\nillustrates the benefit of tracking both detected and undetected targets.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 12:05:59 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 10:00:45 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 08:35:10 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 18:58:25 GMT"}, {"version": "v5", "created": "Mon, 8 Apr 2019 08:14:17 GMT"}, {"version": "v6", "created": "Fri, 6 Dec 2019 09:12:45 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Granstrom", "Karl", ""], ["Fatemi", "Maryam", ""], ["Svensson", "Lennart", ""]]}, {"id": "1605.06325", "submitter": "Xing Wei", "authors": "Xing Wei, Qingxiong Yang, Yihong Gong, Ming-Hsuan Yang, Narendra Ahuja", "title": "Superpixel Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixel segmentation is becoming ubiquitous in computer vision. In\npractice, an object can either be represented by a number of segments in finer\nlevels of detail or included in a surrounding region at coarser levels of\ndetail, and thus a superpixel segmentation hierarchy is useful for applications\nthat require different levels of image segmentation detail depending on the\nparticular image objects segmented. Unfortunately, there is no method that can\ngenerate all scales of superpixels accurately in real-time. As a result, a\nsimple yet effective algorithm named Super Hierarchy (SH) is proposed in this\npaper. It is as accurate as the state-of-the-art but 1-2 orders of magnitude\nfaster. The proposed method can be directly integrated with recent efficient\nedge detectors like the structured forest edges to significantly outperforms\nthe state-of-the-art in terms of segmentation accuracy. Quantitative and\nqualitative evaluation on a number of computer vision applications was\nconducted, demonstrating that the proposed method is the top performer.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 12:38:24 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Wei", "Xing", ""], ["Yang", "Qingxiong", ""], ["Gong", "Yihong", ""], ["Yang", "Ming-Hsuan", ""], ["Ahuja", "Narendra", ""]]}, {"id": "1605.06402", "submitter": "Philipp Gysel", "authors": "Philipp Gysel", "title": "Ristretto: Hardware-Oriented Approximation of Convolutional Neural\n  Networks", "comments": "Master's Thesis, University of California, Davis; 73 pages and 29\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNN) have achieved major breakthroughs in\nrecent years. Their performance in computer vision have matched and in some\nareas even surpassed human capabilities. Deep neural networks can capture\ncomplex non-linear features; however this ability comes at the cost of high\ncomputational and memory requirements. State-of-art networks require billions\nof arithmetic operations and millions of parameters. To enable embedded devices\nsuch as smartphones, Google glasses and monitoring cameras with the astonishing\npower of deep learning, dedicated hardware accelerators can be used to decrease\nboth execution time and power consumption. In applications where fast\nconnection to the cloud is not guaranteed or where privacy is important,\ncomputation needs to be done locally. Many hardware accelerators for deep\nneural networks have been proposed recently. A first important step of\naccelerator design is hardware-oriented approximation of deep networks, which\nenables energy-efficient inference. We present Ristretto, a fast and automated\nframework for CNN approximation. Ristretto simulates the hardware arithmetic of\na custom hardware accelerator. The framework reduces the bit-width of network\nparameters and outputs of resource-intense layers, which reduces the chip area\nfor multiplication units significantly. Alternatively, Ristretto can remove the\nneed for multipliers altogether, resulting in an adder-only arithmetic. The\ntool fine-tunes trimmed networks to achieve high classification accuracy. Since\ntraining of deep neural networks can be time-consuming, Ristretto uses highly\noptimized routines which run on the GPU. This enables fast compression of any\ngiven network. Given a maximum tolerance of 1%, Ristretto can successfully\ncondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 15:22:29 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Gysel", "Philipp", ""]]}, {"id": "1605.06409", "submitter": "Kaiming He", "authors": "Jifeng Dai, Yi Li, Kaiming He, Jian Sun", "title": "R-FCN: Object Detection via Region-based Fully Convolutional Networks", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present region-based, fully convolutional networks for accurate and\nefficient object detection. In contrast to previous region-based detectors such\nas Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of\ntimes, our region-based detector is fully convolutional with almost all\ncomputation shared on the entire image. To achieve this goal, we propose\nposition-sensitive score maps to address a dilemma between\ntranslation-invariance in image classification and translation-variance in\nobject detection. Our method can thus naturally adopt fully convolutional image\nclassifier backbones, such as the latest Residual Networks (ResNets), for\nobject detection. We show competitive results on the PASCAL VOC datasets (e.g.,\n83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is\nachieved at a test-time speed of 170ms per image, 2.5-20x faster than the\nFaster R-CNN counterpart. Code is made publicly available at:\nhttps://github.com/daijifeng001/r-fcn\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 15:50:11 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 15:28:57 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Dai", "Jifeng", ""], ["Li", "Yi", ""], ["He", "Kaiming", ""], ["Sun", "Jian", ""]]}, {"id": "1605.06417", "submitter": "Yuan Jiang", "authors": "Wei Shen, Yuan Jiang, Wenjing Gao, Dan Zeng, Xinggang Wang", "title": "Shape Recognition by Bag of Skeleton-associated Contour Parts", "comments": "10 pages. Has been Accepted by Pattern Recognition Letters 2016", "journal-ref": null, "doi": "10.1007/978-3-662-45646-0_40", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Contour and skeleton are two complementary representations for shape\nrecognition. However combining them in a principal way is nontrivial, as they\nare generally abstracted by different structures (closed string vs graph),\nrespectively. This paper aims at addressing the shape recognition problem by\ncombining contour and skeleton according to the correspondence between them.\nThe correspondence provides a straightforward way to associate skeletal\ninformation with a shape contour. More specifically, we propose a new shape\ndescriptor. named Skeleton-associated Shape Context (SSC), which captures the\nfeatures of a contour fragment associated with skeletal information. Benefited\nfrom the association, the proposed shape descriptor provides the complementary\ngeometric information from both contour and skeleton parts, including the\nspatial distribution and the thickness change along the shape part. To form a\nmeaningful shape feature vector for an overall shape, the Bag of Features\nframework is applied to the SSC descriptors extracted from it. Finally, the\nshape feature vector is fed into a linear SVM classifier to recognize the\nshape. The encouraging experimental results demonstrate that the proposed way\nto combine contour and skeleton is effective for shape recognition, which\nachieves the state-of-the-art performances on several standard shape\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:07:41 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Shen", "Wei", ""], ["Jiang", "Yuan", ""], ["Gao", "Wenjing", ""], ["Zeng", "Dan", ""], ["Wang", "Xinggang", ""]]}, {"id": "1605.06431", "submitter": "Andreas Veit", "authors": "Andreas Veit, Michael Wilber, Serge Belongie", "title": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a novel interpretation of residual networks showing\nthat they can be seen as a collection of many paths of differing length.\nMoreover, residual networks seem to enable very deep networks by leveraging\nonly the short paths during training. To support this observation, we rewrite\nresidual networks as an explicit collection of paths. Unlike traditional\nmodels, paths through residual networks vary in length. Further, a lesion study\nreveals that these paths show ensemble-like behavior in the sense that they do\nnot strongly depend on each other. Finally, and most surprising, most paths are\nshorter than one might expect, and only the short paths are needed during\ntraining, as longer paths do not contribute any gradient. For example, most of\nthe gradient in a residual network with 110 layers comes from paths that are\nonly 10-34 layers deep. Our results reveal one of the key characteristics that\nseem to enable the training of very deep networks: Residual networks avoid the\nvanishing gradient problem by introducing short paths which can carry gradient\nthroughout the extent of very deep networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 16:44:03 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 00:43:58 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Veit", "Andreas", ""], ["Wilber", "Michael", ""], ["Belongie", "Serge", ""]]}, {"id": "1605.06437", "submitter": "Davide Boscaini", "authors": "Davide Boscaini, Jonathan Masci, Emanuele Rodol\\`a, Michael M.\n  Bronstein", "title": "Learning shape correspondence with anisotropic convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Establishing correspondence between shapes is a fundamental problem in\ngeometry processing, arising in a wide variety of applications. The problem is\nespecially difficult in the setting of non-isometric deformations, as well as\nin the presence of topological noise and missing parts, mainly due to the\nlimited capability to model such deformations axiomatically. Several recent\nworks showed that invariance to complex shape transformations can be learned\nfrom examples. In this paper, we introduce an intrinsic convolutional neural\nnetwork architecture based on anisotropic diffusion kernels, which we term\nAnisotropic Convolutional Neural Network (ACNN). In our construction, we\ngeneralize convolutions to non-Euclidean domains by constructing a set of\noriented anisotropic diffusion kernels, creating in this way a local intrinsic\npolar representation of the data (`patch'), which is then correlated with a\nfilter. Several cascades of such filters, linear, and non-linear operators are\nstacked to form a deep neural network whose parameters are learned by\nminimizing a task-specific cost. We use ACNNs to effectively learn intrinsic\ndense correspondences between deformable shapes in very challenging settings,\nachieving state-of-the-art results on some of the most difficult recent\ncorrespondence benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 17:02:40 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Boscaini", "Davide", ""], ["Masci", "Jonathan", ""], ["Rodol\u00e0", "Emanuele", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1605.06457", "submitter": "Adrien Gaidon", "authors": "Adrien Gaidon, Qiao Wang, Yohann Cabon, Eleonora Vig", "title": "Virtual Worlds as Proxy for Multi-Object Tracking Analysis", "comments": "CVPR 2016, Virtual KITTI dataset download at\n  http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computer vision algorithms typically require expensive data\nacquisition and accurate manual labeling. In this work, we instead leverage the\nrecent progress in computer graphics to generate fully labeled, dynamic, and\nphoto-realistic proxy virtual worlds. We propose an efficient real-to-virtual\nworld cloning method, and validate our approach by building and publicly\nreleasing a new video dataset, called Virtual KITTI (see\nhttp://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds),\nautomatically labeled with accurate ground truth for object detection,\ntracking, scene and instance segmentation, depth, and optical flow. We provide\nquantitative experimental evidence suggesting that (i) modern deep learning\nalgorithms pre-trained on real data behave similarly in real and virtual\nworlds, and (ii) pre-training on virtual data improves performance. As the gap\nbetween real and virtual worlds is small, virtual worlds enable measuring the\nimpact of various weather and imaging conditions on recognition performance,\nall other things being equal. We show these factors may affect drastically\notherwise high-performing deep models for tracking.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 18:03:07 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Gaidon", "Adrien", ""], ["Wang", "Qiao", ""], ["Cabon", "Yohann", ""], ["Vig", "Eleonora", ""]]}, {"id": "1605.06465", "submitter": "Saurabh Singh", "authors": "Saurabh Singh, Derek Hoiem and David Forsyth", "title": "Swapout: Learning an ensemble of deep architectures", "comments": "Submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Swapout, a new stochastic training method, that outperforms\nResNets of identical network structure yielding impressive results on CIFAR-10\nand CIFAR-100. Swapout samples from a rich set of architectures including\ndropout, stochastic depth and residual architectures as special cases. When\nviewed as a regularization method swapout not only inhibits co-adaptation of\nunits in a layer, similar to dropout, but also across network layers. We\nconjecture that swapout achieves strong regularization by implicitly tying the\nparameters across layers. When viewed as an ensemble training method, it\nsamples a much richer set of architectures than existing methods such as\ndropout or stochastic depth. We propose a parameterization that reveals\nconnections to exiting architectures and suggests a much richer set of\narchitectures to be explored. We show that our formulation suggests an\nefficient training method and validate our conclusions on CIFAR-10 and\nCIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider\nmodel performs similar to a 1001 layer ResNet model.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 18:39:33 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Singh", "Saurabh", ""], ["Hoiem", "Derek", ""], ["Forsyth", "David", ""]]}, {"id": "1605.06474", "submitter": "Nikos Deligiannis", "authors": "Nikos Deligiannis, Jo\\~ao F. C. Mota, Bruno Cornelis, Miguel R. D.\n  Rodrigues, Ingrid Daubechies", "title": "X-ray image separation via coupled dictionary learning", "comments": "To be presented at the IEEE International Conference on Image\n  Processing (ICIP), 2016", "journal-ref": null, "doi": "10.1109/ICIP.2016.7533017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support of art investigation, we propose a new source sepa- ration method\nthat unmixes a single X-ray scan acquired from double-sided paintings. Unlike\nprior source separation meth- ods, which are based on statistical or structural\nincoherence of the sources, we use visual images taken from the front- and\nback-side of the panel to drive the separation process. The coupling of the two\nimaging modalities is achieved via a new multi-scale dictionary learning\nmethod. Experimental results demonstrate that our method succeeds in the\ndiscrimination of the sources, while state-of-the-art methods fail to do so.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 19:11:04 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Deligiannis", "Nikos", ""], ["Mota", "Jo\u00e3o F. C.", ""], ["Cornelis", "Bruno", ""], ["Rodrigues", "Miguel R. D.", ""], ["Daubechies", "Ingrid", ""]]}, {"id": "1605.06489", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Duncan Robertson, Roberto Cipolla, Antonio Criminisi", "title": "Deep Roots: Improving CNN Efficiency with Hierarchical Filter Groups", "comments": "Updated full version of paper, in full letter paper two-column paper.\n  Includes many textual changes, updated CIFAR10 results, and new analysis of\n  inter/intra-layer correlation", "journal-ref": null, "doi": "10.1109/CVPR.2017.633", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for creating computationally efficient and compact\nconvolutional neural networks (CNNs) using a novel sparse connection structure\nthat resembles a tree root. This allows a significant reduction in\ncomputational cost and number of parameters compared to state-of-the-art deep\nCNNs, without compromising accuracy, by exploiting the sparsity of inter-layer\nfilter dependencies. We validate our approach by using it to train more\nefficient variants of state-of-the-art CNN architectures, evaluated on the\nCIFAR10 and ILSVRC datasets. Our results show similar or higher accuracy than\nthe baseline architectures with much less computation, as measured by CPU and\nGPU timings. For example, for ResNet 50, our model has 40% fewer parameters,\n45% fewer floating point operations, and is 31% (12%) faster on a CPU (GPU).\nFor the deeper ResNet 200 our model has 25% fewer floating point operations and\n44% fewer parameters, while maintaining state-of-the-art accuracy. For\nGoogLeNet, our model has 7% fewer parameters and is 21% (16%) faster on a CPU\n(GPU).\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 19:51:37 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 17:29:01 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 15:32:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ioannou", "Yani", ""], ["Robertson", "Duncan", ""], ["Cipolla", "Roberto", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1605.06595", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "WAHRSIS: A Low-cost, High-resolution Whole Sky Imager With Near-Infrared\n  Capabilities", "comments": "Proc. IS&T/SPIE Infrared Imaging Systems, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud imaging using ground-based whole sky imagers is essential for a\nfine-grained understanding of the effects of cloud formations, which can be\nuseful in many applications. Some such imagers are available commercially, but\ntheir cost is relatively high, and their flexibility is limited. Therefore, we\nbuilt a new daytime Whole Sky Imager (WSI) called Wide Angle High-Resolution\nSky Imaging System. The strengths of our new design are its simplicity, low\nmanufacturing cost and high resolution. Our imager captures the entire\nhemisphere in a single high-resolution picture via a digital camera using a\nfish-eye lens. The camera was modified to capture light across the visible as\nwell as the near-infrared spectral ranges. This paper describes the design of\nthe device as well as the geometric and radiometric calibration of the imaging\nsystem.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 06:49:59 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 04:33:42 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1605.06597", "submitter": "Shu Zhang", "authors": "Shu Zhang, Qi Zhu, Amit Roy-Chowdhury", "title": "Adaptive Algorithm and Platform Selection for Visual Detection and\n  Tracking", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision algorithms are known to be extremely sensitive to the\nenvironmental conditions in which the data is captured, e.g., lighting\nconditions and target density. Tuning of parameters or choosing a completely\nnew algorithm is often needed to achieve a certain performance level,\nespecially when there is a limitation of the computation source. In this paper,\nwe focus on this problem and propose a framework to adaptively select the\n\"best\" algorithm-parameter combination and the computation platform under\nperformance and cost constraints at design time, and adapt the algorithms at\nruntime based on real-time inputs. This necessitates developing a mechanism to\nswitch between different algorithms as the nature of the input video changes.\nOur proposed algorithm calculates a similarity function between a test video\nscenario and each training scenario, where the similarity calculation is based\non learning a manifold of image features that is shared by both the training\nand test datasets. Similarity between training and test dataset indicates the\nsame algorithm can be applied to both of them and achieve similar performance.\nWe design a cost function with this similarity measure to find the most similar\ntraining scenario to the test data. The \"best\" algorithm under a given platform\nis obtained by selecting the algorithm with a specific parameter combination\nthat performs the best on the corresponding training data. The proposed\nframework can be used first offline to choose the platform based on performance\nand cost constraints, and then online whereby the \"best\" algorithm is selected\nfor each new incoming video segment for a given platform. In the experiments,\nwe apply our algorithm to the problems of pedestrian detection and tracking. We\nshow how to adaptively select platforms and algorithm-parameter combinations.\nOur results provide optimal performance on 3 publicly available datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 06:58:02 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Zhang", "Shu", ""], ["Zhu", "Qi", ""], ["Roy-Chowdhury", "Amit", ""]]}, {"id": "1605.06695", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Judy Hoffman, Stella X. Yu, Kate Saenko", "title": "Fine-to-coarse Knowledge Transfer For Low-Res Image Classification", "comments": "5 pages, accepted by ICIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the difficult problem of distinguishing fine-grained object\ncategories in low resolution images. Wepropose a simple an effective deep\nlearning approach that transfers fine-grained knowledge gained from high\nresolution training data to the coarse low-resolution test scenario. Such\nfine-to-coarse knowledge transfer has many real world applications, such as\nidentifying objects in surveillance photos or satellite images where the image\nresolution at the test time is very low but plenty of high resolution photos of\nsimilar objects are available. Our extensive experiments on two standard\nbenchmark datasets containing fine-grained car models and bird species\ndemonstrate that our approach can effectively transfer fine-detail knowledge to\ncoarse-detail imagery.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 20:08:53 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Peng", "Xingchao", ""], ["Hoffman", "Judy", ""], ["Yu", "Stella X.", ""], ["Saenko", "Kate", ""]]}, {"id": "1605.06708", "submitter": "Agostinho Rosa", "authors": "Andre Rosado and Agostinho C Rosa", "title": "Automatic Detection of Epileptiform Discharges in the EEG", "comments": "6 pages, 7 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis of epilepsy generally includes a visual inspection of EEG\nrecorded data by the Neurologist, with the purpose of checking the occurrence\nof transient waveforms called interictal epileptiform discharges. These\nwaveforms have short duration (less than 100 ms), so the inspection process is\nusually time-consuming, particularly for ambulatory long term EEG records.\nTherefore, an automatic detection system of epileptiform discharges can be a\nvaluable tool for a Neurology service. The proposed approach is the development\nof a multi stage detection algorithm, which processes the complete EEG signals\nand applies decision criteria to selected waveforms. It employs EEG analysis\ntechniques such as Wavelet Transform and Mimetic Analysis, complemented with a\nclassification based on Fuzzy Logic. In order to evaluate the algorithm's\nperformance, data were collected from several epileptic patients, with\nepileptiform activity marked by a Neurologist. The average values obtained for\nboth Sensitivity and Specificity were respectively higher than 80 and 70\npercent.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2016 22:49:47 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Rosado", "Andre", ""], ["Rosa", "Agostinho C", ""]]}, {"id": "1605.06742", "submitter": "Wenshuo Wang", "authors": "Wenshuo Wang and Junqiang Xi", "title": "A Rapid Pattern-Recognition Method for Driving Types Using\n  Clustering-Based Support Vector Machines", "comments": "6 pages, 9 figures, 2 tables. To be appear in 2016 American Control\n  Conference, Boston, MA, USA, 2016", "journal-ref": "2017 American Control Conference", "doi": "10.1109/ACC.2016.7526495", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapid pattern-recognition approach to characterize driver's\ncurve-negotiating behavior is proposed. To shorten the recognition time and\nimprove the recognition of driving styles, a k-means clustering-based support\nvector machine ( kMC-SVM) method is developed and used for classifying drivers\ninto two types: aggressive and moderate. First, vehicle speed and throttle\nopening are treated as the feature parameters to reflect the driving styles.\nSecond, to discriminate driver curve-negotiating behaviors and reduce the\nnumber of support vectors, the k-means clustering method is used to extract and\ngather the two types of driving data and shorten the recognition time. Then,\nbased on the clustering results, a support vector machine approach is utilized\nto generate the hyperplane for judging and predicting to which types the human\ndriver are subject. Lastly, to verify the validity of the kMC-SVM method, a\ncross-validation experiment is designed and conducted. The research results\nshow that the $ k $MC-SVM is an effective method to classify driving styles\nwith a short time, compared with SVM method.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 06:15:11 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Wang", "Wenshuo", ""], ["Xi", "Junqiang", ""]]}, {"id": "1605.06764", "submitter": "Patrik Huber", "authors": "Patrik Huber, Philipp Kopp, Matthias R\\\"atsch, William Christmas,\n  Josef Kittler", "title": "3D Face Tracking and Texture Fusion in the Wild", "comments": null, "journal-ref": "IEEE Signal Processing Letters (Volume: 24, Issue: 4, April 2017)", "doi": "10.1109/LSP.2016.2643284", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully automatic approach to real-time 3D face reconstruction\nfrom monocular in-the-wild videos. With the use of a cascaded-regressor based\nface tracking and a 3D Morphable Face Model shape fitting, we obtain a\nsemi-dense 3D face shape. We further use the texture information from multiple\nframes to build a holistic 3D face representation from the video frames. Our\nsystem is able to capture facial expressions and does not require any\nperson-specific training. We demonstrate the robustness of our approach on the\nchallenging 300 Videos in the Wild (300-VW) dataset. Our real-time fitting\nframework is available as an open source library at http://4dface.org.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 09:52:16 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Huber", "Patrik", ""], ["Kopp", "Philipp", ""], ["R\u00e4tsch", "Matthias", ""], ["Christmas", "William", ""], ["Kittler", "Josef", ""]]}, {"id": "1605.06776", "submitter": "Huynh Van Luong", "authors": "Huynh Van Luong, J\\\"urgen Seiler, Andr\\'e Kaup, S{\\o}ren Forchhammer", "title": "Sparse Signal Reconstruction with Multiple Side Information using\n  Adaptive Weights for Multiview Sources", "comments": "Submitted to the IEEE International Conference on Image Processing\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers reconstructing a target signal in a context of\ndistributed sparse sources. We propose an efficient reconstruction algorithm\nwith the aid of other given sources as multiple side information (SI). The\nproposed algorithm takes advantage of compressive sensing (CS) with SI and\nadaptive weights by solving a proposed weighted $n$-$\\ell_{1}$ minimization.\nThe proposed algorithm computes the adaptive weights in two levels, first each\nindividual intra-SI and then inter-SI weights are iteratively updated at every\nreconstructed iteration. This two-level optimization leads the proposed\nreconstruction algorithm with multiple SI using adaptive weights (RAMSIA) to\nrobustly exploit the multiple SIs with different qualities. We experimentally\nperform our algorithm on generated sparse signals and also correlated feature\nhistograms as multiview sparse sources from a multiview image database. The\nresults show that RAMSIA significantly outperforms both classical CS and CS\nwith single SI, and RAMSIA with higher number of SIs gained more than the one\nwith smaller number of SIs.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 12:00:34 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Van Luong", "Huynh", ""], ["Seiler", "J\u00fcrgen", ""], ["Kaup", "Andr\u00e9", ""], ["Forchhammer", "S\u00f8ren", ""]]}, {"id": "1605.06778", "submitter": "Maximilian Schmitt", "authors": "Maximilian Schmitt and Bj\\\"orn W. Schuller", "title": "openXBOW - Introducing the Passau Open-Source Crossmodal Bag-of-Words\n  Toolkit", "comments": "9 pages, 1 figure, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce openXBOW, an open-source toolkit for the generation of\nbag-of-words (BoW) representations from multimodal input. In the BoW principle,\nword histograms were first used as features in document classification, but the\nidea was and can easily be adapted to, e.g., acoustic or visual low-level\ndescriptors, introducing a prior step of vector quantisation. The openXBOW\ntoolkit supports arbitrary numeric input features and text input and\nconcatenates computed subbags to a final bag. It provides a variety of\nextensions and options. To our knowledge, openXBOW is the first publicly\navailable toolkit for the generation of crossmodal bags-of-words. The\ncapabilities of the tool are exemplified in two sample scenarios:\ntime-continuous speech-based emotion recognition and sentiment analysis in\ntweets where improved results over other feature representation forms were\nobserved.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 12:14:55 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Schmitt", "Maximilian", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "1605.06820", "submitter": "Hamid Tizhoosh", "authors": "Fares Al-Qunaieer, Hamid R. Tizhoosh, Shahryar Rahnamayan", "title": "Automated Resolution Selection for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known in image processing that computational cost increases\nrapidly with the number and dimensions of the images to be processed. Several\nfields, such as medical imaging, routinely use numerous very large images,\nwhich might also be 3D and/or captured at several frequency bands, all adding\nto the computational expense. Multiresolution analysis is a method of\nincreasing the efficiency of the segmentation process. One multiresolution\napproach is the coarse-to-fine segmentation strategy, whereby the segmentation\nstarts at a coarse resolution and is then fine-tuned during subsequent steps.\nThe starting resolution for segmentation is generally selected arbitrarily with\nno clear selection criteria. The research reported in this paper showed that\nstarting from different resolutions for image segmentation results in different\naccuracies and computational times, even for images of the same category\n(depicting similar scenes or objects). An automated method for resolution\nselection for an input image would thus be beneficial. This paper introduces a\nframework for the automated selection of the best resolution for image\nsegmentation. We propose a measure for defining the best resolution based on\nuser/system criteria, offering a trade-off between accuracy and computation\ntime. A learning approach is then introduced for the selection of the\nresolution, whereby extracted image features are mapped to the previously\ndetermined best resolution. In the learning process, class (i.e., resolution)\ndistribution is generally imbalanced, making effective learning from the data\ndifficult. Experiments conducted with three datasets using two different\nsegmentation algorithms show that the resolutions selected through learning\nenable much faster segmentation than the original ones, while retaining at\nleast the original accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 17:09:29 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Al-Qunaieer", "Fares", ""], ["Tizhoosh", "Hamid R.", ""], ["Rahnamayan", "Shahryar", ""]]}, {"id": "1605.06863", "submitter": "Enliang Zheng", "authors": "Enliang Zheng, Dinghuang Ji, Enrique Dunn and Jan-Michael Frahm", "title": "Self-expressive Dictionary Learning for Dynamic 3D Reconstruction", "comments": "15 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We target the problem of sparse 3D reconstruction of dynamic objects observed\nby multiple unsynchronized video cameras with unknown temporal overlap. To this\nend, we develop a framework to recover the unknown structure without sequencing\ninformation across video sequences. Our proposed compressed sensing framework\nposes the estimation of 3D structure as the problem of dictionary learning,\nwhere the dictionary is defined as an aggregation of the temporally varying 3D\nstructures. Given the smooth motion of dynamic objects, we observe any element\nin the dictionary can be well approximated by a sparse linear combination of\nother elements in the same dictionary (i. e. self-expression). Moreover, the\nsparse coefficients describing a locally linear 3D structural interpolation\nreveal the local sequencing information. Our formulation optimizes a biconvex\ncost function that leverages a compressed sensing formulation and enforces both\nstructural dependency coherence across video streams, as well as motion\nsmoothness across estimates from common video sources. We further analyze the\nreconstructability of our approach under different capture scenarios, and its\ncomparison and relation to existing methods. Experimental results on large\namounts of synthetic data as well as real imagery demonstrate the effectiveness\nof our approach.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 23:56:34 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Zheng", "Enliang", ""], ["Ji", "Dinghuang", ""], ["Dunn", "Enrique", ""], ["Frahm", "Jan-Michael", ""]]}, {"id": "1605.06878", "submitter": "Jianxin Wu", "authors": "Xiu-Shen Wei and Chen-Wei Xie and Jianxin Wu", "title": "Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained\n  Image Recognition", "comments": "Submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image recognition is a challenging computer vision problem, due\nto the small inter-class variations caused by highly similar subordinate\ncategories, and the large intra-class variations in poses, scales and\nrotations. In this paper, we propose a novel end-to-end Mask-CNN model without\nthe fully connected layers for fine-grained recognition. Based on the part\nannotations of fine-grained images, the proposed model consists of a fully\nconvolutional network to both locate the discriminative parts (e.g., head and\ntorso), and more importantly generate object/part masks for selecting useful\nand meaningful convolutional descriptors. After that, a four-stream Mask-CNN\nmodel is built for aggregating the selected object- and part-level descriptors\nsimultaneously. The proposed Mask-CNN model has the smallest number of\nparameters, lowest feature dimensionality and highest recognition accuracy when\ncompared with state-of-the-arts fine-grained approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 02:46:47 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Wei", "Xiu-Shen", ""], ["Xie", "Chen-Wei", ""], ["Wu", "Jianxin", ""]]}, {"id": "1605.06885", "submitter": "Chunhua Shen", "authors": "Zifeng Wu, Chunhua Shen, Anton van den Hengel", "title": "Bridging Category-level and Instance-level Semantic Image Segmentation", "comments": "14 pages. arXiv admin note: substantial text overlap with\n  arXiv:1604.04339", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to instance-level image segmentation that is built on\ntop of category-level segmentation. Specifically, for each pixel in a semantic\ncategory mask, its corresponding instance bounding box is predicted using a\ndeep fully convolutional regression network. Thus it follows a different\npipeline to the popular detect-then-segment approaches that first predict\ninstances' bounding boxes, which are the current state-of-the-art in instance\nsegmentation. We show that, by leveraging the strength of our state-of-the-art\nsemantic segmentation models, the proposed method can achieve comparable or\neven better results to detect-then-segment approaches. We make the following\ncontributions. (i) First, we propose a simple yet effective approach to\nsemantic instance segmentation. (ii) Second, we propose an online bootstrapping\nmethod during training, which is critically important for achieving good\nperformance for both semantic category segmentation and instance-level\nsegmentation. (iii) As the performance of semantic category segmentation has a\nsignificant impact on the instance-level segmentation, which is the second step\nof our approach, we train fully convolutional residual networks to achieve the\nbest semantic category segmentation accuracy. On the PASCAL VOC 2012 dataset,\nwe obtain the currently best mean intersection-over-union score of 79.1%. (iv)\nWe also achieve state-of-the-art results for instance-level segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 03:43:00 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Wu", "Zifeng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1605.06914", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do and Ngai-Man Cheung", "title": "Embedding based on function approximation for large scale image search", "comments": "Accepted to TPAMI 2017. The implementation and precomputed features\n  of the proposed F-FAemb are released at the following link:\n  http://tinyurl.com/F-FAemb", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to design an embedding method that maps local\nfeatures describing an image (e.g. SIFT) to a higher dimensional representation\nuseful for the image retrieval problem. First, motivated by the relationship\nbetween the linear approximation of a nonlinear function in high dimensional\nspace and the stateof-the-art feature representation used in image retrieval,\ni.e., VLAD, we propose a new approach for the approximation. The embedded\nvectors resulted by the function approximation process are then aggregated to\nform a single representation for image retrieval. Second, in order to make the\nproposed embedding method applicable to large scale problem, we further derive\nits fast version in which the embedded vectors can be efficiently computed,\ni.e., in the closed-form. We compare the proposed embedding methods with the\nstate of the art in the context of image search under various settings: when\nthe images are represented by medium length vectors, short vectors, or binary\nvectors. The experimental results show that the proposed embedding methods\noutperform existing the state of the art on the standard public image retrieval\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 07:12:54 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 09:24:05 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 02:54:44 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1605.07003", "submitter": "Afonso Teodoro", "authors": "Afonso M. Teodoro, Jos\\'e M. Bioucas-Dias, M\\'ario A. T. Figueiredo", "title": "Image Restoration with Locally Selected Class-Adapted Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms for imaging inverse problems (namely deblurring\nand reconstruction) are typically iterative, involving a denoising operation as\none of its steps. Using a state-of-the-art denoising method in this context is\nnot trivial, and is the focus of current work. Recently, we have proposed to\nuse a class-adapted denoiser (patch-based using Gaussian mixture models) in a\nso-called plug-and-play scheme, wherein a state-of-the-art denoiser is plugged\ninto an iterative algorithm, leading to results that outperform the best\ngeneral-purpose algorithms, when applied to an image of a known class (e.g.\nfaces, text, brain MRI). In this paper, we extend that approach to handle\nsituations where the image being processed is from one of a collection of\npossible classes or, more importantly, contains regions of different classes.\nMore specifically, we propose a method to locally select one of a set of\nclass-adapted Gaussian mixture patch priors, previously estimated from clean\nimages of those classes. Our approach may be seen as simultaneously performing\nsegmentation and restoration, thus contributing to bridging the gap between\nimage restoration/reconstruction and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 13:00:38 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 09:37:11 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Teodoro", "Afonso M.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1605.07026", "submitter": "Bappaditya Mandal", "authors": "Bappaditya Mandal and Nizar Ouarti", "title": "Spontaneous vs. Posed smiles - can we tell the difference?", "comments": "10 pages, 5 figures, 6 tables, International Conference on Computer\n  Vision and Image Processing (CVIP 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smile is an irrefutable expression that shows the physical state of the mind\nin both true and deceptive ways. Generally, it shows happy state of the mind,\nhowever, `smiles' can be deceptive, for example people can give a smile when\nthey feel happy and sometimes they might also give a smile (in a different way)\nwhen they feel pity for others. This work aims to distinguish spontaneous\n(felt) smile expressions from posed (deliberate) smiles by extracting and\nanalyzing both global (macro) motion of the face and subtle (micro) changes in\nthe facial expression features through both tracking a series of facial\nfiducial markers as well as using dense optical flow. Specifically the eyes and\nlips features are captured and used for analysis. It aims to automatically\nclassify all smiles into either `spontaneous' or `posed' categories, by using\nsupport vector machines (SVM). Experimental results on large database show\npromising results as compared to other relevant methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 14:21:30 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Mandal", "Bappaditya", ""], ["Ouarti", "Nizar", ""]]}, {"id": "1605.07081", "submitter": "Ayan Chakrabarti", "authors": "Ayan Chakrabarti, Jingyu Shao, Gregory Shakhnarovich", "title": "Depth from a Single Image by Harmonizing Overcomplete Local Network\n  Predictions", "comments": "NIPS 2016. Project page at http://www.ttic.edu/chakrabarti/mdepth/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single color image can contain many cues informative towards different\naspects of local geometric structure. We approach the problem of monocular\ndepth estimation by using a neural network to produce a mid-level\nrepresentation that summarizes these cues. This network is trained to\ncharacterize local scene geometry by predicting, at every image location, depth\nderivatives of different orders, orientations and scales. However, instead of a\nsingle estimate for each derivative, the network outputs probability\ndistributions that allow it to express confidence about some coefficients, and\nambiguity about others. Scene depth is then estimated by harmonizing this\novercomplete set of network predictions, using a globalization procedure that\nfinds a single consistent depth map that best matches all the local derivative\ndistributions. We demonstrate the efficacy of this approach through evaluation\non the NYU v2 depth data set.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:31:43 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 18:20:19 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Chakrabarti", "Ayan", ""], ["Shao", "Jingyu", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1605.07104", "submitter": "Ran Tao", "authors": "Ran Tao, Arnold W.M. Smeulders, Shih-Fu Chang", "title": "Generic Instance Search and Re-identification from One Example via\n  Attributes and Categories", "comments": "This technical report is an extended version of our previous\n  conference paper 'Attributes and Categories for Generic Instance Search from\n  One Example' (CVPR 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims for generic instance search from one example where the\ninstance can be an arbitrary object like shoes, not just near-planar and\none-sided instances like buildings and logos. First, we evaluate\nstate-of-the-art instance search methods on this problem. We observe that what\nworks for buildings loses its generality on shoes. Second, we propose to use\nautomatically learned category-specific attributes to address the large\nappearance variations present in generic instance search. Searching among\ninstances from the same category as the query, the category-specific attributes\noutperform existing approaches by a large margin on shoes and cars and perform\non par with the state-of-the-art on buildings. Third, we treat person\nre-identification as a special case of generic instance search. On the popular\nVIPeR dataset, we reach state-of-the-art performance with the same method.\nFourth, we extend our method to search objects without restriction to the\nspecifically known category. We show that the combination of category-level\ninformation and the category-specific attributes is superior to the alternative\nmethod combining category-level information with low-level features such as\nFisher vector.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 17:25:40 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Tao", "Ran", ""], ["Smeulders", "Arnold W. M.", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1605.07116", "submitter": "Fernando Azevedo Fardo", "authors": "Fernando A. Fardo, Victor H. Conforto, Francisco C. de Oliveira and\n  Paulo S. Rodrigues", "title": "A Formal Evaluation of PSNR as Quality Measurement Parameter for Image\n  Segmentation Algorithms", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality evaluation of image segmentation algorithms are still subject of\ndebate and research. Currently, there is no generic metric that could be\napplied to any algorithm reliably. This article contains an evaluation for the\nPSRN (Peak Signal-To-Noise Ratio) as a metric which has been used to evaluate\nthreshold level selection as well as the number of thresholds in the case of\nmulti-level segmentation. The results obtained in this study suggest that the\nPSNR is not an adequate quality measurement for segmentation algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 17:54:57 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Fardo", "Fernando A.", ""], ["Conforto", "Victor H.", ""], ["de Oliveira", "Francisco C.", ""], ["Rodrigues", "Paulo S.", ""]]}, {"id": "1605.07133", "submitter": "Angeliki  Lazaridou", "authors": "Angeliki Lazaridou, Nghia The Pham and Marco Baroni", "title": "Towards Multi-Agent Communication-Based Language Learning", "comments": "9 pages, manuscript under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an interactive multimodal framework for language learning. Instead\nof being passively exposed to large amounts of natural text, our learners\n(implemented as feed-forward neural networks) engage in cooperative referential\ngames starting from a tabula rasa setup, and thus develop their own language\nfrom the need to communicate in order to succeed at the game. Preliminary\nexperiments provide promising results, but also suggest that it is important to\nensure that agents trained in this way do not develop an adhoc communication\ncode only effective for the game they are playing\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 18:46:46 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Lazaridou", "Angeliki", ""], ["Pham", "Nghia The", ""], ["Baroni", "Marco", ""]]}, {"id": "1605.07146", "submitter": "Sergey Zagoruyko", "authors": "Sergey Zagoruyko, Nikos Komodakis", "title": "Wide Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks were shown to be able to scale up to thousands of\nlayers and still have improving performance. However, each fraction of a\npercent of improved accuracy costs nearly doubling the number of layers, and so\ntraining very deep residual networks has a problem of diminishing feature\nreuse, which makes these networks very slow to train. To tackle these problems,\nin this paper we conduct a detailed experimental study on the architecture of\nResNet blocks, based on which we propose a novel architecture where we decrease\ndepth and increase width of residual networks. We call the resulting network\nstructures wide residual networks (WRNs) and show that these are far superior\nover their commonly used thin and very deep counterparts. For example, we\ndemonstrate that even a simple 16-layer-deep wide residual network outperforms\nin accuracy and efficiency all previous deep residual networks, including\nthousand-layer-deep networks, achieving new state-of-the-art results on CIFAR,\nSVHN, COCO, and significant improvements on ImageNet. Our code and models are\navailable at https://github.com/szagoruyko/wide-residual-networks\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:27:13 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 19:59:22 GMT"}, {"version": "v3", "created": "Tue, 17 Jan 2017 15:35:14 GMT"}, {"version": "v4", "created": "Wed, 14 Jun 2017 06:06:48 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Zagoruyko", "Sergey", ""], ["Komodakis", "Nikos", ""]]}, {"id": "1605.07157", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Ian Goodfellow, Sergey Levine", "title": "Unsupervised Learning for Physical Interaction through Video Prediction", "comments": "To appear in NIPS '16; Video results, code, and data available at:\n  http://www.sites.google.com/site/robotprediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core challenge for an agent learning to interact with the world is to\npredict how its actions affect objects in its environment. Many existing\nmethods for learning the dynamics of physical interactions require labeled\nobject information. However, to scale real-world interaction learning to a\nvariety of scenes and objects, acquiring labeled data becomes increasingly\nimpractical. To learn about physical object motion without labels, we develop\nan action-conditioned video prediction model that explicitly models pixel\nmotion, by predicting a distribution over pixel motion from previous frames.\nBecause our model explicitly predicts motion, it is partially invariant to\nobject appearance, enabling it to generalize to previously unseen objects. To\nexplore video prediction for real-world interactive agents, we also introduce a\ndataset of 59,000 robot interactions involving pushing motions, including a\ntest set with novel objects. In this dataset, accurate prediction of videos\nconditioned on the robot's future actions amounts to learning a \"visual\nimagination\" of different futures based on different courses of action. Our\nexperiments show that our proposed method produces more accurate video\npredictions both quantitatively and qualitatively, when compared to prior\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 19:45:55 GMT"}, {"version": "v2", "created": "Tue, 24 May 2016 19:33:23 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 00:29:37 GMT"}, {"version": "v4", "created": "Mon, 17 Oct 2016 20:09:56 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Finn", "Chelsea", ""], ["Goodfellow", "Ian", ""], ["Levine", "Sergey", ""]]}, {"id": "1605.07251", "submitter": "Jianxin Wu", "authors": "Jianxin Wu and Chen-Wei Xie and Jian-Hao Luo", "title": "Dense CNN Learning with Equivalent Mappings", "comments": "submitted to NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large receptive field and dense prediction are both important for achieving\nhigh accuracy in pixel labeling tasks such as semantic segmentation. These two\nproperties, however, contradict with each other. A pooling layer (with stride\n2) quadruples the receptive field size but reduces the number of predictions to\n25\\%. Some existing methods lead to dense predictions using computations that\nare not equivalent to the original model. In this paper, we propose the\nequivalent convolution (eConv) and equivalent pooling (ePool) layers, leading\nto predictions that are both dense and equivalent to the baseline CNN model.\nDense prediction models learned using eConv and ePool can transfer the baseline\nCNN's parameters as a starting point, and can inverse transfer the learned\nparameters in a dense model back to the original one, which has both fast\ntesting speed and high accuracy. The proposed eConv and ePool layers have\nachieved higher accuracy than baseline CNN in various tasks, including semantic\nsegmentation, object localization, image categorization and apparent age\nestimation, not only in those tasks requiring dense pixel labeling.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 01:24:26 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Wu", "Jianxin", ""], ["Xie", "Chen-Wei", ""], ["Luo", "Jian-Hao", ""]]}, {"id": "1605.07262", "submitter": "Osbert Bastani", "authors": "Osbert Bastani, Yani Ioannou, Leonidas Lampropoulos, Dimitrios\n  Vytiniotis, Aditya Nori, Antonio Criminisi", "title": "Measuring Neural Net Robustness with Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite having high accuracy, neural nets have been shown to be susceptible\nto adversarial examples, where a small perturbation to an input can cause it to\nbecome mislabeled. We propose metrics for measuring the robustness of a neural\nnet and devise a novel algorithm for approximating these metrics based on an\nencoding of robustness as a linear program. We show how our metrics can be used\nto evaluate the robustness of deep neural nets with experiments on the MNIST\nand CIFAR-10 datasets. Our algorithm generates more informative estimates of\nrobustness metrics compared to estimates based on existing algorithms.\nFurthermore, we show how existing approaches to improving robustness \"overfit\"\nto adversarial examples generated using a specific algorithm. Finally, we show\nthat our techniques can be used to additionally improve neural net robustness\nboth according to the metrics that we propose, but also according to previously\nproposed metrics.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:18:21 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 11:58:51 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Bastani", "Osbert", ""], ["Ioannou", "Yani", ""], ["Lampropoulos", "Leonidas", ""], ["Vytiniotis", "Dimitrios", ""], ["Nori", "Aditya", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1605.07264", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson", "title": "Trajectory probability hypothesis density filter", "comments": "Published in the Proceedings of the 21st International Conference on\n  Information Fusion (FUSION)", "journal-ref": null, "doi": "10.23919/ICIF.2018.8455270", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the probability hypothesis density (PHD) filter for sets\nof trajectories: the trajectory probability density (TPHD) filter. The TPHD\nfilter is capable of estimating trajectories in a principled way without\nrequiring to evaluate all measurement-to-target association hypotheses. The\nTPHD filter is based on recursively obtaining the best Poisson approximation to\nthe multitrajectory filtering density in the sense of minimising the\nKullback-Leibler divergence. We also propose a Gaussian mixture implementation\nof the TPHD recursion. Finally, we include simulation results to show the\nperformance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:19:11 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 14:05:43 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1605.07270", "submitter": "Yonatan Wexler Dr.", "authors": "Oren Tadmor and Yonatan Wexler and Tal Rosenwein and Shai\n  Shalev-Shwartz and Amnon Shashua", "title": "Learning a Metric Embedding for Face Recognition using the Multibatch\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the engineering task of achieving a near\nstate-of-the-art face recognition on a minimal computing budget running on an\nembedded system. Our main technical contribution centers around a novel\ntraining method, called Multibatch, for similarity learning, i.e., for the task\nof generating an invariant \"face signature\" through training pairs of \"same\"\nand \"not-same\" face images. The Multibatch method first generates signatures\nfor a mini-batch of $k$ face images and then constructs an unbiased estimate of\nthe full gradient by relying on all $k^2-k$ pairs from the mini-batch. We prove\nthat the variance of the Multibatch estimator is bounded by $O(1/k^2)$, under\nsome mild conditions. In contrast, the standard gradient estimator that relies\non random $k/2$ pairs has a variance of order $1/k$. The smaller variance of\nthe Multibatch estimator significantly speeds up the convergence rate of\nstochastic gradient descent. Using the Multibatch method we train a deep\nconvolutional neural network that achieves an accuracy of $98.2\\%$ on the LFW\nbenchmark, while its prediction runtime takes only $30$msec on a single ARM\nCortex A9 core. Furthermore, the entire training process took only 12 hours on\na single Titan X GPU.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 02:42:53 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Tadmor", "Oren", ""], ["Wexler", "Yonatan", ""], ["Rosenwein", "Tal", ""], ["Shalev-Shwartz", "Shai", ""], ["Shashua", "Amnon", ""]]}, {"id": "1605.07289", "submitter": "Zheng Shou", "authors": "Dongang Wang, Zheng Shou, Hongyi Liu, Shih-Fu Chang", "title": "EventNet Version 1.1 Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EventNet is a large-scale video corpus and event ontology consisting of 500\nevents associated with event-specific concepts. In order to improve the quality\nof the current EventNet, we conduct the following steps and introduce EventNet\nversion 1.1: (1) manually verify the correctness of event labels for all\nvideos; (2) remove the YouTube user bias by limiting the maximum number of\nvideos in each event from the same YouTube user as 3; (3) remove the videos\nwhich are currently not accessible online; (4) remove the video belonging to\nmultiple event categories. After the above procedure, some events may contain\nonly a small number of videos, and therefore we crawl more videos for those\nevents to ensure every event will contain more than 50 videos. Finally,\nEventNet version 1.1 contains 67,641 videos, 500 events, and 5,028\nevent-specific concepts. In addition, we train a Convolutional Neural Network\n(CNN) model for event classification via fine-tuning AlexNet using EventNet\nversion 1.1. Then we use the trained CNN model to extract FC7 layer feature and\ntrain binary classifiers using linear SVM for each event-specific concept. We\nbelieve this new version of EventNet will significantly facilitate research in\ncomputer vision and multimedia, and will put it online for public downloading\nin the future.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 05:06:29 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 19:10:33 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Wang", "Dongang", ""], ["Shou", "Zheng", ""], ["Liu", "Hongyi", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1605.07314", "submitter": "Lianwen Jin", "authors": "Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, Ziyong Feng", "title": "DeepText: A Unified Framework for Text Proposal Generation and Text\n  Detection in Natural Images", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a novel unified framework called DeepText for text\nregion proposal generation and text detection in natural images via a fully\nconvolutional neural network (CNN). First, we propose the inception region\nproposal network (Inception-RPN) and design a set of text characteristic prior\nbounding boxes to achieve high word recall with only hundred level candidate\nproposals. Next, we present a powerful textdetection network that embeds\nambiguous text category (ATC) information and multilevel region-of-interest\npooling (MLRP) for text and non-text classification and accurate localization.\nFinally, we apply an iterative bounding box voting scheme to pursue high recall\nin a complementary manner and introduce a filtering algorithm to retain the\nmost suitable bounding box, while removing redundant inner and outer boxes for\neach text instance. Our approach achieves an F-measure of 0.83 and 0.85 on the\nICDAR 2011 and 2013 robust text detection benchmarks, outperforming previous\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 06:48:23 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Zhong", "Zhuoyao", ""], ["Jin", "Lianwen", ""], ["Zhang", "Shuye", ""], ["Feng", "Ziyong", ""]]}, {"id": "1605.07363", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Mateusz Malinowski, Mario Fritz", "title": "Spatio-Temporal Image Boundary Extrapolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary prediction in images as well as video has been a very active topic\nof research and organizing visual information into boundaries and segments is\nbelieved to be a corner stone of visual perception. While prior work has\nfocused on predicting boundaries for observed frames, our work aims at\npredicting boundaries of future unobserved frames. This requires our model to\nlearn about the fate of boundaries and extrapolate motion patterns. We\nexperiment on established real-world video segmentation dataset, which provides\na testbed for this new task. We show for the first time spatio-temporal\nboundary extrapolation in this challenging scenario. Furthermore, we show\nlong-term prediction of boundaries in situations where the motion is governed\nby the laws of physics. We successfully predict boundaries in a billiard\nscenario without any assumptions of a strong parametric model or any object\nnotion. We argue that our model has with minimalistic model assumptions derived\na notion of 'intuitive physics' that can be applied to novel scenes.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:22:33 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1605.07369", "submitter": "Ganesh Sundaramoorthi", "authors": "Dong Lao and Ganesh Sundaramoorthi", "title": "Quickest Moving Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework and method for simultaneous detection and\nsegmentation of an object in a video that moves (or comes into view of the\ncamera) at some unknown time in the video. The method is an online approach\nbased on motion segmentation, and it operates under dynamic backgrounds caused\nby a moving camera or moving nuisances. The goal of the method is to detect and\nsegment the object as soon as it moves. Due to stochastic variability in the\nvideo and unreliability of the motion signal, several frames are needed to\nreliably detect the object. The method is designed to detect and segment with\nminimum delay subject to a constraint on the false alarm rate. The method is\nderived as a problem of Quickest Change Detection. Experiments on a dataset\nshow the effectiveness of our method in minimizing detection delay subject to\nfalse alarm constraints.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 10:40:13 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Lao", "Dong", ""], ["Sundaramoorthi", "Ganesh", ""]]}, {"id": "1605.07586", "submitter": "Mohsen Kheirandishfard", "authors": "Fariba Zohrizadeh, Mohsen Kheirandishfard, and Farhad Kamangar", "title": "Natural Scene Image Segmentation Based on Multi-Layer Feature Extraction", "comments": "This paper has been withdrawn by the author due to the fact that the\n  contents need further research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of natural image segmentation by extracting\ninformation from a multi-layer array which is constructed based on color,\ngradient, and statistical properties of the local neighborhoods in an image. A\nGaussian Mixture Model (GMM) is used to improve the effectiveness of local\nspectral histogram features. Grouping these features leads to forming a rough\ninitial over-segmented layer which contains coherent regions of pixels. The\nregions are merged by using two proposed functions for calculating the distance\nbetween two neighboring regions and making decisions about their merging.\nExtensive experiments are performed on the Berkeley Segmentation Dataset to\nevaluate the performance of our proposed method and compare the results with\nthe recent state-of-the-art methods. The experimental results indicate that our\nmethod achieves higher level of accuracy for natural images compared to recent\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 19:03:54 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 03:58:34 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Zohrizadeh", "Fariba", ""], ["Kheirandishfard", "Mohsen", ""], ["Kamangar", "Farhad", ""]]}, {"id": "1605.07648", "submitter": "Michael Maire", "authors": "Gustav Larsson, Michael Maire, Gregory Shakhnarovich", "title": "FractalNet: Ultra-Deep Neural Networks without Residuals", "comments": "updated with ImageNet results; published as a conference paper at\n  ICLR 2017; project page at http://people.cs.uchicago.edu/~larsson/fractalnet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a design strategy for neural network macro-architecture based on\nself-similarity. Repeated application of a simple expansion rule generates deep\nnetworks whose structural layouts are precisely truncated fractals. These\nnetworks contain interacting subpaths of different lengths, but do not include\nany pass-through or residual connections; every internal signal is transformed\nby a filter and nonlinearity before being seen by subsequent layers. In\nexperiments, fractal networks match the excellent performance of standard\nresidual networks on both CIFAR and ImageNet classification tasks, thereby\ndemonstrating that residual representations may not be fundamental to the\nsuccess of extremely deep convolutional neural networks. Rather, the key may be\nthe ability to transition, during training, from effectively shallow to deep.\nWe note similarities with student-teacher behavior and develop drop-path, a\nnatural extension of dropout, to regularize co-adaptation of subpaths in\nfractal architectures. Such regularization allows extraction of\nhigh-performance fixed-depth subnetworks. Additionally, fractal networks\nexhibit an anytime property: shallow subnetworks provide a quick answer, while\ndeeper subnetworks, with higher latency, provide a more accurate answer.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 20:28:53 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 17:34:04 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 21:37:25 GMT"}, {"version": "v4", "created": "Fri, 26 May 2017 18:53:56 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Larsson", "Gustav", ""], ["Maire", "Michael", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1605.07650", "submitter": "Sohini Roychowdhury", "authors": "Sohini Roychowdhury, Nathan Hollraft, Adam Alessio", "title": "Blind Analysis of CT Image Noise Using Residual Denoised Images", "comments": "4 pages, 6 figures, IEEE NSS/MIC 2015", "journal-ref": null, "doi": "10.1109/NSSMIC.2015.7582055", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT protocol design and quality control would benefit from automated tools to\nestimate the quality of generated CT images. These tools could be used to\nidentify erroneous CT acquisitions or refine protocols to achieve certain\nsignal to noise characteristics. This paper investigates blind estimation\nmethods to determine global signal strength and noise levels in chest CT\nimages. Methods: We propose novel performance metrics corresponding to the\naccuracy of noise and signal estimation. We implement and evaluate the noise\nestimation performance of six spatial- and frequency- based methods, derived\nfrom conventional image filtering algorithms. Algorithms were tested on patient\ndata sets from whole-body repeat CT acquisitions performed with a higher and\nlower dose technique over the same scan region. Results: The proposed\nperformance metrics can evaluate the relative tradeoff of filter parameters and\nnoise estimation performance. The proposed automated methods tend to\nunderestimate CT image noise at low-flux levels. Initial application of\nmethodology suggests that anisotropic diffusion and Wavelet-transform based\nfilters provide optimal estimates of noise. Furthermore, methodology does not\nprovide accurate estimates of absolute noise levels, but can provide estimates\nof relative change and/or trends in noise levels.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 20:31:39 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Roychowdhury", "Sohini", ""], ["Hollraft", "Nathan", ""], ["Alessio", "Adam", ""]]}, {"id": "1605.07651", "submitter": "Moin Nabi", "authors": "Enver Sangineto, Moin Nabi, Dubravko Culibrk, Nicu Sebe", "title": "Self Paced Deep Learning for Weakly Supervised Object Detection", "comments": "To appear at IEEE Transactions on PAMI", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2804907", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a weakly-supervised scenario object detectors need to be trained using\nimage-level annotation alone. Since bounding-box-level ground truth is not\navailable, most of the solutions proposed so far are based on an iterative,\nMultiple Instance Learning framework in which the current classifier is used to\nselect the highest-confidence boxes in each image, which are treated as\npseudo-ground truth in the next training iteration. However, the errors of an\nimmature classifier can make the process drift, usually introducing many of\nfalse positives in the training dataset. To alleviate this problem, we propose\nin this paper a training protocol based on the self-paced learning paradigm.\nThe main idea is to iteratively select a subset of images and boxes that are\nthe most reliable, and use them for training. While in the past few years\nsimilar strategies have been adopted for SVMs and other classifiers, we are the\nfirst showing that a self-paced approach can be used with deep-network-based\nclassifiers in an end-to-end training pipeline. The method we propose is built\non the fully-supervised Fast-RCNN architecture and can be applied to similar\narchitectures which represent the input image as a bag of boxes. We show\nstate-of-the-art results on Pascal VOC 2007, Pascal VOC 2010 and ILSVRC 2013.\nOn ILSVRC 2013 our results based on a low-capacity AlexNet network outperform\neven those weakly-supervised approaches which are based on much higher-capacity\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 20:34:03 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 16:04:30 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 16:33:44 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Sangineto", "Enver", ""], ["Nabi", "Moin", ""], ["Culibrk", "Dubravko", ""], ["Sebe", "Nicu", ""]]}, {"id": "1605.07678", "submitter": "Alfredo Canziani", "authors": "Alfredo Canziani, Adam Paszke, Eugenio Culurciello", "title": "An Analysis of Deep Neural Network Models for Practical Applications", "comments": "7 pages, 10 figures, legend for Figure 2 got lost :/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the emergence of Deep Neural Networks (DNNs) as a prominent technique\nin the field of computer vision, the ImageNet classification challenge has\nplayed a major role in advancing the state-of-the-art. While accuracy figures\nhave steadily increased, the resource utilisation of winning models has not\nbeen properly taken into account. In this work, we present a comprehensive\nanalysis of important metrics in practical applications: accuracy, memory\nfootprint, parameters, operations count, inference time and power consumption.\nKey findings are: (1) power consumption is independent of batch size and\narchitecture; (2) accuracy and inference time are in a hyperbolic relationship;\n(3) energy constraint is an upper bound on the maximum achievable accuracy and\nmodel complexity; (4) the number of operations is a reliable estimate of the\ninference time. We believe our analysis provides a compelling set of\ninformation that helps design and engineer efficient DNNs.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 22:36:02 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 14:56:44 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 20:18:12 GMT"}, {"version": "v4", "created": "Fri, 14 Apr 2017 23:40:21 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Canziani", "Alfredo", ""], ["Paszke", "Adam", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1605.07681", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Lorenzo Torresani, Stella X. Yu, Jianbo Shi", "title": "Convolutional Random Walk Networks for Semantic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current semantic segmentation methods rely on fully convolutional\nnetworks (FCNs). However, their use of large receptive fields and many pooling\nlayers cause low spatial resolution inside the deep layers. This leads to\npredictions with poor localization around the boundaries. Prior work has\nattempted to address this issue by post-processing predictions with CRFs or\nMRFs. But such models often fail to capture semantic relationships between\nobjects, which causes spatially disjoint predictions. To overcome these\nproblems, recent methods integrated CRFs or MRFs into an FCN framework. The\ndownside of these new models is that they have much higher complexity than\ntraditional FCNs, which renders training and testing more challenging.\n  In this work we introduce a simple, yet effective Convolutional Random Walk\nNetwork (RWN) that addresses the issues of poor boundary localization and\nspatially fragmented predictions with very little increase in model complexity.\nOur proposed RWN jointly optimizes the objectives of pixelwise affinity and\nsemantic segmentation. It combines these two objectives via a novel random walk\nlayer that enforces consistent spatial grouping in the deep layers of the\nnetwork. Our RWN is implemented using standard convolution and matrix\nmultiplication. This allows an easy integration into existing FCN frameworks\nand it enables end-to-end training of the whole network via standard\nback-propagation. Our implementation of RWN requires just $131$ additional\nparameters compared to the traditional FCNs, and yet it consistently produces\nan improvement over the FCNs on semantic segmentation and scene labeling.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 23:00:52 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 16:10:04 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 17:49:21 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Bertasius", "Gedas", ""], ["Torresani", "Lorenzo", ""], ["Yu", "Stella X.", ""], ["Shi", "Jianbo", ""]]}, {"id": "1605.07686", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Qiang Liu, Lorenzo Torresani, Jianbo Shi", "title": "Local Perturb-and-MAP for Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional random fields (CRFs) provide a powerful tool for structured\nprediction, but cast significant challenges in both the learning and inference\nsteps. Approximation techniques are widely used in both steps, which should be\nconsidered jointly to guarantee good performance (a.k.a. \"inferning\").\nPerturb-and-MAP models provide a promising alternative to CRFs, but require\nglobal combinatorial optimization and hence they are usable only on specific\nmodels. In this work, we present a new Local Perturb-and-MAP (locPMAP)\nframework that replaces the global optimization with a local optimization by\nexploiting our observed connection between locPMAP and the pseudolikelihood of\nthe original CRF model. We test our approach on three different vision tasks\nand show that our method achieves consistently improved performance over other\napproximate inference techniques optimized to a pseudolikelihood objective.\nAdditionally, we demonstrate that we can integrate our method in the fully\nconvolutional network framework to increase our model's complexity. Finally,\nour observed connection between locPMAP and the pseudolikelihood leads to a\nnovel perspective for understanding and using pseudolikelihood.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2016 23:25:23 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 20:32:42 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Bertasius", "Gedas", ""], ["Liu", "Qiang", ""], ["Torresani", "Lorenzo", ""], ["Shi", "Jianbo", ""]]}, {"id": "1605.07699", "submitter": "Luming Zhang", "authors": "Yanxiang Chen, Yuxing Hu, Luming Zhang, Ping Li, and Chao Zhang", "title": "Engineering Deep Representations for Modeling Aesthetic Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many aesthetic models in computer vision suffer from two shortcomings: 1) the\nlow descriptiveness and interpretability of those hand-crafted aesthetic\ncriteria (i.e., nonindicative of region-level aesthetics), and 2) the\ndifficulty of engineering aesthetic features adaptively and automatically\ntoward different image sets. To remedy these problems, we develop a deep\narchitecture to learn aesthetically-relevant visual attributes from Flickr1,\nwhich are localized by multiple textual attributes in a weakly-supervised\nsetting. More specifically, using a bag-ofwords (BoW) representation of the\nfrequent Flickr image tags, a sparsity-constrained subspace algorithm discovers\na compact set of textual attributes (e.g., landscape and sunset) for each\nimage. Then, a weakly-supervised learning algorithm projects the textual\nattributes at image-level to the highly-responsive image patches at\npixel-level. These patches indicate where humans look at appealing regions with\nrespect to each textual attribute, which are employed to learn the visual\nattributes. Psychological and anatomical studies have shown that humans\nperceive visual concepts hierarchically. Hence, we normalize these patches and\nfeed them into a five-layer convolutional neural network (CNN) to mimick the\nhierarchy of human perceiving the visual attributes. We apply the learned deep\nfeatures on image retargeting, aesthetics ranking, and retrieval. Both\nsubjective and objective experimental results thoroughly demonstrate the\ncompetitiveness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 01:30:12 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 05:40:58 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Chen", "Yanxiang", ""], ["Hu", "Yuxing", ""], ["Zhang", "Luming", ""], ["Li", "Ping", ""], ["Zhang", "Chao", ""]]}, {"id": "1605.07708", "submitter": "Michael Milford", "authors": "James Mount, Michael Milford", "title": "2D Visual Place Recognition for Domestic Service Robots at Night", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domestic service robots such as lawn mowing and vacuum cleaning robots are\nthe most numerous consumer robots in existence today. While early versions\nemployed random exploration, recent systems fielded by most of the major\nmanufacturers have utilized range-based and visual sensors and user-placed\nbeacons to enable robots to map and localize. However, active range and visual\nsensing solutions have the disadvantages of being intrusive, expensive, or only\nproviding a 1D scan of the environment, while the requirement for beacon\nplacement imposes other practical limitations. In this paper we present a\npassive and potentially cheap vision-based solution to 2D localization at night\nthat combines easily obtainable day-time maps with low resolution\ncontrast-normalized image matching algorithms, image sequence-based matching in\ntwo-dimensions, place match interpolation and recent advances in conventional\nlow light camera technology. In a range of experiments over a domestic lawn and\nin a lounge room, we demonstrate that the proposed approach enables 2D\nlocalization at night, and analyse the effect on performance of varying\nodometry noise levels, place match interpolation and sequence matching length.\nFinally we benchmark the new low light camera technology and show how it can\nenable robust place recognition even in an environment lit only by a moonless\nsky, raising the tantalizing possibility of being able to apply all\nconventional vision algorithms, even in the darkest of nights.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 02:11:14 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Mount", "James", ""], ["Milford", "Michael", ""]]}, {"id": "1605.07716", "submitter": "Jingdong Wang", "authors": "Jingdong Wang, Zhen Wei, Ting Zhang, Wenjun Zeng", "title": "Deeply-Fused Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel deep learning approach, deeply-fused nets.\nThe central idea of our approach is deep fusion, i.e., combine the intermediate\nrepresentations of base networks, where the fused output serves as the input of\nthe remaining part of each base network, and perform such combinations deeply\nover several intermediate representations. The resulting deeply fused net\nenjoys several benefits. First, it is able to learn multi-scale representations\nas it enjoys the benefits of more base networks, which could form the same\nfused network, other than the initial group of base networks. Second, in our\nsuggested fused net formed by one deep and one shallow base networks, the flows\nof the information from the earlier intermediate layer of the deep base network\nto the output and from the input to the later intermediate layer of the deep\nbase network are both improved. Last, the deep and shallow base networks are\njointly learnt and can benefit from each other. More interestingly, the\nessential depth of a fused net composed from a deep base network and a shallow\nbase network is reduced because the fused net could be composed from a less\ndeep base network, and thus training the fused net is less difficult than\ntraining the initial deep base network. Empirical results demonstrate that our\napproach achieves superior performance over two closely-related methods, ResNet\nand Highway, and competitive performance compared to the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 03:35:11 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Wang", "Jingdong", ""], ["Wei", "Zhen", ""], ["Zhang", "Ting", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1605.07722", "submitter": "Longqi Yang", "authors": "Longqi Yang, Cheng-Kang Hsieh, Hongjian Yang, Nicola Dell, Serge\n  Belongie, Curtis Cole, Deborah Estrin", "title": "Yum-me: A Personalized Nutrient-based Meal Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nutrient-based meal recommendations have the potential to help individuals\nprevent or manage conditions such as diabetes and obesity. However, learning\npeople's food preferences and making recommendations that simultaneously appeal\nto their palate and satisfy nutritional expectations are challenging. Existing\napproaches either only learn high-level preferences or require a prolonged\nlearning period. We propose Yum-me, a personalized nutrient-based meal\nrecommender system designed to meet individuals' nutritional expectations,\ndietary restrictions, and fine-grained food preferences. Yum-me enables a\nsimple and accurate food preference profiling procedure via a visual quiz-based\nuser interface, and projects the learned profile into the domain of\nnutritionally appropriate food options to find ones that will appeal to the\nuser. We present the design and implementation of Yum-me, and further describe\nand evaluate two innovative contributions. The first contriution is an open\nsource state-of-the-art food image analysis model, named FoodDist. We\ndemonstrate FoodDist's superior performance through careful benchmarking and\ndiscuss its applicability across a wide array of dietary applications. The\nsecond contribution is a novel online learning framework that learns food\npreference from item-wise and pairwise image comparisons. We evaluate the\nframework in a field study of 227 anonymous users and demonstrate that it\noutperforms other baselines by a significant margin. We further conducted an\nend-to-end validation of the feasibility and effectiveness of Yum-me through a\n60-person user study, in which Yum-me improves the recommendation acceptance\nrate by 42.63%.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:13:49 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 14:48:18 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 17:43:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yang", "Longqi", ""], ["Hsieh", "Cheng-Kang", ""], ["Yang", "Hongjian", ""], ["Dell", "Nicola", ""], ["Belongie", "Serge", ""], ["Cole", "Curtis", ""], ["Estrin", "Deborah", ""]]}, {"id": "1605.07824", "submitter": "Amir Rosenfeld", "authors": "Amir Rosenfeld, Shimon Ullman", "title": "Action Classification via Concepts and Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classes in natural images tend to follow long tail distributions. This is\nproblematic when there are insufficient training examples for rare classes.\nThis effect is emphasized in compound classes, involving the conjunction of\nseveral concepts, such as those appearing in action-recognition datasets. In\nthis paper, we propose to address this issue by learning how to utilize common\nvisual concepts which are readily available. We detect the presence of\nprominent concepts in images and use them to infer the target labels instead of\nusing visual features directly, combining tools from vision and\nnatural-language processing. We validate our method on the recently introduced\nHICO dataset reaching a mAP of 31.54\\% and on the Stanford-40 Actions dataset,\nwhere the proposed method outperforms that obtained by direct visual features,\nobtaining an accuracy 83.12\\%. Moreover, the method provides for each class a\nsemantically meaningful list of keywords and relevant image regions relating it\nto its constituent concepts.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 11:06:58 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 02:48:27 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Rosenfeld", "Amir", ""], ["Ullman", "Shimon", ""]]}, {"id": "1605.07866", "submitter": "Martin Rajchl PhD", "authors": "Martin Rajchl, Matthew C.H. Lee, Ozan Oktay, Konstantinos Kamnitsas,\n  Jonathan Passerat-Palmbach, Wenjia Bai, Mellisa Damodaram, Mary A.\n  Rutherford, Joseph V. Hajnal, Bernhard Kainz, Daniel Rueckert", "title": "DeepCut: Object Segmentation from Bounding Box Annotations using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose DeepCut, a method to obtain pixelwise object\nsegmentations given an image dataset labelled with bounding box annotations. It\nextends the approach of the well-known GrabCut method to include machine\nlearning by training a neural network classifier from bounding box annotations.\nWe formulate the problem as an energy minimisation problem over a\ndensely-connected conditional random field and iteratively update the training\ntargets to obtain pixelwise object segmentations. Additionally, we propose\nvariants of the DeepCut method and compare those to a naive approach to CNN\ntraining under weak supervision. We test its applicability to solve brain and\nlung segmentation problems on a challenging fetal magnetic resonance dataset\nand obtain encouraging results in terms of accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 13:03:48 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 22:00:49 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Rajchl", "Martin", ""], ["Lee", "Matthew C. H.", ""], ["Oktay", "Ozan", ""], ["Kamnitsas", "Konstantinos", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Bai", "Wenjia", ""], ["Damodaram", "Mellisa", ""], ["Rutherford", "Mary A.", ""], ["Hajnal", "Joseph V.", ""], ["Kainz", "Bernhard", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1605.07912", "submitter": "Zhilin Yang", "authors": "Zhilin Yang, Ye Yuan, Yuexin Wu, Ruslan Salakhutdinov, William W.\n  Cohen", "title": "Review Networks for Caption Generation", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel extension of the encoder-decoder framework, called a\nreview network. The review network is generic and can enhance any existing\nencoder- decoder model: in this paper, we consider RNN decoders with both CNN\nand RNN encoders. The review network performs a number of review steps with\nattention mechanism on the encoder hidden states, and outputs a thought vector\nafter each review step; the thought vectors are used as the input of the\nattention mechanism in the decoder. We show that conventional encoder-decoders\nare a special case of our framework. Empirically, we show that our framework\nimproves over state-of- the-art encoder-decoder systems on the tasks of image\ncaptioning and source code captioning.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 14:49:58 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 00:47:21 GMT"}, {"version": "v3", "created": "Tue, 7 Jun 2016 01:39:35 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2016 17:50:27 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Yang", "Zhilin", ""], ["Yuan", "Ye", ""], ["Wu", "Yuexin", ""], ["Salakhutdinov", "Ruslan", ""], ["Cohen", "William W.", ""]]}, {"id": "1605.07960", "submitter": "Aijun Bai", "authors": "Aijun Bai", "title": "Multi-Object Tracking and Identification over Sets", "comments": "Draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability for an autonomous agent or robot to track and identify\npotentially multiple objects in a dynamic environment is essential for many\napplications, such as automated surveillance, traffic monitoring, human-robot\ninteraction, etc. The main challenge is due to the noisy and incomplete\nperception including inevitable false negative and false positive errors from a\nlow-level detector. In this paper, we propose a novel multi-object tracking and\nidentification over sets approach to address this challenge. We define joint\nstates and observations both as finite sets, and develop motion and observation\nfunctions accordingly. The object identification problem is then formulated and\nsolved by using expectation-maximization methods. The set formulation enables\nus to avoid directly performing observation-to-object association. We\nempirically confirm that the overall algorithm outperforms the state-of-the-art\nin a popular PETS dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 16:40:05 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Bai", "Aijun", ""]]}, {"id": "1605.08068", "submitter": "Alireza Shafaei", "authors": "Alireza Shafaei, James J. Little", "title": "Real-Time Human Motion Capture with Multiple Depth Cameras", "comments": "Accepted to computer robot vision 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly used human motion capture systems require intrusive attachment of\nmarkers that are visually tracked with multiple cameras. In this work we\npresent an efficient and inexpensive solution to markerless motion capture\nusing only a few Kinect sensors. Unlike the previous work on 3d pose estimation\nusing a single depth camera, we relax constraints on the camera location and do\nnot assume a co-operative user. We apply recent image segmentation techniques\nto depth images and use curriculum learning to train our system on purely\nsynthetic data. Our method accurately localizes body parts without requiring an\nexplicit shape model. The body joint locations are then recovered by combining\nevidence from multiple views in real-time. We also introduce a dataset of ~6\nmillion synthetic depth frames for pose estimation from multiple cameras and\nexceed state-of-the-art results on the Berkeley MHAD dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 20:52:28 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Shafaei", "Alireza", ""], ["Little", "James J.", ""]]}, {"id": "1605.08104", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "Deep Predictive Coding Networks for Video Prediction and Unsupervised\n  Learning", "comments": "Code and example video clips can be found here:\n  https://coxlab.github.io/prednet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While great strides have been made in using deep learning algorithms to solve\nsupervised learning tasks, the problem of unsupervised learning - leveraging\nunlabeled examples to learn about the structure of a domain - remains a\ndifficult unsolved challenge. Here, we explore prediction of future frames in a\nvideo sequence as an unsupervised learning rule for learning about the\nstructure of the visual world. We describe a predictive neural network\n(\"PredNet\") architecture that is inspired by the concept of \"predictive coding\"\nfrom the neuroscience literature. These networks learn to predict future frames\nin a video sequence, with each layer in the network making local predictions\nand only forwarding deviations from those predictions to subsequent network\nlayers. We show that these networks are able to robustly learn to predict the\nmovement of synthetic (rendered) objects, and that in doing so, the networks\nlearn internal representations that are useful for decoding latent object\nparameters (e.g. pose) that support object recognition with fewer training\nviews. We also show that these networks can scale to complex natural image\nstreams (car-mounted camera videos), capturing key aspects of both egocentric\nmovement and the movement of objects in the visual scene, and the\nrepresentation learned in this setting is useful for estimating the steering\nangle. Altogether, these results suggest that prediction represents a powerful\nframework for unsupervised learning, allowing for implicit learning of object\nand scene structure.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 23:58:55 GMT"}, {"version": "v2", "created": "Tue, 14 Jun 2016 14:36:09 GMT"}, {"version": "v3", "created": "Tue, 30 Aug 2016 00:08:34 GMT"}, {"version": "v4", "created": "Wed, 31 Aug 2016 16:06:03 GMT"}, {"version": "v5", "created": "Wed, 1 Mar 2017 01:00:54 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1605.08110", "submitter": "Ke Zhang", "authors": "Ke Zhang, Wei-Lun Chao, Fei Sha, Kristen Grauman", "title": "Video Summarization with Long Short-term Memory", "comments": "To appear in ECCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel supervised learning technique for summarizing videos by\nautomatically selecting keyframes or key subshots. Casting the problem as a\nstructured prediction problem on sequential data, our main idea is to use Long\nShort-Term Memory (LSTM), a special type of recurrent neural networks to model\nthe variable-range dependencies entailed in the task of video summarization.\nOur learning models attain the state-of-the-art results on two benchmark video\ndatasets. Detailed analysis justifies the design of the models. In particular,\nwe show that it is crucial to take into consideration the sequential structures\nin videos and model them. Besides advances in modeling techniques, we introduce\ntechniques to address the need of a large number of annotated data for training\ncomplex learning models. There, our main idea is to exploit the existence of\nauxiliary annotated video datasets, albeit heterogeneous in visual styles and\ncontents. Specifically, we show domain adaptation techniques can improve\nsummarization by reducing the discrepancies in statistical properties across\nthose datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 00:46:35 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2016 07:05:34 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Zhang", "Ke", ""], ["Chao", "Wei-Lun", ""], ["Sha", "Fei", ""], ["Grauman", "Kristen", ""]]}, {"id": "1605.08125", "submitter": "Waqas Sultani Mr", "authors": "Waqas Sultani and Mubarak Shah", "title": "Automatic Action Annotation in Weakly Labeled Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual spatio-temporal annotation of human action in videos is laborious,\nrequires several annotators and contains human biases. In this paper, we\npresent a weakly supervised approach to automatically obtain spatio-temporal\nannotations of an actor in action videos. We first obtain a large number of\naction proposals in each video. To capture a few most representative action\nproposals in each video and evade processing thousands of them, we rank them\nusing optical flow and saliency in a 3D-MRF based framework and select a few\nproposals using MAP based proposal subset selection method. We demonstrate that\nthis ranking preserves the high quality action proposals. Several such\nproposals are generated for each video of the same action. Our next challenge\nis to iteratively select one proposal from each video so that all proposals are\nglobally consistent. We formulate this as Generalized Maximum Clique Graph\nproblem using shape, global and fine grained similarity of proposals across the\nvideos. The output of our method is the most action representative proposals\nfrom each video. Our method can also annotate multiple instances of the same\naction in a video. We have validated our approach on three challenging action\ndatasets: UCF Sport, sub-JHMDB and THUMOS'13 and have obtained promising\nresults compared to several baseline methods. Moreover, on UCF Sports, we\ndemonstrate that action classifiers trained on these automatically obtained\nspatio-temporal annotations have comparable performance to the classifiers\ntrained on ground truth annotation.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 02:22:57 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Sultani", "Waqas", ""], ["Shah", "Mubarak", ""]]}, {"id": "1605.08140", "submitter": "Michael S. Ryoo", "authors": "AJ Piergiovanni, Chenyou Fan, Michael S. Ryoo", "title": "Learning Latent Sub-events in Activity Videos Using Temporal Attention\n  Filters", "comments": null, "journal-ref": "AAAI 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we newly introduce the concept of temporal attention filters,\nand describe how they can be used for human activity recognition from videos.\nMany high-level activities are often composed of multiple temporal parts (e.g.,\nsub-events) with different duration/speed, and our objective is to make the\nmodel explicitly learn such temporal structure using multiple attention filters\nand benefit from them. Our temporal filters are designed to be fully\ndifferentiable, allowing end-of-end training of the temporal filters together\nwith the underlying frame-based or segment-based convolutional neural network\narchitectures. This paper presents an approach of learning a set of optimal\nstatic temporal attention filters to be shared across different videos, and\nextends this approach to dynamically adjust attention filters per testing video\nusing recurrent long short-term memory networks (LSTMs). This allows our\ntemporal attention filters to learn latent sub-events specific to each\nactivity. We experimentally confirm that the proposed concept of temporal\nattention filters benefits the activity recognition, and we visualize the\nlearned latent sub-events.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 04:02:01 GMT"}, {"version": "v2", "created": "Wed, 21 Sep 2016 07:48:56 GMT"}, {"version": "v3", "created": "Mon, 26 Dec 2016 11:16:33 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Piergiovanni", "AJ", ""], ["Fan", "Chenyou", ""], ["Ryoo", "Michael S.", ""]]}, {"id": "1605.08151", "submitter": "Soravit Changpinyo", "authors": "Soravit Changpinyo, Wei-Lun Chao, Fei Sha", "title": "Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning", "comments": "ICCV2017 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging class semantic descriptions and examples of known objects,\nzero-shot learning makes it possible to train a recognition model for an object\nclass whose examples are not available. In this paper, we propose a novel\nzero-shot learning model that takes advantage of clustering structures in the\nsemantic embedding space. The key idea is to impose the structural constraint\nthat semantic representations must be predictive of the locations of their\ncorresponding visual exemplars. To this end, this reduces to training multiple\nkernel-based regressors from semantic representation-exemplar pairs from\nlabeled data of the seen object categories. Despite its simplicity, our\napproach significantly outperforms existing zero-shot learning methods on\nstandard benchmark datasets, including the ImageNet dataset with more than\n20,000 unseen categories.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 05:50:09 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 05:18:39 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Changpinyo", "Soravit", ""], ["Chao", "Wei-Lun", ""], ["Sha", "Fei", ""]]}, {"id": "1605.08153", "submitter": "Alexander Anderson", "authors": "Alexander G. Anderson, Cory P. Berg, Daniel P. Mossing, Bruno A.\n  Olshausen", "title": "DeepMovie: Using Optical Flow and Deep Neural Networks to Stylize Movies", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper by Gatys et al. describes a method for rendering an image in\nthe style of another image. First, they use convolutional neural network\nfeatures to build a statistical model for the style of an image. Then they\ncreate a new image with the content of one image but the style statistics of\nanother image. Here, we extend this method to render a movie in a given\nartistic style. The naive solution that independently renders each frame\nproduces poor results because the features of the style move substantially from\none frame to the next. The other naive method that initializes the optimization\nfor the next frame using the rendered version of the previous frame also\nproduces poor results because the features of the texture stay fixed relative\nto the frame of the movie instead of moving with objects in the scene. The main\ncontribution of this paper is to use optical flow to initialize the style\ntransfer optimization so that the texture features move with the objects in the\nvideo. Finally, we suggest a method to incorporate optical flow explicitly into\nthe cost function.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 05:52:10 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Anderson", "Alexander G.", ""], ["Berg", "Cory P.", ""], ["Mossing", "Daniel P.", ""], ["Olshausen", "Bruno A.", ""]]}, {"id": "1605.08154", "submitter": "Chongyang Wang", "authors": "Chongyang Wang, Ming Peng, Lingfeng Xu, Tong Chen", "title": "A single scale retinex based method for palm vein extraction", "comments": "4 pages, 4 figures, received by 2016 IEEE Information\n  Technology,Networking,Electronic and Automation Control Conference(ITNEC\n  2016)", "journal-ref": null, "doi": "10.1109/ITNEC.2016.7560322", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Palm vein recognition is a novel biometric identification technology. But how\nto gain a better vein extraction result from the raw palm image is still a\nchallenging problem, especially when the raw data collection has the problem of\nasymmetric illumination. This paper proposes a method based on single scale\nRetinex algorithm to extract palm vein image when strong shadow presents due to\nasymmetric illumination and uneven geometry of the palm. We test our method on\na multispectral palm image. The experimental result shows that the proposed\nmethod is robust to the influence of illumination angle and shadow. Compared to\nthe traditional extraction methods, the proposed method can obtain palm vein\nlines with better visualization performance (the contrast ratio increases by\n18.4%, entropy increases by 1.07%, and definition increases by 18.8%).\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 06:09:24 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Chongyang", ""], ["Peng", "Ming", ""], ["Xu", "Lingfeng", ""], ["Chen", "Tong", ""]]}, {"id": "1605.08163", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson, Mark R. Morelande", "title": "Multiple target tracking based on sets of trajectories", "comments": "MATLAB implementations of algorithms based on sets of trajectories\n  can be found at https://github.com/Agarciafernandez", "journal-ref": "in IEEE Transactions on Aerospace and Electronic Systems, vol. 56,\n  no. 3, pp. 1685-1707, June 2020", "doi": "10.1109/TAES.2019.2921210", "report-no": null, "categories": "cs.CV cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a solution of the multiple target tracking (MTT) problem based on\nsets of trajectories and the random finite set framework. A full Bayesian\napproach to MTT should characterise the distribution of the trajectories given\nthe measurements, as it contains all information about the trajectories. We\nattain this by considering multi-object density functions in which objects are\ntrajectories. For the standard tracking models, we also describe a conjugate\nfamily of multitrajectory density functions.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 06:52:13 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 11:07:40 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 08:02:43 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 07:40:29 GMT"}, {"version": "v5", "created": "Wed, 22 May 2019 07:14:04 GMT"}, {"version": "v6", "created": "Thu, 11 Jun 2020 15:05:00 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""], ["Morelande", "Mark R.", ""]]}, {"id": "1605.08179", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz, Robert Nishihara, Soumith Chintala, Bernhard\n  Sch\\\"olkopf, L\\'eon Bottou", "title": "Discovering Causal Signals in Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the existence of observable footprints that reveal the\n\"causal dispositions\" of the object categories appearing in collections of\nimages. We achieve this goal in two steps. First, we take a learning approach\nto observational causal discovery, and build a classifier that achieves\nstate-of-the-art performance on finding the causal direction between pairs of\nrandom variables, given samples from their joint distribution. Second, we use\nour causal direction classifier to effectively distinguish between features of\nobjects and features of their contexts in collections of static images. Our\nexperiments demonstrate the existence of a relation between the direction of\ncausality and the difference between objects and their contexts, and by the\nsame token, the existence of observable signals that reveal the causal\ndispositions of objects.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 07:36:56 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 11:14:18 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Lopez-Paz", "David", ""], ["Nishihara", "Robert", ""], ["Chintala", "Soumith", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bottou", "L\u00e9on", ""]]}, {"id": "1605.08247", "submitter": "Hirokatsu Kataoka", "authors": "Hirokatsu Kataoka and Yudai Miyashita and Tomoaki Yamabe and Soma\n  Shirakabe and Shin'ichi Sato and Hironori Hoshino and Ryo Kato and Kaori Abe\n  and Takaaki Imanari and Naomichi Kobayashi and Shinichiro Morita and Akio\n  Nakamura", "title": "cvpaper.challenge in 2015 - A review of CVPR2015 and DeepSurvey", "comments": "Survey Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"cvpaper.challenge\" is a group composed of members from AIST, Tokyo Denki\nUniv. (TDU), and Univ. of Tsukuba that aims to systematically summarize papers\non computer vision, pattern recognition, and related fields. For this\nparticular review, we focused on reading the ALL 602 conference papers\npresented at the CVPR2015, the premier annual computer vision event held in\nJune 2015, in order to grasp the trends in the field. Further, we are proposing\n\"DeepSurvey\" as a mechanism embodying the entire process from the reading\nthrough all the papers, the generation of ideas, and to the writing of paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 12:08:55 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Kataoka", "Hirokatsu", ""], ["Miyashita", "Yudai", ""], ["Yamabe", "Tomoaki", ""], ["Shirakabe", "Soma", ""], ["Sato", "Shin'ichi", ""], ["Hoshino", "Hironori", ""], ["Kato", "Ryo", ""], ["Abe", "Kaori", ""], ["Imanari", "Takaaki", ""], ["Kobayashi", "Naomichi", ""], ["Morita", "Shinichiro", ""], ["Nakamura", "Akio", ""]]}, {"id": "1605.08283", "submitter": "Thomas Wiatowski", "authors": "Thomas Wiatowski and Michael Tschannen and Aleksandar Stani\\'c and\n  Philipp Grohs and Helmut B\\\"olcskei", "title": "Discrete Deep Feature Extraction: A Theory and New Architectures", "comments": "Proc. of International Conference on Machine Learning (ICML), New\n  York, USA, June 2016, to appear", "journal-ref": "Proc. of International Conference on Machine Learning (ICML), New\n  York, USA, pp. 2149-2158, June 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT cs.NE math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First steps towards a mathematical theory of deep convolutional neural\nnetworks for feature extraction were made---for the continuous-time case---in\nMallat, 2012, and Wiatowski and B\\\"olcskei, 2015. This paper considers the\ndiscrete case, introduces new convolutional neural network architectures, and\nproposes a mathematical framework for their analysis. Specifically, we\nestablish deformation and translation sensitivity results of local and global\nnature, and we investigate how certain structural properties of the input\nsignal are reflected in the corresponding feature vectors. Our theory applies\nto general filters and general Lipschitz-continuous non-linearities and pooling\noperators. Experiments on handwritten digit classification and facial landmark\ndetection---including feature importance evaluation---complement the\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 13:55:07 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Wiatowski", "Thomas", ""], ["Tschannen", "Michael", ""], ["Stani\u0107", "Aleksandar", ""], ["Grohs", "Philipp", ""], ["B\u00f6lcskei", "Helmut", ""]]}, {"id": "1605.08313", "submitter": "Anvesha Amaravati", "authors": "Anvesha A, Shaojie Xu, Ningyuan Cao, Justin Romberg and Arijit\n  Raychowdhury", "title": "A Light-powered, Always-On, Smart Camera with Compressed Domain Gesture\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an energy-efficient camera-based gesture recognition\nsystem powered by light energy for \"always on\" applications. Low energy\nconsumption is achieved by directly extracting gesture features from the\ncompressed measurements, which are the block averages and the linear\ncombinations of the image sensor's pixel values. The gestures are recognized\nusing a nearest-neighbour (NN) classifier followed by Dynamic Time Warping\n(DTW). The system has been implemented on an Analog Devices Black Fin ULP\nvision processor and powered by PV cells whose output is regulated by TI's\nDC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured data\nreveals that with only 400 compressed measurements (768x compression ratio) per\nframe, the system is able to recognize key wake-up gestures with greater than\n80% accuracy and only 95mJ of energy per frame. Owing to its fully self-powered\noperation, the proposed system can find wide applications in \"always-on\" vision\nsystems such as in surveillance, robotics and consumer electronics with\ntouch-less operation.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 14:52:19 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 06:38:45 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["A", "Anvesha", ""], ["Xu", "Shaojie", ""], ["Cao", "Ningyuan", ""], ["Romberg", "Justin", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1605.08323", "submitter": "Marius Leordeanu", "authors": "Dragos Costea and Marius Leordeanu", "title": "Aerial image geolocalization from recognition and matching of roads and\n  intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial image analysis at a semantic level is important in many applications\nwith strong potential impact in industry and consumer use, such as automated\nmapping, urban planning, real estate and environment monitoring, or disaster\nrelief. The problem is enjoying a great interest in computer vision and remote\nsensing, due to increased computer power and improvement in automated image\nunderstanding algorithms. In this paper we address the task of automatic\ngeolocalization of aerial images from recognition and matching of roads and\nintersections. Our proposed method is a novel contribution in the literature\nthat could enable many applications of aerial image analysis when GPS data is\nnot available. We offer a complete pipeline for geolocalization, from the\ndetection of roads and intersections, to the identification of the enclosing\ngeographic region by matching detected intersections to previously learned\nmanually labeled ones, followed by accurate geometric alignment between the\ndetected roads and the manually labeled maps. We test on a novel dataset with\naerial images of two European cities and use the publicly available\nOpenStreetMap project for collecting ground truth roads annotations. We show in\nextensive experiments that our approach produces highly accurate localizations\nin the challenging case when we train on images from one city and test on the\nother and the quality of the aerial images is relatively poor. We also show\nthat the the alignment between detected roads and pre-stored manual annotations\ncan be effectively used for improving the quality of the road detection\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 15:11:09 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Costea", "Dragos", ""], ["Leordeanu", "Marius", ""]]}, {"id": "1605.08350", "submitter": "Tizita Nesibu Shewaye Mrs", "authors": "Tizita Nesibu Shewaye and Alhayat Ali Mekonnen", "title": "Benign-Malignant Lung Nodule Classification with Geometric and\n  Appearance Histogram Features", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer accounts for the highest number of cancer deaths globally. Early\ndiagnosis of lung nodules is very important to reduce the mortality rate of\npatients by improving the diagnosis and treatment of lung cancer. This work\nproposes an automated system to classify lung nodules as malignant and benign\nin CT images. It presents extensive experimental results using a combination of\ngeometric and histogram lung nodule image features and different linear and\nnon-linear discriminant classifiers. The proposed approach is experimentally\nvalidated on the LIDC-IDRI public lung cancer screening thoracic computed\ntomography (CT) dataset containing nodule level diagnostic data. The obtained\nresults are very encouraging correctly classifying 82% of malignant and 93% of\nbenign nodules on unseen test data at best.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 16:06:58 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Shewaye", "Tizita Nesibu", ""], ["Mekonnen", "Alhayat Ali", ""]]}, {"id": "1605.08359", "submitter": "Edward Johns", "authors": "Edward Johns and Stefan Leutenegger and Andrew J. Davison", "title": "Pairwise Decomposition of Image Sequences for Active Multi-View\n  Recognition", "comments": "CVPR 2016 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-view image sequence provides a much richer capacity for object\nrecognition than from a single image. However, most existing solutions to\nmulti-view recognition typically adopt hand-crafted, model-based geometric\nmethods, which do not readily embrace recent trends in deep learning. We\npropose to bring Convolutional Neural Networks to generic multi-view\nrecognition, by decomposing an image sequence into a set of image pairs,\nclassifying each pair independently, and then learning an object classifier by\nweighting the contribution of each pair. This allows for recognition over\narbitrary camera trajectories, without requiring explicit training over the\npotentially infinite number of camera paths and lengths. Building these\npairwise relationships then naturally extends to the next-best-view problem in\nan active recognition framework. To achieve this, we train a second\nConvolutional Neural Network to map directly from an observed image to next\nviewpoint. Finally, we incorporate this into a trajectory optimisation task,\nwhereby the best recognition confidence is sought for a given trajectory\nlength. We present state-of-the-art results in both guided and unguided\nmulti-view recognition on the ModelNet dataset, and show how our method can be\nused with depth images, greyscale images, or both.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 16:44:19 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Johns", "Edward", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1605.08397", "submitter": "Jiayong Liu", "authors": "Ke Wang, Jiayong Liu, Daniel Gonz\\'alez", "title": "Domain Transfer Multi-Instance Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we invest the domain transfer learning problem with\nmulti-instance data. We assume we already have a well-trained multi-instance\ndictionary and its corresponding classifier from the source domain, which can\nbe used to represent and classify the bags. But it cannot be directly used to\nthe target domain. Thus we propose to adapt them to the target domain by adding\nan adaptive term to the source domain classifier. The adaptive function is a\nlinear function based a domain transfer multi-instance dictionary. Given a\ntarget domain bag, we first map it to a bag-level feature space using the\ndomain transfer dictionary, and then apply a the linear adaptive function to\nits bag-level feature vector. To learn the domain-transfer dictionary and the\nadaptive function parameter, we simultaneously minimize the average\nclassification error of the target domain classifier over the target domain\ntraining set, and the complexities of both the adaptive function parameter and\nthe domain transfer dictionary. The minimization problem is solved by an\niterative algorithm which update the dictionary and the function parameter\nalternately. Experiments over several benchmark data sets show the advantage of\nthe proposed method over existing state-of-the-art domain transfer\nmulti-instance learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 18:28:49 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Wang", "Ke", ""], ["Liu", "Jiayong", ""], ["Gonz\u00e1lez", "Daniel", ""]]}, {"id": "1605.08401", "submitter": "Jameson Merkow", "authors": "Jameson Merkow and David Kriegman and Alison Marsden and Zhuowen Tu", "title": "Dense Volume-to-Volume Vascular Boundary Detection", "comments": "Accepted to MICCAI2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel 3D-Convolutional Neural Network (CNN)\narchitecture called I2I-3D that predicts boundary location in volumetric data.\nOur fine-to-fine, deeply supervised framework addresses three critical issues\nto 3D boundary detection: (1) efficient, holistic, end-to-end volumetric label\ntraining and prediction (2) precise voxel-level prediction to capture fine\nscale structures prevalent in medical data and (3) directed multi-scale,\nmulti-level feature learning. We evaluate our approach on a dataset consisting\nof 93 medical image volumes with a wide variety of anatomical regions and\nvascular structures. In the process, we also introduce HED-3D, a 3D extension\nof the state-of-the-art 2D edge detector (HED). We show that our deep learning\napproach out-performs, the current state-of-the-art in 3D vascular boundary\ndetection (structured forests 3D), by a large margin, as well as HED applied to\nslices, and HED-3D while successfully localizing fine structures. With our\napproach, boundary detection takes about one minute on a typical 512x512x512\nvolume.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 18:40:31 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Merkow", "Jameson", ""], ["Kriegman", "David", ""], ["Marsden", "Alison", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1605.08412", "submitter": "Tobias Strau{\\ss}", "authors": "Gundram Leifert and Tobias Strau{\\ss} and Tobias Gr\\\"uning and Roger\n  Labahn", "title": "CITlab ARGUS for historical handwritten documents", "comments": "Description of CITlab's System for the HTRtS 2015 Task : Handwritten\n  Text Recognition on the tranScriptorium Dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe CITlab's recognition system for the HTRtS competition attached to\nthe 13. International Conference on Document Analysis and Recognition, ICDAR\n2015. The task comprises the recognition of historical handwritten documents.\nThe core algorithms of our system are based on multi-dimensional recurrent\nneural networks (MDRNN) and connectionist temporal classification (CTC). The\nsoftware modules behind that as well as the basic utility technologies are\nessentially powered by PLANET's ARGUS framework for intelligent text\nrecognition and image processing.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 19:19:43 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Leifert", "Gundram", ""], ["Strau\u00df", "Tobias", ""], ["Gr\u00fcning", "Tobias", ""], ["Labahn", "Roger", ""]]}, {"id": "1605.08464", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Sule Yildirim-Yayilgan and Luc Van Gool", "title": "Low-Cost Scene Modeling using a Density Function Improves Segmentation\n  Performance", "comments": "accepted for publication at 25th IEEE International Symposium on\n  Robot and Human Interactive Communication (RO-MAN), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a low cost and effective way to combine a free simulation software\nand free CAD models for modeling human-object interaction in order to improve\nhuman & object segmentation. It is intended for research scenarios related to\nsafe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial\ndomain. The task of human and object modeling has been used for detecting\nactivity, and for inferring and predicting actions, different from those works,\nwe do human and object modeling in order to learn interactions in RGB-D data\nfor improving segmentation. For this purpose, we define a novel density\nfunction to model a three dimensional (3D) scene in a virtual environment\n(VREP). This density function takes into account various possible\nconfigurations of human-object and object-object relationships and interactions\ngoverned by their affordances. Using this function, we synthesize a large,\nrealistic and highly varied synthetic RGB-D dataset that we use for training.\nWe train a random forest classifier, and the pixelwise predictions obtained is\nintegrated as a unary term in a pairwise conditional random fields (CRF). Our\nevaluation shows that modeling these interactions improves segmentation\nperformance by ~7\\% in mean average precision and recall over state-of-the-art\nmethods that ignore these interactions in real-world data. Our approach is\ncomputationally efficient, robust and can run real-time on consumer hardware.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 22:34:37 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Sharma", "Vivek", ""], ["Yildirim-Yayilgan", "Sule", ""], ["Van Gool", "Luc", ""]]}, {"id": "1605.08470", "submitter": "Rajer Sindhu", "authors": "Rajer Sindhu", "title": "A Feature based Approach for Video Compression", "comments": "Conference on Image Recognition, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a high cost problem for panoramic image stitching via image matching\nalgorithm and not practical for real-time performance. In this paper, we take\nfull advantage ofHarris corner invariant characterization method light\nintensity parallel meaning, translation and rotation, and made a realtime\npanoramic image stitching algorithm. According to the basic characteristics and\nperformance FPGA classical algorithm, several modules such as the feature point\nextraction, and matching description is to optimize the feature-based logic.\nReal-time optimization system to achieve high precision match. The new\nalgorithm process the image from pixel domain and obtained from CCD camera\nXilinx Spartan-6 hardware platform. After the image stitching algorithm, will\neventually form a portable interface to output high-definition content on the\ndisplay. The results showed that, the proposed algorithm has higher precision\nwith good real-time performance and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 23:04:24 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Sindhu", "Rajer", ""]]}, {"id": "1605.08512", "submitter": "Milad Mohammadi", "authors": "Milad Mohammadi, Subhasis Das", "title": "SNN: Stacked Neural Networks", "comments": "8pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proven that transfer learning provides an easy way to achieve\nstate-of-the-art accuracies on several vision tasks by training a simple\nclassifier on top of features obtained from pre-trained neural networks. The\ngoal of this work is to generate better features for transfer learning from\nmultiple publicly available pre-trained neural networks. To this end, we\npropose a novel architecture called Stacked Neural Networks which leverages the\nfast training time of transfer learning while simultaneously being much more\naccurate. We show that using a stacked NN architecture can result in up to 8%\nimprovements in accuracy over state-of-the-art techniques using only one\npre-trained network for transfer learning. A second aim of this work is to make\nnetwork fine- tuning retain the generalizability of the base network to unseen\ntasks. To this end, we propose a new technique called \"joint fine-tuning\" that\nis able to give accuracies comparable to finetuning the same network\nindividually over two datasets. We also show that a jointly finetuned network\ngeneralizes better to unseen tasks when compared to a network finetuned over a\nsingle task.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 06:02:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Mohammadi", "Milad", ""], ["Das", "Subhasis", ""]]}, {"id": "1605.08543", "submitter": "Sam Leroux", "authors": "Sam Leroux, Steven Bohez, Cedric De Boom, Elias De Coninck, Tim\n  Verbelen, Bert Vankeirsbilck, Pieter Simoens, Bart Dhoedt", "title": "Lazy Evaluation of Convolutional Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a technique which avoids the evaluation of certain\nconvolutional filters in a deep neural network. This allows to trade-off the\naccuracy of a deep neural network with the computational and memory\nrequirements. This is especially important on a constrained device unable to\nhold all the weights of the network in memory.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 08:49:21 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Leroux", "Sam", ""], ["Bohez", "Steven", ""], ["De Boom", "Cedric", ""], ["De Coninck", "Elias", ""], ["Verbelen", "Tim", ""], ["Vankeirsbilck", "Bert", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1605.08680", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Duc-Son Pham, Ognjen Arandjelovic, Svetha Venkatesh", "title": "Achieving stable subspace clustering by post-processing generic\n  clustering results", "comments": "International Joint Conference on Neural Networks, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective subspace selection scheme as a post-processing step\nto improve results obtained by sparse subspace clustering (SSC). Our method\nstarts by the computation of stable subspaces using a novel random sampling\nscheme. Thus constructed preliminary subspaces are used to identify the\ninitially incorrectly clustered data points and then to reassign them to more\nsuitable clusters based on their goodness-of-fit to the preliminary model. To\nimprove the robustness of the algorithm, we use a dominant nearest subspace\nclassification scheme that controls the level of sensitivity against\nreassignment. We demonstrate that our algorithm is convergent and superior to\nthe direct application of a generic alternative such as principal component\nanalysis. On several popular datasets for motion segmentation and face\nclustering pervasively used in the sparse subspace clustering literature the\nproposed method is shown to reduce greatly the incidence of clustering errors\nwhile introducing negligible disturbance to the data points already correctly\nclustered.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 15:15:04 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Pham", "Duc-Son", ""], ["Arandjelovic", "Ognjen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1605.08764", "submitter": "Nazneen Fatema Rajani", "authors": "Nazneen Fatema Rajani and Raymond J. Mooney", "title": "Stacking With Auxiliary Features", "comments": "arXiv admin note: substantial text overlap with arXiv:1604.04802", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembling methods are well known for improving prediction accuracy. However,\nthey are limited in the sense that they cannot discriminate among component\nmodels effectively. In this paper, we propose stacking with auxiliary features\nthat learns to fuse relevant information from multiple systems to improve\nperformance. Auxiliary features enable the stacker to rely on systems that not\njust agree on an output but also the provenance of the output. We demonstrate\nour approach on three very different and difficult problems -- the Cold Start\nSlot Filling, the Tri-lingual Entity Discovery and Linking and the ImageNet\nobject detection tasks. We obtain new state-of-the-art results on the first two\ntasks and substantial improvements on the detection task, thus verifying the\npower and generality of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 19:31:54 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Rajani", "Nazneen Fatema", ""], ["Mooney", "Raymond J.", ""]]}, {"id": "1605.08831", "submitter": "Falong Shen", "authors": "Falong Shen, Gang Zeng", "title": "Weighted Residuals for Very Deep Networks", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks have recently shown appealing performance on many\nchallenging computer vision tasks. However, the original residual structure\nstill has some defects making it difficult to converge on very deep networks.\nIn this paper, we introduce a weighted residual network to address the\nincompatibility between \\texttt{ReLU} and element-wise addition and the deep\nnetwork initialization problem. The weighted residual network is able to learn\nto combine residuals from different layers effectively and efficiently. The\nproposed models enjoy a consistent improvement over accuracy and convergence\nwith increasing depths from 100+ layers to 1000+ layers. Besides, the weighted\nresidual networks have little more computation and GPU memory burden than the\noriginal residual networks. The networks are optimized by projected stochastic\ngradient descent. Experiments on CIFAR-10 have shown that our algorithm has a\n\\emph{faster convergence speed} than the original residual networks and reaches\na \\emph{high accuracy} at 95.3\\% with a 1192-layer model.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 02:13:32 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Shen", "Falong", ""], ["Zeng", "Gang", ""]]}, {"id": "1605.08856", "submitter": "R Vivek", "authors": "Siddu P Algur, N H Ayachit, Vivek R", "title": "A Channelized Binning Method for Extraction of Dominant Color Pixel\n  Value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Color is one of the most important and easily identifiable features for\ndescribing the visual content. The MPEG standard has developed a number of\ndescriptors that covers different aspects of the visual content. The Dominant\ncolor descriptor is one of them. This paper proposes a channelized binning\napproach a novel method for extraction of the dominant color pixel value which\nis a variant of the dominant color descriptor. The Channelized binning method\ntreats the problem as a statistical problem and tries to avoid color\nquantization and interpolation guessing of number and centroid of dominant\ncolors. Channelized binning is an iterative approach which automatically\nestimates the number of dominant pixel values and their centroids. It operates\non 24 bit full RGB color space, by considering one color channel at a time and\nhence avoiding the color quantization. Results show that the proposed method\ncan successfully extract dominant color pixel values.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 07:18:51 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Algur", "Siddu P", ""], ["Ayachit", "N H", ""], ["R", "Vivek", ""]]}, {"id": "1605.08857", "submitter": "R Vivek", "authors": "Siddu P Algur, Vivek R", "title": "Video Key Frame Extraction using Entropy value as Global and Local\n  Feature", "comments": "Key Frame Extraction, Entropy value", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key frames play an important role in video annotation. It is one of the\nwidely used methods for video abstraction as this will help us for processing a\nlarge set of video data with sufficient content representation in faster way.\nIn this paper a novel approach for key-frame extraction using entropy value is\nproposed. The proposed approach classifies frames based on entropy values as\nglobal feature and selects frame from each class as representative key-frame.\nIt also eliminates redundant frames from selected key-frames using entropy\nvalue as local feature. Evaluation of the approach on several video clips has\nbeen presented. Results show that the algorithm is successful in helping\nannotators automatically identify video key-frames.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 07:22:55 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Algur", "Siddu P", ""], ["R", "Vivek", ""]]}, {"id": "1605.08881", "submitter": "Jing Wang", "authors": "Risheng Liu, Jing Wang, Yiyang Wang, Zhixun Su, Yu Cai", "title": "Sparse Coding and Counting for Robust Visual Tracking", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0168093", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel sparse coding and counting method under\nBayesian framwork for visual tracking. In contrast to existing methods, the\nproposed method employs the combination of L0 and L1 norm to regularize the\nlinear coefficients of incrementally updated linear basis. The sparsity\nconstraint enables the tracker to effectively handle difficult challenges, such\nas occlusion or image corruption. To achieve realtime processing, we propose a\nfast and efficient numerical algorithm for solving the proposed model. Although\nit is an NP-hard problem, the proposed accelerated proximal gradient (APG)\napproach is guaranteed to converge to a solution quickly. Besides, we provide a\nclosed solution of combining L0 and L1 regularized representation to obtain\nbetter sparsity. Experimental results on challenging video sequences\ndemonstrate that the proposed method achieves state-of-the-art results both in\naccuracy and speed.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 12:09:53 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Liu", "Risheng", ""], ["Wang", "Jing", ""], ["Wang", "Yiyang", ""], ["Su", "Zhixun", ""], ["Cai", "Yu", ""]]}, {"id": "1605.08912", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Vinay Venkataraman, Karthikeyan Natesan Ramamurthy,\n  Pavan Turaga", "title": "A Riemannian Framework for Statistical Analysis of Topological\n  Persistence Diagrams", "comments": "Accepted at DiffCVML 2016 (CVPR 2016 Workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis is becoming a popular way to study high dimensional\nfeature spaces without any contextual clues or assumptions. This paper concerns\nitself with one popular topological feature, which is the number of\n$d-$dimensional holes in the dataset, also known as the Betti$-d$ number. The\npersistence of the Betti numbers over various scales is encoded into a\npersistence diagram (PD), which indicates the birth and death times of these\nholes as scale varies. A common way to compare PDs is by a point-to-point\nmatching, which is given by the $n$-Wasserstein metric. However, a big drawback\nof this approach is the need to solve correspondence between points before\ncomputing the distance; for $n$ points, the complexity grows according to\n$\\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new framework\nbuilt on Riemannian geometry, that models PDs as 2D probability density\nfunctions that are represented in the square-root framework on a Hilbert\nSphere. The resulting space is much more intuitive with closed form expressions\nfor common operations. The distance metric is 1) correspondence-free and also\n2) independent of the number of points in the dataset. The complexity of\ncomputing distance between PDs now grows according to $\\mathcal{O}(K^2)$, for a\n$K \\times K$ discretization of $[0,1]^2$. This also enables the use of existing\nmachinery in differential geometry towards statistical analysis of PDs such as\ncomputing the mean, geodesics, classification etc. We report competitive\nresults with the Wasserstein metric, at a much lower computational load,\nindicating the favorable properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 16:55:40 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Anirudh", "Rushil", ""], ["Venkataraman", "Vinay", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Turaga", "Pavan", ""]]}, {"id": "1605.09016", "submitter": "Seyed Mohsen Shojaee", "authors": "Seyed Mohsen Shojaee and Mahdieh Soleymani Baghshah", "title": "Semi-supervised Zero-Shot Learning by a Clustering-based Approach", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some of object recognition problems, labeled data may not be available for\nall categories. Zero-shot learning utilizes auxiliary information (also called\nsignatures) describing each category in order to find a classifier that can\nrecognize samples from categories with no labeled instance. In this paper, we\npropose a novel semi-supervised zero-shot learning method that works on an\nembedding space corresponding to abstract deep visual features. We seek a\nlinear transformation on signatures to map them onto the visual features, such\nthat the mapped signatures of the seen classes are close to labeled samples of\nthe corresponding classes and unlabeled data are also close to the mapped\nsignatures of one of the unseen classes.\n  We use the idea that the rich deep visual features provide a representation\nspace in which samples of each class are usually condensed in a cluster. The\neffectiveness of the proposed method is demonstrated through extensive\nexperiments on four public benchmarks improving the state-of-the-art prediction\naccuracy on three of them.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 15:33:15 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 17:06:25 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Shojaee", "Seyed Mohsen", ""], ["Baghshah", "Mahdieh Soleymani", ""]]}, {"id": "1605.09062", "submitter": "S Shankar", "authors": "Yoad Lewenberg, Yoram Bachrach, Sukrit Shankar, Antonio Criminisi", "title": "Predicting Personal Traits from Facial Images using Convolutional Neural\n  Networks Augmented with Facial Landmark Information", "comments": "7 pages, 5 figures, IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of predicting various traits of a person given an image\nof their face. We estimate both objective traits, such as gender, ethnicity and\nhair-color; as well as subjective traits, such as the emotion a person\nexpresses or whether he is humorous or attractive. For sizeable\nexperimentation, we contribute a new Face Attributes Dataset (FAD), having\nroughly 200,000 attribute labels for the above traits, for over 10,000 facial\nimages. Due to the recent surge of research on Deep Convolutional Neural\nNetworks (CNNs), we begin by using a CNN architecture for estimating facial\nattributes and show that they indeed provide an impressive baseline\nperformance. To further improve performance, we propose a novel approach that\nincorporates facial landmark information for input images as an additional\nchannel, helping the CNN learn better attribute-specific features so that the\nlandmarks across various training images hold correspondence. We empirically\nanalyse the performance of our method, showing consistent improvement over the\nbaseline across traits.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 21:07:10 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Lewenberg", "Yoad", ""], ["Bachrach", "Yoram", ""], ["Shankar", "Sukrit", ""], ["Criminisi", "Antonio", ""]]}, {"id": "1605.09085", "submitter": "Matthew Blaschko", "authors": "Amal Rannen Triki and Matthew B. Blaschko", "title": "Stochastic Function Norm Regularization of Deep Networks", "comments": "arXiv admin note: text overlap with arXiv:1710.06703", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have had an enormous impact on image analysis.\nState-of-the-art training methods, based on weight decay and DropOut, result in\nimpressive performance when a very large training set is available. However,\nthey tend to have large problems overfitting to small data sets. Indeed, the\navailable regularization methods deal with the complexity of the network\nfunction only indirectly. In this paper, we study the feasibility of directly\nusing the $L_2$ function norm for regularization. Two methods to integrate this\nnew regularization in the stochastic backpropagation are proposed. Moreover,\nthe convergence of these new algorithms is studied. We finally show that they\noutperform the state-of-the-art methods in the low sample regime on benchmark\ndatasets (MNIST and CIFAR10). The obtained results demonstrate very clear\nimprovement, especially in the context of small sample regimes with data laying\nin a low dimensional manifold. Source code of the method can be found at\n\\url{https://github.com/AmalRT/DNN_Reg}.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 01:49:18 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 14:14:30 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 14:38:32 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Triki", "Amal Rannen", ""], ["Blaschko", "Matthew B.", ""]]}, {"id": "1605.09116", "submitter": "Zhifeng Pang", "authors": "Baoli Shi, Zhi-Feng Pang and Jing Xu", "title": "Image segmentation based on the hybrid total variation model and the\n  K-means clustering strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of image segmentation highly relies on the original inputting\nimage. When the image is contaminated by some noises or blurs, we can not\nobtain the efficient segmentation result by using direct segmentation methods.\nIn order to efficiently segment the contaminated image, this paper proposes a\ntwo step method based on the hybrid total variation model with a box constraint\nand the K-means clustering method. In the first step, the hybrid model is based\non the weighted convex combination between the total variation functional and\nthe high-order total variation as the regularization term to obtain the\noriginal clustering data. In order to deal with non-smooth regularization term,\nwe solve this model by employing the alternating split Bregman method. Then, in\nthe second step, the segmentation can be obtained by thresholding this\nclustering data into different phases, where the thresholds can be given by\nusing the K-means clustering method. Numerical comparisons show that our\nproposed model can provide more efficient segmentation results dealing with the\nnoise image and blurring image.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 06:50:31 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Shi", "Baoli", ""], ["Pang", "Zhi-Feng", ""], ["Xu", "Jing", ""]]}, {"id": "1605.09128", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Valliappa Chockalingam, Satinder Singh, Honglak Lee", "title": "Control of Memory, Active Perception, and Action in Minecraft", "comments": "ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new set of reinforcement learning (RL) tasks in\nMinecraft (a flexible 3D world). We then use these tasks to systematically\ncompare and contrast existing deep reinforcement learning (DRL) architectures\nwith our new memory-based DRL architectures. These tasks are designed to\nemphasize, in a controllable manner, issues that pose challenges for RL methods\nincluding partial observability (due to first-person visual observations),\ndelayed rewards, high-dimensional visual observations, and the need to use\nactive perception in a correct manner so as to perform well in the tasks. While\nthese tasks are conceptually simple to describe, by virtue of having all of\nthese challenges simultaneously they are difficult for current DRL\narchitectures. Additionally, we evaluate the generalization performance of the\narchitectures on environments not used during training. The experimental\nresults show that our new architectures generalize to unseen environments\nbetter than existing DRL architectures.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 07:40:13 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Oh", "Junhyuk", ""], ["Chockalingam", "Valliappa", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""]]}, {"id": "1605.09136", "submitter": "Gianni Franchi Gianni Franchi", "authors": "Gianni Franchi, Jesus Angulo, and Dino Sejdinovic", "title": "Hyperspectral Image Classification with Support Vector Machines on\n  Kernel Distribution Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for pixel classification in hyperspectral images,\nleveraging on both the spatial and spectral information in the data. The\nintroduced method relies on a recently proposed framework for learning on\ndistributions -- by representing them with mean elements in reproducing kernel\nHilbert spaces (RKHS) and formulating a classification algorithm therein. In\nparticular, we associate each pixel to an empirical distribution of its\nneighbouring pixels, a judicious representation of which in an RKHS, in\nconjunction with the spectral information contained in the pixel itself, give a\nnew explicit set of features that can be fed into a suite of standard\nclassification techniques -- we opt for a well-established framework of support\nvector machines (SVM). Furthermore, the computational complexity is reduced via\nrandom Fourier features formalism. We study the consistency and the convergence\nrates of the proposed method and the experiments demonstrate strong performance\non hyperspectral data with gains in comparison to the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 08:26:28 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Franchi", "Gianni", ""], ["Angulo", "Jesus", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1605.09211", "submitter": "Brendan Jou", "authors": "Brendan Jou and Shih-Fu Chang", "title": "Going Deeper for Multilingual Visual Sentiment Detection", "comments": "technical report, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report details several improvements to the visual concept\ndetector banks built on images from the Multilingual Visual Sentiment Ontology\n(MVSO). The detector banks are trained to detect a total of 9,918\nsentiment-biased visual concepts from six major languages: English, Spanish,\nItalian, French, German and Chinese. In the original MVSO release,\nadjective-noun pair (ANP) detectors were trained for the six languages using an\nAlexNet-styled architecture by fine-tuning from DeepSentiBank. Here, through a\nmore extensive set of experiments, parameter tuning, and training runs, we\ndetail and release higher accuracy models for detecting ANPs across six\nlanguages from the same image pool and setting as in the original release using\na more modern architecture, GoogLeNet, providing comparable or better\nperformance with reduced network parameter cost.\n  In addition, since the image pool in MVSO can be corrupted by user noise from\nsocial interactions, we partitioned out a sub-corpus of MVSO images based on\ntag-restricted queries for higher fidelity labels. We show that as a result of\nthese higher fidelity labels, higher performing AlexNet-styled ANP detectors\ncan be trained using the tag-restricted image subset as compared to the models\nin full corpus. We release all these newly trained models for public research\nuse along with the list of tag-restricted images from the MVSO dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 12:57:44 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1605.09299", "submitter": "Eirikur Agustsson", "authors": "Eirikur Agustsson, Radu Timofte and Luc Van Gool", "title": "k2-means for fast and accurate large scale clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose k^2-means, a new clustering method which efficiently copes with\nlarge numbers of clusters and achieves low energy solutions. k^2-means builds\nupon the standard k-means (Lloyd's algorithm) and combines a new strategy to\naccelerate the convergence with a new low time complexity divisive\ninitialization. The accelerated convergence is achieved through only looking at\nk_n nearest clusters and using triangle inequality bounds in the assignment\nstep while the divisive initialization employs an optimal 2-clustering along a\ndirection. The worst-case time complexity per iteration of our k^2-means is\nO(nk_nd+k^2d), where d is the dimension of the n data points and k is the\nnumber of clusters and usually n << k << k_n. Compared to k-means' O(nkd)\ncomplexity, our k^2-means complexity is significantly lower, at the expense of\nslightly increasing the memory complexity by O(nk_n+k^2). In our extensive\nexperiments k^2-means is order(s) of magnitude faster than standard methods in\ncomputing accurate clusterings on several standard datasets and settings with\nhundreds of clusters and high dimensional data. Moreover, the proposed divisive\ninitialization generally leads to clustering energies comparable to those\nachieved with the standard k-means++ initialization, while being significantly\nfaster.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 16:17:45 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Agustsson", "Eirikur", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1605.09304", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Alexey Dosovitskiy, Jason Yosinski, Thomas Brox, Jeff\n  Clune", "title": "Synthesizing the preferred inputs for neurons in neural networks via\n  deep generator networks", "comments": "29 pages, 35 figures, NIPS camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated state-of-the-art results on\nmany pattern recognition tasks, especially vision classification problems.\nUnderstanding the inner workings of such computational brains is both\nfascinating basic science that is interesting in its own right - similar to why\nwe study the human brain - and will enable researchers to further improve DNNs.\nOne path to understanding how a neural network functions internally is to study\nwhat each of its neurons has learned to detect. One such method is called\nactivation maximization (AM), which synthesizes an input (e.g. an image) that\nhighly activates a neuron. Here we dramatically improve the qualitative state\nof the art of activation maximization by harnessing a powerful, learned prior:\na deep generator network (DGN). The algorithm (1) generates qualitatively\nstate-of-the-art synthetic images that look almost real, (2) reveals the\nfeatures learned by each neuron in an interpretable way, (3) generalizes well\nto new datasets and somewhat well to different network architectures without\nrequiring the prior to be relearned, and (4) can be considered as a\nhigh-quality generative method (in this case, by generating novel, creative,\ninteresting, recognizable images).\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 16:22:54 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 15:52:04 GMT"}, {"version": "v3", "created": "Mon, 6 Jun 2016 17:34:59 GMT"}, {"version": "v4", "created": "Thu, 27 Oct 2016 22:16:07 GMT"}, {"version": "v5", "created": "Wed, 23 Nov 2016 18:41:12 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Nguyen", "Anh", ""], ["Dosovitskiy", "Alexey", ""], ["Yosinski", "Jason", ""], ["Brox", "Thomas", ""], ["Clune", "Jeff", ""]]}, {"id": "1605.09332", "submitter": "Ludovic Trottier", "authors": "Ludovic Trottier, Philippe Gigu\\`ere, Brahim Chaib-draa", "title": "Parametric Exponential Linear Unit for Deep Convolutional Neural\n  Networks", "comments": "16th IEEE International Conference On Machine Learning And\n  Applications, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition is an important task for improving the ability of visual\nsystems to perform complex scene understanding. Recently, the Exponential\nLinear Unit (ELU) has been proposed as a key component for managing bias shift\nin Convolutional Neural Networks (CNNs), but defines a parameter that must be\nset by hand. In this paper, we propose learning a parameterization of ELU in\norder to learn the proper activation shape at each layer in the CNNs. Our\nresults on the MNIST, CIFAR-10/100 and ImageNet datasets using the NiN,\nOverfeat, All-CNN and ResNet networks indicate that our proposed Parametric ELU\n(PELU) has better performances than the non-parametric ELU. We have observed as\nmuch as a 7.28% relative error improvement on ImageNet with the NiN network,\nwith only 0.0003% parameter increase. Our visual examination of the non-linear\nbehaviors adopted by Vgg using PELU shows that the network took advantage of\nthe added flexibility by learning different activations at different layers.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 17:16:40 GMT"}, {"version": "v2", "created": "Tue, 31 May 2016 19:24:04 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 20:26:25 GMT"}, {"version": "v4", "created": "Wed, 10 Jan 2018 15:18:48 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Trottier", "Ludovic", ""], ["Gigu\u00e8re", "Philippe", ""], ["Chaib-draa", "Brahim", ""]]}, {"id": "1605.09336", "submitter": "Haomiao Jiang", "authors": "Haomiao Jiang, Qiyuan Tian, Joyce Farrell, Brian Wandell", "title": "Learning the image processing pipeline", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2017.2713942", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many creative ideas are being proposed for image sensor designs, and these\nmay be useful in applications ranging from consumer photography to computer\nvision. To understand and evaluate each new design, we must create a\ncorresponding image processing pipeline that transforms the sensor data into a\nform that is appropriate for the application. The need to design and optimize\nthese pipelines is time-consuming and costly. We explain a method that combines\nmachine learning and image systems simulation that automates the pipeline\ndesign. The approach is based on a new way of thinking of the image processing\npipeline as a large collection of local linear filters. We illustrate how the\nmethod has been used to design pipelines for novel sensor architectures in\nconsumer photography applications.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 17:28:02 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Jiang", "Haomiao", ""], ["Tian", "Qiyuan", ""], ["Farrell", "Joyce", ""], ["Wandell", "Brian", ""]]}, {"id": "1605.09410", "submitter": "Mengye Ren", "authors": "Mengye Ren, Richard S. Zemel", "title": "End-to-End Instance Segmentation with Recurrent Attention", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While convolutional neural networks have gained impressive success recently\nin solving structured prediction problems such as semantic segmentation, it\nremains a challenge to differentiate individual object instances in the scene.\nInstance segmentation is very important in a variety of applications, such as\nautonomous driving, image captioning, and visual question answering. Techniques\nthat combine large graphical models with low-level vision have been proposed to\naddress this problem; however, we propose an end-to-end recurrent neural\nnetwork (RNN) architecture with an attention mechanism to model a human-like\ncounting process, and produce detailed instance segmentations. The network is\njointly trained to sequentially produce regions of interest as well as a\ndominant object segmentation within each region. The proposed model achieves\ncompetitive results on the CVPPP, KITTI, and Cityscapes datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 20:40:20 GMT"}, {"version": "v2", "created": "Tue, 6 Sep 2016 15:09:06 GMT"}, {"version": "v3", "created": "Sun, 27 Nov 2016 17:41:57 GMT"}, {"version": "v4", "created": "Mon, 16 Jan 2017 23:08:35 GMT"}, {"version": "v5", "created": "Thu, 13 Jul 2017 00:53:33 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ren", "Mengye", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1605.09441", "submitter": "Harishchandra Dubey", "authors": "Harishchandra Dubey, Nandita, and Ashutosh Kumar Tiwari", "title": "Blind Modulation Classification based on MLP and PNN", "comments": "6 Pages, 12 Figures, 12 Tables", "journal-ref": "Harishchandra Dubey, Nandita and A. K. Tiwari, \"Blind modulation\n  classification based on MLP and PNN,\" Engineering and Systems (SCES), 2012\n  Students Conference on, llahabad, Uttar Pradesh, 2012, pp. 1-6", "doi": "10.1109/SCES.2012.6199042", "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a pattern recognition system is investigated for blind\nautomatic classification of digitally modulated communication signals. The\nproposed technique is able to discriminate the type of modulation scheme which\nis eventually used for demodulation followed by information extraction. The\nproposed system is composed of two subsystems namely feature extraction\nsub-system (FESS) and classifier sub-system (CSS). The FESS consists of\ncontinuous wavelet transform (CWT) for feature generation and principal\ncomponent analysis (PCA) for selection of the feature subset which is rich in\ndiscriminatory information. The CSS uses the selected features to accurately\nclassify the modulation class of the received signal. The proposed technique\nuses probabilistic neural network (PNN) and multilayer perceptron forward\nneural network (MLPFN) for comparative study of their recognition ability. PNN\nhave been found to perform better in terms of classification accuracy as well\nas testing and training time than MLPFN. The proposed approach is robust to\npresence of phase offset and additive Gaussian noise.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 23:00:32 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Dubey", "Harishchandra", ""], ["Nandita", "", ""], ["Tiwari", "Ashutosh Kumar", ""]]}, {"id": "1605.09451", "submitter": "Flora Ponjou Tasse", "authors": "Flora Ponjou Tasse and Ji\\v{r}\\'i Kosinka and Neil Anthony Dodgson", "title": "Quantitative Analysis of Saliency Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous saliency detection research required the reader to evaluate\nperformance qualitatively, based on renderings of saliency maps on a few\nshapes. This qualitative approach meant it was unclear which saliency models\nwere better, or how well they compared to human perception. This paper provides\na quantitative evaluation framework that addresses this issue. In the first\nquantitative analysis of 3D computational saliency models, we evaluate four\ncomputational saliency models and two baseline models against ground-truth\nsaliency collected in previous work.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 00:33:04 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Tasse", "Flora Ponjou", ""], ["Kosinka", "Ji\u0159\u00ed", ""], ["Dodgson", "Neil Anthony", ""]]}, {"id": "1605.09452", "submitter": "Yang Liu", "authors": "Yang Liu, Minh Hoai, Mang Shao, Tae-Kyun Kim", "title": "Latent Bi-constraint SVM for Video-based Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the task of recognizing objects from video input. This important\nproblem is relatively unexplored, compared with image-based object recognition.\nTo this end, we make the following contributions. First, we introduce two\ncomprehensive datasets for video-based object recognition. Second, we propose\nLatent Bi-constraint SVM (LBSVM), a maximum-margin framework for video-based\nobject recognition. LBSVM is based on Structured-Output SVM, but extends it to\nhandle noisy video data and ensure consistency of the output decision\nthroughout time. We apply LBSVM to recognize office objects and museum\nsculptures, and we demonstrate its benefits over image-based, set-based, and\nother video-based object recognition.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 00:34:37 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Liu", "Yang", ""], ["Hoai", "Minh", ""], ["Shao", "Mang", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1605.09507", "submitter": "Yoonchang Han", "authors": "Yoonchang Han, Jaehun Kim, Kyogu Lee", "title": "Deep convolutional neural networks for predominant instrument\n  recognition in polyphonic music", "comments": "13 pages, 7 figures, accepted for publication in IEEE/ACM\n  Transactions on Audio, Speech, and Language Processing on 16-Nov-2016. This\n  is initial submission version. Fully edited version is available at\n  http://ieeexplore.ieee.org/document/7755799/", "journal-ref": "Published in: IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing ( Volume: 25, Issue: 1, Jan. 2017 ) Page(s): 208 - 221", "doi": "10.1109/TASLP.2016.2632307", "report-no": null, "categories": "cs.SD cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying musical instruments in polyphonic music recordings is a\nchallenging but important problem in the field of music information retrieval.\nIt enables music search by instrument, helps recognize musical genres, or can\nmake music transcription easier and more accurate. In this paper, we present a\nconvolutional neural network framework for predominant instrument recognition\nin real-world polyphonic music. We train our network from fixed-length music\nexcerpts with a single-labeled predominant instrument and estimate an arbitrary\nnumber of predominant instruments from an audio signal with a variable length.\nTo obtain the audio-excerpt-wise result, we aggregate multiple outputs from\nsliding windows over the test audio. In doing so, we investigated two different\naggregation methods: one takes the average for each instrument and the other\ntakes the instrument-wise sum followed by normalization. In addition, we\nconducted extensive experiments on several important factors that affect the\nperformance, including analysis window size, identification threshold, and\nactivation functions for neural networks to find the optimal set of parameters.\nUsing a dataset of 10k audio excerpts from 11 instruments for evaluation, we\nfound that convolutional neural networks are more robust than conventional\nmethods that exploit spectral features and source separation with support\nvector machines. Experimental results showed that the proposed convolutional\nnetwork architecture obtained an F1 measure of 0.602 for micro and 0.503 for\nmacro, respectively, achieving 19.6% and 16.4% in performance improvement\ncompared with other state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 07:11:18 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 08:54:57 GMT"}, {"version": "v3", "created": "Mon, 26 Dec 2016 12:29:26 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Han", "Yoonchang", ""], ["Kim", "Jaehun", ""], ["Lee", "Kyogu", ""]]}, {"id": "1605.09526", "submitter": "Jacopo Cavazza", "authors": "Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina\n  Becchio and Vittorio Murino", "title": "Predicting Human Intentions from Motion Only: A 2D+3D Fusion Approach", "comments": "accepted as poster at the 25th ACM Multimedia (ACM MM) 2017, Mountain\n  View, California, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the new problem of the prediction of human intents.\nThere is neuro-psychological evidence that actions performed by humans are\nanticipated by peculiar motor acts which are discriminant of the type of action\ngoing to be performed afterwards. In other words, an actual intent can be\nforecast by looking at the kinematics of the immediately preceding movement. To\nprove it in a computational and quantitative manner, we devise a new\nexperimental setup where, without using contextual information, we predict\nhuman intents all originating from the same motor act. We posit the problem as\na classification task and we introduce a new multi-modal dataset consisting of\na set of motion capture marker 3D data and 2D video sequences, where, by only\nanalysing very similar movements in both training and test phases, we are able\nto predict the underlying intent, i.e., the future, never observed action. We\nalso present an extensive experimental evaluation as a baseline, customizing\nstate-of-the-art techniques for either 3D and 2D data analysis. Realizing that\nvideo processing methods lead to inferior performance but show complementary\ninformation with respect to 3D data sequences, we developed a 2D+3D fusion\nanalysis where we achieve better classification accuracies, attesting the\nsuperiority of the multimodal approach for the context-free prediction of human\nintents.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 08:43:25 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 12:56:41 GMT"}, {"version": "v3", "created": "Fri, 9 Sep 2016 14:05:08 GMT"}, {"version": "v4", "created": "Wed, 6 Sep 2017 09:11:10 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Zunino", "Andrea", ""], ["Cavazza", "Jacopo", ""], ["Koul", "Atesh", ""], ["Cavallo", "Andrea", ""], ["Becchio", "Cristina", ""], ["Murino", "Vittorio", ""]]}, {"id": "1605.09527", "submitter": "Thomas Goldstein", "authors": "Sohil Shah, Abhay Kumar, Carlos Castillo, David Jacobs, Christoph\n  Studer, Tom Goldstein", "title": "Biconvex Relaxation for Semidefinite Programming in Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programming is an indispensable tool in computer vision, but\ngeneral-purpose solvers for semidefinite programs are often too slow and memory\nintensive for large-scale problems. We propose a general framework to\napproximately solve large-scale semidefinite problems (SDPs) at low complexity.\nOur approach, referred to as biconvex relaxation (BCR), transforms a general\nSDP into a specific biconvex optimization problem, which can then be solved in\nthe original, low-dimensional variable space at low complexity. The resulting\nbiconvex problem is solved using an efficient alternating minimization (AM)\nprocedure. Since AM has the potential to get stuck in local minima, we propose\na general initialization scheme that enables BCR to start close to a global\noptimum - this is key for our algorithm to quickly converge to optimal or\nnear-optimal solutions. We showcase the efficacy of our approach on three\napplications in computer vision, namely segmentation, co-segmentation, and\nmanifold metric learning. BCR achieves solution quality comparable to\nstate-of-the-art SDP methods with speedups between 4X and 35X. At the same\ntime, BCR handles a more general set of SDPs than previous approaches, which\nare more specialized.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 08:43:44 GMT"}, {"version": "v2", "created": "Mon, 8 Aug 2016 23:23:28 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Shah", "Sohil", ""], ["Kumar", "Abhay", ""], ["Castillo", "Carlos", ""], ["Jacobs", "David", ""], ["Studer", "Christoph", ""], ["Goldstein", "Tom", ""]]}, {"id": "1605.09533", "submitter": "Matthias Limmer", "authors": "Matthias Limmer, Julian Forster, Dennis Baudach, Florian Sch\\\"ule,\n  Roland Schweiger, Hendrik P.A. Lensch", "title": "Robust Deep-Learning-Based Road-Prediction for Augmented Reality\n  Navigation Systems", "comments": "8 pages, 12 figures, submitted to ITSC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach that predicts the road course from camera\nsensors leveraging deep learning techniques. Road pixels are identified by\ntraining a multi-scale convolutional neural network on a large number of\nfull-scene-labeled night-time road images including adverse weather conditions.\nA framework is presented that applies the proposed approach to longer distance\nroad course estimation, which is the basis for an augmented reality navigation\napplication. In this framework long range sensor data (radar) and data from a\nmap database are fused with short range sensor data (camera) to produce a\nprecise longitudinal and lateral localization and road course estimation. The\nproposed approach reliably detects roads with and without lane markings and\nthus increases the robustness and availability of road course estimations and\naugmented reality navigation. Evaluations on an extensive set of high precision\nground truth data taken from a differential GPS and an inertial measurement\nunit show that the proposed approach reaches state-of-the-art performance\nwithout the limitation of requiring existing lane markings.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 09:00:33 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Limmer", "Matthias", ""], ["Forster", "Julian", ""], ["Baudach", "Dennis", ""], ["Sch\u00fcle", "Florian", ""], ["Schweiger", "Roland", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1605.09546", "submitter": "Miaomiao Liu", "authors": "Miaomiao Liu, Mathieu Salzmann, Xuming He", "title": "Semantic-Aware Depth Super-Resolution in Outdoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While depth sensors are becoming increasingly popular, their spatial\nresolution often remains limited. Depth super-resolution therefore emerged as a\nsolution to this problem. Despite much progress, state-of-the-art techniques\nsuffer from two drawbacks: (i) they rely on the assumption that intensity edges\ncoincide with depth discontinuities, which, unfortunately, is only true in\ncontrolled environments; and (ii) they typically exploit the availability of\nhigh-resolution training depth maps, which can often not be acquired in\npractice due to the sensors' limitations. By contrast, here, we introduce an\napproach to performing depth super-resolution in more challenging conditions,\nsuch as in outdoor scenes. To this end, we first propose to exploit semantic\ninformation to better constrain the super-resolution process. In particular, we\ndesign a co-sparse analysis model that learns filters from joint intensity,\ndepth and semantic information. Furthermore, we show how low-resolution\ntraining depth maps can be employed in our learning strategy. We demonstrate\nthe benefits of our approach over state-of-the-art depth super-resolution\nmethods on two outdoor scene datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 09:37:55 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Liu", "Miaomiao", ""], ["Salzmann", "Mathieu", ""], ["He", "Xuming", ""]]}, {"id": "1605.09553", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Junhua Mao, Fei Sha, Alan Yuille", "title": "Attention Correctness in Neural Image Captioning", "comments": "To appear in AAAI-17. See http://www.cs.jhu.edu/~cxliu/ for\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have recently been introduced in deep learning for\nvarious tasks in natural language processing and computer vision. But despite\ntheir popularity, the \"correctness\" of the implicitly-learned attention maps\nhas only been assessed qualitatively by visualization of several examples. In\nthis paper we focus on evaluating and improving the correctness of attention in\nneural image captioning models. Specifically, we propose a quantitative\nevaluation metric for the consistency between the generated attention maps and\nhuman annotations, using recently released datasets with alignment between\nregions in images and entities in captions. We then propose novel models with\ndifferent levels of explicit supervision for learning attention maps during\ntraining. The supervision can be strong when alignment between regions and\ncaption entities are available, or weak when only object segments and\ncategories are provided. We show on the popular Flickr30k and COCO datasets\nthat introducing supervision of attention maps during training solidly improves\nboth attention correctness and caption quality, showing the promise of making\nmachine perception more human-like.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:04:20 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 07:29:46 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Liu", "Chenxi", ""], ["Mao", "Junhua", ""], ["Sha", "Fei", ""], ["Yuille", "Alan", ""]]}, {"id": "1605.09559", "submitter": "Zihan Zhou", "authors": "Zihan Zhou, Siqiong He, Jia Li, James Z. Wang", "title": "Modeling Photographic Composition via Triangles", "comments": "22 pages, 25 figures. The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity of automatically modeling photographic composition is valuable\nfor many real-world machine vision applications such as digital photography,\nimage retrieval, image understanding, and image aesthetics assessment. The\ntriangle technique is among those indispensable composition methods on which\nprofessional photographers often rely. This paper proposes a system that can\nidentify prominent triangle arrangements in two major categories of\nphotographs: natural or urban scenes, and portraits. For the natural or urban\nscene pictures, the focus is on the effect of linear perspective. For\nportraits, we carefully examine the positioning of human subjects in a photo.\nWe show that line analysis is highly advantageous for modeling composition in\nboth categories. Based on the detected triangles, new mathematical descriptors\nfor composition are formulated and used to retrieve similar images. Leveraging\nthe rich source of high aesthetics photos online, similar approaches can\npotentially be incorporated in future smart cameras to enhance a person's photo\ncomposition skills.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 10:22:38 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Zhou", "Zihan", ""], ["He", "Siqiong", ""], ["Li", "Jia", ""], ["Wang", "James Z.", ""]]}, {"id": "1605.09582", "submitter": "V S R Veeravasarapu", "authors": "V S R Veeravasarapu, Constantin Rothkopf, Visvanathan Ramesh", "title": "Model-driven Simulations for Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of simulated virtual environments to train deep convolutional neural\nnetworks (CNN) is a currently active practice to reduce the\n(real)data-hungriness of the deep CNN models, especially in application domains\nin which large scale real data and/or groundtruth acquisition is difficult or\nlaborious. Recent approaches have attempted to harness the capabilities of\nexisting video games, animated movies to provide training data with high\nprecision groundtruth. However, a stumbling block is in how one can certify\ngeneralization of the learned models and their usefulness in real world data\nsets. This opens up fundamental questions such as: What is the role of\nphotorealism of graphics simulations in training CNN models? Are the trained\nmodels valid in reality? What are possible ways to reduce the performance bias?\nIn this work, we begin to address theses issues systematically in the context\nof urban semantic understanding with CNNs. Towards this end, we (a) propose a\nsimple probabilistic urban scene model, (b) develop a parametric rendering tool\nto synthesize the data with groundtruth, followed by (c) a systematic\nexploration of the impact of level-of-realism on the generality of the trained\nCNN model to real world; and domain adaptation concepts to minimize the\nperformance bias.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 11:13:20 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Veeravasarapu", "V S R", ""], ["Rothkopf", "Constantin", ""], ["Ramesh", "Visvanathan", ""]]}, {"id": "1605.09584", "submitter": "Mohamed Lamine Mekhalfi", "authors": "Mawloud Guermoui, Mohamed L. Mekhalfi", "title": "A Sparse Representation of Complete Local Binary Pattern Histogram for\n  Human Face Recognition", "comments": "Accepted (but unattended) in IEEE-EMBS International Conferences on\n  Biomedical and Health Informatics (BHI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human face recognition has been a long standing problem in computer vision\nand pattern recognition. Facial analysis can be viewed as a two-fold problem,\nnamely (i) facial representation, and (ii) classification. So far, many face\nrepresentations have been proposed, a well-known method is the Local Binary\nPattern (LBP), which has witnessed a growing interest. In this respect, we\ntreat in this paper the issues of face representation as well as classification\nin a novel manner. On the one hand, we use a variant to LBP, so-called Complete\nLocal Binary Pattern (CLBP), which differs from the basic LBP by coding a given\nlocal region using a given central pixel and Sing_ Magnitude difference.\nSubsequently, most of LBPbased descriptors use a fixed grid to code a given\nfacial image, which technique is, in most cases, not robust to pose variation\nand misalignment. To cope with such issue, a representative Multi-Resolution\nHistogram (MH) decomposition is adopted in our work. On the other hand, having\nthe histograms of the considered images extracted, we exploit their sparsity to\nconstruct a so-called Sparse Representation Classifier (SRC) for further face\nclassification. Experimental results have been conducted on ORL face database,\nand pointed out the superiority of our scheme over other popular\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 11:19:01 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Guermoui", "Mawloud", ""], ["Mekhalfi", "Mohamed L.", ""]]}, {"id": "1605.09612", "submitter": "Laura Florea", "authors": "M.S. Badea, I.I. Felea, L.M. Florea, C. Vertan", "title": "The use of deep learning in image segmentation, classification and\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have shown that deep learned neural networks are a valuable tool\nin the field of computer vision. This paper addresses the use of two different\nkinds of network architectures, namely LeNet and Network in Network (NiN). They\nwill be compared in terms of both performance and computational efficiency by\naddressing the classification and detection problems. In this paper, multiple\ndatabases will be used to test the networks. One of them contains images\ndepicting burn wounds from pediatric cases, another one contains an extensive\nnumber of art images and other facial databases were used for facial keypoints\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 13:09:40 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Badea", "M. S.", ""], ["Felea", "I. I.", ""], ["Florea", "L. M.", ""], ["Vertan", "C.", ""]]}, {"id": "1605.09653", "submitter": "Srikrishna Karanam", "authors": "Srikrishna Karanam, Mengran Gou, Ziyan Wu, Angels Rates-Borras,\n  Octavia Camps, Richard J. Radke", "title": "A Systematic Evaluation and Benchmark for Person Re-Identification:\n  Features, Metrics, and Datasets", "comments": "Preliminary work on person Re-Id benchmark. S. Karanam and M. Gou\n  contributed equally. 14 pages, 6 figures, 4 tables. For supplementary\n  material, see\n  http://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/supmat/ReID_benchmark_supp.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-id) is a critical problem in video analytics\napplications such as security and surveillance. The public release of several\ndatasets and code for vision algorithms has facilitated rapid progress in this\narea over the last few years. However, directly comparing re-id algorithms\nreported in the literature has become difficult since a wide variety of\nfeatures, experimental protocols, and evaluation metrics are employed. In order\nto address this need, we present an extensive review and performance evaluation\nof single- and multi-shot re-id algorithms. The experimental protocol\nincorporates the most recent advances in both feature extraction and metric\nlearning. To ensure a fair comparison, all of the approaches were implemented\nusing a unified code library that includes 11 feature extraction algorithms and\n22 metric learning and ranking techniques. All approaches were evaluated using\na new large-scale dataset that closely mimics a real-world problem setting, in\naddition to 16 other publicly available datasets: VIPeR, GRID, CAVIAR,\nDukeMTMC4ReID, 3DPeS, PRID, V47, WARD, SAIVT-SoftBio, CUHK01, CHUK02, CUHK03,\nRAiD, iLIDSVID, HDA+ and Market1501. The evaluation codebase and results will\nbe made publicly available for community use.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:01:46 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 05:55:46 GMT"}, {"version": "v3", "created": "Tue, 29 Nov 2016 19:50:51 GMT"}, {"version": "v4", "created": "Fri, 18 Aug 2017 03:39:58 GMT"}, {"version": "v5", "created": "Wed, 14 Feb 2018 16:27:31 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Karanam", "Srikrishna", ""], ["Gou", "Mengran", ""], ["Wu", "Ziyan", ""], ["Rates-Borras", "Angels", ""], ["Camps", "Octavia", ""], ["Radke", "Richard J.", ""]]}, {"id": "1605.09673", "submitter": "Xu Jia", "authors": "Bert De Brabandere, Xu Jia, Tinne Tuytelaars, Luc Van Gool", "title": "Dynamic Filter Networks", "comments": "submitted to NIPS16; X. Jia and B. De Brabandere contributed equally\n  to this work and are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a traditional convolutional layer, the learned filters stay fixed after\ntraining. In contrast, we introduce a new framework, the Dynamic Filter\nNetwork, where filters are generated dynamically conditioned on an input. We\nshow that this architecture is a powerful one, with increased flexibility\nthanks to its adaptive nature, yet without an excessive increase in the number\nof model parameters. A wide variety of filtering operations can be learned this\nway, including local spatial transformations, but also others like selective\n(de)blurring or adaptive feature extraction. Moreover, multiple such layers can\nbe combined, e.g. in a recurrent architecture. We demonstrate the effectiveness\nof the dynamic filter network on the tasks of video and stereo prediction, and\nreach state-of-the-art performance on the moving MNIST dataset with a much\nsmaller model. By visualizing the learned filters, we illustrate that the\nnetwork has picked up flow information by only looking at unlabelled training\ndata. This suggests that the network can be used to pretrain networks for\nvarious supervised tasks in an unsupervised way, like optical flow and depth\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 15:29:36 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 15:39:10 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["De Brabandere", "Bert", ""], ["Jia", "Xu", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1605.09696", "submitter": "Guanqun Cao Mr", "authors": "Guanqun Cao, Alexandros Iosifidis, Ke Chen, Moncef Gabbouj", "title": "Generalized Multi-view Embedding for Visual Recognition and Cross-modal\n  Retrieval", "comments": null, "journal-ref": null, "doi": "10.1109/TCYB.2017.2742705", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of multi-view embedding from different visual cues\nand modalities is considered. We propose a unified solution for subspace\nlearning methods using the Rayleigh quotient, which is extensible for multiple\nviews, supervised learning, and non-linear embeddings. Numerous methods\nincluding Canonical Correlation Analysis, Partial Least Sqaure regression and\nLinear Discriminant Analysis are studied using specific intrinsic and penalty\ngraphs within the same framework. Non-linear extensions based on kernels and\n(deep) neural networks are derived, achieving better performance than the\nlinear ones. Moreover, a novel Multi-view Modular Discriminant Analysis (MvMDA)\nis proposed by taking the view difference into consideration. We demonstrate\nthe effectiveness of the proposed multi-view embedding methods on visual object\nrecognition and cross-modal image retrieval, and obtain superior results in\nboth applications compared to related methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 16:11:16 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 15:58:20 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 09:17:50 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Cao", "Guanqun", ""], ["Iosifidis", "Alexandros", ""], ["Chen", "Ke", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1605.09757", "submitter": "Francois Scharffe", "authors": "Sanchit Arora, Chuck Cho, Paul Fitzpatrick, Francois Scharffe", "title": "Towards ontology driven learning of visual concept detectors", "comments": "unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The maturity of deep learning techniques has led in recent years to a\nbreakthrough in object recognition in visual media. While for some specific\nbenchmarks, neural techniques seem to match if not outperform human judgement,\nchallenges are still open for detecting arbitrary concepts in arbitrary videos.\nIn this paper, we propose a system that combines neural techniques, a large\nscale visual concepts ontology, and an active learning loop, to provide on the\nfly model learning of arbitrary concepts. We give an overview of the system as\na whole, and focus on the central role of the ontology for guiding and\nbootstrapping the learning of new concepts, improving the recall of concept\ndetection, and, on the user end, providing semantic search on a library of\nannotated videos.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 18:35:44 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Arora", "Sanchit", ""], ["Cho", "Chuck", ""], ["Fitzpatrick", "Paul", ""], ["Scharffe", "Francois", ""]]}, {"id": "1605.09759", "submitter": "Yang Zhang", "authors": "Yang Zhang, Boqing Gong, Mubarak Shah", "title": "Fast Zero-Shot Image Tagging", "comments": "9 pages (not including reference). This paper has been accepted by\n  CVPR 2016", "journal-ref": "CVPR 2016", "doi": "10.1109/CVPR.2016.644", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known word analogy experiments show that the recent word vectors\ncapture fine-grained linguistic regularities in words by linear vector offsets,\nbut it is unclear how well the simple vector offsets can encode visual\nregularities over words. We study a particular image-word relevance relation in\nthis paper. Our results show that the word vectors of relevant tags for a given\nimage rank ahead of the irrelevant tags, along a principal direction in the\nword vector space. Inspired by this observation, we propose to solve image\ntagging by estimating the principal direction for an image. Particularly, we\nexploit linear mappings and nonlinear deep neural networks to approximate the\nprincipal direction from an input image. We arrive at a quite versatile tagging\nmodel. It runs fast given a test image, in constant time w.r.t.\\ the training\nset size. It not only gives superior performance for the conventional tagging\ntask on the NUS-WIDE dataset, but also outperforms competitive baselines on\nannotating images with previously unseen tags\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 18:39:48 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Zhang", "Yang", ""], ["Gong", "Boqing", ""], ["Shah", "Mubarak", ""]]}, {"id": "1605.09782", "submitter": "Jeff Donahue", "authors": "Jeff Donahue, Philipp Kr\\\"ahenb\\\"uhl, Trevor Darrell", "title": "Adversarial Feature Learning", "comments": "Published as a conference paper at ICLR 2017. Changelog: (v7) Table 2\n  results improved 1-2% due to averaging predictions over 10 crops at test\n  time, as done in Noroozi & Favaro; Table 3 VOC classification results\n  slightly improved due to minor bugfix. (See v6 changelog for previous\n  versions.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:37:29 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 19:52:42 GMT"}, {"version": "v3", "created": "Mon, 18 Jul 2016 03:25:03 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 18:40:47 GMT"}, {"version": "v5", "created": "Fri, 6 Jan 2017 02:49:57 GMT"}, {"version": "v6", "created": "Mon, 9 Jan 2017 05:38:18 GMT"}, {"version": "v7", "created": "Mon, 3 Apr 2017 20:34:36 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Donahue", "Jeff", ""], ["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Darrell", "Trevor", ""]]}]