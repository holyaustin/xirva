[{"id": "1708.00042", "submitter": "Zhenheng Yang", "authors": "Zhenheng Yang, Jiyang Gao, Ram Nevatia", "title": "Spatio-Temporal Action Detection with Cascade Proposal and Location\n  Anticipation", "comments": "Accepted at BMVC 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of spatio-temporal action detection in\ntemporally untrimmed videos. It is an important and challenging task as finding\naccurate human actions in both temporal and spatial space is important for\nanalyzing large-scale video data. To tackle this problem, we propose a cascade\nproposal and location anticipation (CPLA) model for frame-level action\ndetection. There are several salient points of our model: (1) a cascade region\nproposal network (casRPN) is adopted for action proposal generation and shows\nbetter localization accuracy compared with single region proposal network\n(RPN); (2) action spatio-temporal consistencies are exploited via a location\nanticipation network (LAN) and thus frame-level action detection is not\nconducted independently. Frame-level detections are then linked by solving an\nlinking score maximization problem, and temporally trimmed into spatio-temporal\naction tubes. We demonstrate the effectiveness of our model on the challenging\nUCF101 and LIRIS-HARL datasets, both achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 19:03:19 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Yang", "Zhenheng", ""], ["Gao", "Jiyang", ""], ["Nevatia", "Ram", ""]]}, {"id": "1708.00045", "submitter": "Rudrasis Chakraborty Mr", "authors": "Rudrasis Chakraborty and Baba Vemuri", "title": "Statistics on the (compact) Stiefel manifold: Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Stiefel manifold of the compact type is often encountered in many fields of\nEngineering including, signal and image processing, machine learning, numerical\noptimization and others. The Stiefel manifold is a Riemannian homogeneous space\nbut not a symmetric space. In previous work, researchers have defined\nprobability distributions on symmetric spaces and performed statistical\nanalysis of data residing in these spaces. In this paper, we present original\nwork involving definition of Gaussian distributions on a homogeneous space and\nshow that the maximum-likelihood estimate of the location parameter of a\nGaussian distribution on the homogeneous space yields the Fr\\'echet mean (FM)\nof the samples drawn from this distribution. Further, we present an algorithm\nto sample from the Gaussian distribution on the Stiefel manifold and\nrecursively compute the FM of these samples. We also prove the weak consistency\nof this recursive FM estimator. Several synthetic and real data experiments are\nthen presented, demonstrating the superior computational performance of this\nestimator over the gradient descent based non-recursive counter part as well as\nthe stochastic gradient descent based method prevalent in literature.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 19:32:18 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Chakraborty", "Rudrasis", ""], ["Vemuri", "Baba", ""]]}, {"id": "1708.00052", "submitter": "Evgenii Zheltonozhskii", "authors": "Chaim Baskin, Natan Liss, Evgenii Zheltonozhskii, Alex M. Bronshtein,\n  Avi Mendelson", "title": "Streaming Architecture for Large-Scale Quantized Neural Networks on an\n  FPGA-Based Dataflow Platform", "comments": "Will appear in RAW 2018", "journal-ref": null, "doi": "10.1109/IPDPSW.2018.00032", "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are used by different applications that are\nexecuted on a range of computer architectures, from IoT devices to\nsupercomputers. The footprint of these networks is huge as well as their\ncomputational and communication needs. In order to ease the pressure on\nresources, research indicates that in many cases a low precision representation\n(1-2 bit per parameter) of weights and other parameters can achieve similar\naccuracy while requiring less resources. Using quantized values enables the use\nof FPGAs to run NNs, since FPGAs are well fitted to these primitives; e.g.,\nFPGAs provide efficient support for bitwise operations and can work with\narbitrary-precision representation of numbers.\n  This paper presents a new streaming architecture for running QNNs on FPGAs.\nThe proposed architecture scales out better than alternatives, allowing us to\ntake advantage of systems with multiple FPGAs. We also included support for\nskip connections, that are used in state-of-the art NNs, and shown that our\narchitecture allows to add those connections almost for free. All this allowed\nus to implement an 18-layer ResNet for 224x224 images classification, achieving\n57.5% top-1 accuracy.\n  In addition, we implemented a full-sized quantized AlexNet. In contrast to\nprevious works, we use 2-bit activations instead of 1-bit ones, which improves\nAlexNet's top-1 accuracy from 41.8% to 51.03% for the ImageNet classification.\nBoth AlexNet and ResNet can handle 1000-class real-time classification on an\nFPGA.\n  Our implementation of ResNet-18 consumes 5x less power and is 4x slower for\nImageNet, when compared to the same NN on the latest Nvidia GPUs. Smaller NNs,\nthat fit a single FPGA, are running faster then on GPUs on small (32x32)\ninputs, while consuming up to 20x less energy and power.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 19:53:48 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 11:44:36 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 13:20:13 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Baskin", "Chaim", ""], ["Liss", "Natan", ""], ["Zheltonozhskii", "Evgenii", ""], ["Bronshtein", "Alex M.", ""], ["Mendelson", "Avi", ""]]}, {"id": "1708.00069", "submitter": "Peng Zheng", "authors": "Peng Zheng, Aleksandr Y. Aravkin, Karthikeyan Natesan Ramamurthy and\n  Jayaraman Jayaraman Thiagarajan", "title": "Learning Robust Representations for Computer Vision", "comments": "8 pages, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning techniques in computer vision often require learning\nlatent representations, such as low-dimensional linear and non-linear\nsubspaces. Noise and outliers in the data can frustrate these approaches by\nobscuring the latent spaces.\n  Our main goal is deeper understanding and new development of robust\napproaches for representation learning. We provide a new interpretation for\nexisting robust approaches and present two specific contributions: a new robust\nPCA approach, which can separate foreground features from dynamic background,\nand a novel robust spectral clustering method, that can cluster facial images\nwith high accuracy. Both contributions show superior performance to standard\nmethods on real-world test sets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 20:50:01 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zheng", "Peng", ""], ["Aravkin", "Aleksandr Y.", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Thiagarajan", "Jayaraman Jayaraman", ""]]}, {"id": "1708.00079", "submitter": "Fan Yang", "authors": "Mahyar Najibi, Fan Yang, Qiaosong Wang, and Robinson Piramuthu", "title": "Towards the Success Rate of One: Real-time Unconstrained Salient Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an efficient and effective approach for\nunconstrained salient object detection in images using deep convolutional\nneural networks. Instead of generating thousands of candidate bounding boxes\nand refining them, our network directly learns to generate the saliency map\ncontaining the exact number of salient objects. During training, we convert the\nground-truth rectangular boxes to Gaussian distributions that better capture\nthe ROI regarding individual salient objects. During inference, the network\npredicts Gaussian distributions centered at salient objects with an appropriate\ncovariance, from which bounding boxes are easily inferred. Notably, our network\nperforms saliency map prediction without pixel-level annotations, salient\nobject detection without object proposals, and salient object subitizing\nsimultaneously, all in a single pass within a unified framework. Extensive\nexperiments show that our approach outperforms existing methods on various\ndatasets by a large margin, and achieves more than 100 fps with VGG16 network\non a single GPU during inference.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 21:56:14 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 05:22:48 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Najibi", "Mahyar", ""], ["Yang", "Fan", ""], ["Wang", "Qiaosong", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1708.00106", "submitter": "Guilin Liu", "authors": "Guilin Liu, Duygu Ceylan, Ersin Yumer, Jimei Yang, Jyh-Ming Lien", "title": "Material Editing Using a Physically Based Rendering Network", "comments": "14 pages, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to edit materials of objects in images is desirable by many\ncontent creators. However, this is an extremely challenging task as it requires\nto disentangle intrinsic physical properties of an image. We propose an\nend-to-end network architecture that replicates the forward image formation\nprocess to accomplish this task. Specifically, given a single image, the\nnetwork first predicts intrinsic properties, i.e. shape, illumination, and\nmaterial, which are then provided to a rendering layer. This layer performs\nin-network image synthesis, thereby enabling the network to understand the\nphysics behind the image formation process. The proposed rendering layer is\nfully differentiable, supports both diffuse and specular materials, and thus\ncan be applicable in a variety of problem settings. We demonstrate a rich set\nof visually plausible material editing examples and provide an extensive\ncomparative study.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 00:02:03 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 17:28:54 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Liu", "Guilin", ""], ["Ceylan", "Duygu", ""], ["Yumer", "Ersin", ""], ["Yang", "Jimei", ""], ["Lien", "Jyh-Ming", ""]]}, {"id": "1708.00129", "submitter": "Andy Kitchen", "authors": "Andy Kitchen, Jarrel Seah", "title": "Deep Generative Adversarial Neural Networks for Realistic Prostate\n  Lesion MRI Synthesis", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Neural Networks (GANs) are applied to the synthetic\ngeneration of prostate lesion MRI images. GANs have been applied to a variety\nof natural images, is shown show that the same techniques can be used in the\nmedical domain to create realistic looking synthetic lesion images. 16mm x 16mm\npatches are extracted from 330 MRI scans from the SPIE ProstateX Challenge 2016\nand used to train a Deep Convolutional Generative Adversarial Neural Network\n(DCGAN) utilizing cutting edge techniques. Synthetic outputs are compared to\nreal images and the implicit latent representations induced by the GAN are\nexplored. Training techniques and successful neural network architectures are\nexplained in detail.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 02:09:12 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Kitchen", "Andy", ""], ["Seah", "Jarrel", ""]]}, {"id": "1708.00153", "submitter": "Heng Fan", "authors": "Heng Fan, Haibin Ling", "title": "Parallel Tracking and Verifying: A Framework for Real-Time and High\n  Accuracy Visual Tracking", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being intensively studied, visual tracking has seen great recent advances in\neither speed (e.g., with correlation filters) or accuracy (e.g., with deep\nfeatures). Real-time and high accuracy tracking algorithms, however, remain\nscarce. In this paper we study the problem from a new perspective and present a\nnovel parallel tracking and verifying (PTAV) framework, by taking advantage of\nthe ubiquity of multi-thread techniques and borrowing from the success of\nparallel tracking and mapping in visual SLAM. Our PTAV framework typically\nconsists of two components, a tracker T and a verifier V, working in parallel\non two separate threads. The tracker T aims to provide a super real-time\ntracking inference and is expected to perform well most of the time; by\ncontrast, the verifier V checks the tracking results and corrects T when\nneeded. The key innovation is that, V does not work on every frame but only\nupon the requests from T; on the other end, T may adjust the tracking according\nto the feedback from V. With such collaboration, PTAV enjoys both the high\nefficiency provided by T and the strong discriminative power by V. In our\nextensive experiments on popular benchmarks including OTB2013, OTB2015, TC128\nand UAV20L, PTAV achieves the best tracking accuracy among all real-time\ntrackers, and in fact performs even better than many deep learning based\nsolutions. Moreover, as a general framework, PTAV is very flexible and has\ngreat rooms for improvement and generalization.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 04:16:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "1708.00159", "submitter": "Nithish Divakar", "authors": "Nithish Divakar and R. Venkatesh Babu", "title": "Image Denoising via CNNs: An Adversarial Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to recover an image from its noisy version using convolutional\nneural networks? This is an interesting problem as convolutional layers are\ngenerally used as feature detectors for tasks like classification, segmentation\nand object detection. We present a new CNN architecture for blind image\ndenoising which synergically combines three architecture components, a\nmulti-scale feature extraction layer which helps in reducing the effect of\nnoise on feature maps, an l_p regularizer which helps in selecting only the\nappropriate feature maps for the task of reconstruction, and finally a three\nstep training approach which leverages adversarial training to give the final\nperformance boost to the model. The proposed model shows competitive denoising\nperformance when compared to the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 05:04:22 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Divakar", "Nithish", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1708.00163", "submitter": "Albert Haque", "authors": "Albert Haque, Michelle Guo, Alexandre Alahi, Serena Yeung, Zelun Luo,\n  Alisha Rege, Jeffrey Jopling, Lance Downing, William Beninati, Amit Singh,\n  Terry Platchek, Arnold Milstein, Li Fei-Fei", "title": "Towards Vision-Based Smart Hospitals: A System for Tracking and\n  Monitoring Hand Hygiene Compliance", "comments": "Machine Learning for Healthcare Conference (MLHC)", "journal-ref": "PMLR 68:75-87, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One in twenty-five patients admitted to a hospital will suffer from a\nhospital acquired infection. If we can intelligently track healthcare staff,\npatients, and visitors, we can better understand the sources of such\ninfections. We envision a smart hospital capable of increasing operational\nefficiency and improving patient care with less spending. In this paper, we\npropose a non-intrusive vision-based system for tracking people's activity in\nhospitals. We evaluate our method for the problem of measuring hand hygiene\ncompliance. Empirically, our method outperforms existing solutions such as\nproximity-based techniques and covert in-person observational studies. We\npresent intuitive, qualitative results that analyze human movement patterns and\nconduct spatial analytics which convey our method's interpretability. This work\nis a step towards a computer-vision based smart hospital and demonstrates\npromising results for reducing hospital acquired infections.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 05:21:08 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 04:41:40 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 05:06:13 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Haque", "Albert", ""], ["Guo", "Michelle", ""], ["Alahi", "Alexandre", ""], ["Yeung", "Serena", ""], ["Luo", "Zelun", ""], ["Rege", "Alisha", ""], ["Jopling", "Jeffrey", ""], ["Downing", "Lance", ""], ["Beninati", "William", ""], ["Singh", "Amit", ""], ["Platchek", "Terry", ""], ["Milstein", "Arnold", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1708.00169", "submitter": "Milind Gide", "authors": "Milind S. Gide and Lina J. Karam", "title": "A Locally Weighted Fixation Density-Based Metric for Assessing the\n  Quality of Visual Saliency Predictions", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 25, no. 8, pp.\n  3852-3861, Aug. 2016", "doi": "10.1109/TIP.2016.2577498", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased focus on visual attention (VA) in the last decade, a large\nnumber of computational visual saliency methods have been developed over the\npast few years. These models are traditionally evaluated by using performance\nevaluation metrics that quantify the match between predicted saliency and\nfixation data obtained from eye-tracking experiments on human observers. Though\na considerable number of such metrics have been proposed in the literature,\nthere are notable problems in them. In this work, we discuss shortcomings in\nexisting metrics through illustrative examples and propose a new metric that\nuses local weights based on fixation density which overcomes these flaws. To\ncompare the performance of our proposed metric at assessing the quality of\nsaliency prediction with other existing metrics, we construct a ground-truth\nsubjective database in which saliency maps obtained from 17 different VA models\nare evaluated by 16 human observers on a 5-point categorical scale in terms of\ntheir visual resemblance with corresponding ground-truth fixation density maps\nobtained from eye-tracking data. The metrics are evaluated by correlating\nmetric scores with the human subjective ratings. The correlation results show\nthat the proposed evaluation metric outperforms all other popular existing\nmetrics. Additionally, the constructed database and corresponding subjective\nratings provide an insight into which of the existing metrics and future\nmetrics are better at estimating the quality of saliency prediction and can be\nused as a benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 05:39:08 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Gide", "Milind S.", ""], ["Karam", "Lina J.", ""]]}, {"id": "1708.00171", "submitter": "Jonathan Kelly", "authors": "Valentin Peretroukhin, William Vega-Brown, Nicholas Roy and Jonathan\n  Kelly", "title": "PROBE-GK: Predictive Robust Estimation using Generalized Kernels", "comments": "In Proceedings of the IEEE International Conference on Robotics and\n  Automation (ICRA'16), Stockholm, Sweden, May 16-21, 2016", "journal-ref": null, "doi": "10.1109/ICRA.2016.7487212", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms in computer vision and robotics make strong assumptions about\nuncertainty, and rely on the validity of these assumptions to produce accurate\nand consistent state estimates. In practice, dynamic environments may degrade\nsensor performance in predictable ways that cannot be captured with static\nuncertainty parameters. In this paper, we employ fast nonparametric Bayesian\ninference techniques to more accurately model sensor uncertainty. By setting a\nprior on observation uncertainty, we derive a predictive robust estimator, and\nshow how our model can be learned from sample images, both with and without\nknowledge of the motion used to generate the data. We validate our approach\nthrough Monte Carlo simulations, and report significant improvements in\nlocalization accuracy relative to a fixed noise model in several settings,\nincluding on synthetic data, the KITTI dataset, and our own experimental\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 05:45:20 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 21:40:50 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Vega-Brown", "William", ""], ["Roy", "Nicholas", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1708.00174", "submitter": "Jonathan Kelly", "authors": "Valentin Peretroukhin, Lee Clement, Matthew Giamou, and Jonathan Kelly", "title": "PROBE: Predictive Robust Estimation for Visual-Inertial Navigation", "comments": "In Proceedings of the IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS'15), Hamburg, Germany, Sep. 28-Oct. 2,\n  2015", "journal-ref": null, "doi": "10.1109/IROS.2015.7353890", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation in unknown, chaotic environments continues to present a\nsignificant challenge for the robotics community. Lighting changes,\nself-similar textures, motion blur, and moving objects are all considerable\nstumbling blocks for state-of-the-art vision-based navigation algorithms. In\nthis paper we present a novel technique for improving localization accuracy\nwithin a visual-inertial navigation system (VINS). We make use of training data\nto learn a model for the quality of visual features with respect to\nlocalization error in a given environment. This model maps each visual\nobservation from a predefined prediction space of visual-inertial predictors\nonto a scalar weight, which is then used to scale the observation covariance\nmatrix. In this way, our model can adjust the influence of each observation\naccording to its quality. We discuss our choice of predictors and report\nsubstantial reductions in localization error on 4 km of data from the KITTI\ndataset, as well as on experimental datasets consisting of 700 m of indoor and\noutdoor driving on a small ground rover equipped with a Skybotix VI-Sensor.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 06:03:09 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 21:31:11 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 11:19:20 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Peretroukhin", "Valentin", ""], ["Clement", "Lee", ""], ["Giamou", "Matthew", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1708.00180", "submitter": "Martin Kiechle", "authors": "Martin Kiechle, Martin Storath, Andreas Weinmann, Martin Kleinsteuber", "title": "Model-based learning of local image features for unsupervised texture\n  segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2792904", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Features that capture well the textural patterns of a certain class of images\nare crucial for the performance of texture segmentation methods. The manual\nselection of features or designing new ones can be a tedious task. Therefore,\nit is desirable to automatically adapt the features to a certain image or class\nof images. Typically, this requires a large set of training images with similar\ntextures and ground truth segmentation. In this work, we propose a framework to\nlearn features for texture segmentation when no such training data is\navailable. The cost function for our learning process is constructed to match a\ncommonly used segmentation model, the piecewise constant Mumford-Shah model.\nThis means that the features are learned such that they provide an\napproximately piecewise constant feature image with a small jump set. Based on\nthis idea, we develop a two-stage algorithm which first learns suitable\nconvolutional features and then performs a segmentation. We note that the\nfeatures can be learned from a small set of images, from a single image, or\neven from image patches. The proposed method achieves a competitive rank in the\nPrague texture segmentation benchmark, and it is effective for segmenting\nhistological images.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 06:35:46 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Kiechle", "Martin", ""], ["Storath", "Martin", ""], ["Weinmann", "Andreas", ""], ["Kleinsteuber", "Martin", ""]]}, {"id": "1708.00185", "submitter": "Boyan Zhang", "authors": "Mingyuan Bai, Boyan Zhang, and Junbin Gao", "title": "Tensorial Recurrent Neural Networks for Longitudinal Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Recurrent Neural Networks assume vectorized data as inputs.\nHowever many data from modern science and technology come in certain structures\nsuch as tensorial time series data. To apply the recurrent neural networks for\nthis type of data, a vectorisation process is necessary, while such a\nvectorisation leads to the loss of the precise information of the spatial or\nlongitudinal dimensions. In addition, such a vectorized data is not an optimum\nsolution for learning the representation of the longitudinal data. In this\npaper, we propose a new variant of tensorial neural networks which directly\ntake tensorial time series data as inputs. We call this new variant as\nTensorial Recurrent Neural Network (TRNN). The proposed TRNN is based on tensor\nTucker decomposition.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 07:14:36 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bai", "Mingyuan", ""], ["Zhang", "Boyan", ""], ["Gao", "Junbin", ""]]}, {"id": "1708.00187", "submitter": "Haichao Zhu", "authors": "Haichao Zhu, Xueting Liu, Xiangyu Mao, Tien-Tsin Wong", "title": "Real-time Deep Video Deinterlacing", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interlacing is a widely used technique, for television broadcast and video\nrecording, to double the perceived frame rate without increasing the bandwidth.\nBut it presents annoying visual artifacts, such as flickering and silhouette\n\"serration,\" during the playback. Existing state-of-the-art deinterlacing\nmethods either ignore the temporal information to provide real-time performance\nbut lower visual quality, or estimate the motion for better deinterlacing but\nwith a trade-off of higher computational cost. In this paper, we present the\nfirst and novel deep convolutional neural networks (DCNNs) based method to\ndeinterlace with high visual quality and real-time performance. Unlike existing\nmodels for super-resolution problems which relies on the translation-invariant\nassumption, our proposed DCNN model utilizes the temporal information from both\nthe odd and even half frames to reconstruct only the missing scanlines, and\nretains the given odd and even scanlines for producing the full deinterlaced\nframes. By further introducing a layer-sharable architecture, our system can\nachieve real-time performance on a single GPU. Experiments shows that our\nmethod outperforms all existing methods, in terms of reconstruction accuracy\nand computational performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 07:23:53 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Zhu", "Haichao", ""], ["Liu", "Xueting", ""], ["Mao", "Xiangyu", ""], ["Wong", "Tien-Tsin", ""]]}, {"id": "1708.00197", "submitter": "Ziwei Liu", "authors": "Xiaoxiao Li, Yuankai Qi, Zhe Wang, Kai Chen, Ziwei Liu, Jianping Shi,\n  Ping Luo, Xiaoou Tang, Chen Change Loy", "title": "Video Object Segmentation with Re-identification", "comments": "Published in CVPR 2017 Workshop, DAVIS Challenge on Video Object\n  Segmentation 2017 (Winning Entry)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional video segmentation methods often rely on temporal continuity to\npropagate masks. Such an assumption suffers from issues like drifting and\ninability to handle large displacement. To overcome these issues, we formulate\nan effective mechanism to prevent the target from being lost via adaptive\nobject re-identification. Specifically, our Video Object Segmentation with\nRe-identification (VS-ReID) model includes a mask propagation module and a ReID\nmodule. The former module produces an initial probability map by flow warping\nwhile the latter module retrieves missing instances by adaptive matching. With\nthese two modules iteratively applied, our VS-ReID records a global mean\n(Region Jaccard and Boundary F measure) of 0.699, the best performance in 2017\nDAVIS Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 08:17:37 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Qi", "Yuankai", ""], ["Wang", "Zhe", ""], ["Chen", "Kai", ""], ["Liu", "Ziwei", ""], ["Shi", "Jianping", ""], ["Luo", "Ping", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen Change", ""]]}, {"id": "1708.00199", "submitter": "Shiv Surya", "authors": "Deepak Babu Sam, Shiv Surya and R. Venkatesh Babu", "title": "Switching Convolutional Neural Network for Crowd Counting", "comments": "Version 2: Added references. Corrected Typos. Published at CVPR 2017,\n  Honolulu, Hawaii. The first two authors contributed equally. For Project Page\n  see http://val.serc.iisc.ernet.in/crowdcnn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel crowd counting model that maps a given crowd scene to its\ndensity. Crowd analysis is compounded by myriad of factors like inter-occlusion\nbetween people due to extreme crowding, high similarity of appearance between\npeople and background elements, and large variability of camera view-points.\nCurrent state-of-the art approaches tackle these factors by using multi-scale\nCNN architectures, recurrent networks and late fusion of features from\nmulti-column CNN with different receptive fields. We propose switching\nconvolutional neural network that leverages variation of crowd density within\nan image to improve the accuracy and localization of the predicted crowd count.\nPatches from a grid within a crowd scene are relayed to independent CNN\nregressors based on crowd count prediction quality of the CNN established\nduring training. The independent CNN regressors are designed to have different\nreceptive fields and a switch classifier is trained to relay the crowd scene\npatch to the best CNN regressor. We perform extensive experiments on all major\ncrowd counting datasets and evidence better performance compared to current\nstate-of-the-art methods. We provide interpretable representations of the\nmultichotomy of space of crowd scene patches inferred from the switch. It is\nobserved that the switch relays an image patch to a particular CNN column based\non density of crowd.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 08:30:28 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 11:02:02 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sam", "Deepak Babu", ""], ["Surya", "Shiv", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1708.00223", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Shengfeng He, Linchao Bao and Qingxiong\n  Yang", "title": "Learning to Hallucinate Face Images via Component Generation and\n  Enhancement", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sr/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage method for face hallucination. First, we generate\nfacial components of the input image using CNNs. These components represent the\nbasic facial structures. Second, we synthesize fine-grained facial structures\nfrom high resolution training images. The details of these structures are\ntransferred into facial components for enhancement. Therefore, we generate\nfacial components to approximate ground truth global appearance in the first\nstage and enhance them through recovering details in the second stage. The\nexperiments demonstrate that our method performs favorably against\nstate-of-the-art methods\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["He", "Shengfeng", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00224", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang", "title": "Fast Preprocessing for Robust Face Sketch Synthesis", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sketch/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based face sketch synthesis methods usually meet the challenging\nproblem that input photos are captured in different lighting conditions from\ntraining photos. The critical step causing the failure is the search of similar\npatch candidates for an input photo patch. Conventional illumination invariant\npatch distances are adopted rather than directly relying on pixel intensity\ndifference, but they will fail when local contrast within a patch changes. In\nthis paper, we propose a fast preprocessing method named Bidirectional\nLuminance Remapping (BLR), which interactively adjust the lighting of training\nand input photos. Our method can be directly integrated into state-of-the-art\nexemplar-based methods to improve their robustness with ignorable computational\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00225", "submitter": "Yibing Song", "authors": "Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson Lau, Ming-Hsuan\n  Yang", "title": "CREST: Convolutional Residual Learning for Visual Tracking", "comments": "ICCV 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters (DCFs) have been shown to perform\nsuperiorly in visual tracking. They only need a small set of training samples\nfrom the initial frame to generate an appearance model. However, existing DCFs\nlearn the filters separately from feature extraction, and update these filters\nusing a moving average operation with an empirical weight. These DCF trackers\nhardly benefit from the end-to-end training. In this paper, we propose the\nCREST algorithm to reformulate DCFs as a one-layer convolutional neural\nnetwork. Our method integrates feature extraction, response map generation as\nwell as model update into the neural networks for an end-to-end training. To\nreduce model degradation during online update, we apply residual learning to\ntake appearance changes into account. Extensive experiments on the benchmark\ndatasets demonstrate that our CREST tracker performs favorably against\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:47:20 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Gong", "Lijun", ""], ["Zhang", "Jiawei", ""], ["Lau", "Rynson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.00227", "submitter": "Ayan Kumar Bhunia", "authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Ayan Das, Prasenjit Dey, Umapada\n  Pal", "title": "HMM-based Indic Handwritten Word Recognition using Zone Segmentation", "comments": "Published in Pattern Recognition(2016)", "journal-ref": "Pattern Recognition, Volume 60, December 2016, Pages 1057-1075", "doi": "10.1016/j.patcog.2016.04.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach towards Indic handwritten word\nrecognition using zone-wise information. Because of complex nature due to\ncompound characters, modifiers, overlapping and touching, etc., character\nsegmentation and recognition is a tedious job in Indic scripts (e.g.\nDevanagari, Bangla, Gurumukhi, and other similar scripts). To avoid character\nsegmentation in such scripts, HMM-based sequence modeling has been used earlier\nin holistic way. This paper proposes an efficient word recognition framework by\nsegmenting the handwritten word images horizontally into three zones (upper,\nmiddle and lower) and recognize the corresponding zones. The main aim of this\nzone segmentation approach is to reduce the number of distinct component\nclasses compared to the total number of classes in Indic scripts. As a result,\nuse of this zone segmentation approach enhances the recognition performance of\nthe system. The components in middle zone where characters are mostly touching\nare recognized using HMM. After the recognition of middle zone, HMM based\nViterbi forced alignment is applied to mark the left and right boundaries of\nthe characters. Next, the residue components, if any, in upper and lower zones\nin their respective boundary are combined to achieve the final word level\nrecognition. Water reservoir feature has been integrated in this framework to\nimprove the zone segmentation and character alignment defects while\nsegmentation. A novel sliding window-based feature, called Pyramid Histogram of\nOriented Gradient (PHOG) is proposed for middle zone recognition. An exhaustive\nexperiment is performed on two Indic scripts namely, Bangla and Devanagari for\nthe performance evaluation. From the experiment, it has been noted that\nproposed zone-wise recognition improves accuracy with respect to the\ntraditional way of Indic word recognition.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:52:03 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Roy", "Partha Pratim", ""], ["Bhunia", "Ayan Kumar", ""], ["Das", "Ayan", ""], ["Dey", "Prasenjit", ""], ["Pal", "Umapada", ""]]}, {"id": "1708.00251", "submitter": "Michael Gadermayr", "authors": "Michael Gadermayr, Ann-Kathrin Dombrowski, Barbara Mara Klinkhammer,\n  Peter Boor, Dorit Merhof", "title": "CNN Cascades for Segmenting Whole Slide Images of the Kidney", "comments": null, "journal-ref": null, "doi": "10.1016/j.compmedimag.2018.11.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing availability of whole slide scanners facilitating\ndigitization of histopathological tissue, there is a strong demand for the\ndevelopment of computer based image analysis systems. In this work, the focus\nis on the segmentation of the glomeruli constituting a highly relevant\nstructure in renal histopathology, which has not been investigated before in\ncombination with CNNs. We propose two different CNN cascades for segmentation\napplications with sparse objects. These approaches are applied to the problem\nof glomerulus segmentation and compared with conventional fully-convolutional\nnetworks. Overall, with the best performing cascade approach, single CNNs are\noutperformed and a pixel-level Dice similarity coefficient of 0.90 is obtained.\nCombined with qualitative and further object-level analyses the obtained\nresults are assessed as excellent also compared to recent approaches. In\nconclusion, we can state that especially one of the proposed cascade networks\nproved to be a highly powerful tool for segmenting the renal glomeruli\nproviding best segmentation accuracies and also keeping the computing time at a\nlow level.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:13:04 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gadermayr", "Michael", ""], ["Dombrowski", "Ann-Kathrin", ""], ["Klinkhammer", "Barbara Mara", ""], ["Boor", "Peter", ""], ["Merhof", "Dorit", ""]]}, {"id": "1708.00277", "submitter": "Baris Gecer", "authors": "Baris Gecer, Vassileios Balntas, Tae-Kyun Kim", "title": "Learning Deep Convolutional Embeddings for Face Representation Using\n  Joint Sample- and Set-based Supervision", "comments": "8 pages, 5 figures, 2 tables, workshop paper", "journal-ref": "The IEEE International Conference on Computer Vision (ICCV) 2017,\n  p.1665-1672", "doi": "10.1109/ICCVW.2017.195", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate several methods and strategies to learn deep\nembeddings for face recognition, using joint sample- and set-based\noptimization. We explain our framework that expands traditional learning with\nset-based supervision together with the strategies used to maintain set\ncharacteristics. We, then, briefly review the related set-based loss functions,\nand subsequently propose a novel Max-Margin Loss which maximizes maximum\npossible inter-class margin with assistance of Support Vector Machines (SVMs).\nIt implicitly pushes all the samples towards correct side of the margin with a\nvector perpendicular to the hyperplane and a strength exponentially growing\ntowards to negative side of the hyperplane. We show that the introduced loss\noutperform the previous sample-based and set-based ones in terms verification\nof faces on two commonly used benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 12:22:21 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 15:18:18 GMT"}, {"version": "v3", "created": "Fri, 12 Jan 2018 19:03:06 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Gecer", "Baris", ""], ["Balntas", "Vassileios", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1708.00284", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang, Lisa Lee, Wei Dai and Eric P. Xing", "title": "Dual Motion GAN for Future-Flow Embedded Video Prediction", "comments": "ICCV 17 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future frame prediction in videos is a promising avenue for unsupervised\nvideo representation learning. Video frames are naturally generated by the\ninherent pixel flows from preceding frames based on the appearance and motion\ndynamics in the video. However, existing methods focus on directly\nhallucinating pixel values, resulting in blurry predictions. In this paper, we\ndevelop a dual motion Generative Adversarial Net (GAN) architecture, which\nlearns to explicitly enforce future-frame predictions to be consistent with the\npixel-wise flows in the video through a dual-learning mechanism. The primal\nfuture-frame prediction and dual future-flow prediction form a closed loop,\ngenerating informative feedback signals to each other for better video\nprediction. To make both synthesized future frames and flows indistinguishable\nfrom reality, a dual adversarial training method is proposed to ensure that the\nfuture-flow prediction is able to help infer realistic future-frames, while the\nfuture-frame prediction in turn leads to realistic optical flows. Our dual\nmotion GAN also handles natural motion uncertainty in different pixel locations\nwith a new probabilistic motion encoder, which is based on variational\nautoencoders. Extensive experiments demonstrate that the proposed dual motion\nGAN significantly outperforms state-of-the-art approaches on synthesizing new\nvideo frames and predicting future flows. Our model generalizes well across\ndiverse visual scenes and shows superiority in unsupervised video\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 12:38:58 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 04:23:30 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Liang", "Xiaodan", ""], ["Lee", "Lisa", ""], ["Dai", "Wei", ""], ["Xing", "Eric P.", ""]]}, {"id": "1708.00300", "submitter": "Marcelo Saval Calvo", "authors": "Christos Maniatis, Marcelo Saval-Calvo, Radim Tylecek and Robert B.\n  Fisher", "title": "Best Viewpoint Tracking for Camera Mounted on Robotic Arm with Dynamic\n  Obstacles", "comments": "10 pages, 6 figures, poster in 3DV conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of finding a next best viewpoint for 3D modeling or scene mapping\nhas been explored in computer vision over the last decade. This paper tackles a\nsimilar problem, but with different characteristics. It proposes a method for\ndynamic next best viewpoint recovery of a target point while avoiding possible\nocclusions. Since the environment can change, the method has to iteratively\nfind the next best view with a global understanding of the free and occupied\nparts.\n  We model the problem as a set of possible viewpoints which correspond to the\ncenters of the facets of a virtual tessellated hemisphere covering the scene.\nTaking into account occlusions, distances between current and future\nviewpoints, quality of the viewpoint and joint constraints (robot arm joint\ndistances or limits), we evaluate the next best viewpoint. The proposal has\nbeen evaluated on 8 different scenarios with different occlusions and a short\n3D video sequence to validate its dynamic performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 13:18:23 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 13:34:24 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Maniatis", "Christos", ""], ["Saval-Calvo", "Marcelo", ""], ["Tylecek", "Radim", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1708.00315", "submitter": "Xiaodan Liang", "authors": "Xiaodan Liang, Hao Zhang and Eric P. Xing", "title": "Generative Semantic Manipulation with Contrasting GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently achieved significant\nimprovement on paired/unpaired image-to-image translation, such as\nphoto$\\rightarrow$ sketch and artist painting style transfer. However, existing\nmodels can only be capable of transferring the low-level information (e.g.\ncolor or texture changes), but fail to edit high-level semantic meanings (e.g.,\ngeometric structure or content) of objects. On the other hand, while some\nresearches can synthesize compelling real-world images given a class label or\ncaption, they cannot condition on arbitrary shapes or structures, which largely\nlimits their application scenarios and interpretive capability of model\nresults. In this work, we focus on a more challenging semantic manipulation\ntask, which aims to modify the semantic meaning of an object while preserving\nits own characteristics (e.g. viewpoints and shapes), such as\ncow$\\rightarrow$sheep, motor$\\rightarrow$ bicycle, cat$\\rightarrow$dog. To\ntackle such large semantic changes, we introduce a contrasting GAN\n(contrast-GAN) with a novel adversarial contrasting objective. Instead of\ndirectly making the synthesized samples close to target data as previous GANs\ndid, our adversarial contrasting objective optimizes over the distance\ncomparisons between samples, that is, enforcing the manipulated data be\nsemantically closer to the real data with target category than the input data.\nEquipped with the new contrasting objective, a novel mask-conditional\ncontrast-GAN architecture is proposed to enable disentangle image background\nwith object semantic changes. Experiments on several semantic manipulation\ntasks on ImageNet and MSCOCO dataset show considerable performance gain by our\ncontrast-GAN over other conditional GANs. Quantitative results further\ndemonstrate the superiority of our model on generating manipulated results with\nhigh visual fidelity and reasonable object semantics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 13:46:32 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Liang", "Xiaodan", ""], ["Zhang", "Hao", ""], ["Xing", "Eric P.", ""]]}, {"id": "1708.00367", "submitter": "Amir Jamaludin", "authors": "Amir Jamaludin, Timor Kadir, Andrew Zisserman", "title": "Self-Supervised Learning for Spinal MRIs", "comments": "3rd Workshop on Deep Learning in Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant proportion of patients scanned in a clinical setting have\nfollow-up scans. We show in this work that such longitudinal scans alone can be\nused as a form of 'free' self-supervision for training a deep network. We\ndemonstrate this self-supervised learning for the case of T2-weighted sagittal\nlumbar Magnetic Resonance Images (MRIs). A Siamese convolutional neural network\n(CNN) is trained using two losses: (i) a contrastive loss on whether the scan\nis of the same person (i.e. longitudinal) or not, together with (ii) a\nclassification loss on predicting the level of vertebral bodies. The\nperformance of this pre-trained network is then assessed on a grading\nclassification task. We experiment on a dataset of 1016 subjects, 423\npossessing follow-up scans, with the end goal of learning the disc degeneration\nradiological gradings attached to the intervertebral discs. We show that the\nperformance of the pre-trained CNN on the supervised classification task is (i)\nsuperior to that of a network trained from scratch; and (ii) requires far fewer\nannotated training samples to reach an equivalent performance to that of the\nnetwork trained from scratch.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:44:48 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Jamaludin", "Amir", ""], ["Kadir", "Timor", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1708.00370", "submitter": "Behnaz Nojavanasghari", "authors": "Behnaz Nojavanasghari, Charles. E. Hughes, Tadas Baltrusaitis, and\n  Louis-philippe Morency", "title": "Hand2Face: Automatic Synthesis and Recognition of Hand Over Face\n  Occlusions", "comments": "Accepted to International Conference on Affective Computing and\n  Intelligent Interaction (ACII), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person's face discloses important information about their affective state.\nAlthough there has been extensive research on recognition of facial\nexpressions, the performance of existing approaches is challenged by facial\nocclusions. Facial occlusions are often treated as noise and discarded in\nrecognition of affective states. However, hand over face occlusions can provide\nadditional information for recognition of some affective states such as\ncuriosity, frustration and boredom. One of the reasons that this problem has\nnot gained attention is the lack of naturalistic occluded faces that contain\nhand over face occlusions as well as other types of occlusions. Traditional\napproaches for obtaining affective data are time demanding and expensive, which\nlimits researchers in affective computing to work on small datasets. This\nlimitation affects the generalizability of models and deprives researchers from\ntaking advantage of recent advances in deep learning that have shown great\nsuccess in many fields but require large volumes of data. In this paper, we\nfirst introduce a novel framework for synthesizing naturalistic facial\nocclusions from an initial dataset of non-occluded faces and separate images of\nhands, reducing the costly process of data collection and annotation. We then\npropose a model for facial occlusion type recognition to differentiate between\nhand over face occlusions and other types of occlusions such as scarves, hair,\nglasses and objects. Finally, we present a model to localize hand over face\nocclusions and identify the occluded regions of the face.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:46:09 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 02:06:44 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Nojavanasghari", "Behnaz", ""], ["Hughes", "Charles. E.", ""], ["Baltrusaitis", "Tadas", ""], ["Morency", "Louis-philippe", ""]]}, {"id": "1708.00377", "submitter": "Syed Anwar", "authors": "Saddam Hussain, Syed Muhammad Anwar, Muhammad Majid", "title": "Segmentation of Glioma Tumors in Brain Using Deep Convolutional Neural\n  Network", "comments": "Submitted to Neurocomputing", "journal-ref": "Neurocomputing 2018", "doi": "10.1016/j.neucom.2017.12.032", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of brain tumor using a segmentation based approach is critical in\ncases, where survival of a subject depends on an accurate and timely clinical\ndiagnosis. Gliomas are the most commonly found tumors having irregular shape\nand ambiguous boundaries, making them one of the hardest tumors to detect. The\nautomation of brain tumor segmentation remains a challenging problem mainly due\nto significant variations in its structure. An automated brain tumor\nsegmentation algorithm using deep convolutional neural network (DCNN) is\npresented in this paper. A patch based approach along with an inception module\nis used for training the deep network by extracting two co-centric patches of\ndifferent sizes from the input images. Recent developments in deep neural\nnetworks such as drop-out, batch normalization, non-linear activation and\ninception module are used to build a new ILinear nexus architecture. The module\novercomes the over-fitting problem arising due to scarcity of data using\ndrop-out regularizer. Images are normalized and bias field corrected in the\npre-processing step and then extracted patches are passed through a DCNN, which\nassigns an output label to the central pixel of each patch. Morphological\noperators are used for post-processing to remove small false positives around\nthe edges. A two-phase weighted training method is introduced and evaluated\nusing BRATS 2013 and BRATS 2015 datasets, where it improves the performance\nparameters of state-of-the-art techniques under similar settings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 15:06:07 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Hussain", "Saddam", ""], ["Anwar", "Syed Muhammad", ""], ["Majid", "Muhammad", ""]]}, {"id": "1708.00397", "submitter": "Johannes Graeter", "authors": "Johannes Graeter, Tobias Strauss and Martin Lauer", "title": "Momo: Monocular Motion Estimation on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about the location of a vehicle is indispensable for autonomous\ndriving. In order to apply global localisation methods, a pose prior must be\nknown which can be obtained from visual odometry. The quality and robustness of\nthat prior determine the success of localisation. Momo is a monocular\nframe-to-frame motion estimation methodology providing a high quality visual\nodometry for that purpose. By taking into account the motion model of the\nvehicle, reliability and accuracy of the pose prior are significantly improved.\nWe show that especially in low-structure environments Momo outperforms the\nstate of the art. Moreover, the method is designed so that multiple cameras\nwith or without overlap can be integrated. The evaluation on the KITTI-dataset\nand on a proper multi-camera dataset shows that even with only 100--300 feature\nmatches the prior is estimated with high accuracy and in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 15:51:32 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Graeter", "Johannes", ""], ["Strauss", "Tobias", ""], ["Lauer", "Martin", ""]]}, {"id": "1708.00411", "submitter": "Songyou Peng", "authors": "Songyou Peng, Bjoern Haefner, Yvain Qu\\'eau, Daniel Cremers", "title": "Depth Super-Resolution Meets Uncalibrated Photometric Stereo", "comments": "International Conference on Computer Vision (ICCV) Workshop, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel depth super-resolution approach for RGB-D sensors is presented. It\ndisambiguates depth super-resolution through high-resolution photometric clues\nand, symmetrically, it disambiguates uncalibrated photometric stereo through\nlow-resolution depth cues. To this end, an RGB-D sequence is acquired from the\nsame viewing angle, while illuminating the scene from various uncalibrated\ndirections. This sequence is handled by a variational framework which fits\nhigh-resolution shape and reflectance, as well as lighting, to both the\nlow-resolution depth measurements and the high-resolution RGB ones. The key\nnovelty consists in a new PDE-based photometric stereo regularizer which\nimplicitly ensures surface regularity. This allows to carry out depth\nsuper-resolution in a purely data-driven manner, without the need for any\nad-hoc prior or material calibration. Real-world experiments are carried out\nusing an out-of-the-box RGB-D sensor and a hand-held LED light source.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 16:39:56 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 16:24:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Peng", "Songyou", ""], ["Haefner", "Bjoern", ""], ["Qu\u00e9au", "Yvain", ""], ["Cremers", "Daniel", ""]]}, {"id": "1708.00489", "submitter": "Ozan Sener", "authors": "Ozan Sener, Silvio Savarese", "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach", "comments": "ICLR 2018 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully applied to many\nrecognition and learning tasks using a universal recipe; training a deep model\non a very large dataset of supervised examples. However, this approach is\nrather restrictive in practice since collecting a large set of labeled images\nis very expensive. One way to ease this problem is coming up with smart ways\nfor choosing images to be labelled from a very large collection (ie. active\nlearning).\n  Our empirical study suggests that many of the active learning heuristics in\nthe literature are not effective when applied to CNNs in batch setting.\nInspired by these limitations, we define the problem of active learning as\ncore-set selection, ie. choosing set of points such that a model learned over\nthe selected subset is competitive for the remaining data points. We further\npresent a theoretical result characterizing the performance of any selected\nsubset using the geometry of the datapoints. As an active learning algorithm,\nwe choose the subset which is expected to yield best result according to our\ncharacterization. Our experiments show that the proposed method significantly\noutperforms existing approaches in image classification experiments by a large\nmargin.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 19:50:53 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 20:30:22 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 10:55:00 GMT"}, {"version": "v4", "created": "Fri, 1 Jun 2018 10:17:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Sener", "Ozan", ""], ["Savarese", "Silvio", ""]]}, {"id": "1708.00514", "submitter": "Phi Hung Le", "authors": "Phi-Hung Le and Jana Kosecka", "title": "Dense Piecewise Planar RGB-D SLAM for Indoor Environments", "comments": "International Conference on Intelligent Robots and Systems (IROS)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper exploits weak Manhattan constraints to parse the structure of\nindoor environments from RGB-D video sequences in an online setting. We extend\nthe previous approach for single view parsing of indoor scenes to video\nsequences and formulate the problem of recovering the floor plan of the\nenvironment as an optimal labeling problem solved using dynamic programming.\nThe temporal continuity is enforced in a recursive setting, where labeling from\nprevious frames is used as a prior term in the objective function. In addition\nto recovery of piecewise planar weak Manhattan structure of the extended\nenvironment, the orthogonality constraints are also exploited by visual\nodometry and pose graph optimization. This yields reliable estimates in the\npresence of large motions and absence of distinctive features to track. We\nevaluate our method on several challenging indoors sequences demonstrating\naccurate SLAM and dense mapping of low texture environments. On existing TUM\nbenchmark we achieve competitive results with the alternative approaches which\nfail in our environments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 21:07:14 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Le", "Phi-Hung", ""], ["Kosecka", "Jana", ""]]}, {"id": "1708.00573", "submitter": "Lequan Yu", "authors": "Lequan Yu, Jie-Zhi Cheng, Qi Dou, Xin Yang, Hao Chen, Jing Qin,\n  Pheng-Ann Heng", "title": "Automatic 3D Cardiovascular MR Segmentation with Densely-Connected\n  Volumetric ConvNets", "comments": "Accepted at MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic and accurate whole-heart and great vessel segmentation from 3D\ncardiac magnetic resonance (MR) images plays an important role in the\ncomputer-assisted diagnosis and treatment of cardiovascular disease. However,\nthis task is very challenging due to ambiguous cardiac borders and large\nanatomical variations among different subjects. In this paper, we propose a\nnovel densely-connected volumetric convolutional neural network, referred as\nDenseVoxNet, to automatically segment the cardiac and vascular structures from\n3D cardiac MR images. The DenseVoxNet adopts the 3D fully convolutional\narchitecture for effective volume-to-volume prediction. From the learning\nperspective, our DenseVoxNet has three compelling advantages. First, it\npreserves the maximum information flow between layers by a densely-connected\nmechanism and hence eases the network training. Second, it avoids learning\nredundant feature maps by encouraging feature reuse and hence requires fewer\nparameters to achieve high performance, which is essential for medical\napplications with limited training data. Third, we add auxiliary side paths to\nstrengthen the gradient propagation and stabilize the learning process. We\ndemonstrate the effectiveness of DenseVoxNet by comparing it with the\nstate-of-the-art approaches from HVSMR 2016 challenge in conjunction with\nMICCAI, and our network achieves the best dice coefficient. We also show that\nour network can achieve better performance than other 3D ConvNets but with\nfewer parameters.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 01:47:41 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Yu", "Lequan", ""], ["Cheng", "Jie-Zhi", ""], ["Dou", "Qi", ""], ["Yang", "Xin", ""], ["Chen", "Hao", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1708.00577", "submitter": "Di Wu", "authors": "Di Wu, Wenbin Zou, Xia Li, Yong Zhao", "title": "Kernalised Multi-resolution Convnet for Visual Tracking", "comments": "CVPRW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is intrinsically a temporal problem. Discriminative\nCorrelation Filters (DCF) have demonstrated excellent performance for\nhigh-speed generic visual object tracking. Built upon their seminal work, there\nhas been a plethora of recent improvements relying on convolutional neural\nnetwork (CNN) pretrained on ImageNet as a feature extractor for visual\ntracking. However, most of their works relying on ad hoc analysis to design the\nweights for different layers either using boosting or hedging techniques as an\nensemble tracker. In this paper, we go beyond the conventional DCF framework\nand propose a Kernalised Multi-resolution Convnet (KMC) formulation that\nutilises hierarchical response maps to directly output the target movement.\nWhen directly deployed the learnt network to predict the unseen challenging UAV\ntracking dataset without any weight adjustment, the proposed model consistently\nachieves excellent tracking performance. Moreover, the transfered\nmulti-reslution CNN renders it possible to be integrated into the RNN temporal\nlearning framework, therefore opening the door on the end-to-end temporal deep\nlearning (TDL) for visual tracking.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 02:20:12 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Wu", "Di", ""], ["Zou", "Wenbin", ""], ["Li", "Xia", ""], ["Zhao", "Yong", ""]]}, {"id": "1708.00581", "submitter": "He Zhang", "authors": "He Zhang, Vishwanath Sindagi and Vishal M. Patel", "title": "Joint Transmission Map Estimation and Dehazing using Deep Networks", "comments": "This paper has been accepted in IEEE-TCSVT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image haze removal is an extremely challenging problem due to its\ninherent ill-posed nature. Several prior-based and learning-based methods have\nbeen proposed in the literature to solve this problem and they have achieved\nsuperior results. However, most of the existing methods assume constant\natmospheric light model and tend to follow a two-step procedure involving\nprior-based methods for estimating transmission map followed by calculation of\ndehazed image using the closed form solution. In this paper, we relax the\nconstant atmospheric light assumption and propose a novel unified single image\ndehazing network that jointly estimates the transmission map and performs\ndehazing. In other words, our new approach provides an end-to-end learning\nframework, where the inherent transmission map and dehazed result are learned\ndirectly from the loss function. Extensive experiments on synthetic and real\ndatasets with challenging hazy images demonstrate that the proposed method\nachieves significant improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 02:38:41 GMT"}, {"version": "v2", "created": "Sat, 20 Apr 2019 17:52:49 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Zhang", "He", ""], ["Sindagi", "Vishwanath", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1708.00583", "submitter": "Zhang Chen", "authors": "Zhang Chen, Xinqing Guo, Siyuan Li, Xuan Cao and Jingyi Yu", "title": "A Learning-based Framework for Hybrid Depth-from-Defocus and Stereo\n  Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth from defocus (DfD) and stereo matching are two most studied passive\ndepth sensing schemes. The techniques are essentially complementary: DfD can\nrobustly handle repetitive textures that are problematic for stereo matching\nwhereas stereo matching is insensitive to defocus blurs and can handle large\ndepth range. In this paper, we present a unified learning-based technique to\nconduct hybrid DfD and stereo matching. Our input is image triplets: a stereo\npair and a defocused image of one of the stereo views. We first apply\ndepth-guided light field rendering to construct a comprehensive training\ndataset for such hybrid sensing setups. Next, we adopt the hourglass network\narchitecture to separately conduct depth inference from DfD and stereo.\nFinally, we exploit different connection methods between the two separate\nnetworks for integrating them into a unified solution to produce high fidelity\n3D disparity maps. Comprehensive experiments on real and synthetic data show\nthat our new learning-based hybrid 3D sensing technique can significantly\nimprove accuracy and robustness in 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 02:43:04 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 09:58:53 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 17:04:57 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Chen", "Zhang", ""], ["Guo", "Xinqing", ""], ["Li", "Siyuan", ""], ["Cao", "Xuan", ""], ["Yu", "Jingyi", ""]]}, {"id": "1708.00584", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Jiashi Feng", "title": "A Simple Loss Function for Improving the Convergence and Accuracy of\n  Visual Question Answering Models", "comments": "accepted at CVPR 2017 VQA workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering as recently proposed multimodal learning task has\nenjoyed wide attention from the deep learning community. Lately, the focus was\non developing new representation fusion methods and attention mechanisms to\nachieve superior performance. On the other hand, very little focus has been put\non the models' loss function, arguably one of the most important aspects of\ntraining deep learning models. The prevailing practice is to use cross entropy\nloss function that penalizes the probability given to all the answers in the\nvocabulary except the single most common answer for the particular question.\nHowever, the VQA evaluation function compares the predicted answer with all the\nground-truth answers for the given question and if there is a matching, a\npartial point is given. This causes a discrepancy between the model's cross\nentropy loss and the model's accuracy as calculated by the VQA evaluation\nfunction. In this work, we propose a novel loss, termed as soft cross entropy,\nthat considers all ground-truth answers and thus reduces the loss-accuracy\ndiscrepancy. The proposed loss leads to an improved training convergence of VQA\nmodels and an increase in accuracy as much as 1.6%.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 02:51:32 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ilievski", "Ilija", ""], ["Feng", "Jiashi", ""]]}, {"id": "1708.00598", "submitter": "Minhyeok Lee", "authors": "Minhyeok Lee and Junhee Seok", "title": "Controllable Generative Adversarial Network", "comments": "A fully revised version of this paper is published in IEEE Access.\n  Please refer to https://doi.org/10.1109/ACCESS.2019.2899108", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently introduced generative adversarial network (GAN) has been shown\nnumerous promising results to generate realistic samples. The essential task of\nGAN is to control the features of samples generated from a random distribution.\nWhile the current GAN structures, such as conditional GAN, successfully\ngenerate samples with desired major features, they often fail to produce\ndetailed features that bring specific differences among samples. To overcome\nthis limitation, here we propose a controllable GAN (ControlGAN) structure. By\nseparating a feature classifier from a discriminator, the generator of\nControlGAN is designed to learn generating synthetic samples with the specific\ndetailed features. Evaluated with multiple image datasets, ControlGAN shows a\npower to generate improved samples with well-controlled features. Furthermore,\nwe demonstrate that ControlGAN can generate intermediate features and opposite\nfeatures for interpolated and extrapolated input labels that are not used in\nthe training process. It implies that ControlGAN can significantly contribute\nto the variety of generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 04:17:59 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 10:37:48 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 06:21:20 GMT"}, {"version": "v4", "created": "Tue, 1 May 2018 22:39:24 GMT"}, {"version": "v5", "created": "Sat, 30 Mar 2019 08:00:54 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Lee", "Minhyeok", ""], ["Seok", "Junhee", ""]]}, {"id": "1708.00601", "submitter": "Jonathan Jiang", "authors": "Jonathan Q. Jiang, Michael K. Ng", "title": "Exact Tensor Completion from Sparsely Corrupted Observations via Convex\n  Optimization", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper conducts a rigorous analysis for provable estimation of\nmultidimensional arrays, in particular third-order tensors, from a random\nsubset of its corrupted entries. Our study rests heavily on a recently proposed\ntensor algebraic framework in which we can obtain tensor singular value\ndecomposition (t-SVD) that is similar to the SVD for matrices, and define a new\nnotion of tensor rank referred to as the tubal rank. We prove that by simply\nsolving a convex program, which minimizes a weighted combination of tubal\nnuclear norm, a convex surrogate for the tubal rank, and the $\\ell_1$-norm, one\ncan recover an incoherent tensor exactly with overwhelming probability,\nprovided that its tubal rank is not too large and that the corruptions are\nreasonably sparse. Interestingly, our result includes the recovery guarantees\nfor the problems of tensor completion (TC) and tensor principal component\nanalysis (TRPCA) under the same algebraic setup as special cases. An\nalternating direction method of multipliers (ADMM) algorithm is presented to\nsolve this optimization problem. Numerical experiments verify our theory and\nreal-world applications demonstrate the effectiveness of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 04:45:42 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Jiang", "Jonathan Q.", ""], ["Ng", "Michael K.", ""]]}, {"id": "1708.00607", "submitter": "Hwa Pyung Kim", "authors": "Hyung Suk Park, Sung Min Lee, Hwa Pyung Kim, and Jin Keun Seo", "title": "CT sinogram-consistency learning for metal-induced beam hardening\n  correction", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": "10.1002/mp.13199", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a sinogram consistency learning method to deal with\nbeam-hardening related artifacts in polychromatic computerized tomography (CT).\nThe presence of highly attenuating materials in the scan field causes an\ninconsistent sinogram, that does not match the range space of the Radon\ntransform. When the mismatched data are entered into the range space during CT\nreconstruction, streaking and shading artifacts are generated owing to the\ninherent nature of the inverse Radon transform. The proposed learning method\naims to repair inconsistent sinograms by removing the primary metal-induced\nbeam-hardening factors along the metal trace in the sinogram. Taking account of\nthe fundamental difficulty in obtaining sufficient training data in a medical\nenvironment, the learning method is designed to use simulated training data and\na patient-type specific learning model is used to simplify the learning\nprocess. The feasibility of the proposed method is investigated using a\ndataset, consisting of real CT scan of pelvises containing hip prostheses. The\nanatomical areas in training and test data are different, in order to\ndemonstrate that the proposed method extracts the beam hardening features,\nselectively. The results show that our method successfully corrects sinogram\ninconsistency by extracting beam-hardening sources by means of deep learning.\nThis paper proposed a deep learning method of sinogram correction for beam\nhardening reduction in CT for the first time. Conventional methods for beam\nhardening reduction are based on regularizations, and have the fundamental\ndrawback of being not easily able to use manifold CT images, while a deep\nlearning approach has the potential to do so.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 05:36:25 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 07:05:01 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Park", "Hyung Suk", ""], ["Lee", "Sung Min", ""], ["Kim", "Hwa Pyung", ""], ["Seo", "Jin Keun", ""]]}, {"id": "1708.00631", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos", "title": "On the Importance of Consistency in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain that the difficulties of training deep neural networks come from a\nsyndrome of three consistency issues. This paper describes our efforts in their\nanalysis and treatment. The first issue is the training speed inconsistency in\ndifferent layers. We propose to address it with an intuitive,\nsimple-to-implement, low footprint second-order method. The second issue is the\nscale inconsistency between the layer inputs and the layer residuals. We\nexplain how second-order information provides favorable convenience in removing\nthis roadblock. The third and most challenging issue is the inconsistency in\nresidual propagation. Based on the fundamental theorem of linear algebra, we\nprovide a mathematical characterization of the famous vanishing gradient\nproblem. Thus, an important design principle for future optimization and neural\nnetwork design is derived. We conclude this paper with the construction of a\nnovel contractive neural network.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:05:09 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ye", "Chengxi", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1708.00634", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli", "title": "Dual-Glance Model for Deciphering Social Relationships", "comments": "IEEE International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the beginning of early civilizations, social relationships derived from\neach individual fundamentally form the basis of social structure in our daily\nlife. In the computer vision literature, much progress has been made in scene\nunderstanding, such as object detection and scene parsing. Recent research\nfocuses on the relationship between objects based on its functionality and\ngeometrical relations. In this work, we aim to study the problem of social\nrelationship recognition, in still images. We have proposed a dual-glance model\nfor social relationship recognition, where the first glance fixates at the\nindividual pair of interest and the second glance deploys attention mechanism\nto explore contextual cues. We have also collected a new large scale People in\nSocial Context (PISC) dataset, which comprises of 22,670 images and 76,568\nannotated samples from 9 types of social relationship. We provide benchmark\nresults on the PISC dataset, and qualitatively demonstrate the efficacy of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:13:28 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1708.00636", "submitter": "Jaesung Park", "authors": "Jae Sung Park and Nam Ik Cho", "title": "Generation of High Dynamic Range Illumination from a Single Image for\n  the Enhancement of Undesirably Illuminated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm that enhances undesirably illuminated images\nby generating and fusing multi-level illuminations from a single image.The\ninput image is first decomposed into illumination and reflectance components by\nusing an edge-preserving smoothing filter. Then the reflectance component is\nscaled up to improve the image details in bright areas. The illumination\ncomponent is scaled up and down to generate several illumination images that\ncorrespond to certain camera exposure values different from the original. The\nvirtual multi-exposure illuminations are blended into an enhanced illumination,\nwhere we also propose a method to generate appropriate weight maps for the tone\nfusion. Finally, an enhanced image is obtained by multiplying the equalized\nillumination and enhanced reflectance. Experiments show that the proposed\nalgorithm produces visually pleasing output and also yields comparable\nobjective results to the conventional enhancement methods, while requiring\nmodest computational loads.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:14:18 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Park", "Jae Sung", ""], ["Cho", "Nam Ik", ""]]}, {"id": "1708.00666", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Xiaodan Liang, Xiaolong Wang, Dit-Yan Yeung, Abhinav Gupta", "title": "Temporal Dynamic Graph LSTM for Action-driven Video Object Detection", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a weakly-supervised object detection framework.\nMost existing frameworks focus on using static images to learn object\ndetectors. However, these detectors often fail to generalize to videos because\nof the existing domain shift. Therefore, we investigate learning these\ndetectors directly from boring videos of daily activities. Instead of using\nbounding boxes, we explore the use of action descriptions as supervision since\nthey are relatively easy to gather. A common issue, however, is that objects of\ninterest that are not involved in human actions are often absent in global\naction descriptions known as \"missing label\". To tackle this problem, we\npropose a novel temporal dynamic graph Long Short-Term Memory network (TD-Graph\nLSTM). TD-Graph LSTM enables global temporal reasoning by constructing a\ndynamic graph that is based on temporal correlations of object proposals and\nspans the entire video. The missing label issue for each individual frame can\nthus be significantly alleviated by transferring knowledge across correlated\nobjects proposals in the whole video. Extensive evaluations on a large-scale\ndaily-life action dataset (i.e., Charades) demonstrates the superiority of our\nproposed method. We also release object bounding-box annotations for more than\n5,000 frames in Charades. We believe this annotated data can also benefit other\nresearch on video-based object recognition in the future.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 09:38:26 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Yuan", "Yuan", ""], ["Liang", "Xiaodan", ""], ["Wang", "Xiaolong", ""], ["Yeung", "Dit-Yan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1708.00672", "submitter": "Nicola Strisciuglio", "authors": "Alessia Saggese, Nicola Strisciuglio, Mario Vento and Nicolai Petkov", "title": "Action recognition by learning pose representations", "comments": "Accepted at REACTS workshop (CAIP conference 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose detection is one of the fundamental steps for the recognition of human\nactions. In this paper we propose a novel trainable detector for recognizing\nhuman poses based on the analysis of the skeleton. The main idea is that a\nskeleton pose can be described by the spatial arrangements of its joints.\nStarting from this consideration, we propose a trainable pose detector, that\ncan be configured on a prototype skeleton in an automatic configuration\nprocess. The result of the configuration is a model of the position of the\njoints in the concerned skeleton. In the application phase, the joint positions\ncontained in the model are compared with the ones of their homologous joints in\nthe skeleton under test. The similarity of two skeletons is computed as a\ncombination of the position scores achieved by homologous joints. In this paper\nwe describe an action classification method based on the use of the proposed\ntrainable detectors to extract features from the skeletons. We performed\nexperiments on the publicly available MSDRA data set and the achieved results\nconfirm the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 09:46:02 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Saggese", "Alessia", ""], ["Strisciuglio", "Nicola", ""], ["Vento", "Mario", ""], ["Petkov", "Nicolai", ""]]}, {"id": "1708.00674", "submitter": "Andreas Eitel", "authors": "Andres Vasquez, Marina Kollmitz, Andreas Eitel and Wolfram Burgard", "title": "Deep Detection of People and their Mobility Aids for a Hospital Robot", "comments": "7 pages, ECMR 2017, dataset and videos:\n  http://www2.informatik.uni-freiburg.de/~kollmitz/MobilityAids/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots operating in populated environments encounter many different types of\npeople, some of whom might have an advanced need for cautious interaction,\nbecause of physical impairments or their advanced age. Robots therefore need to\nrecognize such advanced demands to provide appropriate assistance, guidance or\nother forms of support. In this paper, we propose a depth-based perception\npipeline that estimates the position and velocity of people in the environment\nand categorizes them according to the mobility aids they use: pedestrian,\nperson in wheelchair, person in a wheelchair with a person pushing them, person\nwith crutches and person using a walker. We present a fast region proposal\nmethod that feeds a Region-based Convolutional Network (Fast R-CNN). With this,\nwe speed up the object detection process by a factor of seven compared to a\ndense sliding window approach. We furthermore propose a probabilistic position,\nvelocity and class estimator to smooth the CNN's detections and account for\nocclusions and misclassifications. In addition, we introduce a new hospital\ndataset with over 17,000 annotated RGB-D images. Extensive experiments confirm\nthat our pipeline successfully keeps track of people and their mobility aids,\neven in challenging situations with multiple people from different categories\nand frequent occlusions. Videos of our experiments and the dataset are\navailable at http://www2.informatik.uni-freiburg.de/~kollmitz/MobilityAids\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 09:54:22 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Vasquez", "Andres", ""], ["Kollmitz", "Marina", ""], ["Eitel", "Andreas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1708.00684", "submitter": "Gjorgji Strezoski", "authors": "Gjorgji Strezoski, Marcel Worring", "title": "OmniArt: Multi-task Deep Learning for Artistic Data Analysis", "comments": "9 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast amounts of artistic data is scattered on-line from both museums and art\napplications. Collecting, processing and studying it with respect to all\naccompanying attributes is an expensive process. With a motivation to speed up\nand improve the quality of categorical analysis in the artistic domain, in this\npaper we propose an efficient and accurate method for multi-task learning with\na shared representation applied in the artistic domain. We continue to show how\ndifferent multi-task configurations of our method behave on artistic data and\noutperform handcrafted feature approaches as well as convolutional neural\nnetworks. In addition to the method and analysis, we propose a challenge like\nnature to the new aggregated data set with almost half a million samples and\nstructured meta-data to encourage further research and societal engagement.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 10:20:22 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Strezoski", "Gjorgji", ""], ["Worring", "Marcel", ""]]}, {"id": "1708.00710", "submitter": "Sangheum Hwang", "authors": "Sangheum Hwang, Sunggyun Park", "title": "Accurate Lung Segmentation via Network-Wise Training of Convolutional\n  Networks", "comments": "Accepted to the 3rd Workshop on Deep Learning in Medical Image\n  Analysis (DLMIA 2017), MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an accurate lung segmentation model for chest radiographs based\non deep convolutional neural networks. Our model is based on atrous\nconvolutional layers to increase the field-of-view of filters efficiently. To\nimprove segmentation performances further, we also propose a multi-stage\ntraining strategy, network-wise training, which the current stage network is\nfed with both input images and the outputs from pre-stage network. It is shown\nthat this strategy has an ability to reduce falsely predicted labels and\nproduce smooth boundaries of lung fields. We evaluate the proposed model on a\ncommon benchmark dataset, JSRT, and achieve the state-of-the-art segmentation\nperformances with much fewer model parameters.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 11:47:01 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Hwang", "Sangheum", ""], ["Park", "Sunggyun", ""]]}, {"id": "1708.00783", "submitter": "Stuart Golodetz", "authors": "Victor Adrian Prisacariu, Olaf K\\\"ahler, Stuart Golodetz, Michael\n  Sapienza, Tommaso Cavallari, Philip H S Torr, David W Murray", "title": "InfiniTAM v3: A Framework for Large-Scale 3D Reconstruction with Loop\n  Closure", "comments": "This article largely supersedes arxiv:1410.0925 (it describes version\n  3 of the InfiniTAM framework)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric models have become a popular representation for 3D scenes in\nrecent years. One breakthrough leading to their popularity was KinectFusion,\nwhich focuses on 3D reconstruction using RGB-D sensors. However, monocular SLAM\nhas since also been tackled with very similar approaches. Representing the\nreconstruction volumetrically as a TSDF leads to most of the simplicity and\nefficiency that can be achieved with GPU implementations of these systems.\nHowever, this representation is memory-intensive and limits applicability to\nsmall-scale reconstructions. Several avenues have been explored to overcome\nthis. With the aim of summarizing them and providing for a fast, flexible 3D\nreconstruction pipeline, we propose a new, unifying framework called InfiniTAM.\nThe idea is that steps like camera tracking, scene representation and\nintegration of new data can easily be replaced and adapted to the user's needs.\n  This report describes the technical implementation details of InfiniTAM v3,\nthe third version of our InfiniTAM system. We have added various new features,\nas well as making numerous enhancements to the low-level code that\nsignificantly improve our camera tracking performance. The new features that we\nexpect to be of most interest are (i) a robust camera tracking module; (ii) an\nimplementation of Glocker et al.'s keyframe-based random ferns camera\nrelocaliser; (iii) a novel approach to globally-consistent TSDF-based\nreconstruction, based on dividing the scene into rigid submaps and optimising\nthe relative poses between them; and (iv) an implementation of Keller et al.'s\nsurfel-based reconstruction approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:50:02 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Prisacariu", "Victor Adrian", ""], ["K\u00e4hler", "Olaf", ""], ["Golodetz", "Stuart", ""], ["Sapienza", "Michael", ""], ["Cavallari", "Tommaso", ""], ["Torr", "Philip H S", ""], ["Murray", "David W", ""]]}, {"id": "1708.00786", "submitter": "DengPing Fan", "authors": "Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li and Ali Borji", "title": "Structure-measure: A New Way to Evaluate Foreground Maps", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Foreground map evaluation is crucial for gauging the progress of object\nsegmentation algorithms, in particular in the filed of salient object detection\nwhere the purpose is to accurately detect and segment the most salient object\nin a scene. Several widely-used measures such as Area Under the Curve (AUC),\nAverage Precision (AP) and the recently proposed Fbw have been utilized to\nevaluate the similarity between a non-binary saliency map (SM) and a\nground-truth (GT) map. These measures are based on pixel-wise errors and often\nignore the structural similarities. Behavioral vision studies, however, have\nshown that the human visual system is highly sensitive to structures in scenes.\nHere, we propose a novel, efficient, and easy to calculate measure known an\nstructural similarity measure (Structure-measure) to evaluate non-binary\nforeground maps. Our new measure simultaneously evaluates region-aware and\nobject-aware structural similarity between a SM and a GT map. We demonstrate\nsuperiority of our measure over existing ones using 5 meta-measures on 5\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:54:16 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Fan", "Deng-Ping", ""], ["Cheng", "Ming-Ming", ""], ["Liu", "Yun", ""], ["Li", "Tao", ""], ["Borji", "Ali", ""]]}, {"id": "1708.00812", "submitter": "Minkyu Choi", "authors": "Minkyu Choi and Jun Tani", "title": "Predictive Coding for Dynamic Visual Processing: Development of\n  Functional Hierarchy in a Multiple Spatio-Temporal Scales RNN Model", "comments": "Accepted in Neural Computation (MIT press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper proposes a novel predictive coding type neural network\nmodel, the predictive multiple spatio-temporal scales recurrent neural network\n(P-MSTRNN). The P-MSTRNN learns to predict visually perceived human whole-body\ncyclic movement patterns by exploiting multiscale spatio-temporal constraints\nimposed on network dynamics by using differently sized receptive fields as well\nas different time constant values for each layer. After learning, the network\nbecomes able to proactively imitate target movement patterns by inferring or\nrecognizing corresponding intentions by means of the regression of prediction\nerror. Results show that the network can develop a functional hierarchy by\ndeveloping a different type of dynamic structure at each layer. The paper\nexamines how model performance during pattern generation as well as predictive\nimitation varies depending on the stage of learning. The number of limit cycle\nattractors corresponding to target movement patterns increases as learning\nproceeds. And, transient dynamics developing early in the learning process\nsuccessfully perform pattern generation and predictive imitation tasks. The\npaper concludes that exploitation of transient dynamics facilitates successful\ntask performance during early learning periods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 16:17:32 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 04:04:04 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1708.00813", "submitter": "Atharva Sharma", "authors": "Atharva Sharma, Xiuwen Liu and Xiaojun Yang", "title": "Land Cover Classification from Multi-temporal, Multi-spectral Remotely\n  Sensed Imagery using Patch-Based Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.05.019", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sustainability of the global environment is dependent on the accurate land\ncover information over large areas. Even with the increased number of satellite\nsystems and sensors acquiring data with improved spectral, spatial, radiometric\nand temporal characteristics and the new data distribution policy, most\nexisting land cover datasets were derived from a pixel-based single-date\nmulti-spectral remotely sensed image with low accuracy. To improve the\naccuracy, the bottleneck is how to develop an accurate and effective image\nclassification technique. By incorporating and utilizing the complete\nmulti-spectral, multi-temporal and spatial information in remote sensing images\nand considering their inherit spatial and sequential interdependence, we\npropose a new patch-based RNN (PB-RNN) system tailored for multi-temporal\nremote sensing data. The system is designed by incorporating distinctive\ncharacteristics in multi-temporal remote sensing data. In particular, it uses\nmulti-temporal-spectral-spatial samples and deals with pixels contaminated by\nclouds/shadow present in the multi-temporal data series. Using a Florida\nEverglades ecosystem study site covering an area of 771 square kilo-meters, the\nproposed PB-RNN system has achieved a significant improvement in the\nclassification accuracy over pixel-based RNN system, pixel-based single-imagery\nNN system, pixel-based multi-images NN system, patch-based single-imagery NN\nsystem and patch-based multi-images NN system. For example, the proposed system\nachieves 97.21% classification accuracy while a pixel-based single-imagery NN\nsystem achieves 64.74%. By utilizing methods like the proposed PB-RNN one, we\nbelieve that much more accurate land cover datasets can be produced over large\nareas efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 16:18:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Sharma", "Atharva", ""], ["Liu", "Xiuwen", ""], ["Yang", "Xiaojun", ""]]}, {"id": "1708.00838", "submitter": "Wen Tao", "authors": "Feng Jiang, Wen Tao, Shaohui Liu, Jie Ren, Xun Guo, Debin Zhao", "title": "An End-to-End Compression Framework Based on Convolutional Neural\n  Networks", "comments": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, e.g., convolutional neural networks (CNNs), has achieved great\nsuccess in image processing and computer vision especially in high level vision\napplications such as recognition and understanding. However, it is rarely used\nto solve low-level vision problems such as image compression studied in this\npaper. Here, we move forward a step and propose a novel compression framework\nbased on CNNs. To achieve high-quality image compression at low bit rates, two\nCNNs are seamlessly integrated into an end-to-end compression framework. The\nfirst CNN, named compact convolutional neural network (ComCNN), learns an\noptimal compact representation from an input image, which preserves the\nstructural information and is then encoded using an image codec (e.g., JPEG,\nJPEG2000 or BPG). The second CNN, named reconstruction convolutional neural\nnetwork (RecCNN), is used to reconstruct the decoded image with high-quality in\nthe decoding end. To make two CNNs effectively collaborate, we develop a\nunified end-to-end learning algorithm to simultaneously learn ComCNN and\nRecCNN, which facilitates the accurate reconstruction of the decoded image\nusing RecCNN. Such a design also makes the proposed compression framework\ncompatible with existing image coding standards. Experimental results validate\nthat the proposed compression framework greatly outperforms several compression\nframeworks that use existing image coding standards with state-of-the-art\ndeblocking or denoising post-processing methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:26:28 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Jiang", "Feng", ""], ["Tao", "Wen", ""], ["Liu", "Shaohui", ""], ["Ren", "Jie", ""], ["Guo", "Xun", ""], ["Zhao", "Debin", ""]]}, {"id": "1708.00884", "submitter": "Saksham Gupta", "authors": "Saksham Gupta, Sukhad Anand, Atul Rai", "title": "Fingerprint Extraction Using Smartphone Camera", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the previous decade, there has been a considerable rise in the usage of\nsmartphones.Due to exorbitant advancement in technology, computational speed\nand quality of image capturing has increased considerably. With an increase in\nthe need for remote fingerprint verification, smartphones can be used as a\npowerful alternative for fingerprint authentication instead of conventional\noptical sensors. In this research, wepropose a technique to capture\nfinger-images from the smartphones and pre-process them in such a way that it\ncan be easily matched with the optical sensor images.Effective finger-image\ncapturing, image enhancement, fingerprint pattern extraction, core point\ndetection and image alignment techniques have been discussed. The proposed\napproach has been validated on FVC 2004 DB1 & DB2 dataset and the results show\nthe efficacy of the methodology proposed. The method can be deployed for\nreal-time commercial usage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 18:33:53 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Gupta", "Saksham", ""], ["Anand", "Sukhad", ""], ["Rai", "Atul", ""]]}, {"id": "1708.00894", "submitter": "Arno Solin", "authors": "Arno Solin, Santiago Cortes, Esa Rahtu, Juho Kannala", "title": "PIVO: Probabilistic Inertial-Visual Odometry for Occlusion-Robust\n  Navigation", "comments": "10 pages, 4 figures. Paper to be published in WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for visual-inertial odometry. The method\nis based on an information fusion framework employing low-cost IMU sensors and\nthe monocular camera in a standard smartphone. We formulate a sequential\ninference scheme, where the IMU drives the dynamical model and the camera\nframes are used in coupling trailing sequences of augmented poses. The novelty\nin the model is in taking into account all the cross-terms in the updates, thus\npropagating the inter-connected uncertainties throughout the model. Stronger\ncoupling between the inertial and visual data sources leads to robustness\nagainst occlusion and feature-poor environments. We demonstrate results on data\ncollected with an iPhone and provide comparisons against the Tango device and\nusing the EuRoC data set.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 18:58:38 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 14:12:24 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Solin", "Arno", ""], ["Cortes", "Santiago", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1708.00919", "submitter": "Yu-Chuan Su", "authors": "Yu-Chuan Su, Kristen Grauman", "title": "Learning Spherical Convolution for Fast Features from 360{\\deg} Imagery", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While 360{\\deg} cameras offer tremendous new possibilities in vision,\ngraphics, and augmented reality, the spherical images they produce make core\nfeature extraction non-trivial. Convolutional neural networks (CNNs) trained on\nimages from perspective cameras yield \"flat\" filters, yet 360{\\deg} images\ncannot be projected to a single plane without significant distortion. A naive\nsolution that repeatedly projects the viewing sphere to all tangent planes is\naccurate, but much too computationally intensive for real problems. We propose\nto learn a spherical convolutional network that translates a planar CNN to\nprocess 360{\\deg} imagery directly in its equirectangular projection. Our\napproach learns to reproduce the flat filter outputs on 360{\\deg} data,\nsensitive to the varying distortion effects across the viewing sphere. The key\nbenefits are 1) efficient feature extraction for 360{\\deg} images and video,\nand 2) the ability to leverage powerful pre-trained networks researchers have\ncarefully honed (together with massive labeled image training sets) for\nperspective images. We validate our approach compared to several alternative\nmethods in terms of both raw CNN output accuracy as well as applying a\nstate-of-the-art \"flat\" object detector to 360{\\deg} data. Our method yields\nthe most accurate results while saving orders of magnitude in computation\nversus the existing exact reprojection solution.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 20:18:10 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 23:24:42 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 17:34:28 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Su", "Yu-Chuan", ""], ["Grauman", "Kristen", ""]]}, {"id": "1708.00931", "submitter": "Sanchit Alekh", "authors": "Abhinav Gupta, Agrim Khanna, Anmol Jagetia, Devansh Sharma, Sanchit\n  Alekh, Vaibhav Choudhary", "title": "Combining Keystroke Dynamics and Face Recognition for User Verification", "comments": null, "journal-ref": null, "doi": "10.1109/CSE.2015.37", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive explosion and ubiquity of computing devices and the outreach of\nthe web have been the most defining events of the century so far. As more and\nmore people gain access to the internet, traditional know-something and\nhave-something authentication methods such as PINs and passwords are proving to\nbe insufficient for prohibiting unauthorized access to increasingly personal\ndata on the web. Therefore, the need of the hour is a user-verification system\nthat is not only more reliable and secure, but also unobtrusive and\nminimalistic. Keystroke Dynamics is a novel Biometric Technique; it is not only\nunobtrusive, but also transparent and inexpensive. The fusion of keystroke\ndynamics and Face Recognition engenders the most desirable characteristics of a\nverification system. Our implementation uses Hidden Markov Models (HMM) for\nmodelling the Keystroke Dynamics, with the help of two widely used Feature\nVectors: Keypress Latency and Keypress Duration. On the other hand, Face\nRecognition makes use of the traditional Eigenfaces approach.The results show\nthat the system has a high precision, with a False Acceptance Rate of 5.4% and\na False Rejection Rate of 9.2%. Moreover, it is also future-proof, as the\nhardware requirements, i.e. camera and keyboard (physical or on-screen), have\nbecome an indispensable part of modern computing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:13:59 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Gupta", "Abhinav", ""], ["Khanna", "Agrim", ""], ["Jagetia", "Anmol", ""], ["Sharma", "Devansh", ""], ["Alekh", "Sanchit", ""], ["Choudhary", "Vaibhav", ""]]}, {"id": "1708.00938", "submitter": "Philip H\\\"ausser", "authors": "Philip Haeusser, Thomas Frerix, Alexander Mordvintsev, Daniel Cremers", "title": "Associative Domain Adaptation", "comments": "In IEEE International Conference on Computer Vision (ICCV), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose associative domain adaptation, a novel technique for end-to-end\ndomain adaptation with neural networks, the task of inferring class labels for\nan unlabeled target domain based on the statistical properties of a labeled\nsource domain. Our training scheme follows the paradigm that in order to\neffectively derive class labels for the target domain, a network should produce\nstatistically domain invariant embeddings, while minimizing the classification\nerror on the labeled source domain. We accomplish this by reinforcing\nassociations between source and target data directly in embedding space. Our\nmethod can easily be added to any existing classification network with no\nstructural and almost no computational overhead. We demonstrate the\neffectiveness of our approach on various benchmarks and achieve\nstate-of-the-art results across the board with a generic convolutional neural\nnetwork architecture not specifically tuned to the respective tasks. Finally,\nwe show that the proposed association loss produces embeddings that are more\neffective for domain adaptation compared to methods employing maximum mean\ndiscrepancy as a similarity measure in embedding space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:38:45 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Haeusser", "Philip", ""], ["Frerix", "Thomas", ""], ["Mordvintsev", "Alexander", ""], ["Cremers", "Daniel", ""]]}, {"id": "1708.00940", "submitter": "Steven Hickson", "authors": "Bryan Willimon, Steven Hickson, Ian Walker, and Stan Birchfield", "title": "An Energy Minimization Approach to 3D Non-Rigid Deformable Surface\n  Estimation Using RGBD Data", "comments": null, "journal-ref": null, "doi": "10.1109/IROS.2012.6386213", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm that uses energy mini- mization to estimate the\ncurrent configuration of a non-rigid object. Our approach utilizes an RGBD\nimage to calculate corresponding SURF features, depth, and boundary informa-\ntion. We do not use predetermined features, thus enabling our system to operate\non unmodified objects. Our approach relies on a 3D nonlinear energy\nminimization framework to solve for the configuration using a semi-implicit\nscheme. Results show various scenarios of dynamic posters and shirts in\ndifferent configurations to illustrate the performance of the method. In\nparticular, we show that our method is able to estimate the configuration of a\ntextureless nonrigid object with no correspondences available.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:43:29 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Willimon", "Bryan", ""], ["Hickson", "Steven", ""], ["Walker", "Ian", ""], ["Birchfield", "Stan", ""]]}, {"id": "1708.00945", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Siyuan Huang, Ping Wei, and Song-Chun Zhu", "title": "Predicting Human Activities Using Stochastic Grammar", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method to predict future human activities from\npartially observed RGB-D videos. Human activity prediction is generally\ndifficult due to its non-Markovian property and the rich context between human\nand environments.\n  We use a stochastic grammar model to capture the compositional structure of\nevents, integrating human actions, objects, and their affordances. We represent\nthe event by a spatial-temporal And-Or graph (ST-AOG). The ST-AOG is composed\nof a temporal stochastic grammar defined on sub-activities, and spatial graphs\nrepresenting sub-activities that consist of human actions, objects, and their\naffordances. Future sub-activities are predicted using the temporal grammar and\nEarley parsing algorithm. The corresponding action, object, and affordance\nlabels are then inferred accordingly. Extensive experiments are conducted to\nshow the effectiveness of our model on both semantic event parsing and future\nactivity prediction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 22:01:48 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Qi", "Siyuan", ""], ["Huang", "Siyuan", ""], ["Wei", "Ping", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1708.00946", "submitter": "Steven Hickson", "authors": "Steven Hickson, Irfan Essa, Henrik Christensen", "title": "Semantic Instance Labeling Leveraging Hierarchical Segmentation", "comments": null, "journal-ref": null, "doi": "10.1109/WACV.2015.147", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the approaches for indoor RGBD semantic la- beling focus on using\npixels or superpixels to train a classi- fier. In this paper, we implement a\nhigher level segmentation using a hierarchy of superpixels to obtain a better\nsegmen- tation for training our classifier. By focusing on meaningful segments\nthat conform more directly to objects, regardless of size, we train a random\nforest of decision trees as a clas- sifier using simple features such as the 3D\nsize, LAB color histogram, width, height, and shape as specified by a his-\ntogram of surface normals. We test our method on the NYU V2 depth dataset, a\nchallenging dataset of cluttered indoor environments. Our experiments using the\nNYU V2 depth dataset show that our method achieves state of the art re- sults\non both a general semantic labeling introduced by the dataset (floor,\nstructure, furniture, and objects) and a more object specific semantic\nlabeling. We show that training a classifier on a segmentation from a hierarchy\nof super pixels yields better results than training directly on super pixels,\npatches, or pixels as in previous work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 22:04:15 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Hickson", "Steven", ""], ["Essa", "Irfan", ""], ["Christensen", "Henrik", ""]]}, {"id": "1708.00953", "submitter": "Vishwanath Sindagi", "authors": "Vishwanath A. Sindagi and Vishal M. Patel", "title": "Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method called Contextual Pyramid CNN (CP-CNN) for\ngenerating high-quality crowd density and count estimation by explicitly\nincorporating global and local contextual information of crowd images. The\nproposed CP-CNN consists of four modules: Global Context Estimator (GCE), Local\nContext Estimator (LCE), Density Map Estimator (DME) and a Fusion-CNN (F-CNN).\nGCE is a VGG-16 based CNN that encodes global context and it is trained to\nclassify input images into different density classes, whereas LCE is another\nCNN that encodes local context information and it is trained to perform\npatch-wise classification of input images into different density classes. DME\nis a multi-column architecture-based CNN that aims to generate high-dimensional\nfeature maps from the input image which are fused with the contextual\ninformation estimated by GCE and LCE using F-CNN. To generate high resolution\nand high-quality density maps, F-CNN uses a set of convolutional and\nfractionally-strided convolutional layers and it is trained along with the DME\nin an end-to-end fashion using a combination of adversarial loss and\npixel-level Euclidean loss. Extensive experiments on highly challenging\ndatasets show that the proposed method achieves significant improvements over\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 22:54:21 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1708.00961", "submitter": "Qingsong Yang", "authors": "Qingsong Yang, Pingkun Yan, Yanbo Zhang, Hengyong Yu, Yongyi Shi,\n  Xuanqin Mou, Mannudeep K. Kalra, and Ge Wang", "title": "Low Dose CT Image Denoising Using a Generative Adversarial Network with\n  Wasserstein Distance and Perceptual Loss", "comments": null, "journal-ref": "IEEE Trans. Med. Imaging. 37(2018) 1348-1357", "doi": "10.1109/TMI.2018.2827462", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new CT image denoising method based on the\ngenerative adversarial network (GAN) with Wasserstein distance and perceptual\nsimilarity. The Wasserstein distance is a key concept of the optimal transform\ntheory, and promises to improve the performance of the GAN. The perceptual loss\ncompares the perceptual features of a denoised output against those of the\nground truth in an established feature space, while the GAN helps migrate the\ndata noise distribution from strong to weak. Therefore, our proposed method\ntransfers our knowledge of visual perception to the image denoising task, is\ncapable of not only reducing the image noise level but also keeping the\ncritical information at the same time. Promising results have been obtained in\nour experiments with clinical CT images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 00:37:11 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 14:28:27 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Yang", "Qingsong", ""], ["Yan", "Pingkun", ""], ["Zhang", "Yanbo", ""], ["Yu", "Hengyong", ""], ["Shi", "Yongyi", ""], ["Mou", "Xuanqin", ""], ["Kalra", "Mannudeep K.", ""], ["Wang", "Ge", ""]]}, {"id": "1708.00973", "submitter": "Junnan Li Mr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan Kankanhalli", "title": "Attention Transfer from Web Images for Video Recognition", "comments": "ACM Multimedia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep learning based video classifiers for action recognition\nrequires a large amount of labeled videos. The labeling process is\nlabor-intensive and time-consuming. On the other hand, large amount of\nweakly-labeled images are uploaded to the Internet by users everyday. To\nharness the rich and highly diverse set of Web images, a scalable approach is\nto crawl these images to train deep learning based classifier, such as\nConvolutional Neural Networks (CNN). However, due to the domain shift problem,\nthe performance of Web images trained deep classifiers tend to degrade when\ndirectly deployed to videos. One way to address this problem is to fine-tune\nthe trained models on videos, but sufficient amount of annotated videos are\nstill required. In this work, we propose a novel approach to transfer knowledge\nfrom image domain to video domain. The proposed method can adapt to the target\ndomain (i.e. video data) with limited amount of training data. Our method maps\nthe video frames into a low-dimensional feature space using the\nclass-discriminative spatial attention map for CNNs. We design a novel Siamese\nEnergyNet structure to learn energy functions on the attention maps by jointly\noptimizing two loss functions, such that the attention map corresponding to a\nground truth concept would have higher energy. We conduct extensive experiments\non two challenging video recognition datasets (i.e. TVHI and UCF101), and\ndemonstrate the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:41:15 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1708.00975", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying, Ge Li, Sixin Wen, Guozhen Tan", "title": "ORGB: Offset Correction in RGB Color Space for Illumination-Robust Image\n  Processing", "comments": "Project website: https://baidut.github.io/ORGB/", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952418", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single materials have colors which form straight lines in RGB space. However,\nin severe shadow cases, those lines do not intersect the origin, which is\ninconsistent with the description of most literature. This paper is concerned\nwith the detection and correction of the offset between the intersection and\norigin. First, we analyze the reason for forming that offset via an optical\nimaging model. Second, we present a simple and effective way to detect and\nremove the offset. The resulting images, named ORGB, have almost the same\nappearance as the original RGB images while are more illumination-robust for\ncolor space conversion. Besides, image processing using ORGB instead of RGB is\nfree from the interference of shadows. Finally, the proposed offset correction\nmethod is applied to road detection task, improving the performance both in\nquantitative and qualitative evaluations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:54:05 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Li", "Ge", ""], ["Wen", "Sixin", ""], ["Tan", "Guozhen", ""]]}, {"id": "1708.00980", "submitter": "Yudong Guo", "authors": "Yudong Guo, Juyong Zhang, Jianfei Cai, Boyi Jiang and Jianmin Zheng", "title": "CNN-based Real-time Dense Face Reconstruction with Inverse-rendered\n  Photo-realistic Face Images", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the powerfulness of convolution neural networks (CNN), CNN based face\nreconstruction has recently shown promising performance in reconstructing\ndetailed face shape from 2D face images. The success of CNN-based methods\nrelies on a large number of labeled data. The state-of-the-art synthesizes such\ndata using a coarse morphable face model, which however has difficulty to\ngenerate detailed photo-realistic images of faces (with wrinkles). This paper\npresents a novel face data generation method. Specifically, we render a large\nnumber of photo-realistic face images with different attributes based on\ninverse rendering. Furthermore, we construct a fine-detailed face image dataset\nby transferring different scales of details from one image to another. We also\nconstruct a large number of video-type adjacent frame pairs by simulating the\ndistribution of real video data. With these nicely constructed datasets, we\npropose a coarse-to-fine learning framework consisting of three convolutional\nnetworks. The networks are trained for real-time detailed 3D face\nreconstruction from monocular video as well as from a single image. Extensive\nexperimental results demonstrate that our framework can produce high-quality\nreconstruction but with much less computation time compared to the\nstate-of-the-art. Moreover, our method is robust to pose, expression and\nlighting due to the diversity of data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 03:18:34 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 11:32:01 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 07:02:35 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Guo", "Yudong", ""], ["Zhang", "Juyong", ""], ["Cai", "Jianfei", ""], ["Jiang", "Boyi", ""], ["Zheng", "Jianmin", ""]]}, {"id": "1708.00983", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi, Jeremy Burt, Ulas Bagci", "title": "Multi-Planar Deep Segmentation Networks for Cardiac Substructures from\n  MRI and CT", "comments": "The paper is accepted to STACOM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive detection of cardiovascular disorders from radiology scans\nrequires quantitative image analysis of the heart and its substructures. There\nare well-established measurements that radiologists use for diseases assessment\nsuch as ejection fraction, volume of four chambers, and myocardium mass. These\nmeasurements are derived as outcomes of precise segmentation of the heart and\nits substructures. The aim of this paper is to provide such measurements\nthrough an accurate image segmentation algorithm that automatically delineates\nseven substructures of the heart from MRI and/or CT scans. Our proposed method\nis based on multi-planar deep convolutional neural networks (CNN) with an\nadaptive fusion strategy where we automatically utilize complementary\ninformation from different planes of the 3D scans for improved delineations.\nFor CT and MRI, we have separately designed three CNNs (the same architectural\nconfiguration) for three planes, and have trained the networks from scratch for\nvoxel-wise labeling for the following cardiac structures: myocardium of left\nventricle (Myo), left atrium (LA), left ventricle (LV), right atrium (RA),\nright ventricle (RV), ascending aorta (Ao), and main pulmonary artery (PA). We\nhave evaluated the proposed method with 4-fold-cross validation on the\nmulti-modality whole heart segmentation challenge (MM-WHS 2017) dataset. The\nprecision and dice index of 0.93 and 0.90, and 0.87 and 0.85 were achieved for\nCT and MR images, respectively. While a CT volume was segmented about 50\nseconds, an MRI scan was segmented around 17 seconds with the GPUs/CUDA\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 03:30:03 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1708.00999", "submitter": "Michael S. Ryoo", "authors": "Michael S. Ryoo, Kiyoon Kim, Hyun Jong Yang", "title": "Extreme Low Resolution Activity Recognition with Multi-Siamese Embedding\n  Learning", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for recognizing human activities from extreme\nlow resolution (e.g., 16x12) videos. Extreme low resolution recognition is not\nonly necessary for analyzing actions at a distance but also is crucial for\nenabling privacy-preserving recognition of human activities. We design a new\ntwo-stream multi-Siamese convolutional neural network. The idea is to\nexplicitly capture the inherent property of low resolution (LR) videos that two\nimages originated from the exact same scene often have totally different pixel\nvalues depending on their LR transformations. Our approach learns the shared\nembedding space that maps LR videos with the same content to the same location\nregardless of their transformations. We experimentally confirm that our\napproach of jointly learning such transform robust LR video representation and\nthe classifier outperforms the previous state-of-the-art low resolution\nrecognition approaches on two public standard datasets by a meaningful margin.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 04:52:28 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 03:46:19 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Ryoo", "Michael S.", ""], ["Kim", "Kiyoon", ""], ["Yang", "Hyun Jong", ""]]}, {"id": "1708.01001", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Renkun Ni, Jianguo Li, Yurong Chen, Jun Zhu, Hang Su", "title": "Learning Accurate Low-Bit Deep Neural Networks with Stochastic\n  Quantization", "comments": "BMVC 2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-bit deep neural networks (DNNs) become critical for embedded applications\ndue to their low storage requirement and computing efficiency. However, they\nsuffer much from the non-negligible accuracy drop. This paper proposes the\nstochastic quantization (SQ) algorithm for learning accurate low-bit DNNs. The\nmotivation is due to the following observation. Existing training algorithms\napproximate the real-valued elements/filters with low-bit representation all\ntogether in each iteration. The quantization errors may be small for some\nelements/filters, while are remarkable for others, which lead to inappropriate\ngradient direction during training, and thus bring notable accuracy drop.\nInstead, SQ quantizes a portion of elements/filters to low-bit with a\nstochastic probability inversely proportional to the quantization error, while\nkeeping the other portion unchanged with full-precision. The quantized and\nfull-precision portions are updated with corresponding gradients separately in\neach iteration. The SQ ratio is gradually increased until the whole network is\nquantized. This procedure can greatly compensate the quantization error and\nthus yield better accuracy for low-bit DNNs. Experiments show that SQ can\nconsistently and significantly improve the accuracy for different low-bit DNNs\non various datasets and various network structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 05:16:25 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Dong", "Yinpeng", ""], ["Ni", "Renkun", ""], ["Li", "Jianguo", ""], ["Chen", "Yurong", ""], ["Zhu", "Jun", ""], ["Su", "Hang", ""]]}, {"id": "1708.01008", "submitter": "Lei Zhang", "authors": "Lei Zhang, Wei Wei, Qinfeng Shi, Chunhua Shen, Anton van den Hengel\n  and Yanning Zhang", "title": "Beyond Low Rank: A Data-Adaptive Tensor Completion Method", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low rank tensor representation underpins much of recent progress in tensor\ncompletion. In real applications, however, this approach is confronted with two\nchallenging problems, namely (1) tensor rank determination; (2) handling real\ntensor data which only approximately fulfils the low-rank requirement. To\naddress these two issues, we develop a data-adaptive tensor completion model\nwhich explicitly represents both the low-rank and non-low-rank structures in a\nlatent tensor. Representing the non-low-rank structure separately from the\nlow-rank one allows priors which capture the important distinctions between the\ntwo, thus enabling more accurate modelling, and ultimately, completion. Through\ndefining a new tensor rank, we develop a sparsity induced prior for the\nlow-rank structure, with which the tensor rank can be automatically determined.\nThe prior for the non-low-rank structure is established based on a mixture of\nGaussians which is shown to be flexible enough, and powerful enough, to inform\nthe completion process for a variety of real tensor data. With these two\npriors, we develop a Bayesian minimum mean squared error estimate (MMSE)\nframework for inference which provides the posterior mean of missing entries as\nwell as their uncertainty. Compared with the state-of-the-art methods in\nvarious applications, the proposed model produces more accurate completion\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 05:51:27 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Zhang", "Lei", ""], ["Wei", "Wei", ""], ["Shi", "Qinfeng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Zhang", "Yanning", ""]]}, {"id": "1708.01015", "submitter": "Stefan Braun", "authors": "Stefan Braun, Daniel Neil, Enea Ceolini, Jithendar Anumula, Shih-Chii\n  Liu", "title": "Sensor Transformation Attention Networks", "comments": "8 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on encoder-decoder models for sequence-to-sequence mapping has\nshown that integrating both temporal and spatial attention mechanisms into\nneural networks increases the performance of the system substantially. In this\nwork, we report on the application of an attentional signal not on temporal and\nspatial regions of the input, but instead as a method of switching among inputs\nthemselves. We evaluate the particular role of attentional switching in the\npresence of dynamic noise in the sensors, and demonstrate how the attentional\nsignal responds dynamically to changing noise levels in the environment to\nachieve increased performance on both audio and visual tasks in three\ncommonly-used datasets: TIDIGITS, Wall Street Journal, and GRID. Moreover, the\nproposed sensor transformation network architecture naturally introduces a\nnumber of advantages that merit exploration, including ease of adding new\nsensors to existing architectures, attentional interpretability, and increased\nrobustness in a variety of noisy environments not seen during training.\nFinally, we demonstrate that the sensor selection attention mechanism of a\nmodel trained only on the small TIDIGITS dataset can be transferred directly to\na pre-existing larger network trained on the Wall Street Journal dataset,\nmaintaining functionality of switching between sensors to yield a dramatic\nreduction of error in the presence of noise.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 06:35:36 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Braun", "Stefan", ""], ["Neil", "Daniel", ""], ["Ceolini", "Enea", ""], ["Anumula", "Jithendar", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "1708.01022", "submitter": "Jacopo Cavazza", "authors": "Jacopo Cavazza, Pietro Morerio, Vittorio Murino", "title": "When Kernel Methods meet Feature Learning: Log-Covariance Network for\n  Action Recognition from Skeletal Data", "comments": "2017 IEEE Computer Vision and Pattern Recognition (CVPR) Workshops", "journal-ref": null, "doi": "10.1109/CVPRW.2017.165", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition from skeletal data is a hot research topic and\nimportant in many open domain applications of computer vision, thanks to\nrecently introduced 3D sensors. In the literature, naive methods simply\ntransfer off-the-shelf techniques from video to the skeletal representation.\nHowever, the current state-of-the-art is contended between to different\nparadigms: kernel-based methods and feature learning with (recurrent) neural\nnetworks. Both approaches show strong performances, yet they exhibit heavy, but\ncomplementary, drawbacks. Motivated by this fact, our work aims at combining\ntogether the best of the two paradigms, by proposing an approach where a\nshallow network is fed with a covariance representation. Our intuition is that,\nas long as the dynamics is effectively modeled, there is no need for the\nclassification network to be deep nor recurrent in order to score favorably. We\nvalidate this hypothesis in a broad experimental analysis over 6 publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 06:52:51 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Cavazza", "Jacopo", ""], ["Morerio", "Pietro", ""], ["Murino", "Vittorio", ""]]}, {"id": "1708.01034", "submitter": "Jacopo Cavazza", "authors": "Andrea Zunino, Jacopo Cavazza, Atesh Koul, Andrea Cavallo, Cristina\n  Becchio and Vittorio Murino", "title": "What Will I Do Next? The Intention from Motion Experiment", "comments": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops", "journal-ref": null, "doi": "10.1109/CVPRW.2017.7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, video-based approaches have been widely explored for the\nearly classification and the prediction of actions or activities. However, it\nremains unclear whether this modality (as compared to 3D kinematics) can still\nbe reliable for the prediction of human intentions, defined as the overarching\ngoal embedded in an action sequence. Since the same action can be performed\nwith different intentions, this problem is more challenging but yet affordable\nas proved by quantitative cognitive studies which exploit the 3D kinematics\nacquired through motion capture systems. In this paper, we bridge cognitive and\ncomputer vision studies, by demonstrating the effectiveness of video-based\napproaches for the prediction of human intentions. Precisely, we propose\nIntention from Motion, a new paradigm where, without using any contextual\ninformation, we consider instantaneous grasping motor acts involving a bottle\nin order to forecast why the bottle itself has been reached (to pass it or to\nplace in a box, or to pour or to drink the liquid inside). We process only the\ngrasping onsets casting intention prediction as a classification framework.\nLeveraging on our multimodal acquisition (3D motion capture data and 2D optical\nvideos), we compare the most commonly used 3D descriptors from cognitive\nstudies with state-of-the-art video-based techniques. Since the two analyses\nachieve an equivalent performance, we demonstrate that computer vision tools\nare effective in capturing the kinematics and facing the cognitive problem of\nhuman intention prediction.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 07:37:58 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Zunino", "Andrea", ""], ["Cavazza", "Jacopo", ""], ["Koul", "Atesh", ""], ["Cavallo", "Andrea", ""], ["Becchio", "Cristina", ""], ["Murino", "Vittorio", ""]]}, {"id": "1708.01089", "submitter": "Pedram Ghamisi Dr.", "authors": "Shaghayegh Kargozar Nahavandya and Lalit Kumar and Pedram Ghamisi", "title": "Using the SLEUTH urban growth model to simulate the impacts of future\n  policy scenarios on urban land use in the Tehran metropolitan area in Iran", "comments": "27 pages, 6 figures, and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SLEUTH model, based on the Cellular Automata (CA), can be applied to city\ndevelopment simulation in metropolitan areas. In this study the SLEUTH model\nwas used to model the urban expansion and predict the future possible behavior\nof the urban growth in Tehran. The fundamental data were five Landsat TM and\nETM images of 1988, 1992, 1998, 2001 and 2010. Three scenarios were designed to\nsimulate the spatial pattern. The first scenario assumed historical\nurbanization mode would persist and the only limitations for development were\nheight and slope. The second one was a compact scenario which makes the growth\nmostly internal and limited the expansion of suburban areas. The last scenario\nproposed a polycentric urban structure which let the little patches grow\nwithout any limitation and would not consider the areas beyond the specific\nbuffer zone from the larger patches for development. Results showed that the\nurban growth rate was greater in the first scenario in comparison with the\nother two scenarios. Also it was shown that the third scenario was more\nsuitable for Tehran since it could avoid undesirable effects such as congestion\nand pollution and was more in accordance with the conditions of Tehran city.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 10:38:51 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Nahavandya", "Shaghayegh Kargozar", ""], ["Kumar", "Lalit", ""], ["Ghamisi", "Pedram", ""]]}, {"id": "1708.01101", "submitter": "Wei Yang", "authors": "Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, Xiaogang Wang", "title": "Learning Feature Pyramids for Human Pose Estimation", "comments": "Submitted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulated human pose estimation is a fundamental yet challenging task in\ncomputer vision. The difficulty is particularly pronounced in scale variations\nof human body parts when camera view changes or severe foreshortening happens.\nAlthough pyramid methods are widely used to handle scale changes at inference\ntime, learning feature pyramids in deep convolutional neural networks (DCNNs)\nis still not well explored. In this work, we design a Pyramid Residual Module\n(PRMs) to enhance the invariance in scales of DCNNs. Given input features, the\nPRMs learn convolutional filters on various scales of input features, which are\nobtained with different subsampling ratios in a multi-branch network. Moreover,\nwe observe that it is inappropriate to adopt existing methods to initialize the\nweights of multi-branch networks, which achieve superior performance than plain\nnetworks in many tasks recently. Therefore, we provide theoretic derivation to\nextend the current weight initialization scheme to multi-branch network\nstructures. We investigate our method on two standard benchmarks for human pose\nestimation. Our approach obtains state-of-the-art results on both benchmarks.\nCode is available at https://github.com/bearpaw/PyraNet.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 11:26:04 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Yang", "Wei", ""], ["Li", "Shuang", ""], ["Ouyang", "Wanli", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1708.01125", "submitter": "Rajvi Shah", "authors": "Rajvi Shah, Visesh Chari, P J Narayanan", "title": "A Unified View-Graph Selection Framework for Structure from Motion", "comments": "Submitted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  View-graph is an essential input to large-scale structure from motion (SfM)\npipelines. Accuracy and efficiency of large-scale SfM is crucially dependent on\nthe input view-graph. Inconsistent or inaccurate edges can lead to inferior or\nwrong reconstruction. Most SfM methods remove `undesirable' images and pairs\nusing several, fixed heuristic criteria, and propose tailor-made solutions to\nachieve specific reconstruction objectives such as efficiency, accuracy, or\ndisambiguation. In contrast to these disparate solutions, we propose a single\noptimization framework that can be used to achieve these different\nreconstruction objectives with task-specific cost modeling. We also construct a\nvery efficient network-flow based formulation for its approximate solution. The\nabstraction brought on by this selection mechanism separates the challenges\nspecific to datasets and reconstruction objectives from the standard SfM\npipeline and improves its generalization. This paper demonstrates the\napplication of the proposed view-graph framework with standard SfM pipeline for\ntwo particular use-cases, (i) accurate and ghost-free reconstructions of highly\nambiguous datasets using costs based on disambiguation priors, and (ii)\naccurate and efficient reconstruction of large-scale Internet datasets using\ncosts based on commonly used priors.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 13:17:01 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 11:56:25 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Shah", "Rajvi", ""], ["Chari", "Visesh", ""], ["Narayanan", "P J", ""]]}, {"id": "1708.01141", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Tim Leiner and Max A. Viergever and Ivana\n  Isgum", "title": "Automatic Segmentation and Disease Classification Using Cardiac Cine MR\n  Images", "comments": "Accepted in STACOM Automated Cardiac Diagnosis Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of the heart in cardiac cine MR is clinically used to quantify\ncardiac function. We propose a fully automatic method for segmentation and\ndisease classification using cardiac cine MR images. A convolutional neural\nnetwork (CNN) was designed to simultaneously segment the left ventricle (LV),\nright ventricle (RV) and myocardium in end-diastole (ED) and end-systole (ES)\nimages. Features derived from the obtained segmentations were used in a Random\nForest classifier to label patients as suffering from dilated cardiomyopathy,\nhypertrophic cardiomyopathy, heart failure following myocardial infarction,\nright ventricular abnormality, or no cardiac disease. The method was developed\nand evaluated using a balanced dataset containing images of 100 patients, which\nwas provided in the MICCAI 2017 automated cardiac diagnosis challenge (ACDC).\nThe segmentation and classification pipeline were evaluated in a four-fold\nstratified cross-validation. Average Dice scores between reference and\nautomatically obtained segmentations were 0.94, 0.88 and 0.87 for the LV, RV\nand myocardium. The classifier assigned 91% of patients to the correct disease\ncategory. Segmentation and disease classification took 5 s per patient. The\nresults of our study suggest that image-based diagnosis using cine MR cardiac\nscans can be performed automatically with high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 13:55:24 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Leiner", "Tim", ""], ["Viergever", "Max A.", ""], ["Isgum", "Ivana", ""]]}, {"id": "1708.01142", "submitter": "Casey Pellizzari", "authors": "Casey J. Pellizzari, Mark F. Spencer, Charles A. Bouman", "title": "Phase-error estimation and image reconstruction from digital-holography\n  data using a Bayesian framework", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": "10.1364/JOSAA.34.001659", "report-no": null, "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of phase errors from digital-holography data is critical for\napplications such as imaging or wave-front sensing. Conventional techniques\nrequire multiple i.i.d. data and perform poorly in the presence of high noise\nor large phase errors. In this paper we propose a method to estimate\nisoplanatic phase errors from a single data realization. We develop a\nmodel-based iterative reconstruction algorithm which computes the maximum a\nposteriori estimate of the phase and the speckle-free object reflectance. Using\nsimulated data, we show that the algorithm is robust against high noise and\nstrong phase errors.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 14:07:10 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Pellizzari", "Casey J.", ""], ["Spencer", "Mark F.", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1708.01143", "submitter": "Marcelo Saval Calvo", "authors": "Marcelo Saval-Calvo and Jorge Azorin-Lopez and Andres Fuster-Guillo\n  and Jose Garcia-Rodriguez", "title": "Three-dimensional planar model estimation using multi-constraint\n  knowledge based on k-means and RANSAC", "comments": null, "journal-ref": "Applied Soft Computing, Vol. 34, p. 572-586 (2015)", "doi": "10.1016/j.asoc.2015.05.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plane model extraction from three-dimensional point clouds is a necessary\nstep in many different applications such as planar object reconstruction,\nindoor mapping and indoor localization. Different RANdom SAmple Consensus\n(RANSAC)-based methods have been proposed for this purpose in recent years. In\nthis study, we propose a novel method-based on RANSAC called Multiplane Model\nEstimation, which can estimate multiple plane models simultaneously from a\nnoisy point cloud using the knowledge extracted from a scene (or an object) in\norder to reconstruct it accurately. This method comprises two steps: first, it\nclusters the data into planar faces that preserve some constraints defined by\nknowledge related to the object (e.g., the angles between faces); and second,\nthe models of the planes are estimated based on these data using a novel\nmulti-constraint RANSAC. We performed experiments in the clustering and RANSAC\nstages, which showed that the proposed method performed better than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 13:58:18 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Saval-Calvo", "Marcelo", ""], ["Azorin-Lopez", "Jorge", ""], ["Fuster-Guillo", "Andres", ""], ["Garcia-Rodriguez", "Jose", ""]]}, {"id": "1708.01155", "submitter": "Jelmer Wolterink", "authors": "Jelmer M. Wolterink and Anna M. Dinkla and Mark H.F. Savenije and\n  Peter R. Seevinck and Cornelis A.T. van den Berg and Ivana Isgum", "title": "Deep MR to CT Synthesis using Unpaired Data", "comments": "MICCAI 2017 Workshop on Simulation and Synthesis in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MR-only radiotherapy treatment planning requires accurate MR-to-CT synthesis.\nCurrent deep learning methods for MR-to-CT synthesis depend on pairwise aligned\nMR and CT training images of the same patient. However, misalignment between\npaired images could lead to errors in synthesized CT images. To overcome this,\nwe propose to train a generative adversarial network (GAN) with unpaired MR and\nCT images. A GAN consisting of two synthesis convolutional neural networks\n(CNNs) and two discriminator CNNs was trained with cycle consistency to\ntransform 2D brain MR image slices into 2D brain CT image slices and vice\nversa. Brain MR and CT images of 24 patients were analyzed. A quantitative\nevaluation showed that the model was able to synthesize CT images that closely\napproximate reference CT images, and was able to outperform a GAN model trained\nwith paired MR and CT images.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 14:18:43 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Wolterink", "Jelmer M.", ""], ["Dinkla", "Anna M.", ""], ["Savenije", "Mark H. F.", ""], ["Seevinck", "Peter R.", ""], ["Berg", "Cornelis A. T. van den", ""], ["Isgum", "Ivana", ""]]}, {"id": "1708.01179", "submitter": "Xiaofei Du Ms", "authors": "Xiaofei Du, Alessio Dore, Danail Stoyanov", "title": "Patch-based adaptive weighting with segmentation and scale (PAWSS) for\n  visual tracking", "comments": "10 pages, 8 figures. The paper is under consideration at Pattern\n  Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking-by-detection algorithms are widely used for visual tracking, where\nthe problem is treated as a classification task where an object model is\nupdated over time using online learning techniques. In challenging conditions\nwhere an object undergoes deformation or scale variations, the update step is\nprone to include background information in the model appearance or to lack the\nability to estimate the scale change, which degrades the performance of the\nclassifier. In this paper, we incorporate a Patch-based Adaptive Weighting with\nSegmentation and Scale (PAWSS) tracking framework that tackles both the scale\nand background problems. A simple but effective colour-based segmentation model\nis used to suppress background information and multi-scale samples are\nextracted to enrich the training pool, which allows the tracker to handle both\nincremental and abrupt scale variations between frames. Experimentally, we\nevaluate our approach on the online tracking benchmark (OTB) dataset and Visual\nObject Tracking (VOT) challenge datasets. The results show that our approach\noutperforms recent state-of-the-art trackers, and it especially improves the\nsuccessful rate score on the OTB dataset, while on the VOT datasets, PAWSS\nranks among the top trackers while operating at real-time frame rates.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 15:19:27 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Du", "Xiaofei", ""], ["Dore", "Alessio", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1708.01191", "submitter": "Timo Milbich", "authors": "Timo Milbich, Miguel Bautista, Ekaterina Sutter, Bjorn Ommer", "title": "Unsupervised Video Understanding by Reconciliation of Posture\n  Similarities", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human activity and being able to explain it in detail surpasses\nmere action classification by far in both complexity and value. The challenge\nis thus to describe an activity on the basis of its most fundamental\nconstituents, the individual postures and their distinctive transitions.\nSupervised learning of such a fine-grained representation based on elementary\nposes is very tedious and does not scale. Therefore, we propose a completely\nunsupervised deep learning procedure based solely on video sequences, which\nstarts from scratch without requiring pre-trained networks, predefined body\nmodels, or keypoints. A combinatorial sequence matching algorithm proposes\nrelations between frames from subsets of the training data, while a CNN is\nreconciling the transitivity conflicts of the different subsets to learn a\nsingle concerted pose embedding despite changes in appearance across sequences.\nWithout any manual annotation, the model learns a structured representation of\npostures and their temporal development. The model not only enables retrieval\nof similar postures but also temporal super-resolution. Additionally, based on\na recurrent formulation, next frames can be synthesized.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:09:03 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Milbich", "Timo", ""], ["Bautista", "Miguel", ""], ["Sutter", "Ekaterina", ""], ["Ommer", "Bjorn", ""]]}, {"id": "1708.01198", "submitter": "Jithin George", "authors": "Jithin Donny George, Ronan Keane and Conor Zellmer", "title": "Estimating speech from lip dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to develop a limited lip reading algorithm for a\nsubset of the English language. We consider a scenario in which no audio\ninformation is available. The raw video is processed and the position of the\nlips in each frame is extracted. We then prepare the lip data for processing\nand classify the lips into visemes and phonemes. Hidden Markov Models are used\nto predict the words the speaker is saying based on the sequences of classified\nphonemes and visemes. The GRID audiovisual sentence corpus [10][11] database is\nused for our study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:23:13 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["George", "Jithin Donny", ""], ["Keane", "Ronan", ""], ["Zellmer", "Conor", ""]]}, {"id": "1708.01204", "submitter": "Ariel Ephrat", "authors": "Ariel Ephrat, Tavi Halperin, Shmuel Peleg", "title": "Improved Speech Reconstruction from Silent Video", "comments": "Accepted to ICCV 2017 Workshop on Computer Vision for Audio-Visual\n  Media. Supplementary video: https://www.youtube.com/watch?v=Xjbn7h7tpg0.\n  arXiv admin note: text overlap with arXiv:1701.00495", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speechreading is the task of inferring phonetic information from visually\nobserved articulatory facial movements, and is a notoriously difficult task for\nhumans to perform. In this paper we present an end-to-end model based on a\nconvolutional neural network (CNN) for generating an intelligible and\nnatural-sounding acoustic speech signal from silent video frames of a speaking\nperson. We train our model on speakers from the GRID and TCD-TIMIT datasets,\nand evaluate the quality and intelligibility of reconstructed speech using\ncommon objective measurements. We show that speech predictions from the\nproposed model attain scores which indicate significantly improved quality over\nexisting models. In addition, we show promising results towards reconstructing\nspeech from an unconstrained dictionary.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 18:01:08 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 20:59:52 GMT"}, {"version": "v3", "created": "Tue, 29 Aug 2017 19:45:23 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Ephrat", "Ariel", ""], ["Halperin", "Tavi", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1708.01208", "submitter": "Long Chen", "authors": "Long Chen, Karl Francis, Wen Tang", "title": "Semantic Augmented Reality Environment with Material-Aware Physical\n  Interactions", "comments": "ISMAR 2017 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Augmented Reality (AR) environment, realistic interactions between the\nvirtual and real objects play a crucial role in user experience. Much of recent\nadvances in AR has been largely focused on developing geometry-aware\nenvironment, but little has been done in dealing with interactions at the\nsemantic level. High-level scene understanding and semantic descriptions in AR\nwould allow effective design of complex applications and enhanced user\nexperience. In this paper, we present a novel approach and a prototype system\nthat enables the deeper understanding of semantic properties of the real world\nenvironment, so that realistic physical interactions between the real and the\nvirtual objects can be generated. A material-aware AR environment has been\ncreated based on the deep material learning using a fully convolutional network\n(FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM)\nhas been used for the semantic mapping. Together with efficient accelerated 3D\nray casting, natural and realistic physical interactions are generated for\ninteractive AR games. Our approach has significant impact on the future\ndevelopment of advanced AR systems and applications.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:52:14 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 15:31:08 GMT"}, {"version": "v3", "created": "Fri, 16 Mar 2018 14:29:07 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Chen", "Long", ""], ["Francis", "Karl", ""], ["Tang", "Wen", ""]]}, {"id": "1708.01225", "submitter": "Long Chen", "authors": "Long Chen, Thomas Day, Wen Tang, Nigel W. John", "title": "Recent Developments and Future Challenges in Medical Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Reality (MR) is of increasing interest within technology-driven modern\nmedicine but is not yet used in everyday practice. This situation is changing\nrapidly, however, and this paper explores the emergence of MR technology and\nthe importance of its utility within medical applications. A classification of\nmedical MR has been obtained by applying an unbiased text mining method to a\ndatabase of 1,403 relevant research papers published over the last two decades.\nThe classification results reveal a taxonomy for the development of medical MR\nresearch during this period as well as suggesting future trends. We then use\nthe classification to analyse the technology and applications developed in the\nlast five years. Our objective is to aid researchers to focus on the areas\nwhere technology advancements in medical MR are most needed, as well as\nproviding medical practitioners with a useful source of reference.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:15:18 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Chen", "Long", ""], ["Day", "Thomas", ""], ["Tang", "Wen", ""], ["John", "Nigel W.", ""]]}, {"id": "1708.01234", "submitter": "Long Chen", "authors": "Long Chen, Wen Tang, Nigel W. John", "title": "Real-time Geometry-Aware Augmented Reality in Minimally Invasive Surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential of Augmented Reality (AR) technology to assist minimally\ninvasive surgeries (MIS) lies in its computational performance and accuracy in\ndealing with challenging MIS scenes. Even with the latest hardware and software\ntechnologies, achieving both real-time and accurate augmented information\noverlay in MIS is still a formidable task. In this paper, we present a novel\nreal-time AR framework for MIS that achieves interactive geometric aware\naugmented reality in endoscopic surgery with stereo views. Our framework tracks\nthe movement of the endoscopic camera and simultaneously reconstructs a dense\ngeometric mesh of the MIS scene. The movement of the camera is predicted by\nminimising the re-projection error to achieve a fast tracking performance,\nwhile the 3D mesh is incrementally built by a dense zero mean normalised cross\ncorrelation stereo matching method to improve the accuracy of the surface\nreconstruction. Our proposed system does not require any prior template or\npre-operative scan and can infer the geometric information intra-operatively in\nreal-time. With the geometric information available, our proposed AR framework\nis able to interactively add annotations, localisation of tumours and vessels,\nand measurement labelling with greater precision and accuracy compared with the\nstate of the art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:25:38 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Chen", "Long", ""], ["Tang", "Wen", ""], ["John", "Nigel W.", ""]]}, {"id": "1708.01241", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zhuang Liu and Jianguo Li and Yu-Gang Jiang and\n  Yurong Chen and Xiangyang Xue", "title": "DSOD: Learning Deeply Supervised Object Detectors from Scratch", "comments": "ICCV 2017. Code and models are available at:\n  https://github.com/szq0214/DSOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deeply Supervised Object Detector (DSOD), a framework that can\nlearn object detectors from scratch. State-of-the-art object objectors rely\nheavily on the off-the-shelf networks pre-trained on large-scale classification\ndatasets like ImageNet, which incurs learning bias due to the difference on\nboth the loss functions and the category distributions between classification\nand detection tasks. Model fine-tuning for the detection task could alleviate\nthis bias to some extent but not fundamentally. Besides, transferring\npre-trained models from classification to detection between discrepant domains\nis even more difficult (e.g. RGB to depth images). A better solution to tackle\nthese two critical problems is to train object detectors from scratch, which\nmotivates our proposed DSOD. Previous efforts in this direction mostly failed\ndue to much more complicated loss functions and limited training data in object\ndetection. In DSOD, we contribute a set of design principles for training\nobject detectors from scratch. One of the key findings is that deep\nsupervision, enabled by dense layer-wise connections, plays a critical role in\nlearning a good detector. Combining with several other principles, we develop\nDSOD following the single-shot detection (SSD) framework. Experiments on PASCAL\nVOC 2007, 2012 and MS COCO datasets demonstrate that DSOD can achieve better\nresults than the state-of-the-art solutions with much more compact models. For\ninstance, DSOD outperforms SSD on all three benchmarks with real-time detection\nspeed, while requires only 1/2 parameters to SSD and 1/10 parameters to Faster\nRCNN. Our code and models are available at: https://github.com/szq0214/DSOD .\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:33:05 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 02:17:30 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zhuang", ""], ["Li", "Jianguo", ""], ["Jiang", "Yu-Gang", ""], ["Chen", "Yurong", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1708.01244", "submitter": "Yury Korolev", "authors": "Yury Korolev and Jan Lellmann", "title": "Image reconstruction with imperfect forward models and applications in\n  deblurring", "comments": null, "journal-ref": null, "doi": "10.1137/17M1141965", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyse an approach to image reconstruction problems with\nimperfect forward models based on partially ordered spaces - Banach lattices.\nIn this approach, errors in the data and in the forward models are described\nusing order intervals. The method can be characterised as the lattice analogue\nof the residual method, where the feasible set is defined by linear inequality\nconstraints. The study of this feasible set is the main contribution of this\npaper. Convexity of this feasible set is examined in several settings and\nmodifications for introducing additional information about the forward operator\nare considered. Numerical examples demonstrate the performance of the method in\ndeblurring with errors in the blurring kernel.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:49:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 12:49:18 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 08:11:10 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Korolev", "Yury", ""], ["Lellmann", "Jan", ""]]}, {"id": "1708.01246", "submitter": "Jia-Bin Huang", "authors": "Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, Ming-Hsuan Yang", "title": "Unsupervised Representation Learning by Sorting Sequences", "comments": "ICCV 2017. Project page: http://vllab1.ucmerced.edu/~hylee/OPN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised representation learning approach using videos\nwithout semantic labels. We leverage the temporal coherence as a supervisory\nsignal by formulating representation learning as a sequence sorting task. We\ntake temporally shuffled frames (i.e., in non-chronological order) as inputs\nand train a convolutional neural network to sort the shuffled sequences.\nSimilar to comparison-based sorting algorithms, we propose to extract features\nfrom all frame pairs and aggregate them to predict the correct order. As\nsorting shuffled image sequence requires an understanding of the statistical\ntemporal structure of images, training with such a proxy task allows us to\nlearn rich and generalizable visual representation. We validate the\neffectiveness of the learned representation using our method as pre-training on\nhigh-level recognition problems. The experimental results show that our method\ncompares favorably against state-of-the-art methods on action recognition,\nimage classification and object detection tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 17:51:38 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Lee", "Hsin-Ying", ""], ["Huang", "Jia-Bin", ""], ["Singh", "Maneesh", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.01292", "submitter": "Cristina Segalin", "authors": "Cristina Segalin, Fabio Celli, Luca Polonio, Michal Kosinski, David\n  Stillwell, Nicu Sebe, Marco Cristani, Bruno Lepri", "title": "What your Facebook Profile Picture Reveals about your Personality", "comments": null, "journal-ref": null, "doi": "10.1145/3123266.3123331", "report-no": null, "categories": "cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People spend considerable effort managing the impressions they give others.\nSocial psychologists have shown that people manage these impressions\ndifferently depending upon their personality. Facebook and other social media\nprovide a new forum for this fundamental process; hence, understanding people's\nbehaviour on social media could provide interesting insights on their\npersonality. In this paper we investigate automatic personality recognition\nfrom Facebook profile pictures. We analyze the effectiveness of four families\nof visual features and we discuss some human interpretable patterns that\nexplain the personality traits of the individuals. For example, extroverts and\nagreeable individuals tend to have warm colored pictures and to exhibit many\nfaces in their portraits, mirroring their inclination to socialize; while\nneurotic ones have a prevalence of pictures of indoor places. Then, we propose\na classification approach to automatically recognize personality traits from\nthese visual features. Finally, we compare the performance of our\nclassification approach to the one obtained by human raters and we show that\ncomputer-based classifications are significantly more accurate than averaged\nhuman-based classifications for Extraversion and Neuroticism.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:58:36 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 07:51:17 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Segalin", "Cristina", ""], ["Celli", "Fabio", ""], ["Polonio", "Luca", ""], ["Kosinski", "Michal", ""], ["Stillwell", "David", ""], ["Sebe", "Nicu", ""], ["Cristani", "Marco", ""], ["Lepri", "Bruno", ""]]}, {"id": "1708.01311", "submitter": "Xintong Han", "authors": "Xintong Han, Zuxuan Wu, Phoenix X. Huang, Xiao Zhang, Menglong Zhu,\n  Yuan Li, Yang Zhao, and Larry S. Davis", "title": "Automatic Spatially-aware Fashion Concept Discovery", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic spatially-aware concept discovery approach\nusing weakly labeled image-text data from shopping websites. We first fine-tune\nGoogleNet by jointly modeling clothing images and their corresponding\ndescriptions in a visual-semantic embedding space. Then, for each attribute\n(word), we generate its spatially-aware representation by combining its\nsemantic word vector representation with its spatial representation derived\nfrom the convolutional maps of the fine-tuned network. The resulting\nspatially-aware representations are further used to cluster attributes into\nmultiple groups to form spatially-aware concepts (e.g., the neckline concept\nmight consist of attributes like v-neck, round-neck, etc). Finally, we\ndecompose the visual-semantic embedding space into multiple concept-specific\nsubspaces, which facilitates structured browsing and attribute-feedback product\nretrieval by exploiting multimodal linguistic regularities. We conducted\nextensive experiments on our newly collected Fashion200K dataset, and results\non clustering quality evaluation and attribute-feedback product retrieval task\ndemonstrate the effectiveness of our automatically discovered spatially-aware\nconcepts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 21:13:04 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Han", "Xintong", ""], ["Wu", "Zuxuan", ""], ["Huang", "Phoenix X.", ""], ["Zhang", "Xiao", ""], ["Zhu", "Menglong", ""], ["Li", "Yuan", ""], ["Zhao", "Yang", ""], ["Davis", "Larry S.", ""]]}, {"id": "1708.01323", "submitter": "Eric Jonas", "authors": "Eric Jonas and Monica G. Bobra and Vaishaal Shankar and J. Todd\n  Hoeksema and Benjamin Recht", "title": "Flare Prediction Using Photospheric and Coronal Image Data", "comments": "submitted for publication in the Astrophysical Journal", "journal-ref": null, "doi": "10.1007/s11207-018-1258-9", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The precise physical process that triggers solar flares is not currently\nunderstood. Here we attempt to capture the signature of this mechanism in solar\nimage data of various wavelengths and use these signatures to predict flaring\nactivity. We do this by developing an algorithm that [1] automatically\ngenerates features in 5.5 TB of image data taken by the Solar Dynamics\nObservatory of the solar photosphere, chromosphere, transition region, and\ncorona during the time period between May 2010 and May 2014, [2] combines these\nfeatures with other features based on flaring history and a physical\nunderstanding of putative flaring processes, and [3] classifies these features\nto predict whether a solar active region will flare within a time period of $T$\nhours, where $T$ = 2 and 24. We find that when optimizing for the True Skill\nScore (TSS), photospheric vector magnetic field data combined with flaring\nhistory yields the best performance, and when optimizing for the area under the\nprecision-recall curve, all the data are helpful. Our model performance yields\na TSS of $0.84 \\pm 0.03$ and $0.81 \\pm 0.03$ in the $T$ = 2 and 24 hour cases,\nrespectively, and a value of $0.13 \\pm 0.07$ and $0.43 \\pm 0.08$ for the area\nunder the precision-recall curve in the $T$ = 2 and 24 hour cases,\nrespectively. These relatively high scores are similar to, but not greater\nthan, other attempts to predict solar flares. Given the similar values of\nalgorithm performance across various types of models reported in the\nliterature, we conclude that we can expect a certain baseline predictive\ncapacity using these data. This is the first attempt to predict solar flares\nusing photospheric vector magnetic field data as well as multiple wavelengths\nof image data from the chromosphere, transition region, and corona.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 22:31:38 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Jonas", "Eric", ""], ["Bobra", "Monica G.", ""], ["Shankar", "Vaishaal", ""], ["Hoeksema", "J. Todd", ""], ["Recht", "Benjamin", ""]]}, {"id": "1708.01336", "submitter": "Yannis Kalantidis", "authors": "Lu Jiang, Junwei Liang, Liangliang Cao, Yannis Kalantidis, Sachin\n  Farfade, Alexander Hauptmann", "title": "MemexQA: Visual Memex Question Answering", "comments": "https://memexqa.cs.cmu.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new task, MemexQA: given a collection of photos or\nvideos from a user, the goal is to automatically answer questions that help\nusers recover their memory about events captured in the collection. Towards\nsolving the task, we 1) present the MemexQA dataset, a large, realistic\nmultimodal dataset consisting of real personal photos and crowd-sourced\nquestions/answers, 2) propose MemexNet, a unified, end-to-end trainable network\narchitecture for image, text and video question answering. Experimental results\non the MemexQA dataset demonstrate that MemexNet outperforms strong baselines\nand yields the state-of-the-art on this novel and challenging task. The\npromising results on TextQA and VideoQA suggest MemexNet's efficacy and\nscalability across various QA tasks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 00:17:48 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Jiang", "Lu", ""], ["Liang", "Junwei", ""], ["Cao", "Liangliang", ""], ["Kalantidis", "Yannis", ""], ["Farfade", "Sachin", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1708.01354", "submitter": "Adithya Murali", "authors": "Adithyavairavan Murali, Lerrel Pinto, Dhiraj Gandhi, Abhinav Gupta", "title": "CASSL: Curriculum Accelerated Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent self-supervised learning approaches focus on using a few thousand data\npoints to learn policies for high-level, low-dimensional action spaces.\nHowever, scaling this framework for high-dimensional control require either\nscaling up the data collection efforts or using a clever sampling strategy for\ntraining. We present a novel approach - Curriculum Accelerated Self-Supervised\nLearning (CASSL) - to train policies that map visual information to high-level,\nhigher- dimensional action spaces. CASSL orders the sampling of training data\nbased on control dimensions: the learning and sampling are focused on few\ncontrol parameters before other parameters. The right curriculum for learning\nis suggested by variance-based global sensitivity analysis of the control\nspace. We apply our CASSL framework to learning how to grasp using an adaptive,\nunderactuated multi-fingered gripper, a challenging system to control. Our\nexperimental results indicate that CASSL provides significant improvement and\ngeneralization compared to baseline methods such as staged curriculum learning\n(8% increase) and complete end-to-end learning with random exploration (14%\nimprovement) tested on a set of novel objects.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 02:13:50 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 23:24:52 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Murali", "Adithyavairavan", ""], ["Pinto", "Lerrel", ""], ["Gandhi", "Dhiraj", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1708.01405", "submitter": "Marcelo Saval Calvo", "authors": "Marcelo Saval-Calvo and Jorge Azorin-Lopez and Andres Fuster-Guillo\n  and Higinio Mora-Mora", "title": "{\\mu}-MAR: Multiplane 3D Marker based Registration for Depth-sensing\n  Cameras", "comments": null, "journal-ref": "Expert Systems with Applications, Volume 42, Issue 23, Pages\n  9353-9365 (2015)", "doi": "10.1016/j.eswa.2015.08.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications including object reconstruction, robot guidance, and scene\nmapping require the registration of multiple views from a scene to generate a\ncomplete geometric and appearance model of it. In real situations,\ntransformations between views are unknown an it is necessary to apply expert\ninference to estimate them. In the last few years, the emergence of low-cost\ndepth-sensing cameras has strengthened the research on this topic, motivating a\nplethora of new applications. Although they have enough resolution and accuracy\nfor many applications, some situations may not be solved with general\nstate-of-the-art registration methods due to the Signal-to-Noise ratio (SNR)\nand the resolution of the data provided. The problem of working with low SNR\ndata, in general terms, may appear in any 3D system, then it is necessary to\npropose novel solutions in this aspect. In this paper, we propose a method,\n{\\mu}-MAR, able to both coarse and fine register sets of 3D points provided by\nlow-cost depth-sensing cameras, despite it is not restricted to these sensors,\ninto a common coordinate system. The method is able to overcome the noisy data\nproblem by means of using a model-based solution of multiplane registration.\nSpecifically, it iteratively registers 3D markers composed by multiple planes\nextracted from points of multiple views of the scene. As the markers and the\nobject of interest are static in the scenario, the transformations obtained for\nthe markers are applied to the object in order to reconstruct it. Experiments\nhave been performed using synthetic and real data. The synthetic data allows a\nqualitative and quantitative evaluation by means of visual inspection and\nHausdorff distance respectively. The real data experiments show the performance\nof the proposal using data acquired by a Primesense Carmine RGB-D sensor. The\nmethod has been compared to several state-of-the-art methods. The ...\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 07:35:22 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Saval-Calvo", "Marcelo", ""], ["Azorin-Lopez", "Jorge", ""], ["Fuster-Guillo", "Andres", ""], ["Mora-Mora", "Higinio", ""]]}, {"id": "1708.01420", "submitter": "Haifeng Li", "authors": "Jie Chen, Chao Yuan, Min Deng, Chao Tao, Jian Peng, Haifeng Li", "title": "On the Selective and Invariant Representation of DCNN for\n  High-Resolution Remote Sensing Image Recognition", "comments": "68 pages, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision possesses strong invariance in image recognition. The cognitive\ncapability of deep convolutional neural network (DCNN) is close to the human\nvisual level because of hierarchical coding directly from raw image. Owing to\nits superiority in feature representation, DCNN has exhibited remarkable\nperformance in scene recognition of high-resolution remote sensing (HRRS)\nimages and classification of hyper-spectral remote sensing images. In-depth\ninvestigation is still essential for understanding why DCNN can accurately\nidentify diverse ground objects via its effective feature representation. Thus,\nwe train the deep neural network called AlexNet on our large scale remote\nsensing image recognition benchmark. At the neuron level in each convolution\nlayer, we analyze the general properties of DCNN in HRRS image recognition by\nuse of a framework of visual stimulation-characteristic response combined with\nfeature coding-classification decoding. Specifically, we use histogram\nstatistics, representational dissimilarity matrix, and class activation mapping\nto observe the selective and invariance representations of DCNN in HRRS image\nrecognition. We argue that selective and invariance representations play\nimportant roles in remote sensing images tasks, such as classification,\ndetection, and segment. Also selective and invariance representations are\nsignificant to design new DCNN liked models for analyzing and understanding\nremote sensing images.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:33:12 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Chen", "Jie", ""], ["Yuan", "Chao", ""], ["Deng", "Min", ""], ["Tao", "Chao", ""], ["Peng", "Jian", ""], ["Li", "Haifeng", ""]]}, {"id": "1708.01447", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le and Akihiro Sugimoto", "title": "Video Salient Object Detection Using Spatiotemporal Deep Features", "comments": "accepted at TIP", "journal-ref": null, "doi": "10.1109/TIP.2018.2849860", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for detecting salient objects in videos where\ntemporal information in addition to spatial information is fully taken into\naccount. Following recent reports on the advantage of deep features over\nconventional hand-crafted features, we propose a new set of SpatioTemporal Deep\n(STD) features that utilize local and global contexts over frames. We also\npropose new SpatioTemporal Conditional Random Field (STCRF) to compute saliency\nfrom STD features. STCRF is our extension of CRF to the temporal domain and\ndescribes the relationships among neighboring regions both in a frame and over\nframes. STCRF leads to temporally consistent saliency maps over frames,\ncontributing to the accurate detection of salient objects' boundaries and noise\nreduction during detection. Our proposed method first segments an input video\ninto multiple scales and then computes a saliency map at each scale level using\nSTD features with STCRF. The final saliency map is computed by fusing saliency\nmaps at different scale levels. Our experiments, using publicly available\nbenchmark datasets, confirm that the proposed method significantly outperforms\nstate-of-the-art methods. We also applied our saliency computation to the video\nobject segmentation task, showing that our method outperforms existing video\nobject segmentation methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 11:05:14 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 06:45:14 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 01:49:28 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "1708.01448", "submitter": "Nagendra Kumar Mr.", "authors": "Nagendra Kumar and Rohit Sinha", "title": "Correlation and Class Based Block Formation for Improved Structured\n  Dictionary Learning", "comments": "9 pages, Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the creation of block-structured dictionary has attracted a\nlot of interest. Learning such dictionaries involve two step process: block\nformation and dictionary update. Both these steps are important in producing an\neffective dictionary. The existing works mostly assume that the block structure\nis known a priori while learning the dictionary. For finding the unknown block\nstructure given a dictionary commonly sparse agglomerative clustering (SAC) is\nused. It groups atoms based on their consistency in sparse coding with respect\nto the unstructured dictionary. This paper explores two innovations towards\nimproving the reconstruction as well as the classification ability achieved\nwith the block-structured dictionary. First, we propose a novel block\nstructuring approach that makes use of the correlation among dictionary atoms.\nUnlike the SAC approach, which groups diverse atoms, in the proposed approach\nthe blocks are formed by grouping the top most correlated atoms in the\ndictionary. The proposed block clustering approach is noted to yield\nsignificant reductions in redundancy as well as provides a direct control on\nthe block size when compared with the existing SAC-based block structuring.\nLater, motivated by works using supervised \\emph{a priori} known block\nstructure, we also explore the incorporation of class information in the\nproposed block formation approach to further enhance the classification ability\nof the block dictionary. For assessment of the reconstruction ability with\nproposed innovations is done on synthetic data while the classification ability\nhas been evaluated in large variability speaker verification task.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 11:06:42 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 03:26:50 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Kumar", "Nagendra", ""], ["Sinha", "Rohit", ""]]}, {"id": "1708.01471", "submitter": "Zhou Yu", "authors": "Zhou Yu, Jun Yu, Jianping Fan, Dacheng Tao", "title": "Multi-modal Factorized Bilinear Pooling with Co-Attention Learning for\n  Visual Question Answering", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both the visual content of images and the textual\ncontent of questions. The approaches used to represent the images and questions\nin a fine-grained manner and questions and to fuse these multi-modal features\nplay key roles in performance. Bilinear pooling based models have been shown to\noutperform traditional linear models for VQA, but their high-dimensional\nrepresentations and high computational complexity may seriously limit their\napplicability in practice. For multi-modal feature fusion, here we develop a\nMulti-modal Factorized Bilinear (MFB) pooling approach to efficiently and\neffectively combine multi-modal features, which results in superior performance\nfor VQA compared with other bilinear pooling approaches. For fine-grained image\nand question representation, we develop a co-attention mechanism using an\nend-to-end deep network architecture to jointly learn both the image and\nquestion attentions. Combining the proposed MFB approach with co-attention\nlearning in a new network architecture provides a unified model for VQA. Our\nexperimental results demonstrate that the single MFB with co-attention model\nachieves new state-of-the-art performance on the real-world VQA dataset. Code\navailable at https://github.com/yuzcccc/mfb.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 12:17:49 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Yu", "Zhou", ""], ["Yu", "Jun", ""], ["Fan", "Jianping", ""], ["Tao", "Dacheng", ""]]}, {"id": "1708.01494", "submitter": "Akashdeep Goel", "authors": "Akashdeep Goel, Biplab Banerjee, Aleksandra Pizurica", "title": "Hierarchical Metric Learning for Optical Remote Sensing Scene\n  Categorization", "comments": "Undergoing revision in GRSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of scene classification from optical remote sensing\n(RS) images based on the paradigm of hierarchical metric learning. Ideally,\nsupervised metric learning strategies learn a projection from a set of training\ndata points so as to minimize intra-class variance while maximizing inter-class\nseparability to the class label space. However, standard metric learning\ntechniques do not incorporate the class interaction information in learning the\ntransformation matrix, which is often considered to be a bottleneck while\ndealing with fine-grained visual categories. As a remedy, we propose to\norganize the classes in a hierarchical fashion by exploring their visual\nsimilarities and subsequently learn separate distance metric transformations\nfor the classes present at the non-leaf nodes of the tree. We employ an\niterative max-margin clustering strategy to obtain the hierarchical\norganization of the classes. Experiment results obtained on the large-scale\nNWPU-RESISC45 and the popular UC-Merced datasets demonstrate the efficacy of\nthe proposed hierarchical metric learning based RS scene recognition strategy\nin comparison to the standard approaches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 13:38:46 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 07:26:23 GMT"}, {"version": "v3", "created": "Wed, 1 Aug 2018 18:42:33 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Goel", "Akashdeep", ""], ["Banerjee", "Biplab", ""], ["Pizurica", "Aleksandra", ""]]}, {"id": "1708.01519", "submitter": "Mehran Safayani", "authors": "Mehran Safayani and Saeid Momenzadeh", "title": "A Latent Variable Model for Two-Dimensional Canonical Correlation\n  Analysis and its Variational Inference", "comments": null, "journal-ref": null, "doi": "10.1007/s00500-020-04906-8", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the dimension reduction (DR) techniques by means of probabilistic\nmodels has recently been given special attention. Probabilistic models, in\naddition to a better interpretability of the DR methods, provide a framework\nfor further extensions of such algorithms. One of the new approaches to the\nprobabilistic DR methods is to preserving the internal structure of data. It is\nmeant that it is not necessary that the data first be converted from the matrix\nor tensor format to the vector format in the process of dimensionality\nreduction. In this paper, a latent variable model for matrix-variate data for\ncanonical correlation analysis (CCA) is proposed. Since in general there is not\nany analytical maximum likelihood solution for this model, we present two\napproaches for learning the parameters. The proposed methods are evaluated\nusing the synthetic data in terms of convergence and quality of mappings. Also,\nreal data set is employed for assessing the proposed methods with several\nprobabilistic and none-probabilistic CCA based approaches. The results confirm\nthe superiority of the proposed methods with respect to the competing\nalgorithms. Moreover, this model can be considered as a framework for further\nextensions.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 14:16:25 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Safayani", "Mehran", ""], ["Momenzadeh", "Saeid", ""]]}, {"id": "1708.01541", "submitter": "Jianji Wang", "authors": "Jianji Wang, Nanning Zheng, Badong Chen and Jose C. Principe", "title": "Associations among Image Assessments as Cost Functions in Linear\n  Decomposition: MSE, SSIM, and Correlation Coefficient", "comments": "11 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional methods of image assessment, such as mean squared error\n(MSE), signal-to-noise ratio (SNR), and Peak signal-to-noise ratio (PSNR), are\nall based on the absolute error of images. Pearson's inner-product correlation\ncoefficient (PCC) is also usually used to measure the similarity between\nimages. Structural similarity (SSIM) index is another important measurement\nwhich has been shown to be more effective in the human vision system (HVS).\nAlthough there are many essential differences among these image assessments,\nsome important associations among them as cost functions in linear\ndecomposition are discussed in this paper. Firstly, the selected bases from a\nbasis set for a target vector are the same in the linear decomposition schemes\nwith different cost functions MSE, SSIM, and PCC. Moreover, for a target\nvector, the ratio of the corresponding affine parameters in the MSE-based\nlinear decomposition scheme and the SSIM-based scheme is a constant, which is\njust the value of PCC between the target vector and its estimated vector.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 15:03:21 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Wang", "Jianji", ""], ["Zheng", "Nanning", ""], ["Chen", "Badong", ""], ["Principe", "Jose C.", ""]]}, {"id": "1708.01565", "submitter": "Michael Wand", "authors": "Michael Wand, Juergen Schmidhuber", "title": "Improving Speaker-Independent Lipreading with Domain-Adversarial\n  Training", "comments": "Accepted at Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Lipreading system, i.e. a speech recognition system using only\nvisual features, which uses domain-adversarial training for speaker\nindependence. Domain-adversarial training is integrated into the optimization\nof a lipreader based on a stack of feedforward and LSTM (Long Short-Term\nMemory) recurrent neural networks, yielding an end-to-end trainable system\nwhich only requires a very small number of frames of untranscribed target data\nto substantially improve the recognition accuracy on the target speaker. On\npairs of different source and target speakers, we achieve a relative accuracy\nimprovement of around 40% with only 15 to 20 seconds of untranscribed target\nspeech data. On multi-speaker training setups, the accuracy improvements are\nsmaller but still substantial.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 15:57:38 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Wand", "Michael", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1708.01566", "submitter": "Siva Karthik Mustikovela", "authors": "Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, Andreas\n  Geiger, Carsten Rother", "title": "Augmented Reality Meets Computer Vision : Efficient Data Generation for\n  Urban Driving Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in computer vision is based on availability of\nlarge annotated datasets. To lower the need for hand labeled images, virtually\nrendered 3D worlds have recently gained popularity. Creating realistic 3D\ncontent is challenging on its own and requires significant human effort. In\nthis work, we propose an alternative paradigm which combines real and synthetic\ndata for learning semantic instance segmentation and object detection models.\nExploiting the fact that not all aspects of the scene are equally important for\nthis task, we propose to augment real-world imagery with virtual objects of the\ntarget category. Capturing real-world images at large scale is easy and cheap,\nand directly provides real background appearances without the need for creating\ncomplex 3D models of the environment. We present an efficient procedure to\naugment real images with virtual objects. This allows us to create realistic\ncomposite images which exhibit both realistic background appearance and a large\nnumber of complex object arrangements. In contrast to modeling complete 3D\nenvironments, our augmentation approach requires only a few user interactions\nin combination with 3D shapes of the target object. Through extensive\nexperimentation, we conclude the right set of parameters to produce augmented\ndata which can maximally enhance the performance of instance segmentation\nmodels. Further, we demonstrate the utility of our approach on training\nstandard deep models for semantic instance segmentation and object detection of\ncars in outdoor driving scenes. We test the models trained on our augmented\ndata on the KITTI 2015 dataset, which we have annotated with pixel-accurate\nground truth, and on Cityscapes dataset. Our experiments demonstrate that\nmodels trained on augmented imagery generalize better than those trained on\nsynthetic data or models trained on limited amount of annotated real data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 16:03:52 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Alhaija", "Hassan Abu", ""], ["Mustikovela", "Siva Karthik", ""], ["Mescheder", "Lars", ""], ["Geiger", "Andreas", ""], ["Rother", "Carsten", ""]]}, {"id": "1708.01580", "submitter": "Yao Yao", "authors": "Yao Yao, Haolin Liang, Xia Li, Jinbao Zhang, Jialv He", "title": "Sensing Urban Land-Use Patterns By Integrating Google Tensorflow And\n  Scene-Classification Models", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid progress of China's urbanization, research on the automatic\ndetection of land-use patterns in Chinese cities is of substantial importance.\nDeep learning is an effective method to extract image features. To take\nadvantage of the deep-learning method in detecting urban land-use patterns, we\napplied a transfer-learning-based remote-sensing image approach to extract and\nclassify features. Using the Google Tensorflow framework, a powerful\nconvolution neural network (CNN) library was created. First, the transferred\nmodel was previously trained on ImageNet, one of the largest object-image data\nsets, to fully develop the model's ability to generate feature vectors of\nstandard remote-sensing land-cover data sets (UC Merced and WHU-SIRI). Then, a\nrandom-forest-based classifier was constructed and trained on these generated\nvectors to classify the actual urban land-use pattern on the scale of traffic\nanalysis zones (TAZs). To avoid the multi-scale effect of remote-sensing\nimagery, a large random patch (LRP) method was used. The proposed method could\nefficiently obtain acceptable accuracy (OA = 0.794, Kappa = 0.737) for the\nstudy area. In addition, the results show that the proposed method can\neffectively overcome the multi-scale effect that occurs in urban land-use\nclassification at the irregular land-parcel level. The proposed method can help\nplanners monitor dynamic urban land use and evaluate the impact of\nurban-planning schemes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 16:39:22 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Yao", "Yao", ""], ["Liang", "Haolin", ""], ["Li", "Xia", ""], ["Zhang", "Jinbao", ""], ["He", "Jialv", ""]]}, {"id": "1708.01589", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le and Akihiro Sugimoto", "title": "Region-Based Multiscale Spatiotemporal Saliency for Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting salient objects from a video requires exploiting both spatial and\ntemporal knowledge included in the video. We propose a novel region-based\nmultiscale spatiotemporal saliency detection method for videos, where static\nfeatures and dynamic features computed from the low and middle levels are\ncombined together. Our method utilizes such combined features spatially over\neach frame and, at the same time, temporally across frames using consistency\nbetween consecutive frames. Saliency cues in our method are analyzed through a\nmultiscale segmentation model, and fused across scale levels, yielding to\nexploring regions efficiently. An adaptive temporal window using motion\ninformation is also developed to combine saliency values of consecutive frames\nin order to keep temporal consistency across frames. Performance evaluation on\nseveral popular benchmark datasets validates that our method outperforms\nexisting state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 17:01:41 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "1708.01641", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor\n  Darrell, Bryan Russell", "title": "Localizing Moments in Video with Natural Language", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider retrieving a specific temporal segment, or moment, from a video\ngiven a natural language text description. Methods designed to retrieve whole\nvideo clips with natural language determine what occurs in a video but not\nwhen. To address this issue, we propose the Moment Context Network (MCN) which\neffectively localizes natural language queries in videos by integrating local\nand global video features over time. A key obstacle to training our MCN model\nis that current video datasets do not include pairs of localized video segments\nand referring expressions, or text descriptions which uniquely identify a\ncorresponding moment. Therefore, we collect the Distinct Describable Moments\n(DiDeMo) dataset which consists of over 10,000 unedited, personal videos in\ndiverse visual settings with pairs of localized video segments and referring\nexpressions. We demonstrate that MCN outperforms several baseline methods and\nbelieve that our initial results together with the release of DiDeMo will\ninspire further research on localizing video moments with natural language.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 18:57:52 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Wang", "Oliver", ""], ["Shechtman", "Eli", ""], ["Sivic", "Josef", ""], ["Darrell", "Trevor", ""], ["Russell", "Bryan", ""]]}, {"id": "1708.01642", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Ishan Misra, Martial Hebert", "title": "Cut, Paste and Learn: Surprisingly Easy Synthesis for Instance Detection", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major impediment in rapidly deploying object detection models for instance\ndetection is the lack of large annotated datasets. For example, finding a large\nlabeled dataset containing instances in a particular kitchen is unlikely. Each\nnew environment with new instances requires expensive data collection and\nannotation. In this paper, we propose a simple approach to generate large\nannotated instance datasets with minimal effort. Our key insight is that\nensuring only patch-level realism provides enough training signal for current\nobject detector models. We automatically `cut' object instances and `paste'\nthem on random backgrounds. A naive way to do this results in pixel artifacts\nwhich result in poor performance for trained models. We show how to make\ndetectors ignore these artifacts during training and generate data that gives\ncompetitive performance on real data. Our method outperforms existing synthesis\napproaches and when combined with real images improves relative performance by\nmore than 21% on benchmark datasets. In a cross-domain setting, our synthetic\ndata combined with just 10% real data outperforms models trained on all real\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 18:58:03 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Misra", "Ishan", ""], ["Hebert", "Martial", ""]]}, {"id": "1708.01648", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, Derek Hoiem", "title": "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of various applications including robotics, digital content\ncreation, and visualization demand a structured and abstract representation of\nthe 3D world from limited sensor data. Inspired by the nature of human\nperception of 3D shapes as a collection of simple parts, we explore such an\nabstract shape representation based on primitives. Given a single depth image\nof an object, we present 3D-PRNN, a generative recurrent neural network that\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\ngenerative model encodes symmetry characteristics of common man-made objects,\npreserves long-range structural coherence, and describes objects of varying\ncomplexity with a compact representation. We also propose a method based on\nGaussian Fields to generate a large scale dataset of primitive-based shape\nrepresentations to train our network. We evaluate our approach on a wide range\nof examples and show that it outperforms nearest-neighbor based shape retrieval\nmethods and is on-par with voxel-based generative models while using a\nsignificantly reduced parameter space.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 19:30:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zou", "Chuhang", ""], ["Yumer", "Ersin", ""], ["Yang", "Jimei", ""], ["Ceylan", "Duygu", ""], ["Hoiem", "Derek", ""]]}, {"id": "1708.01654", "submitter": "Rui Yu", "authors": "Qi Liu-Yin, Rui Yu, Lourdes Agapito, Andrew Fitzgibbon, Chris Russell", "title": "Better Together: Joint Reasoning for Non-rigid 3D Reconstruction with\n  Specularities and Shading", "comments": "Submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the use of shape-from-shading (SfS) to improve both the\nquality and the robustness of 3D reconstruction of dynamic objects captured by\na single camera. Unlike previous approaches that made use of SfS as a\npost-processing step, we offer a principled integrated approach that solves\ndynamic object tracking and reconstruction and SfS as a single unified cost\nfunction. Moving beyond Lambertian S f S , we propose a general approach that\nmodels both specularities and shading while simultaneously tracking and\nreconstructing general dynamic objects. Solving these problems jointly prevents\nthe kinds of tracking failures which can not be recovered from by pipeline\napproaches. We show state-of-the-art results both qualitatively and\nquantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 20:15:33 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Liu-Yin", "Qi", ""], ["Yu", "Rui", ""], ["Agapito", "Lourdes", ""], ["Fitzgibbon", "Andrew", ""], ["Russell", "Chris", ""]]}, {"id": "1708.01663", "submitter": "Ulugbek Kamilov", "authors": "Yanting Ma, Hassan Mansour, Dehong Liu, Petros T. Boufounos, and\n  Ulugbek S. Kamilov", "title": "Accelerated Image Reconstruction for Nonlinear Diffractive Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of reconstructing an object from the measurements of the light it\nscatters is common in numerous imaging applications. While the most popular\nformulations of the problem are based on linearizing the object-light\nrelationship, there is an increased interest in considering nonlinear\nformulations that can account for multiple light scattering. In this paper, we\npropose an image reconstruction method, called CISOR, for nonlinear diffractive\nimaging, based on a nonconvex optimization formulation with total variation\n(TV) regularization. The nonconvex solver used in CISOR is our new variant of\nfast iterative shrinkage/thresholding algorithm (FISTA). We provide fast and\nmemory-efficient implementation of the new FISTA variant and prove that it\nreliably converges for our nonconvex optimization problem. In addition, we\nsystematically compare our method with other state-of-the-art methods on\nsimulated as well as experimentally measured data in both 2D and 3D settings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 20:52:44 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 15:43:23 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Ma", "Yanting", ""], ["Mansour", "Hassan", ""], ["Liu", "Dehong", ""], ["Boufounos", "Petros T.", ""], ["Kamilov", "Ulugbek S.", ""]]}, {"id": "1708.01670", "submitter": "Robert Maier", "authors": "Robert Maier, Kihwan Kim, Daniel Cremers, Jan Kautz, Matthias\n  Nie{\\ss}ner", "title": "Intrinsic3D: High-Quality 3D Reconstruction by Joint Appearance and\n  Geometry Optimization with Spatially-Varying Lighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method to obtain high-quality 3D reconstructions from\nconsumer RGB-D sensors. Our core idea is to simultaneously optimize for\ngeometry encoded in a signed distance field (SDF), textures from\nautomatically-selected keyframes, and their camera poses along with material\nand scene lighting. To this end, we propose a joint surface reconstruction\napproach that is based on Shape-from-Shading (SfS) techniques and utilizes the\nestimation of spatially-varying spherical harmonics (SVSH) from subvolumes of\nthe reconstructed scene. Through extensive examples and evaluations, we\ndemonstrate that our method dramatically increases the level of detail in the\nreconstructed scene geometry and contributes highly to consistent surface\ntexture recovery.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 21:34:46 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Maier", "Robert", ""], ["Kim", "Kihwan", ""], ["Cremers", "Daniel", ""], ["Kautz", "Jan", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1708.01676", "submitter": "Kan Chen", "authors": "Kan Chen, Rama Kovvuri, Ram Nevatia", "title": "Query-guided Regression Network with Context Policy for Phrase Grounding", "comments": "Spotlight in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a textual description of an image, phrase grounding localizes objects\nin the image referred by query phrases in the description. State-of-the-art\nmethods address the problem by ranking a set of proposals based on the\nrelevance to each query, which are limited by the performance of independent\nproposal generation systems and ignore useful cues from context in the\ndescription. In this paper, we adopt a spatial regression method to break the\nperformance limit, and introduce reinforcement learning techniques to further\nleverage semantic context information. We propose a novel Query-guided\nRegression network with Context policy (QRC Net) which jointly learns a\nProposal Generation Network (PGN), a Query-guided Regression Network (QRN) and\na Context Policy Network (CPN). Experiments show QRC Net provides a significant\nimprovement in accuracy on two popular datasets: Flickr30K Entities and Referit\nGame, with 14.25% and 17.14% increase over the state-of-the-arts respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 22:27:08 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Chen", "Kan", ""], ["Kovvuri", "Rama", ""], ["Nevatia", "Ram", ""]]}, {"id": "1708.01682", "submitter": "Feng Zhou", "authors": "Jian Wang, Feng Zhou, Shilei Wen, Xiao Liu and Yuanqing Lin", "title": "Deep Metric Learning with Angular Loss", "comments": "International Conference on Computer Vision 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern image search system requires semantic understanding of image, and\na key yet under-addressed problem is to learn a good metric for measuring the\nsimilarity between images. While deep metric learning has yielded impressive\nperformance gains by extracting high level abstractions from image data, a\nproper objective loss function becomes the central issue to boost the\nperformance. In this paper, we propose a novel angular loss, which takes angle\nrelationship into account, for learning better similarity metric. Whereas\nprevious metric learning methods focus on optimizing the similarity\n(contrastive loss) or relative similarity (triplet loss) of image pairs, our\nproposed method aims at constraining the angle at the negative point of triplet\ntriangles. Several favorable properties are observed when compared with\nconventional methods. First, scale invariance is introduced, improving the\nrobustness of objective against feature variance. Second, a third-order\ngeometric constraint is inherently imposed, capturing additional local\nstructure of triplet triangles than contrastive loss or triplet loss. Third,\nbetter convergence has been demonstrated by experiments on three publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 23:17:29 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Wang", "Jian", ""], ["Zhou", "Feng", ""], ["Wen", "Shilei", ""], ["Liu", "Xiao", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1708.01692", "submitter": "Simon Niklaus", "authors": "Simon Niklaus, Long Mai, Feng Liu", "title": "Video Frame Interpolation via Adaptive Separable Convolution", "comments": "ICCV 2017, http://graphics.cs.pdx.edu/project/sepconv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard video frame interpolation methods first estimate optical flow\nbetween input frames and then synthesize an intermediate frame guided by\nmotion. Recent approaches merge these two steps into a single convolution\nprocess by convolving input frames with spatially adaptive kernels that account\nfor motion and re-sampling simultaneously. These methods require large kernels\nto handle large motion, which limits the number of pixels whose kernels can be\nestimated at once due to the large memory demand. To address this problem, this\npaper formulates frame interpolation as local separable convolution over input\nframes using pairs of 1D kernels. Compared to regular 2D kernels, the 1D\nkernels require significantly fewer parameters to be estimated. Our method\ndevelops a deep fully convolutional neural network that takes two input frames\nand estimates pairs of 1D kernels for all pixels simultaneously. Since our\nmethod is able to estimate kernels and synthesizes the whole video frame at\nonce, it allows for the incorporation of perceptual loss to train the neural\nnetwork to produce visually pleasing frames. This deep neural network is\ntrained end-to-end using widely available video data without any human\nannotation. Both qualitative and quantitative experiments show that our method\nprovides a practical solution to high-quality video frame interpolation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 00:18:03 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Niklaus", "Simon", ""], ["Mai", "Long", ""], ["Liu", "Feng", ""]]}, {"id": "1708.01697", "submitter": "Andras Rozsa", "authors": "Andras Rozsa, Manuel G\\\"unther, and Terrance E. Boult", "title": "Adversarial Robustness: Softmax versus Openmax", "comments": "Accepted to British Machine Vision Conference (BMVC) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) provide state-of-the-art results on various tasks\nand are widely used in real world applications. However, it was discovered that\nmachine learning models, including the best performing DNNs, suffer from a\nfundamental problem: they can unexpectedly and confidently misclassify examples\nformed by slightly perturbing otherwise correctly recognized inputs. Various\napproaches have been developed for efficiently generating these so-called\nadversarial examples, but those mostly rely on ascending the gradient of loss.\nIn this paper, we introduce the novel logits optimized targeting system (LOTS)\nto directly manipulate deep features captured at the penultimate layer. Using\nLOTS, we analyze and compare the adversarial robustness of DNNs using the\ntraditional Softmax layer with Openmax, which was designed to provide open set\nrecognition by defining classes derived from deep representations, and is\nclaimed to be more robust to adversarial perturbations. We demonstrate that\nOpenmax provides less vulnerable systems than Softmax to traditional attacks,\nhowever, we show that it can be equally susceptible to more sophisticated\nadversarial generation techniques that directly work on deep representations.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 01:44:52 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1708.01723", "submitter": "Wenhui Jiang", "authors": "Wenhui Jiang, Thuyen Ngo, B. S. Manjunath, Zhicheng Zhao, Fei Su", "title": "Optimizing Region Selection for Weakly Supervised Object Detection", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training object detectors with only image-level annotations is very\nchallenging because the target objects are often surrounded by a large number\nof background clutters. Many existing approaches tackle this problem through\nobject proposal mining. However, the collected positive regions are either low\nin precision or lack of diversity, and the strategy of collecting negative\nregions is not carefully designed, neither. Moreover, training is often slow\nbecause region selection and object detector training are processed separately.\nIn this context, the primary contribution of this work is to improve weakly\nsupervised detection with an optimized region selection strategy. The proposed\nmethod collects purified positive training regions by progressively removing\neasy background clutters, and selects discriminative negative regions by mining\nclass-specific hard samples. This region selection procedure is further\nintegrated into a CNN-based weakly supervised detection (WSD) framework, and\ncan be performed in each stochastic gradient descent mini-batch during\ntraining. Therefore, the entire model can be trained end-to-end efficiently.\nExtensive evaluation results on PASCAL VOC 2007, VOC 2010 and VOC 2012 datasets\nare presented which demonstrate that the proposed method effectively improves\nWSD.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 07:24:43 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Jiang", "Wenhui", ""], ["Ngo", "Thuyen", ""], ["Manjunath", "B. S.", ""], ["Zhao", "Zhicheng", ""], ["Su", "Fei", ""]]}, {"id": "1708.01729", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Weinan Zhang, Jun Wang", "title": "Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))\n  Alternative", "comments": "An advanced version is included in arXiv:1703.02000 \"Activation\n  Maximization Generative Adversarial Nets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we mathematically study several GAN related topics,\nincluding Inception score, label smoothing, gradient vanishing and the\n-log(D(x)) alternative.\n  ---\n  An advanced version is included in arXiv:1703.02000 \"Activation Maximization\nGenerative Adversarial Nets\".\n  Please refer Section 6 in 1703.02000 for detailed analysis on Inception\nScore, and refer its appendix for the discussions on Label Smoothing, Gradient\nVanishing and -log(D(x)) Alternative.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:15:07 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 02:30:32 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 07:02:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""]]}, {"id": "1708.01741", "submitter": "Anoop Cherian", "authors": "Anoop Cherian, Panagiotis Stanitsas, Mehrtash Harandi, Vassilios\n  Morellas, Nikolaos Papanikolopoulos", "title": "Learning Discriminative Alpha-Beta-divergence for Positive Definite\n  Matrices (Extended Version)", "comments": "Accepted at the International Conference on Computer Vision (ICCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetric positive definite (SPD) matrices are useful for capturing\nsecond-order statistics of visual data. To compare two SPD matrices, several\nmeasures are available, such as the affine-invariant Riemannian metric,\nJeffreys divergence, Jensen-Bregman logdet divergence, etc.; however, their\nbehaviors may be application dependent, raising the need of manual selection to\nachieve the best possible performance. Further and as a result of their\noverwhelming complexity for large-scale problems, computing pairwise\nsimilarities by clever embedding of SPD matrices is often preferred to direct\nuse of the aforementioned measures. In this paper, we propose a discriminative\nmetric learning framework, Information Divergence and Dictionary Learning\n(IDDL), that not only learns application specific measures on SPD matrices\nautomatically, but also embeds them as vectors using a learned dictionary. To\nlearn the similarity measures (which could potentially be distinct for every\ndictionary atom), we use the recently introduced alpha-beta-logdet divergence,\nwhich is known to unify the measures listed above. We propose a novel IDDL\nobjective, that learns the parameters of the divergence and the dictionary\natoms jointly in a discriminative setup and is solved efficiently using\nRiemannian optimization. We showcase extensive experiments on eight computer\nvision datasets, demonstrating state-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 09:38:20 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Cherian", "Anoop", ""], ["Stanitsas", "Panagiotis", ""], ["Harandi", "Mehrtash", ""], ["Morellas", "Vassilios", ""], ["Papanikolopoulos", "Nikolaos", ""]]}, {"id": "1708.01749", "submitter": "Mengqi Ji", "authors": "Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu and Lu Fang", "title": "SurfaceNet: An End-to-end 3D Neural Network for Multiview Stereopsis", "comments": "2017 iccv poster", "journal-ref": "2017 ICCV", "doi": "10.1109/ICCV.2017.253", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end learning framework for multiview\nstereopsis. We term the network SurfaceNet. It takes a set of images and their\ncorresponding camera parameters as input and directly infers the 3D model. The\nkey advantage of the framework is that both photo-consistency as well geometric\nrelations of the surface structure can be directly learned for the purpose of\nmultiview stereopsis in an end-to-end fashion. SurfaceNet is a fully 3D\nconvolutional network which is achieved by encoding the camera parameters\ntogether with the images in a 3D voxel representation. We evaluate SurfaceNet\non the large-scale DTU benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 10:58:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ji", "Mengqi", ""], ["Gall", "Juergen", ""], ["Zheng", "Haitian", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""]]}, {"id": "1708.01783", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Redmonds, Ying Nian\n  Wu, and Song-Chun Zhu", "title": "Interactively Transferring CNN Patterns for Part Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scenario of one/multi-shot learning, conventional end-to-end learning\nstrategies without sufficient supervision are usually not powerful enough to\nlearn correct patterns from noisy signals. Thus, given a CNN pre-trained for\nobject classification, this paper proposes a method that first summarizes the\nknowledge hidden inside the CNN into a dictionary of latent activation\npatterns, and then builds a new model for part localization by manually\nassembling latent patterns related to the target part via human interactions.\nWe use very few (e.g., three) annotations of a semantic object part to retrieve\ncertain latent patterns from conv-layers to represent the target part. We then\nvisualize these latent patterns and ask users to further remove incorrect\npatterns, in order to refine part representation. With the guidance of human\ninteractions, our method exhibited superior performance of part localization in\nexperiments.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 15:55:20 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 03:29:22 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Quanshi", ""], ["Cao", "Ruiming", ""], ["Zhang", "Shengming", ""], ["Redmonds", "Mark", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1708.01785", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ruiming Cao, Feng Shi, Ying Nian Wu, and Song-Chun Zhu", "title": "Interpreting CNN Knowledge via an Explanatory Graph", "comments": "in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper learns a graphical model, namely an explanatory graph, which\nreveals the knowledge hierarchy hidden inside a pre-trained CNN. Considering\nthat each filter in a conv-layer of a pre-trained CNN usually represents a\nmixture of object parts, we propose a simple yet efficient method to\nautomatically disentangles different part patterns from each filter, and\nconstruct an explanatory graph. In the explanatory graph, each node represents\na part pattern, and each edge encodes co-activation relationships and spatial\nrelationships between patterns. More importantly, we learn the explanatory\ngraph for a pre-trained CNN in an unsupervised manner, i.e., without a need of\nannotating object parts. Experiments show that each graph node consistently\nrepresents the same object part through different images. We transfer part\npatterns in the explanatory graph to the task of part localization, and our\nmethod significantly outperforms other approaches.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 15:59:13 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 11:30:45 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 02:29:51 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Quanshi", ""], ["Cao", "Ruiming", ""], ["Shi", "Feng", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1708.01806", "submitter": "Jan Haji\\v{c} Jr", "authors": "Jan Haji\\v{c} Jr., Pavel Pecina", "title": "Detecting Noteheads in Handwritten Scores with ConvNets and Bounding Box\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noteheads are the interface between the written score and music. Each\nnotehead on the page signifies one note to be played, and detecting noteheads\nis thus an unavoidable step for Optical Music Recognition. Noteheads are\nclearly distinct objects, however, the variety of music notation handwriting\nmakes noteheads harder to identify, and while handwritten music notation symbol\n{\\em classification} is a well-studied task, symbol {\\em detection} has usually\nbeen limited to heuristics and rule-based systems instead of machine learning\nmethods better suited to deal with the uncertainties in handwriting. We present\nongoing work on a simple notehead detector using convolutional neural networks\nfor pixel classification and bounding box regression that achieves a detection\nf-score of 0.97 on binary score images in the MUSCIMA++ dataset, does not\nrequire staff removal, and is applicable to a variety of handwriting styles and\nlevels of musical complexity.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 18:54:06 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Haji\u010d", "Jan", "Jr."], ["Pecina", "Pavel", ""]]}, {"id": "1708.01818", "submitter": "Byeongkeun Kang", "authors": "Byeongkeun Kang, Yeejin Lee, and Truong Q. Nguyen", "title": "Depth Adaptive Deep Neural Network for Semantic Segmentation", "comments": "IEEE Transactions on Multimedia, 2018", "journal-ref": null, "doi": "10.1109/TMM.2018.2798282", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the depth-adaptive deep neural network using a depth\nmap for semantic segmentation. Typical deep neural networks receive inputs at\nthe predetermined locations regardless of the distance from the camera. This\nfixed receptive field presents a challenge to generalize the features of\nobjects at various distances in neural networks. Specifically, the\npredetermined receptive fields are too small at a short distance, and vice\nversa. To overcome this challenge, we develop a neural network which is able to\nadapt the receptive field not only for each layer but also for each neuron at\nthe spatial location. To adjust the receptive field, we propose the\ndepth-adaptive multiscale (DaM) convolution layer consisting of the adaptive\nperception neuron and the in-layer multiscale neuron. The adaptive perception\nneuron is to adjust the receptive field at each spatial location using the\ncorresponding depth information. The in-layer multiscale neuron is to apply the\ndifferent size of the receptive field at each feature space to learn features\nat multiple scales. The proposed DaM convolution is applied to two fully\nconvolutional neural networks. We demonstrate the effectiveness of the proposed\nneural networks on the publicly available RGB-D dataset for semantic\nsegmentation and the novel hand segmentation dataset for hand-object\ninteraction. The experimental results show that the proposed method outperforms\nthe state-of-the-art methods without any additional layers or\npre/post-processing.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 19:54:59 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 18:31:30 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Kang", "Byeongkeun", ""], ["Lee", "Yeejin", ""], ["Nguyen", "Truong Q.", ""]]}, {"id": "1708.01844", "submitter": "Moi Hoon Yap", "authors": "Omaima FathElrahman Osman, Remah Mutasim Ibrahim Elbashir, Imad Eldain\n  Abbass, Connah Kendrick, Manu Goyal and Moi Hoon Yap", "title": "Automated Assessment of Facial Wrinkling: a case study on the effect of\n  smoking", "comments": "6 pages, 8 figures, Accepted in 2017 IEEE SMC International\n  Conference", "journal-ref": "2017 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC), Banff, AB, 2017, pp. 1081-1086", "doi": "10.1109/SMC.2017.8122755", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial wrinkle is one of the most prominent biological changes that\naccompanying the natural aging process. However, there are some external\nfactors contributing to premature wrinkles development, such as sun exposure\nand smoking. Clinical studies have shown that heavy smoking causes premature\nwrinkles development. However, there is no computerised system that can\nautomatically assess the facial wrinkles on the whole face. This study\ninvestigates the effect of smoking on facial wrinkling using a social habit\nface dataset and an automated computerised computer vision algorithm. The\nwrinkles pattern represented in the intensity of 0-255 was first extracted\nusing a modified Hybrid Hessian Filter. The face was divided into ten\npredefined regions, where the wrinkles in each region was extracted. Then the\nstatistical analysis was performed to analyse which region is effected mainly\nby smoking. The result showed that the density of wrinkles for smokers in two\nregions around the mouth was significantly higher than the non-smokers, at\np-value of 0.05. Other regions are inconclusive due to lack of large scale\ndataset. Finally, the wrinkle was visually compared between smoker and\nnon-smoker faces by generating a generic 3D face model.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 04:56:57 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 22:06:35 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Osman", "Omaima FathElrahman", ""], ["Elbashir", "Remah Mutasim Ibrahim", ""], ["Abbass", "Imad Eldain", ""], ["Kendrick", "Connah", ""], ["Goyal", "Manu", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1708.01846", "submitter": "Chen Chen", "authors": "Chen Chen and Baochang Zhang and Alessio Del Bue and Vittorio Murino", "title": "Manifold Constrained Low-Rank Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank decomposition (LRD) is a state-of-the-art method for visual data\nreconstruction and modelling. However, it is a very challenging problem when\nthe image data contains significant occlusion, noise, illumination variation,\nand misalignment from rotation or viewpoint changes. We leverage the specific\nstructure of data in order to improve the performance of LRD when the data are\nnot ideal. To this end, we propose a new framework that embeds manifold priors\ninto LRD. To implement the framework, we design an alternating direction method\nof multipliers (ADMM) method which efficiently integrates the manifold\nconstraints during the optimization process. The proposed approach is\nsuccessfully used to calculate low-rank models from face images, hand-written\ndigits and planar surface images. The results show a consistent increase of\nperformance when compared to the state-of-the-art over a wide range of\nrealistic image misalignments and corruptions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 05:12:48 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Chen", "Chen", ""], ["Zhang", "Baochang", ""], ["Del Bue", "Alessio", ""], ["Murino", "Vittorio", ""]]}, {"id": "1708.01871", "submitter": "Mojtaba Sedigh Fazli", "authors": "Mojtaba Sedigh Fazli, Stephen Andrew Vella, Silvia N.J. Moreno,\n  Shannon Quinn", "title": "Computational Motility Tracking of Calcium Dynamics in Toxoplasma gondii", "comments": "7 pages, 13 figures, KDDBigDas Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toxoplasma gondii is the causative agent responsible for toxoplasmosis and\nserves as one of the most common parasites in the world. For a successful lytic\ncycle, T. gondii must traverse biological barriers in order to invade host\ncells, and as such, motility is critical for its virulence. Calcium signaling,\ngoverned by fluctuations in cytosolic calcium (Ca2+) concentrations, is\nutilized universally across life and regulates many cellular processes,\nincluding the stimulation of T. gondii virulence factors such as motility.\nTherefore, increases in cytosolic calcium, called calcium oscillations, serve\nas a means to link and quantify the intracellular signaling processes that lead\nto T. gondii motility and invasion. Here, we describe our work extracting,\nquantifying and modeling motility patterns of T. gondii before and after the\naddition of pharmacological drugs and/or extracellular calcium. We demonstrate\na computational pipeline including a robust tracking system using optical flow\nand dense trajectory features to extract T. gondii motility patterns. Using\nthis pipeline, we were able to track changes in T.gondii motility in response\nto cytosolic Ca2+ fluxes in extracellular parasites. This allows us to study\nhow Ca2+ signaling via release from intracellular Ca2+ stores and/or from\nextracellular Ca2+ entry relates to motility patterns, a crucial first step in\ndeveloping countermeasures for T. gondii virulence.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 04:00:16 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 01:40:37 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Fazli", "Mojtaba Sedigh", ""], ["Vella", "Stephen Andrew", ""], ["Moreno", "Silvia N. J.", ""], ["Quinn", "Shannon", ""]]}, {"id": "1708.01885", "submitter": "Huseyin Coskun", "authors": "Huseyin Coskun, Felix Achilles, Robert DiPietro, Nassir Navab,\n  Federico Tombari", "title": "Long Short-Term Memory Kalman Filters:Recurrent Neural Estimators for\n  Pose Regularization", "comments": "Accepted ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-shot pose estimation for tasks such as body joint localization, camera\npose estimation, and object tracking are generally noisy, and temporal filters\nhave been extensively used for regularization. One of the most widely-used\nmethods is the Kalman filter, which is both extremely simple and general.\nHowever, Kalman filters require a motion model and measurement model to be\nspecified a priori, which burdens the modeler and simultaneously demands that\nwe use explicit models that are often only crude approximations of reality. For\nexample, in the pose-estimation tasks mentioned above, it is common to use\nmotion models that assume constant velocity or constant acceleration, and we\nbelieve that these simplified representations are severely inhibitive. In this\nwork, we propose to instead learn rich, dynamic representations of the motion\nand noise models. In particular, we propose learning these models from data\nusing long short term memory, which allows representations that depend on all\nprevious observations and all previous states. We evaluate our method using\nthree of the most popular pose estimation tasks in computer vision, and in all\ncases we obtain state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 13:08:56 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Coskun", "Huseyin", ""], ["Achilles", "Felix", ""], ["DiPietro", "Robert", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1708.01892", "submitter": "Kota Yamaguchi", "authors": "Kota Yamaguchi, Takayuki Okatani, Takayuki Umeda, Kazuhiko Murasaki,\n  Kyoko Sudo", "title": "End-to-end learning potentials for structured attribute prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a structured inference approach in deep neural networks for\nmultiple attribute prediction. In attribute prediction, a common approach is to\nlearn independent classifiers on top of a good feature representation. However,\nsuch classifiers assume conditional independence on features and do not\nexplicitly consider the dependency between attributes in the inference process.\nWe propose to formulate attribute prediction in terms of marginal inference in\nthe conditional random field. We model potential functions by deep neural\nnetworks and apply the sum-product algorithm to solve for the approximate\nmarginal distribution in feed-forward networks. Our message passing layer\nimplements sparse pairwise potentials by a softplus-linear function that is\nequivalent to a higher-order classifier, and learns all the model parameters by\nend-to-end back propagation. The experimental results using SUN attributes and\nCelebA datasets suggest that the structured inference improves the attribute\nprediction performance, and possibly uncovers the hidden relationship between\nattributes.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 13:56:31 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Yamaguchi", "Kota", ""], ["Okatani", "Takayuki", ""], ["Umeda", "Takayuki", ""], ["Murasaki", "Kazuhiko", ""], ["Sudo", "Kyoko", ""]]}, {"id": "1708.01894", "submitter": "Savas Ozkan", "authors": "Savas Ozkan, Berk Kaya, Gozde Bozdagi Akar", "title": "EndNet: Sparse AutoEncoder Network for Endmember Extraction and\n  Hyperspectral Unmixing", "comments": "To appear in IEEE Transaction on Geoscience and Remote Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data acquired from multi-channel sensors is a highly valuable asset to\ninterpret the environment for a variety of remote sensing applications.\nHowever, low spatial resolution is a critical limitation for previous sensors\nand the constituent materials of a scene can be mixed in different fractions\ndue to their spatial interactions. Spectral unmixing is a technique that allows\nus to obtain the material spectral signatures and their fractions from\nhyperspectral data. In this paper, we propose a novel endmember extraction and\nhyperspectral unmixing scheme, so called \\textit{EndNet}, that is based on a\ntwo-staged autoencoder network. This well-known structure is completely\nenhanced and restructured by introducing additional layers and a projection\nmetric (i.e., spectral angle distance (SAD) instead of inner product) to\nachieve an optimum solution. Moreover, we present a novel loss function that is\ncomposed of a Kullback-Leibler divergence term with SAD similarity and\nadditional penalty terms to improve the sparsity of the estimates. These\nmodifications enable us to set the common properties of endmembers such as\nnon-linearity and sparsity for autoencoder networks. Lastly, due to the\nstochastic-gradient based approach, the method is scalable for large-scale data\nand it can be accelerated on Graphical Processing Units (GPUs). To demonstrate\nthe superiority of our proposed method, we conduct extensive experiments on\nseveral well-known datasets. The results confirm that the proposed method\nconsiderably improves the performance compared to the state-of-the-art\ntechniques in literature.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 14:21:32 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 17:53:46 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 19:36:20 GMT"}, {"version": "v4", "created": "Mon, 16 Jul 2018 08:39:38 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ozkan", "Savas", ""], ["Kaya", "Berk", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1708.01928", "submitter": "Moi Hoon Yap", "authors": "Manu Goyal, Neil D. Reeves, Satyan Rajbhandari, Jennifer Spragg and\n  Moi Hoon Yap", "title": "Fully Convolutional Networks for Diabetic Foot Ulcer Segmentation", "comments": "7 pages, 5 figures, 2017 IEEE SMC International Conference (To\n  appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Foot Ulcer (DFU) is a major complication of Diabetes, which if not\nmanaged properly can lead to amputation. DFU can appear anywhere on the foot\nand can vary in size, colour, and contrast depending on various pathologies.\nCurrent clinical approaches to DFU treatment rely on patients and clinician\nvigilance, which has significant limitations such as the high cost involved in\nthe diagnosis, treatment and lengthy care of the DFU. We introduce a dataset of\n705 foot images. We provide the ground truth of ulcer region and the\nsurrounding skin that is an important indicator for clinicians to assess the\nprogress of ulcer. Then, we propose a two-tier transfer learning from bigger\ndatasets to train the Fully Convolutional Networks (FCNs) to automatically\nsegment the ulcer and surrounding skin. Using 5-fold cross-validation, the\nproposed two-tier transfer learning FCN Models achieve a Dice Similarity\nCoefficient of 0.794 ($\\pm$0.104) for ulcer region, 0.851 ($\\pm$0.148) for\nsurrounding skin region, and 0.899 ($\\pm$0.072) for the combination of both\nregions. This demonstrates the potential of FCNs in DFU segmentation, which can\nbe further improved with a larger dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 19:45:37 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Goyal", "Manu", ""], ["Reeves", "Neil D.", ""], ["Rajbhandari", "Satyan", ""], ["Spragg", "Jennifer", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1708.01936", "submitter": "Sifei Liu", "authors": "Sifei Liu, Jianping Shi, Ji Liang, Ming-Hsuan Yang", "title": "Face Parsing via Recurrent Propagation", "comments": "10 pages, 5 figures, BMVC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face parsing is an important problem in computer vision that finds numerous\napplications including recognition and editing. Recently, deep convolutional\nneural networks (CNNs) have been applied to image parsing and segmentation with\nthe state-of-the-art performance. In this paper, we propose a face parsing\nalgorithm that combines hierarchical representations learned by a CNN, and\naccurate label propagations achieved by a spatially variant recurrent neural\nnetwork (RNN). The RNN-based propagation approach enables efficient inference\nover a global space with the guidance of semantic edges generated by a local\nconvolutional model. Since the convolutional architecture can be shallow and\nthe spatial RNN can have few parameters, the framework is much faster and more\nlight-weighted than the state-of-the-art CNNs for the same task. We apply the\nproposed model to coarse-grained and fine-grained face parsing. For\nfine-grained face parsing, we develop a two-stage approach by first identifying\nthe main regions and then segmenting the detail components, which achieves\nbetter performance in terms of accuracy and efficiency. With a single GPU, the\nproposed algorithm parses face images accurately at 300 frames per second,\nwhich facilitates real-time applications.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 21:34:01 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Liu", "Sifei", ""], ["Shi", "Jianping", ""], ["Liang", "Ji", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.01938", "submitter": "Ori Ganoni", "authors": "Ori Ganoni, Ramakrishnan Mukundan", "title": "A Framework for Visually Realistic Multi-robot Simulation in Natural\n  Environment", "comments": "WSCG 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generalized framework for the simulation of multiple\nrobots and drones in highly realistic models of natural environments. The\nproposed simulation architecture uses the Unreal Engine4 for generating both\noptical and depth sensor outputs from any position and orientation within the\nenvironment and provides several key domain specific simulation capabilities.\nVarious components and functionalities of the system have been discussed in\ndetail. The simulation engine also allows users to test and validate a wide\nrange of computer vision algorithms involving different drone configurations\nunder many types of environmental effects such as wind gusts. The paper\ndemonstrates the effectiveness of the system by giving experimental results for\na test scenario where one drone tracks the simulated motion of another in a\ncomplex natural environment.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 21:41:42 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Ganoni", "Ori", ""], ["Mukundan", "Ramakrishnan", ""]]}, {"id": "1708.01946", "submitter": "Jie Zhang", "authors": "Jie Zhang, Christos Maniatis, Luis Horna, Robert B. Fisher", "title": "Intensity Video Guided 4D Fusion for Improved Highly Dynamic 3D\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of high-speed 3D video sensors has greatly facilitated 3D\nshape acquisition of dynamic and deformable objects, but high frame rate 3D\nreconstruction is always degraded by spatial noise and temporal fluctuations.\nThis paper presents a simple yet powerful intensity video guided multi-frame 4D\nfusion pipeline. Temporal tracking of intensity image points (of moving and\ndeforming objects) allows registration of the corresponding 3D data points,\nwhose 3D noise and fluctuations are then reduced by spatio-temporal multi-frame\n4D fusion. We conducted simulated noise tests and real experiments on four 3D\nobjects using a 1000 fps 3D video sensor. The results demonstrate that the\nproposed algorithm is effective at reducing 3D noise and is robust against\nintensity noise. It outperforms existing algorithms with good scalability on\nboth stationary and dynamic objects.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:25:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Jie", ""], ["Maniatis", "Christos", ""], ["Horna", "Luis", ""], ["Fisher", "Robert B.", ""]]}, {"id": "1708.01956", "submitter": "Hanwang Zhang", "authors": "Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, Shih-Fu Chang", "title": "PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel\n  Pairwise R-FCN", "comments": "To appear in International Conference on Computer Vision (ICCV) 2017,\n  Venice, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to tackle a novel vision task called Weakly Supervised Visual Relation\nDetection (WSVRD) to detect \"subject-predicate-object\" relations in an image\nwith object relation groundtruths available only at the image level. This is\nmotivated by the fact that it is extremely expensive to label the combinatorial\nrelations between objects at the instance level. Compared to the extensively\nstudied problem, Weakly Supervised Object Detection (WSOD), WSVRD is more\nchallenging as it needs to examine a large set of regions pairs, which is\ncomputationally prohibitive and more likely stuck in a local optimal solution\nsuch as those involving wrong spatial context. To this end, we present a\nParallel, Pairwise Region-based, Fully Convolutional Network (PPR-FCN) for\nWSVRD. It uses a parallel FCN architecture that simultaneously performs pair\nselection and classification of single regions and region pairs for object and\nrelation detection, while sharing almost all computation shared over the entire\nimage. In particular, we propose a novel position-role-sensitive score map with\npairwise RoI pooling to efficiently capture the crucial context associated with\na pair of objects. We demonstrate the superiority of PPR-FCN over all baselines\nin solving the WSVRD challenge by using results of extensive experiments over\ntwo visual relation benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 01:07:20 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Hanwang", ""], ["Kyaw", "Zawlin", ""], ["Yu", "Jinyang", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.01964", "submitter": "Jie Chen", "authors": "Jie Chen, Junhui Hou, Yun Ni, and Lap-Pui Chau", "title": "Accurate Light Field Depth Estimation with Superpixel Regularization\n  over Partially Occluded Regions", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2839524", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth estimation is a fundamental problem for light field photography\napplications. Numerous methods have been proposed in recent years, which either\nfocus on crafting cost terms for more robust matching, or on analyzing the\ngeometry of scene structures embedded in the epipolar-plane images. Significant\nimprovements have been made in terms of overall depth estimation error;\nhowever, current state-of-the-art methods still show limitations in handling\nintricate occluding structures and complex scenes with multiple occlusions. To\naddress these challenging issues, we propose a very effective depth estimation\nframework which focuses on regularizing the initial label confidence map and\nedge strength weights. Specifically, we first detect partially occluded\nboundary regions (POBR) via superpixel based regularization. Series of\nshrinkage/reinforcement operations are then applied on the label confidence map\nand edge strength weights over the POBR. We show that after weight\nmanipulations, even a low-complexity weighted least squares model can produce\nmuch better depth estimation than state-of-the-art methods in terms of average\ndisparity error rate, occlusion boundary precision-recall rate, and the\npreservation of intricate visual features.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 02:05:36 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Chen", "Jie", ""], ["Hou", "Junhui", ""], ["Ni", "Yun", ""], ["Chau", "Lap-Pui", ""]]}, {"id": "1708.01986", "submitter": "Takeshi Ise", "authors": "Takeshi Ise, Mari Minagawa, and Masanori Onishi", "title": "Identifying 3 moss species by deep learning, using the \"chopped picture\"\n  method", "comments": "7 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, object identification tends not to work well on ambiguous,\namorphous objects such as vegetation. In this study, we developed a simple but\neffective approach to identify ambiguous objects and applied the method to\nseveral moss species. As a result, the model correctly classified test images\nwith accuracy more than 90%. Using this approach will help progress in computer\nvision studies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 04:37:23 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 01:38:37 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Ise", "Takeshi", ""], ["Minagawa", "Mari", ""], ["Onishi", "Masanori", ""]]}, {"id": "1708.01988", "submitter": "Shuang Li", "authors": "Shuang Li, Tong Xiao, Hongsheng Li, Wei Yang, Xiaogang Wang", "title": "Identity-Aware Textual-Visual Matching with Latent Co-attention", "comments": "Accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual-visual matching aims at measuring similarities between sentence\ndescriptions and images. Most existing methods tackle this problem without\neffectively utilizing identity-level annotations. In this paper, we propose an\nidentity-aware two-stage framework for the textual-visual matching problem. Our\nstage-1 CNN-LSTM network learns to embed cross-modal features with a novel\nCross-Modal Cross-Entropy (CMCE) loss. The stage-1 network is able to\nefficiently screen easy incorrect matchings and also provide initial training\npoint for the stage-2 training. The stage-2 CNN-LSTM network refines the\nmatching results with a latent co-attention mechanism. The spatial attention\nrelates each word with corresponding image regions while the latent semantic\nattention aligns different sentence structures to make the matching results\nmore robust to sentence structure variations. Extensive experiments on three\ndatasets with identity-level annotations show that our framework outperforms\nstate-of-the-art approaches by large margins.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 04:57:45 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Li", "Shuang", ""], ["Xiao", "Tong", ""], ["Li", "Hongsheng", ""], ["Yang", "Wei", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1708.02001", "submitter": "Pingping Zhang", "authors": "Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Xiang Ruan", "title": "Amulet: Aggregating Multi-level Convolutional Features for Salient\n  Object Detection", "comments": "Accepted as a poster in ICCV 2017, including 10 pages, 5 figures and\n  2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional neural networks (FCNs) have shown outstanding performance\nin many dense labeling problems. One key pillar of these successes is mining\nrelevant information from features in convolutional layers. However, how to\nbetter aggregate multi-level convolutional feature maps for salient object\ndetection is underexplored. In this work, we present Amulet, a generic\naggregating multi-level convolutional feature framework for salient object\ndetection. Our framework first integrates multi-level feature maps into\nmultiple resolutions, which simultaneously incorporate coarse semantics and\nfine details. Then it adaptively learns to combine these feature maps at each\nresolution and predict saliency maps with the combined features. Finally, the\npredicted results are efficiently fused to generate the final saliency map. In\naddition, to achieve accurate boundary inference and semantic enhancement,\nedge-aware feature maps in low-level layers and the predicted results of low\nresolution features are recursively embedded into the learning framework. By\naggregating multi-level convolutional features in this efficient and flexible\nmanner, the proposed saliency model provides accurate salient object labeling.\nComprehensive experiments demonstrate that our method performs favorably\nagainst state-of-the art approaches in terms of near all compared evaluation\nmetrics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 06:29:38 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Pingping", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Wang", "Hongyu", ""], ["Ruan", "Xiang", ""]]}, {"id": "1708.02002", "submitter": "Tsung-Yi Lin", "authors": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Doll\\'ar", "title": "Focal Loss for Dense Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highest accuracy object detectors to date are based on a two-stage\napproach popularized by R-CNN, where a classifier is applied to a sparse set of\ncandidate object locations. In contrast, one-stage detectors that are applied\nover a regular, dense sampling of possible object locations have the potential\nto be faster and simpler, but have trailed the accuracy of two-stage detectors\nthus far. In this paper, we investigate why this is the case. We discover that\nthe extreme foreground-background class imbalance encountered during training\nof dense detectors is the central cause. We propose to address this class\nimbalance by reshaping the standard cross entropy loss such that it\ndown-weights the loss assigned to well-classified examples. Our novel Focal\nLoss focuses training on a sparse set of hard examples and prevents the vast\nnumber of easy negatives from overwhelming the detector during training. To\nevaluate the effectiveness of our loss, we design and train a simple dense\ndetector we call RetinaNet. Our results show that when trained with the focal\nloss, RetinaNet is able to match the speed of previous one-stage detectors\nwhile surpassing the accuracy of all existing state-of-the-art two-stage\ndetectors. Code is at: https://github.com/facebookresearch/Detectron.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 06:32:42 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 18:44:44 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Lin", "Tsung-Yi", ""], ["Goyal", "Priya", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "1708.02031", "submitter": "Pingping Zhang", "authors": "Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Baocai Yin", "title": "Learning Uncertain Convolutional Features for Accurate Saliency\n  Detection", "comments": "Accepted as a poster in ICCV 2017,including 10 pages, 7 figures and 3\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have delivered superior performance\nin many computer vision tasks. In this paper, we propose a novel deep fully\nconvolutional network model for accurate salient object detection. The key\ncontribution of this work is to learn deep uncertain convolutional features\n(UCF), which encourage the robustness and accuracy of saliency detection. We\nachieve this via introducing a reformulated dropout (R-dropout) after specific\nconvolutional layers to construct an uncertain ensemble of internal feature\nunits. In addition, we propose an effective hybrid upsampling method to reduce\nthe checkerboard artifacts of deconvolution operators in our decoder network.\nThe proposed methods can also be applied to other deep convolutional networks.\nCompared with existing saliency detection methods, the proposed UCF model is\nable to incorporate uncertainties for more accurate object boundary inference.\nExtensive experiments demonstrate that our proposed saliency model performs\nfavorably against state-of-the-art approaches. The uncertain feature learning\nmechanism as well as the upsampling method can significantly improve\nperformance on other pixel-wise vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 08:18:04 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhang", "Pingping", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Wang", "Hongyu", ""], ["Yin", "Baocai", ""]]}, {"id": "1708.02033", "submitter": "Silvio Giancola", "authors": "Silvio Giancola, Daniele Piron, Pasquale Poppa and Remo Sala", "title": "A Solution for Crime Scene Reconstruction using Time-of-Flight Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for three-dimensional (3D) reconstruction\nof wide crime scene, based on a Simultaneous Localization and Mapping (SLAM)\napproach. We used a Kinect V2 Time-of-Flight (TOF) RGB-D camera to provide\ncolored dense point clouds at a 30 Hz frequency. This device is moved freely (6\ndegrees of freedom) during the scene exploration. The implemented SLAM solution\naligns successive point clouds using an 3D keypoints description and matching\napproach. This type of approach exploits both colorimetric and geometrical\ninformation, and permits reconstruction under poor illumination conditions. Our\nsolution has been tested for indoor crime scene and outdoor archaeological site\nreconstruction, returning a mean error around one centimeter. It is less\nprecise than environmental laser scanner solution, but more practical and\nportable as well as less cumbersome. Also, the hardware is definitively\ncheaper.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 08:29:19 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Giancola", "Silvio", ""], ["Piron", "Daniele", ""], ["Poppa", "Pasquale", ""], ["Sala", "Remo", ""]]}, {"id": "1708.02043", "submitter": "Albert Gatt", "authors": "Marc Tanti, Albert Gatt and Kenneth P. Camilleri", "title": "What is the Role of Recurrent Neural Networks (RNNs) in an Image Caption\n  Generator?", "comments": "Appears in: Proceedings of the 10th International Conference on\n  Natural Language Generation (INLG'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural image captioning systems, a recurrent neural network (RNN) is\ntypically viewed as the primary `generation' component. This view suggests that\nthe image features should be `injected' into the RNN. This is in fact the\ndominant view in the literature. Alternatively, the RNN can instead be viewed\nas only encoding the previously generated words. This view suggests that the\nRNN should only be used to encode linguistic features and that only the final\nrepresentation should be `merged' with the image features at a later stage.\nThis paper compares these two architectures. We find that, in general, late\nmerging outperforms injection, suggesting that RNNs are better viewed as\nencoders, rather than generators.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 09:01:35 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 15:40:00 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tanti", "Marc", ""], ["Gatt", "Albert", ""], ["Camilleri", "Kenneth P.", ""]]}, {"id": "1708.02044", "submitter": "Ziwei Liu", "authors": "Sijie Yan, Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, Xiaoou Tang", "title": "Unconstrained Fashion Landmark Detection via Hierarchical Recurrent\n  Transformer Networks", "comments": "To appear in ACM Multimedia (ACM MM) 2017 as a full research paper.\n  More details at the project page:\n  http://personal.ie.cuhk.edu.hk/~lz013/projects/UnconstrainedLandmarks.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion landmarks are functional key points defined on clothes, such as\ncorners of neckline, hemline, and cuff. They have been recently introduced as\nan effective visual representation for fashion image understanding. However,\ndetecting fashion landmarks are challenging due to background clutters, human\nposes, and scales. To remove the above variations, previous works usually\nassumed bounding boxes of clothes are provided in training and test as\nadditional annotations, which are expensive to obtain and inapplicable in\npractice. This work addresses unconstrained fashion landmark detection, where\nclothing bounding boxes are not provided in both training and test. To this\nend, we present a novel Deep LAndmark Network (DLAN), where bounding boxes and\nlandmarks are jointly estimated and trained iteratively in an end-to-end\nmanner. DLAN contains two dedicated modules, including a Selective Dilated\nConvolution for handling scale discrepancies, and a Hierarchical Recurrent\nSpatial Transformer for handling background clutters. To evaluate DLAN, we\npresent a large-scale fashion landmark dataset, namely Unconstrained Landmark\nDatabase (ULD), consisting of 30K images. Statistics show that ULD is more\nchallenging than existing datasets in terms of image scales, background\nclutters, and human poses. Extensive experiments demonstrate the effectiveness\nof DLAN over the state-of-the-art methods. DLAN also exhibits excellent\ngeneralization across different clothing categories and modalities, making it\nextremely suitable for real-world fashion analysis.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 09:02:52 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Yan", "Sijie", ""], ["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Qiu", "Shi", ""], ["Wang", "Xiaogang", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1708.02071", "submitter": "Chen Zhu", "authors": "Chen Zhu, Yanpeng Zhao, Shuaiyi Huang, Kewei Tu, Yi Ma", "title": "Structured Attentions for Visual Question Answering", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention, which assigns weights to image regions according to their\nrelevance to a question, is considered as an indispensable part by most Visual\nQuestion Answering models. Although the questions may involve complex relations\namong multiple regions, few attention models can effectively encode such\ncross-region relations. In this paper, we demonstrate the importance of\nencoding such relations by showing the limited effective receptive field of\nResNet on two datasets, and propose to model the visual attention as a\nmultivariate distribution over a grid-structured Conditional Random Field on\nimage regions. We demonstrate how to convert the iterative inference\nalgorithms, Mean Field and Loopy Belief Propagation, as recurrent layers of an\nend-to-end neural network. We empirically evaluated our model on 3 datasets, in\nwhich it surpasses the best baseline model of the newly released CLEVR dataset\nby 9.5%, and the best published model on the VQA dataset by 1.25%. Source code\nis available at https: //github.com/zhuchen03/vqa-sva.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:14:11 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zhu", "Chen", ""], ["Zhao", "Yanpeng", ""], ["Huang", "Shuaiyi", ""], ["Tu", "Kewei", ""], ["Ma", "Yi", ""]]}, {"id": "1708.02072", "submitter": "Ronald Kemker", "authors": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and\n  Christopher Kanan", "title": "Measuring Catastrophic Forgetting in Neural Networks", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are used in many state-of-the-art systems for machine\nperception. Once a network is trained to do a specific task, e.g., bird\nclassification, it cannot easily be trained to do new tasks, e.g.,\nincrementally learning to recognize additional bird species or learning an\nentirely different task such as flower recognition. When new tasks are added,\ntypical deep neural networks are prone to catastrophically forgetting previous\ntasks. Networks that are capable of assimilating new information incrementally,\nmuch like how humans form new memories over time, will be more efficient than\nre-training the model from scratch each time a new task needs to be learned.\nThere have been multiple attempts to develop schemes that mitigate catastrophic\nforgetting, but these methods have not been directly compared, the tests used\nto evaluate them vary considerably, and these methods have only been evaluated\non small-scale problems (e.g., MNIST). In this paper, we introduce new metrics\nand benchmarks for directly comparing five different mechanisms designed to\nmitigate catastrophic forgetting in neural networks: regularization,\nensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on\nreal-world images and sounds show that the mechanism(s) that are critical for\noptimal performance vary based on the incremental training paradigm and type of\ndata being used, but they all demonstrate that the catastrophic forgetting\nproblem has yet to be solved.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:18:43 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 09:33:24 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 16:50:39 GMT"}, {"version": "v4", "created": "Thu, 9 Nov 2017 14:53:07 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kemker", "Ronald", ""], ["McClure", "Marc", ""], ["Abitino", "Angelina", ""], ["Hayes", "Tyler", ""], ["Kanan", "Christopher", ""]]}, {"id": "1708.02074", "submitter": "Karel Zimmermann", "authors": "Karel Zimmermann, Tomas Petricek, Vojtech Salansky, Tomas Svoboda", "title": "Learning for Active 3D Mapping", "comments": "ICCV 2017 (oral). See video:\n  https://www.youtube.com/watch?v=KNex0zjeGYE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an active 3D mapping method for depth sensors, which allow\nindividual control of depth-measuring rays, such as the newly emerging\nsolid-state lidars. The method simultaneously (i) learns to reconstruct a dense\n3D occupancy map from sparse depth measurements, and (ii) optimizes the\nreactive control of depth-measuring rays. To make the first step towards the\nonline control optimization, we propose a fast prioritized greedy algorithm,\nwhich needs to update its cost function in only a small fraction of pos- sible\nrays. The approximation ratio of the greedy algorithm is derived. An\nexperimental evaluation on the subset of the KITTI dataset demonstrates\nsignificant improve- ment in the 3D map accuracy when learning-to-reconstruct\nfrom sparse measurements is coupled with the optimization of depth-measuring\nrays.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:21:57 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zimmermann", "Karel", ""], ["Petricek", "Tomas", ""], ["Salansky", "Vojtech", ""], ["Svoboda", "Tomas", ""]]}, {"id": "1708.02096", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Jens Petersen, Jesper H. Pedersen, Marleen de\n  Bruijne", "title": "Extraction of Airways with Probabilistic State-space Models and Bayesian\n  Smoothing", "comments": "10 pages. Pre-print of the paper accepted at Workshop on Graphs in\n  Biomedical Image Analysis. MICCAI 2017. Quebec City", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting tree structures is common in several image processing\napplications. In medical image analysis, reliable segmentations of airways,\nvessels, neurons and other tree structures can enable important clinical\napplications. We present a framework for tracking tree structures comprising of\nelongated branches using probabilistic state-space models and Bayesian\nsmoothing. Unlike most existing methods that proceed with sequential tracking\nof branches, we present an exploratory method, that is less sensitive to local\nanomalies in the data due to acquisition noise and/or interfering structures.\nThe evolution of individual branches is modelled using a process model and the\nobserved data is incorporated into the update step of the Bayesian smoother\nusing a measurement model that is based on a multi-scale blob detector.\nBayesian smoothing is performed using the RTS (Rauch-Tung-Striebel) smoother,\nwhich provides Gaussian density estimates of branch states at each tracking\nstep. We select likely branch seed points automatically based on the response\nof the blob detection and track from all such seed points using the RTS\nsmoother. We use covariance of the marginal posterior density estimated for\neach branch to discriminate false positive and true positive branches. The\nmethod is evaluated on 3D chest CT scans to track airways. We show that the\npresented method results in additional branches compared to a baseline method\nbased on region growing on probability images.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 12:43:26 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Petersen", "Jens", ""], ["Pedersen", "Jesper H.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1708.02108", "submitter": "Dahun Kim", "authors": "Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon", "title": "Two-Phase Learning for Weakly Supervised Object Localization", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation and localiza- tion have a problem of\nfocusing only on the most important parts of an image since they use only\nimage-level annota- tions. In this paper, we solve this problem fundamentally\nvia two-phase learning. Our networks are trained in two steps. In the first\nstep, a conventional fully convolutional network (FCN) is trained to find the\nmost discriminative parts of an image. In the second step, the activations on\nthe most salient parts are suppressed by inference conditional feedback, and\nthen the second learning is performed to find the area of the next most\nimportant parts. By combining the activations of both phases, the entire\nportion of the tar- get object can be captured. Our proposed training scheme is\nnovel and can be utilized in well-designed techniques for weakly supervised\nsemantic segmentation, salient region detection, and object location\nprediction. Detailed experi- ments demonstrate the effectiveness of our\ntwo-phase learn- ing in each task.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:20:50 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:57:03 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 17:51:55 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Kim", "Dahun", ""], ["Cho", "Donghyeon", ""], ["Yoo", "Donggeun", ""], ["Kweon", "In So", ""]]}, {"id": "1708.02136", "submitter": "Weipeng Xu", "authors": "Weipeng Xu, Avishek Chatterjee, Michael Zollh\\\"ofer, Helge Rhodin,\n  Dushyant Mehta, Hans-Peter Seidel, Christian Theobalt", "title": "MonoPerfCap: Human Performance Capture from Monocular Video", "comments": "Accepted to ACM TOG 2018, to be presented on SIGGRAPH 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first marker-less approach for temporally coherent 3D\nperformance capture of a human with general clothing from monocular video. Our\napproach reconstructs articulated human skeleton motion as well as medium-scale\nnon-rigid surface deformations in general scenes. Human performance capture is\na challenging problem due to the large range of articulation, potentially fast\nmotion, and considerable non-rigid deformations, even from multi-view data.\nReconstruction from monocular video alone is drastically more challenging,\nsince strong occlusions and the inherent depth ambiguity lead to a highly\nill-posed reconstruction problem. We tackle these challenges by a novel\napproach that employs sparse 2D and 3D human pose detections from a\nconvolutional neural network using a batch-based pose estimation strategy.\nJoint recovery of per-batch motion allows to resolve the ambiguities of the\nmonocular reconstruction problem based on a low dimensional trajectory\nsubspace. In addition, we propose refinement of the surface geometry based on\nfully automatically extracted silhouettes to enable medium-scale non-rigid\nalignment. We demonstrate state-of-the-art performance capture results that\nenable exciting applications such as video editing and free viewpoint video,\npreviously infeasible from monocular video. Our qualitative and quantitative\nevaluation demonstrates that our approach significantly outperforms previous\nmonocular methods in terms of accuracy, robustness and scene complexity that\ncan be handled.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:43:57 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 12:40:25 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Xu", "Weipeng", ""], ["Chatterjee", "Avishek", ""], ["Zollh\u00f6fer", "Michael", ""], ["Rhodin", "Helge", ""], ["Mehta", "Dushyant", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1708.02165", "submitter": "Maxime Tremblay", "authors": "Maxime Tremblay, Andr\\'e Zaccarin", "title": "Learning to segment on tiny datasets: a new shape model", "comments": "5 pages, 2 figures, ICIP2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current object segmentation algorithms are based on the hypothesis that one\nhas access to a very large amount of data. In this paper, we aim to segment\nobjects using only tiny datasets. To this extent, we propose a new automatic\npart-based object segmentation algorithm for non-deformable and semi-deformable\nobjects in natural backgrounds. We have developed a novel shape descriptor\nwhich models the local boundaries of an object's part. This shape descriptor is\nused in a bag-of-words approach for object detection. Once the detection\nprocess is performed, we use the background and foreground likelihood given by\nour trained shape model, and the information from the image content, to define\na dense CRF model. We use a mean field approximation to solve it and thus\nsegment the object of interest. Performance evaluated on different datasets\nshows that our approach can sometimes achieve results near state-of-the-art\ntechniques based on big data while requiring only a tiny training set.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:26:11 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Tremblay", "Maxime", ""], ["Zaccarin", "Andr\u00e9", ""]]}, {"id": "1708.02179", "submitter": "Tobias Dencker", "authors": "\\\"Omer S\\\"umer, Tobias Dencker, Bj\\\"orn Ommer", "title": "Self-supervised Learning of Pose Embeddings from Spatiotemporal\n  Relations in Videos", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose analysis is presently dominated by deep convolutional networks\ntrained with extensive manual annotations of joint locations and beyond. To\navoid the need for expensive labeling, we exploit spatiotemporal relations in\ntraining videos for self-supervised learning of pose embeddings. The key idea\nis to combine temporal ordering and spatial placement estimation as auxiliary\ntasks for learning pose similarities in a Siamese convolutional network. Since\nthe self-supervised sampling of both tasks from natural videos can result in\nambiguous and incorrect training labels, our method employs a curriculum\nlearning idea that starts training with the most reliable data samples and\ngradually increases the difficulty. To further refine the training process we\nmine repetitive poses in individual videos which provide reliable labels while\nremoving inconsistencies. Our pose embeddings capture visual characteristics of\nhuman pose that can boost existing supervised representations in human pose\nestimation and retrieval. We report quantitative and qualitative results on\nthese tasks in Olympic Sports, Leeds Pose Sports and MPII Human Pose datasets.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:57:32 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["S\u00fcmer", "\u00d6mer", ""], ["Dencker", "Tobias", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1708.02191", "submitter": "Kihyuk Sohn", "authors": "Kihyuk Sohn, Sifei Liu, Guangyu Zhong, Xiang Yu, Ming-Hsuan Yang,\n  Manmohan Chandraker", "title": "Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos", "comments": "accepted for publication at International Conference on Computer\n  Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid advances in face recognition, there remains a clear gap between\nthe performance of still image-based face recognition and video-based face\nrecognition, due to the vast difference in visual quality between the domains\nand the difficulty of curating diverse large-scale video datasets. This paper\naddresses both of those challenges, through an image to video feature-level\ndomain adaptation approach, to learn discriminative video frame\nrepresentations. The framework utilizes large-scale unlabeled video data to\nreduce the gap between different domains while transferring discriminative\nknowledge from large-scale labeled still images. Given a face recognition\nnetwork that is pretrained in the image domain, the adaptation is achieved by\n(i) distilling knowledge from the network to a video adaptation network through\nfeature matching, (ii) performing feature restoration through synthetic data\naugmentation and (iii) learning a domain-invariant feature through a domain\nadversarial discriminator. We further improve performance through a\ndiscriminator-guided feature fusion that boosts high-quality frames while\neliminating those degraded by video domain-specific factors. Experiments on the\nYouTube Faces and IJB-A datasets demonstrate that each module contributes to\nour feature-level domain adaptation framework and substantially improves video\nface recognition performance to achieve state-of-the-art accuracy. We\ndemonstrate qualitatively that the network learns to suppress diverse artifacts\nin videos such as pose, illumination or occlusion without being explicitly\ntrained for them.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:36:54 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Sohn", "Kihyuk", ""], ["Liu", "Sifei", ""], ["Zhong", "Guangyu", ""], ["Yu", "Xiang", ""], ["Yang", "Ming-Hsuan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1708.02209", "submitter": "Ying Tai", "authors": "Ying Tai, Jian Yang, Xiaoming Liu, Chunyan Xu", "title": "MemNet: A Persistent Memory Network for Image Restoration", "comments": "Accepted by ICCV 2017 (Spotlight presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, very deep convolutional neural networks (CNNs) have been attracting\nconsiderable attention in image restoration. However, as the depth grows, the\nlong-term dependency problem is rarely realized for these very deep models,\nwhich results in the prior states/layers having little influence on the\nsubsequent ones. Motivated by the fact that human thoughts have persistency, we\npropose a very deep persistent memory network (MemNet) that introduces a memory\nblock, consisting of a recursive unit and a gate unit, to explicitly mine\npersistent memory through an adaptive learning process. The recursive unit\nlearns multi-level representations of the current state under different\nreceptive fields. The representations and the outputs from the previous memory\nblocks are concatenated and sent to the gate unit, which adaptively controls\nhow much of the previous states should be reserved, and decides how much of the\ncurrent state should be stored. We apply MemNet to three image restoration\ntasks, i.e., image denosing, super-resolution and JPEG deblocking.\nComprehensive experiments demonstrate the necessity of the MemNet and its\nunanimous superiority on all three tasks over the state of the arts. Code is\navailable at https://github.com/tyshiwo/MemNet.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:20:58 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Tai", "Ying", ""], ["Yang", "Jian", ""], ["Liu", "Xiaoming", ""], ["Xu", "Chunyan", ""]]}, {"id": "1708.02212", "submitter": "Nicholas Kolkin", "authors": "Nicholas Kolkin, Gregory Shakhnarovich, Eli Shechtman", "title": "Training Deep Networks to be Spatially Sensitive", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many computer vision tasks, for example saliency prediction or semantic\nsegmentation, the desired output is a foreground map that predicts pixels where\nsome criteria is satisfied. Despite the inherently spatial nature of this task\ncommonly used learning objectives do not incorporate the spatial relationships\nbetween misclassified pixels and the underlying ground truth. The Weighted\nF-measure, a recently proposed evaluation metric, does reweight errors\nspatially, and has been shown to closely correlate with human evaluation of\nquality, and stably rank predictions with respect to noisy ground truths (such\nas a sloppy human annotator might generate). However it suffers from\ncomputational complexity which makes it intractable as an optimization\nobjective for gradient descent, which must be evaluated thousands or millions\nof times while learning a model's parameters. We propose a differentiable and\nefficient approximation of this metric. By incorporating spatial information\ninto the objective we can use a simpler model than competing methods without\nsacrificing accuracy, resulting in faster inference speeds and alleviating the\nneed for pre/post-processing. We match (or improve) performance on several\ntasks compared to prior state of the art by traditional metrics, and in many\ncases significantly improve performance by the weighted F-measure.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:26:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Kolkin", "Nicholas", ""], ["Shakhnarovich", "Gregory", ""], ["Shechtman", "Eli", ""]]}, {"id": "1708.02215", "submitter": "Skanda Koppula", "authors": "Skanda Koppula", "title": "Learning a CNN-based End-to-End Controller for a Formula SAE Racecar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of CNN-based end-to-end models for controls of a Formula SAE\nracecar, along with various benchmarking and visualization tools to understand\nmodel performance. We tackled three main problems in the context of\ncone-delineated racetrack driving: (1) discretized steering, which translates a\nfirst-person frame along to the track to a predicted steering direction. (2)\nreal-value steering, which translates a frame view to a real-value steering\nangle, and (3) a network design for predicting brake and throttle. We\ndemonstrate high accuracy on our discretization task, low theoretical testing\nerrors with our model for real-value steering, and a starting point for future\nwork regarding a controller for our vehicle's brake and throttle. Timing\nbenchmarks suggests that the networks we propose have the latency and\nthroughput required for real-time controllers, when run on GPU-enabled\nhardware.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 08:04:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Koppula", "Skanda", ""]]}, {"id": "1708.02218", "submitter": "Antoine Tixier", "authors": "Antoine Jean-Pierre Tixier, Giannis Nikolentzos, Polykarpos\n  Meladianos, Michalis Vazirgiannis", "title": "Graph Classification with 2D Convolutional Neural Networks", "comments": "Published at ICANN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph learning is currently dominated by graph kernels, which, while\npowerful, suffer some significant limitations. Convolutional Neural Networks\n(CNNs) offer a very appealing alternative, but processing graphs with CNNs is\nnot trivial. To address this challenge, many sophisticated extensions of CNNs\nhave recently been introduced. In this paper, we reverse the problem: rather\nthan proposing yet another graph CNN model, we introduce a novel way to\nrepresent graphs as multi-channel image-like structures that allows them to be\nhandled by vanilla 2D CNNs. Experiments reveal that our method is more accurate\nthan state-of-the-art graph kernels and graph CNNs on 4 out of 6 real-world\ndatasets (with and without continuous node attributes), and close elsewhere.\nOur approach is also preferable to graph kernels in terms of time complexity.\nCode and data are publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 09:20:29 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 15:57:14 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 15:17:31 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 12:28:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tixier", "Antoine Jean-Pierre", ""], ["Nikolentzos", "Giannis", ""], ["Meladianos", "Polykarpos", ""], ["Vazirgiannis", "Michalis", ""]]}, {"id": "1708.02237", "submitter": "Michael Vertolli", "authors": "Michael O. Vertolli and Jim Davies", "title": "Image Quality Assessment Techniques Show Improved Training and\n  Evaluation of Autoencoder Generative Adversarial Networks", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a training and evaluation approach for autoencoder Generative\nAdversarial Networks (GANs), specifically the Boundary Equilibrium Generative\nAdversarial Network (BEGAN), based on methods from the image quality assessment\nliterature. Our approach explores a multidimensional evaluation criterion that\nutilizes three distance functions: an $l_1$ score, the Gradient Magnitude\nSimilarity Mean (GMSM) score, and a chrominance score. We show that each of the\ndifferent distance functions captures a slightly different set of properties in\nimage space and, consequently, requires its own evaluation criterion to\nproperly assess whether the relevant property has been adequately learned. We\nshow that models using the new distance functions are able to produce better\nimages than the original BEGAN model in predicted ways.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 16:31:07 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Vertolli", "Michael O.", ""], ["Davies", "Jim", ""]]}, {"id": "1708.02282", "submitter": "Nadieh Khalili", "authors": "N. Khalili, P. Moeskops, N.H.P. Claessens, S. Scherpenzeel, E. Turk,\n  R. de Heus, M.J.N.L. Benders, M.A. Viergever, J.P.W. Pluim, I. I\\v{s}gum", "title": "Automatic segmentation of the intracranialvolume in fetal MR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MR images of the fetus allow non-invasive analysis of the fetal brain.\nQuantitative analysis of fetal brain development requires automatic brain\ntissue segmentation that is typically preceded by segmentation of the\nintracranial volume (ICV). This is challenging because fetal MR images\nvisualize the whole moving fetus and in addition partially visualize the\nmaternal body. This paper presents an automatic method for segmentation of the\nICV in fetal MR images. The method employs a multi-scale convolutional neural\nnetwork in 2D slices to enable learning spatial information from larger context\nas well as detailed local information. The method is developed and evaluated\nwith 30 fetal T2-weighted MRI scans (average age $33.2\\pm1.2$ weeks\npostmenstrual age). The set contains $10$ scans acquired in axial, $10$ in\ncoronal and $10$ in sagittal imaging planes. A reference standard was defined\nin all images by manual annotation of the intracranial volume in $10$\nequidistantly distributed slices. The automatic analysis was performed by\ntraining and testing the network using scans acquired in the representative\nimaging plane as well as combining the training data from all imaging planes.\nOn average, the automatic method achieved Dice coefficients of 0.90 for the\naxial images, 0.90 for the coronal images and 0.92 for the sagittal images.\nCombining the training sets resulted in average Dice coefficients of 0.91 for\nthe axial images, 0.95 for the coronal images, and 0.92 for the sagittal\nimages. The results demonstrate that the evaluated method achieved good\nperformance in extracting ICV in fetal MR scans regardless of the imaging\nplane.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:44:38 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Khalili", "N.", ""], ["Moeskops", "P.", ""], ["Claessens", "N. H. P.", ""], ["Scherpenzeel", "S.", ""], ["Turk", "E.", ""], ["de Heus", "R.", ""], ["Benders", "M. J. N. L.", ""], ["Viergever", "M. A.", ""], ["Pluim", "J. P. W.", ""], ["I\u0161gum", "I.", ""]]}, {"id": "1708.02283", "submitter": "Ga\\\"el \\'Ecorchard", "authors": "J\\'er\\'emy Taquet, Ga\\\"el \\'Ecorchard, Libor P\\v{r}eu\\v{c}il", "title": "Real-Time Visual Localisation in a Tagged Environment", "comments": "Student Conference on Planning in Artificial Intelligence and\n  Robotics, Sept. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a robotised warehouse a major issue is the safety of human operators in\ncase of intervention in the work area of the robots. The current solution is to\nshut down every robot but it causes a loss of productivity, especially for\nlarge robotised warehouses. In order to avoid this loss we need to ensure the\noperator's security during his/her intervention in the warehouse without\npowering off the robots. The human operator needs to be localised in the\nwarehouse and the trajectories of the robots have to be modified so that they\ndo not interfere with the human. The purpose of this paper is to demonstrate a\nvisual localisation method with visual elements that are already available in\nthe current warehouse setup.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 13:11:55 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Taquet", "J\u00e9r\u00e9my", ""], ["\u00c9corchard", "Ga\u00ebl", ""], ["P\u0159eu\u010dil", "Libor", ""]]}, {"id": "1708.02285", "submitter": "Elaheh Rashedi", "authors": "Elaheh Rashedi, Saba Adabi, Darius Mehregan, Silvia Conforto, Xue-wen\n  Chen", "title": "An Adaptive Cluster-based Filtering Framework for Speckle Reduction of\n  OCT Skin Images", "comments": "17 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) has become a favorable device in the\nDermatology discipline due to its moderate resolution and penetration depth.\nOCT images however contain a grainy pattern, called speckle, due to the use of\na broadband source in the configuration of OCT. So far, a variety of filtering\n(de-speckling) techniques is introduced to reduce speckle in OCT images. Most\nof these methods are generic and can be applied to OCT images of different\ntissues. The ambition of this work is to provide a de-speckling framework\nspecialized for filtering skin tissues for the community to utilize, adapt or\nbuild upon. In this paper, we present an adaptive cluster-based filtering\nframework, optimized for speckle reduction of OCT skin images. In this\nframework, by considering the layered structure of skin, first the OCT skin\nimages are segmented into differentiable layers utilizing clustering\nalgorithms, and then each cluster is de-speckled individually using adaptive\nfiltering techniques. In this study, hierarchical clustering algorithm and\nadaptive Wiener filtering technique are utilized to develop the framework. The\nproposed method is tested on optical solid phantoms with predetermined optical\nproperties. The method is also tested on healthy human skin images. The results\nshow that the proposed cluster-based filtering method can effectively reduce\nthe speckle and increase the signal-to-noise ratio and contrast while\npreserving the edges in the image. The proposed cluster-based filtering\nframework enables researchers to develop unsupervised learning solutions for\nde-speckling OCT skin images using adaptive filtering methods, or extend the\nframework to new applications.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 20:40:40 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 06:01:52 GMT"}, {"version": "v3", "created": "Sat, 26 Aug 2017 01:31:16 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 18:37:22 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Rashedi", "Elaheh", ""], ["Adabi", "Saba", ""], ["Mehregan", "Darius", ""], ["Conforto", "Silvia", ""], ["Chen", "Xue-wen", ""]]}, {"id": "1708.02286", "submitter": "Yu Cheng", "authors": "Shuangjie Xu, Yu Cheng, Kang Gu, Yang Yang, Shiyu Chang, Pan Zhou", "title": "Jointly Attentive Spatial-Temporal Pooling Networks for Video-based\n  Person Re-Identification", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (person re-id) is a crucial task as its applications\nin visual surveillance and human-computer interaction. In this work, we present\na novel joint Spatial and Temporal Attention Pooling Network (ASTPN) for\nvideo-based person re-identification, which enables the feature extractor to be\naware of the current input video sequences, in a way that interdependency from\nthe matching items can directly influence the computation of each other's\nrepresentation. Specifically, the spatial pooling layer is able to select\nregions from each frame, while the attention temporal pooling performed can\nselect informative frames over the sequence, both pooling guided by the\ninformation from distance matching. Experiments are conduced on the iLIDS-VID,\nPRID-2011 and MARS datasets and the results demonstrate that this approach\noutperforms existing state-of-art methods. We also analyze how the joint\npooling in both dimensions can boost the person re-id performance more\neffectively than using either of them separately.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 02:35:17 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 14:41:58 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Xu", "Shuangjie", ""], ["Cheng", "Yu", ""], ["Gu", "Kang", ""], ["Yang", "Yang", ""], ["Chang", "Shiyu", ""], ["Zhou", "Pan", ""]]}, {"id": "1708.02287", "submitter": "Bo Li", "authors": "Bo Li, Yuchao Dai, Mingyi He", "title": "Monocular Depth Estimation with Hierarchical Fusion of Dilated CNNs and\n  Soft-Weighted-Sum Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation is a challenging task in complex compositions\ndepicting multiple objects of diverse scales. Albeit the recent great progress\nthanks to the deep convolutional neural networks (CNNs), the state-of-the-art\nmonocular depth estimation methods still fall short to handle such real-world\nchallenging scenarios. In this paper, we propose a deep end-to-end learning\nframework to tackle these challenges, which learns the direct mapping from a\ncolor image to the corresponding depth map. First, we represent monocular depth\nestimation as a multi-category dense labeling task by contrast to the\nregression based formulation. In this way, we could build upon the recent\nprogress in dense labeling such as semantic segmentation. Second, we fuse\ndifferent side-outputs from our front-end dilated convolutional neural network\nin a hierarchical way to exploit the multi-scale depth cues for depth\nestimation, which is critical to achieve scale-aware depth estimation. Third,\nwe propose to utilize soft-weighted-sum inference instead of the hard-max\ninference, transforming the discretized depth score to continuous depth value.\nThus, we reduce the influence of quantization error and improve the robustness\nof our method. Extensive experiments on the NYU Depth V2 and KITTI datasets\nshow the superiority of our method compared with current state-of-the-art\nmethods. Furthermore, experiments on the NYU V2 dataset reveal that our model\nis able to learn the probability distribution of depth.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 06:41:50 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Bo", ""], ["Dai", "Yuchao", ""], ["He", "Mingyi", ""]]}, {"id": "1708.02288", "submitter": "Yang Wang", "authors": "Yang Wang, Lin Wu", "title": "Beyond Low-Rank Representations: Orthogonal Clustering Basis\n  Reconstruction with Optimized Graph Structure for Multi-view Spectral\n  Clustering", "comments": "Accepted to appear in Neural Networks, Elsevier, on 9th March 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-Rank Representation (LRR) is arguably one of the most powerful paradigms\nfor Multi-view spectral clustering, which elegantly encodes the multi-view\nlocal graph/manifold structures into an intrinsic low-rank self-expressive data\nsimilarity embedded in high-dimensional space, to yield a better graph\npartition than their single-view counterparts. In this paper we revisit it with\na fundamentally different perspective by discovering LRR as essentially a\nlatent clustered orthogonal projection based representation winged with an\noptimized local graph structure for spectral clustering; each column of the\nrepresentation is fundamentally a cluster basis orthogonal to others to\nindicate its members, which intuitively projects the view-specific feature\nrepresentation to be the one spanned by all orthogonal basis to characterize\nthe cluster structures. Upon this finding, we propose our technique with the\nfollowings: (1) We decompose LRR into latent clustered orthogonal\nrepresentation via low-rank matrix factorization, to encode the more flexible\ncluster structures than LRR over primal data objects; (2) We convert the\nproblem of LRR into that of simultaneously learning orthogonal clustered\nrepresentation and optimized local graph structure for each view; (3) The\nlearned orthogonal clustered representations and local graph structures enjoy\nthe same magnitude for multi-view, so that the ideal multi-view consensus can\nbe readily achieved. The experiments over multi-view datasets validate its\nsuperiority.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 03:36:26 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:46:52 GMT"}, {"version": "v3", "created": "Wed, 20 Dec 2017 23:25:57 GMT"}, {"version": "v4", "created": "Thu, 22 Mar 2018 01:08:03 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Wang", "Yang", ""], ["Wu", "Lin", ""]]}, {"id": "1708.02300", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Reinforced Video Captioning with Entailment Rewards", "comments": "EMNLP 2017 (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have shown promising improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning, we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 20:50:24 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1708.02313", "submitter": "Avi Singh", "authors": "Avi Singh, Larry Yang, Sergey Levine", "title": "GPLAC: Generalizing Vision-Based Robotic Skills using Weakly Labeled\n  Images", "comments": "ICCV 2017. Also accepted at ICML 2017 Workshop on Lifelong Learning:\n  A Reinforcement Learning Approach. Webpage:\n  https://people.eecs.berkeley.edu/~avisingh/iccv17/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of learning robotic sensorimotor control policies that\ncan generalize to visually diverse and unseen environments. Achieving broad\ngeneralization typically requires large datasets, which are difficult to obtain\nfor task-specific interactive processes such as reinforcement learning or\nlearning from demonstration. However, much of the visual diversity in the world\ncan be captured through passively collected datasets of images or videos. In\nour method, which we refer to as GPLAC (Generalized Policy Learning with\nAttentional Classifier), we use both interaction data and weakly labeled image\ndata to augment the generalization capacity of sensorimotor policies. Our\nmethod combines multitask learning on action selection and an auxiliary binary\nclassification objective, together with a convolutional neural network\narchitecture that uses an attentional mechanism to avoid distractors. We show\nthat pairing interaction data from just a single environment with a diverse\ndataset of weakly labeled data results in greatly improved generalization to\nunseen environments, and show that this generalization depends on both the\nauxiliary objective and the attentional architecture that we propose. We\ndemonstrate our results in both simulation and on a real robotic manipulator,\nand demonstrate substantial improvement over standard convolutional\narchitectures and domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 21:34:59 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Singh", "Avi", ""], ["Yang", "Larry", ""], ["Levine", "Sergey", ""]]}, {"id": "1708.02314", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi", "title": "Multibiometric Secure System Based on Deep Learning", "comments": "To be published in Proc. IEEE Global SIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a secure multibiometric system that uses deep\nneural networks and error-correction coding. We present a feature-level fusion\nframework to generate a secure multibiometric template from each user's\nmultiple biometrics. Two fusion architectures, fully connected architecture and\nbilinear architecture, are implemented to develop a robust multibiometric\nshared representation. The shared representation is used to generate a\ncancelable biometric template that involves the selection of a different set of\nreliable and discriminative features for each user. This cancelable template is\na binary vector and is passed through an appropriate error-correcting decoder\nto find a closest codeword and this codeword is hashed to generate the final\nsecure template. The efficacy of the proposed approach is shown using a\nmultimodal database where we achieve state-of-the-art matching performance,\nalong with cancelability and security.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 21:35:26 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Talreja", "Veeru", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1708.02330", "submitter": "Jeffrey Hawke", "authors": "Jeffrey Hawke, Alex Bewley, Ingmar Posner", "title": "What Makes a Place? Building Bespoke Place Dependent Object Detectors\n  for Robotics", "comments": "IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is about enabling robots to improve their perceptual performance\nthrough repeated use in their operating environment, creating local expert\ndetectors fitted to the places through which a robot moves. We leverage the\nconcept of 'experiences' in visual perception for robotics, accounting for bias\nin the data a robot sees by fitting object detector models to a particular\nplace. The key question we seek to answer in this paper is simply: how do we\ndefine a place? We build bespoke pedestrian detector models for autonomous\ndriving, highlighting the necessary trade off between generalisation and model\ncapacity as we vary the extent of the place we fit to. We demonstrate a\nsizeable performance gain over a current state-of-the-art detector when using\ncomputationally lightweight bespoke place-fitted detector models.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 23:06:20 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Hawke", "Jeffrey", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1708.02337", "submitter": "Manuel G\\\"unther", "authors": "Manuel G\\\"unther, Peiyun Hu, Christian Herrmann, Chi Ho Chan, Min\n  Jiang, Shufan Yang, Akshay Raj Dhamija, Deva Ramanan, J\\\"urgen Beyerer, Josef\n  Kittler, Mohamad Al Jazaery, Mohammad Iqbal Nouyed, Guodong Guo, Cezary\n  Stankiewicz, Terrance E. Boult", "title": "Unconstrained Face Detection and Open-Set Face Recognition Challenge", "comments": "This is an ERRATA version of the paper originally presented at the\n  International Joint Conference on Biometrics. Due to a bug in our evaluation\n  code, the results of the participants changed. The final conclusion, however,\n  is still the same", "journal-ref": null, "doi": "10.1109/BTAS.2017.8272759", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection and recognition benchmarks have shifted toward more difficult\nenvironments. The challenge presented in this paper addresses the next step in\nthe direction of automatic detection and identification of people from outdoor\nsurveillance cameras. While face detection has shown remarkable success in\nimages collected from the web, surveillance cameras include more diverse\nocclusions, poses, weather conditions and image blur. Although face\nverification or closed-set face identification have surpassed human\ncapabilities on some datasets, open-set identification is much more complex as\nit needs to reject both unknown identities and false accepts from the face\ndetector. We show that unconstrained face detection can approach high detection\nrates albeit with moderate false accept rates. By contrast, open-set face\nrecognition is currently weak and requires much more attention.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 00:28:02 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 15:58:00 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 22:44:35 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["G\u00fcnther", "Manuel", ""], ["Hu", "Peiyun", ""], ["Herrmann", "Christian", ""], ["Chan", "Chi Ho", ""], ["Jiang", "Min", ""], ["Yang", "Shufan", ""], ["Dhamija", "Akshay Raj", ""], ["Ramanan", "Deva", ""], ["Beyerer", "J\u00fcrgen", ""], ["Kittler", "Josef", ""], ["Jazaery", "Mohamad Al", ""], ["Nouyed", "Mohammad Iqbal", ""], ["Guo", "Guodong", ""], ["Stankiewicz", "Cezary", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1708.02349", "submitter": "Xiyang Dai", "authors": "Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S. Davis, Yan Qiu Chen", "title": "Temporal Context Network for Activity Localization in Videos", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Temporal Context Network (TCN) for precise temporal localization\nof human activities. Similar to the Faster-RCNN architecture, proposals are\nplaced at equal intervals in a video which span multiple temporal scales. We\npropose a novel representation for ranking these proposals. Since pooling\nfeatures only inside a segment is not sufficient to predict activity\nboundaries, we construct a representation which explicitly captures context\naround a proposal for ranking it. For each temporal segment inside a proposal,\nfeatures are uniformly sampled at a pair of scales and are input to a temporal\nconvolutional neural network for classification. After ranking proposals,\nnon-maximum suppression is applied and classification is performed to obtain\nfinal detections. TCN outperforms state-of-the-art methods on the ActivityNet\ndataset and the THUMOS14 dataset.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 01:46:03 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Dai", "Xiyang", ""], ["Singh", "Bharat", ""], ["Zhang", "Guyue", ""], ["Davis", "Larry S.", ""], ["Chen", "Yan Qiu", ""]]}, {"id": "1708.02386", "submitter": "Qiantong Xu", "authors": "Qiantong Xu, Ke Yan, Yonghong Tian", "title": "Learning a Repression Network for Precise Vehicle Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing explosion in the use of surveillance cameras in public security\nhighlights the importance of vehicle search from large-scale image databases.\nPrecise vehicle search, aiming at finding out all instances for a given query\nvehicle image, is a challenging task as different vehicles will look very\nsimilar to each other if they share same visual attributes. To address this\nproblem, we propose the Repression Network (RepNet), a novel multi-task\nlearning framework, to learn discriminative features for each vehicle image\nfrom both coarse-grained and detailed level simultaneously. Besides, benefited\nfrom the satisfactory accuracy of attribute classification, a bucket search\nmethod is proposed to reduce the retrieval time while still maintaining\ncompetitive performance. We conduct extensive experiments on the revised\nVehcileID dataset. Experimental results show that our RepNet achieves the\nstate-of-the-art performance and the bucket search method can reduce the\nretrieval time by about 24 times.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 07:14:14 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Xu", "Qiantong", ""], ["Yan", "Ke", ""], ["Tian", "Yonghong", ""]]}, {"id": "1708.02412", "submitter": "Ran He", "authors": "Ran He, Xiang Wu, Zhenan Sun, Tieniu Tan", "title": "Wasserstein CNN: Learning Invariant Features for NIR-VIS Face\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous face recognition (HFR) aims to match facial images acquired\nfrom different sensing modalities with mission-critical applications in\nforensics, security and commercial sectors. However, HFR is a much more\nchallenging problem than traditional face recognition because of large\nintra-class variations of heterogeneous face images and limited training\nsamples of cross-modality face image pairs. This paper proposes a novel\napproach namely Wasserstein CNN (convolutional neural networks, or WCNN for\nshort) to learn invariant features between near-infrared and visual face images\n(i.e. NIR-VIS face recognition). The low-level layers of WCNN are trained with\nwidely available face images in visual spectrum. The high-level layer is\ndivided into three parts, i.e., NIR layer, VIS layer and NIR-VIS shared layer.\nThe first two layers aims to learn modality-specific features and NIR-VIS\nshared layer is designed to learn modality-invariant feature subspace.\nWasserstein distance is introduced into NIR-VIS shared layer to measure the\ndissimilarity between heterogeneous feature distributions. So W-CNN learning\naims to achieve the minimization of Wasserstein distance between NIR\ndistribution and VIS distribution for invariant deep feature representation of\nheterogeneous face images. To avoid the over-fitting problem on small-scale\nheterogeneous face data, a correlation prior is introduced on the\nfully-connected layers of WCNN network to reduce parameter space. This prior is\nimplemented by a low-rank constraint in an end-to-end network. The joint\nformulation leads to an alternating minimization for deep feature\nrepresentation at training stage and an efficient computation for heterogeneous\ndata at testing stage. Extensive experiments on three challenging NIR-VIS face\nrecognition databases demonstrate the significant superiority of Wasserstein\nCNN over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:07:34 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["He", "Ran", ""], ["Wu", "Xiang", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1708.02421", "submitter": "Xin Li Mr.", "authors": "Xin Li, Zequn Jie, Wei Wang, Changsong Liu, Jimei Yang, Xiaohui Shen,\n  Zhe Lin, Qiang Chen, Shuicheng Yan, Jiashi Feng", "title": "FoveaNet: Perspective-aware Urban Scene Parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing urban scene images benefits many applications, especially\nself-driving. Most of the current solutions employ generic image parsing models\nthat treat all scales and locations in the images equally and do not consider\nthe geometry property of car-captured urban scene images. Thus, they suffer\nfrom heterogeneous object scales caused by perspective projection of cameras on\nactual scenes and inevitably encounter parsing failures on distant objects as\nwell as other boundary and recognition errors. In this work, we propose a new\nFoveaNet model to fully exploit the perspective geometry of scene images and\naddress the common failures of generic parsing models. FoveaNet estimates the\nperspective geometry of a scene image through a convolutional network which\nintegrates supportive evidence from contextual objects within the image. Based\non the perspective geometry information, FoveaNet \"undoes\" the camera\nperspective projection analyzing regions in the space of the actual scene, and\nthus provides much more reliable parsing results. Furthermore, to effectively\naddress the recognition errors, FoveaNet introduces a new dense CRFs model that\ntakes the perspective geometry as a prior potential. We evaluate FoveaNet on\ntwo urban scene parsing datasets, Cityspaces and CamVid, which demonstrates\nthat FoveaNet can outperform all the well-established baselines and provide new\nstate-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 09:29:50 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Xin", ""], ["Jie", "Zequn", ""], ["Wang", "Wei", ""], ["Liu", "Changsong", ""], ["Yang", "Jimei", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Chen", "Qiang", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1708.02439", "submitter": "Xin Li Mr.", "authors": "Xin Li, Changsong Liu", "title": "Prune the Convolutional Neural Networks with Sparse Shrink", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, it is still difficult to adapt Convolutional Neural Network (CNN)\nbased models for deployment on embedded devices. The heavy computation and\nlarge memory footprint of CNN models become the main burden in real\napplication. In this paper, we propose a \"Sparse Shrink\" algorithm to prune an\nexisting CNN model. By analyzing the importance of each channel via sparse\nreconstruction, the algorithm is able to prune redundant feature maps\naccordingly. The resulting pruned model thus directly saves computational\nresource. We have evaluated our algorithm on CIFAR-100. As shown in our\nexperiments, we can reduce 56.77% parameters and 73.84% multiplication in total\nwith only minor decrease in accuracy. These results have demonstrated the\neffectiveness of our \"Sparse Shrink\" algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 10:28:20 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Xin", ""], ["Liu", "Changsong", ""]]}, {"id": "1708.02443", "submitter": "S L Happy", "authors": "S L Happy, Ramanarayan Mohanty, Aurobinda Routray", "title": "An Effective Feature Selection Method Based on Pair-Wise Feature\n  Proximity for High Dimensional Low Sample Size Data", "comments": "European Signal Processing Conference 2017", "journal-ref": null, "doi": "10.23919/EUSIPCO.2017.8081474", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection has been studied widely in the literature. However, the\nefficacy of the selection criteria for low sample size applications is\nneglected in most cases. Most of the existing feature selection criteria are\nbased on the sample similarity. However, the distance measures become\ninsignificant for high dimensional low sample size (HDLSS) data. Moreover, the\nvariance of a feature with a few samples is pointless unless it represents the\ndata distribution efficiently. Instead of looking at the samples in groups, we\nevaluate their efficiency based on pairwise fashion. In our investigation, we\nnoticed that considering a pair of samples at a time and selecting the features\nthat bring them closer or put them far away is a better choice for feature\nselection. Experimental results on benchmark data sets demonstrate the\neffectiveness of the proposed method with low sample size, which outperforms\nmany other state-of-the-art feature selection methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 11:05:18 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Happy", "S L", ""], ["Mohanty", "Ramanarayan", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1708.02459", "submitter": "Zhiyuan Shi", "authors": "Zhiyuan Shi, Yongxin Yang, Timothy M. Hospedales, Tao Xiang", "title": "Weakly Supervised Image Annotation and Segmentation with Objects and\n  Attributes", "comments": "Accepted in IEEE Transaction on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model complex visual scenes using a non-parametric Bayesian\nmodel learned from weakly labelled images abundant on media sharing sites such\nas Flickr. Given weak image-level annotations of objects and attributes without\nlocations or associations between them, our model aims to learn the appearance\nof object and attribute classes as well as their association on each object\ninstance. Once learned, given an image, our model can be deployed to tackle a\nnumber of vision problems in a joint and coherent manner, including recognising\nobjects in the scene (automatic object annotation), describing objects using\ntheir attributes (attribute prediction and association), and localising and\ndelineating the objects (object detection and semantic segmentation). This is\nachieved by developing a novel Weakly Supervised Markov Random Field Stacked\nIndian Buffet Process (WS-MRF-SIBP) that models objects and attributes as\nlatent factors and explicitly captures their correlations within and across\nsuperpixels. Extensive experiments on benchmark datasets demonstrate that our\nweakly supervised model significantly outperforms weakly supervised\nalternatives and is often comparable with existing strongly supervised models\non a variety of tasks including semantic segmentation, automatic image\nannotation and retrieval based on object-attribute associations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 12:19:48 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Shi", "Zhiyuan", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy M.", ""], ["Xiang", "Tao", ""]]}, {"id": "1708.02476", "submitter": "Yu Zeng", "authors": "Yu Zeng, Huchuan Lu, Ali Borji, Mengyang Feng", "title": "An Unsupervised Game-Theoretic Approach to Saliency Detection", "comments": "This paper has been submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2838761", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised game-theoretic salient object detection\nalgorithm that does not require labeled training data. First, saliency\ndetection problem is formulated as a non-cooperative game, hereinafter referred\nto as Saliency Game, in which image regions are players who choose to be\n\"background\" or \"foreground\" as their pure strategies. A payoff function is\nconstructed by exploiting multiple cues and combining complementary features.\nSaliency maps are generated according to each region's strategy in the Nash\nequilibrium of the proposed Saliency Game. Second, we explore the complementary\nrelationship between color and deep features and propose an Iterative Random\nWalk algorithm to combine saliency maps produced by the Saliency Game using\ndifferent features. Iterative random walk allows sharing information across\nfeature spaces, and detecting objects that are otherwise very hard to detect.\nExtensive experiments over 6 challenging datasets demonstrate the superiority\nof our proposed unsupervised algorithm compared to several state of the art\nsupervised algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 13:22:21 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zeng", "Yu", ""], ["Lu", "Huchuan", ""], ["Borji", "Ali", ""], ["Feng", "Mengyang", ""]]}, {"id": "1708.02478", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li, Alan Hanjalic, Heng\n  Tao Shen", "title": "From Deterministic to Generative: Multi-Modal Stochastic RNNs for Video\n  Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning in essential is a complex natural process, which is affected\nby various uncertainties stemming from video content, subjective judgment, etc.\nIn this paper we build on the recent progress in using encoder-decoder\nframework for video captioning and address what we find to be a critical\ndeficiency of the existing methods, that most of the decoders propagate\ndeterministic hidden states. Such complex uncertainty cannot be modeled\nefficiently by the deterministic models. In this paper, we propose a generative\napproach, referred to as multi-modal stochastic RNNs networks (MS-RNN), which\nmodels the uncertainty observed in the data using latent stochastic variables.\nTherefore, MS-RNN can improve the performance of video captioning, and generate\nmultiple sentences to describe a video considering different random factors.\nSpecifically, a multi-modal LSTM (M-LSTM) is first proposed to interact with\nboth visual and textual features to capture a high-level representation. Then,\na backward stochastic LSTM (S-LSTM) is proposed to support uncertainty\npropagation by introducing latent variables. Experimental results on the\nchallenging datasets MSVD and MSR-VTT show that our proposed MS-RNN approach\noutperforms the state-of-the-art video captioning benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 13:27:13 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 09:27:14 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Song", "Jingkuan", ""], ["Guo", "Yuyu", ""], ["Gao", "Lianli", ""], ["Li", "Xuelong", ""], ["Hanjalic", "Alan", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1708.02531", "submitter": "Yuming Shen", "authors": "Yuming Shen, Li Liu, Ling Shao, Jingkuan Song", "title": "Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual\n  Cross Retrieval", "comments": "Accepted by ICCV 2017 as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal hashing is usually regarded as an effective technique for\nlarge-scale textual-visual cross retrieval, where data from different\nmodalities are mapped into a shared Hamming space for matching. Most of the\ntraditional textual-visual binary encoding methods only consider holistic image\nrepresentations and fail to model descriptive sentences. This renders existing\nmethods inappropriate to handle the rich semantics of informative cross-modal\ndata for quality textual-visual search tasks. To address the problem of hashing\ncross-modal data with semantic-rich cues, in this paper, a novel integrated\ndeep architecture is developed to effectively encode the detailed semantics of\ninformative images and long descriptive sentences, named as Textual-Visual Deep\nBinaries (TVDB). In particular, region-based convolutional networks with long\nshort-term memory units are introduced to fully explore image regional details\nwhile semantic cues of sentences are modeled by a text convolutional network.\nAdditionally, we propose a stochastic batch-wise training routine, where\nhigh-quality binary codes and deep encoding functions are efficiently optimized\nin an alternating manner. Experiments are conducted on three multimedia\ndatasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the\nproposed TVDB model significantly outperforms state-of-the-art binary coding\nmethods in the task of cross-modal retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:46:16 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Shen", "Yuming", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""], ["Song", "Jingkuan", ""]]}, {"id": "1708.02550", "submitter": "Bert De Brabandere", "authors": "Davy Neven, Bert De Brabandere, Stamatios Georgoulis, Marc Proesmans,\n  Luc Van Gool", "title": "Fast Scene Understanding for Autonomous Driving", "comments": "Published at \"Deep Learning for Vehicle Perception\", workshop at the\n  IEEE Symposium on Intelligent Vehicles 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches for instance-aware semantic labeling traditionally focus on\naccuracy. Other aspects like runtime and memory footprint are arguably as\nimportant for real-time applications such as autonomous driving. Motivated by\nthis observation and inspired by recent works that tackle multiple tasks with a\nsingle integrated architecture, in this paper we present a real-time efficient\nimplementation based on ENet that solves three autonomous driving related tasks\nat once: semantic scene segmentation, instance segmentation and monocular depth\nestimation. Our approach builds upon a branched ENet architecture with a shared\nencoder but different decoder branches for each of the three tasks. The\npresented method can run at 21 fps at a resolution of 1024x512 on the\nCityscapes dataset without sacrificing accuracy compared to running each task\nseparately.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:31:52 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Neven", "Davy", ""], ["De Brabandere", "Bert", ""], ["Georgoulis", "Stamatios", ""], ["Proesmans", "Marc", ""], ["Van Gool", "Luc", ""]]}, {"id": "1708.02551", "submitter": "Bert De Brabandere", "authors": "Bert De Brabandere, Davy Neven, Luc Van Gool", "title": "Semantic Instance Segmentation with a Discriminative Loss Function", "comments": "Published at \"Deep Learning for Robotic Vision\", workshop at CVPR\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic instance segmentation remains a challenging task. In this work we\npropose to tackle the problem with a discriminative loss function, operating at\nthe pixel level, that encourages a convolutional network to produce a\nrepresentation of the image that can easily be clustered into instances with a\nsimple post-processing step. The loss function encourages the network to map\neach pixel to a point in feature space so that pixels belonging to the same\ninstance lie close together while different instances are separated by a wide\nmargin. Our approach of combining an off-the-shelf network with a principled\nloss function inspired by a metric learning objective is conceptually simple\nand distinct from recent efforts in instance segmentation. In contrast to\nprevious works, our method does not rely on object proposals or recurrent\nmechanisms. A key contribution of our work is to demonstrate that such a simple\nsetup without bells and whistles is effective and can perform on par with more\ncomplex methods. Moreover, we show that it does not suffer from some of the\nlimitations of the popular detect-and-segment approaches. We achieve\ncompetitive performance on the Cityscapes and CVPPP leaf segmentation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:32:48 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["De Brabandere", "Bert", ""], ["Neven", "Davy", ""], ["Van Gool", "Luc", ""]]}, {"id": "1708.02599", "submitter": "Jonathan Zung", "authors": "Jonathan Zung, Ignacio Tartavull, Kisuk Lee, H. Sebastian Seung", "title": "An Error Detection and Correction Framework for Connectomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study error detection and correction tasks that are useful for\n3D reconstruction of neurons from electron microscopic imagery, and for image\nsegmentation more generally. Both tasks take as input the raw image and a\nbinary mask representing a candidate object. For the error detection task, the\ndesired output is a map of split and merge errors in the object. For the error\ncorrection task, the desired output is the true object. We call this object\nmask pruning, because the candidate object mask is assumed to be a superset of\nthe true object. We train multiscale 3D convolutional networks to perform both\ntasks. We find that the error-detecting net can achieve high accuracy. The\naccuracy of the error-correcting net is enhanced if its input object mask is\n\"advice\" (union of erroneous objects) from the error-detecting net.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 18:26:12 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 21:46:45 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Zung", "Jonathan", ""], ["Tartavull", "Ignacio", ""], ["Lee", "Kisuk", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1708.02660", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan, Sami Alsheikh, Spandan\n  Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, Aaron Hertzmann", "title": "Learning Visual Importance for Graphic Designs and Data Visualizations", "comments": null, "journal-ref": "UIST 2017", "doi": "10.1145/3126594.3126653", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing where people look and click on visual designs can provide clues about\nhow the designs are perceived, and where the most important or relevant content\nlies. The most important content of a visual design can be used for effective\nsummarization or to facilitate retrieval from a database. We present automated\nmodels that predict the relative importance of different elements in data\nvisualizations and graphic designs. Our models are neural networks trained on\nhuman clicks and importance annotations on hundreds of designs. We collected a\nnew dataset of crowdsourced importance, and analyzed the predictions of our\nmodels with respect to ground truth importance and human eye movements. We\ndemonstrate how such predictions of importance can be used for automatic design\nretargeting and thumbnailing. User studies with hundreds of MTurk participants\nvalidate that, with limited post-processing, our importance-driven applications\nare on par with, or outperform, current state-of-the-art methods, including\nnatural image saliency. We also provide a demonstration of how our importance\npredictions can be built into interactive design tools to offer immediate\nfeedback during the design process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 21:50:51 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Kim", "Nam Wook", ""], ["O'Donovan", "Peter", ""], ["Alsheikh", "Sami", ""], ["Madan", "Spandan", ""], ["Pfister", "Hanspeter", ""], ["Durand", "Fredo", ""], ["Russell", "Bryan", ""], ["Hertzmann", "Aaron", ""]]}, {"id": "1708.02668", "submitter": "Charles Herrmann", "authors": "Chen Wang, Charles Herrmann, Ramin Zabih", "title": "A discriminative view of MRF pre-processing algorithms", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Markov Random Fields (MRFs) are widely used in computer vision, they\npresent a quite challenging inference problem. MRF inference can be accelerated\nby pre-processing techniques like Dead End Elimination (DEE) or QPBO-based\napproaches which compute the optimal labeling of a subset of variables. These\ntechniques are guaranteed to never wrongly label a variable but they often\nleave a large number of variables unlabeled. We address this shortcoming by\ninterpreting pre-processing as a classification problem, which allows us to\ntrade off false positives (i.e., giving a variable an incorrect label) versus\nfalse negatives (i.e., failing to label a variable). We describe an efficient\ndiscriminative rule that finds optimal solutions for a subset of variables. Our\ntechnique provides both per-instance and worst-case guarantees concerning the\nquality of the solution. Empirical studies were conducted over several\nbenchmark datasets. We obtain a speedup factor of 2 to 12 over expansion moves\nwithout preprocessing, and on difficult non-submodular energy functions produce\nslightly lower energy.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 22:41:43 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Wang", "Chen", ""], ["Herrmann", "Charles", ""], ["Zabih", "Ramin", ""]]}, {"id": "1708.02681", "submitter": "He Zhang", "authors": "He Zhang, Vishal M. Patel, Benjamin S. Riggan, and Shuowen Hu", "title": "Generative Adversarial Network-based Synthesis of Visible Faces from\n  Polarimetric Thermal Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large domain discrepancy between faces captured in polarimetric (or\nconventional) thermal and visible domain makes cross-domain face recognition\nquite a challenging problem for both human-examiners and computer vision\nalgorithms. Previous approaches utilize a two-step procedure (visible feature\nestimation and visible image reconstruction) to synthesize the visible image\ngiven the corresponding polarimetric thermal image. However, these are regarded\nas two disjoint steps and hence may hinder the performance of visible face\nreconstruction. We argue that joint optimization would be a better way to\nreconstruct more photo-realistic images for both computer vision algorithms and\nhuman-examiners to examine. To this end, this paper proposes a Generative\nAdversarial Network-based Visible Face Synthesis (GAN-VFS) method to synthesize\nmore photo-realistic visible face images from their corresponding polarimetric\nimages. To ensure that the encoded visible-features contain more semantically\nmeaningful information in reconstructing the visible face image, a guidance\nsub-network is involved into the training procedure. To achieve photo realistic\nproperty while preserving discriminative characteristics for the reconstructed\noutputs, an identity loss combined with the perceptual loss are optimized in\nthe framework. Multiple experiments evaluated on different experimental\nprotocols demonstrate that the proposed method achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 23:57:12 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Zhang", "He", ""], ["Patel", "Vishal M.", ""], ["Riggan", "Benjamin S.", ""], ["Hu", "Shuowen", ""]]}, {"id": "1708.02688", "submitter": "Yu Zeng", "authors": "Yu Zeng, Huchuan Lu, Ali Borji", "title": "Statistics of Deep Generated Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we explore the low-level statistics of images generated by\nstate-of-the-art deep generative models. First, Variational auto-encoder\n(VAE~\\cite{kingma2013auto}), Wasserstein generative adversarial network\n(WGAN~\\cite{arjovsky2017wasserstein}) and deep convolutional generative\nadversarial network (DCGAN~\\cite{radford2015unsupervised}) are trained on the\nImageNet dataset and a large set of cartoon frames from animations. Then, for\nimages generated by these models as well as natural scenes and cartoons,\nstatistics including mean power spectrum, the number of connected components in\na given image area, distribution of random filter responses, and contrast\ndistribution are computed. Our analyses on training images support current\nfindings on scale invariance, non-Gaussianity, and Weibull contrast\ndistribution of natural scenes. We find that although similar results hold over\ncartoon images, there is still a significant difference between statistics of\nnatural scenes and images generated by VAE, DCGAN and WGAN models. In\nparticular, generated images do not have scale invariant mean power spectrum\nmagnitude, which indicates existence of extra structures in these images.\nInspecting how well the statistics of deep generated images match the known\nstatistical properties of natural images, such as scale invariance,\nnon-Gaussianity, and Weibull contrast distribution, can a) reveal the degree to\nwhich deep learning models capture the essence of the natural scenes, b)\nprovide a new dimension to evaluate models, and c) allow possible improvement\nof image generative models (e.g., via defining new loss functions).\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 01:12:20 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 06:53:14 GMT"}, {"version": "v3", "created": "Sat, 31 Mar 2018 13:44:35 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 06:02:53 GMT"}, {"version": "v5", "created": "Sun, 24 Nov 2019 02:59:10 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Zeng", "Yu", ""], ["Lu", "Huchuan", ""], ["Borji", "Ali", ""]]}, {"id": "1708.02694", "submitter": "Chaitanya Prakash Bapat", "authors": "S. Kolkur, D. Kalbande, P. Shimpi, C. Bapat, and J. Jatakia", "title": "Human Skin Detection Using RGB, HSV and YCbCr Color Models", "comments": "ICCASP/ICMMD-2016. Published by Atlantic Press. Part of series: AISR\n  ISBN: 978-94-6252-305-0 ISSN: 1951-6851", "journal-ref": null, "doi": "10.2991/iccasp-16.2017.51", "report-no": null, "categories": "cs.CV q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Skin detection deals with the recognition of skin-colored pixels and\nregions in a given image. Skin color is often used in human skin detection\nbecause it is invariant to orientation and size and is fast to process. A new\nhuman skin detection algorithm is proposed in this paper. The three main\nparameters for recognizing a skin pixel are RGB (Red, Green, Blue), HSV (Hue,\nSaturation, Value) and YCbCr (Luminance, Chrominance) color models. The\nobjective of proposed algorithm is to improve the recognition of skin pixels in\ngiven images. The algorithm not only considers individual ranges of the three\ncolor parameters but also takes into ac- count combinational ranges which\nprovide greater accuracy in recognizing the skin area in a given image.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 02:08:29 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Kolkur", "S.", ""], ["Kalbande", "D.", ""], ["Shimpi", "P.", ""], ["Bapat", "C.", ""], ["Jatakia", "J.", ""]]}, {"id": "1708.02696", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Olga Russakovsky and Abhinav Gupta", "title": "What Actions are Needed for Understanding Human Actions in Videos?", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the right way to reason about human activities? What directions\nforward are most promising? In this work, we analyze the current state of human\nactivity understanding in videos. The goal of this paper is to examine\ndatasets, evaluation metrics, algorithms, and potential future directions. We\nlook at the qualitative attributes that define activities such as pose\nvariability, brevity, and density. The experiments consider multiple\nstate-of-the-art algorithms and multiple datasets. The results demonstrate that\nwhile there is inherent ambiguity in the temporal extent of activities, current\ndatasets still permit effective benchmarking. We discover that fine-grained\nunderstanding of objects and pose when combined with temporal reasoning is\nlikely to yield substantial improvements in algorithmic accuracy. We present\nthe many kinds of information that will be needed to achieve substantial gains\nin activity understanding: objects, verbs, intent, and sequential reasoning.\nThe software and additional information will be made available to provide other\nresearchers detailed diagnostics to understand their own algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 02:25:56 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Russakovsky", "Olga", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1708.02711", "submitter": "Damien Teney", "authors": "Damien Teney, Peter Anderson, Xiaodong He, Anton van den Hengel", "title": "Tips and Tricks for Visual Question Answering: Learnings from the 2017\n  Challenge", "comments": "Winner of the 2017 Visual Question Answering (VQA) Challenge at CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a state-of-the-art model for visual question answering\n(VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of\nsignificant importance for research in artificial intelligence, given its\nmultimodal nature, clear evaluation protocol, and potential real-world\napplications. The performance of deep neural networks for VQA is very dependent\non choices of architectures and hyperparameters. To help further research in\nthe area, we describe in detail our high-performing, though relatively simple\nmodel. Through a massive exploration of architectures and hyperparameters\nrepresenting more than 3,000 GPU-hours, we identified tips and tricks that lead\nto its success, namely: sigmoid outputs, soft training targets, image features\nfrom bottom-up attention, gated tanh activations, output embeddings initialized\nusing GloVe and Google Images, large mini-batches, and smart shuffling of\ntraining data. We provide a detailed analysis of their impact on performance to\nassist others in making an appropriate selection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 04:19:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Teney", "Damien", ""], ["Anderson", "Peter", ""], ["He", "Xiaodong", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1708.02716", "submitter": "Meiyu Yu", "authors": "Qi Jia, Meiyu Yu, Xin Fan and Haojie Li", "title": "Sequential Dual Deep Learning with Shape and Texture Features for Sketch\n  Recognition", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing freehand sketches with high arbitrariness is greatly challenging.\nMost existing methods either ignore the geometric characteristics or treat\nsketches as handwritten characters with fixed structural ordering.\nConsequently, they can hardly yield high recognition performance even though\nsophisticated learning techniques are employed. In this paper, we propose a\nsequential deep learning strategy that combines both shape and texture\nfeatures. A coded shape descriptor is exploited to characterize the geometry of\nsketch strokes with high flexibility, while the outputs of constitutional\nneural networks (CNN) are taken as the abstract texture feature. We develop\ndual deep networks with memorable gated recurrent units (GRUs), and\nsequentially feed these two types of features into the dual networks,\nrespectively. These dual networks enable the feature fusion by another gated\nrecurrent unit (GRU), and thus accurately recognize sketches invariant to\nstroke ordering. The experiments on the TU-Berlin data set show that our method\noutperforms the average of human and state-of-the-art algorithms even when\nsignificant shape and appearance variations occur.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 04:42:25 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Jia", "Qi", ""], ["Yu", "Meiyu", ""], ["Fan", "Xin", ""], ["Li", "Haojie", ""]]}, {"id": "1708.02721", "submitter": "Boyi Jiang", "authors": "Boyi Jiang, Juyong Zhang, Bailin Deng, Yudong Guo and Ligang Liu", "title": "Deep Face Feature for Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning based image feature extraction\nmethod designed specifically for face images. To train the feature extraction\nmodel, we construct a large scale photo-realistic face image dataset with\nground-truth correspondence between multi-view face images, which are\nsynthesized from real photographs via an inverse rendering procedure. The deep\nface feature (DFF) is trained using correspondence between face images rendered\nfrom different views. Using the trained DFF model, we can extract a feature\nvector for each pixel of a face image, which distinguishes different facial\nregions and is shown to be more effective than general-purpose feature\ndescriptors for face-related tasks such as matching and alignment. Based on the\nDFF, we develop a robust face alignment method, which iteratively updates\nlandmarks, pose and 3D shape. Extensive experiments demonstrate that our method\ncan achieve state-of-the-art results for face alignment under highly\nunconstrained face images.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 05:39:36 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 12:30:36 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Jiang", "Boyi", ""], ["Zhang", "Juyong", ""], ["Deng", "Bailin", ""], ["Guo", "Yudong", ""], ["Liu", "Ligang", ""]]}, {"id": "1708.02731", "submitter": "Donghyeon Cho", "authors": "Donghyeon Cho, Jinsun Park, Tae-Hyun Oh, Yu-Wing Tai, In So Kweon", "title": "Weakly- and Self-Supervised Learning for Content-Aware Deep Image\n  Retargeting", "comments": "10 pages, 11 figures. To appear in ICCV 2017, Spotlight Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a weakly- and self-supervised deep convolutional neural\nnetwork (WSSDCNN) for content-aware image retargeting. Our network takes a\nsource image and a target aspect ratio, and then directly outputs a retargeted\nimage. Retargeting is performed through a shift map, which is a pixel-wise\nmapping from the source to the target grid. Our method implicitly learns an\nattention map, which leads to a content-aware shift map for image retargeting.\nAs a result, discriminative parts in an image are preserved, while background\nregions are adjusted seamlessly. In the training phase, pairs of an image and\nits image-level annotation are used to compute content and structure losses. We\ndemonstrate the effectiveness of our proposed method for a retargeting\napplication with insightful analyses.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:43:51 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Cho", "Donghyeon", ""], ["Park", "Jinsun", ""], ["Oh", "Tae-Hyun", ""], ["Tai", "Yu-Wing", ""], ["Kweon", "In So", ""]]}, {"id": "1708.02733", "submitter": "Andrey Savchenko", "authors": "Andrey Savchenko", "title": "Probabilistic Neural Network with Complex Exponential Activation\n  Functions in Image Recognition using Deep Learning Framework", "comments": "14 pages, 13 figures, 5 tables, 69 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If the training dataset is not very large, image recognition is usually\nimplemented with the transfer learning methods. In these methods the features\nare extracted using a deep convolutional neural network, which was\npreliminarily trained with an external very-large dataset. In this paper we\nconsider the nonparametric classification of extracted feature vectors with the\nprobabilistic neural network (PNN). The number of neurons at the pattern layer\nof the PNN is equal to the database size, which causes the low recognition\nperformance and high memory space complexity of this network. We propose to\novercome these drawbacks by replacing the exponential activation function in\nthe Gaussian Parzen kernel to the complex exponential functions in the Fej\\'er\nkernel. We demonstrate that in this case it is possible to implement the\nnetwork with the number of neurons in the pattern layer proportional to the\ncubic root of the database size. Thus, the proposed modification of the PNN\nmakes it possible to significantly decrease runtime and memory complexities\nwithout loosing its main advantages, namely, extremely fast training procedure\nand the convergence to the optimal Bayesian decision. An experimental study in\nvisual object category classification and unconstrained face recognition with\ncontemporary deep neural networks have shown, that our approach obtains very\nefficient and rather accurate decisions for the small training sample in\ncomparison with the well-known classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:50:32 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Savchenko", "Andrey", ""]]}, {"id": "1708.02734", "submitter": "Qijun Zhao", "authors": "Feng Liu, Qijun Zhao, Xiaoming Liu and Dan Zeng", "title": "Joint Face Alignment and 3D Face Reconstruction with Application to Face\n  Recognition", "comments": "IEEE Transactions on Pattern Analysis and Machine Intelligence, Nov.\n  2018", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2885995", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment and 3D face reconstruction are traditionally accomplished as\nseparated tasks. By exploring the strong correlation between 2D landmarks and\n3D shapes, in contrast, we propose a joint face alignment and 3D face\nreconstruction method to simultaneously solve these two problems for 2D face\nimages of arbitrary poses and expressions. This method, based on a summation\nmodel of 3D faces and cascaded regression in 2D and 3D shape spaces,\niteratively and alternately applies two cascaded regressors, one for updating\n2D landmarks and the other for 3D shape. The 3D shape and the landmarks are\ncorrelated via a 3D-to-2D mapping matrix, which is updated in each iteration to\nrefine the location and visibility of 2D landmarks. Unlike existing methods,\nthe proposed method can fully automatically generate both\npose-and-expression-normalized (PEN) and expressive 3D faces and localize both\nvisible and invisible 2D landmarks. Based on the PEN 3D faces, we devise a\nmethod to enhance face recognition accuracy across poses and expressions. Both\nlinear and nonlinear implementations of the proposed method are presented and\nevaluated in this paper. Extensive experiments show that the proposed method\ncan achieve the state-of-the-art accuracy in both face alignment and 3D face\nreconstruction, and benefit face recognition owing to its reconstructed PEN 3D\nface.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:52:26 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 02:32:28 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Liu", "Feng", ""], ["Zhao", "Qijun", ""], ["Liu", "Xiaoming", ""], ["Zeng", "Dan", ""]]}, {"id": "1708.02735", "submitter": "Stanislav Fort", "authors": "Stanislav Fort", "title": "Gaussian Prototypical Networks for Few-Shot Learning on Omniglot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel architecture for $k$-shot classification on the Omniglot\ndataset. Building on prototypical networks, we extend their architecture to\nwhat we call Gaussian prototypical networks. Prototypical networks learn a map\nbetween images and embedding vectors, and use their clustering for\nclassification. In our model, a part of the encoder output is interpreted as a\nconfidence region estimate about the embedding point, and expressed as a\nGaussian covariance matrix. Our network then constructs a direction and class\ndependent distance metric on the embedding space, using uncertainties of\nindividual data points as weights. We show that Gaussian prototypical networks\nare a preferred architecture over vanilla prototypical networks with an\nequivalent number of parameters. We report state-of-the-art performance in\n1-shot and 5-shot classification both in 5-way and 20-way regime (for 5-shot\n5-way, we are comparable to previous state-of-the-art) on the Omniglot dataset.\nWe explore artificially down-sampling a fraction of images in the training set,\nwhich improves our performance even further. We therefore hypothesize that\nGaussian prototypical networks might perform better in less homogeneous,\nnoisier datasets, which are commonplace in real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 06:53:31 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Fort", "Stanislav", ""]]}, {"id": "1708.02747", "submitter": "Na Li", "authors": "Na Li (CSTJF, DRUID), Arnaud Martin (DRUID), R\\'emi Estival (CSTJF)", "title": "An automatic water detection approach based on Dempster-Shafer theory\n  for multi spectral images", "comments": "20th International Conference on Information Fusion, Jul 2017, XI'AN,\n  China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of surface water in natural environment via multi-spectral imagery\nhas been widely utilized in many fields, such land cover identification.\nHowever, due to the similarity of the spectra of water bodies, built-up areas,\napproaches based on high-resolution satellites sometimes confuse these\nfeatures. A popular direction to detect water is spectral index, often\nrequiring the ground truth to find appropriate thresholds manually. As for\ntraditional machine learning methods, they identify water merely via\ndifferences of spectra of various land covers, without taking specific\nproperties of spectral reflection into account. In this paper, we propose an\nautomatic approach to detect water bodies based on Dempster-Shafer theory,\ncombining supervised learning with specific property of water in spectral band\nin a fully unsupervised context. The benefits of our approach are twofold. On\nthe one hand, it performs well in mapping principle water bodies, including\nlittle streams and branches. On the other hand, it labels all objects usually\nconfused with water as `ignorance', including half-dry watery areas, built-up\nareas and semi-transparent clouds and shadows. `Ignorance' indicates not only\nlimitations of the spectral properties of water and supervised learning itself\nbut insufficiency of information from multi-spectral bands as well, providing\nvaluable information for further land cover classification.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 07:59:39 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 08:04:27 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Na", "", "CSTJF, DRUID"], ["Martin", "Arnaud", "", "DRUID"], ["Estival", "R\u00e9mi", "", "CSTJF"]]}, {"id": "1708.02750", "submitter": "Dim Papadopoulos P", "authors": "Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller, Vittorio\n  Ferrari", "title": "Extreme clicking for efficient object annotation", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually annotating object bounding boxes is central to building computer\nvision datasets, and it is very time consuming (annotating ILSVRC [53] took 35s\nfor one high-quality box [62]). It involves clicking on imaginary corners of a\ntight box around the object. This is difficult as these corners are often\noutside the actual object and several adjustments are required to obtain a\ntight box. We propose extreme clicking instead: we ask the annotator to click\non four physical points on the object: the top, bottom, left- and right-most\npoints. This task is more natural and these points are easy to find. We\ncrowd-source extreme point annotations for PASCAL VOC 2007 and 2012 and show\nthat (1) annotation time is only 7s per box, 5x faster than the traditional way\nof drawing boxes [62]; (2) the quality of the boxes is as good as the original\nground-truth drawn the traditional way; (3) detectors trained on our\nannotations are as accurate as those trained on the original ground-truth.\nMoreover, our extreme clicking strategy not only yields box coordinates, but\nalso four accurate boundary points. We show (4) how to incorporate them into\nGrabCut to obtain more accurate segmentations than those delivered when\ninitializing it from bounding boxes; (5) semantic segmentations models trained\non these segmentations outperform those trained on segmentations derived from\nbounding boxes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 08:05:53 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Papadopoulos", "Dim P.", ""], ["Uijlings", "Jasper R. R.", ""], ["Keller", "Frank", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1708.02757", "submitter": "Pim Moeskops", "authors": "Pim Moeskops and Josien P.W. Pluim", "title": "Isointense infant brain MRI segmentation with a dilated convolutional\n  neural network", "comments": "MICCAI grand challenge on 6-month infant brain MRI segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative analysis of brain MRI at the age of 6 months is difficult\nbecause of the limited contrast between white matter and gray matter. In this\nstudy, we use a dilated triplanar convolutional neural network in combination\nwith a non-dilated 3D convolutional neural network for the segmentation of\nwhite matter, gray matter and cerebrospinal fluid in infant brain MR images, as\nprovided by the MICCAI grand challenge on 6-month infant brain MRI\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 08:49:12 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Moeskops", "Pim", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1708.02760", "submitter": "Yining Li", "authors": "Yining Li, Chen Huang, Xiaoou Tang, Chen-Change Loy", "title": "Learning to Disambiguate by Asking Discriminative Questions", "comments": "14 pages, 12 figures, ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to ask questions is a powerful tool to gather information in\norder to learn about the world and resolve ambiguities. In this paper, we\nexplore a novel problem of generating discriminative questions to help\ndisambiguate visual instances. Our work can be seen as a complement and new\nextension to the rich research studies on image captioning and question\nanswering. We introduce the first large-scale dataset with over 10,000\ncarefully annotated images-question tuples to facilitate benchmarking. In\nparticular, each tuple consists of a pair of images and 4.6 discriminative\nquestions (as positive samples) and 5.9 non-discriminative questions (as\nnegative samples) on average. In addition, we present an effective method for\nvisual discriminative question generation. The method can be trained in a\nweakly supervised manner without discriminative images-question tuples but just\nexisting visual question answering datasets. Promising results are shown\nagainst representative baselines through quantitative evaluations and user\nstudies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 08:52:25 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Li", "Yining", ""], ["Huang", "Chen", ""], ["Tang", "Xiaoou", ""], ["Loy", "Chen-Change", ""]]}, {"id": "1708.02766", "submitter": "Simon Andermatt", "authors": "Simon Andermatt, Simon Pezold, Michael Amann, Philippe C. Cattin", "title": "Multi-dimensional Gated Recurrent Units for Automated Anatomical\n  Landmark Localization", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an automated method for localizing an anatomical landmark in\nthree-dimensional medical images. The method combines two recurrent neural\nnetworks in a coarse-to-fine approach: The first network determines a candidate\nneighborhood by analyzing the complete given image volume. The second network\nlocalizes the actual landmark precisely and accurately in the candidate\nneighborhood. Both networks take advantage of multi-dimensional gated recurrent\nunits in their main layers, which allow for high model complexity with a\ncomparatively small set of parameters. We localize the medullopontine sulcus in\n3D magnetic resonance images of the head and neck. We show that the proposed\napproach outperforms similar localization techniques both in terms of mean\ndistance in millimeters and voxels w.r.t. manual labelings of the data. With a\nmean localization error of 1.7 mm, the proposed approach performs on par with\nneurological experts, as we demonstrate in an interrater comparison.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 09:10:02 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Andermatt", "Simon", ""], ["Pezold", "Simon", ""], ["Amann", "Michael", ""], ["Cattin", "Philippe C.", ""]]}, {"id": "1708.02813", "submitter": "Nikita Dvornik", "authors": "Nikita Dvornik, Konstantin Shmelkov, Julien Mairal, Cordelia Schmid", "title": "BlitzNet: A Real-Time Deep Network for Scene Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time scene understanding has become crucial in many applications such as\nautonomous driving. In this paper, we propose a deep architecture, called\nBlitzNet, that jointly performs object detection and semantic segmentation in\none forward pass, allowing real-time computations. Besides the computational\ngain of having a single network to perform several tasks, we show that object\ndetection and semantic segmentation benefit from each other in terms of\naccuracy. Experimental results for VOC and COCO datasets show state-of-the-art\nperformance for object detection and segmentation among real time systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:36:17 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Dvornik", "Nikita", ""], ["Shmelkov", "Konstantin", ""], ["Mairal", "Julien", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1708.02831", "submitter": "Soumyadeep Dey", "authors": "Soumyadeep Dey, Jayanta Mukherjee, Shamik Sural, Amit Vijay Nandedkar", "title": "Anveshak - A Groundtruth Generation Tool for Foreground Regions of\n  Document Images", "comments": "Accepted in DAR 2016", "journal-ref": null, "doi": "10.1007/978-3-319-68124-5_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a graphical user interface based groundtruth generation tool in\nthis paper. Here, annotation of an input document image is done based on the\nforeground pixels. Foreground pixels are grouped together with user interaction\nto form labeling units. These units are then labeled by the user with the user\ndefined labels. The output produced by the tool is an image with an XML file\ncontaining its metadata information. This annotated data can be further used in\ndifferent applications of document image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:41:18 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Dey", "Soumyadeep", ""], ["Mukherjee", "Jayanta", ""], ["Sural", "Shamik", ""], ["Nandedkar", "Amit Vijay", ""]]}, {"id": "1708.02837", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F. Proen\\c{c}a and Yang Gao", "title": "SPLODE: Semi-Probabilistic Point and Line Odometry with Depth Estimation\n  from RGB-D Camera Motion", "comments": "IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active depth cameras suffer from several limitations, which cause incomplete\nand noisy depth maps, and may consequently affect the performance of RGB-D\nOdometry. To address this issue, this paper presents a visual odometry method\nbased on point and line features that leverages both measurements from a depth\nsensor and depth estimates from camera motion. Depth estimates are generated\ncontinuously by a probabilistic depth estimation framework for both types of\nfeatures to compensate for the lack of depth measurements and inaccurate\nfeature depth associations. The framework models explicitly the uncertainty of\ntriangulating depth from both point and line observations to validate and\nobtain precise estimates. Furthermore, depth measurements are exploited by\npropagating them through a depth map registration module and using a\nframe-to-frame motion estimation method that considers 3D-to-2D and 2D-to-3D\nreprojection errors, independently. Results on RGB-D sequences captured on\nlarge indoor and outdoor scenes, where depth sensor limitations are critical,\nshow that the combination of depth measurements and estimates through our\napproach is able to overcome the absence and inaccuracy of depth measurements.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:50:30 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Proen\u00e7a", "Pedro F.", ""], ["Gao", "Yang", ""]]}, {"id": "1708.02843", "submitter": "Qi Chu", "authors": "Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, Nenghai Yu", "title": "Online Multi-Object Tracking Using CNN-based Single Object Tracker with\n  Spatial-Temporal Attention Mechanism", "comments": "Accepted at International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a CNN-based framework for online MOT. This\nframework utilizes the merits of single object trackers in adapting appearance\nmodels and searching for target in the next frame. Simply applying single\nobject tracker for MOT will encounter the problem in computational efficiency\nand drifted results caused by occlusion. Our framework achieves computational\nefficiency by sharing features and using ROI-Pooling to obtain individual\nfeatures for each target. Some online learned target-specific CNN layers are\nused for adapting the appearance model for each target. In the framework, we\nintroduce spatial-temporal attention mechanism (STAM) to handle the drift\ncaused by occlusion and interaction among targets. The visibility map of the\ntarget is learned and used for inferring the spatial attention map. The spatial\nattention map is then applied to weight the features. Besides, the occlusion\nstatus can be estimated from the visibility map, which controls the online\nupdating process via weighted loss on training samples with different occlusion\nstatuses in different frames. It can be considered as temporal attention\nmechanism. The proposed algorithm achieves 34.3% and 46.0% in MOTA on\nchallenging MOT15 and MOT16 benchmark dataset respectively.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:56:13 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 03:30:17 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chu", "Qi", ""], ["Ouyang", "Wanli", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""], ["Liu", "Bin", ""], ["Yu", "Nenghai", ""]]}, {"id": "1708.02862", "submitter": "Wen Li", "authors": "Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, Luc Van Gool", "title": "WebVision Database: Visual Learning and Understanding from Web Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a study on learning visual recognition models from\nlarge scale noisy web data. We build a new database called WebVision, which\ncontains more than $2.4$ million web images crawled from the Internet by using\nqueries generated from the 1,000 semantic concepts of the benchmark ILSVRC 2012\ndataset. Meta information along with those web images (e.g., title,\ndescription, tags, etc.) are also crawled. A validation set and test set\ncontaining human annotated images are also provided to facilitate algorithmic\ndevelopment. Based on our new database, we obtain a few interesting\nobservations: 1) the noisy web images are sufficient for training a good deep\nCNN model for visual recognition; 2) the model learnt from our WebVision\ndatabase exhibits comparable or even better generalization ability than the one\ntrained from the ILSVRC 2012 dataset when being transferred to new datasets and\ntasks; 3) a domain adaptation issue (a.k.a., dataset bias) is observed, which\nmeans the dataset can be used as the largest benchmark dataset for visual\ndomain adaptation. Our new WebVision database and relevant studies in this work\nwould benefit the advance of learning state-of-the-art visual models with\nminimum supervision based on web data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 14:59:30 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Li", "Wen", ""], ["Wang", "Limin", ""], ["Li", "Wei", ""], ["Agustsson", "Eirikur", ""], ["Van Gool", "Luc", ""]]}, {"id": "1708.02863", "submitter": "Yousong Zhu", "authors": "Yousong Zhu, Chaoyang Zhao, Jinqiao Wang, Xu Zhao, Yi Wu, Hanqing Lu", "title": "CoupleNet: Coupling Global Structure with Local Parts for Object\n  Detection", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The region-based Convolutional Neural Network (CNN) detectors such as Faster\nR-CNN or R-FCN have already shown promising results for object detection by\ncombining the region proposal subnetwork and the classification subnetwork\ntogether. Although R-FCN has achieved higher detection speed while keeping the\ndetection performance, the global structure information is ignored by the\nposition-sensitive score maps. To fully explore the local and global\nproperties, in this paper, we propose a novel fully convolutional network,\nnamed as CoupleNet, to couple the global structure with local parts for object\ndetection. Specifically, the object proposals obtained by the Region Proposal\nNetwork (RPN) are fed into the the coupling module which consists of two\nbranches. One branch adopts the position-sensitive RoI (PSRoI) pooling to\ncapture the local part information of the object, while the other employs the\nRoI pooling to encode the global and context information. Next, we design\ndifferent coupling strategies and normalization ways to make full use of the\ncomplementary advantages between the global and local branches. Extensive\nexperiments demonstrate the effectiveness of our approach. We achieve\nstate-of-the-art results on all three challenging datasets, i.e. a mAP of 82.7%\non VOC07, 80.4% on VOC12, and 34.4% on COCO. Codes will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 15:07:23 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Zhu", "Yousong", ""], ["Zhao", "Chaoyang", ""], ["Wang", "Jinqiao", ""], ["Zhao", "Xu", ""], ["Wu", "Yi", ""], ["Lu", "Hanqing", ""]]}, {"id": "1708.02872", "submitter": "Xin Jin", "authors": "Xin Jin, Shiming Ge, Chenggen Song", "title": "Privacy Preserving Face Retrieval in the Cloud for Mobile Users", "comments": "Abuse Preventive Data Mining (APDM2017, IJCAI Workshop), 19-25\n  August, 2017 Melbourne, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, cloud storage and processing have been widely adopted. Mobile users\nin one family or one team may automatically backup their photos to the same\nshared cloud storage space. The powerful face detector trained and provided by\na 3rd party may be used to retrieve the photo collection which contains a\nspecific group of persons from the cloud storage server. However, the privacy\nof the mobile users may be leaked to the cloud server providers. In the\nmeanwhile, the copyright of the face detector should be protected. Thus, in\nthis paper, we propose a protocol of privacy preserving face retrieval in the\ncloud for mobile users, which protects the user photos and the face detector\nsimultaneously. The cloud server only provides the resources of storage and\ncomputing and can not learn anything of the user photos and the face detector.\nWe test our protocol inside several families and classes. The experimental\nresults reveal that our protocol can successfully retrieve the proper photos\nfrom the cloud server and protect the user photos and the face detector.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 15:21:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Jin", "Xin", ""], ["Ge", "Shiming", ""], ["Song", "Chenggen", ""]]}, {"id": "1708.02895", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li", "title": "Interacting with Acoustic Simulation and Fabrication", "comments": "ACM UIST 2017 Doctoral Symposium", "journal-ref": null, "doi": "10.1145/3131785.3131842", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating accurate physics-based simulation into interactive design tools\nis challenging. However, adding the physics accurately becomes crucial to\nseveral emerging technologies. For example, in virtual/augmented reality\n(VR/AR) videos, the faithful reproduction of surrounding audios is required to\nbring the immersion to the next level. Similarly, as personal fabrication is\nmade possible with accessible 3D printers, more intuitive tools that respect\nthe physical constraints can help artists to prototype designs. One main hurdle\nis the sheer amount of computation complexity to accurately reproduce the\nreal-world phenomena through physics-based simulation. In my thesis research, I\ndevelop interactive tools that implement efficient physics-based simulation\nalgorithms for automatic optimization and intuitive user interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:20:12 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:14:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Dingzeyu", ""]]}, {"id": "1708.02898", "submitter": "Matthijs Douze", "authors": "Matthijs Douze, Herv\\'e J\\'egou and Jeff Johnson", "title": "An evaluation of large-scale methods for image instance and class\n  discovery", "comments": "Published at ACM Multimedia workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at discovering meaningful subsets of related images from\nlarge image collections without annotations. We search groups of images related\nat different levels of semantic, i.e., either instances or visual classes.\nWhile k-means is usually considered as the gold standard for this task, we\nevaluate and show the interest of diffusion methods that have been neglected by\nthe state of the art, such as the Markov Clustering algorithm.\n  We report results on the ImageNet and the Paris500k instance dataset, both\nenlarged with images from YFCC100M. We evaluate our methods with a labelling\ncost that reflects how much effort a human would require to correct the\ngenerated clusters.\n  Our analysis highlights several properties. First, when powered with an\nefficient GPU implementation, the cost of the discovery process is small\ncompared to computing the image descriptors, even for collections as large as\n100 million images. Second, we show that descriptions selected for instance\nsearch improve the discovery of object classes. Third, the Markov Clustering\ntechnique consistently outperforms other methods; to our knowledge it has never\nbeen considered in this large scale scenario.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:29:21 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Johnson", "Jeff", ""]]}, {"id": "1708.02901", "submitter": "Xiaolong Wang", "authors": "Xiaolong Wang, Kaiming He, Abhinav Gupta", "title": "Transitive Invariance for Self-supervised Visual Representation Learning", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual representations with self-supervised learning has become\npopular in computer vision. The idea is to design auxiliary tasks where labels\nare free to obtain. Most of these tasks end up providing data to learn specific\nkinds of invariance useful for recognition. In this paper, we propose to\nexploit different self-supervised approaches to learn representations invariant\nto (i) inter-instance variations (two objects in the same class should have\nsimilar features) and (ii) intra-instance variations (viewpoint, pose,\ndeformations, illumination, etc). Instead of combining two approaches with\nmulti-task learning, we argue to organize and reason the data with multiple\nvariations. Specifically, we propose to generate a graph with millions of\nobjects mined from hundreds of thousands of videos. The objects are connected\nby two types of edges which correspond to two types of invariance: \"different\ninstances but a similar viewpoint and category\" and \"different viewpoints of\nthe same instance\". By applying simple transitivity on the graph with these\nedges, we can obtain pairs of images exhibiting richer visual invariance. We\nuse this data to train a Triplet-Siamese network with VGG16 as the base\narchitecture and apply the learned representations to different recognition\ntasks. For object detection, we achieve 63.2% mAP on PASCAL VOC 2007 using Fast\nR-CNN (compare to 67.3% with ImageNet pre-training). For the challenging COCO\ndataset, our method is surprisingly close (23.5%) to the ImageNet-supervised\ncounterpart (24.4%) using the Faster R-CNN framework. We also show that our\nnetwork can perform significantly better than the ImageNet network in the\nsurface normal estimation task.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:32:44 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 17:59:51 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 02:34:50 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Wang", "Xiaolong", ""], ["He", "Kaiming", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1708.02932", "submitter": "Himalaya Jain", "authors": "Himalaya Jain, Joaquin Zepeda, Patrick P\\'erez, R\\'emi Gribonval", "title": "SUBIC: A supervised, structured binary code for image search", "comments": "Accepted at ICCV 2017 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale visual search, highly compressed yet meaningful\nrepresentations of images are essential. Structured vector quantizers based on\nproduct quantization and its variants are usually employed to achieve such\ncompression while minimizing the loss of accuracy. Yet, unlike binary hashing\nschemes, these unsupervised methods have not yet benefited from the\nsupervision, end-to-end learning and novel architectures ushered in by the deep\nlearning revolution. We hence propose herein a novel method to make deep\nconvolutional neural networks produce supervised, compact, structured binary\ncodes for visual search. Our method makes use of a novel block-softmax\nnon-linearity and of batch-based entropy losses that together induce structure\nin the learned encodings. We show that our method outperforms state-of-the-art\ncompact representations based on deep hashing or structured quantization in\nsingle and cross-domain category retrieval, instance retrieval and\nclassification. We make our code and models publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 17:56:27 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Jain", "Himalaya", ""], ["Zepeda", "Joaquin", ""], ["P\u00e9rez", "Patrick", ""], ["Gribonval", "R\u00e9mi", ""]]}, {"id": "1708.02970", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Kyungdon Joo, Neel Joshi, Baoyuan Wang, In So Kweon, Sing\n  Bing Kang", "title": "Personalized Cinemagraphs using Semantic Understanding and Collaborative\n  Learning", "comments": "To appear in ICCV 2017. Total 17 pages including the supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cinemagraphs are a compelling way to convey dynamic aspects of a scene. In\nthese media, dynamic and still elements are juxtaposed to create an artistic\nand narrative experience. Creating a high-quality, aesthetically pleasing\ncinemagraph requires isolating objects in a semantically meaningful way and\nthen selecting good start times and looping periods for those objects to\nminimize visual artifacts (such a tearing). To achieve this, we present a new\ntechnique that uses object recognition and semantic segmentation as part of an\noptimization method to automatically create cinemagraphs from videos that are\nboth visually appealing and semantically meaningful. Given a scene with\nmultiple objects, there are many cinemagraphs one could create. Our method\nevaluates these multiple candidates and presents the best one, as determined by\na model trained to predict human preferences in a collaborative way. We\ndemonstrate the effectiveness of our approach with multiple results and a user\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:03:12 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Joo", "Kyungdon", ""], ["Joshi", "Neel", ""], ["Wang", "Baoyuan", ""], ["Kweon", "In So", ""], ["Kang", "Sing Bing", ""]]}, {"id": "1708.02973", "submitter": "Chen Huang", "authors": "Chen Huang, Simon Lucey, Deva Ramanan", "title": "Learning Policies for Adaptive Tracking with Deep Feature Cascades", "comments": "ICCV 2017 Spotlight, with Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual object tracking is a fundamental and time-critical vision task. Recent\nyears have seen many shallow tracking methods based on real-time pixel-based\ncorrelation filters, as well as deep methods that have top performance but need\na high-end GPU. In this paper, we learn to improve the speed of deep trackers\nwithout losing accuracy. Our fundamental insight is to take an adaptive\napproach, where easy frames are processed with cheap features (such as pixel\nvalues), while challenging frames are processed with invariant but expensive\ndeep features. We formulate the adaptive tracking problem as a decision-making\nprocess, and learn an agent to decide whether to locate objects with high\nconfidence on an early layer, or continue processing subsequent layers of a\nnetwork. This significantly reduces the feed-forward cost for easy frames with\ndistinct or slow-moving objects. We train the agent offline in a reinforcement\nlearning fashion, and further demonstrate that learning all deep layers (so as\nto provide good features for adaptive tracking) can lead to near real-time\naverage tracking speed of 23 fps on a single CPU while achieving\nstate-of-the-art performance. Perhaps most tellingly, our approach provides a\n100X speedup for almost 50% of the time, indicating the power of an adaptive\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:09:11 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 19:39:45 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Huang", "Chen", ""], ["Lucey", "Simon", ""], ["Ramanan", "Deva", ""]]}, {"id": "1708.02976", "submitter": "Tomasz Trzcinski", "authors": "Michal Komorowski, Tomasz Trzcinski", "title": "Random Binary Trees for Approximate Nearest Neighbour Search in Binary\n  Space", "comments": "The final publication is available at Springer via\n  https://doi.org/10.1007/978-3-319-69900-4_60", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate nearest neighbour (ANN) search is one of the most important\nproblems in computer science fields such as data mining or computer vision. In\nthis paper, we focus on ANN for high-dimensional binary vectors and we propose\na simple yet powerful search method that uses Random Binary Search Trees\n(RBST). We apply our method to a dataset of 1.25M binary local feature\ndescriptors obtained from a real-life image-based localisation system provided\nby Google as a part of Project Tango. An extensive evaluation of our method\nagainst the state-of-the-art variations of Locality Sensitive Hashing (LSH),\nnamely Uniform LSH and Multi-probe LSH, shows the superiority of our method in\nterms of retrieval precision with performance boost of over 20%\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:16:01 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 19:38:18 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Komorowski", "Michal", ""], ["Trzcinski", "Tomasz", ""]]}, {"id": "1708.02977", "submitter": "Licheng Yu", "authors": "Licheng Yu and Mohit Bansal and Tamara L. Berg", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "comments": "To appear at EMNLP-2017 (7 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of end-to-end visual storytelling. Given a photo\nalbum, our model first selects the most representative (summary) photos, and\nthen composes a natural language story for the album. For this task, we make\nuse of the Visual Storytelling dataset and a model composed of three\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\nphotos, select representative (summary) photos, and compose the story.\nAutomatic and human evaluations show our model achieves better performance on\nselection, generation, and retrieval than baselines.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:26:47 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Yu", "Licheng", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1708.02982", "submitter": "Joseph DeGol", "authors": "Joseph DeGol and Timothy Bretl and Derek Hoiem", "title": "ChromaTag: A Colored Marker and Fast Detection Algorithm", "comments": "International Conference on Computer Vision (ICCV '17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current fiducial marker detection algorithms rely on marker IDs for false\npositive rejection. Time is wasted on potential detections that will eventually\nbe rejected as false positives. We introduce ChromaTag, a fiducial marker and\ndetection algorithm designed to use opponent colors to limit and quickly reject\ninitial false detections and grayscale for precise localization. Through\nexperiments, we show that ChromaTag is significantly faster than current\nfiducial markers while achieving similar or better detection accuracy. We also\nshow how tag size and viewing direction effect detection accuracy. Our\ncontribution is significant because fiducial markers are often used in\nreal-time applications (e.g. marker assisted robot navigation) where heavy\ncomputation is required by other parts of the system.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:41:51 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["DeGol", "Joseph", ""], ["Bretl", "Timothy", ""], ["Hoiem", "Derek", ""]]}, {"id": "1708.03035", "submitter": "Scott Workman", "authors": "Scott Workman and Menghua Zhai and David J. Crandall and Nathan Jacobs", "title": "A Unified Model for Near and Remote Sensing", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel convolutional neural network architecture for estimating\ngeospatial functions such as population density, land cover, or land use. In\nour approach, we combine overhead and ground-level images in an end-to-end\ntrainable neural network, which uses kernel regression and density estimation\nto convert features extracted from the ground-level images into a dense feature\nmap. The output of this network is a dense estimate of the geospatial function\nin the form of a pixel-level labeling of the overhead image. To evaluate our\napproach, we created a large dataset of overhead and ground-level images from a\nmajor urban area with three sets of labels: land use, building function, and\nbuilding age. We find that our approach is more accurate for all tasks, in some\ncases dramatically so.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 23:55:07 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Workman", "Scott", ""], ["Zhai", "Menghua", ""], ["Crandall", "David J.", ""], ["Jacobs", "Nathan", ""]]}, {"id": "1708.03070", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang, Pingjun Chen, Manish Sapkota, Lin Yang", "title": "TandemNet: Distilling Knowledge from Medical Images Using Diagnostic\n  Reports as Optional Semantic References", "comments": "MICCAI2017 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the semantic knowledge of medical images from\ntheir diagnostic reports to provide an inspirational network training and an\ninterpretable prediction mechanism with our proposed novel multimodal neural\nnetwork, namely TandemNet. Inside TandemNet, a language model is used to\nrepresent report text, which cooperates with the image model in a tandem\nscheme. We propose a novel dual-attention model that facilitates high-level\ninteractions between visual and semantic information and effectively distills\nuseful features for prediction. In the testing stage, TandemNet can make\naccurate image prediction with an optional report text input. It also\ninterprets its prediction by producing attention on the image and text\ninformative feature pieces, and further generating diagnostic report\nparagraphs. Based on a pathological bladder cancer images and their diagnostic\nreports (BCIDR) dataset, sufficient experiments demonstrate that our method\neffectively learns and integrates knowledge from multimodalities and obtains\nsignificantly improved performance than comparing baselines.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 04:12:00 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Zhang", "Zizhao", ""], ["Chen", "Pingjun", ""], ["Sapkota", "Manish", ""], ["Yang", "Lin", ""]]}, {"id": "1708.03088", "submitter": "Raghudeep Gadde", "authors": "Raghudeep Gadde and Varun Jampani and Peter V. Gehler", "title": "Semantic Video CNNs through Representation Warping", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a technique to convert CNN models for semantic\nsegmentation of static images into CNNs for video data. We describe a warping\nmethod that can be used to augment existing architectures with very little\nextra computational cost. This module is called NetWarp and we demonstrate its\nuse for a range of network architectures. The main design principle is to use\noptical flow of adjacent frames for warping internal network representations\nacross time. A key insight of this work is that fast optical flow methods can\nbe combined with many different CNN architectures for improved performance and\nend-to-end training. Experiments validate that the proposed approach incurs\nonly little extra computational cost, while improving performance, when video\nstreams are available. We achieve new state-of-the-art results on the CamVid\nand Cityscapes benchmark datasets and show consistent improvements over\ndifferent baseline networks. Our code and models will be available at\nhttp://segmentation.is.tue.mpg.de\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 06:18:02 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Gadde", "Raghudeep", ""], ["Jampani", "Varun", ""], ["Gehler", "Peter V.", ""]]}, {"id": "1708.03111", "submitter": "Yong Man Ro", "authors": "Hak Gu Kim, Yeoreum Choi, Yong Man Ro", "title": "Modality-bridge Transfer Learning for Medical Image Classification", "comments": "accepted at CISP-BMEI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach of transfer learning-based medical image\nclassification to mitigate insufficient labeled data problem in medical domain.\nInstead of direct transfer learning from source to small number of labeled\ntarget data, we propose a modality-bridge transfer learning which employs the\nbridge database in the same medical imaging acquisition modality as target\ndatabase. By learning the projection function from source to bridge and from\nbridge to target, the domain difference between source (e.g., natural images)\nand target (e.g., X-ray images) can be mitigated. Experimental results show\nthat the proposed method can achieve a high classification performance even for\na small number of labeled target medical images, compared to various transfer\nlearning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 07:57:05 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Kim", "Hak Gu", ""], ["Choi", "Yeoreum", ""], ["Ro", "Yong Man", ""]]}, {"id": "1708.03132", "submitter": "Qingxing Cao", "authors": "Qingxing Cao, Liang Lin, Yukai Shi, Xiaodan Liang, Guanbin Li", "title": "Attention-Aware Face Hallucination via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face hallucination is a domain-specific super-resolution problem with the\ngoal to generate high-resolution (HR) faces from low-resolution (LR) input\nimages. In contrast to existing methods that often learn a single\npatch-to-patch mapping from LR to HR images and are regardless of the\ncontextual interdependency between patches, we propose a novel Attention-aware\nFace Hallucination (Attention-FH) framework which resorts to deep reinforcement\nlearning for sequentially discovering attended patches and then performing the\nfacial part enhancement by fully exploiting the global interdependency of the\nimage. Specifically, in each time step, the recurrent policy network is\nproposed to dynamically specify a new attended region by incorporating what\nhappened in the past. The state (i.e., face hallucination result for the whole\nimage) can thus be exploited and updated by the local enhancement network on\nthe selected region. The Attention-FH approach jointly learns the recurrent\npolicy network and local enhancement network through maximizing the long-term\nreward that reflects the hallucination performance over the whole image.\nTherefore, our proposed Attention-FH is capable of adaptively personalizing an\noptimal searching path for each face image according to its own characteristic.\nExtensive experiments show our approach significantly surpasses the\nstate-of-the-arts on in-the-wild faces with large pose and illumination\nvariations.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:12:03 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cao", "Qingxing", ""], ["Lin", "Liang", ""], ["Shi", "Yukai", ""], ["Liang", "Xiaodan", ""], ["Li", "Guanbin", ""]]}, {"id": "1708.03218", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Farhad Pourkamali-Anaraki, Stephen Becker", "title": "Improved Fixed-Rank Nystr\\\"om Approximation via QR Decomposition:\n  Practical and Theoretical Aspects", "comments": "Accepted in Neurocomputing. arXiv admin note: text overlap with\n  arXiv:1612.06470", "journal-ref": "Neurocomputing, 2019", "doi": "10.1016/j.neucom.2019.06.070", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nystrom method is a popular technique that uses a small number of\nlandmark points to compute a fixed-rank approximation of large kernel matrices\nthat arise in machine learning problems. In practice, to ensure high quality\napproximations, the number of landmark points is chosen to be greater than the\ntarget rank. However, for simplicity the standard Nystrom method uses a\nsub-optimal procedure for rank reduction. In this paper, we examine the\ndrawbacks of the standard Nystrom method in terms of poor performance and lack\nof theoretical guarantees. To address these issues, we present an efficient\nmodification for generating improved fixed-rank Nystrom approximations.\nTheoretical analysis and numerical experiments are provided to demonstrate the\nadvantages of the modified method over the standard Nystrom method. Overall,\nthe aim of this paper is to convince researchers to use the modified method, as\nit has nearly identical computational complexity, is easy to code, has greatly\nimproved accuracy in many cases, and is optimal in a sense that we make\nprecise.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 23:52:53 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 15:11:19 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Pourkamali-Anaraki", "Farhad", ""], ["Becker", "Stephen", ""]]}, {"id": "1708.03273", "submitter": "Chris Tensmeyer", "authors": "Chris Tensmeyer, Tony Martinez", "title": "Analysis of Convolutional Neural Networks for Document Image\n  Classification", "comments": "Accepted ICDAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are state-of-the-art models for document\nimage classification tasks. However, many of these approaches rely on\nparameters and architectures designed for classifying natural images, which\ndiffer from document images. We question whether this is appropriate and\nconduct a large empirical study to find what aspects of CNNs most affect\nperformance on document images. Among other results, we exceed the\nstate-of-the-art on the RVL-CDIP dataset by using shear transform data\naugmentation and an architecture designed for a larger input image.\nAdditionally, we analyze the learned features and find evidence that CNNs\ntrained on RVL-CDIP learn region-specific layout features.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:50:30 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Tensmeyer", "Chris", ""], ["Martinez", "Tony", ""]]}, {"id": "1708.03275", "submitter": "Shida He", "authors": "Shida He, Xuebin Qin, Zichen Zhang, Martin Jagersand", "title": "Incremental 3D Line Segment Extraction from Semi-dense SLAM", "comments": "Accepted at ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although semi-dense Simultaneous Localization and Mapping (SLAM) has been\nbecoming more popular over the last few years, there is a lack of efficient\nmethods for representing and processing their large scale point clouds. In this\npaper, we propose using 3D line segments to simplify the point clouds generated\nby semi-dense SLAM. Specifically, we present a novel incremental approach for\n3D line segment extraction. This approach reduces a 3D line segment fitting\nproblem into two 2D line segment fitting problems and takes advantage of both\nimages and depth maps. In our method, 3D line segments are fitted incrementally\nalong detected edge segments via minimizing fitting errors on two planes. By\nclustering the detected line segments, the resulting 3D representation of the\nscene achieves a good balance between compactness and completeness. Our\nexperimental results show that the 3D line segments generated by our method are\nhighly accurate. As an application, we demonstrate that these line segments\ngreatly improve the quality of 3D surface reconstruction compared to a feature\npoint based baseline.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:52:34 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 19:03:21 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 19:58:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["He", "Shida", ""], ["Qin", "Xuebin", ""], ["Zhang", "Zichen", ""], ["Jagersand", "Martin", ""]]}, {"id": "1708.03276", "submitter": "Chris Tensmeyer", "authors": "Chris Tensmeyer, Tony Martinez", "title": "Document Image Binarization with Fully Convolutional Neural Networks", "comments": "ICDAR 2017 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarization of degraded historical manuscript images is an important\npre-processing step for many document processing tasks. We formulate\nbinarization as a pixel classification learning task and apply a novel Fully\nConvolutional Network (FCN) architecture that operates at multiple image\nscales, including full resolution. The FCN is trained to optimize a continuous\nversion of the Pseudo F-measure metric and an ensemble of FCNs outperform the\ncompetition winners on 4 of 7 DIBCO competitions. This same binarization\ntechnique can also be applied to different domains such as Palm Leaf\nManuscripts with good performance. We analyze the performance of the proposed\nmodel w.r.t. the architectural hyperparameters, size and diversity of training\ndata, and the input features chosen.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:00:35 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Tensmeyer", "Chris", ""], ["Martinez", "Tony", ""]]}, {"id": "1708.03278", "submitter": "Xinghao Chen", "authors": "Xinghao Chen, Hengkai Guo, Guijin Wang, Li Zhang", "title": "Motion Feature Augmented Recurrent Neural Network for Skeleton-based\n  Dynamic Hand Gesture Recognition", "comments": "Accepted by ICIP 2017", "journal-ref": null, "doi": "10.1109/ICIP.2017.8296809", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic hand gesture recognition has attracted increasing interests because\nof its importance for human computer interaction. In this paper, we propose a\nnew motion feature augmented recurrent neural network for skeleton-based\ndynamic hand gesture recognition. Finger motion features are extracted to\ndescribe finger movements and global motion features are utilized to represent\nthe global movement of hand skeleton. These motion features are then fed into a\nbidirectional recurrent neural network (RNN) along with the skeleton sequence,\nwhich can augment the motion features for RNN and improve the classification\nperformance. Experiments demonstrate that our proposed method is effective and\noutperforms start-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:02:58 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Chen", "Xinghao", ""], ["Guo", "Hengkai", ""], ["Wang", "Guijin", ""], ["Zhang", "Li", ""]]}, {"id": "1708.03280", "submitter": "Ke Yang", "authors": "Ke Yang and Peng Qiao and Dongsheng Li and Shaohe Lv and Yong Dou", "title": "Exploring Temporal Preservation Networks for Precise Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action localization is an important task of computer vision. Though\na variety of methods have been proposed, it still remains an open question how\nto predict the temporal boundaries of action segments precisely. Most works use\nsegment-level classifiers to select video segments pre-determined by action\nproposal or dense sliding windows. However, in order to achieve more precise\naction boundaries, a temporal localization system should make dense predictions\nat a fine granularity. A newly proposed work exploits\nConvolutional-Deconvolutional-Convolutional (CDC) filters to upsample the\npredictions of 3D ConvNets, making it possible to perform per-frame action\npredictions and achieving promising performance in terms of temporal action\nlocalization. However, CDC network loses temporal information partially due to\nthe temporal downsampling operation. In this paper, we propose an elegant and\npowerful Temporal Preservation Convolutional (TPC) Network that equips 3D\nConvNets with TPC filters. TPC network can fully preserve temporal resolution\nand downsample the spatial resolution simultaneously, enabling frame-level\ngranularity action localization. TPC network can be trained in an end-to-end\nmanner. Experiment results on public datasets show that TPC network achieves\nsignificant improvement on per-frame action prediction and competing results on\nsegment-level temporal action localization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:07:16 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 08:32:50 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Yang", "Ke", ""], ["Qiao", "Peng", ""], ["Li", "Dongsheng", ""], ["Lv", "Shaohe", ""], ["Dou", "Yong", ""]]}, {"id": "1708.03292", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Tongzhou Wang, Ashwin Sreelal, Ravi Ramamoorthi,\n  Ren Ng", "title": "Learning to Synthesize a 4D RGBD Light Field from a Single Image", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning algorithm that takes as input a 2D RGB image\nand synthesizes a 4D RGBD light field (color and depth of the scene in each ray\ndirection). For training, we introduce the largest public light field dataset,\nconsisting of over 3300 plenoptic camera light fields of scenes containing\nflowers and plants. Our synthesis pipeline consists of a convolutional neural\nnetwork (CNN) that estimates scene geometry, a stage that renders a Lambertian\nlight field using that geometry, and a second CNN that predicts occluded rays\nand non-Lambertian effects. Our algorithm builds on recent view synthesis\nmethods, but is unique in predicting RGBD for each light field ray and\nimproving unsupervised single image depth estimation by enforcing consistency\nof ray depths that should intersect the same scene point. Please see our\nsupplementary video at https://youtu.be/yLCvWoQLnms\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:50:29 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Wang", "Tongzhou", ""], ["Sreelal", "Ashwin", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""]]}, {"id": "1708.03307", "submitter": "Yao Xue", "authors": "Yao Xue, Nilanjan Ray", "title": "Cell Detection in Microscopy Images with Deep Convolutional Neural\n  Network and Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically detect certain types of cells or cellular\nsubunits in microscopy images is of significant interest to a wide range of\nbiomedical research and clinical practices. Cell detection methods have evolved\nfrom employing hand-crafted features to deep learning-based techniques. The\nessential idea of these methods is that their cell classifiers or detectors are\ntrained in the pixel space, where the locations of target cells are labeled. In\nthis paper, we seek a different route and propose a convolutional neural\nnetwork (CNN)-based cell detection method that uses encoding of the output\npixel space. For the cell detection problem, the output space is the sparsely\nlabeled pixel locations indicating cell centers. We employ random projections\nto encode the output space to a compressed vector of fixed dimension. Then, CNN\nregresses this compressed vector from the input pixels. Furthermore, it is\npossible to stably recover sparse cell locations on the output pixel space from\nthe predicted compressed vector using $L_1$-norm optimization. In the past,\noutput space encoding using compressed sensing (CS) has been used in\nconjunction with linear and non-linear predictors. To the best of our\nknowledge, this is the first successful use of CNN with CS-based output space\nencoding. We made substantial experiments on several benchmark datasets, where\nthe proposed CNN + CS framework (referred to as CNNCS) achieved the highest or\nat least top-3 performance in terms of F1-score, compared with other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 17:27:09 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 04:43:28 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 00:11:48 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Xue", "Yao", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1708.03309", "submitter": "Tommaso Dreossi", "authors": "Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli,\n  Sanjit A. Seshia", "title": "Systematic Testing of Convolutional Neural Networks for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to systematically analyze convolutional neural\nnetworks (CNNs) used in classification of cars in autonomous vehicles. Our\nanalysis procedure comprises an image generator that produces synthetic\npictures by sampling in a lower dimension image modification subspace and a\nsuite of visualization tools. The image generator produces images which can be\nused to test the CNN and hence expose its vulnerabilities. The presented\nframework can be used to extract insights of the CNN classifier, compare across\nclassification models, or generate training and validation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 17:33:52 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 17:34:23 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1708.03361", "submitter": "Chandranath Adak", "authors": "Chandranath Adak, Bidyut B. Chaudhuri, Michael Blumenstein", "title": "An Empirical Study on Writer Identification & Verification from\n  Intra-variable Individual Handwriting", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2019.2899908", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The handwriting of an individual may vary substantially with factors such as\nmood, time, space, writing speed, writing medium and tool, writing topic, etc.\nIt becomes challenging to perform automated writer verification/identification\non a particular set of handwritten patterns (e.g., speedy handwriting) of a\nperson, especially when the system is trained using a different set of writing\npatterns (e.g., normal speed) of that same person. However, it would be\ninteresting to experimentally analyze if there exists any implicit\ncharacteristic of individuality which is insensitive to high intra-variable\nhandwriting. In this paper, we study some handcrafted features and auto-derived\nfeatures extracted from intra-variable writing. Here, we work on writer\nidentification/verification from offline Bengali handwriting of high\nintra-variability. To this end, we use various models mainly based on\nhandcrafted features with SVM (Support Vector Machine) and features\nauto-derived by the convolutional network. For experimentation, we have\ngenerated two handwritten databases from two different sets of 100 writers and\nenlarged the dataset by a data-augmentation technique. We have obtained some\ninteresting results.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 19:23:11 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 06:54:38 GMT"}, {"version": "v3", "created": "Sun, 20 Jan 2019 15:23:29 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Adak", "Chandranath", ""], ["Chaudhuri", "Bidyut B.", ""], ["Blumenstein", "Michael", ""]]}, {"id": "1708.03383", "submitter": "Fangting Xia", "authors": "Fangting Xia, Peng Wang, Xianjie Chen, Alan Yuille", "title": "Joint Multi-Person Pose Estimation and Semantic Part Segmentation", "comments": "This paper has been accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose estimation and semantic part segmentation are two complementary\ntasks in computer vision. In this paper, we propose to solve the two tasks\njointly for natural multi-person images, in which the estimated pose provides\nobject-level shape prior to regularize part segments while the part-level\nsegments constrain the variation of pose locations. Specifically, we first\ntrain two fully convolutional neural networks (FCNs), namely Pose FCN and Part\nFCN, to provide initial estimation of pose joint potential and semantic part\npotential. Then, to refine pose joint location, the two types of potentials are\nfused with a fully-connected conditional random field (FCRF), where a novel\nsegment-joint smoothness term is used to encourage semantic and spatial\nconsistency between parts and joints. To refine part segments, the refined pose\nand the original part potential are integrated through a Part FCN, where the\nskeleton feature from pose serves as additional regularization cues for part\nsegments. Finally, to reduce the complexity of the FCRF, we induce human\ndetection boxes and infer the graph inside each box, making the inference forty\ntimes faster.\n  Since there's no dataset that contains both part segments and pose labels, we\nextend the PASCAL VOC part dataset with human pose joints and perform extensive\nexperiments to compare our method against several most recent strategies. We\nshow that on this dataset our algorithm surpasses competing methods by a large\nmargin in both tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 20:59:31 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Xia", "Fangting", ""], ["Wang", "Peng", ""], ["Chen", "Xianjie", ""], ["Yuille", "Alan", ""]]}, {"id": "1708.03416", "submitter": "Xinghao Chen", "authors": "Xinghao Chen, Guijin Wang, Hengkai Guo, Cairong Zhang", "title": "Pose Guided Structured Region Ensemble Network for Cascaded Hand Pose\n  Estimation", "comments": "Accepted by Neurocomputing", "journal-ref": "Neurocomputing 2019", "doi": "10.1016/j.neucom.2018.06.097", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from a single depth image is an essential topic in\ncomputer vision and human computer interaction. Despite recent advancements in\nthis area promoted by convolutional neural network, accurate hand pose\nestimation is still a challenging problem. In this paper we propose a Pose\nguided structured Region Ensemble Network (Pose-REN) to boost the performance\nof hand pose estimation. The proposed method extracts regions from the feature\nmaps of convolutional neural network under the guide of an initially estimated\npose, generating more optimal and representative features for hand pose\nestimation. The extracted feature regions are then integrated hierarchically\naccording to the topology of hand joints by employing tree-structured fully\nconnections. A refined estimation of hand pose is directly regressed by the\nproposed network and the final hand pose is obtained by utilizing an iterative\ncascaded method. Comprehensive experiments on public hand pose datasets\ndemonstrate that our proposed method outperforms state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:36:24 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 02:00:25 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Chen", "Xinghao", ""], ["Wang", "Guijin", ""], ["Guo", "Hengkai", ""], ["Zhang", "Cairong", ""]]}, {"id": "1708.03417", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from\n  Remote Sensing Imagery", "comments": "Under review as a workshop paper at CI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in remote sensing technologies have made it possible to use\nhigh-resolution visual data for weather observation and forecasting tasks. We\npropose the use of multi-layer neural networks for understanding complex\natmospheric dynamics based on multichannel satellite images. The capability of\nour model was evaluated by using a linear regression task for single typhoon\ncoordinates prediction. A specific combination of models and different\nactivation policies enabled us to obtain an interesting prediction result in\nthe northeastern hemisphere (ENH).\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:41:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1708.03423", "submitter": "Wenqi Ren", "authors": "Wenqi Ren, Jinshan Pan, Xiaochun Cao and Ming-Hsuan Yang", "title": "Video Deblurring via Semantic Segmentation and Pixel-Wise Non-Linear\n  Kernel", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video deblurring is a challenging problem as the blur is complex and usually\ncaused by the combination of camera shakes, object motions, and depth\nvariations. Optical flow can be used for kernel estimation since it predicts\nmotion trajectories. However, the estimates are often inaccurate in complex\nscenes at object boundaries, which are crucial in kernel estimation. In this\npaper, we exploit semantic segmentation in each blurry frame to understand the\nscene contents and use different motion models for image regions to guide\noptical flow estimation. While existing pixel-wise blur models assume that the\nblur kernel is the same as optical flow during the exposure time, this\nassumption does not hold when the motion blur trajectory at a pixel is\ndifferent from the estimated linear optical flow. We analyze the relationship\nbetween motion blur trajectory and optical flow, and present a novel pixel-wise\nnon-linear kernel model to account for motion blur. The proposed blur model is\nbased on the non-linear optical flow, which describes complex motion blur more\neffectively. Extensive experiments on challenging blurry videos demonstrate the\nproposed algorithm performs favorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 03:15:48 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Ren", "Wenqi", ""], ["Pan", "Jinshan", ""], ["Cao", "Xiaochun", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.03431", "submitter": "Yong Man Ro", "authors": "Jung Uk Kim, Hak Gu Kim, Yong Man Ro", "title": "Iterative Deep Convolutional Encoder-Decoder Network for Medical Image\n  Segmentation", "comments": "accepted at EMBC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel medical image segmentation using iterative\ndeep learning framework. We have combined an iterative learning approach and an\nencoder-decoder network to improve segmentation results, which enables to\nprecisely localize the regions of interest (ROIs) including complex shapes or\ndetailed textures of medical images in an iterative manner. The proposed\niterative deep convolutional encoder-decoder network consists of two main\npaths: convolutional encoder path and convolutional decoder path with iterative\nlearning. Experimental results show that the proposed iterative deep learning\nframework is able to yield excellent medical image segmentation performances\nfor various medical images. The effectiveness of the proposed method has been\nproved by comparing with other state-of-the-art medical image segmentation\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 04:58:46 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Kim", "Jung Uk", ""], ["Kim", "Hak Gu", ""], ["Ro", "Yong Man", ""]]}, {"id": "1708.03474", "submitter": "Jiaolong Yang", "authors": "Qingnan Fan, Jiaolong Yang, Gang Hua, Baoquan Chen, David Wipf", "title": "A Generic Deep Architecture for Single Image Reflection Removal and\n  Image Smoothing", "comments": "Appeared at ICCV'17 (International Conference on Computer Vision)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep neural network structure that exploits edge\ninformation in addressing representative low-level vision tasks such as layer\nseparation and image filtering. Unlike most other deep learning strategies\napplied in this context, our approach tackles these challenging problems by\nestimating edges and reconstructing images using only cascaded convolutional\nlayers arranged such that no handcrafted or application-specific\nimage-processing components are required. We apply the resulting transferrable\npipeline to two different problem domains that are both sensitive to edges,\nnamely, single image reflection removal and image smoothing. For the former,\nusing a mild reflection smoothness assumption and a novel synthetic data\ngeneration method that acts as a type of weak supervision, our network is able\nto solve much more difficult reflection cases that cannot be handled by\nprevious methods. For the latter, we also exceed the state-of-the-art\nquantitative and qualitative results by wide margins. In all cases, the\nproposed framework is simple, fast, and easy to transfer across disparate\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 08:47:33 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 13:12:31 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Fan", "Qingnan", ""], ["Yang", "Jiaolong", ""], ["Hua", "Gang", ""], ["Chen", "Baoquan", ""], ["Wipf", "David", ""]]}, {"id": "1708.03615", "submitter": "Federico Pernici", "authors": "Federico Pernici and Alberto Del Bimbo", "title": "Unsupervised Incremental Learning of Deep Descriptors From Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel unsupervised method for face identity learning from video\nsequences. The method exploits the ResNet deep network for face detection and\nVGGface fc7 face descriptors together with a smart learning mechanism that\nexploits the temporal coherence of visual data in video streams. We present a\nnovel feature matching solution based on Reverse Nearest Neighbour and a\nfeature forgetting strategy that supports incremental learning with memory size\ncontrol, while time progresses. It is shown that the proposed learning\nprocedure is asymptotically stable and can be effectively applied to relevant\napplications like multiple face tracking.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 17:04:03 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Pernici", "Federico", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "1708.03619", "submitter": "Zhou Yu", "authors": "Zhou Yu, Jun Yu, Chenchao Xiang, Jianping Fan, Dacheng Tao", "title": "Beyond Bilinear: Generalized Multimodal Factorized High-order Pooling\n  for Visual Question Answering", "comments": "13 pages, 9 figures. arXiv admin note: substantial text overlap with\n  arXiv:1708.01471", "journal-ref": "IEEE Transactions On Neural Networks And Learning Systems, Vol.\n  26, No. 10, October 2015, Pp. 2275-2290", "doi": "10.1109/TNNLS.2018.2817340", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) is challenging because it requires a\nsimultaneous understanding of both visual content of images and textual content\nof questions. To support the VQA task, we need to find good solutions for the\nfollowing three issues: 1) fine-grained feature representations for both the\nimage and the question; 2) multi-modal feature fusion that is able to capture\nthe complex interactions between multi-modal features; 3) automatic answer\nprediction that is able to consider the complex correlations between multiple\ndiverse answers for the same question. For fine-grained image and question\nrepresentations, a `co-attention' mechanism is developed by using a deep neural\nnetwork architecture to jointly learn the attentions for both the image and the\nquestion, which can allow us to reduce the irrelevant features effectively and\nobtain more discriminative features for image and question representations. For\nmulti-modal feature fusion, a generalized Multi-modal Factorized High-order\npooling approach (MFH) is developed to achieve more effective fusion of\nmulti-modal features by exploiting their correlations sufficiently, which can\nfurther result in superior VQA performance as compared with the\nstate-of-the-art approaches. For answer prediction, the KL (Kullback-Leibler)\ndivergence is used as the loss function to achieve precise characterization of\nthe complex correlations between multiple diverse answers with the same or\nsimilar meaning, which can allow us to achieve faster convergence rate and\nobtain slightly better accuracy on answer prediction. A deep neural network\narchitecture is designed to integrate all these aforementioned modules into a\nunified model for achieving superior VQA performance. With an ensemble of our\nMFH models, we achieve the state-of-the-art performance on the large-scale VQA\ndatasets and win the runner-up in VQA Challenge 2017.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 09:09:23 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 04:16:57 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Yu", "Zhou", ""], ["Yu", "Jun", ""], ["Xiang", "Chenchao", ""], ["Fan", "Jianping", ""], ["Tao", "Dacheng", ""]]}, {"id": "1708.03669", "submitter": "Chris Tensmeyer", "authors": "Chris Tensmeyer, Daniel Saunders, and Tony Martinez", "title": "Convolutional Neural Networks for Font Classification", "comments": "ICDAR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying pages or text lines into font categories aids transcription\nbecause single font Optical Character Recognition (OCR) is generally more\naccurate than omni-font OCR. We present a simple framework based on\nConvolutional Neural Networks (CNNs), where a CNN is trained to classify small\npatches of text into predefined font classes. To classify page or line images,\nwe average the CNN predictions over densely extracted patches. We show that\nthis method achieves state-of-the-art performance on a challenging dataset of\n40 Arabic computer fonts with 98.8\\% line level accuracy. This same method also\nachieves the highest reported accuracy of 86.6% in predicting paleographic\nscribal script classes at the page level on medieval Latin manuscripts.\nFinally, we analyze what features are learned by the CNN on Latin manuscripts\nand find evidence that the CNN is learning both the defining morphological\ndifferences between scribal script classes as well as overfitting to\nclass-correlated nuisance factors. We propose a novel form of data augmentation\nthat improves robustness to text darkness, further increasing classification\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 19:25:44 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Tensmeyer", "Chris", ""], ["Saunders", "Daniel", ""], ["Martinez", "Tony", ""]]}, {"id": "1708.03694", "submitter": "Dino Ienco", "authors": "Dinh Ho Tong Minh, Dino Ienco, Raffaele Gaetano, Nathalie Lalande,\n  Emile Ndikumana, Faycal Osman, Pierre Maurel", "title": "Deep Recurrent Neural Networks for mapping winter vegetation quality\n  coverage via multi-temporal SAR Sentinel-1", "comments": "In submission to IEEE Geoscience and Remote Sensing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping winter vegetation quality coverage is a challenge problem of remote\nsensing. This is due to the cloud coverage in winter period, leading to use\nradar rather than optical images. The objective of this paper is to provide a\nbetter understanding of the capabilities of radar Sentinel-1 and deep learning\nconcerning about mapping winter vegetation quality coverage. The analysis\npresented in this paper is carried out on multi-temporal Sentinel-1 data over\nthe site of La Rochelle, France, during the campaign in December 2016. This\ndataset were processed in order to produce an intensity radar data stack from\nOctober 2016 to February 2017. Two deep Recurrent Neural Network (RNN) based\nclassifier methods were employed. We found that the results of RNNs clearly\noutperformed the classical machine learning approaches (Support Vector Machine\nand Random Forest). This study confirms that the time series radar Sentinel-1\nand RNNs could be exploited for winter vegetation quality cover mapping.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 20:28:07 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Minh", "Dinh Ho Tong", ""], ["Ienco", "Dino", ""], ["Gaetano", "Raffaele", ""], ["Lalande", "Nathalie", ""], ["Ndikumana", "Emile", ""], ["Osman", "Faycal", ""], ["Maurel", "Pierre", ""]]}, {"id": "1708.03698", "submitter": "Abdullah Hamdi", "authors": "Abdullah Hamdi, Bernard Ghanem", "title": "Learning Rotation for Kernel Correlation Filter", "comments": "6 pages, 11 figures, tracking, CVPR, Correlation Filters,KCF, visual\n  object tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Correlation Filters have shown a very promising scheme for visual\ntracking in terms of speed and accuracy on several benchmarks. However it\nsuffers from problems that affect its performance like occlusion, rotation and\nscale change. This paper tries to tackle the problem of rotation by\nreformulating the optimization problem for learning the correlation filter.\nThis modification (RKCF) includes learning rotation filter that utilizes\ncirculant structure of HOG feature to guesstimate rotation from one frame to\nanother and enhance the detection of KCF. Hence it gains boost in overall\naccuracy in many of OBT50 detest videos with minimal additional computation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 20:35:25 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Hamdi", "Abdullah", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1708.03704", "submitter": "Alan Mosca", "authors": "Alan Mosca, George D Magoulas", "title": "Deep Incremental Boosting", "comments": null, "journal-ref": "Christoph Benzm\\\"uller, Geoff Sutcliffe and Raul Rojas (editors).\n  GCAI 2016. 2nd Global Conference on Artificial Intelligence, vol 41, pages\n  293--302", "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Deep Incremental Boosting, a new technique derived from\nAdaBoost, specifically adapted to work with Deep Learning methods, that reduces\nthe required training time and improves generalisation. We draw inspiration\nfrom Transfer of Learning approaches to reduce the start-up time to training\neach incremental Ensemble member. We show a set of experiments that outlines\nsome preliminary results on some common Deep Learning datasets and discuss the\npotential improvements Deep Incremental Boosting brings to traditional Ensemble\nmethods in Deep Learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 21:05:58 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Mosca", "Alan", ""], ["Magoulas", "George D", ""]]}, {"id": "1708.03725", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Fillipe DM de Souza and Sudeep Sarkar", "title": "Going Deeper with Semantics: Video Activity Interpretation using\n  Semantic Contextualization", "comments": "Accepted to WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deeper understanding of video activities extends beyond recognition of\nunderlying concepts such as actions and objects: constructing deep semantic\nrepresentations requires reasoning about the semantic relationships among these\nconcepts, often beyond what is directly observed in the data. To this end, we\npropose an energy minimization framework that leverages large-scale commonsense\nknowledge bases, such as ConceptNet, to provide contextual cues to establish\nsemantic relationships among entities directly hypothesized from video signal.\nWe mathematically express this using the language of Grenander's canonical\npattern generator theory. We show that the use of prior encoded commonsense\nknowledge alleviate the need for large annotated training datasets and help\ntackle imbalance in training through prior knowledge. Using three different\npublicly available datasets - Charades, Microsoft Visual Description Corpus and\nBreakfast Actions datasets, we show that the proposed model can generate video\ninterpretations whose quality is better than those reported by state-of-the-art\napproaches, which have substantial training needs. Through extensive\nexperiments, we show that the use of commonsense knowledge from ConceptNet\nallows the proposed approach to handle various challenges such as training data\nimbalance, weak features, and complex semantic relationships and visual scenes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 22:49:47 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 17:22:26 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 15:58:14 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["de Souza", "Fillipe DM", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1708.03736", "submitter": "Lei Zhou", "authors": "Lei Zhou, Zhi Liu, Xiangjian He", "title": "Face Parsing via a Fully-Convolutional Continuous CRF Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the face parsing task with a Fully-Convolutional\ncontinuous CRF Neural Network (FC-CNN) architecture. In contrast to previous\nface parsing methods that apply region-based subnetwork hundreds of times, our\nFC-CNN is fully convolutional with high segmentation accuracy. To achieve this\ngoal, FC-CNN integrates three subnetworks, a unary network, a pairwise network\nand a continuous Conditional Random Field (C-CRF) network into a unified\nframework. The high-level semantic information and low-level details across\ndifferent convolutional layers are captured by the convolutional and\ndeconvolutional structures in the unary network. The semantic edge context is\nlearnt by the pairwise network branch to construct pixel-wise affinity. Based\non a differentiable superpixel pooling layer and a differentiable C-CRF layer,\nthe unary network and pairwise network are combined via a novel continuous CRF\nnetwork to achieve spatial consistency in both training and test procedure of a\ndeep neural network. Comprehensive evaluations on LFW-PL and HELEN datasets\ndemonstrate that FC-CNN achieves better performance over the other\nstate-of-arts for accurate face labeling on challenging images.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 01:17:38 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Zhou", "Lei", ""], ["Liu", "Zhi", ""], ["He", "Xiangjian", ""]]}, {"id": "1708.03748", "submitter": "Nazim Haouchine", "authors": "Nazim Haouchine, Frederick Roy, Hadrien Courtecuisse, Matthias\n  Nie{\\ss}ner and Stephane Cotin", "title": "Calipso: Physics-based Image and Video Editing through CAD Model Proxies", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Calipso, an interactive method for editing images and videos in a\nphysically-coherent manner. Our main idea is to realize physics-based\nmanipulations by running a full physics simulation on proxy geometries given by\nnon-rigidly aligned CAD models. Running these simulations allows us to apply\nnew, unseen forces to move or deform selected objects, change physical\nparameters such as mass or elasticity, or even add entire new objects that\ninteract with the rest of the underlying scene. In Calipso, the user makes\nedits directly in 3D; these edits are processed by the simulation and then\ntransfered to the target 2D content using shape-to-image correspondences in a\nphoto-realistic rendering process. To align the CAD models, we introduce an\nefficient CAD-to-image alignment procedure that jointly minimizes for rigid and\nnon-rigid alignment while preserving the high-level structure of the input\nshape. Moreover, the user can choose to exploit image flow to estimate scene\nmotion, producing coherent physical behavior with ambient dynamics. We\ndemonstrate Calipso's physics-based editing on a wide range of examples\nproducing myriad physical behavior while preserving geometric and visual\nconsistency.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 07:40:39 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Haouchine", "Nazim", ""], ["Roy", "Frederick", ""], ["Courtecuisse", "Hadrien", ""], ["Nie\u00dfner", "Matthias", ""], ["Cotin", "Stephane", ""]]}, {"id": "1708.03763", "submitter": "Viraj Mavani", "authors": "Ayesha Gurnani, Viraj Mavani, Vandit Gajjar and Yash Khandhediya", "title": "Flower Categorization using Deep Convolutional Neural Networks", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We have developed a deep learning network for classification of different\nflowers. For this, we have used Visual Geometry Group's 102 category flower\ndataset having 8189 images of 102 different flowers from University of Oxford.\nThe method is basically divided into two parts; Image segmentation and\nclassification. We have compared the performance of two different Convolutional\nNeural Network architectures GoogLeNet and AlexNet for classification purpose.\nBy keeping the hyper parameters same for both architectures, we have found that\nthe top 1 and top 5 accuracies of GoogLeNet are 47.15% and 69.17% respectively\nwhereas the top 1 and top 5 accuracies of AlexNet are 43.39% and 68.68%\nrespectively. These results are extremely good when compared to random\nclassification accuracy of 0.98%. This method for classification of flowers can\nbe implemented in real time applications and can be used to help botanists for\ntheir research as well as camping enthusiasts.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 11:01:57 GMT"}, {"version": "v2", "created": "Fri, 8 Dec 2017 18:07:22 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Gurnani", "Ayesha", ""], ["Mavani", "Viraj", ""], ["Gajjar", "Vandit", ""], ["Khandhediya", "Yash", ""]]}, {"id": "1708.03769", "submitter": "Binghui Chen", "authors": "Binghui Chen, Weihong Deng, Junping Du", "title": "Noisy Softmax: Improving the Generalization Ability of DCNN via\n  Postponing the Early Softmax Saturation", "comments": "10 pages, 7 figures, CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, softmax and SGD have become a commonly used\ncomponent and the default training strategy in CNN frameworks, respectively.\nHowever, when optimizing CNNs with SGD, the saturation behavior behind softmax\nalways gives us an illusion of training well and then is omitted. In this\npaper, we first emphasize that the early saturation behavior of softmax will\nimpede the exploration of SGD, which sometimes is a reason for model converging\nat a bad local-minima, then propose Noisy Softmax to mitigating this early\nsaturation issue by injecting annealed noise in softmax during each iteration.\nThis operation based on noise injection aims at postponing the early saturation\nand further bringing continuous gradients propagation so as to significantly\nencourage SGD solver to be more exploratory and help to find a better\nlocal-minima. This paper empirically verifies the superiority of the early\nsoftmax desaturation, and our method indeed improves the generalization ability\nof CNN model by regularization. We experimentally find that this early\ndesaturation helps optimization in many tasks, yielding state-of-the-art or\ncompetitive results on several popular benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 11:43:18 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Binghui", ""], ["Deng", "Weihong", ""], ["Du", "Junping", ""]]}, {"id": "1708.03795", "submitter": "Shihao Zhang", "authors": "Shihao Zhang, Weiyao Lin, Ping Lu, Weihua Li, Shuo Deng", "title": "Kill Two Birds With One Stone: Boosting Both Object Detection Accuracy\n  and Speed With adaptive Patch-of-Interest Composition", "comments": "The project page for this paper is available at\n  http://min.sjtu.edu.cn/lwydemo/Dete/demo/detection.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important yet challenging task in video understanding\n& analysis, where one major challenge lies in the proper balance between two\ncontradictive factors: detection accuracy and detection speed. In this paper,\nwe propose a new adaptive patch-of-interest composition approach for boosting\nboth the accuracy and speed for object detection. The proposed approach first\nextracts patches in a video frame which have the potential to include\nobjects-of-interest. Then, an adaptive composition process is introduced to\ncompose the extracted patches into an optimal number of sub-frames for object\ndetection. With this process, we are able to maintain the resolution of the\noriginal frame during object detection (for guaranteeing the accuracy), while\nminimizing the number of inputs in detection (for boosting the speed).\nExperimental results on various datasets demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 16:52:43 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 15:45:21 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 17:39:55 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Zhang", "Shihao", ""], ["Lin", "Weiyao", ""], ["Lu", "Ping", ""], ["Li", "Weihua", ""], ["Deng", "Shuo", ""]]}, {"id": "1708.03798", "submitter": "Yadong Mu", "authors": "Lu Chi and Yadong Mu", "title": "Deep Steering: Learning End-to-End Driving Model from Spatial and\n  Temporal Visual Cues", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, autonomous driving algorithms using low-cost vehicle-mounted\ncameras have attracted increasing endeavors from both academia and industry.\nThere are multiple fronts to these endeavors, including object detection on\nroads, 3-D reconstruction etc., but in this work we focus on a vision-based\nmodel that directly maps raw input images to steering angles using deep\nnetworks. This represents a nascent research topic in computer vision. The\ntechnical contributions of this work are three-fold. First, the model is\nlearned and evaluated on real human driving videos that are time-synchronized\nwith other vehicle sensors. This differs from many prior models trained from\nsynthetic data in racing games. Second, state-of-the-art models, such as\nPilotNet, mostly predict the wheel angles independently on each video frame,\nwhich contradicts common understanding of driving as a stateful process.\nInstead, our proposed model strikes a combination of spatial and temporal cues,\njointly investigating instantaneous monocular camera observations and vehicle's\nhistorical states. This is in practice accomplished by inserting\ncarefully-designed recurrent units (e.g., LSTM and Conv-LSTM) at proper network\nlayers. Third, to facilitate the interpretability of the learned model, we\nutilize a visual back-propagation scheme for discovering and visualizing image\nregions crucially influencing the final steering prediction. Our experimental\nstudy is based on about 6 hours of human driving data provided by Udacity.\nComprehensive quantitative evaluations demonstrate the effectiveness and\nrobustness of our model, even under scenarios like drastic lighting changes and\nabrupt turning. The comparison with other state-of-the-art models clearly\nreveals its superior performance in predicting the due wheel angle for a\nself-driving car.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 17:18:05 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chi", "Lu", ""], ["Mu", "Yadong", ""]]}, {"id": "1708.03805", "submitter": "Chuang Gan", "authors": "Yunlong Bian, Chuang Gan, Xiao Liu, Fu Li, Xiang Long, Yandong Li,\n  Heng Qi, Jie Zhou, Shilei Wen, Yuanqing Lin", "title": "Revisiting the Effectiveness of Off-the-shelf Temporal Modeling\n  Approaches for Large-scale Video Classification", "comments": "A brief summary of the winner solution on Activity Kinetics challenge\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our solution for the video recognition task of\nActivityNet Kinetics challenge that ranked the 1st place. Most of existing\nstate-of-the-art video recognition approaches are in favor of an end-to-end\npipeline. One exception is the framework of DevNet. The merit of DevNet is that\nthey first use the video data to learn a network (i.e. fine-tuning or training\nfrom scratch). Instead of directly using the end-to-end classification scores\n(e.g. softmax scores), they extract the features from the learned network and\nthen fed them into the off-the-shelf machine learning models to conduct video\nclassification. However, the effectiveness of this line work has long-term been\nignored and underestimated. In this submission, we extensively use this\nstrategy. Particularly, we investigate four temporal modeling approaches using\nthe learned features: Multi-group Shifting Attention Network, Temporal Xception\nNetwork, Multi-stream sequence Model and Fast-Forward Sequence Model.\nExperiment results on the challenging Kinetics dataset demonstrate that our\nproposed temporal modeling approaches can significantly improve existing\napproaches in the large-scale video recognition tasks. Most remarkably, our\nbest single Multi-group Shifting Attention Network can achieve 77.7% in term of\ntop-1 accuracy and 93.2% in term of top-5 accuracy on the validation set.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 18:38:19 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Bian", "Yunlong", ""], ["Gan", "Chuang", ""], ["Liu", "Xiao", ""], ["Li", "Fu", ""], ["Long", "Xiang", ""], ["Li", "Yandong", ""], ["Qi", "Heng", ""], ["Zhou", "Jie", ""], ["Wen", "Shilei", ""], ["Lin", "Yuanqing", ""]]}, {"id": "1708.03816", "submitter": "Natalia Neverova", "authors": "Natalia Neverova and Iasonas Kokkinos", "title": "Mass Displacement Networks", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the large improvements in performance attained by using deep learning\nin computer vision, one can often further improve results with some additional\npost-processing that exploits the geometric nature of the underlying task. This\ncommonly involves displacing the posterior distribution of a CNN in a way that\nmakes it more appropriate for the task at hand, e.g. better aligned with local\nimage features, or more compact. In this work we integrate this geometric\npost-processing within a deep architecture, introducing a differentiable and\nprobabilistically sound counterpart to the common geometric voting technique\nused for evidence accumulation in vision. We refer to the resulting neural\nmodels as Mass Displacement Networks (MDNs), and apply them to human pose\nestimation in two distinct setups: (a) landmark localization, where we collapse\na distribution to a point, allowing for precise localization of body keypoints\nand (b) communication across body parts, where we transfer evidence from one\npart to the other, allowing for a globally consistent pose estimate. We\nevaluate on large-scale pose estimation benchmarks, such as MPII Human Pose and\nCOCO datasets, and report systematic improvements when compared to strong\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 19:42:44 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Neverova", "Natalia", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1708.03867", "submitter": "Qi Dou", "authors": "Qi Dou, Hao Chen, Yueming Jin, Huangjing Lin, Jing Qin, Pheng-Ann Heng", "title": "Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample\n  Filtering and Hybrid-Loss Residual Learning", "comments": "Accepted to MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework with 3D convolutional networks\n(ConvNets) for automated detection of pulmonary nodules from low-dose CT scans,\nwhich is a challenging yet crucial task for lung cancer early diagnosis and\ntreatment. Different from previous standard ConvNets, we try to tackle the\nsevere hard/easy sample imbalance problem in medical datasets and explore the\nbenefits of localized annotations to regularize the learning, and hence boost\nthe performance of ConvNets to achieve more accurate detections. Our proposed\nframework consists of two stages: 1) candidate screening, and 2) false positive\nreduction. In the first stage, we establish a 3D fully convolutional network,\neffectively trained with an online sample filtering scheme, to sensitively and\nrapidly screen the nodule candidates. In the second stage, we design a\nhybrid-loss residual network which harnesses the location and size information\nas important cues to guide the nodule recognition procedure. Experimental\nresults on the public large-scale LUNA16 dataset demonstrate superior\nperformance of our proposed method compared with state-of-the-art approaches\nfor the pulmonary nodule detection task.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 07:33:55 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Dou", "Qi", ""], ["Chen", "Hao", ""], ["Jin", "Yueming", ""], ["Lin", "Huangjing", ""], ["Qin", "Jing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1708.03874", "submitter": "Tianyu Yang", "authors": "Tianyu Yang, Antoni B. Chan", "title": "Recurrent Filter Learning for Visual Tracking", "comments": "ICCV2017 Workshop on VOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently using convolutional neural networks (CNNs) has gained popularity in\nvisual tracking, due to its robust feature representation of images. Recent\nmethods perform online tracking by fine-tuning a pre-trained CNN model to the\nspecific target object using stochastic gradient descent (SGD)\nback-propagation, which is usually time-consuming. In this paper, we propose a\nrecurrent filter generation methods for visual tracking. We directly feed the\ntarget's image patch to a recurrent neural network (RNN) to estimate an\nobject-specific filter for tracking. As the video sequence is a spatiotemporal\ndata, we extend the matrix multiplications of the fully-connected layers of the\nRNN to a convolution operation on feature maps, which preserves the target's\nspatial structure and also is memory-efficient. The tracked object in the\nsubsequent frames will be fed into the RNN to adapt the generated filters to\nappearance variations of the target. Note that once the off-line training\nprocess of our network is finished, there is no need to fine-tune the network\nfor specific objects, which makes our approach more efficient than methods that\nuse iterative fine-tuning to online learn the target. Extensive experiments\nconducted on widely used benchmarks, OTB and VOT, demonstrate encouraging\nresults compared to other recent methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 08:18:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yang", "Tianyu", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1708.03880", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Weisi Lin, Shiqi Wang, Long Xu, Leida Li", "title": "Image Quality Assessment Guided Deep Neural Networks Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many computer vision problems, the deep neural networks are trained and\nvalidated based on the assumption that the input images are pristine (i.e.,\nartifact-free). However, digital images are subject to a wide range of\ndistortions in real application scenarios, while the practical issues regarding\nimage quality in high level visual information understanding have been largely\nignored. In this paper, in view of the fact that most widely deployed deep\nlearning models are susceptible to various image distortions, the distorted\nimages are involved for data augmentation in the deep neural network training\nprocess to learn a reliable model for practical applications. In particular, an\nimage quality assessment based label smoothing method, which aims at\nregularizing the label distribution of training images, is further proposed to\ntune the objective functions in learning the neural network. Experimental\nresults show that the proposed method is effective in dealing with both low and\nhigh quality images in the typical image classification task.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 09:51:07 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Zhuo", ""], ["Lin", "Weisi", ""], ["Wang", "Shiqi", ""], ["Xu", "Long", ""], ["Li", "Leida", ""]]}, {"id": "1708.03888", "submitter": "Yang You", "authors": "Yang You, Igor Gitman, Boris Ginsburg", "title": "Large Batch Training of Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common way to speed up training of large convolutional networks is to add\ncomputational units. Training is then performed using data-parallel synchronous\nStochastic Gradient Descent (SGD) with mini-batch divided between computational\nunits. With an increase in the number of nodes, the batch size grows. But\ntraining with large batch size often results in the lower model accuracy. We\nargue that the current recipe for large batch training (linear learning rate\nscaling with warm-up) is not general enough and training may diverge. To\novercome this optimization difficulties we propose a new training algorithm\nbased on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet\nup to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 11:01:57 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 23:18:36 GMT"}, {"version": "v3", "created": "Wed, 13 Sep 2017 23:25:07 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["You", "Yang", ""], ["Gitman", "Igor", ""], ["Ginsburg", "Boris", ""]]}, {"id": "1708.03898", "submitter": "Artur Laskowski", "authors": "Maciej A. Czyzewski, Artur Laskowski, Szymon Wasik", "title": "Chessboard and chess piece recognition with the support of neural\n  networks", "comments": "11 pages, 14 figures; for implementation, see\n  https://github.com/maciejczyzewski/neural-chessboard; Submitted to FCDS, In\n  Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chessboard and chess piece recognition is a computer vision problem that has\nnot yet been efficiently solved. However, its solution is crucial for many\nexperienced players who wish to compete against AI bots, but also prefer to\nmake decisions based on the analysis of a physical chessboard. It is also\nimportant for organizers of chess tournaments who wish to digitize play for\nonline broadcasting or ordinary players who wish to share their gameplay with\nfriends. Typically, such digitization tasks are performed by humans or with the\naid of specialized chessboards and pieces. However, neither solution is easy or\nconvenient. To solve this problem, we propose a novel algorithm for digitizing\nchessboard configurations.\n  We designed a method that is resistant to lighting conditions and the angle\nat which images are captured, and works correctly with numerous chessboard\nstyles. The proposed algorithm processes pictures iteratively. During each\niteration, it executes three major sub-processes: detecting straight lines,\nfinding lattice points, and positioning the chessboard. Finally, we identify\nall chess pieces and generate a description of the board utilizing standard\nnotation. For each of these steps, we designed our own algorithm that surpasses\nexisting solutions. We support our algorithms by utilizing machine learning\ntechniques whenever possible.\n  The described method performs extraordinarily well and achieves an accuracy\nover $99.5\\%$ for detecting chessboard lattice points (compared to the $74\\%$\nfor the best alternative), $95\\%$ (compared to $60\\%$ for the best alternative)\nfor positioning the chessboard in an image, and almost $95\\%$ for chess piece\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 12:34:11 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 19:46:10 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2020 19:34:34 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Czyzewski", "Maciej A.", ""], ["Laskowski", "Artur", ""], ["Wasik", "Szymon", ""]]}, {"id": "1708.03901", "submitter": "Mohsen Malmir", "authors": "Mohsen Malmir and Garrison W. Cottrell", "title": "Belief Tree Search for Active Object Recognition", "comments": "IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Object Recognition (AOR) has been approached as an unsupervised\nlearning problem, in which optimal trajectories for object inspection are not\nknown and are to be discovered by reducing label uncertainty measures or\ntraining with reinforcement learning. Such approaches have no guarantees of the\nquality of their solution. In this paper, we treat AOR as a Partially\nObservable Markov Decision Process (POMDP) and find near-optimal policies on\ntraining data using Belief Tree Search (BTS) on the corresponding belief Markov\nDecision Process (MDP). AOR then reduces to the problem of knowledge transfer\nfrom near-optimal policies on training set to the test set. We train a Long\nShort Term Memory (LSTM) network to predict the best next action on the\ntraining set rollouts. We sho that the proposed AOR method generalizes well to\nnovel views of familiar objects and also to novel objects. We compare this\nsupervised scheme against guided policy search, and find that the LSTM network\nreaches higher recognition accuracy compared to the guided policy method. We\nfurther look into optimizing the observation function to increase the total\ncollected reward of optimal policy. In AOR, the observation function is known\nonly approximately. We propose a gradient-based method update to this\napproximate observation function to increase the total reward of any policy. We\nshow that by optimizing the observation function and retraining the supervised\nLSTM network, the AOR performance on the test set improves significantly.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 13:24:28 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Malmir", "Mohsen", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1708.03911", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ying Nian Wu, Hao Zhang, and Song-Chun Zhu", "title": "Mining Deep And-Or Object Structures via Cost-Sensitive\n  Question-Answer-Based Active Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cost-sensitive active Question-Answering (QA) framework\nfor learning a nine-layer And-Or graph (AOG) from web images. The AOG\nexplicitly represents object categories, poses/viewpoints, parts, and detailed\nstructures within the parts in a compositional hierarchy. The QA framework is\ndesigned to minimize an overall risk, which trades off the loss and query\ncosts. The loss is defined for nodes in all layers of the AOG, including the\ngenerative loss (measuring the likelihood of the images) and the discriminative\nloss (measuring the fitness to human answers). The cost comprises both the\nhuman labor of answering questions and the computational cost of model\nlearning. The cost-sensitive QA framework iteratively selects different\nstorylines of questions to update different nodes in the AOG. Experiments\nshowed that our method required much less human supervision (e.g., labeling\nparts on 3--10 training objects for each category) and achieved better\nperformance than baseline methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 14:11:15 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 19:19:00 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 01:24:54 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Zhang", "Quanshi", ""], ["Wu", "Ying Nian", ""], ["Zhang", "Hao", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1708.03918", "submitter": "Yantao Shen", "authors": "Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi and Xiaogang Wang", "title": "Learning Deep Neural Networks for Vehicle Re-ID with\n  Visual-spatio-temporal Path Proposals", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification is an important problem and has many applications\nin video surveillance and intelligent transportation. It gains increasing\nattention because of the recent advances of person re-identification\ntechniques. However, unlike person re-identification, the visual differences\nbetween pairs of vehicle images are usually subtle and even challenging for\nhumans to distinguish. Incorporating additional spatio-temporal information is\nvital for solving the challenging re-identification task. Existing vehicle\nre-identification methods ignored or used over-simplified models for the\nspatio-temporal relations between vehicle images. In this paper, we propose a\ntwo-stage framework that incorporates complex spatio-temporal information for\neffectively regularizing the re-identification results. Given a pair of vehicle\nimages with their spatio-temporal information, a candidate\nvisual-spatio-temporal path is first generated by a chain MRF model with a\ndeeply learned potential function, where each visual-spatio-temporal state\ncorresponds to an actual vehicle image with its spatio-temporal information. A\nSiamese-CNN+Path-LSTM model takes the candidate path as well as the pairwise\nqueries to generate their similarity score. Extensive experiments and analysis\nshow the effectiveness of our proposed method and individual components.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 14:53:30 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Shen", "Yantao", ""], ["Xiao", "Tong", ""], ["Li", "Hongsheng", ""], ["Yi", "Shuai", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1708.03921", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Xuan Song, and Ryosuke Shibasaki", "title": "Visual Graph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we formulate the concept of \"mining maximal-size frequent\nsubgraphs\" in the challenging domain of visual data (images and videos). In\ngeneral, visual knowledge can usually be modeled as attributed relational\ngraphs (ARGs) with local attributes representing local parts and pairwise\nattributes describing the spatial relationship between parts. Thus, from a\npractical perspective, such mining of maximal-size subgraphs can be regarded as\na general platform for discovering and modeling the common objects within\ncluttered and unlabeled visual data. Then, from a theoretical perspective,\nvisual graph mining should encode and overcome the great fuzziness of messy\ndata collected from complex real-world situations, which conflicts with the\nconventional theoretical basis of graph mining designed for tabular data.\nCommon subgraphs hidden in these ARGs usually have soft attributes, with\nconsiderable inter-graph variation. More importantly, we should also discover\nthe latent pattern space, including similarity metrics for the pattern and\nhidden node relations, during the mining process. In this study, we redefine\nthe visual subgraph pattern that encodes all of these challenges in a general\nway, and propose an approximate but efficient solution to graph mining. We\nconduct five experiments to evaluate our method with different kinds of visual\ndata, including videos and RGB/RGB-D images. These experiments demonstrate the\ngenerality of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 15:21:22 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Zhang", "Quanshi", ""], ["Song", "Xuan", ""], ["Shibasaki", "Ryosuke", ""]]}, {"id": "1708.03958", "submitter": "Lin Sun", "authors": "Lin Sun, Kui Jia, Kevin Chen, Dit Yan Yeung, Bertram E. Shi, Silvio\n  Savarese", "title": "Lattice Long Short-Term Memory for Human Action Recognition", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human actions captured in video sequences are three-dimensional signals\ncharacterizing visual appearance and motion dynamics. To learn action patterns,\nexisting methods adopt Convolutional and/or Recurrent Neural Networks (CNNs and\nRNNs). CNN based methods are effective in learning spatial appearances, but are\nlimited in modeling long-term motion dynamics. RNNs, especially Long Short-Term\nMemory (LSTM), are able to learn temporal motion dynamics. However, naively\napplying RNNs to video sequences in a convolutional manner implicitly assumes\nthat motions in videos are stationary across different spatial locations. This\nassumption is valid for short-term motions but invalid when the duration of the\nmotion is long.\n  In this work, we propose Lattice-LSTM (L2STM), which extends LSTM by learning\nindependent hidden state transitions of memory cells for individual spatial\nlocations. This method effectively enhances the ability to model dynamics\nacross time and addresses the non-stationary issue of long-term motion dynamics\nwithout significantly increasing the model complexity. Additionally, we\nintroduce a novel multi-modal training procedure for training our network.\nUnlike traditional two-stream architectures which use RGB and optical flow\ninformation as input, our two-stream model leverages both modalities to jointly\ntrain both input gates and both forget gates in the network rather than\ntreating the two streams as separate entities with no information about the\nother. We apply this end-to-end system to benchmark datasets (UCF-101 and\nHMDB-51) of human action recognition. Experiments show that on both datasets,\nour proposed method outperforms all existing ones that are based on LSTM and/or\nCNNs of similar model complexities.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 19:27:34 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Sun", "Lin", ""], ["Jia", "Kui", ""], ["Chen", "Kevin", ""], ["Yeung", "Dit Yan", ""], ["Shi", "Bertram E.", ""], ["Savarese", "Silvio", ""]]}, {"id": "1708.03979", "submitter": "Mahyar Najibi", "authors": "Mahyar Najibi, Pouya Samangouei, Rama Chellappa, Larry Davis", "title": "SSH: Single Stage Headless Face Detector", "comments": "International Conference on Computer Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Single Stage Headless (SSH) face detector. Unlike two stage\nproposal-classification detectors, SSH detects faces in a single stage directly\nfrom the early convolutional layers in a classification network. SSH is\nheadless. That is, it is able to achieve state-of-the-art results while\nremoving the \"head\" of its underlying classification network -- i.e. all fully\nconnected layers in the VGG-16 which contains a large number of parameters.\nAdditionally, instead of relying on an image pyramid to detect faces with\nvarious scales, SSH is scale-invariant by design. We simultaneously detect\nfaces with different scales in a single forward pass of the network, but from\ndifferent layers. These properties make SSH fast and light-weight.\nSurprisingly, with a headless VGG-16, SSH beats the ResNet-101-based\nstate-of-the-art on the WIDER dataset. Even though, unlike the current\nstate-of-the-art, SSH does not use an image pyramid and is 5X faster. Moreover,\nif an image pyramid is deployed, our light-weight network achieves\nstate-of-the-art on all subsets of the WIDER dataset, improving the AP by 2.5%.\nSSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets\nwhile using a small input size, leading to a runtime of 50 ms/image on a GPU.\nThe code is available at https://github.com/mahyarnajibi/SSH.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 01:12:24 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 20:04:56 GMT"}, {"version": "v3", "created": "Wed, 18 Oct 2017 00:07:03 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Najibi", "Mahyar", ""], ["Samangouei", "Pouya", ""], ["Chellappa", "Rama", ""], ["Davis", "Larry", ""]]}, {"id": "1708.03985", "submitter": "Ali Mollahosseini", "authors": "Ali Mollahosseini, Behzad Hasani, Mohammad H. Mahoor", "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal\n  Computing in the Wild", "comments": "IEEE Transactions on Affective Computing, 2017", "journal-ref": null, "doi": "10.1109/TAFFC.2017.2740923", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated affective computing in the wild setting is a challenging problem in\ncomputer vision. Existing annotated databases of facial expressions in the wild\nare small and mostly cover discrete emotions (aka the categorical model). There\nare very limited annotated facial databases for affective computing in the\ncontinuous dimensional model (e.g., valence and arousal). To meet this need, we\ncollected, annotated, and prepared for public distribution a new database of\nfacial emotions in the wild (called AffectNet). AffectNet contains more than\n1,000,000 facial images from the Internet by querying three major search\nengines using 1250 emotion related keywords in six different languages. About\nhalf of the retrieved images were manually annotated for the presence of seven\ndiscrete facial expressions and the intensity of valence and arousal. AffectNet\nis by far the largest database of facial expression, valence, and arousal in\nthe wild enabling research in automated facial expression recognition in two\ndifferent emotion models. Two baseline deep neural networks are used to\nclassify images in the categorical model and predict the intensity of valence\nand arousal. Various evaluation metrics show that our deep neural network\nbaselines can perform better than conventional machine learning methods and\noff-the-shelf facial expression recognition systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 01:40:47 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 04:08:06 GMT"}, {"version": "v3", "created": "Thu, 5 Oct 2017 04:48:10 GMT"}, {"version": "v4", "created": "Mon, 9 Oct 2017 21:55:09 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Mollahosseini", "Ali", ""], ["Hasani", "Behzad", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1708.04006", "submitter": "Jiaolong Yang", "authors": "Chen Zhou, Jiaolong Yang, Chunshui Zhao, Gang Hua", "title": "Fast, Accurate Thin-Structure Obstacle Detection for Autonomous Mobile\n  Robots", "comments": "Appeared at IEEE CVPR 2017 Workshop on Embedded Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safety is paramount for mobile robotic platforms such as self-driving cars\nand unmanned aerial vehicles. This work is devoted to a task that is\nindispensable for safety yet was largely overlooked in the past -- detecting\nobstacles that are of very thin structures, such as wires, cables and tree\nbranches. This is a challenging problem, as thin objects can be problematic for\nactive sensors such as lidar and sonar and even for stereo cameras. In this\nwork, we propose to use video sequences for thin obstacle detection. We\nrepresent obstacles with edges in the video frames, and reconstruct them in 3D\nusing efficient edge-based visual odometry techniques. We provide both a\nmonocular camera solution and a stereo camera solution. The former incorporates\nInertial Measurement Unit (IMU) data to solve scale ambiguity, while the latter\nenjoys a novel, purely vision-based solution. Experiments demonstrated that the\nproposed methods are fast and able to detect thin obstacles robustly and\naccurately under various conditions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 04:35:04 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Zhou", "Chen", ""], ["Yang", "Jiaolong", ""], ["Zhao", "Chunshui", ""], ["Hua", "Gang", ""]]}, {"id": "1708.04014", "submitter": "Jamie Seol", "authors": "Hanbit Lee, Jinseok Seol, Sang-goo Lee", "title": "Style2Vec: Representation Learning for Fashion Items from Style Sets", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of online fashion market, demand for effective fashion\nrecommendation systems has never been greater. In fashion recommendation, the\nability to find items that goes well with a few other items based on style is\nmore important than picking a single item based on the user's entire purchase\nhistory. Since the same user may have purchased dress suits in one month and\ncasual denims in another, it is impossible to learn the latent style features\nof those items using only the user ratings. If we were able to represent the\nstyle features of fashion items in a reasonable way, we will be able to\nrecommend new items that conform to some small subset of pre-purchased items\nthat make up a coherent style set. We propose Style2Vec, a vector\nrepresentation model for fashion items. Based on the intuition of\ndistributional semantics used in word embeddings, Style2Vec learns the\nrepresentation of a fashion item using other items in matching outfits as\ncontext. Two different convolutional neural networks are trained to maximize\nthe probability of item co-occurrences. For evaluation, a fashion analogy test\nis conducted to show that the resulting representation connotes diverse fashion\nrelated semantics like shapes, colors, patterns and even latent styles. We also\nperform style classification using Style2Vec features and show that our method\noutperforms other baselines.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 05:22:54 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Lee", "Hanbit", ""], ["Seol", "Jinseok", ""], ["Lee", "Sang-goo", ""]]}, {"id": "1708.04069", "submitter": "Elhocine Boutellaa", "authors": "Elhocine Boutellaa, Miguel Bordallo L\\'opez, Samy Ait-Aoudia, Xiaoyi\n  Feng, and Abdenour Hadid", "title": "Kinship Verification from Videos using Spatio-Temporal Texture Features\n  and Deep Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic kinship verification using facial images is a relatively new and\nchallenging research problem in computer vision. It consists in automatically\npredicting whether two persons have a biological kin relation by examining\ntheir facial attributes. While most of the existing works extract shallow\nhandcrafted features from still face images, we approach this problem from\nspatio-temporal point of view and explore the use of both shallow texture\nfeatures and deep features for characterizing faces. Promising results,\nespecially those of deep features, are obtained on the benchmark UvA-NEMO Smile\ndatabase. Our extensive experiments also show the superiority of using videos\nover still images, hence pointing out the important role of facial dynamics in\nkinship verification. Furthermore, the fusion of the two types of features\n(i.e. shallow spatio-temporal texture features and deep features) shows\nsignificant performance improvements compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 10:41:33 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Boutellaa", "Elhocine", ""], ["L\u00f3pez", "Miguel Bordallo", ""], ["Ait-Aoudia", "Samy", ""], ["Feng", "Xiaoyi", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1708.04099", "submitter": "Steffen Schneider", "authors": "Daniel Bug, Steffen Schneider, Anne Grote, Eva Oswald, Friedrich\n  Feuerhake, Julia Sch\\\"uler, Dorit Merhof", "title": "Context-based Normalization of Histological Stains using Deep\n  Convolutional Features", "comments": "In: 3rd Workshop on Deep Learning in Medical Image Analysis (DLMIA\n  2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While human observers are able to cope with variations in color and\nappearance of histological stains, digital pathology algorithms commonly\nrequire a well-normalized setting to achieve peak performance, especially when\na limited amount of labeled data is available. This work provides a fully\nautomated, end-to-end learning-based setup for normalizing histological stains,\nwhich considers the texture context of the tissue. We introduce Feature Aware\nNormalization, which extends the framework of batch normalization in\ncombination with gating elements from Long Short-Term Memory units for\nnormalization among different spatial regions of interest. By incorporating a\npretrained deep neural network as a feature extractor steering a pixelwise\nprocessing pipeline, we achieve excellent normalization results and ensure a\nconsistent representation of color and texture. The evaluation comprises a\ncomparison of color histogram deviations, structural similarity and measures\nthe color volume obtained by the different methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 12:40:23 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Bug", "Daniel", ""], ["Schneider", "Steffen", ""], ["Grote", "Anne", ""], ["Oswald", "Eva", ""], ["Feuerhake", "Friedrich", ""], ["Sch\u00fcler", "Julia", ""], ["Merhof", "Dorit", ""]]}, {"id": "1708.04146", "submitter": "Michel Melo Silva", "authors": "Michel Melo Silva, Washington Luis Souza Ramos, Joao Pedro Klock\n  Ferreira, Mario Fernando Montenegro Campos, Erickson Rangel Nascimento", "title": "Towards Semantic Fast-Forward and Stabilized Egocentric Videos", "comments": "Accepted for publication and presented in the First International\n  Workshop on Egocentric Perception, Interaction and Computing at European\n  Conference on Computer Vision (EPIC@ECCV) 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46604-0_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of low-cost personal mobiles devices and wearable cameras and\nthe increasing storage capacity of video-sharing websites have pushed forward a\ngrowing interest towards first-person videos. Since most of the recorded videos\ncompose long-running streams with unedited content, they are tedious and\nunpleasant to watch. The fast-forward state-of-the-art methods are facing\nchallenges of balancing the smoothness of the video and the emphasis in the\nrelevant frames given a speed-up rate. In this work, we present a methodology\ncapable of summarizing and stabilizing egocentric videos by extracting the\nsemantic information from the frames. This paper also describes a dataset\ncollection with several semantically labeled videos and introduces a new\nsmoothness evaluation metric for egocentric videos that is used to test our\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 14:32:11 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 13:36:34 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Silva", "Michel Melo", ""], ["Ramos", "Washington Luis Souza", ""], ["Ferreira", "Joao Pedro Klock", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "1708.04150", "submitter": "Jingkuan Song Dr.", "authors": "Jingkuan Song", "title": "Binary Generative Adversarial Networks for Image Retrieval", "comments": "arXiv admin note: text overlap with arXiv:1702.00758 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most striking successes in image retrieval using deep hashing have mostly\ninvolved discriminative models, which require labels. In this paper, we use\nbinary generative adversarial networks (BGAN) to embed images to binary codes\nin an unsupervised way. By restricting the input noise variable of generative\nadversarial networks (GAN) to be binary and conditioned on the features of each\ninput image, BGAN can simultaneously learn a binary representation per image,\nand generate an image plausibly similar to the original one. In the proposed\nframework, we address two main problems: 1) how to directly generate binary\ncodes without relaxation? 2) how to equip the binary representation with the\nability of accurate image retrieval? We resolve these problems by proposing new\nsign-activation strategy and a loss function steering the learning process,\nwhich consists of new models for adversarial loss, a content loss, and a\nneighborhood structure loss. Experimental results on standard datasets\n(CIFAR-10, NUSWIDE, and Flickr) demonstrate that our BGAN significantly\noutperforms existing hashing methods by up to 107\\% in terms of~mAP (See Table\ntab.res.map.comp) Our anonymous code is available at:\nhttps://github.com/htconquer/BGAN.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 14:02:40 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Song", "Jingkuan", ""]]}, {"id": "1708.04160", "submitter": "Michel Melo Silva", "authors": "Washington Luis Souza Ramos, Michel Melo Silva, Mario Fernando\n  Montenegro Campos and Erickson Rangel Nascimento", "title": "Fast-Forward Video Based on Semantic Extraction", "comments": "Accepted for publication and presented in 2016 IEEE International\n  Conference on Image Processing (ICIP)", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532977", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the low operational cost and large storage capacity of smartphones\nand wearable devices, people are recording many hours of daily activities,\nsport actions and home videos. These videos, also known as egocentric videos,\nare generally long-running streams with unedited content, which make them\nboring and visually unpalatable, bringing up the challenge to make egocentric\nvideos more appealing. In this work we propose a novel methodology to compose\nthe new fast-forward video by selecting frames based on semantic information\nextracted from images. The experiments show that our approach outperforms the\nstate-of-the-art as far as semantic information is concerned and that it is\nalso able to produce videos that are more pleasant to be watched.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 14:55:22 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 13:11:43 GMT"}, {"version": "v3", "created": "Wed, 16 Aug 2017 13:34:56 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ramos", "Washington Luis Souza", ""], ["Silva", "Michel Melo", ""], ["Campos", "Mario Fernando Montenegro", ""], ["Nascimento", "Erickson Rangel", ""]]}, {"id": "1708.04169", "submitter": "Zhichao Zhou", "authors": "Rui Yu, Zhichao Zhou, Song Bai, Xiang Bai", "title": "Divide and Fuse: A Re-ranking Approach for Person Re-identification", "comments": "Accepted by BMVC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As re-ranking is a necessary procedure to boost person re-identification\n(re-ID) performance on large-scale datasets, the diversity of feature becomes\ncrucial to person reID for its importance both on designing pedestrian\ndescriptions and re-ranking based on feature fusion. However, in many\ncircumstances, only one type of pedestrian feature is available. In this paper,\nwe propose a \"Divide and use\" re-ranking framework for person re-ID. It\nexploits the diversity from different parts of a high-dimensional feature\nvector for fusion-based re-ranking, while no other features are accessible.\nSpecifically, given an image, the extracted feature is divided into\nsub-features. Then the contextual information of each sub-feature is\niteratively encoded into a new feature. Finally, the new features from the same\nimage are fused into one vector for re-ranking. Experimental results on two\nperson re-ID benchmarks demonstrate the effectiveness of the proposed\nframework. Especially, our method outperforms the state-of-the-art on the\nMarket-1501 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 03:41:26 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yu", "Rui", ""], ["Zhou", "Zhichao", ""], ["Bai", "Song", ""], ["Bai", "Xiang", ""]]}, {"id": "1708.04181", "submitter": "Canyi Lu", "authors": "Canyi Lu, Jiashi Feng, Yudong Chen, Wei Liu, Zhouchen Lin, Shuicheng\n  Yan", "title": "Tensor Robust Principal Component Analysis: Exact Recovery of Corrupted\n  Low-Rank Tensors via Convex Optimization", "comments": "IEEE International Conference on Computer Vision and Pattern\n  Recognition (CVPR, 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the Tensor Robust Principal Component (TRPCA) problem\nwhich extends the known Robust PCA (Candes et al. 2011) to the tensor case. Our\nmodel is based on a new tensor Singular Value Decomposition (t-SVD) (Kilmer and\nMartin 2011) and its induced tensor tubal rank and tensor nuclear norm.\nConsider that we have a 3-way tensor ${\\mathcal{X}}\\in\\mathbb{R}^{n_1\\times\nn_2\\times n_3}$ such that ${\\mathcal{X}}={\\mathcal{L}}_0+{\\mathcal{E}}_0$,\nwhere ${\\mathcal{L}}_0$ has low tubal rank and ${\\mathcal{E}}_0$ is sparse. Is\nthat possible to recover both components? In this work, we prove that under\ncertain suitable assumptions, we can recover both the low-rank and the sparse\ncomponents exactly by simply solving a convex program whose objective is a\nweighted combination of the tensor nuclear norm and the $\\ell_1$-norm, i.e.,\n$\\min_{{\\mathcal{L}},\\ {\\mathcal{E}}} \\\n\\|{{\\mathcal{L}}}\\|_*+\\lambda\\|{{\\mathcal{E}}}\\|_1, \\ \\text{s.t.} \\\n{\\mathcal{X}}={\\mathcal{L}}+{\\mathcal{E}}$, where $\\lambda=\n{1}/{\\sqrt{\\max(n_1,n_2)n_3}}$. Interestingly, TRPCA involves RPCA as a special\ncase when $n_3=1$ and thus it is a simple and elegant tensor extension of RPCA.\nAlso numerical experiments verify our theory and the application for the image\ndenoising demonstrates the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 15:42:05 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 01:55:55 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 21:22:35 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Lu", "Canyi", ""], ["Feng", "Jiashi", ""], ["Chen", "Yudong", ""], ["Liu", "Wei", ""], ["Lin", "Zhouchen", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1708.04208", "submitter": "Patrick Wieschollek", "authors": "Patrick Wieschollek, Michael Hirsch, Bernhard Sch\\\"olkopf, Hendrik\n  P.A. Lensch", "title": "Learning Blind Motion Deblurring", "comments": "International Conference on Computer Vision (ICCV) (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As handheld video cameras are now commonplace and available in every\nsmartphone, images and videos can be recorded almost everywhere at anytime.\nHowever, taking a quick shot frequently yields a blurry result due to unwanted\ncamera shake during recording or moving objects in the scene. Removing these\nartifacts from the blurry recordings is a highly ill-posed problem as neither\nthe sharp image nor the motion blur kernel is known. Propagating information\nbetween multiple consecutive blurry observations can help restore the desired\nsharp image or video. Solutions for blind deconvolution based on neural\nnetworks rely on a massive amount of ground-truth data which is hard to\nacquire. In this work, we propose an efficient approach to produce a\nsignificant amount of realistic training data and introduce a novel recurrent\nnetwork architecture to deblur frames taking temporal information into account,\nwhich can efficiently handle arbitrary spatial and temporal input sizes. We\ndemonstrate the versatility of our approach in a comprehensive comparison on a\nnumber of challening real-world examples.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:03:53 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Wieschollek", "Patrick", ""], ["Hirsch", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1708.04225", "submitter": "Coline Devin", "authors": "Coline Devin, Pieter Abbeel, Trevor Darrell, Sergey Levine", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation in complex open-world scenarios requires both reliable\nphysical manipulation skills and effective and generalizable perception. In\nthis paper, we propose a method where general purpose pretrained visual models\nserve as an object-centric prior for the perception system of a learned policy.\nWe devise an object-level attentional mechanism that can be used to determine\nrelevant objects from a few trajectories or demonstrations, and then\nimmediately incorporate those objects into a learned policy. A task-independent\nmeta-attention locates possible objects in the scene, and a task-specific\nattention identifies which objects are predictive of the trajectories. The\nscope of the task-specific attention is easily adjusted by showing\ndemonstrations with distractor objects or with diverse relevant objects. Our\nresults indicate that this approach exhibits good generalization across object\ninstances using very few samples, and can be used to learn a variety of\nmanipulation tasks using reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:42:59 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 00:14:15 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 17:06:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Devin", "Coline", ""], ["Abbeel", "Pieter", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""]]}, {"id": "1708.04232", "submitter": "Arash Rahnama", "authors": "Arash Rahnama, Abdullah Alchihabi, Vijay Gupta, Panos Antsaklis, Fatos\n  T. Yarman Vural", "title": "Encoding Multi-Resolution Brain Networks Using Unsupervised Deep\n  Learning", "comments": "6 pages, 3 figures, submitted to The 17th annual IEEE International\n  Conference on BioInformatics and BioEngineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this study is to extract a set of brain networks in multiple\ntime-resolutions to analyze the connectivity patterns among the anatomic\nregions for a given cognitive task. We suggest a deep architecture which learns\nthe natural groupings of the connectivity patterns of human brain in multiple\ntime-resolutions. The suggested architecture is tested on task data set of\nHuman Connectome Project (HCP) where we extract multi-resolution networks, each\nof which corresponds to a cognitive task. At the first level of this\narchitecture, we decompose the fMRI signal into multiple sub-bands using\nwavelet decompositions. At the second level, for each sub-band, we estimate a\nbrain network extracted from short time windows of the fMRI signal. At the\nthird level, we feed the adjacency matrices of each mesh network at each\ntime-resolution into an unsupervised deep learning algorithm, namely, a Stacked\nDe- noising Auto-Encoder (SDAE). The outputs of the SDAE provide a compact\nconnectivity representation for each time window at each sub-band of the fMRI\nsignal. We concatenate the learned representations of all sub-bands at each\nwindow and cluster them by a hierarchical algorithm to find the natural\ngroupings among the windows. We observe that each cluster represents a\ncognitive task with a performance of 93% Rand Index and 71% Adjusted Rand\nIndex. We visualize the mean values and the precisions of the networks at each\ncomponent of the cluster mixture. The mean brain networks at cluster centers\nshow the variations among cognitive tasks and the precision of each cluster\nshows the within cluster variability of networks, across the subjects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 01:43:11 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Rahnama", "Arash", ""], ["Alchihabi", "Abdullah", ""], ["Gupta", "Vijay", ""], ["Antsaklis", "Panos", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1708.04301", "submitter": "Hossein Hosseini", "authors": "Hossein Hosseini, Baicen Xiao, Andrew Clark and Radha Poovendran", "title": "Attacking Automatic Video Analysis Algorithms: A Case Study of Google\n  Cloud Video Intelligence API", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growth of video data on Internet, automatic video analysis has\ngained a lot of attention from academia as well as companies such as Facebook,\nTwitter and Google. In this paper, we examine the robustness of video analysis\nalgorithms in adversarial settings. Specifically, we propose targeted attacks\non two fundamental classes of video analysis algorithms, namely video\nclassification and shot detection. We show that an adversary can subtly\nmanipulate a video in such a way that a human observer would perceive the\ncontent of the original video, but the video analysis algorithm will return the\nadversary's desired outputs.\n  We then apply the attacks on the recently released Google Cloud Video\nIntelligence API. The API takes a video file and returns the video labels\n(objects within the video), shot changes (scene changes within the video) and\nshot labels (description of video events over time). Through experiments, we\nshow that the API generates video and shot labels by processing only the first\nframe of every second of the video. Hence, an adversary can deceive the API to\noutput only her desired video and shot labels by periodically inserting an\nimage into the video at the rate of one frame per second. We also show that the\npattern of shot changes returned by the API can be mostly recovered by an\nalgorithm that compares the histograms of consecutive frames. Based on our\nequivalent model, we develop a method for slightly modifying the video frames,\nin order to deceive the API into generating our desired pattern of shot\nchanges. We perform extensive experiments with different videos and show that\nour attacks are consistently successful across videos with different\ncharacteristics. At the end, we propose introducing randomness to video\nanalysis algorithms as a countermeasure to our attacks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:10:04 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Hosseini", "Hossein", ""], ["Xiao", "Baicen", ""], ["Clark", "Andrew", ""], ["Poovendran", "Radha", ""]]}, {"id": "1708.04308", "submitter": "Yuxin Peng", "authors": "Xin Huang, Yuxin Peng and Mingkuan Yuan", "title": "MHTN: Modal-adversarial Hybrid Transfer Network for Cross-modal\n  Retrieval", "comments": "12 pages, submitted to IEEE Transactions on Cybernetics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval has drawn wide interest for retrieval across different\nmodalities of data. However, existing methods based on DNN face the challenge\nof insufficient cross-modal training data, which limits the training\neffectiveness and easily leads to overfitting. Transfer learning is for\nrelieving the problem of insufficient training data, but it mainly focuses on\nknowledge transfer only from large-scale datasets as single-modal source domain\nto single-modal target domain. Such large-scale single-modal datasets also\ncontain rich modal-independent semantic knowledge that can be shared across\ndifferent modalities. Besides, large-scale cross-modal datasets are very\nlabor-consuming to collect and label, so it is significant to fully exploit the\nknowledge in single-modal datasets for boosting cross-modal retrieval. This\npaper proposes modal-adversarial hybrid transfer network (MHTN), which to the\nbest of our knowledge is the first work to realize knowledge transfer from\nsingle-modal source domain to cross-modal target domain, and learn cross-modal\ncommon representation. It is an end-to-end architecture with two subnetworks:\n(1) Modal-sharing knowledge transfer subnetwork is proposed to jointly transfer\nknowledge from a large-scale single-modal dataset in source domain to all\nmodalities in target domain with a star network structure, which distills\nmodal-independent supplementary knowledge for promoting cross-modal common\nrepresentation learning. (2) Modal-adversarial semantic learning subnetwork is\nproposed to construct an adversarial training mechanism between common\nrepresentation generator and modality discriminator, making the common\nrepresentation discriminative for semantics but indiscriminative for modalities\nto enhance cross-modal semantic consistency during transfer process.\nComprehensive experiments on 4 widely-used datasets show its effectiveness and\ngenerality.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 07:50:52 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Huang", "Xin", ""], ["Peng", "Yuxin", ""], ["Yuan", "Mingkuan", ""]]}, {"id": "1708.04317", "submitter": "Tianyang Wang", "authors": "Tianyang Wang, Zhengrui Qin, Michelle Zhu", "title": "An ELU Network with Total Variation for Image Denoising", "comments": "10 pages, Accepted by the 24th International Conference on Neural\n  Information Processing (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel convolutional neural network (CNN) for\nimage denoising, which uses exponential linear unit (ELU) as the activation\nfunction. We investigate the suitability by analyzing ELU's connection with\ntrainable nonlinear reaction diffusion model (TNRD) and residual denoising. On\nthe other hand, batch normalization (BN) is indispensable for residual\ndenoising and convergence purpose. However, direct stacking of BN and ELU\ndegrades the performance of CNN. To mitigate this issue, we design an\ninnovative combination of activation layer and normalization layer to exploit\nand leverage the ELU network, and discuss the corresponding rationale.\nMoreover, inspired by the fact that minimizing total variation (TV) can be\napplied to image denoising, we propose a TV regularized L2 loss to evaluate the\ntraining effect during the iterations. Finally, we conduct extensive\nexperiments, showing that our model outperforms some recent and popular\napproaches on Gaussian denoising with specific or randomized noise levels for\nboth gray and color images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:47:35 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Wang", "Tianyang", ""], ["Qin", "Zhengrui", ""], ["Zhu", "Michelle", ""]]}, {"id": "1708.04320", "submitter": "Makarand Tapaswi", "authors": "Ruiyu Li, Makarand Tapaswi, Renjie Liao, Jiaya Jia, Raquel Urtasun,\n  Sanja Fidler", "title": "Situation Recognition with Graph Neural Networks", "comments": "ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of recognizing situations in images. Given an image,\nthe task is to predict the most salient verb (action), and fill its semantic\nroles such as who is performing the action, what is the source and target of\nthe action, etc. Different verbs have different roles (e.g. attacking has\nweapon), and each role can take on many possible values (nouns). We propose a\nmodel based on Graph Neural Networks that allows us to efficiently capture\njoint dependencies between roles using neural networks defined on a graph.\nExperiments with different graph connectivities show that our approach that\npropagates information between roles significantly outperforms existing work,\nas well as multiple baselines. We obtain roughly 3-5% improvement over previous\nwork in predicting the full situation. We also provide a thorough qualitative\nanalysis of our model and influence of different roles in the verbs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:51:05 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Li", "Ruiyu", ""], ["Tapaswi", "Makarand", ""], ["Liao", "Renjie", ""], ["Jia", "Jiaya", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1708.04347", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Shahrokh Valaee, Timothy Dowdell, Joseph Barfett", "title": "Image Augmentation using Radial Transform for Training Deep Neural\n  Networks", "comments": "This paper is accepted for presentation at IEEE International\n  Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models have a large number of free parameters that must be\nestimated by efficient training of the models on a large number of training\ndata samples to increase their generalization performance. In real-world\napplications, the data available to train these networks is often limited or\nimbalanced. We propose a sampling method based on the radial transform in a\npolar coordinate system for image augmentation to facilitate the training of\ndeep learning models from limited source data. This pixel-wise transform\nprovides representations of the original image in the polar coordinate system\nby generating a new image from each pixel. This technique can generate radial\ntransformed images up to the number of pixels in the original image to increase\nthe diversity of poorly represented image classes. Our experiments show\nimproved generalization performance in training deep convolutional neural\nnetworks with radial transformed images.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 22:35:35 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 13:29:24 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 16:14:01 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 15:58:46 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""], ["Dowdell", "Timothy", ""], ["Barfett", "Joseph", ""]]}, {"id": "1708.04366", "submitter": "Yuchao Dai Dr.", "authors": "Jing Zhang, Yuchao Dai, Fatih Porikli and Mingyi He", "title": "Deep Edge-Aware Saliency Detection", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been profound progress in visual saliency thanks to the deep\nlearning architectures, however, there still exist three major challenges that\nhinder the detection performance for scenes with complex compositions, multiple\nsalient objects, and salient objects of diverse scales. In particular, output\nmaps of the existing methods remain low in spatial resolution causing blurred\nedges due to the stride and pooling operations, networks often neglect\ndescriptive statistical and handcrafted priors that have potential to\ncomplement saliency detection results, and deep features at different layers\nstay mainly desolate waiting to be effectively fused to handle multi-scale\nsalient objects. In this paper, we tackle these issues by a new fully\nconvolutional neural network that jointly learns salient edges and saliency\nlabels in an end-to-end fashion. Our framework first employs convolutional\nlayers that reformulate the detection task as a dense labeling problem, then\nintegrates handcrafted saliency features in a hierarchical manner into lower\nand higher levels of the deep network to leverage available information for\nmulti-scale response, and finally refines the saliency map through dilated\nconvolutions by imposing context. In this way, the salient edge priors are\nefficiently incorporated and the output resolution is significantly improved\nwhile keeping the memory requirements low, leading to cleaner and sharper\nobject boundaries. Extensive experimental analyses on ten benchmarks\ndemonstrate that our framework achieves consistently superior performance and\nattains robustness for complex scenes in comparison to the very recent\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 00:35:55 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhang", "Jing", ""], ["Dai", "Yuchao", ""], ["Porikli", "Fatih", ""], ["He", "Mingyi", ""]]}, {"id": "1708.04370", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz and James M. Rehg", "title": "Dockerface: an Easy to Install and Use Faster R-CNN Face Detector in a\n  Docker Container", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face detection is a very important task and a necessary pre-processing step\nfor many applications such as facial landmark detection, pose estimation,\nsentiment analysis and face recognition. Not only is face detection an\nimportant pre-processing step in computer vision applications but also in\ncomputational psychology, behavioral imaging and other fields where researchers\nmight not be initiated in computer vision frameworks and state-of-the-art\ndetection applications. A large part of existing research that includes face\ndetection as a pre-processing step uses existing out-of-the-box detectors such\nas the HoG-based dlib and the OpenCV Haar face detector which are no longer\nstate-of-the-art - they are primarily used because of their ease of use and\naccessibility. We introduce Dockerface, a very accurate Faster R-CNN face\ndetector in a Docker container which requires no training and is easy to\ninstall and use.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 00:55:57 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 05:32:40 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Rehg", "James M.", ""]]}, {"id": "1708.04398", "submitter": "Suryansh Kumar", "authors": "Suryansh Kumar, Yuchao Dai, Hongdong Li", "title": "Monocular Dense 3D Reconstruction of a Complex Dynamic Scene from Two\n  Perspective Frames", "comments": "International Conference on Computer Vision (ICCV) 2017 pp: 4649-4657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for monocular dense 3D reconstruction of a\ncomplex dynamic scene from two perspective frames. By applying superpixel\nover-segmentation to the image, we model a generically dynamic (hence\nnon-rigid) scene with a piecewise planar and rigid approximation. In this way,\nwe reduce the dynamic reconstruction problem to a \"3D jigsaw puzzle\" problem\nwhich takes pieces from an unorganized \"soup of superpixels\". We show that our\nmethod provides an effective solution to the inherent relative scale ambiguity\nin structure-from-motion. Since our method does not assume a template prior, or\nper-object segmentation, or knowledge about the rigidity of the dynamic scene,\nit is applicable to a wide range of scenarios. Extensive experiments on both\nsynthetic and real monocular sequences demonstrate the superiority of our\nmethod compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 05:33:33 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 14:42:02 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Kumar", "Suryansh", ""], ["Dai", "Yuchao", ""], ["Li", "Hongdong", ""]]}, {"id": "1708.04400", "submitter": "Fatemeh Sadat Saleh", "authors": "Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann,\n  Lars Petersson, Jose M. Alvarez", "title": "Bringing Background into the Foreground: Making All Classes Equal in\n  Weakly-supervised Video Semantic Segmentation", "comments": "11 pages, 4 figures, 7 tables, Accepted in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pixel-level annotations are expensive and time-consuming to obtain. Hence,\nweak supervision using only image tags could have a significant impact in\nsemantic segmentation. Recent years have seen great progress in\nweakly-supervised semantic segmentation, whether from a single image or from\nvideos. However, most existing methods are designed to handle a single\nbackground class. In practical applications, such as autonomous navigation, it\nis often crucial to reason about multiple background classes. In this paper, we\nintroduce an approach to doing so by making use of classifier heatmaps. We then\ndevelop a two-stream deep architecture that jointly leverages appearance and\nmotion, and design a loss based on our heatmaps to train it. Our experiments\ndemonstrate the benefits of our classifier heatmaps and of our two-stream\narchitecture on challenging urban scene datasets and on the YouTube-Objects\nbenchmark, where we obtain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 05:36:52 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Saleh", "Fatemeh Sadat", ""], ["Aliakbarian", "Mohammad Sadegh", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1708.04432", "submitter": "Shan Luo Dr", "authors": "Shan Luo, Leqi Zhu, Kaspar Althoefer, Hongbin Liu", "title": "Knock-Knock: Acoustic Object Recognition by using Stacked Denoising\n  Autoencoders", "comments": "6 pages, 10 figures, Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2017.03.014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a successful application of deep learning for object\nrecognition based on acoustic data. The shortcomings of previously employed\napproaches where handcrafted features describing the acoustic data are being\nused, include limiting the capability of the found representation to be widely\napplicable and facing the risk of capturing only insignificant characteristics\nfor a task. In contrast, there is no need to define the feature representation\nformat when using multilayer/deep learning architecture methods: features can\nbe learned from raw sensor data without defining discriminative characteristics\na-priori. In this paper, stacked denoising autoencoders are applied to train a\ndeep learning model. Knocking each object in our test set 120 times with a\nmarker pen to obtain the auditory data, thirty different objects were\nsuccessfully classified in our experiment and each object was knocked 120 times\nby a marker pen to obtain the auditory data. By employing the proposed deep\nlearning framework, a high accuracy of 91.50% was achieved. A traditional\nmethod using handcrafted features with a shallow classifier was taken as a\nbenchmark and the attained recognition rate was only 58.22%. Interestingly, a\nrecognition rate of 82.00% was achieved when using a shallow classifier with\nraw acoustic data as input. In addition, we could show that the time taken to\nclassify one object using deep learning was far less (by a factor of more than\n6) than utilizing the traditional method. It was also explored how different\nmodel parameters in our deep architecture affect the recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 08:40:02 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Luo", "Shan", ""], ["Zhu", "Leqi", ""], ["Althoefer", "Kaspar", ""], ["Liu", "Hongbin", ""]]}, {"id": "1708.04483", "submitter": "Xin Li Mr.", "authors": "Xin Li, Zequn Jie, Jiashi Feng, Changsong Liu, Shuicheng Yan", "title": "Learning with Rethinking: Recurrently Improving Convolutional Neural\n  Networks through Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the great success of convolutional neural network\n(CNN) based models in the field of computer vision. CNN is able to learn\nhierarchically abstracted features from images in an end-to-end training\nmanner. However, most of the existing CNN models only learn features through a\nfeedforward structure and no feedback information from top to bottom layers is\nexploited to enable the networks to refine themselves. In this paper, we\npropose a \"Learning with Rethinking\" algorithm. By adding a feedback layer and\nproducing the emphasis vector, the model is able to recurrently boost the\nperformance based on previous prediction. Particularly, it can be employed to\nboost any pre-trained models. This algorithm is tested on four object\nclassification benchmark datasets: CIFAR-100, CIFAR-10, MNIST-background-image\nand ILSVRC-2012 dataset. These results have demonstrated the advantage of\ntraining CNN models with the proposed feedback mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 13:09:52 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Li", "Xin", ""], ["Jie", "Zequn", ""], ["Feng", "Jiashi", ""], ["Liu", "Changsong", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1708.04503", "submitter": "Ziyue Xu", "authors": "Kevin George, Adam P. Harrison, Dakai Jin, Ziyue Xu, Daniel J. Mollura", "title": "Pathological Pulmonary Lobe Segmentation from CT Images using\n  Progressive Holistically Nested Neural Networks and Random Walker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic pathological pulmonary lobe segmentation(PPLS) enables regional\nanalyses of lung disease, a clinically important capability. Due to often\nincomplete lobe boundaries, PPLS is difficult even for experts, and most prior\nart requires inference from contextual information. To address this, we propose\na novel PPLS method that couples deep learning with the random walker (RW)\nalgorithm. We first employ the recent progressive holistically-nested network\n(P-HNN) model to identify potential lobar boundaries, then generate final\nsegmentations using a RW that is seeded and weighted by the P-HNN output. We\nare the first to apply deep learning to PPLS. The advantages are independence\nfrom prior airway/vessel segmentations, increased robustness in diseased lungs,\nand methodological simplicity that does not sacrifice accuracy. Our method\nposts a high mean Jaccard score of 0.888$\\pm$0.164 on a held-out set of 154 CT\nscans from lung-disease patients, while also significantly (p < 0.001)\noutperforming a state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:11:26 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["George", "Kevin", ""], ["Harrison", "Adam P.", ""], ["Jin", "Dakai", ""], ["Xu", "Ziyue", ""], ["Mollura", "Daniel J.", ""]]}, {"id": "1708.04512", "submitter": "Yun-Fu Liu", "authors": "Yun-Fu Liu, Da-Wei Jaw, Shih-Chia Huang, Jenq-Neng Hwang", "title": "DesnowNet: Context-Aware Deep Network for Snow Removal", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2018.2806202", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing learning-based atmospheric particle-removal approaches such as those\nused for rainy and hazy images are designed with strong assumptions regarding\nspatial frequency, trajectory, and translucency. However, the removal of snow\nparticles is more complicated because it possess the additional attributes of\nparticle size and shape, and these attributes may vary within a single image.\nCurrently, hand-crafted features are still the mainstream for snow removal,\nmaking significant generalization difficult to achieve. In response, we have\ndesigned a multistage network codenamed DesnowNet to in turn deal with the\nremoval of translucent and opaque snow particles. We also differentiate snow\ninto attributes of translucency and chromatic aberration for accurate\nestimation. Moreover, our approach individually estimates residual complements\nof the snow-free images to recover details obscured by opaque snow.\nAdditionally, a multi-scale design is utilized throughout the entire network to\nmodel the diversity of snow. As demonstrated in experimental results, our\napproach outperforms state-of-the-art learning-based atmospheric phenomena\nremoval methods and one semantic segmentation baseline on the proposed Snow100K\ndataset in both qualitative and quantitative comparisons. The results indicate\nour network would benefit applications involving computer vision and graphics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:33:52 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Liu", "Yun-Fu", ""], ["Jaw", "Da-Wei", ""], ["Huang", "Shih-Chia", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1708.04538", "submitter": "Manuel Ruder", "authors": "Manuel Ruder, Alexey Dosovitskiy, Thomas Brox", "title": "Artistic style transfer for videos and spherical images", "comments": "v3: added ref to conference. This paper is a successor of and\n  overlaps with arXiv:1604.08610, International Journal of Computer Vision\n  (IJCV), 2018", "journal-ref": null, "doi": "10.1007/s11263-018-1089-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually re-drawing an image in a certain artistic style takes a professional\nartist a long time. Doing this for a video sequence single-handedly is beyond\nimagination. We present two computational approaches that transfer the style\nfrom one image (for example, a painting) to a whole video sequence. In our\nfirst approach, we adapt to videos the original image style transfer technique\nby Gatys et al. based on energy minimization. We introduce new ways of\ninitialization and new loss functions to generate consistent and stable\nstylized video sequences even in cases with large motion and strong occlusion.\nOur second approach formulates video stylization as a learning problem. We\npropose a deep network architecture and training procedures that allow us to\nstylize arbitrary-length videos in a consistent and stable way, and nearly in\nreal time. We show that the proposed methods clearly outperform simpler\nbaselines both qualitatively and quantitatively. Finally, we propose a way to\nadapt these approaches also to 360 degree images and videos as they emerge with\nrecent virtual reality hardware.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 21:17:59 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 21:29:43 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2018 09:07:11 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Ruder", "Manuel", ""], ["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1708.04552", "submitter": "Terrance DeVries", "authors": "Terrance DeVries and Graham W. Taylor", "title": "Improved Regularization of Convolutional Neural Networks with Cutout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks are capable of learning powerful\nrepresentational spaces, which are necessary for tackling complex learning\ntasks. However, due to the model capacity required to capture such\nrepresentations, they are often susceptible to overfitting and therefore\nrequire proper regularization in order to generalize well. In this paper, we\nshow that the simple regularization technique of randomly masking out square\nregions of input during training, which we call cutout, can be used to improve\nthe robustness and overall performance of convolutional neural networks. Not\nonly is this method extremely easy to implement, but we also demonstrate that\nit can be used in conjunction with existing forms of data augmentation and\nother regularizers to further improve model performance. We evaluate this\nmethod by applying it to current state-of-the-art architectures on the\nCIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results\nof 2.56%, 15.20%, and 1.30% test error respectively. Code is available at\nhttps://github.com/uoguelph-mlrg/Cutout\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 15:21:53 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 14:51:40 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["DeVries", "Terrance", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1708.04607", "submitter": "Adam Harley", "authors": "Adam W. Harley, Konstantinos G. Derpanis, Iasonas Kokkinos", "title": "Segmentation-Aware Convolutional Networks Using Local Attention Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to integrate segmentation information within a\nconvolutional neural network (CNN). This counter-acts the tendency of CNNs to\nsmooth information across regions and increases their spatial precision. To\nobtain segmentation information, we set up a CNN to provide an embedding space\nwhere region co-membership can be estimated based on Euclidean distance. We use\nthese embeddings to compute a local attention mask relative to every neuron\nposition. We incorporate such masks in CNNs and replace the convolution\noperation with a \"segmentation-aware\" variant that allows a neuron to\nselectively attend to inputs coming from its own region. We call the resulting\nnetwork a segmentation-aware CNN because it adapts its filters at each image\npoint according to local segmentation cues. We demonstrate the merit of our\nmethod on two widely different dense prediction tasks, that involve\nclassification (semantic segmentation) and regression (optical flow). Our\nresults show that in semantic segmentation we can match the performance of\nDenseCRFs while being faster and simpler, and in optical flow we obtain clearly\nsharper responses than networks that do not use local attention masks. In both\ncases, segmentation-aware convolution yields systematic improvements over\nstrong baselines. Source code for this work is available online at\nhttp://cs.cmu.edu/~aharley/segaware.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 17:55:36 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Harley", "Adam W.", ""], ["Derpanis", "Konstantinos G.", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1708.04664", "submitter": "Suraiya Jabin", "authors": "Farhana Javed Zareen, and Suraiya Jabin", "title": "A Novel data Pre-processing method for multi-dimensional and non-uniform\n  data", "comments": "11 pages, 4 Figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are in the era of data analytics and data science which is on full bloom.\nThere is abundance of all kinds of data for example biometrics based data,\nsatellite images data, chip-seq data, social network data, sensor based data\netc. from a variety of sources. This data abundance is the result of the fact\nthat storage cost is getting cheaper day by day, so people as well as almost\nall business or scientific organizations are storing more and more data. Most\nof the real data is multi-dimensional, non-uniform, and big in size, such that\nit requires a unique pre-processing before analyzing it. In order to make data\nuseful for any kind of analysis, pre-processing is a very important step. This\npaper presents a unique and novel pre-processing method for multi-dimensional\nand non-uniform data with the aim of making it uniform and reduced in size\nwithout losing much of its value. We have chosen biometric signature data to\ndemonstrate the proposed method as it qualifies for the attributes of being\nmulti-dimensional, non-uniform and big in size. Biometric signature data does\nnot only captures the structural characteristics of a signature but also its\nbehavioral characteristics that are captured using a dynamic signature capture\ndevice. These features like pen pressure, pen tilt angle, time taken to sign a\ndocument when collected in real-time turn out to be of varying dimensions. This\nfeature data set along with the structural data needs to be pre-processed in\norder to use it to train a machine learning based model for signature\nverification purposes. We demonstrate the success of the proposed method over\nother methods using experimental results for biometric signature data but the\nsame can be implemented for any other data with similar properties from a\ndifferent domain.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 13:15:36 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zareen", "Farhana Javed", ""], ["Jabin", "Suraiya", ""]]}, {"id": "1708.04669", "submitter": "Suhas Lohit", "authors": "Suhas Lohit, Kuldeep Kulkarni, Ronan Kerviche, Pavan Turaga and Amit\n  Ashok", "title": "Convolutional Neural Networks for Non-iterative Reconstruction of\n  Compressively Sensed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional algorithms for compressive sensing recovery are computationally\nexpensive and are ineffective at low measurement rates. In this work, we\npropose a data driven non-iterative algorithm to overcome the shortcomings of\nearlier iterative algorithms. Our solution, ReconNet, is a deep neural network,\nwhose parameters are learned end-to-end to map block-wise compressive\nmeasurements of the scene to the desired image blocks. Reconstruction of an\nimage becomes a simple forward pass through the network and can be done in\nreal-time. We show empirically that our algorithm yields reconstructions with\nhigher PSNRs compared to iterative algorithms at low measurement rates and in\npresence of measurement noise. We also propose a variant of ReconNet which uses\nadversarial loss in order to further improve reconstruction quality. We discuss\nhow adding a fully connected layer to the existing ReconNet architecture allows\nfor jointly learning the measurement matrix and the reconstruction algorithm in\na single network. Experiments on real data obtained from a block compressive\nimager show that our networks are robust to unseen sensor noise. Finally,\nthrough an experiment in object tracking, we show that even at very low\nmeasurement rates, reconstructions using our algorithm possess rich semantic\ncontent that can be used for high level inference.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 20:14:17 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 00:18:44 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Lohit", "Suhas", ""], ["Kulkarni", "Kuldeep", ""], ["Kerviche", "Ronan", ""], ["Turaga", "Pavan", ""], ["Ashok", "Amit", ""]]}, {"id": "1708.04670", "submitter": "Dianbo Liu Mr", "authors": "Dianbo Liu, Fengjiao Peng, Andrew Shea, Ognjen (Oggi) Rudovic,\n  Rosalind Picard", "title": "DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation\n  of Self-Reported Pain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on automatic pain estimation from facial expressions has\nfocused primarily on \"one-size-fits-all\" metrics (such as PSPI). In this work,\nwe focus on directly estimating each individual's self-reported visual-analog\nscale (VAS) pain metric, as this is considered the gold standard for pain\nmeasurement. The VAS pain score is highly subjective and context-dependent, and\nits range can vary significantly among different persons. To tackle these\nissues, we propose a novel two-stage personalized model, named DeepFaceLIFT,\nfor automatic estimation of VAS. This model is based on (1) Neural Network and\n(2) Gaussian process regression models, and is used to personalize the\nestimation of self-reported pain via a set of hand-crafted personal features\nand multi-task learning. We show on the benchmark dataset for pain analysis\n(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed\npersonalized model largely outperforms the traditional, unpersonalized models:\nthe intra-class correlation improves from a baseline performance of 19\\% to a\npersonalized performance of 35\\% while also providing confidence in the\nmodel\\textquotesingle s estimates -- in contrast to existing models for the\ntarget task. Additionally, DeepFaceLIFT automatically discovers the\npain-relevant facial regions for each person, allowing for an easy\ninterpretation of the pain-related facial cues.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:07:45 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Liu", "Dianbo", "", "Oggi"], ["Peng", "Fengjiao", "", "Oggi"], ["Shea", "Andrew", "", "Oggi"], ["Ognjen", "", "", "Oggi"], ["Rudovic", "", ""], ["Picard", "Rosalind", ""]]}, {"id": "1708.04671", "submitter": "Karel Driesen", "authors": "Yasuhisa Fujii, Karel Driesen, Jonathan Baccash, Ash Hurst, Ashok C.\n  Popat", "title": "Sequence-to-Label Script Identification for Multilingual OCR", "comments": "ICDAR2017, The 14th IAPR International Conference on Document\n  Analysis and Recognition, Kyoto, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a novel line-level script identification method. Previous work\nrepurposed an OCR model generating per-character script codes, counted to\nobtain line-level script identification. This has two shortcomings. First, as a\nsequence-to-sequence model it is more complex than necessary for the\nsequence-to-label problem of line script identification. This makes it harder\nto train and inefficient to run. Second, the counting heuristic may be\nsuboptimal compared to a learned model. Therefore we reframe line script\nidentification as a sequence-to-label problem and solve it using two\ncomponents, trained end-toend: Encoder and Summarizer. The encoder converts a\nline image into a feature sequence. The summarizer aggregates the sequence to\nclassify the line. We test various summarizers with identical inception-style\nconvolutional networks as encoders. Experiments on scanned books and photos\ncontaining 232 languages in 30 scripts show 16% reduction of script\nidentification error rate compared to the baseline. This improved script\nidentification reduces the character error rate attributable to script\nmisidentification by 33%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 20:14:51 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 20:20:25 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Fujii", "Yasuhisa", ""], ["Driesen", "Karel", ""], ["Baccash", "Jonathan", ""], ["Hurst", "Ash", ""], ["Popat", "Ashok C.", ""]]}, {"id": "1708.04672", "submitter": "Andrey Kurenkov", "authors": "Andrey Kurenkov, Jingwei Ji, Animesh Garg, Viraj Mehta, JunYoung Gwak,\n  Christopher Choy, Silvio Savarese", "title": "DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction\n  from a Single Image", "comments": "11 pages, 9 figures, NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from a single image is a key problem in multiple\napplications ranging from robotic manipulation to augmented reality. Prior\nmethods have tackled this problem through generative models which predict 3D\nreconstructions as voxels or point clouds. However, these methods can be\ncomputationally expensive and miss fine details. We introduce a new\ndifferentiable layer for 3D data deformation and use it in DeformNet to learn a\nmodel for 3D reconstruction-through-deformation. DeformNet takes an image\ninput, searches the nearest shape template from a database, and deforms the\ntemplate to match the query image. We evaluate our approach on the ShapeNet\ndataset and show that - (a) the Free-Form Deformation layer is a powerful new\nbuilding block for Deep Learning models that manipulate 3D data (b) DeformNet\nuses this FFD layer combined with shape retrieval for smooth and\ndetail-preserving 3D reconstruction of qualitatively plausible point clouds\nwith respect to a single query image (c) compared to other state-of-the-art 3D\nreconstruction methods, DeformNet quantitatively matches or outperforms their\nbenchmarks by significant margins. For more information, visit:\nhttps://deformnet-site.github.io/DeformNet-website/ .\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:43:19 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Kurenkov", "Andrey", ""], ["Ji", "Jingwei", ""], ["Garg", "Animesh", ""], ["Mehta", "Viraj", ""], ["Gwak", "JunYoung", ""], ["Choy", "Christopher", ""], ["Savarese", "Silvio", ""]]}, {"id": "1708.04673", "submitter": "Qingming Tang", "authors": "Qingming Tang, Weiran Wang and Karen Livescu", "title": "Acoustic Feature Learning via Deep Variational Canonical Correlation\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of acoustic feature learning in the setting where we\nhave access to another (non-acoustic) modality for feature learning but not at\ntest time. We use deep variational canonical correlation analysis (VCCA), a\nrecently proposed deep generative method for multi-view representation\nlearning. We also extend VCCA with improved latent variable priors and with\nadversarial learning. Compared to other techniques for multi-view feature\nlearning, VCCA's advantages include an intuitive latent variable interpretation\nand a variational lower bound objective that can be trained end-to-end\nefficiently. We compare VCCA and its extensions with previous feature learning\nmethods on the University of Wisconsin X-ray Microbeam Database, and show that\nVCCA-based feature learning improves over previous methods for\nspeaker-independent phonetic recognition.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 03:14:44 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 06:30:12 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Tang", "Qingming", ""], ["Wang", "Weiran", ""], ["Livescu", "Karen", ""]]}, {"id": "1708.04675", "submitter": "Ruoyu Li", "authors": "Ruoyu Li, Junzhou Huang", "title": "Learning Graph While Training: An Evolving Graph Convolutional Neural\n  Network", "comments": "10 pages, submitted to NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution Neural Networks on Graphs are important generalization and\nextension of classical CNNs. While previous works generally assumed that the\ngraph structures of samples are regular with unified dimensions, in many\napplications, they are highly diverse or even not well defined. Under some\ncircumstances, e.g. chemical molecular data, clustering or coarsening for\nsimplifying the graphs is hard to be justified chemically. In this paper, we\npropose a more general and flexible graph convolution network (EGCN) fed by\nbatch of arbitrarily shaped data together with their evolving graph Laplacians\ntrained in supervised fashion. Extensive experiments have been conducted to\ndemonstrate the superior performance in terms of both the acceleration of\nparameter fitting and the significantly improved prediction accuracy on\nmultiple graph-structured datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 01:55:01 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Li", "Ruoyu", ""], ["Huang", "Junzhou", ""]]}, {"id": "1708.04680", "submitter": "Marcus Bloice", "authors": "Marcus D. Bloice, Christof Stocker, Andreas Holzinger", "title": "Augmentor: An Image Augmentation Library for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of artificial data based on existing observations, known as\ndata augmentation, is a technique used in machine learning to improve model\naccuracy, generalisation, and to control overfitting. Augmentor is a software\npackage, available in both Python and Julia versions, that provides a high\nlevel API for the expansion of image data using a stochastic, pipeline-based\napproach which effectively allows for images to be sampled from a distribution\nof augmented images at runtime. Augmentor provides methods for most standard\naugmentation practices as well as several advanced features such as\nlabel-preserving, randomised elastic distortions, and provides many helper\nfunctions for typical augmentation tasks used in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 11:19:44 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bloice", "Marcus D.", ""], ["Stocker", "Christof", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1708.04682", "submitter": "Eric Mason", "authors": "Bariscan Yonel, Eric Mason, Birsen Yaz{\\i}c{\\i}", "title": "Deep Learning for Passive Synthetic Aperture Radar", "comments": "Submitted to IEEE Journal of Selected Topics in Signal Processing", "journal-ref": null, "doi": "10.1109/JSTSP.2017.2784181", "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning (DL) framework for inverse problems in imaging,\nand demonstrate the advantages and applicability of this approach in passive\nsynthetic aperture radar (SAR) image reconstruction. We interpret image recon-\nstruction as a machine learning task and utilize deep networks as forward and\ninverse solvers for imaging. Specifically, we design a recurrent neural network\n(RNN) architecture as an inverse solver based on the iterations of proximal\ngradient descent optimization methods. We further adapt the RNN architecture to\nimage reconstruction problems by transforming the network into a recurrent\nauto-encoder, thereby allowing for unsupervised training. Our DL based inverse\nsolver is particularly suitable for a class of image formation problems in\nwhich the forward model is only partially known. The ability to learn forward\nmodels and hyper parameters combined with unsupervised training approach\nestablish our recurrent auto-encoder suitable for real world applications. We\ndemonstrate the performance of our method in passive SAR image reconstruction.\nIn this regime a source of opportunity, with unknown location and transmitted\nwaveform, is used to illuminate a scene of interest. We investigate recurrent\nauto- encoder architecture based on the 1 and 0 constrained least- squares\nproblem. We present a projected stochastic gradient descent based training\nscheme which incorporates constraints of the unknown model parameters. We\ndemonstrate through extensive numerical simulations that our DL based approach\nout performs conventional sparse coding methods in terms of computation and\nreconstructed image quality, specifically, when no information about the\ntransmitter is available.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 00:25:10 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Yonel", "Bariscan", ""], ["Mason", "Eric", ""], ["Yaz\u0131c\u0131", "Birsen", ""]]}, {"id": "1708.04685", "submitter": "Renata Rychtarikova", "authors": "Renata Rychtarikova, Pavel Soucek, Dalibor Stys", "title": "Colorimetric Calibration of a Digital Camera", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel - physico-chemical - approach for\ncalibration of a digital camera chip. This approach utilizes results of\nmeasurement of incident light spectra of calibration films of different levels\nof gray for construction of calibration curve (number of incident photons vs.\nimage pixel intensity) for each camera pixel. We show spectral characteristics\nof such corrected digital raw image files (a primary camera signal) and\ndemonstrate their suitability for next image processing and analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 09:04:05 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Rychtarikova", "Renata", ""], ["Soucek", "Pavel", ""], ["Stys", "Dalibor", ""]]}, {"id": "1708.04686", "submitter": "Chuang Gan", "authors": "Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, Boqing Gong", "title": "VQS: Linking Segmentations to Questions and Answers for Supervised\n  Attention in VQA and Question-Focused Semantic Segmentation", "comments": "To appear on ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rich and dense human labeled datasets are among the main enabling factors for\nthe recent advance on vision-language understanding. Many seemingly distant\nannotations (e.g., semantic segmentation and visual question answering (VQA))\nare inherently connected in that they reveal different levels and perspectives\nof human understandings about the same visual scenes --- and even the same set\nof images (e.g., of COCO). The popularity of COCO correlates those annotations\nand tasks. Explicitly linking them up may significantly benefit both individual\ntasks and the unified vision and language modeling. We present the preliminary\nwork of linking the instance segmentations provided by COCO to the questions\nand answers (QAs) in the VQA dataset, and name the collected links visual\nquestions and segmentation answers (VQS). They transfer human supervision\nbetween the previously separate tasks, offer more effective leverage to\nexisting problems, and also open the door for new research problems and models.\nWe study two applications of the VQS data in this paper: supervised attention\nfor VQA and a novel question-focused semantic segmentation task. For the\nformer, we obtain state-of-the-art results on the VQA real multiple-choice task\nby simply augmenting the multilayer perceptrons with some attention features\nthat are learned using the segmentation-QA links as explicit supervision. To\nput the latter in perspective, we study two plausible methods and compare them\nto an oracle method assuming that the instance segmentations are given at the\ntest stage.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 20:47:02 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Gan", "Chuang", ""], ["Li", "Yandong", ""], ["Li", "Haoxiang", ""], ["Sun", "Chen", ""], ["Gong", "Boqing", ""]]}, {"id": "1708.04692", "submitter": "Anton Osokin", "authors": "Anton Osokin, Anatole Chessel, Rafael E. Carazo Salas and Federico\n  Vaggi", "title": "GANs for Biological Image Synthesis", "comments": "The paper appearing at the International Conference on Computer\n  Vision (ICCV) 2017 + its supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel application of Generative Adversarial\nNetworks (GAN) to the synthesis of cells imaged by fluorescence microscopy.\nCompared to natural images, cells tend to have a simpler and more geometric\nglobal structure that facilitates image generation. However, the correlation\nbetween the spatial pattern of different fluorescent proteins reflects\nimportant biological functions, and synthesized images have to capture these\nrelationships to be relevant for biological applications. We adapt GANs to the\ntask at hand and propose new models with casual dependencies between image\nchannels that can generate multi-channel images, which would be impossible to\nobtain experimentally. We evaluate our approach using two independent\ntechniques and compare it against sensible baselines. Finally, we demonstrate\nthat by interpolating across the latent space we can mimic the known changes in\nprotein localization that occur through time during the cell cycle, allowing us\nto predict temporal evolution from static images.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 21:04:11 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 09:18:24 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Osokin", "Anton", ""], ["Chessel", "Anatole", ""], ["Salas", "Rafael E. Carazo", ""], ["Vaggi", "Federico", ""]]}, {"id": "1708.04728", "submitter": "Dawei Li", "authors": "Dawei Li, Xiaolong Wang and Deguang Kong", "title": "DeepRebirth: Accelerating Deep Neural Network Execution on Mobile\n  Devices", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying deep neural networks on mobile devices is a challenging task.\nCurrent model compression methods such as matrix decomposition effectively\nreduce the deployed model size, but still cannot satisfy real-time processing\nrequirement. This paper first discovers that the major obstacle is the\nexcessive execution time of non-tensor layers such as pooling and normalization\nwithout tensor-like trainable parameters. This motivates us to design a novel\nacceleration framework: DeepRebirth through \"slimming\" existing consecutive and\nparallel non-tensor and tensor layers. The layer slimming is executed at\ndifferent substructures: (a) streamline slimming by merging the consecutive\nnon-tensor and tensor layer vertically; (b) branch slimming by merging\nnon-tensor and tensor branches horizontally. The proposed optimization\noperations significantly accelerate the model execution and also greatly reduce\nthe run-time memory cost since the slimmed model architecture contains less\nhidden layers. To maximally avoid accuracy loss, the parameters in new\ngenerated layers are learned with layer-wise fine-tuning based on both\ntheoretical analysis and empirical verification. As observed in the experiment,\nDeepRebirth achieves more than 3x speed-up and 2.5x run-time memory saving on\nGoogLeNet with only 0.4% drop of top-5 accuracy on ImageNet. Furthermore, by\ncombining with other model compression techniques, DeepRebirth offers an\naverage of 65ms inference time on the CPU of Samsung Galaxy S6 with 86.5% top-5\naccuracy, 14% faster than SqueezeNet which only has a top-5 accuracy of 80.5%.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 00:45:22 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 23:41:57 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Li", "Dawei", ""], ["Wang", "Xiaolong", ""], ["Kong", "Deguang", ""]]}, {"id": "1708.04747", "submitter": "Chenyang Xu", "authors": "Chenyang Xu, Mengxin Li", "title": "An Improved Neural Segmentation Method Based on U-NET", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural segmentation has a great impact on the smooth implementation of local\nanesthesia surgery. At present, the network for the segmentation includes U-NET\n[1] and SegNet [2]. U-NET network has short training time and less training\nparameters, but the depth is not deep enough. SegNet network has deeper\nstructure, but it needs longer training time, and more training samples. In\nthis paper, we propose an improved U-NET neural network for the segmentation.\nThis network deepens the original structure through importing residual network.\nCompared with U-NET and SegNet, the improved U-NET network has fewer training\nparameters, shorter training time and get a great improvement in segmentation\neffect. The improved U-NET network structure has a good application scene in\nneural segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 02:17:59 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Xu", "Chenyang", ""], ["Li", "Mengxin", ""]]}, {"id": "1708.04764", "submitter": "Yuantao Gu", "authors": "Yanxi Chen, Gen Li and Yuantao Gu", "title": "Active Orthogonal Matching Pursuit for Sparse Subspace Clustering", "comments": "14 pages, 5 figures, 1 table", "journal-ref": null, "doi": "10.1109/LSP.2017.2741509", "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Subspace Clustering (SSC) is a state-of-the-art method for clustering\nhigh-dimensional data points lying in a union of low-dimensional subspaces.\nHowever, while $\\ell_1$ optimization-based SSC algorithms suffer from high\ncomputational complexity, other variants of SSC, such as Orthogonal Matching\nPursuit-based SSC (OMP-SSC), lose clustering accuracy in pursuit of improving\ntime efficiency. In this letter, we propose a novel Active OMP-SSC, which\nimproves clustering accuracy of OMP-SSC by adaptively updating data points and\nrandomly dropping data points in the OMP process, while still enjoying the low\ncomputational complexity of greedy pursuit algorithms. We provide heuristic\nanalysis of our approach, and explain how these two active steps achieve a\nbetter tradeoff between connectivity and separation. Numerical results on both\nsynthetic data and real-world data validate our analyses and show the\nadvantages of the proposed active algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 04:15:37 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chen", "Yanxi", ""], ["Li", "Gen", ""], ["Gu", "Yuantao", ""]]}, {"id": "1708.04776", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jinwei Qi and Yuxin Yuan", "title": "Modality-specific Cross-modal Similarity Measurement with Recurrent\n  Attention Network", "comments": "13 pages, submitted to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, cross-modal retrieval plays an indispensable role to flexibly find\ninformation across different modalities of data. Effectively measuring the\nsimilarity between different modalities of data is the key of cross-modal\nretrieval. Different modalities such as image and text have imbalanced and\ncomplementary relationships, which contain unequal amount of information when\ndescribing the same semantics. For example, images often contain more details\nthat cannot be demonstrated by textual descriptions and vice versa. Existing\nworks based on Deep Neural Network (DNN) mostly construct one common space for\ndifferent modalities to find the latent alignments between them, which lose\ntheir exclusive modality-specific characteristics. Different from the existing\nworks, we propose modality-specific cross-modal similarity measurement (MCSM)\napproach by constructing independent semantic space for each modality, which\nadopts end-to-end framework to directly generate modality-specific cross-modal\nsimilarity without explicit common representation. For each semantic space,\nmodality-specific characteristics within one modality are fully exploited by\nrecurrent attention network, while the data of another modality is projected\ninto this space with attention based joint embedding to utilize the learned\nattention weights for guiding the fine-grained cross-modal correlation\nlearning, which can capture the imbalanced and complementary relationships\nbetween different modalities. Finally, the complementarity between the semantic\nspaces for different modalities is explored by adaptive fusion of the\nmodality-specific cross-modal similarities to perform cross-modal retrieval.\nExperiments on the widely-used Wikipedia and Pascal Sentence datasets as well\nas our constructed large-scale XMediaNet dataset verify the effectiveness of\nour proposed approach, outperforming 9 state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 05:43:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Peng", "Yuxin", ""], ["Qi", "Jinwei", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1708.04804", "submitter": "Tobias B\\\"ottger", "authors": "Tobias B\\\"ottger, Christina Eisenhofer", "title": "Efficiently Tracking Homogeneous Regions in Multichannel Images", "comments": "to be published in ICPRS 2017 proceedings", "journal-ref": null, "doi": "10.1049/cp.2017.0143", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for tracking Maximally Stable Homogeneous Regions (MSHR)\nin images with an arbitrary number of channels. MSHR are conceptionally very\nsimilar to Maximally Stable Extremal Regions (MSER) and Maximally Stable Color\nRegions (MSCR), but can also be applied to hyperspectral and color images while\nremaining extremely efficient. The presented approach makes use of the\nedge-based component-tree which can be calculated in linear time. In the\ntracking step, the MSHR are localized by matching them to the nodes in the\ncomponent-tree. We use rotationally invariant region and gray-value features\nthat can be calculated through first and second order moments at low\ncomputational complexity. Furthermore, we use a weighted feature vector to\nimprove the data association in the tracking step. The algorithm is evaluated\non a collection of different tracking scenes from the literature. Furthermore,\nwe present two different applications: 2D object tracking and the 3D\nsegmentation of organs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:30:47 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["B\u00f6ttger", "Tobias", ""], ["Eisenhofer", "Christina", ""]]}, {"id": "1708.04811", "submitter": "Christian Bartz", "authors": "Christian Bartz, Tom Herold, Haojin Yang and Christoph Meinel", "title": "Language Identification Using Deep Convolutional Recurrent Neural\n  Networks", "comments": "to be presented at ICONIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language Identification (LID) systems are used to classify the spoken\nlanguage from a given audio sample and are typically the first step for many\nspoken language processing tasks, such as Automatic Speech Recognition (ASR)\nsystems. Without automatic language detection, speech utterances cannot be\nparsed correctly and grammar rules cannot be applied, causing subsequent speech\nrecognition steps to fail. We propose a LID system that solves the problem in\nthe image domain, rather than the audio domain. We use a hybrid Convolutional\nRecurrent Neural Network (CRNN) that operates on spectrogram images of the\nprovided audio snippets. In extensive experiments we show, that our model is\napplicable to a range of noisy scenarios and can easily be extended to\npreviously unknown languages, while maintaining its classification accuracy. We\nrelease our code and a large scale training set for LID systems to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:42:22 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bartz", "Christian", ""], ["Herold", "Tom", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1708.04814", "submitter": "Chengzhou Tang", "authors": "Chengzhou Tang, Oliver Wang and Ping Tan", "title": "GSLAM: Initialization-robust Monocular Visual SLAM via Global\n  Structure-from-Motion", "comments": "3DV 2017 Project Page: https://frobelbest.github.io/gslam", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many monocular visual SLAM algorithms are derived from incremental\nstructure-from-motion (SfM) methods. This work proposes a novel monocular SLAM\nmethod which integrates recent advances made in global SfM. In particular, we\npresent two main contributions to visual SLAM. First, we solve the visual\nodometry problem by a novel rank-1 matrix factorization technique which is more\nrobust to the errors in map initialization. Second, we adopt a recent global\nSfM method for the pose-graph optimization, which leads to a multi-stage linear\nformulation and enables L1 optimization for better robustness to false loops.\nThe combination of these two approaches generates more robust reconstruction\nand is significantly faster (4X) than recent state-of-the-art SLAM systems. We\nalso present a new dataset recorded with ground truth camera motion in a Vicon\nmotion capture room, and compare our method to prior systems on it and\nestablished benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:53:16 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 22:10:37 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 06:43:12 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Tang", "Chengzhou", ""], ["Wang", "Oliver", ""], ["Tan", "Ping", ""]]}, {"id": "1708.04816", "submitter": "Nikolaos Mitianoudis", "authors": "Nikolaos Mitianoudis", "title": "A Generalised Directional Laplacian Distribution: Estimation, Mixture\n  Models and Audio Source Separation", "comments": null, "journal-ref": "IEEE Transactions on Audio, Speech and Language Processing, Vol.\n  20, No. 9, pp. 2397- 2408 (2012)", "doi": "10.1109/TASL.2012.2203804", "report-no": null, "categories": "cs.SD cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional or Circular statistics are pertaining to the analysis and\ninterpretation of directions or rotations. In this work, a novel probability\ndistribution is proposed to model multidimensional sparse directional data. The\nGeneralised Directional Laplacian Distribution (DLD) is a hybrid between the\nLaplacian distribution and the von Mises-Fisher distribution. The\ndistribution's parameters are estimated using Maximum-Likelihood Estimation\nover a set of training data points. Mixtures of Directional Laplacian\nDistributions (MDLD) are also introduced in order to model multiple\nconcentrations of sparse directional data. The author explores the application\nof the derived DLD mixture model to cluster sound sources that exist in an\nunderdetermined instantaneous sound mixture. The proposed model can solve the\ngeneral K x L (K<L) underdetermined instantaneous source separation problem,\noffering a fast and stable solution.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:56:51 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mitianoudis", "Nikolaos", ""]]}, {"id": "1708.04879", "submitter": "Elizabeth Lucas", "authors": "Elizabeth Lucas", "title": "Interstitial Content Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interstitial content is online content which grays out, or otherwise obscures\nthe main page content. In this technical report, we discuss exploratory\nresearch into detecting the presence of interstitial content in web pages. We\ndiscuss the use of computer vision techniques to detect interstitials, and the\npotential use of these techniques to provide a labelled dataset for machine\nlearning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 19:09:02 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Lucas", "Elizabeth", ""]]}, {"id": "1708.04890", "submitter": "Naila Murray", "authors": "Naila Murray and Albert Gordo", "title": "A deep architecture for unified aesthetic prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image aesthetics has become an important criterion for visual content\ncuration on social media sites and media content repositories. Previous work on\naesthetic prediction models in the computer vision community has focused on\naesthetic score prediction or binary image labeling. However, raw aesthetic\nannotations are in the form of score histograms and provide richer and more\nprecise information than binary labels or mean scores. Consequently, in this\nwork we focus on the rarely-studied problem of predicting aesthetic score\ndistributions and propose a novel architecture and training procedure for our\nmodel. Our model achieves state-of-the-art results on the standard AVA\nlarge-scale benchmark dataset for three tasks: (i) aesthetic quality\nclassification; (ii) aesthetic score regression; and (iii) aesthetic score\ndistribution prediction, all while using one model trained only for the\ndistribution prediction task. We also introduce a method to modify an image\nsuch that its predicted aesthetics changes, and use this modification to gain\ninsight into our model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 13:33:41 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Murray", "Naila", ""], ["Gordo", "Albert", ""]]}, {"id": "1708.04896", "submitter": "Zhun Zhong", "authors": "Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang", "title": "Random Erasing Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Random Erasing, a new data augmentation method\nfor training the convolutional neural network (CNN). In training, Random\nErasing randomly selects a rectangle region in an image and erases its pixels\nwith random values. In this process, training images with various levels of\nocclusion are generated, which reduces the risk of over-fitting and makes the\nmodel robust to occlusion. Random Erasing is parameter learning free, easy to\nimplement, and can be integrated with most of the CNN-based recognition models.\nAlbeit simple, Random Erasing is complementary to commonly used data\naugmentation techniques such as random cropping and flipping, and yields\nconsistent improvement over strong baselines in image classification, object\ndetection and person re-identification. Code is available at:\nhttps://github.com/zhunzhong07/Random-Erasing.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 13:56:48 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 10:05:35 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Zhong", "Zhun", ""], ["Zheng", "Liang", ""], ["Kang", "Guoliang", ""], ["Li", "Shaozi", ""], ["Yang", "Yi", ""]]}, {"id": "1708.04907", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni, Marco Ciccone, Francesco Visin and Matteo Matteucci", "title": "Multi-View Stereo with Single-View Semantic Mesh Refinement", "comments": "{\\pounds}D Reconstruction Meets Semantic, ICCV workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While 3D reconstruction is a well-established and widely explored research\ntopic, semantic 3D reconstruction has only recently witnessed an increasing\nshare of attention from the Computer Vision community. Semantic annotations\nallow in fact to enforce strong class-dependent priors, as planarity for ground\nand walls, which can be exploited to refine the reconstruction often resulting\nin non-trivial performance improvements. State-of-the art methods propose\nvolumetric approaches to fuse RGB image data with semantic labels; even if\nsuccessful, they do not scale well and fail to output high resolution meshes.\nIn this paper we propose a novel method to refine both the geometry and the\nsemantic labeling of a given mesh. We refine the mesh geometry by applying a\nvariational method that optimizes a composite energy made of a state-of-the-art\npairwise photo-metric term and a single-view term that models the semantic\nconsistency between the labels of the 3D mesh and those of the segmented\nimages. We also update the semantic labeling through a novel Markov Random\nField (MRF) formulation that, together with the classical data and smoothness\nterms, takes into account class-specific priors estimated directly from the\nannotated mesh. This is in contrast to state-of-the-art methods that are\ntypically based on handcrafted or learned priors. We are the first, jointly\nwith the very recent and seminal work of [M. Blaha et al arXiv:1706.08336,\n2017], to propose the use of semantics inside a mesh refinement framework.\nDifferently from [M. Blaha et al arXiv:1706.08336, 2017], which adopts a more\nclassical pairwise comparison to estimate the flow of the mesh, we apply a\nsingle-view comparison between the semantically annotated image and the current\n3D mesh labels; this improves the robustness in case of noisy segmentations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 14:29:49 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 20:12:42 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Romanoni", "Andrea", ""], ["Ciccone", "Marco", ""], ["Visin", "Francesco", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1708.04943", "submitter": "Jun Fu", "authors": "Jun Fu, Jing Liu, Yuhang Wang, and Hanqing Lu", "title": "Stacked Deconvolutional Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in semantic segmentation has been driven by improving the\nspatial resolution under Fully Convolutional Networks (FCNs). To address this\nproblem, we propose a Stacked Deconvolutional Network (SDN) for semantic\nsegmentation. In SDN, multiple shallow deconvolutional networks, which are\ncalled as SDN units, are stacked one by one to integrate contextual information\nand guarantee the fine recovery of localization information. Meanwhile,\ninter-unit and intra-unit connections are designed to assist network training\nand enhance feature fusion since the connections improve the flow of\ninformation and gradient propagation throughout the network. Besides,\nhierarchical supervision is applied during the upsampling process of each SDN\nunit, which guarantees the discrimination of feature representations and\nbenefits the network optimization. We carry out comprehensive experiments and\nachieve the new state-of-the-art results on three datasets, including PASCAL\nVOC 2012, CamVid, GATECH. In particular, our best model without CRF\npost-processing achieves an intersection-over-union score of 86.6% in the test\nset.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 15:35:02 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Fu", "Jun", ""], ["Liu", "Jing", ""], ["Wang", "Yuhang", ""], ["Lu", "Hanqing", ""]]}, {"id": "1708.04975", "submitter": "Eric Laloy", "authors": "Eric Laloy, Romain H\\'erault, Diederik Jacques, Niklas Linde", "title": "Training-image based geostatistical inversion using a spatial generative\n  adversarial neural network", "comments": null, "journal-ref": "Water Resources Research, 54, 381-406, 2018", "doi": "10.1002/2017WR022148", "report-no": null, "categories": "stat.ML cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inversion within a multiple-point statistics framework is often\ncomputationally prohibitive for high-dimensional problems. To partly address\nthis, we introduce and evaluate a new training-image based inversion approach\nfor complex geologic media. Our approach relies on a deep neural network of the\ngenerative adversarial network (GAN) type. After training using a training\nimage (TI), our proposed spatial GAN (SGAN) can quickly generate 2D and 3D\nunconditional realizations. A key characteristic of our SGAN is that it defines\na (very) low-dimensional parameterization, thereby allowing for efficient\nprobabilistic inversion using state-of-the-art Markov chain Monte Carlo (MCMC)\nmethods. In addition, available direct conditioning data can be incorporated\nwithin the inversion. Several 2D and 3D categorical TIs are first used to\nanalyze the performance of our SGAN for unconditional geostatistical\nsimulation. Training our deep network can take several hours. After training,\nrealizations containing a few millions of pixels/voxels can be produced in a\nmatter of seconds. This makes it especially useful for simulating many\nthousands of realizations (e.g., for MCMC inversion) as the relative cost of\nthe training per realization diminishes with the considered number of\nrealizations. Synthetic inversion case studies involving 2D steady-state flow\nand 3D transient hydraulic tomography with and without direct conditioning data\nare used to illustrate the effectiveness of our proposed SGAN-based inversion.\nFor the 2D case, the inversion rapidly explores the posterior model\ndistribution. For the 3D case, the inversion recovers model realizations that\nfit the data close to the target level and visually resemble the true model\nwell.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:04:52 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2019 14:44:42 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Laloy", "Eric", ""], ["H\u00e9rault", "Romain", ""], ["Jacques", "Diederik", ""], ["Linde", "Niklas", ""]]}, {"id": "1708.04989", "submitter": "Raghavender Sahdev", "authors": "Raghavender Sahdev", "title": "Free Space Estimation using Occupancy Grids and Dynamic Object Detection", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to estimate Free Space from a Stereo\nimage pair using stochastic occupancy grids. We do this in the domain of\nautonomous driving on the famous benchmark dataset KITTI. Later based on the\ngenerated occupancy grid we match 2 image sequences to compute the top view\nrepresentation of the map. We do this to map the environment. We compute a\ntransformation between the occupancy grids of two successive images and use it\nto compute the top view map. Two issues need to be addressed for mapping are\ndiscussed - computing a map and dealing with dynamic objects for computing the\nmap. Dynamic Objects are detected in successive images based on an idea similar\nto tracking of foreground objects from the background objects based on motion\nflow. A novel RANSAC based segmentation approach has been proposed here to\naddress this issue.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:28:21 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Sahdev", "Raghavender", ""]]}, {"id": "1708.05019", "submitter": "Nikolaos Mitianoudis", "authors": "Dimitrios Tourtounis, Nikolaos Mitianoudis, Georgios Ch. Sirakoulis", "title": "Salt-n-pepper noise filtering using Cellular Automata", "comments": null, "journal-ref": "Journal of Cellular Automata, Vol. 13, No. 1-2, pp. 81-101, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Automata (CA) have been considered one of the most pronounced\nparallel computational tools in the recent era of nature and bio-inspired\ncomputing. Taking advantage of their local connectivity, the simplicity of\ntheir design and their inherent parallelism, CA can be effectively applied to\nmany image processing tasks. In this paper, a CA approach for efficient\nsalt-n-pepper noise filtering in grayscale images is presented. Using a 2D\nMoore neighborhood, the classified \"noisy\" cells are corrected by averaging the\nnon-noisy neighboring cells. While keeping the computational burden really low,\nthe proposed approach succeeds in removing high-noise levels from various\nimages and yields promising qualitative and quantitative results, compared to\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:02:02 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Tourtounis", "Dimitrios", ""], ["Mitianoudis", "Nikolaos", ""], ["Sirakoulis", "Georgios Ch.", ""]]}, {"id": "1708.05029", "submitter": "Xin Chen", "authors": "Aosen Wang, Hua Zhou, Wenyao Xu, Xin Chen", "title": "Deep Neural Network Capacity", "comments": "There is an error in Average Valid Bits computation in figure 1 in\n  page 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural network exhibits its powerful superiority on\ninformation discrimination in many computer vision applications. However, the\ncapacity of deep neural network architecture is still a mystery to the\nresearchers. Intuitively, larger capacity of neural network can always deposit\nmore information to improve the discrimination ability of the model. But, the\nlearnable parameter scale is not feasible to estimate the capacity of deep\nneural network. Due to the overfitting, directly increasing hidden nodes number\nand hidden layer number are already demonstrated not necessary to effectively\nincrease the network discrimination ability.\n  In this paper, we propose a novel measurement, named \"total valid bits\", to\nevaluate the capacity of deep neural networks for exploring how to\nquantitatively understand the deep learning and the insights behind its super\nperformance. Specifically, our scheme to retrieve the total valid bits\nincorporates the skilled techniques in both training phase and inference phase.\nIn the network training, we design decimal weight regularization and 8-bit\nforward quantization to obtain the integer-oriented network representations.\nMoreover, we develop adaptive-bitwidth and non-uniform quantization strategy in\nthe inference phase to find the neural network capacity, total valid bits. By\nallowing zero bitwidth, our adaptive-bitwidth quantization can execute the\nmodel reduction and valid bits finding simultaneously. In our extensive\nexperiments, we first demonstrate that our total valid bits is a good indicator\nof neural network capacity. We also analyze the impact on network capacity from\nthe network architecture and advanced training skills, such as dropout and\nbatch normalization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:28:22 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 18:59:27 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 18:42:40 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Wang", "Aosen", ""], ["Zhou", "Hua", ""], ["Xu", "Wenyao", ""], ["Chen", "Xin", ""]]}, {"id": "1708.05038", "submitter": "Du Tran", "authors": "Du Tran, Jamie Ray, Zheng Shou, Shih-Fu Chang, Manohar Paluri", "title": "ConvNet Architecture Search for Spatiotemporal Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning image representations with ConvNets by pre-training on ImageNet has\nproven useful across many visual understanding tasks including object\ndetection, semantic segmentation, and image captioning. Although any image\nrepresentation can be applied to video frames, a dedicated spatiotemporal\nrepresentation is still vital in order to incorporate motion patterns that\ncannot be captured by appearance based models alone. This paper presents an\nempirical ConvNet architecture search for spatiotemporal feature learning,\nculminating in a deep 3-dimensional (3D) Residual ConvNet. Our proposed\narchitecture outperforms C3D by a good margin on Sports-1M, UCF101, HMDB51,\nTHUMOS14, and ASLAN while being 2 times faster at inference time, 2 times\nsmaller in model size, and having a more compact representation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 18:54:39 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Tran", "Du", ""], ["Ray", "Jamie", ""], ["Shou", "Zheng", ""], ["Chang", "Shih-Fu", ""], ["Paluri", "Manohar", ""]]}, {"id": "1708.05071", "submitter": "Jaebok Kim", "authors": "Jaebok Kim, Khiet P. Truong, Gwenn Englebienne, and Vanessa Evers", "title": "Learning spectro-temporal features with 3D CNNs for speech emotion\n  recognition", "comments": "ACII, 2017, San Antonio", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to use deep 3-dimensional convolutional networks\n(3D CNNs) in order to address the challenge of modelling spectro-temporal\ndynamics for speech emotion recognition (SER). Compared to a hybrid of\nConvolutional Neural Network and Long-Short-Term-Memory (CNN-LSTM), our\nproposed 3D CNNs simultaneously extract short-term and long-term spectral\nfeatures with a moderate number of parameters. We evaluated our proposed and\nother state-of-the-art methods in a speaker-independent manner using aggregated\ncorpora that give a large and diverse set of speakers. We found that 1) shallow\ntemporal and moderately deep spectral kernels of a homogeneous architecture are\noptimal for the task; and 2) our 3D CNNs are more effective for\nspectro-temporal feature learning compared to other methods. Finally, we\nvisualised the feature space obtained with our proposed method using\nt-distributed stochastic neighbour embedding (T-SNE) and could observe distinct\nclusters of emotions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:32:06 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Kim", "Jaebok", ""], ["Truong", "Khiet P.", ""], ["Englebienne", "Gwenn", ""], ["Evers", "Vanessa", ""]]}, {"id": "1708.05081", "submitter": "Dibya Jyoti Bora", "authors": "Dibya Jyoti Bora", "title": "Importance of Image Enhancement Techniques in Color Image Segmentation:\n  A Comprehensive and Comparative Study", "comments": "27 pages, 17 figures, 2 Tables, 1 flowchart", "journal-ref": "Indian J.Sci.Res. 15 (1): 115-131, 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color image segmentation is a very emerging research topic in the area of\ncolor image analysis and pattern recognition. Many state-of-the-art algorithms\nhave been developed for this purpose. But, often the segmentation results of\nthese algorithms seem to be suffering from miss-classifications and\nover-segmentation. The reasons behind these are the degradation of image\nquality during the acquisition, transmission and color space conversion. So,\nhere arises the need of an efficient image enhancement technique which can\nremove the redundant pixels or noises from the color image before proceeding\nfor final segmentation. In this paper, an effort has been made to study and\nanalyze different image enhancement techniques and thereby finding out the\nbetter one for color image segmentation. Also, this comparative study is done\non two well-known color spaces HSV and LAB separately to find out which color\nspace supports segmentation task more efficiently with respect to those\nenhancement techniques.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:35:20 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bora", "Dibya Jyoti", ""]]}, {"id": "1708.05095", "submitter": "Rodrigo Lobos", "authors": "Rodrigo A. Lobos, Tae Hyung Kim, W. Scott Hoge, Justin P. Haldar", "title": "Navigator-free EPI Ghost Correction with Structured Low-Rank Matrix\n  Models: New Theory and Methods", "comments": "13 pages, 9 figures ; Submitted to IEEE Transactions on Medical\n  Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2822053", "report-no": null, "categories": "cs.CV cs.IT math.IT physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured low-rank matrix models have previously been introduced to enable\ncalibrationless MR image reconstruction from sub-Nyquist data, and such ideas\nhave recently been extended to enable navigator-free echo-planar imaging (EPI)\nghost correction. This paper presents novel theoretical analysis which shows\nthat, because of uniform subsampling, the structured low-rank matrix\noptimization problems for EPI data will always have either undesirable or\nnon-unique solutions in the absence of additional constraints. This theory\nleads us to recommend and investigate problem formulations for navigator-free\nEPI that incorporate side information from either image-domain or k-space\ndomain parallel imaging methods. The importance of using nonconvex low-rank\nmatrix regularization is also identified. We demonstrate using phantom and\n\\emph{in vivo} data that the proposed methods are able to eliminate ghost\nartifacts for several navigator-free EPI acquisition schemes, obtaining better\nperformance in comparison to state-of-the-art methods across a range of\ndifferent scenarios. Results are shown for both single-channel acquisition and\nhighly accelerated multi-channel acquisition.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 22:11:50 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 02:57:45 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 01:41:59 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Lobos", "Rodrigo A.", ""], ["Kim", "Tae Hyung", ""], ["Hoge", "W. Scott", ""], ["Haldar", "Justin P.", ""]]}, {"id": "1708.05122", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun\n  Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games", "comments": "HCOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI continues to advance, human-AI teams are inevitable. However, progress\nin AI is routinely measured in isolation, without a human in the loop. It is\ncrucial to benchmark progress in AI, not just in isolation, but also in terms\nof how it translates to helping humans perform certain tasks, i.e., the\nperformance of human-AI teams.\n  In this work, we design a cooperative game - GuessWhich - to measure human-AI\nteam performance in the specific context of the AI being a visual\nconversational agent. GuessWhich involves live interaction between the human\nand the AI. The AI, which we call ALICE, is provided an image which is unseen\nby the human. Following a brief description of the image, the human questions\nALICE about this secret image to identify it from a fixed pool of images.\n  We measure performance of the human-ALICE team by the number of guesses it\ntakes the human to correctly identify the secret image after a fixed number of\ndialog rounds with ALICE. We compare performance of the human-ALICE teams for\ntwo versions of ALICE. Our human studies suggest a counterintuitive trend -\nthat while AI literature shows that one version outperforms the other when\npaired with an AI questioner bot, we find that this improvement in AI-AI\nperformance does not translate to improved human-AI performance. This suggests\na mismatch between benchmarking of AI in isolation and in the context of\nhuman-AI teams.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 03:27:53 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Yadav", "Deshraj", ""], ["Prabhu", "Viraj", ""], ["Chandrasekaran", "Arjun", ""], ["Das", "Abhishek", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1708.05125", "submitter": "Feiyun Zhu", "authors": "Feiyun Zhu", "title": "Hyperspectral Unmixing: Ground Truth Labeling, Datasets, Benchmark\n  Performances and Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing (HU) is a very useful and increasingly popular\npreprocessing step for a wide range of hyperspectral applications. However, the\nHU research has been constrained a lot by three factors: (a) the number of\nhyperspectral images (especially the ones with ground truths) are very limited;\n(b) the ground truths of most hyperspectral images are not shared on the web,\nwhich may cause lots of unnecessary troubles for researchers to evaluate their\nalgorithms; (c) the codes of most state-of-the-art methods are not shared,\nwhich may also delay the testing of new methods.\n  Accordingly, this paper deals with the above issues from the following three\nperspectives: (1) as a profound contribution, we provide a general labeling\nmethod for the HU. With it, we labeled up to 15 hyperspectral images, providing\n18 versions of ground truths. To the best of our knowledge, this is the first\npaper to summarize and share up to 15 hyperspectral images and their 18\nversions of ground truths for the HU. Observing that the hyperspectral\nclassification (HyC) has much more standard datasets (whose ground truths are\ngenerally publicly shared) than the HU, we propose an interesting method to\ntransform the HyC datasets for the HU research. (2) To further facilitate the\nevaluation of HU methods under different conditions, we reviewed and\nimplemented the algorithm to generate a complex synthetic hyperspectral image.\nBy tuning the hyper-parameters in the code, we may verify the HU methods from\nfour perspectives. The code would also be shared on the web. (3) To provide a\nstandard comparison, we reviewed up to 10 state-of-the-art HU algorithms, then\nselected the 5 most benchmark HU algorithms, and compared them on the 15 real\nhyperspectral datasets. The experiment results are surely reproducible; the\nimplemented codes would be shared on the web.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 03:35:02 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 16:22:06 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Zhu", "Feiyun", ""]]}, {"id": "1708.05127", "submitter": "Di Hu", "authors": "Xuelong Li and Di Hu and Feiping Nie", "title": "Deep Binary Reconstruction for Cross-modal Hashing", "comments": "8 pages, 5 figures, accepted by ACM Multimedia 2017", "journal-ref": null, "doi": "10.1145/3123266.3123355", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand of massive multimodal data storage and\norganization, cross-modal retrieval based on hashing technique has drawn much\nattention nowadays. It takes the binary codes of one modality as the query to\nretrieve the relevant hashing codes of another modality. However, the existing\nbinary constraint makes it difficult to find the optimal cross-modal hashing\nfunction. Most approaches choose to relax the constraint and perform\nthresholding strategy on the real-value representation instead of directly\nsolving the original objective. In this paper, we first provide a concrete\nanalysis about the effectiveness of multimodal networks in preserving the\ninter- and intra-modal consistency. Based on the analysis, we provide a\nso-called Deep Binary Reconstruction (DBRC) network that can directly learn the\nbinary hashing codes in an unsupervised fashion. The superiority comes from a\nproposed simple but efficient activation function, named as Adaptive Tanh\n(ATanh). The ATanh function can adaptively learn the binary codes and be\ntrained via back-propagation. Extensive experiments on three benchmark datasets\ndemonstrate that DBRC outperforms several state-of-the-art methods in both\nimage2text and text2image retrieval task.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 04:05:58 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 01:35:54 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Li", "Xuelong", ""], ["Hu", "Di", ""], ["Nie", "Feiping", ""]]}, {"id": "1708.05133", "submitter": "Fan Jiang", "authors": "Fan Jiang, Zhihui Hao, Xinran Liu", "title": "Deep Scene Text Detection with Connected Component Proposals", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing demand for natural-scene text detection has been witnessed by the\ncomputer vision community since text information plays a significant role in\nscene understanding and image indexing. Deep neural networks are being used due\nto their strong capabilities of pixel-wise classification or word localization,\nsimilar to being used in common vision problems. In this paper, we present a\nnovel two-task network with integrating bottom and top cues. The first task\naims to predict a pixel-by-pixel labeling and based on which, word proposals\nare generated with a canonical connected component analysis. The second task\naims to output a bundle of character candidates used later to verify the word\nproposals. The two sub-networks share base convolutional features and moreover,\nwe present a new loss to strengthen the interaction between them. We evaluate\nthe proposed network on public benchmark datasets and show it can detect\narbitrary-orientation scene text with a finer output boundary. In ICDAR 2013\ntext localization task, we achieve the state-of-the-art performance with an\nF-score of 0.919 and a much better recall of 0.915.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 04:44:03 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Jiang", "Fan", ""], ["Hao", "Zhihui", ""], ["Liu", "Xinran", ""]]}, {"id": "1708.05137", "submitter": "Jae Shin Yoon", "authors": "Jae Shin Yoon, Francois Rameau, Junsik Kim, Seokju Lee, Seunghak Shin,\n  In So Kweon", "title": "Pixel-Level Matching for Video Object Segmentation using Convolutional\n  Neural Networks", "comments": "To appear on ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel video object segmentation algorithm based on pixel-level\nmatching using Convolutional Neural Networks (CNN). Our network aims to\ndistinguish the target area from the background on the basis of the pixel-level\nsimilarity between two object units. The proposed network represents a target\nobject using features from different depth layers in order to take advantage of\nboth the spatial details and the category-level semantic information.\nFurthermore, we propose a feature compression technique that drastically\nreduces the memory requirements while maintaining the capability of feature\nrepresentation. Two-stage training (pre-training and fine-tuning) allows our\nnetwork to handle any target object regardless of its category (even if the\nobject's type does not belong to the pre-training data) or of variations in its\nappearance through a video sequence. Experiments on large datasets demonstrate\nthe effectiveness of our model - against related methods - in terms of\naccuracy, speed, and stability. Finally, we introduce the transferability of\nour network to different domains, such as the infrared data domain.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 05:04:23 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Yoon", "Jae Shin", ""], ["Rameau", "Francois", ""], ["Kim", "Junsik", ""], ["Lee", "Seokju", ""], ["Shin", "Seunghak", ""], ["Kweon", "In So", ""]]}, {"id": "1708.05170", "submitter": "Congbo Cai", "authors": "Congbo Cai, Yiqing Zeng, Chao Wang, Shuhui Cai, Jun Zhang, Zhong Chen,\n  Xinghao Ding, and Jianhui Zhong", "title": "High Efficient Reconstruction of Single-shot T2 Mapping from\n  OverLapping-Echo Detachment Planar Imaging Based on Deep Residual Network", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: An end-to-end deep convolutional neural network (CNN) based on deep\nresidual network (ResNet) was proposed to efficiently reconstruct reliable T2\nmapping from single-shot OverLapping-Echo Detachment (OLED) planar imaging.\nMethods: The training dataset was obtained from simulations carried out on\nSPROM software developed by our group. The relationship between the original\nOLED image containing two echo signals and the corresponded T2 mapping was\nlearned by ResNet training. After the ResNet was trained, it was applied to\nreconstruct the T2 mapping from simulation and in vivo human brain data.\nResults: Though the ResNet was trained entirely on simulated data, the trained\nnetwork was generalized well to real human brain data. The results from\nsimulation and in vivo human brain experiments show that the proposed method\nsignificantly outperformed the echo-detachment-based method. Reliable T2\nmapping was achieved within tens of milliseconds after the network had been\ntrained while the echo-detachment-based OLED reconstruction method took\nminutes. Conclusion: The proposed method will greatly facilitate real-time\ndynamic and quantitative MR imaging via OLED sequence, and ResNet has the\npotential to reconstruct images from complex MRI sequence efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 08:48:21 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Cai", "Congbo", ""], ["Zeng", "Yiqing", ""], ["Wang", "Chao", ""], ["Cai", "Shuhui", ""], ["Zhang", "Jun", ""], ["Chen", "Zhong", ""], ["Ding", "Xinghao", ""], ["Zhong", "Jianhui", ""]]}, {"id": "1708.05206", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Haojin Yang and Christoph Meinel", "title": "Brain Abnormality Detection by Deep Convolutional Neural Network", "comments": "Accepted for presenting in ACM-womENcourage_2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe our method for classification of brain magnetic\nresonance (MR) images into different abnormalities and healthy classes based on\nthe deep neural network. We propose our method to detect high and low-grade\nglioma, multiple sclerosis, and Alzheimer diseases as well as healthy cases.\nOur network architecture has ten learning layers that include seven\nconvolutional layers and three fully connected layers. We have achieved a\npromising result in five categories of brain images (classification task) with\n95.7% accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 11:24:58 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Rezaei", "Mina", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1708.05211", "submitter": "Tu Dinh Nguyen", "authors": "Hung Vu, Dinh Phung, Tu Dinh Nguyen, Anthony Trevors, Svetha Venkatesh", "title": "Energy-based Models for Video Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of abnormalities in data has been studied in research\narea in recent years because of its diverse applications in practice including\nvideo surveillance, industrial damage detection and network intrusion\ndetection. However, building an effective anomaly detection system is a\nnon-trivial task since it requires to tackle challenging issues of the shortage\nof annotated data, inability of defining anomaly objects explicitly and the\nexpensive cost of feature engineering procedure. Unlike existing appoaches\nwhich only partially solve these problems, we develop a unique framework to\ncope the problems above simultaneously. Instead of hanlding with ambiguous\ndefinition of anomaly objects, we propose to work with regular patterns whose\nunlabeled data is abundant and usually easy to collect in practice. This allows\nour system to be trained completely in an unsupervised procedure and liberate\nus from the need for costly data annotation. By learning generative model that\ncapture the normality distribution in data, we can isolate abnormal data points\nthat result in low normality scores (high abnormality scores). Moreover, by\nleverage on the power of generative networks, i.e. energy-based models, we are\nalso able to learn the feature representation automatically rather than\nreplying on hand-crafted features that have been dominating anomaly detection\nresearch over many decades. We demonstrate our proposal on the specific\napplication of video anomaly detection and the experimental results indicate\nthat our method performs better than baselines and are comparable with\nstate-of-the-art methods in many benchmark video anomaly detection datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 11:44:34 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Vu", "Hung", ""], ["Phung", "Dinh", ""], ["Nguyen", "Tu Dinh", ""], ["Trevors", "Anthony", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1708.05221", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Haojin Yang and Christoph Meinel", "title": "Deep Neural Network with l2-norm Unit for Brain Lesions Detection", "comments": "Accepted for presentation in ICONIP-2017", "journal-ref": "Springer2017", "doi": "10.1007/978-3-319-70093-9_85", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated brain lesions detection is an important and very challenging\nclinical diagnostic task because the lesions have different sizes, shapes,\ncontrasts, and locations. Deep Learning recently has shown promising progress\nin many application fields, which motivates us to apply this technology for\nsuch important problem. In this paper, we propose a novel and end-to-end\ntrainable approach for brain lesions classification and detection by using deep\nConvolutional Neural Network (CNN). In order to investigate the applicability,\nwe applied our approach on several brain diseases including high and low-grade\nglioma tumor, ischemic stroke, Alzheimer diseases, by which the brain Magnetic\nResonance Images (MRI) have been applied as an input for the analysis. We\nproposed a new operating unit which receives features from several projections\nof a subset units of the bottom layer and computes a normalized l2-norm for\nnext layer. We evaluated the proposed approach on two different CNN\narchitectures and number of popular benchmark datasets. The experimental\nresults demonstrate the superior ability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:11:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Rezaei", "Mina", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1708.05227", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Konstantin Harmuth, Willi Gierke, Thomas Kellermeier,\n  Martin Fischer, Haojin Yang, Christoph Meinel", "title": "Conditional Adversarial Network for Semantic Segmentation of Brain Tumor", "comments": "Submitted to BraTS challenges which is part of MICCAI-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated medical image analysis has a significant value in diagnosis and\ntreatment of lesions. Brain tumors segmentation has a special importance and\ndifficulty due to the difference in appearances and shapes of the different\ntumor regions in magnetic resonance images. Additionally, the data sets are\nheterogeneous and usually limited in size in comparison with the computer\nvision problems. The recently proposed adversarial training has shown promising\nresults in generative image modeling. In this paper, we propose a novel\nend-to-end trainable architecture for brain tumor semantic segmentation through\nconditional adversarial training. We exploit conditional Generative Adversarial\nNetwork (cGAN) and train a semantic segmentation Convolution Neural Network\n(CNN) along with an adversarial network that discriminates segmentation maps\ncoming from the ground truth or from the segmentation network for BraTS 2017\nsegmentation task[15, 4, 2, 3]. We also propose an end-to-end trainable CNN for\nsurvival day prediction based on deep learning techniques for BraTS 2017\nprediction task [15, 4, 2, 3]. The experimental results demonstrate the\nsuperior ability of the proposed approach for both tasks. The proposed model\nachieves on validation data a DICE score, Sensitivity and Specificity\nrespectively 0.68, 0.99 and 0.98 for the whole tumor, regarding online judgment\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:18:54 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Rezaei", "Mina", ""], ["Harmuth", "Konstantin", ""], ["Gierke", "Willi", ""], ["Kellermeier", "Thomas", ""], ["Fischer", "Martin", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1708.05234", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, Stan Z.\n  Li", "title": "FaceBoxes: A CPU Real-time Face Detector with High Accuracy", "comments": "Accepted by IJCB 2017; Added references; Released codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although tremendous strides have been made in face detection, one of the\nremaining open challenges is to achieve real-time speed on the CPU as well as\nmaintain high performance, since effective models for face detection tend to be\ncomputationally prohibitive. To address this challenge, we propose a novel face\ndetector, named FaceBoxes, with superior performance on both speed and\naccuracy. Specifically, our method has a lightweight yet powerful network\nstructure that consists of the Rapidly Digested Convolutional Layers (RDCL) and\nthe Multiple Scale Convolutional Layers (MSCL). The RDCL is designed to enable\nFaceBoxes to achieve real-time speed on the CPU. The MSCL aims at enriching the\nreceptive fields and discretizing anchors over different layers to handle faces\nof various scales. Besides, we propose a new anchor densification strategy to\nmake different types of anchors have the same density on the image, which\nsignificantly improves the recall rate of small faces. As a consequence, the\nproposed detector runs at 20 FPS on a single CPU core and 125 FPS using a GPU\nfor VGA-resolution images. Moreover, the speed of FaceBoxes is invariant to the\nnumber of faces. We comprehensively evaluate this method and present\nstate-of-the-art detection performance on several face detection benchmark\ndatasets, including the AFW, PASCAL face, and FDDB. Code is available at\nhttps://github.com/sfzhang15/FaceBoxes\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:38:32 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 08:58:10 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 12:48:51 GMT"}, {"version": "v4", "created": "Tue, 25 Dec 2018 05:49:00 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhang", "Shifeng", ""], ["Zhu", "Xiangyu", ""], ["Lei", "Zhen", ""], ["Shi", "Hailin", ""], ["Wang", "Xiaobo", ""], ["Li", "Stan Z.", ""]]}, {"id": "1708.05237", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang, Stan Z.\n  Li", "title": "S$^3$FD: Single Shot Scale-invariant Face Detector", "comments": "Accepted by ICCV 2017 + its supplementary materials; Updated the\n  latest results on WIDER FACE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time face detector, named Single Shot\nScale-invariant Face Detector (S$^3$FD), which performs superiorly on various\nscales of faces with a single deep neural network, especially for small faces.\nSpecifically, we try to solve the common problem that anchor-based detectors\ndeteriorate dramatically as the objects become smaller. We make contributions\nin the following three aspects: 1) proposing a scale-equitable face detection\nframework to handle different scales of faces well. We tile anchors on a wide\nrange of layers to ensure that all scales of faces have enough features for\ndetection. Besides, we design anchor scales based on the effective receptive\nfield and a proposed equal proportion interval principle; 2) improving the\nrecall rate of small faces by a scale compensation anchor matching strategy; 3)\nreducing the false positive rate of small faces via a max-out background label.\nAs a consequence, our method achieves state-of-the-art detection performance on\nall the common face detection benchmarks, including the AFW, PASCAL face, FDDB\nand WIDER FACE datasets, and can run at 36 FPS on a Nvidia Titan X (Pascal) for\nVGA-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:40:35 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 08:41:26 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 16:22:41 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Zhang", "Shifeng", ""], ["Zhu", "Xiangyu", ""], ["Lei", "Zhen", ""], ["Shi", "Hailin", ""], ["Wang", "Xiaobo", ""], ["Li", "Stan Z.", ""]]}, {"id": "1708.05256", "submitter": "Thorsten Kurth", "authors": "Thorsten Kurth, Jian Zhang, Nadathur Satish, Ioannis Mitliagkas, Evan\n  Racah, Mostofa Ali Patwary, Tareq Malas, Narayanan Sundaram, Wahid Bhimji,\n  Mikhail Smorkalov, Jack Deslippe, Mikhail Shiryaev, Srinivas Sridharan,\n  Prabhat, Pradeep Dubey", "title": "Deep Learning at 15PF: Supervised and Semi-Supervised Classification for\n  Scientific Data", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first, 15-PetaFLOP Deep Learning system for solving\nscientific pattern classification problems on contemporary HPC architectures.\nWe develop supervised convolutional architectures for discriminating signals in\nhigh-energy physics data as well as semi-supervised architectures for\nlocalizing and classifying extreme weather in climate data. Our\nIntelcaffe-based implementation obtains $\\sim$2TFLOP/s on a single Cori\nPhase-II Xeon-Phi node. We use a hybrid strategy employing synchronous\nnode-groups, while using asynchronous communication across groups. We use this\nstrategy to scale training of a single model to $\\sim$9600 Xeon-Phi nodes;\nobtaining peak performance of 11.73-15.07 PFLOP/s and sustained performance of\n11.41-13.27 PFLOP/s. At scale, our HEP architecture produces state-of-the-art\nclassification accuracy on a dataset with 10M images, exceeding that achieved\nby selections on high-level physics-motivated features. Our semi-supervised\narchitecture successfully extracts weather patterns in a 15TB climate dataset.\nOur results demonstrate that Deep Learning can be optimized and scaled\neffectively on many-core, HPC systems.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:21:36 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Kurth", "Thorsten", ""], ["Zhang", "Jian", ""], ["Satish", "Nadathur", ""], ["Mitliagkas", "Ioannis", ""], ["Racah", "Evan", ""], ["Patwary", "Mostofa Ali", ""], ["Malas", "Tareq", ""], ["Sundaram", "Narayanan", ""], ["Bhimji", "Wahid", ""], ["Smorkalov", "Mikhail", ""], ["Deslippe", "Jack", ""], ["Shiryaev", "Mikhail", ""], ["Sridharan", "Srinivas", ""], ["Prabhat", "", ""], ["Dubey", "Pradeep", ""]]}, {"id": "1708.05271", "submitter": "Ting Yao", "authors": "Ting Yao and Yingwei Pan and Yehao Li and Tao Mei", "title": "Incorporating Copying Mechanism in Image Captioning for Learning Novel\n  Objects", "comments": "CVPR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning often requires a large set of training image-sentence pairs.\nIn practice, however, acquiring sufficient training pairs is always expensive,\nmaking the recent captioning models limited in their ability to describe\nobjects outside of training corpora (i.e., novel objects). In this paper, we\npresent Long Short-Term Memory with Copying Mechanism (LSTM-C) --- a new\narchitecture that incorporates copying into the Convolutional Neural Networks\n(CNN) plus Recurrent Neural Networks (RNN) image captioning framework, for\ndescribing novel objects in captions. Specifically, freely available object\nrecognition datasets are leveraged to develop classifiers for novel objects.\nOur LSTM-C then nicely integrates the standard word-by-word sentence generation\nby a decoder RNN with copying mechanism which may instead select words from\nnovel objects at proper places in the output sentence. Extensive experiments\nare conducted on both MSCOCO image captioning and ImageNet datasets,\ndemonstrating the ability of our proposed LSTM-C architecture to describe novel\nobjects. Furthermore, superior results are reported when compared to\nstate-of-the-art deep models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 13:51:39 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Mei", "Tao", ""]]}, {"id": "1708.05340", "submitter": "Daniel Crispell", "authors": "Maxim Bazik, Daniel Crispell", "title": "Robust Registration and Geometry Estimation from Unstructured Facial\n  Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercial off the shelf (COTS) 3D scanners are capable of generating point\nclouds covering visible portions of a face with sub-millimeter accuracy at\nclose range, but lack the coverage and specialized anatomic registration\nprovided by more expensive 3D facial scanners. We demonstrate an effective\npipeline for joint alignment of multiple unstructured 3D point clouds and\nregistration to a parameterized 3D model which represents shape variation of\nthe human head. Most algorithms separate the problems of pose estimation and\nmesh warping, however we propose a new iterative method where these steps are\ninterwoven. Error decreases with each iteration, showing the proposed approach\nis effective in improving geometry and alignment. The approach described is\nused to align the NDOff-2007 dataset, which contains 7,358 individual scans at\nvarious poses of 396 subjects. The dataset has a number of full profile scans\nwhich are correctly aligned and contribute directly to the associated mesh\ngeometry. The dataset in its raw form contains a significant number of\nmislabeled scans, which are identified and corrected based on alignment error\nusing the proposed algorithm. The average point to surface distance between the\naligned scans and the produced geometries is one half millimeter.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 15:50:27 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bazik", "Maxim", ""], ["Crispell", "Daniel", ""]]}, {"id": "1708.05349", "submitter": "Aayush Bansal", "authors": "Aayush Bansal and Yaser Sheikh and Deva Ramanan", "title": "PixelNN: Example-based Image Synthesis", "comments": "Project Page: http://www.cs.cmu.edu/~aayushb/pixelNN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple nearest-neighbor (NN) approach that synthesizes\nhigh-frequency photorealistic images from an \"incomplete\" signal such as a\nlow-resolution image, a surface normal map, or edges. Current state-of-the-art\ndeep generative models designed for such conditional image synthesis lack two\nimportant things: (1) they are unable to generate a large set of diverse\noutputs, due to the mode collapse problem. (2) they are not interpretable,\nmaking it difficult to control the synthesized output. We demonstrate that NN\napproaches potentially address such limitations, but suffer in accuracy on\nsmall datasets. We design a simple pipeline that combines the best of both\nworlds: the first stage uses a convolutional neural network (CNN) to maps the\ninput to a (overly-smoothed) image, and the second stage uses a pixel-wise\nnearest neighbor method to map the smoothed output to multiple high-quality,\nhigh-frequency outputs in a controllable manner. We demonstrate our approach\nfor various input modalities, and for various domains ranging from human faces\nto cats-and-dogs to shoes and handbags.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:13:42 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bansal", "Aayush", ""], ["Sheikh", "Yaser", ""], ["Ramanan", "Deva", ""]]}, {"id": "1708.05355", "submitter": "Junhwa Hur", "authors": "Junhwa Hur and Stefan Roth", "title": "MirrorFlow: Exploiting Symmetries in Joint Optical Flow and Occlusion\n  Estimation", "comments": "14 pages, To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation is one of the most studied problems in computer\nvision, yet recent benchmark datasets continue to reveal problem areas of\ntoday's approaches. Occlusions have remained one of the key challenges. In this\npaper, we propose a symmetric optical flow method to address the well-known\nchicken-and-egg relation between optical flow and occlusions. In contrast to\nmany state-of-the-art methods that consider occlusions as outliers, possibly\nfiltered out during post-processing, we highlight the importance of joint\nocclusion reasoning in the optimization and show how to utilize occlusion as an\nimportant cue for estimating optical flow. The key feature of our model is to\nfully exploit the symmetry properties that characterize optical flow and\nocclusions in the two consecutive images. Specifically through utilizing\nforward-backward consistency and occlusion-disocclusion symmetry in the energy,\nour model jointly estimates optical flow in both forward and backward\ndirection, as well as consistent occlusion maps in both views. We demonstrate\nsignificant performance benefits on standard benchmarks, especially from the\nocclusion-disocclusion symmetry. On the challenging KITTI dataset we report the\nmost accurate two-frame results to date.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:32:34 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Hur", "Junhwa", ""], ["Roth", "Stefan", ""]]}, {"id": "1708.05375", "submitter": "Abhishek Kar", "authors": "Abhishek Kar, Christian H\\\"ane, Jitendra Malik", "title": "Learning a Multi-View Stereo Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learnt system for multi-view stereopsis. In contrast to recent\nlearning based methods for 3D reconstruction, we leverage the underlying 3D\ngeometry of the problem through feature projection and unprojection along\nviewing rays. By formulating these operations in a differentiable manner, we\nare able to learn the system end-to-end for the task of metric 3D\nreconstruction. End-to-end learning allows us to jointly reason about shape\npriors while conforming geometric constraints, enabling reconstruction from\nmuch fewer images (even a single image) than required by classical approaches\nas well as completion of unseen surfaces. We thoroughly evaluate our approach\non the ShapeNet dataset and demonstrate the benefits over classical approaches\nas well as recent learning based methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 17:36:40 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Kar", "Abhishek", ""], ["H\u00e4ne", "Christian", ""], ["Malik", "Jitendra", ""]]}, {"id": "1708.05401", "submitter": "arXiv Admin", "authors": "Vamshhi Pavan Kumar Varma Vegeshna", "title": "Deformable Modeling for Human Body Acquired from Depth Sensors", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to reconstruct complete 3D deformable\nmodels over time by a single depth camera. These are the steps employed for\ndeforming objects from single depth camera. The partial surfaces reconstructed\nfrom various times of capture are assembled together to form a complete 3D\nsurface. A mesh warping algorithm is used to align different partial surfaces\nbased on linear mesh deformation. A volumetric method is then applied to\ncombine partial surfaces, fix missing holes and smooth alignment errors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 18:15:52 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 13:32:14 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Vegeshna", "Vamshhi Pavan Kumar Varma", ""]]}, {"id": "1708.05464", "submitter": "Dustin Morley", "authors": "Dustin Morley, Hassan Foroosh, Saad Shaikh, Ulas Bagci", "title": "Simultaneous Detection and Quantification of Retinal Fluid with Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new deep learning approach for automatic detection and\nsegmentation of fluid within retinal OCT images. The proposed framework\nutilizes both ResNet and Encoder-Decoder neural network architectures. When\ntraining the network, we apply a novel data augmentation method called myopic\nwarping together with standard rotation-based augmentation to increase the\ntraining set size to 45 times the original amount. Finally, the network output\nis post-processed with an energy minimization algorithm (graph cut) along with\na few other knowledge guided morphological operations to finalize the\nsegmentation process. Based on OCT imaging data and its ground truth from the\nRETOUCH challenge, the proposed system achieves dice indices of 0.522, 0.682,\nand 0.612, and average absolute volume differences of 0.285, 0.115, and 0.156\nmm$^3$ for intaretinal fluid, subretinal fluid, and pigment epithelial\ndetachment respectively.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 23:31:05 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Morley", "Dustin", ""], ["Foroosh", "Hassan", ""], ["Shaikh", "Saad", ""], ["Bagci", "Ulas", ""]]}, {"id": "1708.05465", "submitter": "Yang Wang", "authors": "Yang Wang, Vinh Tran, Minh Hoai", "title": "Eigen Evolution Pooling for Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Eigen Evolution Pooling, an efficient method to aggregate a\nsequence of feature vectors. Eigen evolution pooling is designed to produce\ncompact feature representations for a sequence of feature vectors, while\nmaximally preserving as much information about the sequence as possible,\nespecially the temporal evolution of the features over time. Eigen evolution\npooling is a general pooling method that can be applied to any sequence of\nfeature vectors, from low-level RGB values to high-level Convolutional Neural\nNetwork (CNN) feature vectors. We show that eigen evolution pooling is more\neffective than average, max, and rank pooling for encoding the dynamics of\nhuman actions in video. We demonstrate the power of eigen evolution pooling on\nUCF101 and Hollywood2 datasets, two human action recognition benchmarks, and\nachieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 23:34:45 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Wang", "Yang", ""], ["Tran", "Vinh", ""], ["Hoai", "Minh", ""]]}, {"id": "1708.05473", "submitter": "Tianyang Wang", "authors": "Tianyang Wang, Mingxuan Sun, Kaoning Hu", "title": "Dilated Deep Residual Network for Image Denoising", "comments": "camera ready, 8 pages, accepted to IEEE ICTAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variations of deep neural networks such as convolutional neural network (CNN)\nhave been successfully applied to image denoising. The goal is to automatically\nlearn a mapping from a noisy image to a clean image given training data\nconsisting of pairs of noisy and clean images. Most existing CNN models for\nimage denoising have many layers. In such cases, the models involve a large\namount of parameters and are computationally expensive to train. In this paper,\nwe develop a dilated residual CNN for Gaussian image denoising. Compared with\nthe recently proposed residual denoiser, our method can achieve comparable\nperformance with less computational cost. Specifically, we enlarge receptive\nfield by adopting dilated convolution in residual network, and the dilation\nfactor is set to a certain value. We utilize appropriate zero padding to make\nthe dimension of the output the same as the input. It has been proven that the\nexpansion of receptive field can boost the CNN performance in image\nclassification, and we further demonstrate that it can also lead to competitive\nperformance for denoising problem. Moreover, we present a formula to calculate\nreceptive field size when dilated convolution is incorporated. Thus, the change\nof receptive field can be interpreted mathematically. To validate the efficacy\nof our approach, we conduct extensive experiments for both gray and color image\ndenoising with specific or randomized noise levels. Both of the quantitative\nmeasurements and the visual results of denoising are promising comparing with\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 00:30:41 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 05:24:31 GMT"}, {"version": "v3", "created": "Wed, 27 Sep 2017 23:37:46 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Wang", "Tianyang", ""], ["Sun", "Mingxuan", ""], ["Hu", "Kaoning", ""]]}, {"id": "1708.05493", "submitter": "Yinpeng Dong", "authors": "Yinpeng Dong, Hang Su, Jun Zhu, Fan Bao", "title": "Towards Interpretable Deep Neural Networks by Leveraging Adversarial\n  Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have demonstrated impressive performance on a\nwide array of tasks, but they are usually considered opaque since internal\nstructure and learned parameters are not interpretable. In this paper, we\nre-examine the internal representations of DNNs using adversarial images, which\nare generated by an ensemble-optimization algorithm. We find that: (1) the\nneurons in DNNs do not truly detect semantic objects/parts, but respond to\nobjects/parts only as recurrent discriminative patches; (2) deep visual\nrepresentations are not robust distributed codes of visual concepts because the\nrepresentations of adversarial images are largely not consistent with those of\nreal images, although they have similar visual appearance, both of which are\ndifferent from previous findings. To further improve the interpretability of\nDNNs, we propose an adversarial training scheme with a consistent loss such\nthat the neurons are endowed with human-interpretable concepts. The induced\ninterpretable representations enable us to trace eventual outcomes back to\ninfluential neurons. Therefore, human users can know how the models make\npredictions, as well as when and why they make errors.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 03:01:35 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Dong", "Yinpeng", ""], ["Su", "Hang", ""], ["Zhu", "Jun", ""], ["Bao", "Fan", ""]]}, {"id": "1708.05509", "submitter": "Yingtao Tian", "authors": "Yanghua Jin, Jiakai Zhang, Minjun Li, Yingtao Tian, Huachun Zhu,\n  Zhihao Fang", "title": "Towards the Automatic Anime Characters Creation with Generative\n  Adversarial Networks", "comments": "16 pages, 15 figures. This paper is presented as a Doujinshi in\n  Comiket 92, summer 2017, with the booth number 05a, East-U, Third Day", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of facial images has been well studied after the\nGenerative Adversarial Network (GAN) came out. There exists some attempts\napplying the GAN model to the problem of generating facial images of anime\ncharacters, but none of the existing work gives a promising result. In this\nwork, we explore the training of GAN models specialized on an anime facial\nimage dataset. We address the issue from both the data and the model aspect, by\ncollecting a more clean, well-suited dataset and leverage proper, empirical\napplication of DRAGAN. With quantitative analysis and case studies we\ndemonstrate that our efforts lead to a stable and high-quality model. Moreover,\nto assist people with anime character design, we build a website\n(http://make.girls.moe) with our pre-trained model available online, which\nmakes the model easily accessible to general public.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 04:57:28 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Jin", "Yanghua", ""], ["Zhang", "Jiakai", ""], ["Li", "Minjun", ""], ["Tian", "Yingtao", ""], ["Zhu", "Huachun", ""], ["Fang", "Zhihao", ""]]}, {"id": "1708.05512", "submitter": "Sanping Zhou", "authors": "Sanping Zhou, Jinjun Wang, Rui Shi, Qiqi Hou, Yihong Gong, Nanning\n  Zheng", "title": "Large Margin Learning in Set to Set Similarity Comparison for Person\n  Re-identification", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) aims at matching images of the same person\nacross disjoint camera views, which is a challenging problem in multimedia\nanalysis, multimedia editing and content-based media retrieval communities. The\nmajor challenge lies in how to preserve similarity of the same person across\nvideo footages with large appearance variations, while discriminating different\nindividuals. To address this problem, conventional methods usually consider the\npairwise similarity between persons by only measuring the point to point (P2P)\ndistance. In this paper, we propose to use deep learning technique to model a\nnovel set to set (S2S) distance, in which the underline objective focuses on\npreserving the compactness of intra-class samples for each camera view, while\nmaximizing the margin between the intra-class set and inter-class set. The S2S\ndistance metric is consisted of three terms, namely the class-identity term,\nthe relative distance term and the regularization term. The class-identity term\nkeeps the intra-class samples within each camera view gathering together, the\nrelative distance term maximizes the distance between the intra-class class set\nand inter-class set across different camera views, and the regularization term\nsmoothness the parameters of deep convolutional neural network (CNN). As a\nresult, the final learned deep model can effectively find out the matched\ntarget to the probe object among various candidates in the video gallery by\nlearning discriminative and stable feature representations. Using the CUHK01,\nCUHK03, PRID2011 and Market1501 benchmark datasets, we extensively conducted\ncomparative evaluations to demonstrate the advantages of our method over the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 05:19:01 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Shi", "Rui", ""], ["Hou", "Qiqi", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1708.05514", "submitter": "Weimin Wang", "authors": "Weimin Wang, Ken Sakurada, Nobuo Kawaguchi", "title": "Reflectance Intensity Assisted Automatic and Accurate Extrinsic\n  Calibration of 3D LiDAR and Panoramic Camera Using a Printed Chessboard", "comments": "20 pages, submitted to the journal of Remote Sensing", "journal-ref": "Remote Sensing, 9(8):851 (2017)", "doi": "10.3390/rs9080851", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for fully automatic and convenient\nextrinsic calibration of a 3D LiDAR and a panoramic camera with a normally\nprinted chessboard. The proposed method is based on the 3D corner estimation of\nthe chessboard from the sparse point cloud generated by one frame scan of the\nLiDAR. To estimate the corners, we formulate a full-scale model of the\nchessboard and fit it to the segmented 3D points of the chessboard. The model\nis fitted by optimizing the cost function under constraints of correlation\nbetween the reflectance intensity of laser and the color of the chessboard's\npatterns. Powell's method is introduced for resolving the discontinuity problem\nin optimization. The corners of the fitted model are considered as the 3D\ncorners of the chessboard. Once the corners of the chessboard in the 3D point\ncloud are estimated, the extrinsic calibration of the two sensors is converted\nto a 3D-2D matching problem. The corresponding 3D-2D points are used to\ncalculate the absolute pose of the two sensors with Unified Perspective-n-Point\n(UPnP). Further, the calculated parameters are regarded as initial values and\nare refined using the Levenberg-Marquardt method. The performance of the\nproposed corner detection method from the 3D point cloud is evaluated using\nsimulations. The results of experiments, conducted on a Velodyne HDL-32e LiDAR\nand a Ladybug3 camera under the proposed re-projection error metric,\nqualitatively and quantitatively demonstrate the accuracy and stability of the\nfinal extrinsic calibration parameters.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 05:58:02 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Wang", "Weimin", ""], ["Sakurada", "Ken", ""], ["Kawaguchi", "Nobuo", ""]]}, {"id": "1708.05529", "submitter": "Ayan Kumar Bhunia", "authors": "Partha Pratim Roy, Ayan Kumar Bhunia, Avirup Bhattacharyya, Umapada\n  Pal", "title": "Word Searching in Scene Image and Video Frame in Multi-Script Scenario\n  using Dynamic Shape Coding", "comments": "Multimedia Tools and Applications, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrieval of text information from natural scene images and video frames is a\nchallenging task due to its inherent problems like complex character shapes,\nlow resolution, background noise, etc. Available OCR systems often fail to\nretrieve such information in scene/video frames. Keyword spotting, an\nalternative way to retrieve information, performs efficient text searching in\nsuch scenarios. However, current word spotting techniques in scene/video images\nare script-specific and they are mainly developed for Latin script. This paper\npresents a novel word spotting framework using dynamic shape coding for text\nretrieval in natural scene image and video frames. The framework is designed to\nsearch query keyword from multiple scripts with the help of on-the-fly\nscript-wise keyword generation for the corresponding script. We have used a\ntwo-stage word spotting approach using Hidden Markov Model (HMM) to detect the\ntranslated keyword in a given text line by identifying the script of the line.\nA novel unsupervised dynamic shape coding based scheme has been used to group\nsimilar shape characters to avoid confusion and to improve text alignment.\nNext, the hypotheses locations are verified to improve retrieval performance.\nTo evaluate the proposed system for searching keyword from natural scene image\nand video frames, we have considered two popular Indic scripts such as Bangla\n(Bengali) and Devanagari along with English. Inspired by the zone-wise\nrecognition approach in Indic scripts[1], zone-wise text information has been\nused to improve the traditional word spotting performance in Indic scripts. For\nour experiment, a dataset consisting of images of different scenes and video\nframes of English, Bangla and Devanagari scripts were considered. The results\nobtained showed the effectiveness of our proposed word spotting approach.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:47:05 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 16:45:52 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 00:45:20 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 06:23:12 GMT"}, {"version": "v5", "created": "Wed, 27 Jun 2018 06:35:13 GMT"}, {"version": "v6", "created": "Mon, 30 Jul 2018 10:41:30 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Roy", "Partha Pratim", ""], ["Bhunia", "Ayan Kumar", ""], ["Bhattacharyya", "Avirup", ""], ["Pal", "Umapada", ""]]}, {"id": "1708.05543", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni and Daniele Fiorenti and Matteo Matteucci", "title": "Mesh-based 3D Textured Urban Mapping", "comments": "accepted at iros 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of autonomous driving, urban mapping represents a core step to let\nvehicles interact with the urban context. Successful mapping algorithms have\nbeen proposed in the last decade building the map leveraging on data from a\nsingle sensor. The focus of the system presented in this paper is twofold: the\njoint estimation of a 3D map from lidar data and images, based on a 3D mesh,\nand its texturing. Indeed, even if most surveying vehicles for mapping are\nendowed by cameras and lidar, existing mapping algorithms usually rely on\neither images or lidar data; moreover both image-based and lidar-based systems\noften represent the map as a point cloud, while a continuous textured mesh\nrepresentation would be useful for visualization and navigation purposes. In\nthe proposed framework, we join the accuracy of the 3D lidar data, and the\ndense information and appearance carried by the images, in estimating a\nvisibility consistent map upon the lidar measurements, and refining it\nphotometrically through the acquired images. We evaluate the proposed framework\nagainst the KITTI dataset and we show the performance improvement with respect\nto two state of the art urban mapping algorithms, and two widely used surface\nreconstruction algorithms in Computer Graphics.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 09:43:10 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Romanoni", "Andrea", ""], ["Fiorenti", "Daniele", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1708.05545", "submitter": "Amarnath R", "authors": "Amarnath R and P. Nagabhushan", "title": "Spotting Separator Points at Line Terminals in Compressed Document\n  Images for Text-line Segmentation", "comments": "Line separators, Document image analysis, Handwritten text,\n  Compression and decompression, RLE, CCITT. Line separator points at every\n  line terminal in a compressed handwritten document images enabling text line\n  segmentation", "journal-ref": "International Journal of Computer Applications 172(4): 40-47\n  (2017)", "doi": "10.5120/ijca2017915133", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line separators are used to segregate text-lines from one another in document\nimage analysis. Finding the separator points at every line terminal in a\ndocument image would enable text-line segmentation. In particular, identifying\nthe separators in handwritten text could be a thrilling exercise. Obviously it\nwould be challenging to perform this in the compressed version of a document\nimage and that is the proposed objective in this research. Such an effort would\nprevent the computational burden of decompressing a document for text-line\nsegmentation. Since document images are generally compressed using run length\nencoding (RLE) technique as per the CCITT standards, the first column in the\nRLE will be a white column. The value (depth) in the white column is very low\nwhen a particular line is a text line and the depth could be larger at the\npoint of text line separation. A longer consecutive sequence of such larger\ndepth should indicate the gap between the text lines, which provides the\nseparator region. In case of over separation and under separation issues,\ncorrective actions such as deletion and insertion are suggested respectively.\nAn extensive experimentation is conducted on the compressed images of the\nbenchmark datasets of ICDAR13 and Alireza et al [17] to demonstrate the\nefficacy.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 09:51:17 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["R", "Amarnath", ""], ["Nagabhushan", "P.", ""]]}, {"id": "1708.05552", "submitter": "Zhao Zhong", "authors": "Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, Cheng-Lin Liu", "title": "Practical Block-wise Neural Network Architecture Generation", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have gained a remarkable success in computer\nvision. However, most usable network architectures are hand-crafted and usually\nrequire expertise and elaborate design. In this paper, we provide a block-wise\nnetwork generation pipeline called BlockQNN which automatically builds\nhigh-performance networks using the Q-Learning paradigm with epsilon-greedy\nexploration strategy. The optimal network block is constructed by the learning\nagent which is trained sequentially to choose component layers. We stack the\nblock to construct the whole auto-generated network. To accelerate the\ngeneration process, we also propose a distributed asynchronous framework and an\nearly stop strategy. The block-wise generation brings unique advantages: (1) it\nperforms competitive results in comparison to the hand-crafted state-of-the-art\nnetworks on image classification, additionally, the best network generated by\nBlockQNN achieves 3.54% top-1 error rate on CIFAR-10 which beats all existing\nauto-generate networks. (2) in the meanwhile, it offers tremendous reduction of\nthe search space in designing networks which only spends 3 days with 32 GPUs,\nand (3) moreover, it has strong generalizability that the network built on\nCIFAR also performs well on a larger-scale ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 10:12:43 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 04:28:33 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 15:18:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhong", "Zhao", ""], ["Yan", "Junjie", ""], ["Wu", "Wei", ""], ["Shao", "Jing", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1708.05595", "submitter": "Huaxin Xiao", "authors": "Huaxin Xiao, Jiashi Feng, Yunchao Wei, Maojun Zhang", "title": "Self-explanatory Deep Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection has seen remarkable progress driven by deep learning\ntechniques. However, most of deep learning based salient object detection\nmethods are black-box in nature and lacking in interpretability. This paper\nproposes the first self-explanatory saliency detection network that explicitly\nexploits low- and high-level features for salient object detection. We\ndemonstrate that such supportive clues not only significantly enhances\nperformance of salient object detection but also gives better justified\ndetection results. More specifically, we develop a multi-stage saliency encoder\nto extract multi-scale features which contain both low- and high-level saliency\ncontext. Dense short- and long-range connections are introduced to reuse these\nfeatures iteratively. Benefiting from the direct access to low- and high-level\nfeatures, the proposed saliency encoder can not only model the object context\nbut also preserve the boundary. Furthermore, a self-explanatory generator is\nproposed to interpret how the proposed saliency encoder or other deep saliency\nmodels making decisions. The generator simulates the absence of interesting\nfeatures by preventing these features from contributing to the saliency\nclassifier and estimates the corresponding saliency prediction without these\nfeatures. A comparison function, saliency explanation, is defined to measure\nthe prediction changes between deep saliency models and corresponding\ngenerator. Through visualizing the differences, we can interpret the capability\nof different deep neural networks based saliency detection models and\ndemonstrate that our proposed model indeed uses more reasonable structure for\nsalient object detection. Extensive experiments on five popular benchmark\ndatasets and the visualized saliency explanation demonstrate that the proposed\nmethod provides new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 13:19:01 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Xiao", "Huaxin", ""], ["Feng", "Jiashi", ""], ["Wei", "Yunchao", ""], ["Zhang", "Maojun", ""]]}, {"id": "1708.05625", "submitter": "Roman Rabinovich", "authors": "Roman Rabinovich, Ibrahim Jubran, Aaron Wetzler, and Ron Kimmel", "title": "CoBe -- Coded Beacons for Localization, Object Tracking, and SLAM\n  Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel beacon light coding protocol, which enables fast\nand accurate identification of the beacons in an image. The protocol is\nprovably robust to a predefined set of detection and decoding errors, and does\nnot require any synchronization between the beacons themselves and the optical\nsensor. A detailed guide is then given for developing an optical tracking and\nlocalization system, which is based on the suggested protocol and readily\navailable hardware. Such a system operates either as a standalone system for\nrecovering the six degrees of freedom of fast moving objects, or integrated\nwith existing SLAM pipelines providing them with error-free and easily\nidentifiable landmarks. Based on this guide, we implemented a low-cost\npositional tracking system which can run in real-time on an IoT board. We\nevaluate our system's accuracy and compare it to other popular methods which\nutilize the same optical hardware, in experiments where the ground truth is\nknown. A companion video containing multiple real-world experiments\ndemonstrates the accuracy, speed, and applicability of the proposed system in a\nwide range of environments and real-world tasks. Open source code is provided\nto encourage further development of low-cost localization systems integrating\nthe suggested technology at its navigation core.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:22:45 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 19:16:56 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 20:48:15 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Rabinovich", "Roman", ""], ["Jubran", "Ibrahim", ""], ["Wetzler", "Aaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1708.05628", "submitter": "Siddharth Mahendran", "authors": "Siddharth Mahendran and Haider Ali and Rene Vidal", "title": "3D Pose Regression using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D pose estimation is a key component of many important computer vision tasks\nsuch as autonomous navigation and 3D scene understanding. Most state-of-the-art\napproaches to 3D pose estimation solve this problem as a pose-classification\nproblem in which the pose space is discretized into bins and a CNN classifier\nis used to predict a pose bin. We argue that the 3D pose space is continuous\nand propose to solve the pose estimation problem in a CNN regression framework\nwith a suitable representation, data augmentation and loss function that\ncaptures the geometry of the pose space. Experiments on PASCAL3D+ show that the\nproposed 3D pose regression approach achieves competitive performance compared\nto the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:36:11 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mahendran", "Siddharth", ""], ["Ali", "Haider", ""], ["Vidal", "Rene", ""]]}, {"id": "1708.05636", "submitter": "Daigo Shoji", "authors": "Daigo Shoji", "title": "What does a convolutional neural network recognize in the moon?", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV astro-ph.EP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people see a human face or animals in the pattern of the maria on the\nmoon. Although the pattern corresponds to the actual variation in composition\nof the lunar surface, the culture and environment of each society influence the\nrecognition of these objects (i.e., symbols) as specific entities. In contrast,\na convolutional neural network (CNN) recognizes objects from characteristic\nshapes in a training data set. Using CNN, this study evaluates the\nprobabilities of the pattern of lunar maria categorized into the shape of a\ncrab, a lion and a hare. If Mare Frigoris (a dark band on the moon) is included\nin the lunar image, the lion is recognized. However, in an image without Mare\nFrigoris, the hare has the highest probability of recognition. Thus, the\nrecognition of objects similar to the lunar pattern depends on which part of\nthe lunar maria is taken into account. In human recognition, before we find\nsimilarities between the lunar maria and objects such as animals, we may be\npersuaded in advance to see a particular image from our culture and environment\nand then adjust the lunar pattern to the shape of the imagined object.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:59:51 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 05:30:33 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Shoji", "Daigo", ""]]}, {"id": "1708.05711", "submitter": "Jan Egger", "authors": "Jan Egger, J\\\"urgen Wallner, Markus Gall, Xiaojun Chen, Katja\n  Schwenzer-Zimmerer, Knut Reinbacher, Dieter Schmalstieg", "title": "Computer-aided position planning of miniplates to treat facial bone\n  defects", "comments": "19 pages, 13 Figures, 2 Tables", "journal-ref": "PLoS ONE 12(8): e0182839 (2017)", "doi": "10.1371/journal.pone.0182839", "report-no": null, "categories": "cs.CV cs.CE cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, a software system for computer-aided position planning\nof miniplates to treat facial bone defects is proposed. The intra-operatively\nused bone plates have to be passively adapted on the underlying bone contours\nfor adequate bone fragment stabilization. However, this procedure can lead to\nfrequent intra-operatively performed material readjustments especially in\ncomplex surgical cases. Our approach is able to fit a selection of common\nimplant models on the surgeon's desired position in a 3D computer model. This\nhappens with respect to the surrounding anatomical structures, always including\nthe possibility of adjusting both the direction and the position of the used\nosteosynthesis material. By using the proposed software, surgeons are able to\npre-plan the out coming implant in its form and morphology with the aid of a\ncomputer-visualized model within a few minutes. Further, the resulting model\ncan be stored in STL file format, the commonly used format for 3D printing.\nUsing this technology, surgeons are able to print the virtual generated\nimplant, or create an individually designed bending tool. This method leads to\nadapted osteosynthesis materials according to the surrounding anatomy and\nrequires further a minimum amount of money and time.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 18:37:02 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Egger", "Jan", ""], ["Wallner", "J\u00fcrgen", ""], ["Gall", "Markus", ""], ["Chen", "Xiaojun", ""], ["Schwenzer-Zimmerer", "Katja", ""], ["Reinbacher", "Knut", ""], ["Schmalstieg", "Dieter", ""]]}, {"id": "1708.05812", "submitter": "Gustav Larsson", "authors": "Gustav Larsson", "title": "Discovery of Visual Semantics by Unsupervised and Self-Supervised\n  Representation Learning", "comments": "Ph.D. thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning in computer vision is rooted in the ability of\ndeep networks to scale up model complexity as demanded by challenging visual\ntasks. As complexity is increased, so is the need for large amounts of labeled\ndata to train the model. This is associated with a costly human annotation\neffort. To address this concern, with the long-term goal of leveraging the\nabundance of cheap unlabeled data, we explore methods of unsupervised\n\"pre-training.\" In particular, we propose to use self-supervised automatic\nimage colorization.\n  We show that traditional methods for unsupervised learning, such as\nlayer-wise clustering or autoencoders, remain inferior to supervised\npre-training. In search for an alternative, we develop a fully automatic image\ncolorization method. Our method sets a new state-of-the-art in revitalizing old\nblack-and-white photography, without requiring human effort or expertise.\nAdditionally, it gives us a method for self-supervised representation learning.\nIn order for the model to appropriately re-color a grayscale object, it must\nfirst be able to identify it. This ability, learned entirely self-supervised,\ncan be used to improve other visual tasks, such as classification and semantic\nsegmentation. As a future direction for self-supervision, we investigate if\nmultiple proxy tasks can be combined to improve generalization. This turns out\nto be a challenging open problem. We hope that our contributions to this\nendeavor will provide a foundation for future efforts in making\nself-supervision compete with supervised pre-training.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 06:38:53 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Larsson", "Gustav", ""]]}, {"id": "1708.05827", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng, William B. Shen, De-An Huang, Min Sun, Juan Carlos\n  Niebles", "title": "Visual Forecasting by Imitating Dynamics in Natural Sequences", "comments": "10 pages, 9 figures, accepted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for visual forecasting, which directly\nimitates visual sequences without additional supervision. As a result, our\nmodel can be applied at several semantic levels and does not require any domain\nknowledge or handcrafted features. We achieve this by formulating visual\nforecasting as an inverse reinforcement learning (IRL) problem, and directly\nimitate the dynamics in natural sequences from their raw pixel values. The key\nchallenge is the high-dimensional and continuous state-action space that\nprohibits the application of previous IRL algorithms. We address this\ncomputational bottleneck by extending recent progress in model-free imitation\nwith trainable deep feature representations, which (1) bypasses the exhaustive\nstate-action pair visits in dynamic programming by using a dual formulation and\n(2) avoids explicit state sampling at gradient computation using a deep feature\nreparametrization. This allows us to apply IRL at scale and directly imitate\nthe dynamics in high-dimensional continuous visual sequences from the raw pixel\nvalues. We evaluate our approach at three different level-of-abstraction, from\nlow level pixels to higher level semantics: future frame generation, action\nanticipation, visual story forecasting. At all levels, our approach outperforms\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 09:45:52 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Shen", "William B.", ""], ["Huang", "De-An", ""], ["Sun", "Min", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1708.05828", "submitter": "Alex James Dr", "authors": "Damira Pernebayeva, Mehdi Bagheri, and Alex Pappachen James", "title": "High Voltage Insulator Surface Evaluation Using Image Processing", "comments": "2017 International Symposium on Electrical Insulating Materials,\n  September 12-15, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High voltage insulators are widely deployed in power systems to isolate the\nlive- and dead-part of overhead lines as well as to support the power line\nconductors mechanically. Permanent, secure and safe operation of power\ntransmission lines require that the high voltage insulators are inspected and\nmonitor, regularly. Severe environment conditions will influence insulator\nsurface and change creepage distance. Consequently, power utilities and\ntransmission companies face significant problem in operation due to insulator\ndamage or contamination. In this study, a new technique is developed for\nreal-time inspection of insulator and estimating the snow, ice and water over\nthe insulator surface which can be a potential risk of operation breakdown. To\nexamine the proposed system, practical experiment is conducted using ceramic\ninsulator for capturing the images with snow, ice and wet surface conditions.\nGabor and Standard deviation filters are utilized for image feature extraction.\nThe best achieved recognition accuracy rate was 87% using statistical approach\nthe Standard deviation.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 10:10:24 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Pernebayeva", "Damira", ""], ["Bagheri", "Mehdi", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1708.05840", "submitter": "Disha Shrivastava", "authors": "Disha Shrivastava, Santanu Chaudhury and Dr. Jayadeva", "title": "A Data and Model-Parallel, Distributed and Scalable Framework for\n  Training of Deep Networks in Apache Spark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is expensive and time-consuming with the training\nperiod increasing with data size and growth in model parameters. In this paper,\nwe provide a framework for distributed training of deep networks over a cluster\nof CPUs in Apache Spark. The framework implements both Data Parallelism and\nModel Parallelism making it suitable to use for deep networks which require\nhuge training data and model parameters which are too big to fit into the\nmemory of a single machine. It can be scaled easily over a cluster of cheap\ncommodity hardware to attain significant speedup and obtain better results\nmaking it quite economical as compared to farm of GPUs and supercomputers. We\nhave proposed a new algorithm for training of deep networks for the case when\nthe network is partitioned across the machines (Model Parallelism) along with\ndetailed cost analysis and proof of convergence of the same. We have developed\nimplementations for Fully-Connected Feedforward Networks, Convolutional Neural\nNetworks, Recurrent Neural Networks and Long Short-Term Memory architectures.\nWe present the results of extensive simulations demonstrating the speedup and\naccuracy obtained by our framework for different sizes of the data and model\nparameters with variation in the number of worker cores/partitions; thereby\nshowing that our proposed framework can achieve significant speedup (upto 11X\nfor CNN) and is also quite scalable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 13:17:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Shrivastava", "Disha", ""], ["Chaudhury", "Santanu", ""], ["Jayadeva", "Dr.", ""]]}, {"id": "1708.05851", "submitter": "Di Hu", "authors": "Xuelong Li and Di Hu and Xiaoqiang Lu", "title": "Image2song: Song Retrieval via Bridging Image Content and Lyric Words", "comments": "13 pages, 13 figures, accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image is usually taken for expressing some kinds of emotions or purposes,\nsuch as love, celebrating Christmas. There is another better way that combines\nthe image and relevant song to amplify the expression, which has drawn much\nattention in the social network recently. Hence, the automatic selection of\nsongs should be expected. In this paper, we propose to retrieve semantic\nrelevant songs just by an image query, which is named as the image2song\nproblem. Motivated by the requirements of establishing correlation in\nsemantic/content, we build a semantic-based song retrieval framework, which\nlearns the correlation between image content and lyric words. This model uses a\nconvolutional neural network to generate rich tags from image regions, a\nrecurrent neural network to model lyric, and then establishes correlation via a\nmulti-layer perceptron. To reduce the content gap between image and lyric, we\npropose to make the lyric modeling focus on the main image content via a tag\nattention. We collect a dataset from the social-sharing multimodal data to\nstudy the proposed problem, which consists of (image, music clip, lyric)\ntriplets. We demonstrate that our proposed model shows noticeable results in\nthe image2song retrieval task and provides suitable songs. Besides, the\nsong2image task is also performed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 14:17:44 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Li", "Xuelong", ""], ["Hu", "Di", ""], ["Lu", "Xiaoqiang", ""]]}, {"id": "1708.05866", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony\n  Bharath", "title": "A Brief Survey of Deep Reinforcement Learning", "comments": "IEEE Signal Processing Magazine, Special Issue on Deep Learning for\n  Image Understanding (arXiv extended version)", "journal-ref": null, "doi": "10.1109/MSP.2017.2743240", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 15:55:31 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 21:51:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Deisenroth", "Marc Peter", ""], ["Brundage", "Miles", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1708.05869", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Vincent Casser, Jean Lahoud, Neil Smith, Bernard\n  Ghanem", "title": "Sim4CV: A Photo-Realistic Simulator for Computer Vision Applications", "comments": "Published at the International Journal of Computer Vision (IJCV),\n  2018", "journal-ref": null, "doi": "10.1007/s11263-018-1073-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a photo-realistic training and evaluation simulator (Sim4CV) with\nextensive applications across various fields of computer vision. Built on top\nof the Unreal Engine, the simulator integrates full featured physics based\ncars, unmanned aerial vehicles (UAVs), and animated human actors in diverse\nurban and suburban 3D environments. We demonstrate the versatility of the\nsimulator with two case studies: autonomous UAV-based tracking of moving\nobjects and autonomous driving using supervised learning. The simulator fully\nintegrates both several state-of-the-art tracking algorithms with a benchmark\nevaluation tool and a deep neural network (DNN) architecture for training\nvehicles to drive autonomously. It generates synthetic photo-realistic datasets\nwith automatic ground truth annotations to easily extend existing real-world\ndatasets and provides extensive synthetic data variety through its ability to\nreconfigure synthetic worlds on the fly using an automatic world generation\ntool. The supplementary video can be viewed a https://youtu.be/SqAxzsQ7qUU\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 16:09:06 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 14:34:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Casser", "Vincent", ""], ["Lahoud", "Jean", ""], ["Smith", "Neil", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1708.05884", "submitter": "Matthias M\\\"uller", "authors": "Matthias M\\\"uller, Vincent Casser, Neil Smith, Dominik L. Michels,\n  Bernard Ghanem", "title": "Teaching UAVs to Race: End-to-End Regression of Agile Controls in\n  Simulation", "comments": "Accepted at ECCVW'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the navigation of unmanned aerial vehicles (UAVs) in diverse\nscenarios has gained much attention in recent years. However, teaching UAVs to\nfly in challenging environments remains an unsolved problem, mainly due to the\nlack of training data. In this paper, we train a deep neural network to predict\nUAV controls from raw image data for the task of autonomous UAV racing in a\nphoto-realistic simulation. Training is done through imitation learning with\ndata augmentation to allow for the correction of navigation mistakes. Extensive\nexperiments demonstrate that our trained network (when sufficient data\naugmentation is used) outperforms state-of-the-art methods and flies more\nconsistently than many human pilots. Additionally, we show that our optimized\nnetwork architecture can run in real-time on embedded hardware, allowing for\nefficient on-board processing critical for real-world deployment. From a\nbroader perspective, our results underline the importance of extensive data\naugmentation techniques to improve robustness in end-to-end learning setups.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 18:13:23 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 16:16:38 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 14:51:12 GMT"}, {"version": "v4", "created": "Thu, 22 Nov 2018 15:12:02 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["M\u00fcller", "Matthias", ""], ["Casser", "Vincent", ""], ["Smith", "Neil", ""], ["Michels", "Dominik L.", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1708.05897", "submitter": "Mizuho Nishio", "authors": "Mizuho Nishio, Mitsuo Nishizawa, Osamu Sugiyama, Ryosuke Kojima,\n  Masahiro Yakami, Tomohiro Kuroda, Kaori Togashi", "title": "Computer-aided diagnosis of lung nodule using gradient tree boosting and\n  Bayesian optimization", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0195875", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aimed to evaluate computer-aided diagnosis (CADx) system for lung nodule\nclassification focusing on (i) usefulness of gradient tree boosting (XGBoost)\nand (ii) effectiveness of parameter optimization using Bayesian optimization\n(Tree Parzen Estimator, TPE) and random search. 99 lung nodules (62 lung\ncancers and 37 benign lung nodules) were included from public databases of CT\nimages. A variant of local binary pattern was used for calculating feature\nvectors. Support vector machine (SVM) or XGBoost was trained using the feature\nvectors and their labels. TPE or random search was used for parameter\noptimization of SVM and XGBoost. Leave-one-out cross-validation was used for\noptimizing and evaluating the performance of our CADx system. Performance was\nevaluated using area under the curve (AUC) of receiver operating characteristic\nanalysis. AUC was calculated 10 times, and its average was obtained. The best\naveraged AUC of SVM and XGBoost were 0.850 and 0.896, respectively; both were\nobtained using TPE. XGBoost was generally superior to SVM. Optimal parameters\nfor achieving high AUC were obtained with fewer numbers of trials when using\nTPE, compared with random search. In conclusion, XGBoost was better than SVM\nfor classifying lung nodules. TPE was more efficient than random search for\nparameter optimization.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 20:20:34 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 11:52:13 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Nishio", "Mizuho", ""], ["Nishizawa", "Mitsuo", ""], ["Sugiyama", "Osamu", ""], ["Kojima", "Ryosuke", ""], ["Yakami", "Masahiro", ""], ["Kuroda", "Tomohiro", ""], ["Togashi", "Kaori", ""]]}, {"id": "1708.05966", "submitter": "Ribana Roscher", "authors": "Ribana Roscher, Bj\\\"orn Waske, Wolfgang F\\\"orstner", "title": "Incremental Import Vector Machines for Classifying Hyperspectral Data", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Vol.50, No.09,\n  September 2012, 3463-3473", "doi": "10.1109/TGRS.2012.2184292", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an incremental learning strategy for import vector\nmachines (IVM), which is a sparse kernel logistic regression approach. We use\nthe procedure for the concept of self-training for sequential classification of\nhyperspectral data. The strategy comprises the inclusion of new training\nsamples to increase the classification accuracy and the deletion of\nnon-informative samples to be memory- and runtime-efficient. Moreover, we\nupdate the parameters in the incremental IVM model without re-training from\nscratch. Therefore, the incremental classifier is able to deal with large data\nsets. The performance of the IVM in comparison to support vector machines (SVM)\nis evaluated in terms of accuracy and experiments are conducted to assess the\npotential of the probabilistic outputs of the IVM. Experimental results\ndemonstrate that the IVM and SVM perform similar in terms of classification\naccuracy. However, the number of import vectors is significantly lower when\ncompared to the number of support vectors and thus, the computation time during\nclassification can be decreased. Moreover, the probabilities provided by IVM\nare more reliable, when compared to the probabilistic information, derived from\nan SVM's output. In addition, the proposed self-training strategy can increase\nthe classification accuracy. Overall, the IVM and the its incremental version\nis worthwhile for the classification of hyperspectral data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 13:59:00 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Roscher", "Ribana", ""], ["Waske", "Bj\u00f6rn", ""], ["F\u00f6rstner", "Wolfgang", ""]]}, {"id": "1708.05969", "submitter": "Abdul Kawsar Tushar", "authors": "Akm Ashiquzzaman, Abdul Kawsar Tushar, Ashiqur Rahman", "title": "Applying Data Augmentation to Handwritten Arabic Numeral Recognition\n  Using Deep Learning Neural Networks", "comments": "5 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten character recognition has been the center of research and a\nbenchmark problem in the sector of pattern recognition and artificial\nintelligence, and it continues to be a challenging research topic. Due to its\nenormous application many works have been done in this field focusing on\ndifferent languages. Arabic, being a diversified language has a huge scope of\nresearch with potential challenges. A convolutional neural network model for\nrecognizing handwritten numerals in Arabic language is proposed in this paper,\nwhere the dataset is subject to various augmentation in order to add robustness\nneeded for deep learning approach. The proposed method is empowered by the\npresence of dropout regularization to do away with the problem of data\noverfitting. Moreover, suitable change is introduced in activation function to\novercome the problem of vanishing gradient. With these modifications, the\nproposed system achieves an accuracy of 99.4\\% which performs better than every\nprevious work on the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:21:05 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 15:18:37 GMT"}, {"version": "v3", "created": "Thu, 24 Aug 2017 16:29:29 GMT"}, {"version": "v4", "created": "Wed, 27 Sep 2017 14:54:32 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Ashiquzzaman", "Akm", ""], ["Tushar", "Abdul Kawsar", ""], ["Rahman", "Ashiqur", ""]]}, {"id": "1708.05974", "submitter": "Ribana Roscher", "authors": "Ribana Roscher, Bj\\\"orn Waske", "title": "Shapelet-based Sparse Representation for Landcover Classification of\n  Hyperspectral Images", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, Volume: 54,\n  Issue: 3, March 2016, 1623 - 1634", "doi": "10.1109/TGRS.2015.2484619", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a sparse representation-based classification approach\nwith a novel dictionary construction procedure. By using the constructed\ndictionary sophisticated prior knowledge about the spatial nature of the image\ncan be integrated. The approach is based on the assumption that each image\npatch can be factorized into characteristic spatial patterns, also called\nshapelets, and patch-specific spectral information. A set of shapelets is\nlearned in an unsupervised way and spectral information are embodied by\ntraining samples. A combination of shapelets and spectral information are\nrepresented in an undercomplete spatial-spectral dictionary for each individual\npatch, where the elements of the dictionary are linearly combined to a sparse\nrepresentation of the patch. The patch-based classification is obtained by\nmeans of the representation error. Experiments are conducted on three\nwell-known hyperspectral image datasets. They illustrate that our proposed\napproach shows superior results in comparison to sparse representation-based\nclassifiers that use only limited spatial information and behaves competitively\nwith or better than state-of-the-art classifiers utilizing spatial information\nand kernelized sparse representation-based classifiers.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:36:11 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Roscher", "Ribana", ""], ["Waske", "Bj\u00f6rn", ""]]}, {"id": "1708.05979", "submitter": "Abdul Kawsar Tushar", "authors": "Mohammad Asiful Hossain, Abdul Kawsar Tushar, Shofiullah Babor", "title": "An Efficient Single Chord-based Accumulation Technique (SCA) to Detect\n  More Reliable Corners", "comments": "5 pages, 7 figures, 2 tables, Accepted on 4th International\n  Conference on Advances in Electrical Engineering (ICAEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corner detection is a vital operation in numerous computer vision\napplications. The Chord-to-Point Distance Accumulation (CPDA) detector is\nrecognized as the contour-based corner detector producing the lowest\nlocalization error while localizing corners in an image. However, in our\nexperiment part, we demonstrate that CPDA detector often misses some potential\ncorners. Moreover, the detection algorithm of CPDA is computationally costly.\nIn this paper, We focus on reducing localization error as well as increasing\naverage repeatability. The preprocessing and refinements steps of proposed\nprocess are similar to CPDA. Our experimental results will show the\neffectiveness and robustness of proposed process over CPDA.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:55:02 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Hossain", "Mohammad Asiful", ""], ["Tushar", "Abdul Kawsar", ""], ["Babor", "Shofiullah", ""]]}, {"id": "1708.05980", "submitter": "Gaurav Mittal", "authors": "Tanya Marwah, Gaurav Mittal, Vineeth N. Balasubramanian", "title": "Attentive Semantic Video Generation using Captions", "comments": null, "journal-ref": "Presented at ICCV 2017 (International Conference on Computer\n  Vision)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a network architecture to perform variable length\nsemantic video generation using captions. We adopt a new perspective towards\nvideo generation where we allow the captions to be combined with the long-term\nand short-term dependencies between video frames and thus generate a video in\nan incremental manner. Our experiments demonstrate our network architecture's\nability to distinguish between objects, actions and interactions in a video and\ncombine them to generate videos for unseen captions. The network also exhibits\nthe capability to perform spatio-temporal style transfer when asked to generate\nvideos for a sequence of captions. We also show that the network's ability to\nlearn a latent representation allows it generate videos in an unsupervised\nmanner and perform other tasks such as action recognition. (Accepted in\nInternational Conference in Computer Vision (ICCV) 2017)\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 14:55:19 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 16:25:53 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 21:12:41 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Marwah", "Tanya", ""], ["Mittal", "Gaurav", ""], ["Balasubramanian", "Vineeth N.", ""]]}, {"id": "1708.06023", "submitter": "Jiankang Deng", "authors": "Jiankang Deng, George Trigeorgis, Yuxiang Zhou, Stefanos Zafeiriou", "title": "Joint Multi-view Face Alignment in the Wild", "comments": "submit to IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The de facto algorithm for facial landmark estimation involves running a face\ndetector with a subsequent deformable model fitting on the bounding box. This\nencompasses two basic problems: i) the detection and deformable fitting steps\nare performed independently, while the detector might not provide best-suited\ninitialisation for the fitting step, ii) the face appearance varies hugely\nacross different poses, which makes the deformable face fitting very\nchallenging and thus distinct models have to be used (\\eg, one for profile and\none for frontal faces). In this work, we propose the first, to the best of our\nknowledge, joint multi-view convolutional network to handle large pose\nvariations across faces in-the-wild, and elegantly bridge face detection and\nfacial landmark localisation tasks. Existing joint face detection and landmark\nlocalisation methods focus only on a very small set of landmarks. By contrast,\nour method can detect and align a large number of landmarks for semi-frontal\n(68 landmarks) and profile (39 landmarks) faces. We evaluate our model on a\nplethora of datasets including standard static image datasets such as IBUG,\n300W, COFW, and the latest Menpo Benchmark for both semi-frontal and profile\nfaces. Significant improvement over state-of-the-art methods on deformable face\ntracking is witnessed on 300VW benchmark. We also demonstrate state-of-the-art\nresults for face detection on FDDB and MALF datasets.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 21:20:18 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Deng", "Jiankang", ""], ["Trigeorgis", "George", ""], ["Zhou", "Yuxiang", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1708.06026", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze and Simon J. Julier", "title": "DeepBreath: Deep Learning of Breathing Patterns for Automatic Stress\n  Recognition using Low-Cost Thermal Imaging in Unconstrained Settings", "comments": "Submitted to \"2017 7th International Conference on Affective\n  Computing and Intelligent Interaction (ACII)\" - ACII 2017", "journal-ref": null, "doi": "10.1109/ACII.2017.8273639", "report-no": null, "categories": "cs.HC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepBreath, a deep learning model which automatically recognises\npeople's psychological stress level (mental overload) from their breathing\npatterns. Using a low cost thermal camera, we track a person's breathing\npatterns as temperature changes around his/her nostril. The paper's technical\ncontribution is threefold. First of all, instead of creating hand-crafted\nfeatures to capture aspects of the breathing patterns, we transform the\nuni-dimensional breathing signals into two dimensional respiration variability\nspectrogram (RVS) sequences. The spectrograms easily capture the complexity of\nthe breathing dynamics. Second, a spatial pattern analysis based on a deep\nConvolutional Neural Network (CNN) is directly applied to the spectrogram\nsequences without the need of hand-crafting features. Finally, a data\naugmentation technique, inspired from solutions for over-fitting problems in\ndeep learning, is applied to allow the CNN to learn with a small-scale dataset\nfrom short-term measurements (e.g., up to a few hours). The model is trained\nand tested with data collected from people exposed to two types of cognitive\ntasks (Stroop Colour Word Test, Mental Computation test) with sessions of\ndifferent difficulty levels. Using normalised self-report as ground truth, the\nCNN reaches 84.59% accuracy in discriminating between two levels of stress and\n56.52% in discriminating between three levels. In addition, the CNN\noutperformed powerful shallow learning methods based on a single layer neural\nnetwork. Finally, the dataset of labelled thermal images will be open to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 21:36:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Julier", "Simon J.", ""]]}, {"id": "1708.06039", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier\n  Giro-i-Nieto, Brendan Jou and Shih-Fu Chang", "title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "comments": "Oral paper at ACM Multimedia 2017 Workshop on Multimodal\n  Understanding of Social, Affective and Subjective Attributes (MUSA2)", "journal-ref": null, "doi": "10.1145/3132515.3132520", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of affect-rich multimedia resources has bolstered\ninterest in understanding sentiment and emotions in and from visual content.\nAdjective-noun pairs (ANP) are a popular mid-level semantic construct for\ncapturing affect via visually detectable concepts such as \"cute dog\" or\n\"beautiful landscape\". Current state-of-the-art methods approach ANP prediction\nby considering each of these compound concepts as individual tokens, ignoring\nthe underlying relationships in ANPs. This work aims at disentangling the\ncontributions of the `adjectives' and `nouns' in the visual prediction of ANPs.\nTwo specialised classifiers, one trained for detecting adjectives and another\nfor nouns, are fused to predict 553 different ANPs. The resulting ANP\nprediction model is more interpretable as it allows us to study contributions\nof the adjective and noun components. Source code and models are available at\nhttps://imatge-upc.github.io/affective-2017-musa2/ .\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 00:33:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Fernandez", "Delia", ""], ["Woodward", "Alejandro", ""], ["Campos", "Victor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06118", "submitter": "Tommi Kerola", "authors": "Satoshi Tsutsui, Tommi Kerola and Shunta Saito", "title": "Distantly Supervised Road Segmentation", "comments": "Accepted for ICCV workshop CVRSUAD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for road segmentation that only requires image-level\nannotations at training time. We leverage distant supervision, which allows us\nto train our model using images that are different from the target domain.\nUsing large publicly available image databases as distant supervisors, we\ndevelop a simple method to automatically generate weak pixel-wise road masks.\nThese are used to iteratively train a fully convolutional neural network, which\nproduces our final segmentation model. We evaluate our method on the Cityscapes\ndataset, where we compare it with a fully supervised approach. Further, we\ndiscuss the trade-off between annotation cost and performance. Overall, our\ndistantly supervised approach achieves 93.8% of the performance of the fully\nsupervised approach, while using orders of magnitude less annotation work.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 08:34:17 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Tsutsui", "Satoshi", ""], ["Kerola", "Tommi", ""], ["Saito", "Shunta", ""]]}, {"id": "1708.06126", "submitter": "Albert Berenguel", "authors": "Albert Berenguel, Oriol Ramos Terrades, Josep Llad\\'os, Cristina\n  Ca\\~nero", "title": "e-Counterfeit: a mobile-server platform for document counterfeit\n  detection", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel application to detect counterfeit identity\ndocuments forged by a scan-printing operation. Texture analysis approaches are\nproposed to extract validation features from security background that is\nusually printed in documents as IDs or banknotes. The main contribution of this\nwork is the end-to-end mobile-server architecture, which provides a service for\nnon-expert users and therefore can be used in several scenarios. The system\nalso provides a crowdsourcing mode so labeled images can be gathered,\ngenerating databases for incremental training of the algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 09:24:46 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Berenguel", "Albert", ""], ["Terrades", "Oriol Ramos", ""], ["Llad\u00f3s", "Josep", ""], ["Ca\u00f1ero", "Cristina", ""]]}, {"id": "1708.06128", "submitter": "Jasper Uijlings", "authors": "Jasper Uijlings, Stefan Popov and Vittorio Ferrari", "title": "Revisiting knowledge transfer for training object class detectors", "comments": "CVPR 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to revisit knowledge transfer for training object detectors on\ntarget classes from weakly supervised training images, helped by a set of\nsource classes with bounding-box annotations. We present a unified knowledge\ntransfer framework based on training a single neural network multi-class object\ndetector over all source classes, organized in a semantic hierarchy. This\ngenerates proposals with scores at multiple levels in the hierarchy, which we\nuse to explore knowledge transfer over a broad range of generality, ranging\nfrom class-specific (bicycle to motorbike) to class-generic (objectness to any\nclass). Experiments on the 200 object classes in the ILSVRC 2013 detection\ndataset show that our technique: (1) leads to much better performance on the\ntarget classes (70.3% CorLoc, 36.9% mAP) than a weakly supervised baseline\nwhich uses manually engineered objectness [11] (50.5% CorLoc, 25.4% mAP). (2)\ndelivers target object detectors reaching 80% of the mAP of their fully\nsupervised counterparts. (3) outperforms the best reported transfer learning\nresults on this dataset (+41% CorLoc and +3% mAP over [18, 46], +16.2% mAP over\n[32]). Moreover, we also carry out several across-dataset knowledge transfer\nexperiments [27, 24, 35] and find that (4) our technique outperforms the weakly\nsupervised baseline in all dataset pairs by 1.5x-1.9x, establishing its general\napplicability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 09:39:44 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 09:53:53 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 14:51:59 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Uijlings", "Jasper", ""], ["Popov", "Stefan", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1708.06197", "submitter": "Karthik Gopinath", "authors": "Karthik Gopinath and Jayanthi Sivaswamy", "title": "Segmentation of retinal cysts from Optical Coherence Tomography volumes\n  via selective enhancement", "comments": "Under review in Journal of Biomedical and Health Informatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated and accurate segmentation of cystoid structures in Optical\nCoherence Tomography (OCT) is of interest in the early detection of retinal\ndiseases. It is, however, a challenging task. We propose a novel method for\nlocalizing cysts in 3D OCT volumes. The proposed work is biologically inspired\nand based on selective enhancement of the cysts, by inducing motion to a given\nOCT slice. A Convolutional Neural Network (CNN) is designed to learn a mapping\nfunction that combines the result of multiple such motions to produce a\nprobability map for cyst locations in a given slice. The final segmentation of\ncysts is obtained via simple clustering of the detected cyst locations. The\nproposed method is evaluated on two public datasets and one private dataset.\nThe public datasets include the one released for the OPTIMA Cyst segmentation\nchallenge (OCSC) in MICCAI 2015 and the DME dataset. After training on the OCSC\ntrain set, the method achieves a mean Dice Coefficient (DC) of 0.71 on the OCSC\ntest set. The robustness of the algorithm was examined by cross-validation on\nthe DME and AEI (private) datasets and a mean DC values obtained were 0.69 and\n0.79, respectively. Overall, the proposed system outperforms all benchmarks.\nThese results underscore the strengths of the proposed method in handling\nvariations in both data acquisition protocols and scanners.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 13:02:25 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 10:25:43 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Gopinath", "Karthik", ""], ["Sivaswamy", "Jayanthi", ""]]}, {"id": "1708.06227", "submitter": "Hoda Mohammadzade", "authors": "Mozhgan Mokari, Hoda Mohammadzade, Benyamin Ghojogh", "title": "Recognizing Involuntary Actions from 3D Skeleton Data Using Body States", "comments": null, "journal-ref": null, "doi": "10.24200/SCI.2018.20446", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition has been one of the most active fields of research\nin computer vision for last years. Two dimensional action recognition methods\nare facing serious challenges such as occlusion and missing the third dimension\nof data. Development of depth sensors has made it feasible to track positions\nof human body joints over time. This paper proposes a novel method of action\nrecognition which uses temporal 3D skeletal Kinect data. This method introduces\nthe definition of body states and then every action is modeled as a sequence of\nthese states. The learning stage uses Fisher Linear Discriminant Analysis (LDA)\nto construct discriminant feature space for discriminating the body states.\nMoreover, this paper suggests the use of the Mahalonobis distance as an\nappropriate distance metric for the classification of the states of involuntary\nactions. Hidden Markov Model (HMM) is then used to model the temporal\ntransition between the body states in each action. According to the results,\nthis method significantly outperforms other popular methods, with recognition\nrate of 88.64% for eight different actions and up to 96.18% for classifying\nfall actions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 13:59:53 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Mokari", "Mozhgan", ""], ["Mohammadzade", "Hoda", ""], ["Ghojogh", "Benyamin", ""]]}, {"id": "1708.06250", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Pillar Networks++: Distributed non-parametric deep and wide networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.06923", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:51:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1708.06297", "submitter": "Martin Rajchl PhD", "authors": "Martin Rajchl, Lisa M. Koch, Christian Ledig, Jonathan\n  Passerat-Palmbach, Kazunari Misawa, Kensaku Mori, Daniel Rueckert", "title": "Employing Weak Annotations for Medical Image Analysis Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To efficiently establish training databases for machine learning methods,\ncollaborative and crowdsourcing platforms have been investigated to\ncollectively tackle the annotation effort. However, when this concept is ported\nto the medical imaging domain, reading expertise will have a direct impact on\nthe annotation accuracy. In this study, we examine the impact of expertise and\nthe amount of available annotations on the accuracy outcome of a liver\nsegmentation problem in an abdominal computed tomography (CT) image database.\nIn controlled experiments, we study this impact for different types of weak\nannotations. To address the decrease in accuracy associated with lower\nexpertise, we propose a method for outlier correction making use of a weakly\nlabelled atlas. Using this approach, we demonstrate that weak annotations\nsubject to high error rates can achieve a similarly high accuracy as\nstate-of-the-art multi-atlas segmentation approaches relying on a large amount\nof expert manual segmentations. Annotations of this nature can realistically be\nobtained from a non-expert crowd and can potentially enable crowdsourcing of\nweak annotation tasks for medical image analysis.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 15:44:45 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Rajchl", "Martin", ""], ["Koch", "Lisa M.", ""], ["Ledig", "Christian", ""], ["Passerat-Palmbach", "Jonathan", ""], ["Misawa", "Kazunari", ""], ["Mori", "Kensaku", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1708.06320", "submitter": "Xu Zhang", "authors": "Xu Zhang, Felix X. Yu, Sanjiv Kumar, Shih-Fu Chang", "title": "Learning Spread-out Local Feature Descriptors", "comments": "ICCV 2017. 9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple, yet powerful regularization technique that can be used\nto significantly improve both the pairwise and triplet losses in learning local\nfeature descriptors. The idea is that in order to fully utilize the expressive\npower of the descriptor space, good local feature descriptors should be\nsufficiently \"spread-out\" over the space. In this work, we propose a\nregularization term to maximize the spread in feature descriptor inspired by\nthe property of uniform distribution. We show that the proposed regularization\nwith triplet loss outperforms existing Euclidean distance based descriptor\nlearning techniques by a large margin. As an extension, the proposed\nregularization technique can also be used to improve image-level deep feature\nembedding.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 16:47:20 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Zhang", "Xu", ""], ["Yu", "Felix X.", ""], ["Kumar", "Sanjiv", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06418", "submitter": "Mahdi Biparva", "authors": "Mahdi Biparva, John Tsotsos", "title": "STNet: Selective Tuning of Convolutional Networks for Object\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention modeling has recently gained momentum in developing visual\nhierarchies provided by Convolutional Neural Networks. Despite recent successes\nof feedforward processing on the abstraction of concepts form raw images, the\ninherent nature of feedback processing has remained computationally\ncontroversial. Inspired by the computational models of covert visual attention,\nwe propose the Selective Tuning of Convolutional Networks (STNet). It is\ncomposed of both streams of Bottom-Up and Top-Down information processing to\nselectively tune the visual representation of Convolutional networks. We\nexperimentally evaluate the performance of STNet for the weakly-supervised\nlocalization task on the ImageNet benchmark dataset. We demonstrate that STNet\nnot only successfully surpasses the state-of-the-art results but also generates\nattention-driven class hypothesis maps.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 20:58:53 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Biparva", "Mahdi", ""], ["Tsotsos", "John", ""]]}, {"id": "1708.06433", "submitter": "Nian Liu", "authors": "Nian Liu, Junwei Han, Ming-Hsuan Yang", "title": "PiCANet: Learning Pixel-wise Contextual Attention for Saliency Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contexts play an important role in the saliency detection task. However,\ngiven a context region, not all contextual information is helpful for the final\ntask. In this paper, we propose a novel pixel-wise contextual attention\nnetwork, i.e., the PiCANet, to learn to selectively attend to informative\ncontext locations for each pixel. Specifically, for each pixel, it can generate\nan attention map in which each attention weight corresponds to the contextual\nrelevance at each context location. An attended contextual feature can then be\nconstructed by selectively aggregating the contextual information. We formulate\nthe proposed PiCANet in both global and local forms to attend to global and\nlocal contexts, respectively. Both models are fully differentiable and can be\nembedded into CNNs for joint training. We also incorporate the proposed models\nwith the U-Net architecture to detect salient objects. Extensive experiments\nshow that the proposed PiCANets can consistently improve saliency detection\nperformance. The global and local PiCANets facilitate learning global contrast\nand homogeneousness, respectively. As a result, our saliency model can detect\nsalient objects more accurately and uniformly, thus performing favorably\nagainst the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:12:45 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 09:50:03 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Liu", "Nian", ""], ["Han", "Junwei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.06453", "submitter": "Xin Yi", "authors": "Xin Yi, Paul Babyn", "title": "Sharpness-aware Low dose CT denoising using conditional generative\n  adversarial network", "comments": "1. updated results, related works and discussion 2. fixed an error\n  for the noise level calculation 3. redrawn two diagrams 4. added an\n  experiment on unknown dose level CT scans", "journal-ref": null, "doi": "10.1007/s10278-018-0056-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low Dose Computed Tomography (LDCT) has offered tremendous benefits in\nradiation restricted applications, but the quantum noise as resulted by the\ninsufficient number of photons could potentially harm the diagnostic\nperformance. Current image-based denoising methods tend to produce a blur\neffect on the final reconstructed results especially in high noise levels. In\nthis paper, a deep learning based approach was proposed to mitigate this\nproblem. An adversarially trained network and a sharpness detection network\nwere trained to guide the training process. Experiments on both simulated and\nreal dataset shows that the results of the proposed method have very small\nresolution loss and achieves better performance relative to the-state-of-art\nmethods both quantitatively and visually.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 00:16:51 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 18:55:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Yi", "Xin", ""], ["Babyn", "Paul", ""]]}, {"id": "1708.06495", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Li Liu, Fan Zhu, Dongxiang Zhang,\n  and Heng-Tao Shen", "title": "Towards Automatic Construction of Diverse, High-quality Image Dataset", "comments": "Accepted by IEEE Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of labeled image datasets has been shown critical for\nhigh-level image understanding, which continuously drives the progress of\nfeature designing and models developing. However, constructing labeled image\ndatasets is laborious and monotonous. To eliminate manual annotation, in this\nwork, we propose a novel image dataset construction framework by employing\nmultiple textual queries. We aim at collecting diverse and accurate images for\ngiven queries from the Web. Specifically, we formulate noisy textual queries\nremoving and noisy images filtering as a multi-view and multi-instance learning\nproblem separately. Our proposed approach not only improves the accuracy but\nalso enhances the diversity of the selected images. To verify the effectiveness\nof our proposed approach, we construct an image dataset with 100 categories.\nThe experiments show significant performance gains by using the generated data\nof our approach on several tasks, such as image classification, cross-dataset\ngeneralization, and object detection. The proposed method also consistently\noutperforms existing weakly supervised and web-supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 04:36:12 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 07:08:08 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Liu", "Li", ""], ["Zhu", "Fan", ""], ["Zhang", "Dongxiang", ""], ["Shen", "Heng-Tao", ""]]}, {"id": "1708.06500", "submitter": "Nick Schneider", "authors": "Jonas Uhrig, Nick Schneider, Lukas Schneider, Uwe Franke, Thomas Brox,\n  Andreas Geiger", "title": "Sparsity Invariant CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider convolutional neural networks operating on sparse\ninputs with an application to depth upsampling from sparse laser scan data.\nFirst, we show that traditional convolutional networks perform poorly when\napplied to sparse data even when the location of missing data is provided to\nthe network. To overcome this problem, we propose a simple yet effective sparse\nconvolution layer which explicitly considers the location of missing data\nduring the convolution operation. We demonstrate the benefits of the proposed\nnetwork architecture in synthetic and real experiments with respect to various\nbaseline approaches. Compared to dense baselines, the proposed sparse\nconvolution network generalizes well to novel datasets and is invariant to the\nlevel of sparsity in the data. For our evaluation, we derive a novel dataset\nfrom the KITTI benchmark, comprising 93k depth annotated RGB images. Our\ndataset allows for training and evaluating depth upsampling and depth\nprediction techniques in challenging real-world settings and will be made\navailable upon publication.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 05:29:20 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 18:12:47 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Uhrig", "Jonas", ""], ["Schneider", "Nick", ""], ["Schneider", "Lukas", ""], ["Franke", "Uwe", ""], ["Brox", "Thomas", ""], ["Geiger", "Andreas", ""]]}, {"id": "1708.06509", "submitter": "Anne S. Wannenwetsch", "authors": "Anne S. Wannenwetsch, Margret Keuper, Stefan Roth", "title": "ProbFlow: Joint Optical Flow and Uncertainty Estimation", "comments": "To appear at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow estimation remains challenging due to untextured areas, motion\nboundaries, occlusions, and more. Thus, the estimated flow is not equally\nreliable across the image. To that end, post-hoc confidence measures have been\nintroduced to assess the per-pixel reliability of the flow. We overcome the\nartificial separation of optical flow and confidence estimation by introducing\na method that jointly predicts optical flow and its underlying uncertainty.\nStarting from common energy-based formulations, we rely on the corresponding\nposterior distribution of the flow given the images. We derive a variational\ninference scheme based on mean field, which incorporates best practices from\nenergy minimization. An uncertainty measure is obtained along the flow at every\npixel as the (marginal) entropy of the variational distribution. We demonstrate\nthe flexibility of our probabilistic approach by applying it to two different\nenergies and on two benchmarks. We not only obtain flow results that are\ncompetitive with the underlying energy minimization approach, but also a\nreliable uncertainty measure that significantly outperforms existing post-hoc\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 06:40:32 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Wannenwetsch", "Anne S.", ""], ["Keuper", "Margret", ""], ["Roth", "Stefan", ""]]}, {"id": "1708.06519", "submitter": "Zhuang Liu", "authors": "Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng\n  Yan and Changshui Zhang", "title": "Learning Efficient Convolutional Networks through Network Slimming", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of deep convolutional neural networks (CNNs) in many real\nworld applications is largely hindered by their high computational cost. In\nthis paper, we propose a novel learning scheme for CNNs to simultaneously 1)\nreduce the model size; 2) decrease the run-time memory footprint; and 3) lower\nthe number of computing operations, without compromising accuracy. This is\nachieved by enforcing channel-level sparsity in the network in a simple but\neffective way. Different from many existing approaches, the proposed method\ndirectly applies to modern CNN architectures, introduces minimum overhead to\nthe training process, and requires no special software/hardware accelerators\nfor the resulting models. We call our approach network slimming, which takes\nwide and large networks as input models, but during training insignificant\nchannels are automatically identified and pruned afterwards, yielding thin and\ncompact models with comparable accuracy. We empirically demonstrate the\neffectiveness of our approach with several state-of-the-art CNN models,\nincluding VGGNet, ResNet and DenseNet, on various image classification\ndatasets. For VGGNet, a multi-pass version of network slimming gives a 20x\nreduction in model size and a 5x reduction in computing operations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 07:35:26 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Liu", "Zhuang", ""], ["Li", "Jianguo", ""], ["Shen", "Zhiqiang", ""], ["Huang", "Gao", ""], ["Yan", "Shoumeng", ""], ["Zhang", "Changshui", ""]]}, {"id": "1708.06561", "submitter": "Basavaraju H T", "authors": "P. Shivakumara, D. S. Guru, and H.T. Basavaraju", "title": "Color and Gradient Features for Text Segmentation from Video Frames", "comments": null, "journal-ref": "Multimedia Processing, Communication and Computing Applications\n  (pp. 267-278). Springer, New Delhi (2013)", "doi": "10.1007/978-81-322-1143-3_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text segmentation in a video is drawing attention of researchers in the field\nof image processing, pattern recognition and document image analysis because it\nhelps in annotating and labeling video events accurately. We propose a novel\nidea of generating an enhanced frame from the R, G, and B channels of an input\nframe by grouping high and low values using Min-Max clustering criteria. We\nalso perform sliding window on enhanced frame to group high and low values from\nthe neighboring pixel values to further enhance the frame. Subsequently, we use\nk-means with k=2 clustering algorithm to separate text and non-text regions.\nThe fully connected components will be identified in the skeleton of the frame\nobtained by k-means clustering. Concept of connected component analysis based\non gradient feature has been adapted for the purpose of symmetry verification.\nThe components which satisfy symmetric verification are selected to be the\nrepresentatives of text regions and they are permitted to grow to cover their\nrespective region fully containing text. The method is tested on variety of\nvideo frames to evaluate the performance of the method in terms of recall,\nprecision and f-measure. The results show that method is promising and\nencouraging.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 11:10:57 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Shivakumara", "P.", ""], ["Guru", "D. S.", ""], ["Basavaraju", "H. T.", ""]]}, {"id": "1708.06590", "submitter": "Fernando Wario V\\'azquez", "authors": "Fernando Wario, Benjamin Wild, Ra\\'ul Rojas, Tim Landgraf", "title": "Automatic detection and decoding of honey bee waggle dances", "comments": "16 pages, LaTeX; a new value for the ratio distance-waggle run\n  duration was computed. Figure 2 has been updated and discussion section was\n  improved", "journal-ref": null, "doi": "10.1371/journal.pone.0188626", "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The waggle dance is one of the most popular examples of animal communication.\nForager bees direct their nestmates to profitable resources via a complex motor\ndisplay. Essentially, the dance encodes the polar coordinates to the resource\nin the field. Unemployed foragers follow the dancer's movements and then search\nfor the advertised spots in the field. Throughout the last decades, biologists\nhave employed different techniques to measure key characteristics of the waggle\ndance and decode the information it conveys. Early techniques involved the use\nof protractors and stopwatches to measure the dance orientation and duration\ndirectly from the observation hive. Recent approaches employ digital video\nrecordings and manual measurements on screen. However, manual approaches are\nvery time-consuming. Most studies, therefore, regard only small numbers of\nanimals in short periods of time. We have developed a system capable of\nautomatically detecting, decoding and mapping communication dances in\nreal-time. In this paper, we describe our recording setup, the image processing\nsteps performed for dance detection and decoding and an algorithm to map dances\nto the field. The proposed system performs with a detection accuracy of\n90.07\\%. The decoded waggle orientation has an average error of -2.92{\\deg}\n($\\pm$ 7.37{\\deg} ), well within the range of human error. To evaluate and\nexemplify the system's performance, a group of bees was trained to an\nartificial feeder, and all dances in the colony were automatically detected,\ndecoded and mapped. The system presented here is the first of this kind made\npublicly available, including source code and hardware specifications. We hope\nthis will foster quantitative analyses of the honey bee waggle dance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 13:02:08 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 14:38:42 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 02:06:23 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Wario", "Fernando", ""], ["Wild", "Benjamin", ""], ["Rojas", "Ra\u00fal", ""], ["Landgraf", "Tim", ""]]}, {"id": "1708.06616", "submitter": "Tonghan Wang", "authors": "Huizhen Jia, Lu Zhang, Tonghan Wang", "title": "Contrast and visual saliency similarity-induced index for assessing\n  image quality", "comments": "The paper was accepted for publication in IEEE ACCESS. The code has\n  been published online at\n  https://ww2.mathworks.cn/matlabcentral/fileexchange/69123-vspluscontrast", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image quality that is consistent with human opinion is assessed by a\nperceptual image quality assessment (IQA) that defines/utilizes a computational\nmodel. A good model should take effectiveness and efficiency into\nconsideration, but most of the previously proposed IQA models do not\nsimultaneously consider these factors. Therefore, this study attempts to\ndevelop an effective and efficient IQA metric. Contrast is an inherent visual\nattribute that indicates image quality, and visual saliency (VS) is a quality\nthat attracts the attention of human beings. The proposed model utilized these\ntwo features to characterize the image local quality. After obtaining the local\ncontrast quality map and the global VS quality map, we added the weighted\nstandard deviation of the previous two quality maps together to yield the final\nquality score. The experimental results for three benchmark databases (LIVE,\nTID2008, and CSIQ) demonstrated that our model performs the best in terms of a\ncorrelation with the human judgment of visual quality. Furthermore, compared\nwith competing IQA models, this proposed model is more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:03:19 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 03:51:52 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 09:54:59 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Jia", "Huizhen", ""], ["Zhang", "Lu", ""], ["Wang", "Tonghan", ""]]}, {"id": "1708.06637", "submitter": "Carlos Caetano", "authors": "Carlos Caetano, Victor H. C. de Melo, Jefersson A. dos Santos, William\n  Robson Schwartz", "title": "Activity Recognition based on a Magnitude-Orientation Stream Network", "comments": "8 pages, SIBGRAPI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal component of videos provides an important clue for activity\nrecognition, as a number of activities can be reliably recognized based on the\nmotion information. In view of that, this work proposes a novel temporal stream\nfor two-stream convolutional networks based on images computed from the optical\nflow magnitude and orientation, named Magnitude-Orientation Stream (MOS), to\nlearn the motion in a better and richer manner. Our method applies simple\nnonlinear transformations on the vertical and horizontal components of the\noptical flow to generate input images for the temporal stream. Experimental\nresults, carried on two well-known datasets (HMDB51 and UCF101), demonstrate\nthat using our proposed temporal stream as input to existing neural network\narchitectures can improve their performance for activity recognition. Results\ndemonstrate that our temporal stream provides complementary information able to\nimprove the classical two-stream methods, indicating the suitability of our\napproach to be used as a temporal video representation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:27:26 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Caetano", "Carlos", ""], ["de Melo", "Victor H. C.", ""], ["Santos", "Jefersson A. dos", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1708.06656", "submitter": "Zheyan Shen", "authors": "Zheyan Shen, Peng Cui, Kun Kuang, Bo Li, Peixuan Chen", "title": "Causally Regularized Learning with Agnostic Data Selection Bias", "comments": "Oral paper of 2018 ACM Multimedia Conference (MM'18)", "journal-ref": null, "doi": "10.1145/3240508.3240577", "report-no": null, "categories": "cs.CV cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of previous machine learning algorithms are proposed based on the i.i.d.\nhypothesis. However, this ideal assumption is often violated in real\napplications, where selection bias may arise between training and testing\nprocess. Moreover, in many scenarios, the testing data is not even available\nduring the training process, which makes the traditional methods like transfer\nlearning infeasible due to their need on prior of test distribution. Therefore,\nhow to address the agnostic selection bias for robust model learning is of\nparamount importance for both academic research and real applications. In this\npaper, under the assumption that causal relationships among variables are\nrobust across domains, we incorporate causal technique into predictive modeling\nand propose a novel Causally Regularized Logistic Regression (CRLR) algorithm\nby jointly optimize global confounder balancing and weighted logistic\nregression. Global confounder balancing helps to identify causal features,\nwhose causal effect on outcome are stable across domains, then performing\nlogistic regression on those causal features constructs a robust predictive\nmodel against the agnostic bias. To validate the effectiveness of our CRLR\nalgorithm, we conduct comprehensive experiments on both synthetic and real\nworld datasets. Experimental results clearly demonstrate that our CRLR\nalgorithm outperforms the state-of-the-art methods, and the interpretability of\nour method can be fully depicted by the feature visualization.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 14:49:07 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 16:33:36 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Shen", "Zheyan", ""], ["Cui", "Peng", ""], ["Kuang", "Kun", ""], ["Li", "Bo", ""], ["Chen", "Peixuan", ""]]}, {"id": "1708.06670", "submitter": "Konda Reddy Mopuri", "authors": "Konda Reddy Mopuri, Utsav Garg and R. Venkatesh Babu", "title": "CNN Fixations: An unraveling approach to visualize the discriminative\n  image regions", "comments": "Accepted in Trans. on Image Processing (TIP) 2018 and Codes are\n  available at https://github.com/utsavgarg/cnn-fixations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have revolutionized various fields\nof vision research and have seen unprecedented adoption for multiple tasks such\nas classification, detection, captioning, etc. However, they offer little\ntransparency into their inner workings and are often treated as black boxes\nthat deliver excellent performance. In this work, we aim at alleviating this\nopaqueness of CNNs by providing visual explanations for the network's\npredictions. Our approach can analyze variety of CNN based models trained for\nvision applications such as object recognition and caption generation. Unlike\nexisting methods, we achieve this via unraveling the forward pass operation.\nProposed method exploits feature dependencies across the layer hierarchy and\nuncovers the discriminative image locations that guide the network's\npredictions. We name these locations CNN-Fixations, loosely analogous to human\neye fixations.\n  Our approach is a generic method that requires no architectural changes,\nadditional training or gradient computation and computes the important image\nlocations (CNN Fixations). We demonstrate through a variety of applications\nthat our approach is able to localize the discriminative image locations across\ndifferent network architectures, diverse vision tasks and data modalities.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 15:23:31 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 05:08:48 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 18:40:26 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Garg", "Utsav", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1708.06673", "submitter": "Siddhartha Chaudhuri", "authors": "Sanjeev Muralikrishnan, Vladimir G. Kim, Siddhartha Chaudhuri", "title": "Tags2Parts: Discovering Semantic Regions from Shape Tags", "comments": "To appear at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for discovering shape regions that strongly\ncorrelate with user-prescribed tags. For example, given a collection of chairs\ntagged as either \"has armrest\" or \"lacks armrest\", our system correctly\nhighlights the armrest regions as the main distinctive parts between the two\nchair types. To obtain point-wise predictions from shape-wise tags we develop a\nnovel neural network architecture that is trained with tag classification loss,\nbut is designed to rely on segmentation to predict the tag. Our network is\ninspired by U-Net, but we replicate shallow U structures several times with new\nskip connections and pooling layers, and call the resulting architecture\n\"WU-Net\". We test our method on segmentation benchmarks and show that even with\nweak supervision of whole shape tags, our method can infer meaningful semantic\nregions, without ever observing shape segmentations. Further, once trained, the\nmodel can process shapes for which the tag is entirely unknown. As a bonus, our\narchitecture is directly operational under full supervision and performs\nstrongly on standard benchmarks. We validate our method through experiments\nwith many variant architectures and prior baselines, and demonstrate several\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 15:26:00 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 11:26:29 GMT"}, {"version": "v3", "created": "Mon, 30 Apr 2018 20:29:41 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Muralikrishnan", "Sanjeev", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""]]}, {"id": "1708.06690", "submitter": "Isma Hadji", "authors": "Isma Hadji and Richard P. Wildes", "title": "A Spatiotemporal Oriented Energy Network for Dynamic Texture Recognition", "comments": "accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel hierarchical spatiotemporal orientation\nrepresentation for spacetime image analysis. It is designed to combine the\nbenefits of the multilayer architecture of ConvNets and a more controlled\napproach to spacetime analysis. A distinguishing aspect of the approach is that\nunlike most contemporary convolutional networks no learning is involved;\nrather, all design decisions are specified analytically with theoretical\nmotivations. This approach makes it possible to understand what information is\nbeing extracted at each stage and layer of processing as well as to minimize\nheuristic choices in design. Another key aspect of the network is its recurrent\nnature, whereby the output of each layer of processing feeds back to the input.\nTo keep the network size manageable across layers, a novel cross-channel\nfeature pooling is proposed. The multilayer architecture that results\nsystematically reveals hierarchical image structure in terms of multiscale,\nmultiorientation properties of visual spacetime. To illustrate its utility, the\nnetwork has been applied to the task of dynamic texture recognition. Empirical\nevaluation on multiple standard datasets shows that it sets a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:06:19 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Hadji", "Isma", ""], ["Wildes", "Richard P.", ""]]}, {"id": "1708.06703", "submitter": "Anil Bas", "authors": "Anil Bas and William A. P. Smith", "title": "What does 2D geometric information really tell us about 3D face shape?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A face image contains geometric cues in the form of configurational\ninformation and contours that can be used to estimate 3D face shape. While it\nis clear that 3D reconstruction from 2D points is highly ambiguous if no\nfurther constraints are enforced, one might expect that the face-space\nconstraint solves this problem. We show that this is not the case and that\ngeometric information is an ambiguous cue. There are two sources for this\nambiguity. The first is that, within the space of 3D face shapes, there are\nflexibility modes that remain when some parts of the face are fixed. The second\noccurs only under perspective projection and is a result of perspective\ntransformation as camera distance varies. Two different faces, when viewed at\ndifferent distances, can give rise to the same 2D geometry. To demonstrate\nthese ambiguities, we develop new algorithms for fitting a 3D morphable model\nto 2D landmarks or contours under either orthographic or perspective projection\nand show how to compute flexibility modes for both cases. We show that both\nfitting problems can be posed as a separable nonlinear least squares problem\nand solved efficiently. We demonstrate both quantitatively and qualitatively\nthat the ambiguity is present in reconstructions from geometric information\nalone but also in reconstructions from a state-of-the-art CNN-based method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:21:13 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 15:11:48 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 19:08:29 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Bas", "Anil", ""], ["Smith", "William A. P.", ""]]}, {"id": "1708.06720", "submitter": "Han Hu", "authors": "Han Hu, Chengquan Zhang, Yuxuan Luo, Yuzhuo Wang, Junyu Han, Errui\n  Ding", "title": "WordSup: Exploiting Word Annotations for Character based Text Detection", "comments": "2017 International Conference on Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imagery texts are usually organized as a hierarchy of several visual\nelements, i.e. characters, words, text lines and text blocks. Among these\nelements, character is the most basic one for various languages such as\nWestern, Chinese, Japanese, mathematical expression and etc. It is natural and\nconvenient to construct a common text detection engine based on character\ndetectors. However, training character detectors requires a vast of location\nannotated characters, which are expensive to obtain. Actually, the existing\nreal text datasets are mostly annotated in word or line level. To remedy this\ndilemma, we propose a weakly supervised framework that can utilize word\nannotations, either in tight quadrangles or the more loose bounding boxes, for\ncharacter detector training. When applied in scene text detection, we are thus\nable to train a robust character detector by exploiting word annotations in the\nrich large-scale real scene text datasets, e.g. ICDAR15 and COCO-text. The\ncharacter detector acts as a key role in the pipeline of our text detection\nengine. It achieves the state-of-the-art performance on several challenging\nscene text detection benchmarks. We also demonstrate the flexibility of our\npipeline by various scenarios, including deformed text detection and math\nexpression recognition.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:55:24 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Hu", "Han", ""], ["Zhang", "Chengquan", ""], ["Luo", "Yuxuan", ""], ["Wang", "Yuzhuo", ""], ["Han", "Junyu", ""], ["Ding", "Errui", ""]]}, {"id": "1708.06724", "submitter": "Chao Shang", "authors": "Chao Shang, Aaron Palmer, Jiangwen Sun, Ko-Shin Chen, Jin Lu, Jinbo Bi", "title": "VIGAN: Missing View Imputation with Generative Adversarial Networks", "comments": "10 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an era when big data are becoming the norm, there is less concern with the\nquantity but more with the quality and completeness of the data. In many\ndisciplines, data are collected from heterogeneous sources, resulting in\nmulti-view or multi-modal datasets. The missing data problem has been\nchallenging to address in multi-view data analysis. Especially, when certain\nsamples miss an entire view of data, it creates the missing view problem.\nClassic multiple imputations or matrix completion methods are hardly effective\nhere when no information can be based on in the specific view to impute data\nfor such samples. The commonly-used simple method of removing samples with a\nmissing view can dramatically reduce sample size, thus diminishing the\nstatistical power of a subsequent analysis. In this paper, we propose a novel\napproach for view imputation via generative adversarial networks (GANs), which\nwe name by VIGAN. This approach first treats each view as a separate domain and\nidentifies domain-to-domain mappings via a GAN using randomly-sampled data from\neach view, and then employs a multi-modal denoising autoencoder (DAE) to\nreconstruct the missing view from the GAN outputs based on paired data across\nthe views. Then, by optimizing the GAN and DAE jointly, our model enables the\nknowledge integration for domain mappings and view correspondences to\neffectively recover the missing view. Empirical results on benchmark datasets\nvalidate the VIGAN approach by comparing against the state of the art. The\nevaluation of VIGAN in a genetic study of substance use disorders further\nproves the effectiveness and usability of this approach in life science.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 17:05:38 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 20:14:32 GMT"}, {"version": "v3", "created": "Mon, 4 Sep 2017 21:18:52 GMT"}, {"version": "v4", "created": "Wed, 11 Oct 2017 16:21:13 GMT"}, {"version": "v5", "created": "Wed, 1 Nov 2017 15:43:36 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Shang", "Chao", ""], ["Palmer", "Aaron", ""], ["Sun", "Jiangwen", ""], ["Chen", "Ko-Shin", ""], ["Lu", "Jin", ""], ["Bi", "Jinbo", ""]]}, {"id": "1708.06734", "submitter": "Mehdi Noroozi", "authors": "Mehdi Noroozi, Hamed Pirsiavash and Paolo Favaro", "title": "Representation Learning by Learning to Count", "comments": "ICCV 2017(oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method for representation learning that uses an\nartificial supervision signal based on counting visual primitives. This\nsupervision signal is obtained from an equivariance relation, which does not\nrequire any manual annotation. We relate transformations of images to\ntransformations of the representations. More specifically, we look for the\nrepresentation that satisfies such relation rather than the transformations\nthat match a given representation. In this paper, we use two image\ntransformations in the context of counting: scaling and tiling. The first\ntransformation exploits the fact that the number of visual primitives should be\ninvariant to scale. The second transformation allows us to equate the total\nnumber of visual primitives in each tile to that in the whole image. These two\ntransformations are combined in one constraint and used to train a neural\nnetwork with a contrastive loss. The proposed task produces representations\nthat perform on par or exceed the state of the art in transfer learning\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 17:33:47 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Noroozi", "Mehdi", ""], ["Pirsiavash", "Hamed", ""], ["Favaro", "Paolo", ""]]}, {"id": "1708.06767", "submitter": "Aviv Gabbay", "authors": "Aviv Gabbay, Ariel Ephrat, Tavi Halperin, Shmuel Peleg", "title": "Seeing Through Noise: Visually Driven Speaker Separation and Enhancement", "comments": "Supplementary video: https://www.youtube.com/watch?v=qmsyj7vAzoI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isolating the voice of a specific person while filtering out other voices or\nbackground noises is challenging when video is shot in noisy environments. We\npropose audio-visual methods to isolate the voice of a single speaker and\neliminate unrelated sounds. First, face motions captured in the video are used\nto estimate the speaker's voice, by passing the silent video frames through a\nvideo-to-speech neural network-based model. Then the speech predictions are\napplied as a filter on the noisy input audio. This approach avoids using\nmixtures of sounds in the learning process, as the number of such possible\nmixtures is huge, and would inevitably bias the trained model. We evaluate our\nmethod on two audio-visual datasets, GRID and TCD-TIMIT, and show that our\nmethod attains significant SDR and PESQ improvements over the raw\nvideo-to-speech predictions, and a well-known audio-only method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 18:02:27 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 19:18:41 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 21:34:19 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Gabbay", "Aviv", ""], ["Ephrat", "Ariel", ""], ["Halperin", "Tavi", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1708.06779", "submitter": "Mehdi Noroozi", "authors": "Paramanand Chandramouli, Mehdi Noroozi and Paolo Favaro", "title": "Reflection Separation and Deblurring of Plenoptic Images", "comments": "ACCV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of reflection removal and deblurring\nfrom a single image captured by a plenoptic camera. We develop a two-stage\napproach to recover the scene depth and high resolution textures of the\nreflected and transmitted layers. For depth estimation in the presence of\nreflections, we train a classifier through convolutional neural networks. For\nrecovering high resolution textures, we assume that the scene is composed of\nplanar regions and perform the reconstruction of each layer by using an\nexplicit form of the plenoptic camera point spread function. The proposed\nframework also recovers the sharp scene texture with different motion blurs\napplied to each layer. We demonstrate our method on challenging real and\nsynthetic images.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 18:34:32 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Chandramouli", "Paramanand", ""], ["Noroozi", "Mehdi", ""], ["Favaro", "Paolo", ""]]}, {"id": "1708.06794", "submitter": "Jonti Talukdar", "authors": "Jonti Talukdar and Bhavana Mehta", "title": "Human Action Recognition System using Good Features and Multilayer\n  Perceptron Network", "comments": "6 pages, 7 Figures, IEEE International Conference on Communication\n  and Signal Processing 2017 (ICCSP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition involves the characterization of human actions\nthrough the automated analysis of video data and is integral in the development\nof smart computer vision systems. However, several challenges like dynamic\nbackgrounds, camera stabilization, complex actions, occlusions etc. make action\nrecognition in a real time and robust fashion difficult. Several complex\napproaches exist but are computationally intensive. This paper presents a novel\napproach of using a combination of good features along with iterative optical\nflow algorithm to compute feature vectors which are classified using a\nmultilayer perceptron (MLP) network. The use of multiple features for motion\ndescriptors enhances the quality of tracking. Resilient backpropagation\nalgorithm is used for training the feedforward neural network reducing the\nlearning time. The overall system accuracy is improved by optimizing the\nvarious parameters of the multilayer perceptron network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 19:39:45 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Talukdar", "Jonti", ""], ["Mehta", "Bhavana", ""]]}, {"id": "1708.06822", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yasin Almalioglu, Helder Araujo, Ender Konukoglu, Metin\n  Sitti", "title": "Deep EndoVO: A Recurrent Convolutional Neural Network (RCNN) based\n  Visual Odometry Approach for Endoscopic Capsule Robots", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2017.10.014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ingestible wireless capsule endoscopy is an emerging minimally invasive\ndiagnostic technology for inspection of the GI tract and diagnosis of a wide\nrange of diseases and pathologies. Medical device companies and many research\ngroups have recently made substantial progresses in converting passive capsule\nendoscopes to active capsule robots, enabling more accurate, precise, and\nintuitive detection of the location and size of the diseased areas. Since a\nreliable real time pose estimation functionality is crucial for actively\ncontrolled endoscopic capsule robots, in this study, we propose a monocular\nvisual odometry (VO) method for endoscopic capsule robot operations. Our method\nlies on the application of the deep Recurrent Convolutional Neural Networks\n(RCNNs) for the visual odometry task, where Convolutional Neural Networks\n(CNNs) and Recurrent Neural Networks (RNNs) are used for the feature extraction\nand inference of dynamics across the frames, respectively. Detailed analyses\nand evaluations made on a real pig stomach dataset proves that our system\nachieves high translational and rotational accuracies for different types of\nendoscopic capsule robot trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:13:18 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 13:47:53 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Turan", "Mehmet", ""], ["Almalioglu", "Yasin", ""], ["Araujo", "Helder", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1708.06831", "submitter": "Zheng Tang", "authors": "Zheng Tang, Gaoang Wang, Tao Liu, Young-Gun Lee, Adwin Jahn, Xu Liu,\n  Xiaodong He and Jenq-Neng Hwang", "title": "Multiple-Kernel Based Vehicle Tracking Using 3D Deformable Model and\n  Camera Self-Calibration", "comments": "6 pages, 6 figures, 3 tables, 2017 IEEE Smart World NVIDIA AI City\n  Challenge (Winner of Track 2: Applications)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking of multiple objects is an important application in AI City geared\ntowards solving salient problems related to safety and congestion in an urban\nenvironment. Frequent occlusion in traffic surveillance has been a major\nproblem in this research field. In this challenge, we propose a model-based\nvehicle localization method, which builds a kernel at each patch of the 3D\ndeformable vehicle model and associates them with constraints in 3D space. The\nproposed method utilizes shape fitness evaluation besides color information to\ntrack vehicle objects robustly and efficiently. To build 3D car models in a\nfully unsupervised manner, we also implement evolutionary camera\nself-calibration from tracking of walking humans to automatically compute\ncamera parameters. Additionally, the segmented foreground masks which are\ncrucial to 3D modeling and camera self-calibration are adaptively refined by\nmultiple-kernel feedback from tracking. For object detection/classification,\nthe state-of-the-art single shot multibox detector (SSD) is adopted to train\nand test on the NVIDIA AI City Dataset. To improve the accuracy on categories\nwith only few objects, like bus, bicycle and motorcycle, we also employ the\npretrained model from YOLO9000 with multi-scale testing. We combine the results\nfrom SSD and YOLO9000 based on ensemble learning. Experiments show that our\nproposed tracking system outperforms both state-of-the-art of tracking by\nsegmentation and tracking by detection.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:41:11 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Tang", "Zheng", ""], ["Wang", "Gaoang", ""], ["Liu", "Tao", ""], ["Lee", "Young-Gun", ""], ["Jahn", "Adwin", ""], ["Liu", "Xu", ""], ["He", "Xiaodong", ""], ["Hwang", "Jenq-Neng", ""]]}, {"id": "1708.06834", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Brendan Jou, Xavier Giro-i-Nieto, Jordi Torres and\n  Shih-Fu Chang", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "comments": "Accepted as conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time. We introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models. Source code is publicly available at\nhttps://imatge-upc.github.io/skiprnn-2017-telecombcn/ .\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:53:34 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 00:54:45 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 17:14:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Campos", "Victor", ""], ["Jou", "Brendan", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06858", "submitter": "Yale Song", "authors": "Haojian Jin, Yale Song, Koji Yatani", "title": "ElasticPlay: Interactive Video Summarization with Dynamic Time Budgets", "comments": "ACM Multimedia 2017 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video consumption is being shifted from sit-and-watch to selective skimming.\nExisting video player interfaces, however, only provide indirect manipulation\nto support this emerging behavior. Video summarization alleviates this issue to\nsome extent, shortening a video based on the desired length of a summary as an\ninput variable. But an optimal length of a summarized video is often not\navailable in advance. Moreover, the user cannot edit the summary once it is\nproduced, limiting its practical applications. We argue that video\nsummarization should be an interactive, mixed-initiative process in which users\nhave control over the summarization procedure while algorithms help users\nachieve their goal via video understanding. In this paper, we introduce\nElasticPlay, a mixed-initiative approach that combines an advanced video\nsummarization technique with direct interface manipulation to help users\ncontrol the video summarization process. Users can specify a time budget for\nthe remaining content while watching a video; our system then immediately\nupdates the playback plan using our proposed cut-and-forward algorithm,\ndetermining which parts to skip or to fast-forward. This interactive process\nallows users to fine-tune the summarization result with immediate feedback. We\nshow that our system outperforms existing video summarization techniques on the\nTVSum50 dataset. We also report two lab studies (22 participants) and a\nMechanical Turk deployment study (60 participants), and show that the\nparticipants responded favorably to ElasticPlay.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 00:25:13 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Jin", "Haojian", ""], ["Song", "Yale", ""], ["Yatani", "Koji", ""]]}, {"id": "1708.06963", "submitter": "Anders Glent Buch", "authors": "Anders Glent Buch, Dirk Kraft, Joni-Kristian Kamarainen, Henrik Gordon\n  Petersen and Norbert Kr\\\"uger", "title": "Pose Estimation using Local Structure-Specific Shape and Appearance\n  Context", "comments": null, "journal-ref": "2013 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA.2013.6630856", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the alignment pose between two models\nusing structure-specific local descriptors. Our descriptors are generated using\na combination of 2D image data and 3D contextual shape data, resulting in a set\nof semi-local descriptors containing rich appearance and shape information for\nboth edge and texture structures. This is achieved by defining feature space\nrelations which describe the neighborhood of a descriptor. By quantitative\nevaluations, we show that our descriptors provide high discriminative power\ncompared to state of the art approaches. In addition, we show how to utilize\nthis for the estimation of the alignment pose between two point sets. We\npresent experiments both in controlled and real-life scenarios to validate our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 11:46:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Buch", "Anders Glent", ""], ["Kraft", "Dirk", ""], ["Kamarainen", "Joni-Kristian", ""], ["Petersen", "Henrik Gordon", ""], ["Kr\u00fcger", "Norbert", ""]]}, {"id": "1708.06966", "submitter": "Anders Glent Buch", "authors": "Anders Glent Buch, Yang Yang, Norbert Kr\\\"uger, Henrik Gordon Petersen", "title": "In search of inliers: 3d correspondence by local and global voting", "comments": null, "journal-ref": "2014 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "doi": "10.1109/CVPR.2014.266", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for finding correspondence between 3D models. From an\ninitial set of feature correspondences, our method uses a fast voting scheme to\nseparate the inliers from the outliers. The novelty of our method lies in the\nuse of a combination of local and global constraints to determine if a vote\nshould be cast. On a local scale, we use simple, low-level geometric\ninvariants. On a global scale, we apply covariant constraints for finding\ncompatible correspondences. We guide the sampling for collecting voters by\ndownward dependencies on previous voting stages. All of this together results\nin an accurate matching procedure. We evaluate our algorithm by controlled and\ncomparative testing on different datasets, giving superior performance compared\nto state of the art methods. In a final experiment, we apply our method for 3D\nobject detection, showing potential use of our method within higher-level\nvision.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 11:54:47 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Buch", "Anders Glent", ""], ["Yang", "Yang", ""], ["Kr\u00fcger", "Norbert", ""], ["Petersen", "Henrik Gordon", ""]]}, {"id": "1708.06973", "submitter": "Mehmet Ayg\\\"un", "authors": "Mehmet Ayg\\\"un, Yusuf Aytar, Haz{\\i}m Kemal Ekenel", "title": "Exploiting Convolution Filter Patterns for Transfer Learning", "comments": "Accepted to TASK-CV Workshop at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new regularization technique for transfer\nlearning. The aim of the proposed approach is to capture statistical\nrelationships among convolution filters learned from a well-trained network and\ntransfer this knowledge to another network. Since convolution filters of the\nprevalent deep Convolutional Neural Network (CNN) models share a number of\nsimilar patterns, in order to speed up the learning procedure, we capture such\ncorrelations by Gaussian Mixture Models (GMMs) and transfer them using a\nregularization term. We have conducted extensive experiments on the CIFAR10,\nPlaces2, and CMPlaces datasets to assess generalizability, task\ntransferability, and cross-model transferability of the proposed approach,\nrespectively. The experimental results show that the feature representations\nhave efficiently been learned and transferred through the proposed statistical\nregularization scheme. Moreover, our method is an architecture independent\napproach, which is applicable for a variety of CNN architectures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:13:30 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Ayg\u00fcn", "Mehmet", ""], ["Aytar", "Yusuf", ""], ["Ekenel", "Haz\u0131m Kemal", ""]]}, {"id": "1708.06975", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (1), St\\'ephane Herbin (1), Fr\\'ed\\'eric Jurie ((1)\n  Palaiseau)", "title": "Generating Visual Representations for Zero-Shot Classification", "comments": null, "journal-ref": "International Conference on Computer Vision (ICCV) Workshops :\n  TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct\n  2017, venise, Italy. International Conference on Computer Vision (ICCV)\n  Workshops, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of learning an image clas-sifier when some\ncategories are defined by semantic descriptions only (e.g. visual attributes)\nwhile the others are defined by exemplar images as well. This task is often\nreferred to as the Zero-Shot classification task (ZSC). Most of the previous\nmethods rely on learning a common embedding space allowing to compare visual\nfeatures of unknown categories with semantic descriptions. This paper argues\nthat these approaches are limited as i) efficient discrimi-native classifiers\ncan't be used ii) classification tasks with seen and unseen categories\n(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently.\nIn contrast , this paper suggests to address ZSC and GZSC by i) learning a\nconditional generator using seen classes ii) generate artificial training\nexamples for the categories without exemplars. ZSC is then turned into a\nstandard supervised learning problem. Experiments with 4 generative models and\n5 datasets experimentally validate the approach, giving state-of-the-art\nresults on both ZSC and GZSC.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:23:51 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 12:56:18 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 16:16:01 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Bucher", "Maxime", ""], ["Herbin", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1708.06977", "submitter": "Konstantin Shmelkov", "authors": "Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari", "title": "Incremental Learning of Object Detectors without Catastrophic Forgetting", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their success for object detection, convolutional neural networks are\nill-equipped for incremental learning, i.e., adapting the original model\ntrained on a set of classes to additionally detect objects of new classes, in\nthe absence of the initial training data. They suffer from \"catastrophic\nforgetting\" - an abrupt degradation of performance on the original set of\nclasses, when the training objective is adapted to the new classes. We present\na method to address this issue, and learn object detectors incrementally, when\nneither the original training data nor annotations for the original classes in\nthe new training set are available. The core of our proposed solution is a loss\nfunction to balance the interplay between predictions on the new classes and a\nnew distillation loss which minimizes the discrepancy between responses for old\nclasses from the original and the updated networks. This incremental learning\ncan be performed multiple times, for a new set of classes in each step, with a\nmoderate drop in performance compared to the baseline network trained on the\nensemble of data. We present object detection results on the PASCAL VOC 2007\nand COCO datasets, along with a detailed empirical analysis of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:33:44 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Shmelkov", "Konstantin", ""], ["Schmid", "Cordelia", ""], ["Alahari", "Karteek", ""]]}, {"id": "1708.06997", "submitter": "\\v{Z}iga Emer\\v{s}i\\v{c}", "authors": "\\v{Z}iga Emer\\v{s}i\\v{c}, Dejan \\v{S}tepec, Vitomir \\v{S}truc, Peter\n  Peer, Anjith George, Adil Ahmad, Elshibani Omar, Terrance E. Boult, Reza\n  Safdari, Yuxiang Zhou, Stefanos Zafeiriou, Dogucan Yaman, Fevziye I. Eyiokur,\n  Hazim K. Ekenel", "title": "The Unconstrained Ear Recognition Challenge", "comments": "International Joint Conference on Biometrics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the results of the Unconstrained Ear Recognition\nChallenge (UERC), a group benchmarking effort centered around the problem of\nperson recognition from ear images captured in uncontrolled conditions. The\ngoal of the challenge was to assess the performance of existing ear recognition\ntechniques on a challenging large-scale dataset and identify open problems that\nneed to be addressed in the future. Five groups from three continents\nparticipated in the challenge and contributed six ear recognition techniques\nfor the evaluation, while multiple baselines were made available for the\nchallenge by the UERC organizers. A comprehensive analysis was conducted with\nall participating approaches addressing essential research questions pertaining\nto the sensitivity of the technology to head rotation, flipping, gallery size,\nlarge-scale recognition and others. The top performer of the UERC was found to\nensure robust performance on a smaller part of the dataset (with 180 subjects)\nregardless of image characteristics, but still exhibited a significant\nperformance drop when the entire dataset comprising 3,704 subjects was used for\ntesting.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:45:55 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 07:52:53 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Emer\u0161i\u010d", "\u017diga", ""], ["\u0160tepec", "Dejan", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""], ["George", "Anjith", ""], ["Ahmad", "Adil", ""], ["Omar", "Elshibani", ""], ["Boult", "Terrance E.", ""], ["Safdari", "Reza", ""], ["Zhou", "Yuxiang", ""], ["Zafeiriou", "Stefanos", ""], ["Yaman", "Dogucan", ""], ["Eyiokur", "Fevziye I.", ""], ["Ekenel", "Hazim K.", ""]]}, {"id": "1708.07021", "submitter": "Mohammad Tariqul Islam", "authors": "Ramesh Basnet, Mohammad Tariqul Islam, Tamanna Howlader, S. M.\n  Mahbubur Rahman, Dimitrios Hatzinakos", "title": "Statistical Selection of CNN-Based Audiovisual Features for\n  Instantaneous Estimation of Human Emotional States", "comments": "Accepted in International Conference on new Trends in Computer\n  Sciences (ICTCS), Amman, Jordan, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic prediction of continuous-level emotional state requires selection\nof suitable affective features to develop a regression system based on\nsupervised machine learning. This paper investigates the performance of\nfeatures statistically learned using convolutional neural networks for\ninstantaneously predicting the continuous dimensions of emotional states.\nFeatures with minimum redundancy and maximum relevancy are chosen by using the\nmutual information-based selection process. The performance of frame-by-frame\nprediction of emotional state using the moderate length features as proposed in\nthis paper is evaluated on spontaneous and naturalistic human-human\nconversation of RECOLA database. Experimental results show that the proposed\nmodel can be used for instantaneous prediction of emotional state with an\naccuracy higher than traditional audio or video features that are used for\naffective computation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:27:19 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Basnet", "Ramesh", ""], ["Islam", "Mohammad Tariqul", ""], ["Howlader", "Tamanna", ""], ["Rahman", "S. M. Mahbubur", ""], ["Hatzinakos", "Dimitrios", ""]]}, {"id": "1708.07023", "submitter": "Mohammad Tariqul Islam", "authors": "Mohaiminul Al Nahian, A. S. M. Iftekhar, Mohammad Tariqul Islam, S. M.\n  Mahbubur Rahman, Dimitrios Hatzinakos", "title": "CNN-Based Prediction of Frame-Level Shot Importance for Video\n  Summarization", "comments": "Accepted in International Conference on new Trends in Computer\n  Sciences (ICTCS), Amman-Jordan, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet, ubiquitous presence of redundant, unedited, raw videos has\nmade video summarization an important problem. Traditional methods of video\nsummarization employ a heuristic set of hand-crafted features, which in many\ncases fail to capture subtle abstraction of a scene. This paper presents a deep\nlearning method that maps the context of a video to the importance of a scene\nsimilar to that is perceived by humans. In particular, a convolutional neural\nnetwork (CNN)-based architecture is proposed to mimic the frame-level shot\nimportance for user-oriented video summarization. The weights and biases of the\nCNN are trained extensively through off-line processing, so that it can provide\nthe importance of a frame of an unseen video almost instantaneously.\nExperiments on estimating the shot importance is carried out using the publicly\navailable database TVSum50. It is shown that the performance of the proposed\nnetwork is substantially better than that of commonly referred feature-based\nmethods for estimating the shot importance in terms of mean absolute error,\nabsolute error variance, and relative F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:35:10 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Nahian", "Mohaiminul Al", ""], ["Iftekhar", "A. S. M.", ""], ["Islam", "Mohammad Tariqul", ""], ["Rahman", "S. M. Mahbubur", ""], ["Hatzinakos", "Dimitrios", ""]]}, {"id": "1708.07029", "submitter": "Longguang Wang", "authors": "Longguang Wang, Zaiping Lin, Jinyan Gao, Xinpu Deng, Wei An", "title": "Fast single image super-resolution based on sigmoid transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super-resolution aims to generate a high-resolution image from a\nsingle low-resolution image, which is of great significance in extensive\napplications. As an ill-posed problem, numerous methods have been proposed to\nreconstruct the missing image details based on exemplars or priors. In this\npaper, we propose a fast and simple single image super-resolution strategy\nutilizing patch-wise sigmoid transformation as an imposed sharpening\nregularization term in the reconstruction, which realizes amazing\nreconstruction performance. Extensive experiments compared with other\nstate-of-the-art approaches demonstrate the superior effectiveness and\nefficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 14:46:26 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 14:59:23 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 04:42:11 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Wang", "Longguang", ""], ["Lin", "Zaiping", ""], ["Gao", "Jinyan", ""], ["Deng", "Xinpu", ""], ["An", "Wei", ""]]}, {"id": "1708.07034", "submitter": "Lara Lloret Iglesias", "authors": "Celia Fern\\'andez Madrazo, Ignacio Heredia Cacha, Lara Lloret Iglesias\n  and Jes\\'us Marco de Lucas", "title": "Application of a Convolutional Neural Network for image classification\n  to the analysis of collisions in High Energy Physics", "comments": "14 pages, 8 figures, educational", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of deep learning techniques using convolutional neural\nnetworks to the classification of particle collisions in High Energy Physics is\nexplored. An intuitive approach to transform physical variables, like momenta\nof particles and jets, into a single image that captures the relevant\ninformation, is proposed. The idea is tested using a well known deep learning\nframework on a simulation dataset, including leptonic ttbar events and the\ncorresponding background at 7 TeV from the CMS experiment at LHC, available as\nOpen Data. This initial test shows competitive results when compared to more\nclassical approaches, like those using feedforward neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:03:08 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Madrazo", "Celia Fern\u00e1ndez", ""], ["Cacha", "Ignacio Heredia", ""], ["Iglesias", "Lara Lloret", ""], ["de Lucas", "Jes\u00fas Marco", ""]]}, {"id": "1708.07038", "submitter": "Georgios Zoumpourlis", "authors": "Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, Petros\n  Daras", "title": "Non-linear Convolution Filters for CNN-based Learning", "comments": "9 pages, 5 figures, code link, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, Convolutional Neural Networks (CNNs) have achieved\nstate-of-the-art performance in image classification. Their architectures have\nlargely drawn inspiration by models of the primate visual system. However,\nwhile recent research results of neuroscience prove the existence of non-linear\noperations in the response of complex visual cells, little effort has been\ndevoted to extend the convolution technique to non-linear forms. Typical\nconvolutional layers are linear systems, hence their expressiveness is limited.\nTo overcome this, various non-linearities have been used as activation\nfunctions inside CNNs, while also many pooling strategies have been applied. We\naddress the issue of developing a convolution method in the context of a\ncomputational model of the visual cortex, exploring quadratic forms through the\nVolterra kernels. Such forms, constituting a more rich function space, are used\nas approximations of the response profile of visual cells. Our proposed\nsecond-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a\nnetwork which combines linear and non-linear filters in its convolutional\nlayers, can outperform networks that use standard linear filters with the same\narchitecture, yielding results competitive with the state-of-the-art on these\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:07:35 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Zoumpourlis", "Georgios", ""], ["Doumanoglou", "Alexandros", ""], ["Vretos", "Nicholas", ""], ["Daras", "Petros", ""]]}, {"id": "1708.07066", "submitter": "Xin Jin", "authors": "Xin Jin, Yannan Li, Ningning Liu, Xiaodong Li, Xianggang Jiang, Chaoen\n  Xiao, Shiming Ge", "title": "Single Reference Image based Scene Relighting via Material Guided\n  Filtering", "comments": "Best student paper award of ISAIR 2017 (International Symposium on\n  Artificial Intelligence and Robotics), recommended to Optics and Laser\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image relighting is to change the illumination of an image to a target\nillumination effect without known the original scene geometry, material\ninformation and illumination condition. We propose a novel outdoor scene\nrelighting method, which needs only a single reference image and is based on\nmaterial constrained layer decomposition. Firstly, the material map is\nextracted from the input image. Then, the reference image is warped to the\ninput image through patch match based image warping. Lastly, the input image is\nrelit using material constrained layer decomposition. The experimental results\nreveal that our method can produce similar illumination effect as that of the\nreference image on the input image using only a single reference image.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:51:47 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Jin", "Xin", ""], ["Li", "Yannan", ""], ["Liu", "Ningning", ""], ["Li", "Xiaodong", ""], ["Jiang", "Xianggang", ""], ["Xiao", "Chaoen", ""], ["Ge", "Shiming", ""]]}, {"id": "1708.07089", "submitter": "Xin Jin", "authors": "Xin Jin, Le Wu, Xiaodong Li, Siyu Chen, Siwei Peng, Jingying Chi,\n  Shiming Ge, Chenggen Song, Geng Zhao", "title": "Predicting Aesthetic Score Distribution through Cumulative\n  Jensen-Shannon Divergence", "comments": "AAAI Conference on Artificial Intelligence (AAAI), New Orleans,\n  Louisiana, USA. 2-7 Feb. 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic quality prediction is a challenging task in the computer vision\ncommunity because of the complex interplay with semantic contents and\nphotographic technologies. Recent studies on the powerful deep learning based\naesthetic quality assessment usually use a binary high-low label or a numerical\nscore to represent the aesthetic quality. However the scalar representation\ncannot describe well the underlying varieties of the human perception of\naesthetics. In this work, we propose to predict the aesthetic score\ndistribution (i.e., a score distribution vector of the ordinal basic human\nratings) using Deep Convolutional Neural Network (DCNN). Conventional DCNNs\nwhich aim to minimize the difference between the predicted scalar numbers or\nvectors and the ground truth cannot be directly used for the ordinal basic\nrating distribution. Thus, a novel CNN based on the Cumulative distribution\nwith Jensen-Shannon divergence (CJS-CNN) is presented to predict the aesthetic\nscore distribution of human ratings, with a new reliability-sensitive learning\nmethod based on the kurtosis of the score distribution, which eliminates the\nrequirement of the original full data of human ratings (without normalization).\nExperimental results on large scale aesthetic dataset demonstrate the\neffectiveness of our introduced CJS-CNN in this task.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 16:37:50 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 20:12:21 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Jin", "Xin", ""], ["Wu", "Le", ""], ["Li", "Xiaodong", ""], ["Chen", "Siyu", ""], ["Peng", "Siwei", ""], ["Chi", "Jingying", ""], ["Ge", "Shiming", ""], ["Song", "Chenggen", ""], ["Zhao", "Geng", ""]]}, {"id": "1708.07120", "submitter": "Leslie Smith", "authors": "Leslie N. Smith and Nicholay Topin", "title": "Super-Convergence: Very Fast Training of Neural Networks Using Large\n  Learning Rates", "comments": "This paper was significantly revised to show super-convergence as a\n  general fast training methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a phenomenon, which we named \"super-convergence\",\nwhere neural networks can be trained an order of magnitude faster than with\nstandard training methods. The existence of super-convergence is relevant to\nunderstanding why deep networks generalize well. One of the key elements of\nsuper-convergence is training with one learning rate cycle and a large maximum\nlearning rate. A primary insight that allows super-convergence training is that\nlarge learning rates regularize the training, hence requiring a reduction of\nall other forms of regularization in order to preserve an optimal\nregularization balance. We also derive a simplification of the Hessian Free\noptimization method to compute an estimate of the optimal learning rate.\nExperiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet\ndatasets, and resnet, wide-resnet, densenet, and inception architectures. In\naddition, we show that super-convergence provides a greater boost in\nperformance relative to standard training when the amount of labeled training\ndata is limited. The architectures and code to replicate the figures in this\npaper are available at github.com/lnsmith54/super-convergence. See\nhttp://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of\nsuper-convergence to win the DAWNBench challenge (see\nhttps://dawn.cs.stanford.edu/benchmark/).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 17:51:57 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 18:24:34 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 17:40:34 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Smith", "Leslie N.", ""], ["Topin", "Nicholay", ""]]}, {"id": "1708.07199", "submitter": "Patrik Huber", "authors": "Anil Bas, Patrik Huber, William A. P. Smith, Muhammad Awais, Josef\n  Kittler", "title": "3D Morphable Models as Spatial Transformer Networks", "comments": "Accepted to ICCV 2017 2nd Workshop on Geometry Meets Deep Learning", "journal-ref": null, "doi": "10.1109/ICCVW.2017.110", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how a 3D Morphable Model (i.e. a statistical model of\nthe 3D shape of a class of objects such as faces) can be used to spatially\ntransform input data as a module (a 3DMM-STN) within a convolutional neural\nnetwork. This is an extension of the original spatial transformer network in\nthat we are able to interpret and normalise 3D pose changes and\nself-occlusions. The trained localisation part of the network is independently\nuseful since it learns to fit a 3D morphable model to a single image. We show\nthat the localiser can be trained using only simple geometric loss functions on\na relatively small dataset yet is able to perform robust normalisation on\nhighly uncontrolled images including occlusion, self-occlusion and large pose\nchanges.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 22:01:03 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bas", "Anil", ""], ["Huber", "Patrik", ""], ["Smith", "William A. P.", ""], ["Awais", "Muhammad", ""], ["Kittler", "Josef", ""]]}, {"id": "1708.07265", "submitter": "Diego Amancio Dr.", "authors": "Henrique F. de Arruda, Vanessa Q. Marinho, Thales S. Lima, Diego R.\n  Amancio, Luciano da F. Costa", "title": "An Image Analysis Approach to the Calligraphy of Books", "comments": null, "journal-ref": "Physica A 510, 110--120 (2018)", "doi": "10.1016/j.physa.2018.06.110", "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text network analysis has received increasing attention as a consequence of\nits wide range of applications. In this work, we extend a previous work founded\non the study of topological features of mesoscopic networks. Here, the\ngeometrical properties of visualized networks are quantified in terms of\nseveral image analysis techniques and used as subsidies for authorship\nattribution. It was found that the visual features account for performance\nsimilar to that achieved by using topological measurements. In addition, the\ncombination of these two types of features improved the performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 03:12:22 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["de Arruda", "Henrique F.", ""], ["Marinho", "Vanessa Q.", ""], ["Lima", "Thales S.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1708.07281", "submitter": "Zizhao Zhang", "authors": "Zizhao Zhang and Fuyong Xing and Hai Su and Xiaoshuang Shi and Lin\n  Yang", "title": "Recent Advances in the Applications of Convolutional Neural Networks to\n  Medical Image Contour Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast growing deep learning technologies have become the main solution of\nmany machine learning problems for medical image analysis. Deep convolution\nneural networks (CNNs), as one of the most important branch of the deep\nlearning family, have been widely investigated for various computer-aided\ndiagnosis tasks including long-term problems and continuously emerging new\nproblems. Image contour detection is a fundamental but challenging task that\nhas been studied for more than four decades. Recently, we have witnessed the\nsignificantly improved performance of contour detection thanks to the\ndevelopment of CNNs. Beyond purusing performance in existing natural image\nbenchmarks, contour detection plays a particularly important role in medical\nimage analysis. Segmenting various objects from radiology images or pathology\nimages requires accurate detection of contours. However, some problems, such as\ndiscontinuity and shape constraints, are insufficiently studied in CNNs. It is\nnecessary to clarify the challenges to encourage further exploration. The\nperformance of CNN based contour detection relies on the state-of-the-art CNN\narchitectures. Careful investigation of their design principles and motivations\nis critical and beneficial to contour detection. In this paper, we first review\nrecent development of medical image contour detection and point out the current\nconfronting challenges and problems. We discuss the development of general CNNs\nand their applications in image contours (or edges) detection. We compare those\nmethods in detail, clarify their strengthens and weaknesses. Then we review\ntheir recent applications in medical image analysis and point out limitations,\nwith the goal to light some potential directions in medical image analysis. We\nexpect the paper to cover comprehensive technical ingredients of advanced CNNs\nto enrich the study in the medical image domain.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 05:28:42 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Zhang", "Zizhao", ""], ["Xing", "Fuyong", ""], ["Su", "Hai", ""], ["Shi", "Xiaoshuang", ""], ["Yang", "Lin", ""]]}, {"id": "1708.07303", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak,\n  Abhinav Gupta, James Davidson, Honglak Lee", "title": "Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D\n  Representations", "comments": "Published at ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of learning 6-DOF grasping with a parallel\njaw gripper in simulation. We propose the notion of a geometry-aware\nrepresentation in grasping based on the assumption that knowledge of 3D\ngeometry is at the heart of interaction. Our key idea is constraining and\nregularizing grasping interaction learning through 3D geometry prediction.\nSpecifically, we formulate the learning of deep geometry-aware grasping model\nin two steps: First, we learn to build mental geometry-aware representation by\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\nits internal geometry-aware representation. The learned outcome prediction\nmodel is used to sequentially propose grasping solutions via\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\nof our knowledge, we are presenting for the first time a method to learn a\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\ndemonstrations in virtual reality with rich sensory and interaction\nannotations. This dataset includes 101 everyday objects spread across 7\ncategories, additionally, we propose a data augmentation strategy for effective\nlearning; (3) We demonstrate that the learned geometry-aware representation\nleads to about 10 percent relative performance improvement over the baseline\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\nmodel generalizes to novel viewpoints and object instances.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 08:09:04 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 02:50:28 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 18:57:26 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 03:40:53 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Yan", "Xinchen", ""], ["Hsu", "Jasmine", ""], ["Khansari", "Mohi", ""], ["Bai", "Yunfei", ""], ["Pathak", "Arkanath", ""], ["Gupta", "Abhinav", ""], ["Davidson", "James", ""], ["Lee", "Honglak", ""]]}, {"id": "1708.07335", "submitter": "Savas Ozkan", "authors": "Savas Ozkan and Gozde Bozdagi Akar", "title": "Relaxed Spatio-Temporal Deep Feature Aggregation for Real-Fake\n  Expression Prediction", "comments": "Submitted to International Conference on Computer Vision Workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Frame-level visual features are generally aggregated in time with the\ntechniques such as LSTM, Fisher Vectors, NetVLAD etc. to produce a robust\nvideo-level representation. We here introduce a learnable aggregation technique\nwhose primary objective is to retain short-time temporal structure between\nframe-level features and their spatial interdependencies in the representation.\nAlso, it can be easily adapted to the cases where there have very scarce\ntraining samples. We evaluate the method on a real-fake expression prediction\ndataset to demonstrate its superiority. Our method obtains 65% score on the\ntest dataset in the official MAP evaluation and there is only one misclassified\ndecision with the best reported result in the Chalearn Challenge (i.e. 66:7%) .\nLastly, we believe that this method can be extended to different problems such\nas action/event recognition in future.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 09:40:29 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Ozkan", "Savas", ""], ["Akar", "Gozde Bozdagi", ""]]}, {"id": "1708.07338", "submitter": "Inwook Shim", "authors": "Inwook Shim, Tae-Hyun Oh, Joon-Young Lee, Jinwook Choi, Dong-Geol\n  Choi, In So Kweon", "title": "Gradient-based Camera Exposure Control for Outdoor Mobile Platforms", "comments": "Extended version of IROS 2014", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2846292", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel method to automatically adjust camera exposure for image\nprocessing and computer vision applications on mobile robot platforms. Because\nmost image processing algorithms rely heavily on low-level image features that\nare based mainly on local gradient information, we consider that gradient\nquantity can determine the proper exposure level, allowing a camera to capture\nthe important image features in a manner robust to illumination conditions. We\nthen extend this concept to a multi-camera system and present a new control\nalgorithm to achieve both brightness consistency between adjacent cameras and a\nproper exposure level for each camera. We implement our prototype system with\noff-the-shelf machine-vision cameras and demonstrate the effectiveness of the\nproposed algorithms on practical applications, including pedestrian detection,\nvisual odometry, surround-view imaging, panoramic imaging and stereo matching.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 09:53:07 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 15:48:31 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 13:07:14 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Shim", "Inwook", ""], ["Oh", "Tae-Hyun", ""], ["Lee", "Joon-Young", ""], ["Choi", "Jinwook", ""], ["Choi", "Dong-Geol", ""], ["Kweon", "In So", ""]]}, {"id": "1708.07452", "submitter": "Ariel Curiale A.H.C", "authors": "Ariel H. Curiale, Flavio D. Colavecchia, Pablo Kaluza, Roberto A.\n  Isoardi and German Mato", "title": "Automatic Myocardial Segmentation by Using A Deep Learning Network in\n  Cardiac MRI", "comments": "Accepted on CLEI-JAIIO-SLCGRVPI 2017 (submitted on April 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac function is of paramount importance for both prognosis and treatment\nof different pathologies such as mitral regurgitation, ischemia, dyssynchrony\nand myocarditis. Cardiac behavior is determined by structural and functional\nfeatures. In both cases, the analysis of medical imaging studies requires to\ndetect and segment the myocardium. Nowadays, magnetic resonance imaging (MRI)\nis one of the most relevant and accurate non-invasive diagnostic tools for\ncardiac structure and function.\n  In this work we propose to use a deep learning technique to assist the\nautomatization of myocardial segmentation in cardiac MRI. We present several\nimprovements to previous works in this paper: we propose to use the Jaccard\ndistance as optimization objective function, we integrate a residual learning\nstrategy into the code, and we introduce a batch normalization layer to train\nthe fully convolutional neural network. Our results demonstrate that this\narchitecture outperforms previous approaches based on a similar network\narchitecture, and that provides a suitable approach for myocardial\nsegmentation. Our benchmark shows that the automatic myocardial segmentation\ntakes less than 22 seg. for a volume of 128~x~128~x~13 pixels in a 3.1 GHz\nintel core i7.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:19:04 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Curiale", "Ariel H.", ""], ["Colavecchia", "Flavio D.", ""], ["Kaluza", "Pablo", ""], ["Isoardi", "Roberto A.", ""], ["Mato", "German", ""]]}, {"id": "1708.07455", "submitter": "Laura Lopez-Fuentes", "authors": "Laura Lopez-Fuentes, Joost van de Weijer, Manuel Gonzalez-Hidalgo,\n  Harald Skinnemoen, Andrew D. Bagdanov", "title": "Review on Computer Vision Techniques in Emergency Situation", "comments": "25 pages", "journal-ref": "Multimedia Tools and Applications, 2017, p. 1-39", "doi": "10.1007/s11042-017-5276-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In emergency situations, actions that save lives and limit the impact of\nhazards are crucial. In order to act, situational awareness is needed to decide\nwhat to do. Geolocalized photos and video of the situations as they evolve can\nbe crucial in better understanding them and making decisions faster. Cameras\nare almost everywhere these days, either in terms of smartphones, installed\nCCTV cameras, UAVs or others. However, this poses challenges in big data and\ninformation overflow. Moreover, most of the time there are no disasters at any\ngiven location, so humans aiming to detect sudden situations may not be as\nalert as needed at any point in time. Consequently, computer vision tools can\nbe an excellent decision support. The number of emergencies where computer\nvision tools has been considered or used is very wide, and there is a great\noverlap across related emergency research. Researchers tend to focus on\nstate-of-the-art systems that cover the same emergency as they are studying,\nobviating important research in other fields. In order to unveil this overlap,\nthe survey is divided along four main axes: the types of emergencies that have\nbeen studied in computer vision, the objective that the algorithms can address,\nthe type of hardware needed and the algorithms used. Therefore, this review\nprovides a broad overview of the progress of computer vision covering all sorts\nof emergencies.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:24:47 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 12:59:22 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Lopez-Fuentes", "Laura", ""], ["van de Weijer", "Joost", ""], ["Gonzalez-Hidalgo", "Manuel", ""], ["Skinnemoen", "Harald", ""], ["Bagdanov", "Andrew D.", ""]]}, {"id": "1708.07517", "submitter": "Tal Hassner", "authors": "Fengju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia,\n  Gerard Medioni", "title": "FacePoseNet: Making a Case for Landmark-Free Face Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how a simple convolutional neural network (CNN) can be trained to\naccurately and robustly regress 6 degrees of freedom (6DoF) 3D head pose,\ndirectly from image intensities. We further explain how this FacePoseNet (FPN)\ncan be used to align faces in 2D and 3D as an alternative to explicit facial\nlandmark detection for these tasks. We claim that in many cases the standard\nmeans of measuring landmark detector accuracy can be misleading when comparing\ndifferent face alignments. Instead, we compare our FPN with existing methods by\nevaluating how they affect face recognition accuracy on the IJB-A and IJB-B\nbenchmarks: using the same recognition pipeline, but varying the face alignment\nmethod. Our results show that (a) better landmark detection accuracy measured\non the 300W benchmark does not necessarily imply better face recognition\naccuracy. (b) Our FPN provides superior 2D and 3D face alignment on both\nbenchmarks. Finally, (c), FPN aligns faces at a small fraction of the\ncomputational cost of comparably accurate landmark detectors. For many\npurposes, FPN is thus a far faster and far more accurate face alignment method\nthan using facial landmark detectors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 18:06:54 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 22:17:44 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Chang", "Fengju", ""], ["Tran", "Anh Tuan", ""], ["Hassner", "Tal", ""], ["Masi", "Iacopo", ""], ["Nevatia", "Ram", ""], ["Medioni", "Gerard", ""]]}, {"id": "1708.07522", "submitter": "J.T. Turner", "authors": "JT Turner, Kalyan Moy Gupta, David Aha", "title": "SPARCNN: SPAtially Related Convolutional Neural Networks", "comments": "6 pages, AIPR 2016 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to accurately detect and classify objects at varying pixel sizes\nin cluttered scenes is crucial to many Navy applications. However, detection\nperformance of existing state-of the-art approaches such as convolutional\nneural networks (CNNs) degrade and suffer when applied to such cluttered and\nmulti-object detection tasks. We conjecture that spatial relationships between\nobjects in an image could be exploited to significantly improve detection\naccuracy, an approach that had not yet been considered by any existing\ntechniques (to the best of our knowledge) at the time the research was\nconducted. We introduce a detection and classification technique called\nSpatially Related Detection with Convolutional Neural Networks (SPARCNN) that\nlearns and exploits a probabilistic representation of inter-object spatial\nconfigurations within images from training sets for more effective region\nproposals to use with state-of-the-art CNNs. Our empirical evaluation of\nSPARCNN on the VOC 2007 dataset shows that it increases classification accuracy\nby 8% when compared to a region proposal technique that does not exploit\nspatial relations. More importantly, we obtained a higher performance boost of\n18.8% when task difficulty in the test set is increased by including highly\nobscured objects and increased image clutter.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 18:31:03 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Turner", "JT", ""], ["Gupta", "Kalyan Moy", ""], ["Aha", "David", ""]]}, {"id": "1708.07549", "submitter": "Moi Hoon Yap", "authors": "Adrian K. Davison, Walied Merghani and Moi Hoon Yap", "title": "Objective Classes for Micro-Facial Expression Recognition", "comments": "11 pages, 4 figures and 5 tables. This paper will be submitted for\n  journal review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-expressions are brief spontaneous facial expressions that appear on a\nface when a person conceals an emotion, making them different to normal facial\nexpressions in subtlety and duration. Currently, emotion classes within the\nCASME II dataset are based on Action Units and self-reports, creating conflicts\nduring machine learning training. We will show that classifying expressions\nusing Action Units, instead of predicted emotion, removes the potential bias of\nhuman reporting. The proposed classes are tested using LBP-TOP, HOOF and HOG 3D\nfeature descriptors. The experiments are evaluated on two benchmark FACS coded\ndatasets: CASME II and SAMM. The best result achieves 86.35\\% accuracy when\nclassifying the proposed 5 classes on CASME II using HOG 3D, outperforming the\nresult of the state-of-the-art 5-class emotional-based classification in CASME\nII. Results indicate that classification based on Action Units provides an\nobjective method to improve micro-expression recognition.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 20:37:10 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 06:12:57 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Davison", "Adrian K.", ""], ["Merghani", "Walied", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1708.07555", "submitter": "Camila Laranjeira", "authors": "Guilherme Nascimento, Camila Laranjeira, Vinicius Braz, Anisio\n  Lacerda, Erickson R. Nascimento", "title": "A Robust Indoor Scene Recognition Method based on Sparse Representation", "comments": "CIARP 2017. To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a robust method for scene recognition, which\nleverages Convolutional Neural Networks (CNNs) features and Sparse Coding\nsetting by creating a new representation of indoor scenes. Although CNNs highly\nbenefited the fields of computer vision and pattern recognition, convolutional\nlayers adjust weights on a global-approach, which might lead to losing\nimportant local details such as objects and small structures. Our proposed\nscene representation relies on both: global features that mostly refers to\nenvironment's structure, and local features that are sparsely combined to\ncapture characteristics of common objects of a given scene. This new\nrepresentation is based on fragments of the scene and leverages features\nextracted by CNNs. The experimental evaluation shows that the resulting\nrepresentation outperforms previous scene recognition methods on Scene15 and\nMIT67 datasets, and performs competitively on SUN397, while being highly robust\nto perturbations in the input image such as noise and occlusion.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 21:01:08 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Nascimento", "Guilherme", ""], ["Laranjeira", "Camila", ""], ["Braz", "Vinicius", ""], ["Lacerda", "Anisio", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "1708.07570", "submitter": "Shubhra Aich", "authors": "Shubhra Aich and Ian Stavness", "title": "Leaf Counting with Deep Convolutional and Deconvolutional Networks", "comments": "Workshop: ICCV 2017 Workshop on Computer Vision Problems in Plant\n  Phenotyping (Code repository: https://github.com/p2irc/leaf_count_ICCVW-2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of counting rosette leaves from an\nRGB image, an important task in plant phenotyping. We propose a data-driven\napproach for this task generalized over different plant species and imaging\nsetups. To accomplish this task, we use state-of-the-art deep learning\narchitectures: a deconvolutional network for initial segmentation and a\nconvolutional network for leaf counting. Evaluation is performed on the leaf\ncounting challenge dataset at CVPPP-2017. Despite the small number of training\nsamples in this dataset, as compared to typical deep learning image sets, we\nobtain satisfactory performance on segmenting leaves from the background as a\nwhole and counting the number of leaves using simple data augmentation\nstrategies. Comparative analysis is provided against methods evaluated on the\nprevious competition datasets. Our framework achieves mean and standard\ndeviation of absolute count difference of 1.62 and 2.30 averaged over all five\ntest datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 22:32:23 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 00:59:53 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Aich", "Shubhra", ""], ["Stavness", "Ian", ""]]}, {"id": "1708.07590", "submitter": "Shiyang Yan", "authors": "Shiyang Yan, Jeremy S. Smith, Wenjin Lu, Bailing Zhang", "title": "Hierarchical Multi-scale Attention Networks for Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely used in natural language\nprocessing and computer vision. Among them, the Hierarchical Multi-scale RNN\n(HM-RNN), a kind of multi-scale hierarchical RNN proposed recently, can learn\nthe hierarchical temporal structure from data automatically. In this paper, we\nextend the work to solve the computer vision task of action recognition.\nHowever, in sequence-to-sequence models like RNN, it is normally very hard to\ndiscover the relationships between inputs and outputs given static inputs. As a\nsolution, attention mechanism could be applied to extract the relevant\ninformation from input thus facilitating the modeling of input-output\nrelationships. Based on these considerations, we propose a novel attention\nnetwork, namely Hierarchical Multi-scale Attention Network (HM-AN), by\ncombining the HM-RNN and the attention mechanism and apply it to action\nrecognition. A newly proposed gradient estimation method for stochastic\nneurons, namely Gumbel-softmax, is exploited to implement the temporal boundary\ndetectors and the stochastic hard attention mechanism. To amealiate the\nnegative effect of sensitive temperature of the Gumbel-softmax, an adaptive\ntemperature training method is applied to better the system performance. The\nexperimental results demonstrate the improved effect of HM-AN over LSTM with\nattention on the vision task. Through visualization of what have been learnt by\nthe networks, it can be observed that both the attention regions of images and\nthe hierarchical temporal structure can be captured by HM-AN.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 01:08:10 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 05:23:58 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yan", "Shiyang", ""], ["Smith", "Jeremy S.", ""], ["Lu", "Wenjin", ""], ["Zhang", "Bailing", ""]]}, {"id": "1708.07601", "submitter": "Wei Wang", "authors": "Wei Wang, Xiang-Gen Xia, Shengli Zhang and Chuanjiang He", "title": "A wavelet frame coefficient total variational model for image\n  restoration", "comments": "19 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a vector total variation (VTV) of feature image\nmodel for image restoration. The VTV imposes different smoothing powers on\ndifferent features (e.g. edges and cartoons) based on choosing various\nregularization parameters. Thus, the model can simultaneously preserve edges\nand remove noises. Next, the existence of solution for the model is proved and\nthe split Bregman algorithm is used to solve the model. At last, we use the\nwavelet filter banks to explicitly define the feature operator and present some\nexperimental results to show its advantage over the related methods in both\nquality and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 02:22:23 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 05:36:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Wang", "Wei", ""], ["Xia", "Xiang-Gen", ""], ["Zhang", "Shengli", ""], ["He", "Chuanjiang", ""]]}, {"id": "1708.07632", "submitter": "Kensho Hara", "authors": "Kensho Hara, Hirokatsu Kataoka, Yutaka Satoh", "title": "Learning Spatio-Temporal Features with 3D Residual Networks for Action\n  Recognition", "comments": "To appear in ICCV 2017 Workshop (Chalearn)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks with spatio-temporal 3D kernels (3D CNNs) have\nan ability to directly extract spatio-temporal features from videos for action\nrecognition. Although the 3D kernels tend to overfit because of a large number\nof their parameters, the 3D CNNs are greatly improved by using recent huge\nvideo databases. However, the architecture of 3D CNNs is relatively shallow\nagainst to the success of very deep neural networks in 2D-based CNNs, such as\nresidual networks (ResNets). In this paper, we propose a 3D CNNs based on\nResNets toward a better action representation. We describe the training\nprocedure of our 3D ResNets in details. We experimentally evaluate the 3D\nResNets on the ActivityNet and Kinetics datasets. The 3D ResNets trained on the\nKinetics did not suffer from overfitting despite the large number of parameters\nof the model, and achieved better performance than relatively shallow networks,\nsuch as C3D. Our code and pretrained models (e.g. Kinetics and ActivityNet) are\npublicly available at https://github.com/kenshohara/3D-ResNets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 07:05:49 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Hara", "Kensho", ""], ["Kataoka", "Hirokatsu", ""], ["Satoh", "Yutaka", ""]]}, {"id": "1708.07689", "submitter": "Sebastian Lapuschkin", "authors": "Sebastian Lapuschkin, Alexander Binder, Klaus-Robert M\\\"uller,\n  Wojciech Samek", "title": "Understanding and Comparing Deep Neural Networks for Age and Gender\n  Classification", "comments": "8 pages, 5 figures, 5 tables. Presented at ICCV 2017 Workshop: 7th\n  IEEE International Workshop on Analysis and Modeling of Faces and Gestures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have demonstrated excellent performances in\nrecognizing the age and gender on human face images. However, these models were\napplied in a black-box manner with no information provided about which facial\nfeatures are actually used for prediction and how these features depend on\nimage preprocessing, model initialization and architecture choice. We present a\nstudy investigating these different effects.\n  In detail, our work compares four popular neural network architectures,\nstudies the effect of pretraining, evaluates the robustness of the considered\nalignment preprocessings via cross-method test set swapping and intuitively\nvisualizes the model's prediction strategies in given preprocessing conditions\nusing the recent Layer-wise Relevance Propagation (LRP) algorithm. Our\nevaluations on the challenging Adience benchmark show that suitable parameter\ninitialization leads to a holistic perception of the input, compensating\nartefactual data representations. With a combination of simple preprocessing\nsteps, we reach state of the art performance in gender recognition.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 11:08:38 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Lapuschkin", "Sebastian", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1708.07718", "submitter": "William Smith", "authors": "Silvia Tozza, William A. P. Smith, Dizhong Zhu, Ravi Ramamoorthi and\n  Edwin R. Hancock", "title": "Linear Differential Constraints for Photo-polarimetric Height Estimation", "comments": "To appear at International Conference on Computer Vision (ICCV),\n  Venice, Italy, October 22-29, 2017", "journal-ref": null, "doi": null, "report-no": "Roma01.Math.NA", "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a differential approach to photo-polarimetric shape\nestimation. We propose several alternative differential constraints based on\npolarisation and photometric shading information and show how to express them\nin a unified partial differential system. Our method uses the image ratios\ntechnique to combine shading and polarisation information in order to directly\nreconstruct surface height, without first computing surface normal vectors.\nMoreover, we are able to remove the non-linearities so that the problem reduces\nto solving a linear differential problem. We also introduce a new method for\nestimating a polarisation image from multichannel data and, finally, we show it\nis possible to estimate the illumination directions in a two source setup,\nextending the method into an uncalibrated scenario. From a numerical point of\nview, we use a least-squares formulation of the discrete version of the\nproblem. To the best of our knowledge, this is the first work to consider a\nunified differential approach to solve photo-polarimetric shape estimation\ndirectly for height. Numerical results on synthetic and real-world data confirm\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 13:01:05 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Tozza", "Silvia", ""], ["Smith", "William A. P.", ""], ["Zhu", "Dizhong", ""], ["Ramamoorthi", "Ravi", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1708.07747", "submitter": "Han Xiao", "authors": "Han Xiao, Kashif Rasul, Roland Vollgraf", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning\n  Algorithms", "comments": "Dataset is freely available at\n  https://github.com/zalandoresearch/fashion-mnist Benchmark is available at\n  http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images\nof 70,000 fashion products from 10 categories, with 7,000 images per category.\nThe training set has 60,000 images and the test set has 10,000 images.\nFashion-MNIST is intended to serve as a direct drop-in replacement for the\noriginal MNIST dataset for benchmarking machine learning algorithms, as it\nshares the same image size, data format and the structure of training and\ntesting splits. The dataset is freely available at\nhttps://github.com/zalandoresearch/fashion-mnist\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 14:01:29 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 21:29:49 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Xiao", "Han", ""], ["Rasul", "Kashif", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1708.07755", "submitter": "Michal Balazia", "authors": "Michal Balazia and Petr Sojka", "title": "Gait Recognition from Motion Capture Data", "comments": "Preprint. Full paper accepted at the ACM Transactions on Multimedia\n  Computing, Communications, and Applications (TOMM), special issue on\n  Representation, Analysis and Recognition of 3D Humans. 18 pages. arXiv admin\n  note: substantial text overlap with arXiv:1701.00995, arXiv:1609.04392,\n  arXiv:1609.06936", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition from motion capture data, as a pattern classification\ndiscipline, can be improved by the use of machine learning. This paper\ncontributes to the state-of-the-art with a statistical approach for extracting\nrobust gait features directly from raw data by a modification of Linear\nDiscriminant Analysis with Maximum Margin Criterion. Experiments on the CMU\nMoCap database show that the suggested method outperforms thirteen relevant\nmethods based on geometric features and a method to learn the features by a\ncombination of Principal Component Analysis and Linear Discriminant Analysis.\nThe methods are evaluated in terms of the distribution of biometric templates\nin respective feature spaces expressed in a number of class separability\ncoefficients and classification metrics. Results also indicate a high\nportability of learned features, that means, we can learn what aspects of walk\npeople generally differ in and extract those as general gait features.\nRecognizing people without needing group-specific features is convenient as\nparticular people might not always provide annotated learning data. As a\ncontribution to reproducible research, our evaluation framework and database\nhave been made publicly available. This research makes motion capture\ntechnology directly applicable for human recognition.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 12:00:14 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Balazia", "Michal", ""], ["Sojka", "Petr", ""]]}, {"id": "1708.07770", "submitter": "Sebastian Stabinger MSc", "authors": "Sebastian Stabinger, Antonio Rodriguez-Sanchez", "title": "Evaluation of Deep Learning on an Abstract Image Classification Dataset", "comments": "Copyright IEEE. To be published in the proceedings of MBCC at\n  ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have become state of the art methods for image\nclassification over the last couple of years. By now they perform better than\nhuman subjects on many of the image classification datasets. Most of these\ndatasets are based on the notion of concrete classes (i.e. images are\nclassified by the type of object in the image). In this paper we present a\nnovel image classification dataset, using abstract classes, which should be\neasy to solve for humans, but variations of it are challenging for CNNs. The\nclassification performance of popular CNN architectures is evaluated on this\ndataset and variations of the dataset that might be interesting for further\nresearch are identified.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:10:22 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Stabinger", "Sebastian", ""], ["Rodriguez-Sanchez", "Antonio", ""]]}, {"id": "1708.07785", "submitter": "Hendrik Weideman", "authors": "Hendrik J. Weideman, Zachary M. Jablons, Jason Holmberg, Kiirsten\n  Flynn, John Calambokidis, Reny B. Tyson, Jason B. Allen, Randall S. Wells,\n  Krista Hupman, Kim Urian, Charles V. Stewart", "title": "Integral Curvature Representation and Matching Algorithms for\n  Identification of Dolphins and Whales", "comments": "To appear in ICCV 2017 First Workshop on Visual Wildlife Monitoring", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of identifying individual cetaceans from images\nshowing the trailing edge of their fins. Given the trailing edge from an\nunknown individual, we produce a ranking of known individuals from a database.\nThe nicks and notches along the trailing edge define an individual's unique\nsignature. We define a representation based on integral curvature that is\nrobust to changes in viewpoint and pose, and captures the pattern of nicks and\nnotches in a local neighborhood at multiple scales. We explore two ranking\nmethods that use this representation. The first uses a dynamic programming\ntime-warping algorithm to align two representations, and interprets the\nalignment cost as a measure of similarity. This algorithm also exploits learned\nspatial weights to downweight matches from regions of unstable curvature. The\nsecond interprets the representation as a feature descriptor. Feature keypoints\nare defined at the local extrema of the representation. Descriptors for the set\nof known individuals are stored in a tree structure, which allows us to perform\nqueries given the descriptors from an unknown trailing edge. We evaluate the\ntop-k accuracy on two real-world datasets to demonstrate the effectiveness of\nthe curvature representation, achieving top-1 accuracy scores of approximately\n95% and 80% for bottlenose dolphins and humpback whales, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:44:46 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Weideman", "Hendrik J.", ""], ["Jablons", "Zachary M.", ""], ["Holmberg", "Jason", ""], ["Flynn", "Kiirsten", ""], ["Calambokidis", "John", ""], ["Tyson", "Reny B.", ""], ["Allen", "Jason B.", ""], ["Wells", "Randall S.", ""], ["Hupman", "Krista", ""], ["Urian", "Kim", ""], ["Stewart", "Charles V.", ""]]}, {"id": "1708.07791", "submitter": "Mair\\'ead Grogan", "authors": "Mair\\'ead Grogan and Rozenn Dahyot", "title": "Shape Registration with Directional Data", "comments": "v2: Updated v1 by adding supplementary material", "journal-ref": "Pattern Recognition 79 (2018) 452-466", "doi": "10.1016/j.patcog.2018.02.021", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several cost functions for registration of shapes encoded with\nEuclidean and/or non-Euclidean information (unit vectors). Our framework is\nassessed for estimation of both rigid and non-rigid transformations between the\ntarget and model shapes corresponding to 2D contours and 3D surfaces. The\nexperimental results obtained confirm that using the combination of a point's\nposition and unit normal vector in a cost function can enhance the registration\nresults compared to state of the art methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:57:14 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 15:08:51 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Grogan", "Mair\u00e9ad", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1708.07808", "submitter": "Cagdas Ulas", "authors": "Cagdas Ulas, Christine Preibisch, Jonathan Sperl, Thomas Pyka,\n  Jayashree Kalpathy-Cramer, Bjoern Menze", "title": "Accelerated Reconstruction of Perfusion-Weighted MRI Enforcing Jointly\n  Local and Nonlocal Spatio-temporal Constraints", "comments": "Submission to IEEE Transactions on Medical Imaging (August 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfusion-weighted magnetic resonance imaging (MRI) is an imaging technique\nthat allows one to measure tissue perfusion in an organ of interest through the\ninjection of an intravascular paramagnetic contrast agent (CA). Due to a\npreference for high temporal and spatial resolution in many applications, this\nmodality could significantly benefit from accelerated data acquisitions. In\nthis paper, we specifically address the problem of reconstructing perfusion MR\nimage series from a subset of k-space data. Our proposed approach is motivated\nby the observation that temporal variations (dynamics) in perfusion imaging\noften exhibit correlation across different spatial scales. Hence, we propose a\nmodel that jointly penalizes the voxel-wise deviations in temporal gradient\nimages obtained based on a baseline, and the patch-wise dissimilarities between\nthe spatio-temporal neighborhoods of entire image sequence. We validate our\nmethod on dynamic susceptibility contrast (DSC)-MRI and dynamic\ncontrast-enhanced (DCE)-MRI brain perfusion datasets acquired from 10 tumor\npatients in total. We provide extensive analysis of reconstruction performance\nand perfusion parameter estimation in comparison to state-of-the-art\nreconstruction methods. Experimental results on clinical datasets demonstrate\nthat our reconstruction model can potentially achieve up to 8-fold acceleration\nby enabling accurate estimation of perfusion parameters while preserving\nspatial image details and reconstructing the complete perfusion time-intensity\ncurves (TICs).\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 16:52:04 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Ulas", "Cagdas", ""], ["Preibisch", "Christine", ""], ["Sperl", "Jonathan", ""], ["Pyka", "Thomas", ""], ["Kalpathy-Cramer", "Jayashree", ""], ["Menze", "Bjoern", ""]]}, {"id": "1708.07819", "submitter": "Christos Sakaridis", "authors": "Christos Sakaridis, Dengxin Dai, Luc Van Gool", "title": "Semantic Foggy Scene Understanding with Synthetic Data", "comments": "Accepted manuscript, 21 pages. The datasets, models and code have\n  been made publicly available", "journal-ref": "International Journal of Computer Vision, 126(9):973-992, 2018", "doi": "10.1007/s11263-018-1072-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of semantic foggy scene understanding (SFSU).\nAlthough extensive research has been performed on image dehazing and on\nsemantic scene understanding with clear-weather images, little attention has\nbeen paid to SFSU. Due to the difficulty of collecting and annotating foggy\nimages, we choose to generate synthetic fog on real images that depict\nclear-weather outdoor scenes, and then leverage these partially synthetic data\nfor SFSU by employing state-of-the-art convolutional neural networks (CNN). In\nparticular, a complete pipeline to add synthetic fog to real, clear-weather\nimages using incomplete depth information is developed. We apply our fog\nsynthesis on the Cityscapes dataset and generate Foggy Cityscapes with 20550\nimages. SFSU is tackled in two ways: 1) with typical supervised learning, and\n2) with a novel type of semi-supervised learning, which combines 1) with an\nunsupervised supervision transfer from clear-weather images to their synthetic\nfoggy counterparts. In addition, we carefully study the usefulness of image\ndehazing for SFSU. For evaluation, we present Foggy Driving, a dataset with 101\nreal-world images depicting foggy driving scenes, which come with ground truth\nannotations for semantic segmentation and object detection. Extensive\nexperiments show that 1) supervised learning with our synthetic data\nsignificantly improves the performance of state-of-the-art CNN for SFSU on\nFoggy Driving; 2) our semi-supervised learning strategy further improves\nperformance; and 3) image dehazing marginally advances SFSU with our learning\nstrategy. The datasets, models and code are made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 17:36:02 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 13:22:10 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 18:55:04 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sakaridis", "Christos", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1708.07850", "submitter": "Benjamin Haeffele", "authors": "Benjamin D. Haeffele and Rene Vidal", "title": "Structured Low-Rank Matrix Factorization: Global Optimality, Algorithms,\n  and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, convex formulations of low-rank matrix factorization problems have\nreceived considerable attention in machine learning. However, such formulations\noften require solving for a matrix of the size of the data matrix, making it\nchallenging to apply them to large scale datasets. Moreover, in many\napplications the data can display structures beyond simply being low-rank,\ne.g., images and videos present complex spatio-temporal structures that are\nlargely ignored by standard low-rank methods. In this paper we study a matrix\nfactorization technique that is suitable for large datasets and captures\nadditional structure in the factors by using a particular form of\nregularization that includes well-known regularizers such as total variation\nand the nuclear norm as particular cases. Although the resulting optimization\nproblem is non-convex, we show that if the size of the factors is large enough,\nunder certain conditions, any local minimizer for the factors yields a global\nminimizer. A few practical algorithms are also provided to solve the matrix\nfactorization problem, and bounds on the distance from a given approximate\nsolution of the optimization problem to the global optimum are derived.\nExamples in neural calcium imaging video segmentation and hyperspectral\ncompressed recovery show the advantages of our approach on high-dimensional\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:14:44 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Haeffele", "Benjamin D.", ""], ["Vidal", "Rene", ""]]}, {"id": "1708.07853", "submitter": "David Barina", "authors": "David Barina, Pavel Najman, Petr Kleparnik, Michal Kula and Pavel\n  Zemcik", "title": "The Parallel Algorithm for the 2-D Discrete Wavelet Transform", "comments": "accepted for publication at ICGIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete wavelet transform can be found at the heart of many\nimage-processing algorithms. Until now, the transform on general-purpose\nprocessors (CPUs) was mostly computed using a separable lifting scheme. As the\nlifting scheme consists of a small number of operations, it is preferred for\nprocessing using single-core CPUs. However, considering a parallel processing\nusing multi-core processors, this scheme is inappropriate due to a large number\nof steps. On such architectures, the number of steps corresponds to the number\nof points that represent the exchange of data. Consequently, these points often\nform a performance bottleneck. Our approach appropriately rearranges\ncalculations inside the transform, and thereby reduces the number of steps. In\nother words, we propose a new scheme that is friendly to parallel environments.\nWhen evaluating on multi-core CPUs, we consistently overcome the original\nlifting scheme. The evaluation was performed on 61-core Intel Xeon Phi and\n8-core Intel Xeon processors.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:19:08 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 11:57:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 07:19:38 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Barina", "David", ""], ["Najman", "Pavel", ""], ["Kleparnik", "Petr", ""], ["Kula", "Michal", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1708.07860", "submitter": "Carl Doersch", "authors": "Carl Doersch, Andrew Zisserman", "title": "Multi-task Self-Supervised Visual Learning", "comments": "Published at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate methods for combining multiple self-supervised tasks--i.e.,\nsupervised tasks where data can be collected without manual labeling--in order\nto train a single visual representation. First, we provide an apples-to-apples\ncomparison of four different self-supervised tasks using the very deep\nResNet-101 architecture. We then combine tasks to jointly train a network. We\nalso explore lasso regularization to encourage the network to factorize the\ninformation in its representation, and methods for \"harmonizing\" network inputs\nin order to learn a more unified representation. We evaluate all methods on\nImageNet classification, PASCAL VOC detection, and NYU depth prediction. Our\nresults show that deeper networks work better, and that combining tasks--even\nvia a naive multi-head architecture--always improves performance. Our best\njoint network nearly matches the PASCAL performance of a model pre-trained on\nImageNet classification, and matches the ImageNet network on NYU depth\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 18:52:17 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Doersch", "Carl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1708.07878", "submitter": "Rui Wang", "authors": "Rui Wang, Martin Schw\\\"orer, Daniel Cremers", "title": "Stereo DSO: Large-Scale Direct Sparse Visual Odometry with Stereo\n  Cameras", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Stereo Direct Sparse Odometry (Stereo DSO) as a novel method for\nhighly accurate real-time visual odometry estimation of large-scale\nenvironments from stereo cameras. It jointly optimizes for all the model\nparameters within the active window, including the intrinsic/extrinsic camera\nparameters of all keyframes and the depth values of all selected pixels. In\nparticular, we propose a novel approach to integrate constraints from static\nstereo into the bundle adjustment pipeline of temporal multi-view stereo.\nReal-time optimization is realized by sampling pixels uniformly from image\nregions with sufficient intensity gradient. Fixed-baseline stereo resolves\nscale drift. It also reduces the sensitivities to large optical flow and to\nrolling shutter effect which are known shortcomings of direct image alignment\nmethods. Quantitative evaluation demonstrates that the proposed Stereo DSO\noutperforms existing state-of-the-art visual odometry methods both in terms of\ntracking accuracy and robustness. Moreover, our method delivers a more precise\nmetric 3D reconstruction than previous dense/semi-dense direct approaches while\nproviding a higher reconstruction density than feature-based methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 20:50:54 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wang", "Rui", ""], ["Schw\u00f6rer", "Martin", ""], ["Cremers", "Daniel", ""]]}, {"id": "1708.07887", "submitter": "Joshua Engelsma", "authors": "Joshua J. Engelsma, Kai Cao, Anil K. Jain", "title": "RaspiReader: An Open Source Fingerprint Reader Facilitating Spoof\n  Detection", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design and prototype of an open source, optical fingerprint\nreader, called RaspiReader, using ubiquitous components. RaspiReader, a\nlow-cost and easy to assemble reader, provides the fingerprint research\ncommunity a seamless and simple method for gaining more control over the\nsensing component of fingerprint recognition systems. In particular, we posit\nthat this versatile fingerprint reader will encourage researchers to explore\nnovel spoof detection methods that integrate both hardware and software.\nRaspiReader's hardware is customized with two cameras for fingerprint\nacquisition with one camera providing high contrast, frustrated total internal\nreflection (FTIR) images, and the other camera outputting direct images. Using\nboth of these image streams, we extract complementary information which, when\nfused together, results in highly discriminative features for fingerprint spoof\n(presentation attack) detection. Our experimental results demonstrate a marked\nimprovement over previous spoof detection methods which rely only on FTIR\nimages provided by COTS optical readers. Finally, fingerprint matching\nexperiments between images acquired from the FTIR output of the RaspiReader and\nimages acquired from a COTS fingerprint reader verify the interoperability of\nthe RaspiReader with existing COTS optical readers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:11:28 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Cao", "Kai", ""], ["Jain", "Anil K.", ""]]}, {"id": "1708.07889", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas, Mariella Dimiccoli, Petia Radeva", "title": "Batch-Based Activity Recognition from Egocentric Photo-Streams", "comments": "8 pages, 7 figures, 1 table. To appear at the ICCV 2017 workshop on\n  Egocentric Perception, Interaction and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition from long unstructured egocentric photo-streams has\nseveral applications in assistive technology such as health monitoring and\nfrailty detection, just to name a few. However, one of its main technical\nchallenges is to deal with the low frame rate of wearable photo-cameras, which\ncauses abrupt appearance changes between consecutive frames. In consequence,\nimportant discriminatory low-level features from motion such as optical flow\ncannot be estimated. In this paper, we present a batch-driven approach for\ntraining a deep learning architecture that strongly rely on Long short-term\nunits to tackle this problem. We propose two different implementations of the\nsame approach that process a photo-stream sequence using batches of fixed size\nwith the goal of capturing the temporal evolution of high-level features. The\nmain difference between these implementations is that one explicitly models\nconsecutive batches by overlapping them. Experimental results over a public\ndataset acquired by three users demonstrate the validity of the proposed\narchitectures to exploit the temporal evolution of convolutional features over\ntime without relying on event boundaries.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 21:12:42 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Cartas", "Alejandro", ""], ["Dimiccoli", "Mariella", ""], ["Radeva", "Petia", ""]]}, {"id": "1708.07920", "submitter": "Hidetoshi Furukawa", "authors": "Hidetoshi Furukawa", "title": "Deep Learning for Target Classification from SAR Imagery: Data\n  Augmentation and Translation Invariance", "comments": "Technical Report, 5 pages, 8 figures, Copyright(C)2017 IEICE", "journal-ref": "IEICE Technical Report, vol.117, no.182, SANE2017-30, pp.13-17,\n  Aug. 2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report deals with translation invariance of convolutional neural\nnetworks (CNNs) for automatic target recognition (ATR) from synthetic aperture\nradar (SAR) imagery. In particular, the translation invariance of CNNs for SAR\nATR represents the robustness against misalignment of target chips extracted\nfrom SAR images. To understand the translation invariance of the CNNs, we\ntrained CNNs which classify the target chips from the MSTAR into the ten\nclasses under the condition of with and without data augmentation, and then\nvisualized the translation invariance of the CNNs. According to our results,\neven if we use a deep residual network, the translation invariance of the CNN\nwithout data augmentation using the aligned images such as the MSTAR target\nchips is not so large. A more important factor of translation invariance is the\nuse of augmented training data. Furthermore, our CNN using augmented training\ndata achieved a state-of-the-art classification accuracy of 99.6%. These\nresults show an importance of domain-specific data augmentation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 02:44:09 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Furukawa", "Hidetoshi", ""]]}, {"id": "1708.07933", "submitter": "Ehsan Shojaedini", "authors": "Ehsan Shojaedini and Reza Safabakhsh", "title": "Robust Stereo Feature Descriptor for Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a simple way to utilize stereo camera data to\nimprove feature descriptors. Computer vision algorithms that use a stereo\ncamera require some calculations of 3D information. We leverage this\npre-calculated information to improve feature descriptor algorithms. We use the\n3D feature information to estimate the scale of each feature. This way, each\nfeature descriptor will be more robust to scale change without significant\ncomputations. In addition, we use stereo images to construct the descriptor\nvector. The Scale-Invariant Feature Transform (SIFT) and Fast Retina Keypoint\n(FREAK) descriptors are used to evaluate the proposed method. The scale\nnormalization technique in feature tracking test improves the standard SIFT by\n8.75% and improves the standard FREAK by 28.65%. Using the proposed stereo\nfeature descriptor, a visual odometry algorithm is designed and tested on the\nKITTI dataset. The stereo FREAK descriptor raises the number of inlier matches\nby 19% and consequently improves the accuracy of visual odometry by 23%.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 05:35:02 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 14:17:48 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Shojaedini", "Ehsan", ""], ["Safabakhsh", "Reza", ""]]}, {"id": "1708.07937", "submitter": "Siddharth Srivastava", "authors": "Siddharth Srivastava, Brejesh Lall", "title": "3D Binary Signatures", "comments": "Tenth Indian Conference on Computer Vision, Graphics and Image\n  Processing 2016", "journal-ref": null, "doi": "10.1145/3009977.3010009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel binary descriptor for 3D point clouds. The\nproposed descriptor termed as 3D Binary Signature (3DBS) is motivated from the\nmatching efficiency of the binary descriptors for 2D images. 3DBS describes\nkeypoints from point clouds with a binary vector resulting in extremely fast\nmatching. The method uses keypoints from standard keypoint detectors. The\ndescriptor is built by constructing a Local Reference Frame and aligning a\nlocal surface patch accordingly. The local surface patch constitutes of\nidentifying nearest neighbours based upon an angular constraint among them. The\npoints are ordered with respect to the distance from the keypoints. The normals\nof the ordered pairs of these keypoints are projected on the axes and the\nrelative magnitude is used to assign a binary digit. The vector thus\nconstituted is used as a signature for representing the keypoints. The matching\nis done by using hamming distance. We show that 3DBS outperforms state of the\nart descriptors on various evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 06:01:09 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Srivastava", "Siddharth", ""], ["Lall", "Brejesh", ""]]}, {"id": "1708.07954", "submitter": "Karthikeyan Natesan Ramamurthy", "authors": "Karthikeyan Natesan Ramamurthy, Chung-Ching Lin, Aleksandr Aravkin,\n  Sharath Pankanti, Raphael Viguier", "title": "Distributed Bundle Adjustment", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most methods for Bundle Adjustment (BA) in computer vision are either\ncentralized or operate incrementally. This leads to poor scaling and affects\nthe quality of solution as the number of images grows in large scale structure\nfrom motion (SfM). Furthermore, they cannot be used in scenarios where image\nacquisition and processing must be distributed. We address this problem with a\nnew distributed BA algorithm. Our distributed formulation uses alternating\ndirection method of multipliers (ADMM), and, since each processor sees only a\nsmall portion of the data, we show that robust formulations improve\nperformance. We analyze convergence of the proposed algorithm, and illustrate\nnumerical performance, accuracy of the parameter estimates, and scalability of\nthe distributed implementation in the context of synthetic 3D datasets with\nknown camera position and orientation ground truth. The results are comparable\nto an alternate state-of-the-art centralized bundle adjustment algorithm on\nsynthetic and real 3D reconstruction problems. The runtime of our\nimplementation scales linearly with the number of observed points.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 09:13:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Ramamurthy", "Karthikeyan Natesan", ""], ["Lin", "Chung-Ching", ""], ["Aravkin", "Aleksandr", ""], ["Pankanti", "Sharath", ""], ["Viguier", "Raphael", ""]]}, {"id": "1708.07969", "submitter": "Bo Yang", "authors": "Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, Niki\n  Trigoni", "title": "3D Object Reconstruction from a Single Depth View with Adversarial\n  Learning", "comments": "ICCV Workshops 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the\ncomplete 3D structure of a given object from a single arbitrary depth view\nusing generative adversarial networks. Unlike the existing work which typically\nrequires multiple views of the same object or class labels to recover the full\n3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of\na depth view of the object as input, and is able to generate the complete 3D\noccupancy grid by filling in the occluded/missing regions. The key idea is to\ncombine the generative capabilities of autoencoders and the conditional\nGenerative Adversarial Networks (GAN) framework, to infer accurate and\nfine-grained 3D structures of objects in high-dimensional voxel space.\nExtensive experiments on large synthetic datasets show that the proposed\n3D-RecGAN significantly outperforms the state of the art in single view 3D\nobject reconstruction, and is able to reconstruct unseen types of objects. Our\ncode and data are available at: https://github.com/Yang7879/3D-RecGAN.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 13:46:21 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yang", "Bo", ""], ["Wen", "Hongkai", ""], ["Wang", "Sen", ""], ["Clark", "Ronald", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1708.07972", "submitter": "Andrey Savchenko", "authors": "Andrey V. Savchenko, Natalya S. Belova", "title": "Maximum A Posteriori Estimation of Distances Between Deep Features in\n  Still-to-Video Face Recognition", "comments": "20 pages, 5 figures, 40 references", "journal-ref": null, "doi": "10.1016/j.eswa.2018.04.039", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the still-to-video face recognition for the small sample\nsize problem based on computation of distances between high-dimensional deep\nbottleneck features. We present the novel statistical recognition method, in\nwhich the still-to-video recognition task is casted into Maximum A Posteriori\nestimation. In this method we maximize the joint probabilistic density of the\ndistances to all reference still images. It is shown that this likelihood can\nbe estimated with the known asymptotically normal distribution of the\nKullback-Leibler discriminations between nonnegative features. The experimental\nstudy with the LFW (Labeled Faces in the Wild), YTF (YouTube Faces) and IJB-A\n(IARPA Janus Benchmark A) datasets has been provided. We demonstrated, that the\nproposed approach can be applied with the state-of-the-art deep features and\ndissimilarity measures. Our algorithm achieves 3-5% higher accuracy when\ncompared with conventional aggregation of decisions obtained for all frames.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 13:56:48 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Savchenko", "Andrey V.", ""], ["Belova", "Natalya S.", ""]]}, {"id": "1708.07977", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Keylor Daniel Chaves Viquez and Ognjen Arandjelovic and Andrew Blaikie\n  and In Ae Hwang", "title": "Synthesising Wider Field Images from Narrow-Field Retinal Video Acquired\n  Using a Low-Cost Direct Ophthalmoscope (Arclight) Attached to a Smartphone", "comments": "International Conference on Computer Vision Workshop on BioImage\n  Computing, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to low cost retinal imaging devices in low and middle income countries\nis limited, compromising progress in preventing needless blindness. The\nArclight is a recently developed low-cost solar powered direct ophthalmoscope\nwhich can be attached to the camera of a smartphone to acquire retinal images\nand video. However, the acquired data is inherently limited by the optics of\ndirect ophthalmoscopy, resulting in a narrow field of view with associated\ncorneal reflections, limiting its usefulness. In this work we describe the\nfirst fully automatic method utilizing videos acquired using the Arclight\nattached to a mobile phone camera to create wider view, higher quality still\nimages comparable with images obtained using much more expensive and bulky\ndedicated traditional retinal cameras.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 14:28:21 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Viquez", "Keylor Daniel Chaves", ""], ["Arandjelovic", "Ognjen", ""], ["Blaikie", "Andrew", ""], ["Hwang", "In Ae", ""]]}, {"id": "1708.07987", "submitter": "arXiv Admin", "authors": "Vamshhi Pavan Kumar Varma Vegeshna", "title": "Stereo Matching With Color-Weighted Correlation, Hierarchical Belief\n  Propagation And Occlusion Handling", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we contrive a stereo matching algorithm with careful handling\nof disparity, discontinuity and occlusion. This algorithm works a worldwide\nmatching stereo model which is based on minimization of energy. The global\nenergy comprises two terms, firstly the data term and secondly the smoothness\nterm. The data term is approximated by a color-weighted correlation, then\nrefined in obstruct and low-texture areas in many applications of hierarchical\nloopy belief propagation algorithm. The results during the experiment are\nevaluated on the Middlebury data sets, showing that out algorithm is the top\nperformer among all the algorithms listed there\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 15:43:39 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 13:32:04 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Vegeshna", "Vamshhi Pavan Kumar Varma", ""]]}, {"id": "1708.08016", "submitter": "Viraj Mavani", "authors": "Viraj Mavani, Shanmuganathan Raman, Krishna P Miyapuram", "title": "Facial Expression Recognition using Visual Saliency and Deep Learning", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We have developed a convolutional neural network for the purpose of\nrecognizing facial expressions in human beings. We have fine-tuned the existing\nconvolutional neural network model trained on the visual recognition dataset\nused in the ILSVRC2012 to two widely used facial expression datasets - CFEE and\nRaFD, which when trained and tested independently yielded test accuracies of\n74.79% and 95.71%, respectively. Generalization of results was evident by\ntraining on one dataset and testing on the other. Further, the image product of\nthe cropped faces and their visual saliency maps were computed using Deep\nMulti-Layer Network for saliency prediction and were fed to the facial\nexpression recognition CNN. In the most generalized experiment, we observed the\ntop-1 accuracy in the test set to be 65.39%. General confusion trends between\ndifferent facial expressions as exhibited by humans were also observed.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 20:03:38 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Mavani", "Viraj", ""], ["Raman", "Shanmuganathan", ""], ["Miyapuram", "Krishna P", ""]]}, {"id": "1708.08042", "submitter": "Songqing Yue", "authors": "Songqing Yue", "title": "Imbalanced Malware Images Classification: a CNN based Approach", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) can be applied to malware binary\ndetection through images classification. The performance, however, is degraded\ndue to the imbalance of malware families (classes). To mitigate this issue, we\npropose a simple yet effective weighted softmax loss which can be employed as\nthe final layer of deep CNNs. The original softmax loss is weighted, and the\nweight value can be determined according to class size. A scaling parameter is\nalso included in computing the weight. Proper selection of this parameter has\nbeen studied and an empirical option is given. The weighted loss aims at\nalleviating the impact of data imbalance in an end-to-end learning fashion. To\nvalidate the efficacy, we deploy the proposed weighted loss in a pre-trained\ndeep CNN model and fine-tune it to achieve promising results on malware images\nclassification. Extensive experiments also indicate that the new loss function\ncan fit other typical CNNs with an improved classification performance.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 02:27:59 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yue", "Songqing", ""]]}, {"id": "1708.08062", "submitter": "Hong-Xing Yu", "authors": "Hong-Xing Yu, Ancong Wu, Wei-Shi Zheng", "title": "Cross-view Asymmetric Metric Learning for Unsupervised Person\n  Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While metric learning is important for Person re-identification (RE-ID), a\nsignificant problem in visual surveillance for cross-view pedestrian matching,\nexisting metric models for RE-ID are mostly based on supervised learning that\nrequires quantities of labeled samples in all pairs of camera views for\ntraining. However, this limits their scalabilities to realistic applications,\nin which a large amount of data over multiple disjoint camera views is\navailable but not labelled. To overcome the problem, we propose unsupervised\nasymmetric metric learning for unsupervised RE-ID. Our model aims to learn an\nasymmetric metric, i.e., specific projection for each view, based on asymmetric\nclustering on cross-view person images. Our model finds a shared space where\nview-specific bias is alleviated and thus better matching performance can be\nachieved. Extensive experiments have been conducted on a baseline and five\nlarge-scale RE-ID datasets to demonstrate the effectiveness of the proposed\nmodel. Through the comparison, we show that our model works much more suitable\nfor unsupervised RE-ID compared to classical unsupervised metric learning\nmodels. We also compare with existing unsupervised RE-ID methods, and our model\noutperforms them with notable margins. Specifically, we report the results on\nlarge-scale unlabelled RE-ID dataset, which is important but unfortunately less\nconcerned in literatures.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 07:59:29 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 14:21:46 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Yu", "Hong-Xing", ""], ["Wu", "Ancong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1708.08117", "submitter": "Jonas Pichat", "authors": "Jonas Pichat, Juan Eugenio Iglesias, Sotiris Nousias, Tarek Yousry,\n  Sebastien Ourselin, Marc Modat", "title": "Part-to-whole Registration of Histology and MRI using Shape Elements", "comments": "Paper accepted at ICCV Workshop (Bio-Image Computing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image registration between histology and magnetic resonance imaging (MRI) is\na challenging task due to differences in structural content and contrast. Too\nthick and wide specimens cannot be processed all at once and must be cut into\nsmaller pieces. This dramatically increases the complexity of the problem,\nsince each piece should be individually and manually pre-aligned. To the best\nof our knowledge, no automatic method can reliably locate such piece of tissue\nwithin its respective whole in the MRI slice, and align it without any prior\ninformation. We propose here a novel automatic approach to the joint problem of\nmultimodal registration between histology and MRI, when only a fraction of\ntissue is available from histology. The approach relies on the representation\nof images using their level lines so as to reach contrast invariance. Shape\nelements obtained via the extraction of bitangents are encoded in a\nprojective-invariant manner, which permits the identification of common pieces\nof curves between two images. We evaluated the approach on human brain\nhistology and compared resulting alignments against manually annotated ground\ntruths. Considering the complexity of the brain folding patterns, preliminary\nresults are promising and suggest the use of characteristic and meaningful\nshape elements for improved robustness and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 18:01:41 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Pichat", "Jonas", ""], ["Iglesias", "Juan Eugenio", ""], ["Nousias", "Sotiris", ""], ["Yousry", "Tarek", ""], ["Ourselin", "Sebastien", ""], ["Modat", "Marc", ""]]}, {"id": "1708.08141", "submitter": "Abrar Ahmed", "authors": "Abrar Ahmed, Anish Bikmal", "title": "One-Shot Concept Learning by Simulating Evolutionary Instinct\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition has become a crucial part of machine learning and computer\nvision recently. The current approach to object recognition involves Deep\nLearning and uses Convolutional Neural Networks to learn the pixel patterns of\nthe objects implicitly through backpropagation. However, CNNs require thousands\nof examples in order to generalize successfully and often require heavy\ncomputing resources for training. This is considered rather sluggish when\ncompared to the human ability to generalize and learn new categories given just\na single example. Additionally, CNNs make it difficult to explicitly\nprogrammatically modify or intuitively interpret their learned representations.\n  We propose a computational model that can successfully learn an object\ncategory from as few as one example and allows its learning style to be\ntailored explicitly to a scenario. Our model decomposes each image into two\nattributes: shape and color distribution. We then use a Bayesian criterion to\nprobabilistically determine the likelihood of each category. The model takes\neach factor into account based on importance and calculates the conditional\nprobability of the object belonging to each learned category. Our model is not\nonly applicable to visual scenarios, but can also be implemented in a broader\nand more practical scope of situations such as Natural Language Processing as\nwell as other places where it is possible to retrieve and construct individual\nattributes. Because the only condition our model presents is the ability to\nretrieve and construct individual attributes such as shape and color, it can be\napplied to essentially any class of visual objects.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 21:15:23 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Ahmed", "Abrar", ""], ["Bikmal", "Anish", ""]]}, {"id": "1708.08169", "submitter": "Yusuke Niitani", "authors": "Yusuke Niitani, Toru Ogawa, Shunta Saito, Masaki Saito", "title": "ChainerCV: a Library for Deep Learning in Computer Vision", "comments": "Accepted to ACM MM 2017 Open Source Software Competition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant progress of deep learning in the field of computer\nvision, there has not been a software library that covers these methods in a\nunifying manner. We introduce ChainerCV, a software library that is intended to\nfill this gap. ChainerCV supports numerous neural network models as well as\nsoftware components needed to conduct research in computer vision. These\nimplementations emphasize simplicity, flexibility and good software engineering\npractices. The library is designed to perform on par with the results reported\nin published papers and its tools can be used as a baseline for future research\nin computer vision. Our implementation includes sophisticated models like\nFaster R-CNN and SSD, and covers tasks such as object detection and semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 02:54:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Niitani", "Yusuke", ""], ["Ogawa", "Toru", ""], ["Saito", "Shunta", ""], ["Saito", "Masaki", ""]]}, {"id": "1708.08180", "submitter": "Jun Chen", "authors": "Jun Chen, Qiang Yao, Houari Sabirin, Keisuke Nonaka, Hiroshi Sankoh,\n  Sei Naito", "title": "An Optimized Union-Find Algorithm for Connected Components Labeling\n  Using GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report an optimized union-find (UF) algorithm that can\nlabel the connected components on a 2D image efficiently by employing the GPU\narchitecture. The proposed method contains three phases: UF-based local merge,\nboundary analysis, and link. The coarse labeling in local merge reduces the\nnumber atomic operations, while the boundary analysis only manages the pixels\non the boundary of each block. Evaluation results showed that the proposed\nalgorithm speed up the average running time by more than 1.3X.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 03:54:19 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 00:54:28 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Chen", "Jun", ""], ["Yao", "Qiang", ""], ["Sabirin", "Houari", ""], ["Nonaka", "Keisuke", ""], ["Sankoh", "Hiroshi", ""], ["Naito", "Sei", ""]]}, {"id": "1708.08189", "submitter": "Ali Al-Bayaty", "authors": "Ahmed F. Hussein, Abbas K. AlZubaidi, Ali Al-Bayaty, Qais A. Habash", "title": "An IoT Real-Time Biometric Authentication System Based on ECG Fiducial\n  Extracted Features Using Discrete Cosine Transform", "comments": "6 pages, 8 figures, IoT, Authentication, ECG, DCT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional authentication technologies, like RFID tags and\nauthentication cards/badges, suffer from different weaknesses, therefore a\nprompt replacement to use biometric method of authentication should be applied\ninstead. Biometrics, such as fingerprints, voices, and ECG signals, are unique\nhuman characters that can be used for authentication processing. In this work,\nwe present an IoT real-time authentication system based on using extracted ECG\nfeatures to identify the unknown persons. The Discrete Cosine Transform (DCT)\nis used as an ECG feature extraction, where it has better characteristics for\nreal-time system implementations. There are a substantial number of researches\nwith a high accuracy of authentication, but most of them ignore the real-time\ncapability of authenticating individuals. With the accuracy rate of 97.78% at\naround 1.21 seconds of processing time, the proposed system is more suitable\nfor use in many applications that require fast and reliable authentication\nprocessing demands.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 04:59:08 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Hussein", "Ahmed F.", ""], ["AlZubaidi", "Abbas K.", ""], ["Al-Bayaty", "Ali", ""], ["Habash", "Qais A.", ""]]}, {"id": "1708.08190", "submitter": "Hui Zeng", "authors": "Hui Zeng, Lei Zhang, Alan C. Bovik", "title": "A Probabilistic Quality Representation Approach to Deep Blind Image\n  Quality Prediction", "comments": "Add the link of source code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image quality assessment (BIQA) remains a very challenging problem due\nto the unavailability of a reference image. Deep learning based BIQA methods\nhave been attracting increasing attention in recent years, yet it remains a\ndifficult task to train a robust deep BIQA model because of the very limited\nnumber of training samples with human subjective scores. Most existing methods\nlearn a regression network to minimize the prediction error of a scalar image\nquality score. However, such a scheme ignores the fact that an image will\nreceive divergent subjective scores from different subjects, which cannot be\nadequately represented by a single scalar number. This is particularly true on\ncomplex, real-world distorted images. Moreover, images may broadly differ in\ntheir distributions of assigned subjective scores. Recognizing this, we propose\na new representation of perceptual image quality, called probabilistic quality\nrepresentation (PQR), to describe the image subjective score distribution,\nwhereby a more robust loss function can be employed to train a deep BIQA model.\nThe proposed PQR method is shown to not only speed up the convergence of deep\nmodel training, but to also greatly improve the achievable level of quality\nprediction accuracy relative to scalar quality score regression methods. The\nsource code is available at https://github.com/HuiZeng/BIQA_Toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 05:09:44 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 04:55:26 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Zeng", "Hui", ""], ["Zhang", "Lei", ""], ["Bovik", "Alan C.", ""]]}, {"id": "1708.08197", "submitter": "Tianyue Zheng", "authors": "Tianyue Zheng, Weihong Deng and Jiani Hu", "title": "Cross-Age LFW: A Database for Studying Cross-Age Face Recognition in\n  Unconstrained Environments", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeled Faces in the Wild (LFW) database has been widely utilized as the\nbenchmark of unconstrained face verification and due to big data driven machine\nlearning methods, the performance on the database approaches nearly 100%.\nHowever, we argue that this accuracy may be too optimistic because of some\nlimiting factors. Besides different poses, illuminations, occlusions and\nexpressions, cross-age face is another challenge in face recognition. Different\nages of the same person result in large intra-class variations and aging\nprocess is unavoidable in real world face verification. However, LFW does not\npay much attention on it. Thereby we construct a Cross-Age LFW (CALFW) which\ndeliberately searches and selects 3,000 positive face pairs with age gaps to\nadd aging process intra-class variance. Negative pairs with same gender and\nrace are also selected to reduce the influence of attribute difference between\npositive/negative pairs and achieve face verification instead of attributes\nclassification. We evaluate several metric learning and deep learning methods\non the new database. Compared to the accuracy on LFW, the accuracy drops about\n10%-17% on CALFW.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 06:07:27 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Zheng", "Tianyue", ""], ["Deng", "Weihong", ""], ["Hu", "Jiani", ""]]}, {"id": "1708.08201", "submitter": "Yalong Bai", "authors": "Yalong Bai, Kuiyuan Yang, Tao Mei, Wei-Ying Ma and Tiejun Zhao", "title": "Automatic Dataset Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale image dataset and deep convolutional neural network (DCNN) are\ntwo primary driving forces for the rapid progress made in generic object\nrecognition tasks in recent years. While lots of network architectures have\nbeen continuously designed to pursue lower error rates, few efforts are devoted\nto enlarge existing datasets due to high labeling cost and unfair comparison\nissues. In this paper, we aim to achieve lower error rate by augmenting\nexisting datasets in an automatic manner. Our method leverages both Web and\nDCNN, where Web provides massive images with rich contextual information, and\nDCNN replaces human to automatically label images under guidance of Web\ncontextual information. Experiments show our method can automatically scale up\nexisting datasets significantly from billions web pages with high accuracy, and\nsignificantly improve the performance on object recognition tasks by using the\nautomatically augmented datasets, which demonstrates that more supervisory\ninformation has been automatically gathered from the Web. Both the dataset and\nmodels trained on the dataset are made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 06:22:00 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 04:38:45 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bai", "Yalong", ""], ["Yang", "Kuiyuan", ""], ["Mei", "Tao", ""], ["Ma", "Wei-Ying", ""], ["Zhao", "Tiejun", ""]]}, {"id": "1708.08245", "submitter": "Wei Lu", "authors": "Ruxin Wang, Wei Lu, Shijun Xiang, Xianfeng Zhao, Jinwei Wang", "title": "Digital image splicing detection based on Markov features in QDCT and\n  QWT domain", "comments": "We found that in the experiments all the features are not normalized\n  with the same parameter and setup. So the whole experiments must be re-done\n  since the data is not proper for its current state. Maybe the method should\n  be re-designed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image splicing detection is of fundamental importance in digital forensics\nand therefore has attracted increasing attention recently. In this paper, a\ncolor image splicing detection approach is proposed based on Markov transition\nprobability of quaternion component separation in quaternion discrete cosine\ntransform (QDCT) domain and quaternion wavelet transform (QWT) domain. Firstly,\nMarkov features of the intra-block and inter-block between block QDCT\ncoefficients are obtained from the real part and three imaginary parts of QDCT\ncoefficients respectively. Then, additional Markov features are extracted from\nluminance (Y) channel in quaternion wavelet transform domain to characterize\nthe dependency of position among quaternion wavelet subband coefficients.\nFinally, ensemble classifier (EC) is exploited to classify the spliced and\nauthentic color images. The experiment results demonstrate that the proposed\napproach can outperforms some state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 09:24:51 GMT"}, {"version": "v2", "created": "Wed, 20 Dec 2017 05:14:22 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 03:42:52 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Wang", "Ruxin", ""], ["Lu", "Wei", ""], ["Xiang", "Shijun", ""], ["Zhao", "Xianfeng", ""], ["Wang", "Jinwei", ""]]}, {"id": "1708.08267", "submitter": "Huan Fu", "authors": "Huan Fu, Mingming Gong, Chaohui Wang, Dacheng Tao", "title": "A Compromise Principle in Deep Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation, which plays a key role in understanding 3D scene\ngeometry, is fundamentally an ill-posed problem. Existing methods based on deep\nconvolutional neural networks (DCNNs) have examined this problem by learning\nconvolutional networks to estimate continuous depth maps from monocular images.\nHowever, we find that training a network to predict a high spatial resolution\ncontinuous depth map often suffers from poor local solutions. In this paper, we\nhypothesize that achieving a compromise between spatial and depth resolutions\ncan improve network training. Based on this \"compromise principle\", we propose\na regression-classification cascaded network (RCCN), which consists of a\nregression branch predicting a low spatial resolution continuous depth map and\na classification branch predicting a high spatial resolution discrete depth\nmap. The two branches form a cascaded structure allowing the classification and\nregression branches to benefit from each other. By leveraging large-scale raw\ntraining datasets and some data augmentation strategies, our network achieves\ntop or state-of-the-art results on the NYU Depth V2, KITTI, and Make3D\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 10:51:35 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 05:33:06 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Fu", "Huan", ""], ["Gong", "Mingming", ""], ["Wang", "Chaohui", ""], ["Tao", "Dacheng", ""]]}, {"id": "1708.08288", "submitter": "Yibing Song", "authors": "Yibing Song, Linchao Bao, Shengfeng He, Qingxiong Yang, Ming-Hsuan\n  Yang", "title": "Stylizing Face Images via Multiple Exemplars", "comments": "In CVIU 2017. Project Page:\n  http://www.cs.cityu.edu.hk/~yibisong/cviu17/index.html", "journal-ref": null, "doi": "10.1016/j.cviu.2017.08.009", "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of transferring the style of a headshot photo to face\nimages. Existing methods using a single exemplar lead to inaccurate results\nwhen the exemplar does not contain sufficient stylized facial components for a\ngiven photo. In this work, we propose an algorithm to stylize face images using\nmultiple exemplars containing different subjects in the same style. Patch\ncorrespondences between an input photo and multiple exemplars are established\nusing a Markov Random Field (MRF), which enables accurate local energy transfer\nvia Laplacian stacks. As image patches from multiple exemplars are used, the\nboundaries of facial components on the target image are inevitably\ninconsistent. The artifacts are removed by a post-processing step using an\nedge-preserving filter. Experimental results show that the proposed algorithm\nconsistently produces visually pleasing results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:36:33 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Song", "Yibing", ""], ["Bao", "Linchao", ""], ["He", "Shengfeng", ""], ["Yang", "Qingxiong", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.08310", "submitter": "Maria-Irina Nicolae", "authors": "Vincent P.A. Lonij, Ambrish Rawat, Maria-Irina Nicolae", "title": "Open-World Visual Recognition Using Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a real-world setting, visual recognition systems can be brought to make\npredictions for images belonging to previously unknown class labels. In order\nto make semantically meaningful predictions for such inputs, we propose a\ntwo-step approach that utilizes information from knowledge graphs. First, a\nknowledge-graph representation is learned to embed a large set of entities into\na semantic space. Second, an image representation is learned to embed images\ninto the same space. Under this setup, we are able to predict structured\nproperties in the form of relationship triples for any open-world image. This\nis true even when a set of labels has been omitted from the training protocols\nof both the knowledge graph and image embeddings. Furthermore, we append this\nlearning framework with appropriate smoothness constraints and show how prior\nknowledge can be incorporated into the model. Both these improvements combined\nincrease performance for visual recognition by a factor of six compared to our\nbaseline. Finally, we propose a new, extended dataset which we use for\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:45:07 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Lonij", "Vincent P. A.", ""], ["Rawat", "Ambrish", ""], ["Nicolae", "Maria-Irina", ""]]}, {"id": "1708.08311", "submitter": "Duc Nguyen", "authors": "Duc Minh Nguyen and Evaggelia Tsiligianni and Nikos Deligiannis", "title": "Deep Learning Sparse Ternary Projections for Compressed Sensing of\n  Images", "comments": "To appear in GlobalSIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing (CS) is a sampling theory that allows reconstruction of\nsparse (or compressible) signals from an incomplete number of measurements,\nusing of a sensing mechanism implemented by an appropriate projection matrix.\nThe CS theory is based on random Gaussian projection matrices, which satisfy\nrecovery guarantees with high probability; however, sparse ternary {0, -1, +1}\nprojections are more suitable for hardware implementation. In this paper, we\npresent a deep learning approach to obtain very sparse ternary projections for\ncompressed sensing. Our deep learning architecture jointly learns a pair of a\nprojection matrix and a reconstruction operator in an end-to-end fashion. The\nexperimental results on real images demonstrate the effectiveness of the\nproposed approach compared to state-of-the-art methods, with significant\nadvantage in terms of complexity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:51:09 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Nguyen", "Duc Minh", ""], ["Tsiligianni", "Evaggelia", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "1708.08325", "submitter": "Markus Oberweger", "authors": "Markus Oberweger and Vincent Lepetit", "title": "DeepPrior++: Improving Fast and Accurate 3D Hand Pose Estimation", "comments": "To appear in ICCV Workshops 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepPrior is a simple approach based on Deep Learning that predicts the joint\n3D locations of a hand given a depth map. Since its publication early 2015, it\nhas been outperformed by several impressive works. Here we show that with\nsimple improvements: adding ResNet layers, data augmentation, and better\ninitial hand localization, we achieve better or similar performance than more\nsophisticated recent methods on the three main benchmarks (NYU, ICVL, MSRA)\nwhile keeping the simplicity of the original method. Our new implementation is\navailable at https://github.com/moberweger/deep-prior-pp .\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:15:49 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Oberweger", "Markus", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1708.08333", "submitter": "Jong Chul Ye", "authors": "Yoseob Han and Jong Chul Ye", "title": "Framing U-Net via Deep Convolutional Framelets: Application to\n  Sparse-view CT", "comments": "This will appear in IEEE Transaction on Medical Imaging, a special\n  issue of Machine Learning for Image Reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray computed tomography (CT) using sparse projection views is a recent\napproach to reduce the radiation dose. However, due to the insufficient\nprojection views, an analytic reconstruction approach using the filtered back\nprojection (FBP) produces severe streaking artifacts. Recently, deep learning\napproaches using large receptive field neural networks such as U-Net have\ndemonstrated impressive performance for sparse- view CT reconstruction.\nHowever, theoretical justification is still lacking. Inspired by the recent\ntheory of deep convolutional framelets, the main goal of this paper is,\ntherefore, to reveal the limitation of U-Net and propose new multi-resolution\ndeep learning schemes. In particular, we show that the alternative U- Net\nvariants such as dual frame and the tight frame U-Nets satisfy the so-called\nframe condition which make them better for effective recovery of high frequency\nedges in sparse view- CT. Using extensive experiments with real patient data\nset, we demonstrate that the new network architectures provide better\nreconstruction performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:31:50 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 10:04:46 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 07:20:28 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Han", "Yoseob", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1708.08417", "submitter": "Vladimir Krylov A.", "authors": "Vladimir A. Krylov, Eamonn Kenny, and Rozenn Dahyot", "title": "Automatic Discovery and Geotagging of Objects from Street View Imagery", "comments": "Video demo at https://youtu.be/X0tM_iSRJMw", "journal-ref": null, "doi": "10.3390/rs10050661", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications such as autonomous navigation, urban planning and asset\nmonitoring, rely on the availability of accurate information about objects and\ntheir geolocations. In this paper we propose to automatically detect and\ncompute the GPS coordinates of recurring stationary objects of interest using\nstreet view imagery. Our processing pipeline relies on two fully convolutional\nneural networks: the first segments objects in the images while the second\nestimates their distance from the camera. To geolocate all the detected objects\ncoherently we propose a novel custom Markov Random Field model to perform\nobjects triangulation. The novelty of the resulting pipeline is the combined\nuse of monocular depth estimation and triangulation to enable automatic mapping\nof complex scenes with multiple visually similar objects of interest. We\nvalidate experimentally the effectiveness of our approach on two object\nclasses: traffic lights and telegraph poles. The experiments report high object\nrecall rates and GPS accuracy within 2 meters, which is comparable with the\nprecision of single-frequency GPS receivers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 16:54:16 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 17:05:02 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Krylov", "Vladimir A.", ""], ["Kenny", "Eamonn", ""], ["Dahyot", "Rozenn", ""]]}, {"id": "1708.08430", "submitter": "J.T. Turner", "authors": "JT Turner, Adam Page, Tinoosh Mohsenin and Tim Oates", "title": "Deep Belief Networks used on High Resolution Multichannel\n  Electroencephalography Data for Seizure Detection", "comments": "Old draft of AAAI paper, AAAI Spring Symposium Series. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous bio-sensing for personalized health monitoring is slowly becoming\na reality with the increasing availability of small, diverse, robust, high\nfidelity sensors. This oncoming flood of data begs the question of how we will\nextract useful information from it. In this paper we explore the use of a\nvariety of representations and machine learning algorithms applied to the task\nof seizure detection in high resolution, multichannel EEG data. We explore\nclassification accuracy, computational complexity and memory requirements with\na view toward understanding which approaches are most suitable for such tasks\nas the number of people involved and the amount of data they produce grows to\nbe quite large. In particular, we show that layered learning approaches such as\nDeep Belief Networks excel along these dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 17:28:48 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Turner", "JT", ""], ["Page", "Adam", ""], ["Mohsenin", "Tinoosh", ""], ["Oates", "Tim", ""]]}, {"id": "1708.08487", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Kai Arulkumaran, Anil A. Bharath", "title": "On denoising autoencoders trained to minimise binary cross-entropy", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising autoencoders (DAEs) are powerful deep learning models used for\nfeature extraction, data generation and network pre-training. DAEs consist of\nan encoder and decoder which may be trained simultaneously to minimise a loss\n(function) between an input and the reconstruction of a corrupted version of\nthe input. There are two common loss functions used for training autoencoders,\nthese include the mean-squared error (MSE) and the binary cross-entropy (BCE).\nWhen training autoencoders on image data a natural choice of loss function is\nBCE, since pixel values may be normalised to take values in [0,1] and the\ndecoder model may be designed to generate samples that take values in (0,1). We\nshow theoretically that DAEs trained to minimise BCE may be used to take\ngradient steps in the data space towards regions of high probability under the\ndata-generating distribution. Previously this had only been shown for DAEs\ntrained using MSE. As a consequence of the theory, iterative application of a\ntrained DAE moves a data sample from regions of low probability to regions of\nhigher probability under the data-generating distribution. Firstly, we validate\nthe theory by showing that novel data samples, consistent with the training\ndata, may be synthesised when the initial data samples are random noise.\nSecondly, we motivate the theory by showing that initial data samples\nsynthesised via other methods may be improved via iterative application of a\ntrained DAE to those initial samples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 19:07:33 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 08:40:39 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Creswell", "Antonia", ""], ["Arulkumaran", "Kai", ""], ["Bharath", "Anil A.", ""]]}, {"id": "1708.08508", "submitter": "Azin Asgarian", "authors": "Azin Asgarian, Ahmed Bilal Ashraf, David Fleet, and Babak Taati", "title": "Subspace Selection to Suppress Confounding Source Domain Information in\n  AAM Transfer Learning", "comments": "Copyright IEEE. To be published in the proceedings of International\n  Joint Conference on Biometrics (IJCB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active appearance models (AAMs) are a class of generative models that have\nseen tremendous success in face analysis. However, model learning depends on\nthe availability of detailed annotation of canonical landmark points. As a\nresult, when accurate AAM fitting is required on a different set of variations\n(expression, pose, identity), a new dataset is collected and annotated. To\novercome the need for time consuming data collection and annotation, transfer\nlearning approaches have received recent attention. The goal is to transfer\nknowledge from previously available datasets (source) to a new dataset\n(target). We propose a subspace transfer learning method, in which we select a\nsubspace from the source that best describes the target space. We propose a\nmetric to compute the directional similarity between the source eigenvectors\nand the target subspace. We show an equivalence between this metric and the\nvariance of target data when projected onto source eigenvectors. Using this\nequivalence, we select a subset of source principal directions that capture the\nvariance in target data. To define our model, we augment the selected source\nsubspace with the target subspace learned from a handful of target examples. In\nexperiments done on six publicly available datasets, we show that our approach\noutperforms the state of the art in terms of the RMS fitting error as well as\nthe percentage of test examples for which AAM fitting converges to the ground\ntruth.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 20:21:21 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 18:26:16 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Asgarian", "Azin", ""], ["Ashraf", "Ahmed Bilal", ""], ["Fleet", "David", ""], ["Taati", "Babak", ""]]}, {"id": "1708.08670", "submitter": "Yuri G. Gordienko", "authors": "Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy, and\n  Yuri Gordienko", "title": "Performance Analysis of Open Source Machine Learning Frameworks for\n  Various Parameters in Single-Threaded and Multi-Threaded Modes", "comments": "15 pages, 11 figures, 4 tables; this paper summarizes the activities\n  which were started recently and described shortly in the previous conference\n  presentations arXiv:1706.02248 and arXiv:1707.04940; it is accepted for\n  Springer book series \"Advances in Intelligent Systems and Computing\"", "journal-ref": "Advances in Intelligent Systems and Computing II. CSIT 2017.\n  Advances in Intelligent Systems and Computing, vol 689, pp 243-256. Springer,\n  Cham", "doi": "10.1007/978-3-319-70581-1_17", "report-no": null, "categories": "cs.LG cs.CV cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic features of some of the most versatile and popular open source\nframeworks for machine learning (TensorFlow, Deep Learning4j, and H2O) are\nconsidered and compared. Their comparative analysis was performed and\nconclusions were made as to the advantages and disadvantages of these\nplatforms. The performance tests for the de facto standard MNIST data set were\ncarried out on H2O framework for deep learning algorithms designed for CPU and\nGPU platforms for single-threaded and multithreaded modes of operation Also, we\npresent the results of testing neural networks architectures on H2O platform\nfor various activation functions, stopping metrics, and other parameters of\nmachine learning algorithm. It was demonstrated for the use case of MNIST\ndatabase of handwritten digits in single-threaded mode that blind selection of\nthese parameters can hugely increase (by 2-3 orders) the runtime without the\nsignificant increase of precision. This result can have crucial influence for\noptimization of available and new machine learning methods, especially for\nimage recognition problems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 09:54:28 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kochura", "Yuriy", ""], ["Stirenko", "Sergii", ""], ["Alienin", "Oleg", ""], ["Novotarskiy", "Michail", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1708.08687", "submitter": "Zefan Li", "authors": "Zefan Li, Bingbing Ni, Wenjun Zhang, Xiaokang Yang and Wen Gao", "title": "Performance Guaranteed Network Acceleration via High-Order Residual\n  Quantization", "comments": "9 pages, 8 figures, Proceeding of IEEE International Conference on\n  Computer Vision 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input binarization has shown to be an effective way for network acceleration.\nHowever, previous binarization scheme could be regarded as simple pixel-wise\nthresholding operations (i.e., order-one approximation) and suffers a big\naccuracy loss. In this paper, we propose a highorder binarization scheme, which\nachieves more accurate approximation while still possesses the advantage of\nbinary operation. In particular, the proposed scheme recursively performs\nresidual quantization and yields a series of binary input images with\ndecreasing magnitude scales. Accordingly, we propose high-order binary\nfiltering and gradient propagation operations for both forward and backward\ncomputations. Theoretical analysis shows approximation error guarantee property\nof proposed method. Extensive experimental results demonstrate that the\nproposed scheme yields great recognition accuracy while being accelerated.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 10:42:29 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Li", "Zefan", ""], ["Ni", "Bingbing", ""], ["Zhang", "Wenjun", ""], ["Yang", "Xiaokang", ""], ["Gao", "Wen", ""]]}, {"id": "1708.08705", "submitter": "Jeremias Sulam", "authors": "Jeremias Sulam, Vardan Papyan, Yaniv Romano, Michael Elad", "title": "Multi-Layer Convolutional Sparse Modeling: Pursuit and Dictionary\n  Learning", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 66, no. 15, pp.\n  4090-4104, Aug.1, 1 2018", "doi": "10.1109/TSP.2018.2846226", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Multi-Layer Convolutional Sparse Coding (ML-CSC) model,\nconsisting of a cascade of convolutional sparse layers, provides a new\ninterpretation of Convolutional Neural Networks (CNNs). Under this framework,\nthe computation of the forward pass in a CNN is equivalent to a pursuit\nalgorithm aiming to estimate the nested sparse representation vectors -- or\nfeature maps -- from a given input signal. Despite having served as a pivotal\nconnection between CNNs and sparse modeling, a deeper understanding of the\nML-CSC is still lacking: there are no pursuit algorithms that can serve this\nmodel exactly, nor are there conditions to guarantee a non-empty model. While\none can easily obtain signals that approximately satisfy the ML-CSC\nconstraints, it remains unclear how to simply sample from the model and, more\nimportantly, how one can train the convolutional filters from real data.\n  In this work, we propose a sound pursuit algorithm for the ML-CSC model by\nadopting a projection approach. We provide new and improved bounds on the\nstability of the solution of such pursuit and we analyze different practical\nalternatives to implement this in practice. We show that the training of the\nfilters is essential to allow for non-trivial signals in the model, and we\nderive an online algorithm to learn the dictionaries from real data,\neffectively resulting in cascaded sparse convolutional layers. Last, but not\nleast, we demonstrate the applicability of the ML-CSC model for several\napplications in an unsupervised setting, providing competitive results. Our\nwork represents a bridge between matrix factorization, sparse dictionary\nlearning and sparse auto-encoders, and we analyze these connections in detail.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 11:43:40 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 19:46:15 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Sulam", "Jeremias", ""], ["Papyan", "Vardan", ""], ["Romano", "Yaniv", ""], ["Elad", "Michael", ""]]}, {"id": "1708.08711", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Setting an attention region for convolutional neural networks using\n  region selective features, for recognition of materials within glass vessels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Convolutional neural networks have emerged as the leading method for the\nclassification and segmentation of images. In some cases, it is desirable to\nfocus the attention of the net on a specific region in the image; one such case\nis the recognition of the contents of transparent vessels, where the vessel\nregion in the image is already known. This work presents a valve filter\napproach for focusing the attention of the net on a region of interest (ROI).\nIn this approach, the ROI is inserted into the net as a binary map. The net\nuses a different set of convolution filters for the ROI and background image\nregions, resulting in a different set of features being extracted from each\nregion. More accurately, for each filter used on the image, a corresponding\nvalve filter exists that acts on the ROI map and determines the regions in\nwhich the corresponding image filter will be used. This valve filter\neffectively acts as a valve that inhibits specific features in different image\nregions according to the ROI map. In addition, a new data set for images of\nmaterials in glassware vessels in a chemistry laboratory setting is presented.\nThis data set contains a thousand images with pixel-wise annotation according\nto categories ranging from filled and empty to the exact phase of the material\ninside the vessel. The results of the valve filter approach and fully\nconvolutional neural nets (FCN) with no ROI input are compared based on this\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 11:53:37 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 13:07:36 GMT"}, {"version": "v3", "created": "Sat, 9 Sep 2017 19:25:27 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1708.08728", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos, Theodore Giannakopoulos, Christophoros Nikou,\n  Ioannis A. Kakadiaris", "title": "Curriculum Learning for Multi-Task Classification of Visual Attributes", "comments": "To appear in ICCV Workshops 2017 (TASK-CV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attributes, from simple objects (e.g., backpacks, hats) to\nsoft-biometrics (e.g., gender, height, clothing) have proven to be a powerful\nrepresentational approach for many applications such as image description and\nhuman identification. In this paper, we introduce a novel method to combine the\nadvantages of both multi-task and curriculum learning in a visual attribute\nclassification framework. Individual tasks are grouped based on their\ncorrelation so that two groups of strongly and weakly correlated tasks are\nformed. The two groups of tasks are learned in a curriculum learning setup by\ntransferring the acquired knowledge from the strongly to the weakly correlated.\nThe learning process within each group though, is performed in a multi-task\nclassification setup. The proposed method learns better and converges faster\nthan learning all the tasks in a typical multi-task learning paradigm. We\ndemonstrate the effectiveness of our approach on the publicly available, SoBiR,\nVIPeR and PETA datasets and report state-of-the-art results across the board.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 12:40:52 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Giannakopoulos", "Theodore", ""], ["Nikou", "Christophoros", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1708.08732", "submitter": "Maria Brbic", "authors": "Maria Brbic and Ivica Kopriva", "title": "Multi-view Low-rank Sparse Subspace Clustering", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2017.08.024", "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing approaches address multi-view subspace clustering problem by\nconstructing the affinity matrix on each view separately and afterwards propose\nhow to extend spectral clustering algorithm to handle multi-view data. This\npaper presents an approach to multi-view subspace clustering that learns a\njoint subspace representation by constructing affinity matrix shared among all\nviews. Relying on the importance of both low-rank and sparsity constraints in\nthe construction of the affinity matrix, we introduce the objective that\nbalances between the agreement across different views, while at the same time\nencourages sparsity and low-rankness of the solution. Related low-rank and\nsparsity constrained optimization problem is for each view solved using the\nalternating direction method of multipliers. Furthermore, we extend our\napproach to cluster data drawn from nonlinear subspaces by solving the\ncorresponding problem in a reproducing kernel Hilbert space. The proposed\nalgorithm outperforms state-of-the-art multi-view subspace clustering\nalgorithms on one synthetic and four real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:07:56 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Brbic", "Maria", ""], ["Kopriva", "Ivica", ""]]}, {"id": "1708.08754", "submitter": "Giovanni Poggi", "authors": "Dario D'Avino, Davide Cozzolino, Giovanni Poggi, Luisa Verdoliva", "title": "Autoencoder with recurrent neural networks for video forgery detection", "comments": "Presented at IS&T Electronic Imaging: Media Watermarking, Security,\n  and Forensics, January 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video forgery detection is becoming an important issue in recent years,\nbecause modern editing software provide powerful and easy-to-use tools to\nmanipulate videos. In this paper we propose to perform detection by means of\ndeep learning, with an architecture based on autoencoders and recurrent neural\nnetworks. A training phase on a few pristine frames allows the autoencoder to\nlearn an intrinsic model of the source. Then, forged material is singled out as\nanomalous, as it does not fit the learned model, and is encoded with a large\nreconstruction error. Recursive networks, implemented with the long short-term\nmemory model, are used to exploit temporal dependencies. Preliminary results on\nforged videos show the potential of this approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 14:06:16 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["D'Avino", "Dario", ""], ["Cozzolino", "Davide", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1708.08760", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Shilpa Manandhar, Yee Hui Lee and Stefan Winkler", "title": "Study of Clear Sky Models for Singapore", "comments": "Published in Proc. Progress In Electromagnetics Research Symposium\n  (PIERS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of total solar irradiance falling on the earth's surface is\nimportant in the field of solar energy generation and forecasting. Several\nclear-sky solar radiation models have been developed over the last few decades.\nMost of these models are based on empirical distribution of various\ngeographical parameters; while a few models consider various atmospheric\neffects in the solar energy estimation. In this paper, we perform a comparative\nanalysis of several popular clear-sky models, in the tropical region of\nSingapore. This is important in countries like Singapore, where we are\nprimarily focused on reliable and efficient solar energy generation. We analyze\nand compare three popular clear-sky models that are widely used in the\nliterature. We validate our solar estimation results using actual solar\nirradiance measurements obtained from collocated weather stations. We finally\nconclude the most reliable clear sky model for Singapore, based on all clear\nsky days in a year.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 17:40:38 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Manandhar", "Shilpa", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1708.08825", "submitter": "Yuankai Huo", "authors": "Yuankai Huo, Susan M. Resnick and Bennett A. Landman", "title": "4D Multi-atlas Label Fusion using Longitudinal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal reproducibility is an essential concern in automated medical\nimage segmentation, yet has proven to be an elusive objective as manual brain\nstructure tracings have shown more than 10% variability. To improve\nreproducibility, lon-gitudinal segmentation (4D) approaches have been\ninvestigated to reconcile tem-poral variations with traditional 3D approaches.\nIn the past decade, multi-atlas la-bel fusion has become a state-of-the-art\nsegmentation technique for 3D image and many efforts have been made to adapt it\nto a 4D longitudinal fashion. However, the previous methods were either limited\nby using application specified energy function (e.g., surface fusion and multi\nmodel fusion) or only considered tem-poral smoothness on two consecutive time\npoints (t and t+1) under sparsity as-sumption. Therefore, a 4D multi-atlas\nlabel fusion theory for general label fusion purpose and simultaneously\nconsidering temporal consistency on all time points is appealing. Herein, we\npropose a novel longitudinal label fusion algorithm, called 4D joint label\nfusion (4DJLF), to incorporate the temporal consistency modeling via non-local\npatch-intensity covariance models. The advantages of 4DJLF include: (1) 4DJLF\nis under the general label fusion framework by simul-taneously incorporating\nthe spatial and temporal covariance on all longitudinal time points. (2) The\nproposed algorithm is a longitudinal generalization of a lead-ing joint label\nfusion method (JLF) that has proven adaptable to a wide variety of\napplications. (3) The spatial temporal consistency of atlases is modeled in a\nprob-abilistic model inspired from both voting based and statistical fusion.\nThe pro-posed approach improves the consistency of the longitudinal\nsegmentation while retaining sensitivity compared with original JLF approach\nusing the same set of atlases. The method is available online in open-source.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 15:33:53 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Huo", "Yuankai", ""], ["Resnick", "Susan M.", ""], ["Landman", "Bennett A.", ""]]}, {"id": "1708.08844", "submitter": "Jan Czarnowski", "authors": "Jan Czarnowski, Stefan Leutenegger, Andrew Davison", "title": "Semantic Texture for Robust Dense Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that robust dense SLAM systems can make valuable use of the layers\nof features coming from a standard CNN as a pyramid of `semantic texture' which\nis suitable for dense alignment while being much more robust to nuisance\nfactors such as lighting than raw RGB values. We use a straightforward\nLucas-Kanade formulation of image alignment, with a schedule of iterations over\nthe coarse-to-fine levels of a pyramid, and simply replace the usual image\npyramid by the hierarchy of convolutional feature maps from a pre-trained CNN.\nThe resulting dense alignment performance is much more robust to lighting and\nother variations, as we show by camera rotation tracking experiments on\ntime-lapse sequences captured over many hours. Looking towards the future of\nscene representation for real-time visual SLAM, we further demonstrate that a\nselection using simple criteria of a small number of the total set of features\noutput by a CNN gives just as accurate but much more efficient tracking\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 15:58:18 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Czarnowski", "Jan", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew", ""]]}, {"id": "1708.08874", "submitter": "Jong-Chyi Su", "authors": "Jong-Chyi Su, Chenyun Wu, Huaizu Jiang, Subhransu Maji", "title": "Reasoning about Fine-grained Attribute Phrases using Reference Games", "comments": "To appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning to describe fine-grained visual\ndifferences between instances using attribute phrases. Attribute phrases\ncapture distinguishing aspects of an object (e.g., \"propeller on the nose\" or\n\"door near the wing\" for airplanes) in a compositional manner. Instances within\na category can be described by a set of these phrases and collectively they\nspan the space of semantic attributes for a category. We collect a large\ndataset of such phrases by asking annotators to describe several visual\ndifferences between a pair of instances within a category. We then learn to\ndescribe and ground these phrases to images in the context of a *reference\ngame* between a speaker and a listener. The goal of a speaker is to describe\nattributes of an image that allows the listener to correctly identify it within\na pair. Data collected in a pairwise manner improves the ability of the speaker\nto generate, and the ability of the listener to interpret visual descriptions.\nMoreover, due to the compositionality of attribute phrases, the trained\nlisteners can interpret descriptions not seen during training for image\nretrieval, and the speakers can generate attribute-based explanations for\ndifferences between previously unseen categories. We also show that embedding\nan image into the semantic space of attribute phrases derived from listeners\noffers 20% improvement in accuracy over existing attribute-based\nrepresentations on the FGVC-aircraft dataset.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 16:57:39 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Su", "Jong-Chyi", ""], ["Wu", "Chenyun", ""], ["Jiang", "Huaizu", ""], ["Maji", "Subhransu", ""]]}, {"id": "1708.08917", "submitter": "Caiwen Ding Kevin Ding", "authors": "Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo,\n  Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian\n  Tang, Qinru Qiu, Xue Lin, Bo Yuan", "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices", "comments": "14 pages, 15 Figures, conference", "journal-ref": null, "doi": "10.1145/3123939.3124552", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 04:18:57 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Ding", "Caiwen", ""], ["Liao", "Siyu", ""], ["Wang", "Yanzhi", ""], ["Li", "Zhe", ""], ["Liu", "Ning", ""], ["Zhuo", "Youwei", ""], ["Wang", "Chao", ""], ["Qian", "Xuehai", ""], ["Bai", "Yu", ""], ["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Zhang", "Yipeng", ""], ["Tang", "Jian", ""], ["Qiu", "Qinru", ""], ["Lin", "Xue", ""], ["Yuan", "Bo", ""]]}, {"id": "1708.08985", "submitter": "Giovanni De Magistris", "authors": "Asim Munawar, Phongtharin Vinayavekhin and Giovanni De Magistris", "title": "Limiting the Reconstruction Capability of Generative Neural Network\n  using Negative Learning", "comments": "Conference: IEEE International Workshop on Machine Learning for\n  Signal Processing (MLSP), Roppongi, Tokyo, Japan, September 25-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are widely used for unsupervised learning with various\napplications, including data compression and signal restoration. Training\nmethods for such systems focus on the generality of the network given limited\namount of training data. A less researched type of techniques concerns\ngeneration of only a single type of input. This is useful for applications such\nas constraint handling, noise reduction and anomaly detection. In this paper we\npresent a technique to limit the generative capability of the network using\nnegative learning. The proposed method searches the solution in the gradient\ndirection for the desired input and in the opposite direction for the undesired\ninput. One of the application can be anomaly detection where the undesired\ninputs are the anomalous data. In the results section we demonstrate the\nfeatures of the algorithm using MNIST handwritten digit dataset and latter\napply the technique to a real-world obstacle detection problem. The results\nclearly show that the proposed learning technique can significantly improve the\nperformance for anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:16:14 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Munawar", "Asim", ""], ["Vinayavekhin", "Phongtharin", ""], ["De Magistris", "Giovanni", ""]]}, {"id": "1708.08986", "submitter": "Ding Zhao", "authors": "Wenshuo Wang, Junqiang Xi, Ding Zhao", "title": "Driving Style Analysis Using Primitive Driving Patterns With Bayesian\n  Nonparametric Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and recognition of driving styles are profoundly important to\nintelligent transportation and vehicle calibration. This paper presents a novel\ndriving style analysis framework using the primitive driving patterns learned\nfrom naturalistic driving data. In order to achieve this, first, a Bayesian\nnonparametric learning method based on a hidden semi-Markov model (HSMM) is\nintroduced to extract primitive driving patterns from time series driving data\nwithout prior knowledge of the number of these patterns. In the Bayesian\nnonparametric approach, we utilize a hierarchical Dirichlet process (HDP)\ninstead of learning the unknown number of smooth dynamical modes of HSMM, thus\ngenerating the primitive driving patterns. Each primitive pattern is clustered\nand then labeled using behavioral semantics according to drivers' physical and\npsychological perception thresholds. For each driver, 75 primitive driving\npatterns in car-following scenarios are learned and semantically labeled. In\norder to show the HDP-HSMM's utility to learn primitive driving patterns, other\ntwo Bayesian nonparametric approaches, HDP-HMM and sticky HDP-HMM, are\ncompared. The naturalistic driving data of 18 drivers were collected from the\nUniversity of Michigan Safety Pilot Model Deployment (SPDM) database. The\nindividual driving styles are discussed according to distribution\ncharacteristics of the learned primitive driving patterns and also the\ndifference in driving styles among drivers are evaluated using the\nKullback-Leibler divergence. The experiment results demonstrate that the\nproposed primitive pattern-based method can allow one to semantically\nunderstand driver behaviors and driving styles.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:36:30 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Wang", "Wenshuo", ""], ["Xi", "Junqiang", ""], ["Zhao", "Ding", ""]]}, {"id": "1708.08987", "submitter": "Mina Rezaei", "authors": "Mina Rezaei, Haojin Yang and Christoph Meinel", "title": "Deep Learning for Medical Image Analysis", "comments": "Presented in doctoral consortium in the AIME-2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes my research activities in the Hasso Plattner Institute\nand summarizes my Ph.D. plan and several novels, end-to-end trainable\napproaches for analyzing medical images using deep learning algorithm. In this\nreport, as an example, we explore different novel methods based on deep\nlearning for brain abnormality detection, recognition, and segmentation. This\nreport prepared for the doctoral consortium in the AIME-2017 conference.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:09:12 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Rezaei", "Mina", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "1708.08988", "submitter": "Tuan Ho", "authors": "Tuan Ho, Madhukar Budagavi", "title": "Dual-fisheye lens stitching for 360-degree imaging", "comments": "ICASSP 17 preprint, Proc. of the 42nd IEEE International Conference\n  on Acoustics, Speech and Signal Processing (ICASSP), New Orleans, USA, March\n  2017", "journal-ref": null, "doi": "10.1109/ICASSP.2017.7952541", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual-fisheye lens cameras have been increasingly used for 360-degree\nimmersive imaging. However, the limited overlapping field of views and\nmisalignment between the two lenses give rise to visible discontinuities in the\nstitching boundaries. This paper introduces a novel method for dual-fisheye\ncamera stitching that adaptively minimizes the discontinuities in the\noverlapping regions to generate full spherical 360-degree images. Results show\nthat this approach can produce good quality stitched images for Samsung Gear\n360 -- a dual-fisheye camera, even with hard-to-stitch objects in the stitching\nborders.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 03:23:48 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Ho", "Tuan", ""], ["Budagavi", "Madhukar", ""]]}, {"id": "1708.08989", "submitter": "Yu Zhao", "authors": "Yu Zhao, Rennong Yang, Guillaume Chevalier, Maoguo Gong", "title": "Deep Residual Bidir-LSTM for Human Activity Recognition Using Wearable\n  Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) has become a popular topic in research\nbecause of its wide application. With the development of deep learning, new\nideas have appeared to address HAR problems. Here, a deep network architecture\nusing residual bidirectional long short-term memory (LSTM) cells is proposed.\nThe advantages of the new network include that a bidirectional connection can\nconcatenate the positive time direction (forward state) and the negative time\ndirection (backward state). Second, residual connections between stacked cells\nact as highways for gradients, which can pass underlying information directly\nto the upper layer, effectively avoiding the gradient vanishing problem.\nGenerally, the proposed network shows improvements on both the temporal (using\nbidirectional cells) and the spatial (residual connections stacked deeply)\ndimensions, aiming to enhance the recognition rate. When tested with the\nOpportunity data set and the public domain UCI data set, the accuracy was\nincreased by 4.78% and 3.68%, respectively, compared with previously reported\nresults. Finally, the confusion matrix of the public domain UCI data set was\nanalyzed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 11:02:13 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 07:36:31 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Zhao", "Yu", ""], ["Yang", "Rennong", ""], ["Chevalier", "Guillaume", ""], ["Gong", "Maoguo", ""]]}, {"id": "1708.08992", "submitter": "Guillaume Noyel", "authors": "Guillaume Noyel (IPRI), Michel Jourlin (IPRI, LHC)", "title": "A simple expression for the map of Asplund's distances with the\n  multiplicative Logarithmic Image Processing (LIP) law", "comments": "Accepted to the 12th European Congress for Stereology and Image\n  Analysis 2017, Kaiserslautern, Germany, September 11-14, 2017", "journal-ref": "12th European Congress for Stereology and Image Analysis 2017, Sep\n  2017, Kaiserslautern, Germany. Proceedings of the 12th European Congress for\n  Stereology and Image Analysis 2017.\n  http://www.mathematik.uni-kl.de/events/ecsia-2017/12th-ecsia-2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple expression for the map of Asplund's distances with the\nmultiplicative Logarithmic Image Processing (LIP) law. It is a difference\nbetween a morphological dilation and a morphological erosion with an additive\nstructuring function which corresponds to a morphological gradient.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:24:41 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 07:54:05 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Noyel", "Guillaume", "", "IPRI"], ["Jourlin", "Michel", "", "IPRI, LHC"]]}, {"id": "1708.08995", "submitter": "Soumyabrata Dev", "authors": "Shilpa Manandhar, Soumyabrata Dev, Yee Hui Lee, and Yu Song Meng", "title": "Analyzing Cloud Optical Properties Using Sky Cameras", "comments": "Published in Proc. Progress In Electromagnetics Research Symposium\n  (PIERS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clouds play a significant role in the fluctuation of solar radiation received\nby the earth's surface. It is important to study the various cloud properties,\nas it impacts the total solar irradiance falling on the earth's surface. One of\nsuch important optical properties of the cloud is the Cloud Optical Thickness\n(COT). It is defined with the amount of light that can pass through the clouds.\nThe COT values are generally obtained from satellite images. However, satellite\nimages have a low temporal- and spatial- resolutions; and are not suitable for\nstudy in applications as solar energy generation and forecasting. Therefore,\nground-based sky cameras are now getting popular in such fields. In this paper,\nwe analyze the cloud optical thickness value, from the ground-based sky\ncameras, and provide future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 16:50:54 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Manandhar", "Shilpa", ""], ["Dev", "Soumyabrata", ""], ["Lee", "Yee Hui", ""], ["Meng", "Yu Song", ""]]}, {"id": "1708.08997", "submitter": "Xiaoshui Huang", "authors": "Xiaoshui Huang", "title": "Learning a 3D descriptor for cross-source point cloud registration from\n  synthetic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the development of 3D sensors, registration of 3D data (e.g. point cloud)\ncoming from different kind of sensor is dispensable and shows great demanding.\nHowever, point cloud registration between different sensors is challenging\nbecause of the variant of density, missing data, different viewpoint, noise and\noutliers, and geometric transformation. In this paper, we propose a method to\nlearn a 3D descriptor for finding the correspondent relations between these\nchallenging point clouds. To train the deep learning framework, we use\nsynthetic 3D point cloud as input. Starting from synthetic dataset, we use\nregion-based sampling method to select reasonable, large and diverse training\nsamples from synthetic samples. Then, we use data augmentation to extend our\nnetwork be robust to rotation transformation. We focus our work on more general\ncases that point clouds coming from different sensors, named cross-source point\ncloud. The experiments show that our descriptor is not only able to generalize\nto new scenes, but also generalize to different sensors. The results\ndemonstrate that the proposed method successfully aligns two 3D cross-source\npoint clouds which outperforms state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 22:38:02 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Huang", "Xiaoshui", ""]]}, {"id": "1708.08998", "submitter": "Shima Kamyab", "authors": "Shima Kamyab, Ali Ghodsi, S. Zohreh Azimifar", "title": "Deep Structure for end-to-end inverse rendering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse rendering in a 3D format denoted to recovering the 3D properties of a\nscene given 2D input image(s) and is typically done using 3D Morphable Model\n(3DMM) based methods from single view images. These models formulate each face\nas a weighted combination of some basis vectors extracted from the training\ndata. In this paper a deep framework is proposed in which the coefficients and\nbasis vectors are computed by training an autoencoder network and a\nConvolutional Neural Network (CNN) simultaneously. The idea is to find a common\ncause which can be mapped to both the 3D structure and corresponding 2D image\nusing deep networks. The empirical results verify the power of deep framework\nin finding accurate 3D shapes of human faces from their corresponding 2D images\non synthetic datasets of human faces.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 14:00:15 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Kamyab", "Shima", ""], ["Ghodsi", "Ali", ""], ["Azimifar", "S. Zohreh", ""]]}, {"id": "1708.08999", "submitter": "Mauro Zucchelli", "authors": "Mauro Zucchelli, Maxime Descoteaux, Gloria Menegaz", "title": "NODDI-SH: a computational efficient NODDI extension for fODF estimation\n  in diffusion MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Magnetic Resonance Imaging (DMRI) is the only non-invasive imaging\ntechnique which is able to detect the principal directions of water diffusion\nas well as neurites density in the human brain. Exploiting the ability of\nSpherical Harmonics (SH) to model spherical functions, we propose a new\nreconstruction model for DMRI data which is able to estimate both the fiber\nOrientation Distribution Function (fODF) and the relative volume fractions of\nthe neurites in each voxel, which is robust to multiple fiber crossings. We\nconsider a Neurite Orientation Dispersion and Density Imaging (NODDI) inspired\nsingle fiber diffusion signal to be derived from three compartments:\nintracellular, extracellular, and cerebrospinal fluid. The model, called\nNODDI-SH, is derived by convolving the single fiber response with the fODF in\neach voxel. NODDI-SH embeds the calculation of the fODF and the neurite density\nin a unified mathematical model providing efficient, robust and accurate\nresults. Results were validated on simulated data and tested on\n\\textit{in-vivo} data of human brain, and compared to and Constrained Spherical\nDeconvolution (CSD) for benchmarking. Results revealed competitive performance\nin all respects and inherent adaptivity to local microstructure, while sensibly\nreducing the computational cost. We also investigated NODDI-SH performance when\nonly a limited number of samples are available for the fitting, demonstrating\nthat 60 samples are enough to obtain reliable results. The fast computational\ntime and the low number of signal samples required, make NODDI-SH feasible for\nclinical application.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:15:14 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 09:43:22 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zucchelli", "Mauro", ""], ["Descoteaux", "Maxime", ""], ["Menegaz", "Gloria", ""]]}, {"id": "1708.09000", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang, Sohae Chung, Xiuyuan Wang, Els Fieremans,\n  Steven Flanagan, Joseph Rath, Yvonne W. Lui", "title": "A Machine Learning Approach For Identifying Patients with Mild Traumatic\n  Brain Injury Using Diffusion MRI Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While diffusion MRI has been extremely promising in the study of MTBI,\nidentifying patients with recent MTBI remains a challenge. The literature is\nmixed with regard to localizing injury in these patients, however, gray matter\nsuch as the thalamus and white matter including the corpus callosum and frontal\ndeep white matter have been repeatedly implicated as areas at high risk for\ninjury. The purpose of this study is to develop a machine learning framework to\nclassify MTBI patients and controls using features derived from multi-shell\ndiffusion MRI in the thalamus, frontal white matter and corpus callosum.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 20:14:28 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""], ["Chung", "Sohae", ""], ["Wang", "Xiuyuan", ""], ["Fieremans", "Els", ""], ["Flanagan", "Steven", ""], ["Rath", "Joseph", ""], ["Lui", "Yvonne W.", ""]]}, {"id": "1708.09006", "submitter": "Daniel Crispell", "authors": "Daniel Crispell and Maxim Bazik", "title": "Pix2face: Direct 3D Face Model Estimation", "comments": "To appear in 2017 ICCV \"300 3D Facial-Videos in-the-Wild Challenge\"\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient, fully automatic method for 3D face shape and pose estimation in\nunconstrained 2D imagery is presented. The proposed method jointly estimates a\ndense set of 3D landmarks and facial geometry using a single pass of a modified\nversion of the popular \"U-Net\" neural network architecture. Additionally, we\npropose a method for directly estimating a set of 3D Morphable Model (3DMM)\nparameters, using the estimated 3D landmarks and geometry as constraints in a\nsimple linear system. Qualitative modeling results are presented, as well as\nquantitative evaluation of predicted 3D face landmarks in unconstrained video\nsequences.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:13:00 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Crispell", "Daniel", ""], ["Bazik", "Maxim", ""]]}, {"id": "1708.09011", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Thanh-Toan Do, Darwin G. Caldwell, and Nikos G. Tsagarakis", "title": "Real-Time 6DOF Pose Relocalization for Event Cameras with Stacked\n  Spatial LSTM Networks", "comments": "7 pages, 5 figures", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to relocalize the 6DOF pose of an event camera solely\nbased on the event stream. Our method first creates the event image from a list\nof events that occurs in a very short time interval, then a Stacked Spatial\nLSTM Network (SP-LSTM) is used to learn the camera pose. Our SP-LSTM is\ncomposed of a CNN to learn deep features from the event images and a stack of\nLSTM to learn spatial dependencies in the image feature space. We show that the\nspatial dependency plays an important role in the relocalization task and the\nSP-LSTM can effectively learn this information. The experimental results on a\npublicly available dataset show that our approach generalizes well and\noutperforms recent methods by a substantial margin. Overall, our proposed\nmethod reduces by approx. 6 times the position error and 3 times the\norientation error compared to the current state of the art. The source code and\ntrained models will be released.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 18:59:22 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 13:55:15 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2018 03:15:12 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Nguyen", "Anh", ""], ["Do", "Thanh-Toan", ""], ["Caldwell", "Darwin G.", ""], ["Tsagarakis", "Nikos G.", ""]]}, {"id": "1708.09038", "submitter": "Brendt Wohlberg", "authors": "Brendt Wohlberg", "title": "Convolutional Sparse Coding with Overlapping Group Norms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used form of convolutional sparse coding uses an $\\ell_1$\nregularization term. While this approach has been successful in a variety of\napplications, a limitation of the $\\ell_1$ penalty is that it is homogeneous\nacross the spatial and filter index dimensions of the sparse representation\narray, so that sparsity cannot be separately controlled across these\ndimensions. The present paper considers the consequences of replacing the\n$\\ell_1$ penalty with a mixed group norm, motivated by recent theoretical\nresults for convolutional sparse representations. Algorithms are developed for\nsolving the resulting problems, which are quite challenging, and the impact on\nthe performance of the denoising problem is evaluated. The mixed group norms\nare found to perform very poorly in this application. While their performance\nis greatly improved by introducing a weighting strategy, such a strategy also\nimproves the performance obtained from the much simpler and computationally\ncheaper $\\ell_1$ norm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 21:37:15 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Wohlberg", "Brendt", ""]]}, {"id": "1708.09066", "submitter": "Fred Moolekamp", "authors": "Fred Moolekamp and Peter Melchior", "title": "Block-Simultaneous Direction Method of Multipliers: A proximal\n  primal-dual splitting algorithm for nonconvex problems with multiple\n  constraints", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": "10.1007/s11081-018-9380-y", "report-no": null, "categories": "math.OC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generalization of the linearized Alternating Direction Method\nof Multipliers to optimize a real-valued function $f$ of multiple arguments\nwith potentially multiple constraints $g_\\circ$ on each of them. The function\n$f$ may be nonconvex as long as it is convex in every argument, while the\nconstraints $g_\\circ$ need to be convex but not smooth. If $f$ is smooth, the\nproposed Block-Simultaneous Direction Method of Multipliers (bSDMM) can be\ninterpreted as a proximal analog to inexact coordinate descent methods under\nconstraints. Unlike alternative approaches for joint solvers of\nmultiple-constraint problems, we do not require linear operators $L$ of a\nconstraint function $g(L\\ \\cdot)$ to be invertible or linked between each\nother. bSDMM is well-suited for a range of optimization problems, in particular\nfor data analysis, where $f$ is the likelihood function of a model and $L$\ncould be a transformation matrix describing e.g. finite differences or basis\ntransforms. We apply bSDMM to the Non-negative Matrix Factorization task of a\nhyperspectral unmixing problem and demonstrate convergence and effectiveness of\nmultiple constraints on both matrix factors. The algorithms are implemented in\npython and released as an open-source package.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 00:15:50 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Moolekamp", "Fred", ""], ["Melchior", "Peter", ""]]}, {"id": "1708.09072", "submitter": "Toby Lightheart", "authors": "Toby Lightheart, Steven Grainger, Tien-Fu Lu", "title": "Continual One-Shot Learning of Hidden Spike-Patterns with Neural Network\n  Simulation Expansion and STDP Convergence Predictions", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a constructive algorithm that achieves successful\none-shot learning of hidden spike-patterns in a competitive detection task. It\nhas previously been shown (Masquelier et al., 2008) that spike-timing-dependent\nplasticity (STDP) and lateral inhibition can result in neurons competitively\ntuned to repeating spike-patterns concealed in high rates of overall\npresynaptic activity. One-shot construction of neurons with synapse weights\ncalculated as estimates of converged STDP outcomes results in immediate\nselective detection of hidden spike-patterns. The capability of continual\nlearning is demonstrated through the successful one-shot detection of new sets\nof spike-patterns introduced after long intervals in the simulation time.\nSimulation expansion (Lightheart et al., 2013) has been proposed as an approach\nto the development of constructive algorithms that are compatible with\nsimulations of biological neural networks. A simulation of a biological neural\nnetwork may have orders of magnitude fewer neurons and connections than the\nrelated biological neural systems; therefore, simulated neural networks can be\nassumed to be a subset of a larger neural system. The constructive algorithm is\ndeveloped using simulation expansion concepts to perform an operation\nequivalent to the exchange of neurons between the simulation and the larger\nhypothetical neural system. The dynamic selection of neurons to simulate within\na larger neural system (hypothetical or stored in memory) may be a starting\npoint for a wide range of developments and applications in machine learning and\nthe simulation of biology.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 01:07:18 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Lightheart", "Toby", ""], ["Grainger", "Steven", ""], ["Lu", "Tien-Fu", ""]]}, {"id": "1708.09083", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos, Michalis Vrigkas, Ioannis A. Kakadiaris", "title": "Adaptive SVM+: Learning with Privileged Information for Domain\n  Adaptation", "comments": "To appear in ICCV Workshops 2017 (TASK-CV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating additional knowledge in the learning process can be beneficial\nfor several computer vision and machine learning tasks. Whether privileged\ninformation originates from a source domain that is adapted to a target domain,\nor as additional features available at training time only, using such\nprivileged (i.e., auxiliary) information is of high importance as it improves\nthe recognition performance and generalization. However, both primary and\nprivileged information are rarely derived from the same distribution, which\nposes an additional challenge to the recognition task. To address these\nchallenges, we present a novel learning paradigm that leverages privileged\ninformation in a domain adaptation setup to perform visual recognition tasks.\nThe proposed framework, named Adaptive SVM+, combines the advantages of both\nthe learning using privileged information (LUPI) paradigm and the domain\nadaptation framework, which are naturally embedded in the objective function of\na regular SVM. We demonstrate the effectiveness of our approach on the publicly\navailable Animals with Attributes and INTERACT datasets and report\nstate-of-the-art results in both of them.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 01:57:16 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Vrigkas", "Michalis", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1708.09086", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Fred Hohman, Bistra Dilkina", "title": "A Deep Learning Approach for Population Estimation from Satellite\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing where people live is a fundamental component of many decision making\nprocesses such as urban development, infectious disease containment, evacuation\nplanning, risk management, conservation planning, and more. While bottom-up,\nsurvey driven censuses can provide a comprehensive view into the population\nlandscape of a country, they are expensive to realize, are infrequently\nperformed, and only provide population counts over broad areas. Population\ndisaggregation techniques and population projection methods individually\naddress these shortcomings, but also have shortcomings of their own. To jointly\nanswer the questions of \"where do people live\" and \"how many people live\nthere,\" we propose a deep learning model for creating high-resolution\npopulation estimations from satellite imagery. Specifically, we train\nconvolutional neural networks to predict population in the USA at a\n$0.01^{\\circ} \\times 0.01^{\\circ}$ resolution grid from 1-year composite\nLandsat imagery. We validate these models in two ways: quantitatively, by\ncomparing our model's grid cell estimates aggregated at a county-level to\nseveral US Census county-level population projections, and qualitatively, by\ndirectly interpreting the model's predictions in terms of the satellite image\ninputs. We find that aggregating our model's estimates gives comparable results\nto the Census county-level population projections and that the predictions made\nby our model can be directly interpreted, which give it advantages over\ntraditional population disaggregation methods. In general, our model is an\nexample of how machine learning techniques can be an effective tool for\nextracting information from inherently unstructured, remotely sensed data to\nprovide effective solutions to social problems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 02:05:16 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Robinson", "Caleb", ""], ["Hohman", "Fred", ""], ["Dilkina", "Bistra", ""]]}, {"id": "1708.09105", "submitter": "Lijun Zhao", "authors": "Lijun Zhao, Huihui Bai, Jie Liang, Bing Zeng, Anhong Wang, Yao Zhao", "title": "Simultaneously Color-Depth Super-Resolution with Conditional Generative\n  Adversarial Network", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Generative Adversarial Network (GAN) has been found wide\napplications in style transfer, image-to-image translation and image\nsuper-resolution. In this paper, a color-depth conditional GAN is proposed to\nconcurrently resolve the problems of depth super-resolution and color\nsuper-resolution in 3D videos. Firstly, given the low-resolution depth image\nand low-resolution color image, a generative network is proposed to leverage\nmutual information of color image and depth image to enhance each other in\nconsideration of the geometry structural dependency of color-depth image in the\nsame scene. Secondly, three loss functions, including data loss, total\nvariation loss, and 8-connected gradient difference loss are introduced to\ntrain this generative network in order to keep generated images close to the\nreal ones, in addition to the adversarial loss. Experimental results\ndemonstrate that the proposed approach produces high-quality color image and\ndepth image from low-quality image pair, and it is superior to several other\nleading methods. Besides, we use the same neural network framework to resolve\nthe problem of image smoothing and edge detection at the same time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 04:17:16 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 14:51:24 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 09:46:59 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Zhao", "Lijun", ""], ["Bai", "Huihui", ""], ["Liang", "Jie", ""], ["Zeng", "Bing", ""], ["Wang", "Anhong", ""], ["Zhao", "Yao", ""]]}, {"id": "1708.09126", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou and Bertram Emil Shi", "title": "Photorealistic Facial Expression Synthesis by the Conditional Difference\n  Adversarial Autoencoder", "comments": "Accepted by ACII2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic facial expression synthesis from single face image can be\nwidely applied to face recognition, data augmentation for emotion recognition\nor entertainment. This problem is challenging, in part due to a paucity of\nlabeled facial expression data, making it difficult for algorithms to\ndisambiguate changes due to identity and changes due to expression. In this\npaper, we propose the conditional difference adversarial autoencoder, CDAAE,\nfor facial expression synthesis. The CDAAE takes a facial image of a previously\nunseen person and generates an image of that human face with a target emotion\nor facial action unit label. The CDAAE adds a feedforward path to an\nautoencoder structure connecting low level features at the encoder to features\nat the corresponding level at the decoder. It handles the problem of\ndisambiguating changes due to identity and changes due to facial expression by\nlearning to generate the difference between low-level features of images of the\nsame person but with different facial expressions. The CDAAE structure can be\nused to generate novel expressions by combining and interpolating between\nfacial expressions/action units within the training set. Our experimental\nresults demonstrate that the CDAAE can preserve identity information when\ngenerating facial expression for unseen subjects more faithfully than previous\napproaches. This is especially advantageous when training with small databases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 05:37:58 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Zhou", "Yuqian", ""], ["Shi", "Bertram Emil", ""]]}, {"id": "1708.09182", "submitter": "Srenivas Varadarajan", "authors": "Srenivas Varadarajan, Parual Datta and Omesh Tickoo", "title": "A Greedy Part Assignment Algorithm for Real-time Multi-person 2D Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human pose-estimation in a multi-person image involves detection of various\nbody parts and grouping them into individual person clusters. While the former\ntask is challenging due to mutual occlusions, the combinatorial complexity of\nthe latter task is very high. We propose a greedy part assignment algorithm\nthat exploits the inherent structure of the human body to achieve a lower\ncomplexity, compared to any of the prior published works. This is accomplished\nby (i) reducing the number of part-candidates using the estimated number of\npeople in the image, (ii) doing a greedy sequential assignment of part-classes,\nfollowing the kinematic chain from head to ankle (iii) doing a greedy\nassignment of parts in each part-class set, to person-clusters (iv) limiting\nthe candidate person clusters to the most proximal clusters using human\nanthropometric data and (v) using only a specific subset of pre-assigned parts\nfor establishing pairwise structural constraints. We show that, these steps\nresult in a sparse body parts relationship graph and reduces the complexity. We\nalso propose methods for improving the accuracy of pose-estimation by (i)\nspawning person-clusters from any unassigned significant body part and (ii)\nsuppressing hallucinated parts. On the MPII multi-person pose database,\npose-estimation using the proposed method takes only 0.14 seconds per image. We\nshow that, our proposed algorithm, by using a large spatial and structural\ncontext, achieves the state-of-the-art accuracy on both MPII and WAF\nmulti-person pose datasets, demonstrating the robustness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 09:13:38 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Varadarajan", "Srenivas", ""], ["Datta", "Parual", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1708.09200", "submitter": "HaiLiang Li", "authors": "Hailiang Li, Kin-Man Lam, Dong Li", "title": "Joint Maximum Purity Forest with Application to Image Super-Resolution", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel random-forest scheme, namely Joint Maximum\nPurity Forest (JMPF), for classification, clustering, and regression tasks. In\nthe JMPF scheme, the original feature space is transformed into a compactly\npre-clustered feature space, via a trained rotation matrix. The rotation matrix\nis obtained through an iterative quantization process, where the input data\nbelonging to different classes are clustered to the respective vertices of the\nnew feature space with maximum purity. In the new feature space, orthogonal\nhyperplanes, which are employed at the split-nodes of decision trees in random\nforests, can tackle the clustering problems effectively. We evaluated our\nproposed method on public benchmark datasets for regression and classification\ntasks, and experiments showed that JMPF remarkably outperforms other\nstate-of-the-art random-forest-based approaches. Furthermore, we applied JMPF\nto image super-resolution, because the transformed, compact features are more\ndiscriminative to the clustering-regression scheme. Experiment results on\nseveral public benchmark datasets also showed that the JMPF-based image\nsuper-resolution scheme is consistently superior to recent state-of-the-art\nimage super-resolution algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 10:00:11 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Li", "Hailiang", ""], ["Lam", "Kin-Man", ""], ["Li", "Dong", ""]]}, {"id": "1708.09204", "submitter": "Jiahao Pang", "authors": "Jiahao Pang, Wenxiu Sun, Jimmy SJ. Ren, Chengxi Yang, Qiong Yan", "title": "Cascade Residual Learning: A Two-stage Convolutional Neural Network for\n  Stereo Matching", "comments": "Accepted at ICCVW 2017. The first two authors contributed equally to\n  this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging on the recent developments in convolutional neural networks\n(CNNs), matching dense correspondence from a stereo pair has been cast as a\nlearning problem, with performance exceeding traditional approaches. However,\nit remains challenging to generate high-quality disparities for the inherently\nill-posed regions. To tackle this problem, we propose a novel cascade CNN\narchitecture composing of two stages. The first stage advances the recently\nproposed DispNet by equipping it with extra up-convolution modules, leading to\ndisparity images with more details. The second stage explicitly rectifies the\ndisparity initialized by the first stage; it couples with the first-stage and\ngenerates residual signals across multiple scales. The summation of the outputs\nfrom the two stages gives the final disparity. As opposed to directly learning\nthe disparity at the second stage, we show that residual learning provides more\neffective refinement. Moreover, it also benefits the training of the overall\ncascade network. Experimentation shows that our cascade residual learning\nscheme provides state-of-the-art performance for matching stereo\ncorrespondence. By the time of the submission of this paper, our method ranks\nfirst in the KITTI 2015 stereo benchmark, surpassing the prior works by a\nnoteworthy margin.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 10:20:37 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 17:01:28 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Pang", "Jiahao", ""], ["Sun", "Wenxiu", ""], ["Ren", "Jimmy SJ.", ""], ["Yang", "Chengxi", ""], ["Yan", "Qiong", ""]]}, {"id": "1708.09212", "submitter": "Amarjot Singh", "authors": "Amarjot Singh and Nick Kingsbury", "title": "ScatterNet Hybrid Deep Learning (SHDL) Network For Object Classification", "comments": "To Appear in the 27th IEEE International Workshop on Machine Learning\n  For Signal Processing (MLSP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes the ScatterNet Hybrid Deep Learning (SHDL) network that\nextracts invariant and discriminative image representations for object\nrecognition. SHDL framework is constructed with a multi-layer ScatterNet\nfront-end, an unsupervised learning middle, and a supervised learning back-end\nmodule. Each layer of the SHDL network is automatically designed as an explicit\noptimization problem leading to an optimal deep learning architecture with\nimproved computational performance as compared to the more usual deep network\narchitectures. SHDL network produces the state-of-the-art classification\nperformance against unsupervised and semi-supervised learning (GANs) on two\nimage datasets. Advantages of the SHDL network over supervised methods (NIN,\nVGG) are also demonstrated with experiments performed on training datasets of\nreduced size.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 11:02:20 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Singh", "Amarjot", ""], ["Kingsbury", "Nick", ""]]}, {"id": "1708.09254", "submitter": "Hojjat Salehinejad", "authors": "Hojjat Salehinejad, Shahrokh Valaee, Aren Mnatzakanian, Tim Dowdell,\n  Joseph Barfett, Errol Colak", "title": "Interpretation of Mammogram and Chest X-Ray Reports Using Deep Neural\n  Networks - Preliminary Results", "comments": "This paper is submitted for peer-review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiology reports are an important means of communication between\nradiologists and other physicians. These reports express a radiologist's\ninterpretation of a medical imaging examination and are critical in\nestablishing a diagnosis and formulating a treatment plan. In this paper, we\npropose a Bi-directional convolutional neural network (Bi-CNN) model for the\ninterpretation and classification of mammograms based on breast density and\nchest radiographic radiology reports based on the basis of chest pathology. The\nproposed approach helps to organize databases of radiology reports, retrieve\nthem expeditiously, and evaluate the radiology report that could be used in an\nauditing system to decrease incorrect diagnoses. Our study revealed that the\nproposed Bi-CNN outperforms the random forest and the support vector machine\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 12:21:22 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 16:59:15 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 22:41:59 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Salehinejad", "Hojjat", ""], ["Valaee", "Shahrokh", ""], ["Mnatzakanian", "Aren", ""], ["Dowdell", "Tim", ""], ["Barfett", "Joseph", ""], ["Colak", "Errol", ""]]}, {"id": "1708.09268", "submitter": "An Tran", "authors": "An Tran, Loong-Fah Cheong", "title": "Two-stream Flow-guided Convolutional Attention Networks for Action\n  Recognition", "comments": "To appear in International Conference of Computer Vision Workshop\n  (ICCVW), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a two-stream flow-guided convolutional attention networks\nfor action recognition in videos. The central idea is that optical flows, when\nproperly compensated for the camera motion, can be used to guide attention to\nthe human foreground. We thus develop cross-link layers from the temporal\nnetwork (trained on flows) to the spatial network (trained on RGB frames).\nThese cross-link layers guide the spatial-stream to pay more attention to the\nhuman foreground areas and be less affected by background clutter. We obtain\npromising performances with our approach on the UCF101, HMDB51 and Hollywood2\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 13:43:32 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Tran", "An", ""], ["Cheong", "Loong-Fah", ""]]}, {"id": "1708.09300", "submitter": "Amarjot Singh", "authors": "Amarjot Singh, Devamanyu Hazarika, Aniruddha Bhattacharya", "title": "Texture and Structure Incorporated ScatterNet Hybrid Deep Learning\n  Network (TS-SHDL) For Brain Matter Segmentation", "comments": "To Appear in the IEEE International Conference on Computer Vision\n  Workshops (ICCVW) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of brain matter segmentation from MR images is a challenging task\ndue to the irregular boundaries between the grey and white matter regions. In\naddition, the presence of intensity inhomogeneity in the MR images further\ncomplicates the problem. In this paper, we propose a texture and vesselness\nincorporated version of the ScatterNet Hybrid Deep Learning Network (TS-SHDL)\nthat extracts hierarchical invariant mid-level features, used by fisher vector\nencoding and a conditional random field (CRF) to perform the desired\nsegmentation. The performance of the proposed network is evaluated by extensive\nexperimentation and comparison with the state-of-the-art methods on several 2D\nMRI scans taken from the synthetic McGill Brain Web as well as on the MRBrainS\ndataset of real 3D MRI scans. The advantages of the TS-SHDL network over\nsupervised deep learning networks is also presented in addition to its superior\nperformance over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 14:38:06 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Singh", "Amarjot", ""], ["Hazarika", "Devamanyu", ""], ["Bhattacharya", "Aniruddha", ""]]}, {"id": "1708.09317", "submitter": "Amarjot Singh", "authors": "Amarjot Singh, Devendra Patil, G Meghana Reddy, SN Omkar", "title": "Disguised Face Identification (DFI) with Facial KeyPoints using Spatial\n  Fusion Convolutional Network", "comments": "To Appear in the IEEE International Conference on Computer Vision\n  Workshops (ICCVW) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disguised face identification (DFI) is an extremely challenging problem due\nto the numerous variations that can be introduced using different disguises.\nThis paper introduces a deep learning framework to first detect 14 facial\nkey-points which are then utilized to perform disguised face identification.\nSince the training of deep learning architectures relies on large annotated\ndatasets, two annotated facial key-points datasets are introduced. The\neffectiveness of the facial keypoint detection framework is presented for each\nkeypoint. The superiority of the key-point detection framework is also\ndemonstrated by a comparison with other deep networks. The effectiveness of\nclassification performance is also demonstrated by comparison with the\nstate-of-the-art face disguise classification methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 15:11:06 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Singh", "Amarjot", ""], ["Patil", "Devendra", ""], ["Reddy", "G Meghana", ""], ["Omkar", "SN", ""]]}, {"id": "1708.09321", "submitter": "Miriam Cha", "authors": "Miriam Cha, Youngjune Gwon, H. T. Kung", "title": "Adversarial nets with perceptual losses for text-to-image synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches in generative adversarial networks (GANs) can automatically\nsynthesize realistic images from descriptive text. Despite the overall fair\nquality, the generated images often expose visible flaws that lack structural\ndefinition for an object of interest. In this paper, we aim to extend state of\nthe art for GAN-based text-to-image synthesis by improving perceptual quality\nof generated images. Differentiated from previous work, our synthetic image\ngenerator optimizes on perceptual loss functions that measure pixel, feature\nactivation, and texture differences against a natural image. We present\nvisually more compelling synthetic images of birds and flowers generated from\ntext descriptions in comparison to some of the most prominent existing work.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 15:21:11 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Cha", "Miriam", ""], ["Gwon", "Youngjune", ""], ["Kung", "H. T.", ""]]}, {"id": "1708.09427", "submitter": "Li Shen", "authors": "Li Shen, Laurie R. Margolies, Joseph H. Rothstein, Eugene Fluder,\n  Russell B. McBride, Weiva Sieh", "title": "Deep Learning to Improve Breast Cancer Early Detection on Screening\n  Mammography", "comments": "Major modification with an additional figure and new results", "journal-ref": "Scientific Reports, volume 9, Article number: 12495 (2019)", "doi": "10.1038/s41598-019-48995-4", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of deep learning, a family of machine learning\ntechniques, has spurred much interest in its application to medical imaging\nproblems. Here, we develop a deep learning algorithm that can accurately detect\nbreast cancer on screening mammograms using an \"end-to-end\" training approach\nthat efficiently leverages training datasets with either complete clinical\nannotation or only the cancer status (label) of the whole image. In this\napproach, lesion annotations are required only in the initial training stage,\nand subsequent stages require only image-level labels, eliminating the reliance\non rarely available lesion annotations. Our all convolutional network method\nfor classifying screening mammograms attained excellent performance in\ncomparison with previous methods. On an independent test set of digitized film\nmammograms from Digital Database for Screening Mammography (DDSM), the best\nsingle model achieved a per-image AUC of 0.88, and four-model averaging\nimproved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a\nvalidation set of full-field digital mammography (FFDM) images from the\nINbreast database, the best single model achieved a per-image AUC of 0.95, and\nfour-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:\n96.1%). We also demonstrate that a whole image classifier trained using our\nend-to-end approach on the DDSM digitized film mammograms can be transferred to\nINbreast FFDM images using only a subset of the INbreast data for fine-tuning\nand without further reliance on the availability of lesion annotations. These\nfindings show that automatic deep learning methods can be readily trained to\nattain high accuracy on heterogeneous mammography platforms, and hold\ntremendous promise for improving clinical tools to reduce false positive and\nfalse negative screening mammography results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:46:16 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 14:40:06 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 02:03:40 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2018 16:10:10 GMT"}, {"version": "v5", "created": "Mon, 31 Dec 2018 23:23:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shen", "Li", ""], ["Margolies", "Laurie R.", ""], ["Rothstein", "Joseph H.", ""], ["Fluder", "Eugene", ""], ["McBride", "Russell B.", ""], ["Sieh", "Weiva", ""]]}, {"id": "1708.09461", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using\n  Adaptive Plant Propagation Algorithm", "comments": "12 Pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1708.07040", "journal-ref": null, "doi": "10.17605/OSF.IO/5KQZD", "report-no": null, "categories": "cs.CV math.OC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most straightforward, direct and efficient approaches to Image\nSegmentation is Image Thresholding. Multi-level Image Thresholding is an\nessential viewpoint in many image processing and Pattern Recognition based\nreal-time applications which can effectively and efficiently classify the\npixels into various groups denoting multiple regions in an Image. Thresholding\nbased Image Segmentation using fuzzy entropy combined with intelligent\noptimization approaches are commonly used direct methods to properly identify\nthe thresholds so that they can be used to segment an Image accurately. In this\npaper a novel approach for multi-level image thresholding is proposed using\nType II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).\nObtaining the optimal thresholds for an image by maximizing the entropy is\nextremely tedious and time consuming with increase in the number of thresholds.\nHence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based\non plant intelligence, is used for fast and efficient selection of optimal\nthresholds. This fact is reasonably justified by comparing the accuracy of the\noutcomes and computational time consumed by other modern state-of-the-art\nalgorithms such as Particle Swarm Optimization (PSO), Gravitational Search\nAlgorithm (GSA) and Genetic Algorithm (GA).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 09:43:00 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1708.09485", "submitter": "Suhas Lohit", "authors": "Suhas Lohit and Pavan Turaga", "title": "Learning Invariant Riemannian Geometric Representations Using Deep Nets", "comments": "Accepted at International Conference on Computer Vision Workshop\n  (ICCVW), 2017 on Manifold Learning: from Euclid to Riemann", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Euclidean constraints are inherent in many kinds of data in computer\nvision and machine learning, typically as a result of specific invariance\nrequirements that need to be respected during high-level inference. Often,\nthese geometric constraints can be expressed in the language of Riemannian\ngeometry, where conventional vector space machine learning does not apply\ndirectly. The central question this paper deals with is: How does one train\ndeep neural nets whose final outputs are elements on a Riemannian manifold? To\nanswer this, we propose a general framework for manifold-aware training of deep\nneural networks -- we utilize tangent spaces and exponential maps in order to\nconvert the proposed problem into a form that allows us to bring current\nadvances in deep learning to bear upon this problem. We describe two specific\napplications to demonstrate this approach: prediction of probability\ndistributions for multi-class image classification, and prediction of\nillumination-invariant subspaces from a single face-image via regression on the\nGrassmannian. These applications show the generality of the proposed framework,\nand result in improved performance over baselines that ignore the geometry of\nthe output space. In addition to solving this specific problem, we believe this\npaper opens new lines of enquiry centered on the implications of Riemannian\ngeometry on deep architectures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 21:57:17 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 23:16:22 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Lohit", "Suhas", ""], ["Turaga", "Pavan", ""]]}, {"id": "1708.09522", "submitter": "Atousa Torabi", "authors": "Atousa Torabi and Leonid Sigal", "title": "Action Classification and Highlighting in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent advances in neural machine translation, that jointly align\nand translate using encoder-decoder networks equipped with attention, we\npropose an attentionbased LSTM model for human activity recognition. Our model\njointly learns to classify actions and highlight frames associated with the\naction, by attending to salient visual information through a jointly learned\nsoft-attention networks. We explore attention informed by various forms of\nvisual semantic features, including those encoding actions, objects and scenes.\nWe qualitatively show that soft-attention can learn to effectively attend to\nimportant objects and scene information correlated with specific human actions.\nFurther, we show that, quantitatively, our attention-based LSTM outperforms the\nvanilla LSTM and CNN models used by stateof-the-art methods. On a large-scale\nyoutube video dataset, ActivityNet, our model outperforms competing methods in\naction classification.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 01:19:57 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Torabi", "Atousa", ""], ["Sigal", "Leonid", ""]]}, {"id": "1708.09533", "submitter": "Chee Seng Chan", "authors": "Wei Ren Tan, Chee Seng Chan, Hernan Aguirre, Kiyoshi Tanaka", "title": "Improved ArtGAN for Conditional Synthesis of Natural Image and Artwork", "comments": "16 pages, 11 figures, accepted version at IEEE Transactions on Image\n  Processing (T-IP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a series of new approaches to improve Generative\nAdversarial Network (GAN) for conditional image synthesis and we name the\nproposed model as ArtGAN. One of the key innovation of ArtGAN is that, the\ngradient of the loss function w.r.t. the label (randomly assigned to each\ngenerated image) is back-propagated from the categorical discriminator to the\ngenerator. With the feedback from the label information, the generator is able\nto learn more efficiently and generate image with better quality. Inspired by\nrecent works, an autoencoder is incorporated into the categorical discriminator\nfor additional complementary information. Last but not least, we introduce a\nnovel strategy to improve the image quality. In the experiments, we evaluate\nArtGAN on CIFAR-10 and STL-10 via ablation studies. The empirical results\nshowed that our proposed model outperforms the state-of-the-art results on\nCIFAR-10 in terms of Inception score. Qualitatively, we demonstrate that ArtGAN\nis able to generate plausible-looking images on Oxford-102 and CUB-200, as well\nas able to draw realistic artworks based on style, artist, and genre. The\nsource code and models are available at: https://github.com/cs-chan/ArtGAN\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 02:13:57 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 15:10:59 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Tan", "Wei Ren", ""], ["Chan", "Chee Seng", ""], ["Aguirre", "Hernan", ""], ["Tanaka", "Kiyoshi", ""]]}, {"id": "1708.09545", "submitter": "Zhong Ji", "authors": "Zhong Ji, Kailin Xiong, Yanwei Pang, Xuelong Li", "title": "Video Summarization with Attention-Based Encoder-Decoder Networks", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of supervised video summarization by\nformulating it as a sequence-to-sequence learning problem, where the input is a\nsequence of original video frames, the output is a keyshot sequence. Our key\nidea is to learn a deep summarization network with attention mechanism to mimic\nthe way of selecting the keyshots of human. To this end, we propose a novel\nvideo summarization framework named Attentive encoder-decoder networks for\nVideo Summarization (AVS), in which the encoder uses a Bidirectional Long\nShort-Term Memory (BiLSTM) to encode the contextual information among the input\nvideo frames. As for the decoder, two attention-based LSTM networks are\nexplored by using additive and multiplicative objective functions,\nrespectively. Extensive experiments are conducted on three video summarization\nbenchmark datasets, i.e., SumMe, and TVSum. The results demonstrate the\nsuperiority of the proposed AVS-based approaches against the state-of-the-art\napproaches,with remarkable improvements from 0.8% to 3% on two\ndatasets,respectively..\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 03:04:17 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 00:30:42 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Ji", "Zhong", ""], ["Xiong", "Kailin", ""], ["Pang", "Yanwei", ""], ["Li", "Xuelong", ""]]}, {"id": "1708.09580", "submitter": "Gee-Sern Hsu", "authors": "Gee-Sern (Jison) Hsu, Hung-Cheng Shie, Cheng-Hua Hsieh", "title": "Fast Landmark Localization with 3D Component Reconstruction and CNN for\n  Cross-Pose Recognition", "comments": "14 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two approaches are proposed for cross-pose face recognition, one is based on\nthe 3D reconstruction of facial components and the other is based on the deep\nConvolutional Neural Network (CNN). Unlike most 3D approaches that consider\nholistic faces, the proposed approach considers 3D facial components. It\nsegments a 2D gallery face into components, reconstructs the 3D surface for\neach component, and recognizes a probe face by component features. The\nsegmentation is based on the landmarks located by a hierarchical algorithm that\ncombines the Faster R-CNN for face detection and the Reduced Tree Structured\nModel for landmark localization. The core part of the CNN-based approach is a\nrevised VGG network. We study the performances with different settings on the\ntraining set, including the synthesized data from 3D reconstruction, the\nreal-life data from an in-the-wild database, and both types of data combined.\nWe investigate the performances of the network when it is employed as a\nclassifier or designed as a feature extractor. The two recognition approaches\nand the fast landmark localization are evaluated in extensive experiments, and\ncompared to stateof-the-art methods to demonstrate their efficacy.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 06:26:42 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Gee-Sern", "", "", "Jison"], ["Hsu", "", ""], ["Shie", "Hung-Cheng", ""], ["Hsieh", "Cheng-Hua", ""]]}, {"id": "1708.09585", "submitter": "Baoguang Shi", "authors": "Baoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan\n  Cui, Serge Belongie, Shijian Lu, Xiang Bai", "title": "ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chinese is the most widely used language in the world. Algorithms that read\nChinese text in natural images facilitate applications of various kinds.\nDespite the large potential value, datasets and competitions in the past\nprimarily focus on English, which bares very different characteristics than\nChinese. This report introduces RCTW, a new competition that focuses on Chinese\ntext reading. The competition features a large-scale dataset with 12,263\nannotated images. Two tasks, namely text localization and end-to-end\nrecognition, are set up. The competition took place from January 20 to May 31,\n2017. 23 valid submissions were received from 19 teams. This report includes\ndataset description, task definitions, evaluation protocols, and results\nsummaries and analysis. Through this competition, we call for more future\nresearch on the Chinese text reading problem. The official website for the\ncompetition is http://rctw.vlrlab.net\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 06:49:06 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 13:15:52 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 16:36:20 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Shi", "Baoguang", ""], ["Yao", "Cong", ""], ["Liao", "Minghui", ""], ["Yang", "Mingkun", ""], ["Xu", "Pei", ""], ["Cui", "Linyan", ""], ["Belongie", "Serge", ""], ["Lu", "Shijian", ""], ["Bai", "Xiang", ""]]}, {"id": "1708.09633", "submitter": "Mahdi Rad", "authors": "Mahdi Rad, Peter M. Roth, Vincent Lepetit", "title": "ALCN: Meta-Learning for Contrast Normalization Applied to Robust 3D Pose\n  Estimation", "comments": "BMVC' 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be robust to illumination changes when detecting objects in images, the\ncurrent trend is to train a Deep Network with training images captured under\nmany different lighting conditions. Unfortunately, creating such a training set\nis very cumbersome, or sometimes even impossible, for some applications such as\n3D pose estimation of specific objects, which is the application we focus on in\nthis paper. We therefore propose a novel illumination normalization method that\nlets us learn to detect objects and estimate their 3D pose under challenging\nillumination conditions from very few training samples. Our key insight is that\nnormalization parameters should adapt to the input image. In particular, we\nrealized this via a Convolutional Neural Network trained to predict the\nparameters of a generalization of the Difference-of-Gaussians method. We show\nthat our method significantly outperforms standard normalization methods and\ndemonstrate it on two challenging 3D detection and pose estimation problems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 09:28:23 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Rad", "Mahdi", ""], ["Roth", "Peter M.", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1708.09641", "submitter": "Huihuang Zhao", "authors": "Huihuang Zhao, Paul L. Rosin, Yu-Kun Lai", "title": "Automatic Semantic Style Transfer using Deep Convolutional Neural\n  Networks and Soft Masks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an automatic image synthesis method to transfer the style\nof an example image to a content image. When standard neural style transfer\napproaches are used, the textures and colours in different semantic regions of\nthe style image are often applied inappropriately to the content image,\nignoring its semantic layout, and ruining the transfer result. In order to\nreduce or avoid such effects, we propose a novel method based on automatically\nsegmenting the objects and extracting their soft semantic masks from the style\nand content images, in order to preserve the structure of the content image\nwhile having the style transferred. Each soft mask of the style image\nrepresents a specific part of the style image, corresponding to the soft mask\nof the content image with the same semantics. Both the soft masks and source\nimages are provided as multichannel input to an augmented deep CNN framework\nfor style transfer which incorporates a generative Markov random field (MRF)\nmodel. Results on various images show that our method outperforms the most\nrecent techniques.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 09:48:00 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Zhao", "Huihuang", ""], ["Rosin", "Paul L.", ""], ["Lai", "Yu-Kun", ""]]}, {"id": "1708.09642", "submitter": "Guanqun Cao Mr", "authors": "Guanqun Cao, Alexandros Iosifidis, Moncef Gabbouj", "title": "Neural Class-Specific Regression for face verification", "comments": "9 pages, 4 figures", "journal-ref": "IET Biometrics, vol. 7, no. 1, pp. 63-70, 2018", "doi": "10.1049/iet-bmt.2017.0081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face verification is a problem approached in the literature mainly using\nnonlinear class-specific subspace learning techniques. While it has been shown\nthat kernel-based Class-Specific Discriminant Analysis is able to provide\nexcellent performance in small- and medium-scale face verification problems,\nits application in today's large-scale problems is difficult due to its\ntraining space and computational requirements. In this paper, generalizing our\nprevious work on kernel-based class-specific discriminant analysis, we show\nthat class-specific subspace learning can be cast as a regression problem. This\nallows us to derive linear, (reduced) kernel and neural network-based\nclass-specific discriminant analysis methods using efficient batch and/or\niterative training schemes, suited for large-scale learning problems. We test\nthe performance of these methods in two datasets describing medium- and\nlarge-scale face verification problems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 09:48:37 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Cao", "Guanqun", ""], ["Iosifidis", "Alexandros", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "1708.09644", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro,\n  Carlo Regazzoni, Nicu Sebe", "title": "Abnormal Event Detection in Videos using Generative Adversarial Nets", "comments": "Best Paper / Student Paper Award Finalist, IEEE International\n  Conference on Image Processing (ICIP), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the abnormality detection problem in crowded scenes.\nWe propose to use Generative Adversarial Nets (GANs), which are trained using\nnormal frames and corresponding optical-flow images in order to learn an\ninternal representation of the scene normality. Since our GANs are trained with\nonly normal data, they are not able to generate abnormal events. At testing\ntime the real data are compared with both the appearance and the motion\nrepresentations reconstructed by our GANs and abnormal areas are detected by\ncomputing local differences. Experimental results on challenging abnormality\ndetection datasets show the superiority of the proposed method compared to the\nstate of the art in both frame-level and pixel-level abnormality detection\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 09:56:02 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Nabi", "Moin", ""], ["Sangineto", "Enver", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""], ["Sebe", "Nicu", ""]]}, {"id": "1708.09666", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Jia Chen, Qin Jin", "title": "Generating Video Descriptions with Topic Guidance", "comments": "Appeared at ICMR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating video descriptions in natural language (a.k.a. video captioning)\nis a more challenging task than image captioning as the videos are\nintrinsically more complicated than images in two aspects. First, videos cover\na broader range of topics, such as news, music, sports and so on. Second,\nmultiple topics could coexist in the same video. In this paper, we propose a\nnovel caption model, topic-guided model (TGM), to generate topic-oriented\ndescriptions for videos in the wild via exploiting topic information. In\naddition to predefined topics, i.e., category tags crawled from the web, we\nalso mine topics in a data-driven way based on training captions by an\nunsupervised topic mining model. We show that data-driven topics reflect a\nbetter topic schema than the predefined topics. As for testing video topic\nprediction, we treat the topic mining model as teacher to train the student,\nthe topic prediction model, by utilizing the full multi-modalities in the video\nespecially the speech modality. We propose a series of caption models to\nexploit topic guidance, including implicitly using the topics as input features\nto generate words related to the topic and explicitly modifying the weights in\nthe decoder with topics to function as an ensemble of topic-aware language\ndecoders. Our comprehensive experimental results on the current largest video\ncaption dataset MSR-VTT prove the effectiveness of our topic-guided model,\nwhich significantly surpasses the winning performance in the 2016 MSR video to\nlanguage challenge.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 11:17:53 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 11:38:38 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Chen", "Shizhe", ""], ["Chen", "Jia", ""], ["Jin", "Qin", ""]]}, {"id": "1708.09667", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Jia Chen, Qin Jin, Alexander Hauptmann", "title": "Video Captioning with Guidance of Multimodal Latent Topics", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topic diversity of open-domain videos leads to various vocabularies and\nlinguistic expressions in describing video contents, and therefore, makes the\nvideo captioning task even more challenging. In this paper, we propose an\nunified caption framework, M&M TGM, which mines multimodal topics in\nunsupervised fashion from data and guides the caption decoder with these\ntopics. Compared to pre-defined topics, the mined multimodal topics are more\nsemantically and visually coherent and can reflect the topic distribution of\nvideos better. We formulate the topic-aware caption generation as a multi-task\nlearning problem, in which we add a parallel task, topic prediction, in\naddition to the caption task. For the topic prediction task, we use the mined\ntopics as the teacher to train a student topic prediction model, which learns\nto predict the latent topics from multimodal contents of videos. The topic\nprediction provides intermediate supervision to the learning process. As for\nthe caption task, we propose a novel topic-aware decoder to generate more\naccurate and detailed video descriptions with the guidance from latent topics.\nThe entire learning procedure is end-to-end and it optimizes both tasks\nsimultaneously. The results from extensive experiments conducted on the MSR-VTT\nand Youtube2Text datasets demonstrate the effectiveness of our proposed model.\nM&M TGM not only outperforms prior state-of-the-art methods on multiple\nevaluation metrics and on both benchmark datasets, but also achieves better\ngeneralization ability.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 11:18:28 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 15:34:44 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Chen", "Shizhe", ""], ["Chen", "Jia", ""], ["Jin", "Qin", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1708.09684", "submitter": "Shounak Datta", "authors": "Shounak Datta, Sayak Nag and Swagatam Das", "title": "Boosting with Lexicographic Programming: Addressing Class Imbalance\n  without Cost Tuning", "comments": "This work has been submitted to the IEEE for publication. Copyright\n  may be transferred without notice, after which this version may no longer be\n  accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large amount of research effort has been dedicated to adapting boosting for\nimbalanced classification. However, boosting methods are yet to be\nsatisfactorily immune to class imbalance, especially for multi-class problems.\nThis is because most of the existing solutions for handling class imbalance\nrely on expensive cost set tuning for determining the proper level of\ncompensation. We show that the assignment of weights to the component\nclassifiers of a boosted ensemble can be thought of as a game of Tug of War\nbetween the classes in the margin space. We then demonstrate how this insight\ncan be used to attain a good compromise between the rare and abundant classes\nwithout having to resort to cost set tuning, which has long been the norm for\nimbalanced classification. The solution is based on a lexicographic linear\nprogramming framework which requires two stages. Initially, class-specific\ncomponent weight combinations are found so as to minimize a hinge loss\nindividually for each of the classes. Subsequently, the final component weights\nare assigned so that the maximum deviation from the class-specific minimum loss\nvalues (obtained in the previous stage) is minimized. Hence, the proposal is\nnot only restricted to two-class situations, but is also readily applicable to\nmulti-class problems. Additionally,we also derive the dual formulation\ncorresponding to the proposed framework. Experiments conducted on artificial\nand real-world imbalanced datasets as well as on challenging applications such\nas hyperspectral image classification and ImageNet classification establish the\nefficacy of the proposal.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:41:29 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 01:08:07 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Datta", "Shounak", ""], ["Nag", "Sayak", ""], ["Das", "Swagatam", ""]]}, {"id": "1708.09687", "submitter": "Cheng Li", "authors": "Yunxuan Zhang, Li Liu, Cheng Li, Chen change Loy", "title": "Quantifying Facial Age by Posterior of Age Comparisons", "comments": "To appear on BMVC 2017 (oral) revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel approach for annotating large quantity of in-the-wild\nfacial images with high-quality posterior age distribution as labels. Each\nposterior provides a probability distribution of estimated ages for a face. Our\napproach is motivated by observations that it is easier to distinguish who is\nthe older of two people than to determine the person's actual age. Given a\nreference database with samples of known ages and a dataset to label, we can\ntransfer reliable annotations from the former to the latter via\nhuman-in-the-loop comparisons. We show an effective way to transform such\ncomparisons to posterior via fully-connected and SoftMax layers, so as to\npermit end-to-end training in a deep network. Thanks to the efficient and\neffective annotation approach, we collect a new large-scale facial age dataset,\ndubbed `MegaAge', which consists of 41,941 images. Data can be downloaded from\nour project page mmlab.ie.cuhk.edu.hk/projects/MegaAge and\ngithub.com/zyx2012/Age_estimation_BMVC2017. With the dataset, we train a\nnetwork that jointly performs ordinal hyperplane classification and posterior\ndistribution learning. Our approach achieves state-of-the-art results on\npopular benchmarks such as MORPH2, Adience, and the newly proposed MegaAge.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 12:49:05 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 00:50:36 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Zhang", "Yunxuan", ""], ["Liu", "Li", ""], ["Li", "Cheng", ""], ["Loy", "Chen change", ""]]}, {"id": "1708.09740", "submitter": "Mehmet Turan", "authors": "Mehmet Turan, Yusuf Yigit Pilavci, Ipek Ganiyusufoglu, Helder Araujo,\n  Ender Konukoglu and Metin Sitti", "title": "Sparse-then-Dense Alignment based 3D Map Reconstruction Method for\n  Endoscopic Capsule Robots", "comments": "arXiv admin note: text overlap with arXiv:1705.06524", "journal-ref": null, "doi": "10.1007/s00138-017-0905-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the development of capsule endoscopcy technology, substantial progress\nwere made in converting passive capsule endoscopes to robotic active capsule\nendoscopes which can be controlled by the doctor. However, robotic capsule\nendoscopy still has some challenges. In particular, the use of such devices to\ngenerate a precise and globally consistent three-dimensional (3D) map of the\nentire inner organ remains an unsolved problem. Such global 3D maps of inner\norgans would help doctors to detect the location and size of diseased areas\nmore accurately, precisely, and intuitively, thus permitting more accurate and\nintuitive diagnoses. The proposed 3D reconstruction system is built in a\nmodular fashion including preprocessing, frame stitching, and shading-based 3D\nreconstruction modules. We propose an efficient scheme to automatically select\nthe key frames out of the huge quantity of raw endoscopic images. Together with\na bundle fusion approach that aligns all the selected key frames jointly in a\nglobally consistent way, a significant improvement of the mosaic and 3D map\naccuracy was reached. To the best of our knowledge, this framework is the first\ncomplete pipeline for an endoscopic capsule robot based 3D map reconstruction\ncontaining all of the necessary steps for a reliable and accurate endoscopic 3D\nmap. For the qualitative evaluations, a real pig stomach is employed. Moreover,\nfor the first time in literature, a detailed and comprehensive quantitative\nanalysis of each proposed pipeline modules is performed using a non-rigid\nesophagus gastro duodenoscopy simulator, four different endoscopic cameras, a\nmagnetically activated soft capsule robot (MASCE), a sub-millimeter precise\noptical motion tracker and a fine-scale 3D optical scanner.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:58:29 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Turan", "Mehmet", ""], ["Pilavci", "Yusuf Yigit", ""], ["Ganiyusufoglu", "Ipek", ""], ["Araujo", "Helder", ""], ["Konukoglu", "Ender", ""], ["Sitti", "Metin", ""]]}, {"id": "1708.09825", "submitter": "Michalis Vrigkas", "authors": "Michalis Vrigkas, Evangelos Kazakos, Christophoros Nikou, Ioannis A.\n  Kakadiaris", "title": "Inferring Human Activities Using Robust Privileged Probabilistic\n  Learning", "comments": "To appear in ICCV Workshops 2017 (TASK-CV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification models may often suffer from \"structure imbalance\" between\ntraining and testing data that may occur due to the deficient data collection\nprocess. This imbalance can be represented by the learning using privileged\ninformation (LUPI) paradigm. In this paper, we present a supervised\nprobabilistic classification approach that integrates LUPI into a hidden\nconditional random field (HCRF) model. The proposed model is called LUPI-HCRF\nand is able to cope with additional information that is only available during\ntraining. Moreover, the proposed method employes Student's t-distribution to\nprovide robustness to outliers by modeling the conditional distribution of the\nprivileged information. Experimental results in three publicly available\ndatasets demonstrate the effectiveness of the proposed approach and improve the\nstate-of-the-art in the LUPI framework for recognizing human activities.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:19:56 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Vrigkas", "Michalis", ""], ["Kazakos", "Evangelos", ""], ["Nikou", "Christophoros", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1708.09832", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, Felix Lucka, Marta Betcke, Nam Huynh, Jonas Adler,\n  Ben Cox, Paul Beard, Sebastien Ourselin, and Simon Arridge", "title": "Model based learning for accelerated, limited-view 3D photoacoustic\n  tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2018.2820382", "report-no": null, "categories": "cs.CV cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning for tomographic reconstructions have shown\ngreat potential to create accurate and high quality images with a considerable\nspeed-up. In this work we present a deep neural network that is specifically\ndesigned to provide high resolution 3D images from restricted photoacoustic\nmeasurements. The network is designed to represent an iterative scheme and\nincorporates gradient information of the data fit to compensate for limited\nview artefacts. Due to the high complexity of the photoacoustic forward\noperator, we separate training and computation of the gradient information. A\nsuitable prior for the desired image structures is learned as part of the\ntraining. The resulting network is trained and tested on a set of segmented\nvessels from lung CT scans and then applied to in-vivo photoacoustic\nmeasurement data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:32:29 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 14:44:48 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 14:12:38 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Lucka", "Felix", ""], ["Betcke", "Marta", ""], ["Huynh", "Nam", ""], ["Adler", "Jonas", ""], ["Cox", "Ben", ""], ["Beard", "Paul", ""], ["Ourselin", "Sebastien", ""], ["Arridge", "Simon", ""]]}, {"id": "1708.09839", "submitter": "Christian H\\\"ane", "authors": "Christian H\\\"ane, Lionel Heng, Gim Hee Lee, Friedrich Fraundorfer,\n  Paul Furgale, Torsten Sattler, Marc Pollefeys", "title": "3D Visual Perception for Self-Driving Cars using a Multi-Camera System:\n  Calibration, Mapping, Localization, and Obstacle Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras are a crucial exteroceptive sensor for self-driving cars as they are\nlow-cost and small, provide appearance information about the environment, and\nwork in various weather conditions. They can be used for multiple purposes such\nas visual navigation and obstacle detection. We can use a surround multi-camera\nsystem to cover the full 360-degree field-of-view around the car. In this way,\nwe avoid blind spots which can otherwise lead to accidents. To minimize the\nnumber of cameras needed for surround perception, we utilize fisheye cameras.\nConsequently, standard vision pipelines for 3D mapping, visual localization,\nobstacle detection, etc. need to be adapted to take full advantage of the\navailability of multiple cameras rather than treat each camera individually. In\naddition, processing of fisheye images has to be supported. In this paper, we\ndescribe the camera calibration and subsequent processing pipeline for\nmulti-fisheye-camera systems developed as part of the V-Charge project. This\nproject seeks to enable automated valet parking for self-driving cars. Our\npipeline is able to precisely calibrate multi-camera systems, build sparse 3D\nmaps for visual navigation, visually localize the car with respect to these\nmaps, generate accurate dense maps, as well as detect obstacles based on\nreal-time depth map extraction.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:45:42 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["H\u00e4ne", "Christian", ""], ["Heng", "Lionel", ""], ["Lee", "Gim Hee", ""], ["Fraundorfer", "Friedrich", ""], ["Furgale", "Paul", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1708.09843", "submitter": "Lily Peng", "authors": "Ryan Poplin, Avinash V. Varadarajan, Katy Blumer, Yun Liu, Michael V.\n  McConnell, Greg S. Corrado, Lily Peng, Dale R. Webster", "title": "Predicting Cardiovascular Risk Factors from Retinal Fundus Photographs\n  using Deep Learning", "comments": null, "journal-ref": "Nature Biomedical Engineering (2018)", "doi": "10.1038/s41551-018-0195-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, medical discoveries are made by observing associations and\nthen designing experiments to test these hypotheses. However, observing and\nquantifying associations in images can be difficult because of the wide variety\nof features, patterns, colors, values, shapes in real data. In this paper, we\nuse deep learning, a machine learning technique that learns its own features,\nto discover new knowledge from retinal fundus images. Using models trained on\ndata from 284,335 patients, and validated on two independent datasets of 12,026\nand 999 patients, we predict cardiovascular risk factors not previously thought\nto be present or quantifiable in retinal images, such as such as age (within\n3.26 years), gender (0.97 AUC), smoking status (0.71 AUC), HbA1c (within\n1.39%), systolic blood pressure (within 11.23mmHg) as well as major adverse\ncardiac events (0.70 AUC). We further show that our models used distinct\naspects of the anatomy to generate each prediction, such as the optic disc or\nblood vessels, opening avenues of further research.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 17:49:15 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 22:48:06 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Poplin", "Ryan", ""], ["Varadarajan", "Avinash V.", ""], ["Blumer", "Katy", ""], ["Liu", "Yun", ""], ["McConnell", "Michael V.", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""]]}]