[{"id": "2003.00052", "submitter": "Kevin Lin", "authors": "Kevin Lin, Lijuan Wang, Ying Jin, Zicheng Liu, Ming-Ting Sun", "title": "Learning Nonparametric Human Mesh Reconstruction from a Single Image\n  without Ground Truth Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric approaches have shown promising results on reconstructing 3D\nhuman mesh from a single monocular image. Unlike previous approaches that use a\nparametric human model like skinned multi-person linear model (SMPL), and\nattempt to regress the model parameters, nonparametric approaches relax the\nheavy reliance on the parametric space. However, existing nonparametric methods\nrequire ground truth meshes as their regression target for each vertex, and\nobtaining ground truth mesh labels is very expensive. In this paper, we propose\na novel approach to learn human mesh reconstruction without any ground truth\nmeshes. This is made possible by introducing two new terms into the loss\nfunction of a graph convolutional neural network (Graph CNN). The first term is\nthe Laplacian prior that acts as a regularizer on the reconstructed mesh. The\nsecond term is the part segmentation loss that forces the projected region of\nthe reconstructed mesh to match the part segmentation. Experimental results on\nmultiple public datasets show that without using 3D ground truth meshes, the\nproposed approach outperforms the previous state-of-the-art approaches that\nrequire ground truth meshes for training.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:30:07 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lin", "Kevin", ""], ["Wang", "Lijuan", ""], ["Jin", "Ying", ""], ["Liu", "Zicheng", ""], ["Sun", "Ming-Ting", ""]]}, {"id": "2003.00063", "submitter": "Gustavo Assun\\c{c}\\~ao", "authors": "Gustavo Assun\\c{c}\\~ao, Nuno Gon\\c{c}alves, Paulo Menezes", "title": "Bio-Inspired Modality Fusion for Active Speaker Detection", "comments": null, "journal-ref": "Appl. Sci. 2021, 11(8), 3397", "doi": "10.3390/app11083397", "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human beings have developed fantastic abilities to integrate information from\nvarious sensory sources exploring their inherent complementarity. Perceptual\ncapabilities are therefore heightened, enabling, for instance, the well-known\n\"cocktail party\" and McGurk effects, i.e., speech disambiguation from a panoply\nof sound signals. This fusion ability is also key in refining the perception of\nsound source location, as in distinguishing whose voice is being heard in a\ngroup conversation. Furthermore, neuroscience has successfully identified the\nsuperior colliculus region in the brain as the one responsible for this\nmodality fusion, with a handful of biological models having been proposed to\napproach its underlying neurophysiological process. Deriving inspiration from\none of these models, this paper presents a methodology for effectively fusing\ncorrelated auditory and visual information for active speaker detection. Such\nan ability can have a wide range of applications, from teleconferencing systems\nto social robotics. The detection approach initially routes auditory and visual\ninformation through two specialized neural network structures. The resulting\nembeddings are fused via a novel layer based on the superior colliculus, whose\ntopological structure emulates spatial neuron cross-mapping of unimodal\nperceptual fields. The validation process employed two publicly available\ndatasets, with achieved results confirming and greatly surpassing initial\nexpectations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 20:56:24 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 11:05:06 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Assun\u00e7\u00e3o", "Gustavo", ""], ["Gon\u00e7alves", "Nuno", ""], ["Menezes", "Paulo", ""]]}, {"id": "2003.00070", "submitter": "Jacob George", "authors": "Jacob A. George, Anna Neibling, Michael D. Paskett, Gregory A. Clark", "title": "Inexpensive surface electromyography sleeve with consistent electrode\n  placement enables dexterous and stable prosthetic control through deep\n  learning", "comments": "MEC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dexterity of conventional myoelectric prostheses is limited in part by\nthe small datasets used to train the control algorithms. Variations in surface\nelectrode positioning make it difficult to collect consistent data and to\nestimate motor intent reliably over time. To address these challenges, we\ndeveloped an inexpensive, easy-to-don sleeve that can record robust and\nrepeatable surface electromyography from 32 embedded monopolar electrodes.\nEmbedded grommets are used to consistently align the sleeve with natural skin\nmarkings (e.g., moles, freckles, scars). The sleeve can be manufactured in a\nfew hours for less than $60. Data from seven intact participants show the\nsleeve provides a signal-to-noise ratio of 14, a don-time under 11 seconds, and\nsub-centimeter precision for electrode placement. Furthermore, in a case study\nwith one intact participant, we use the sleeve to demonstrate that neural\nnetworks can provide simultaneous and proportional control of six degrees of\nfreedom, even 263 days after initial algorithm training. We also highlight that\nconsistent recordings, accumulated over time to establish a large dataset,\nsignificantly improve dexterity. These results suggest that deep learning with\na 74-layer neural network can substantially improve the dexterity and stability\nof myoelectric prosthetic control, and that deep-learning techniques can be\nreadily instantiated and further validated through inexpensive sleeves/sockets\nwith consistent recording locations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 21:24:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["George", "Jacob A.", ""], ["Neibling", "Anna", ""], ["Paskett", "Michael D.", ""], ["Clark", "Gregory A.", ""]]}, {"id": "2003.00080", "submitter": "Artsiom Sanakoyeu", "authors": "Artsiom Sanakoyeu, Vasil Khalidov, Maureen S. McCarthy, Andrea\n  Vedaldi, Natalia Neverova", "title": "Transferring Dense Pose to Proximal Animal Classes", "comments": "Accepted at CVPR 2020; Project page:\n  https://asanakoy.github.io/densepose-evolution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent contributions have demonstrated that it is possible to recognize the\npose of humans densely and accurately given a large dataset of poses annotated\nin detail. In principle, the same approach could be extended to any animal\nclass, but the effort required for collecting new annotations for each case\nmakes this strategy impractical, despite important applications in natural\nconservation, science and business. We show that, at least for proximal animal\nclasses such as chimpanzees, it is possible to transfer the knowledge existing\nin dense pose recognition for humans, as well as in more general object\ndetectors and segmenters, to the problem of dense pose recognition in other\nclasses. We do this by (1) establishing a DensePose model for the new animal\nwhich is also geometrically aligned to humans (2) introducing a multi-head\nR-CNN architecture that facilitates transfer of multiple recognition tasks\nbetween classes, (3) finding which combination of known classes can be\ntransferred most effectively to the new animal and (4) using self-calibrated\nuncertainty heads to generate pseudo-labels graded by quality for training a\nmodel for this class. We also introduce two benchmark datasets labelled in the\nmanner of DensePose for the class chimpanzee and use them to evaluate our\napproach, showing excellent transfer learning performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 21:43:53 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sanakoyeu", "Artsiom", ""], ["Khalidov", "Vasil", ""], ["McCarthy", "Maureen S.", ""], ["Vedaldi", "Andrea", ""], ["Neverova", "Natalia", ""]]}, {"id": "2003.00105", "submitter": "Jianbo Jiao", "authors": "Jianbo Jiao, Richard Droste, Lior Drukker, Aris T. Papageorghiou, J.\n  Alison Noble", "title": "Self-supervised Representation Learning for Ultrasound Video", "comments": "ISBI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have achieved promising performance for\nmedical image analysis, while in most cases ground-truth annotations from human\nexperts are necessary to train the deep model. In practice, such annotations\nare expensive to collect and can be scarce for medical imaging applications.\nTherefore, there is significant interest in learning representations from\nunlabelled raw data. In this paper, we propose a self-supervised learning\napproach to learn meaningful and transferable representations from medical\nimaging video without any type of human annotation. We assume that in order to\nlearn such a representation, the model should identify anatomical structures\nfrom the unlabelled data. Therefore we force the model to address anatomy-aware\ntasks with free supervision from the data itself. Specifically, the model is\ndesigned to correct the order of a reshuffled video clip and at the same time\npredict the geometric transformation applied to the video clip. Experiments on\nfetal ultrasound video show that the proposed approach can effectively learn\nmeaningful and strong representations, which transfer well to downstream tasks\nlike standard plane detection and saliency prediction.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 23:00:26 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jiao", "Jianbo", ""], ["Droste", "Richard", ""], ["Drukker", "Lior", ""], ["Papageorghiou", "Aris T.", ""], ["Noble", "J. Alison", ""]]}, {"id": "2003.00134", "submitter": "Saurav Manchanda", "authors": "Khoa D. Doan and Saurav Manchanda and Sarkhan Badirli and Chandan K.\n  Reddy", "title": "Image Hashing by Minimizing Discrete Component-wise Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image hashing is one of the fundamental problems that demand both efficient\nand effective solutions for various practical scenarios. Adversarial\nautoencoders are shown to be able to implicitly learn a robust,\nlocality-preserving hash function that generates balanced and high-quality hash\ncodes. However, the existing adversarial hashing methods are inefficient to be\nemployed for large-scale image retrieval applications. Specifically, they\nrequire an exponential number of samples to be able to generate optimal hash\ncodes and a significantly high computational cost to train. In this paper, we\nshow that the high sample-complexity requirement often results in sub-optimal\nretrieval performance of the adversarial hashing methods. To address this\nchallenge, we propose a new adversarial-autoencoder hashing approach that has a\nmuch lower sample requirement and computational cost. Specifically, by\nexploiting the desired properties of the hash function in the low-dimensional,\ndiscrete space, our method efficiently estimates a better variant of\nWasserstein distance by averaging a set of easy-to-compute one-dimensional\nWasserstein distances. The resulting hashing approach has an order-of-magnitude\nbetter sample complexity, thus better generalization property, compared to the\nother adversarial hashing methods. In addition, the computational cost is\nsignificantly reduced using our approach. We conduct experiments on several\nreal-world datasets and show that the proposed method outperforms the competing\nhashing methods, achieving up to 10% improvement over the current\nstate-of-the-art image hashing methods. The code accompanying this paper is\navailable on Github (https://github.com/khoadoan/adversarial-hashing).\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 00:22:53 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:45:15 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 01:33:29 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Doan", "Khoa D.", ""], ["Manchanda", "Saurav", ""], ["Badirli", "Sarkhan", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2003.00164", "submitter": "Yinjie Lei", "authors": "Yinjie Lei, Yan Liu, Pingping Zhang, Lingqiao Liu", "title": "Towards Using Count-level Weak Supervision for Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing crowd counting methods require object location-level\nannotation, i.e., placing a dot at the center of an object. While being simpler\nthan the bounding-box or pixel-level annotation, obtaining this annotation is\nstill labor-intensive and time-consuming especially for images with highly\ncrowded scenes. On the other hand, weaker annotations that only know the total\ncount of objects can be almost effortless in many practical scenarios. Thus, it\nis desirable to develop a learning method that can effectively train models\nfrom count-level annotations. To this end, this paper studies the problem of\nweakly-supervised crowd counting which learns a model from only a small amount\nof location-level annotations (fully-supervised) but a large amount of\ncount-level annotations (weakly-supervised). To perform effective training in\nthis scenario, we observe that the direct solution of regressing the integral\nof density map to the object count is not sufficient and it is beneficial to\nintroduce stronger regularizations on the predicted density map of\nweakly-annotated images. We devise a simple-yet-effective training strategy,\nnamely Multiple Auxiliary Tasks Training (MATT), to construct regularizes for\nrestricting the freedom of the generated density maps. Through extensive\nexperiments on existing datasets and a newly proposed dataset, we validate the\neffectiveness of the proposed weakly-supervised method and demonstrate its\nsuperior performance over existing solutions.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 02:58:36 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lei", "Yinjie", ""], ["Liu", "Yan", ""], ["Zhang", "Pingping", ""], ["Liu", "Lingqiao", ""]]}, {"id": "2003.00168", "submitter": "Hardik Uppal", "authors": "Hardik Uppal, Alireza Sepas-Moghaddam, Michael Greenspan and Ali\n  Etemad", "title": "Two-Level Attention-based Fusion Learning for RGB-D Face Recognition", "comments": "8 Pages, 4 figure, Accepted to International Conference on Pattern\n  Recognition (ICPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in RGB-D sensing technologies as well as improvements in\nmachine learning and fusion techniques, RGB-D facial recognition has become an\nactive area of research. A novel attention aware method is proposed to fuse two\nimage modalities, RGB and depth, for enhanced RGB-D facial recognition. The\nproposed method first extracts features from both modalities using a\nconvolutional feature extractor. These features are then fused using a\ntwo-layer attention mechanism. The first layer focuses on the fused feature\nmaps generated by the feature extractor, exploiting the relationship between\nfeature maps using LSTM recurrent learning. The second layer focuses on the\nspatial features of those maps using convolution. The training database is\npreprocessed and augmented through a set of geometric transformations, and the\nlearning process is further aided using transfer learning from a pure 2D RGB\nimage training process. Comparative evaluations demonstrate that the proposed\nmethod outperforms other state-of-the-art approaches, including both\ntraditional and deep neural network-based methods, on the challenging\nCurtinFaces and IIIT-D RGB-D benchmark databases, achieving classification\naccuracies over 98.2% and 99.3% respectively. The proposed attention mechanism\nis also compared with other attention mechanisms, demonstrating more accurate\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 03:18:52 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 20:30:47 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 10:20:01 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Uppal", "Hardik", ""], ["Sepas-Moghaddam", "Alireza", ""], ["Greenspan", "Michael", ""], ["Etemad", "Ali", ""]]}, {"id": "2003.00186", "submitter": "Shuangjie Xu", "authors": "Maosheng Ye, Shuangjie Xu and Tongyi Cao", "title": "HVNet: Hybrid Voxel Network for LiDAR Based 3D Object Detection", "comments": "accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hybrid Voxel Network (HVNet), a novel one-stage unified network\nfor point cloud based 3D object detection for autonomous driving. Recent\nstudies show that 2D voxelization with per voxel PointNet style feature\nextractor leads to accurate and efficient detector for large 3D scenes. Since\nthe size of the feature map determines the computation and memory cost, the\nsize of the voxel becomes a parameter that is hard to balance. A smaller voxel\nsize gives a better performance, especially for small objects, but a longer\ninference time. A larger voxel can cover the same area with a smaller feature\nmap, but fails to capture intricate features and accurate location for smaller\nobjects. We present a Hybrid Voxel network that solves this problem by fusing\nvoxel feature encoder (VFE) of different scales at point-wise level and project\ninto multiple pseudo-image feature maps. We further propose an attentive voxel\nfeature encoding that outperforms plain VFE and a feature fusion pyramid\nnetwork to aggregate multi-scale information at feature map level. Experiments\non the KITTI benchmark show that a single HVNet achieves the best mAP among all\nexisting methods with a real time inference speed of 31Hz.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 05:46:19 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 15:51:05 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Ye", "Maosheng", ""], ["Xu", "Shuangjie", ""], ["Cao", "Tongyi", ""]]}, {"id": "2003.00187", "submitter": "Takehiko Ohkawa", "authors": "Takehiko Ohkawa, Naoto Inoue, Hirokatsu Kataoka, Nakamasa Inoue", "title": "Augmented Cyclic Consistency Regularization for Unpaired Image-to-Image\n  Translation", "comments": "Accepted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image (I2I) translation has received considerable attention\nin pattern recognition and computer vision because of recent advancements in\ngenerative adversarial networks (GANs). However, due to the lack of explicit\nsupervision, unpaired I2I models often fail to generate realistic images,\nespecially in challenging datasets with different backgrounds and poses. Hence,\nstabilization is indispensable for GANs and applications of I2I translation.\nHerein, we propose Augmented Cyclic Consistency Regularization (ACCR), a novel\nregularization method for unpaired I2I translation. Our main idea is to enforce\nconsistency regularization originating from semi-supervised learning on the\ndiscriminators leveraging real, fake, reconstructed, and augmented samples. We\nregularize the discriminators to output similar predictions when fed pairs of\noriginal and perturbed images. We qualitatively clarify why consistency\nregularization on fake and reconstructed samples works well. Quantitatively,\nour method outperforms the consistency regularized GAN (CR-GAN) in real-world\ntranslations and demonstrates efficacy against several data augmentation\nvariants and cycle-consistent constraints.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 06:20:20 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:07:23 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ohkawa", "Takehiko", ""], ["Inoue", "Naoto", ""], ["Kataoka", "Hirokatsu", ""], ["Inoue", "Nakamasa", ""]]}, {"id": "2003.00188", "submitter": "Meng Tian", "authors": "Meng Tian, Liang Pan, Marcelo H Ang Jr and Gim Hee Lee", "title": "Robust 6D Object Pose Estimation by Learning RGB-D Features", "comments": "Accepted at ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 6D object pose estimation is fundamental to robotic manipulation and\ngrasping. Previous methods follow a local optimization approach which minimizes\nthe distance between closest point pairs to handle the rotation ambiguity of\nsymmetric objects. In this work, we propose a novel discrete-continuous\nformulation for rotation regression to resolve this local-optimum problem. We\nuniformly sample rotation anchors in SO(3), and predict a constrained deviation\nfrom each anchor to the target, as well as uncertainty scores for selecting the\nbest prediction. Additionally, the object location is detected by aggregating\npoint-wise vectors pointing to the 3D center. Experiments on two benchmarks:\nLINEMOD and YCB-Video, show that the proposed method outperforms\nstate-of-the-art approaches. Our code is available at\nhttps://github.com/mentian/object-posenet.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 06:24:55 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 14:25:38 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Tian", "Meng", ""], ["Pan", "Liang", ""], ["Ang", "Marcelo H", "Jr"], ["Lee", "Gim Hee", ""]]}, {"id": "2003.00196", "submitter": "Aliaksandr Siarohin", "authors": "Aliaksandr Siarohin, St\\'ephane Lathuili\\`ere, Sergey Tulyakov, Elisa\n  Ricci and Nicu Sebe", "title": "First Order Motion Model for Image Animation", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image animation consists of generating a video sequence so that an object in\na source image is animated according to the motion of a driving video. Our\nframework addresses this problem without using any annotation or prior\ninformation about the specific object to animate. Once trained on a set of\nvideos depicting objects of the same category (e.g. faces, human bodies), our\nmethod can be applied to any object of this class. To achieve this, we decouple\nappearance and motion information using a self-supervised formulation. To\nsupport complex motions, we use a representation consisting of a set of learned\nkeypoints along with their local affine transformations. A generator network\nmodels occlusions arising during target motions and combines the appearance\nextracted from the source image and the motion derived from the driving video.\nOur framework scores best on diverse benchmarks and on a variety of object\ncategories. Our source code is publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 07:08:56 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 08:38:53 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 15:26:15 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Siarohin", "Aliaksandr", ""], ["Lathuili\u00e8re", "St\u00e9phane", ""], ["Tulyakov", "Sergey", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "2003.00197", "submitter": "Longlong Jing", "authors": "Longlong Jing, Toufiq Parag, Zhe Wu, Yingli Tian, Hongcheng Wang", "title": "VideoSSL: Semi-Supervised Learning for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semi-supervised learning approach for video classification,\nVideoSSL, using convolutional neural networks (CNN). Like other computer vision\ntasks, existing supervised video classification methods demand a large amount\nof labeled data to attain good performance. However, annotation of a large\ndataset is expensive and time consuming. To minimize the dependence on a large\nannotated dataset, our proposed semi-supervised method trains from a small\nnumber of labeled examples and exploits two regulatory signals from unlabeled\ndata. The first signal is the pseudo-labels of unlabeled examples computed from\nthe confidences of the CNN being trained. The other is the normalized\nprobabilities, as predicted by an image classifier CNN, that captures the\ninformation about appearances of the interesting objects in the video. We show\nthat, under the supervision of these guiding signals from unlabeled examples, a\nvideo classification CNN can achieve impressive performances utilizing a small\nfraction of annotated examples on three publicly available datasets: UCF101,\nHMDB51 and Kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 07:13:12 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jing", "Longlong", ""], ["Parag", "Toufiq", ""], ["Wu", "Zhe", ""], ["Tian", "Yingli", ""], ["Wang", "Hongcheng", ""]]}, {"id": "2003.00210", "submitter": "Congqi Cao", "authors": "Congqi Cao and Yanning Zhang", "title": "Learning to Compare Relation: Semantic Alignment for Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning is a fundamental and challenging problem since it requires\nrecognizing novel categories from only a few examples. The objects for\nrecognition have multiple variants and can locate anywhere in images. Directly\ncomparing query images with example images can not handle content misalignment.\nThe representation and metric for comparison are critical but challenging to\nlearn due to the scarcity and wide variation of the samples in few-shot\nlearning. In this paper, we present a novel semantic alignment model to compare\nrelations, which is robust to content misalignment. We propose to add two key\ningredients to existing few-shot learning frameworks for better feature and\nmetric learning ability. First, we introduce a semantic alignment loss to align\nthe relation statistics of the features from samples that belong to the same\ncategory. And second, local and global mutual information maximization is\nintroduced, allowing for representations that contain locally-consistent and\nintra-class shared information across structural locations in an image.\nThirdly, we introduce a principled approach to weigh multiple loss functions by\nconsidering the homoscedastic uncertainty of each stream. We conduct extensive\nexperiments on several few-shot learning datasets. Experimental results show\nthat the proposed method is capable of comparing relations with semantic\nalignment strategies, and achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 08:37:02 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Cao", "Congqi", ""], ["Zhang", "Yanning", ""]]}, {"id": "2003.00213", "submitter": "Xing Fan", "authors": "Xing Fan, Hao Luo, Chi Zhang, Wei Jiang", "title": "Cross-Spectrum Dual-Subspace Pairing for RGB-infrared Cross-Modality\n  Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its potential wide applications in video surveillance and other\ncomputer vision tasks like tracking, person re-identification (ReID) has become\npopular and been widely investigated. However, conventional person\nre-identification can only handle RGB color images, which will fail at dark\nconditions. Thus RGB-infrared ReID (also known as Infrared-Visible ReID or\nVisible-Thermal ReID) is proposed. Apart from appearance discrepancy in\ntraditional ReID caused by illumination, pose variations and viewpoint changes,\nmodality discrepancy produced by cameras of the different spectrum also exists,\nwhich makes RGB-infrared ReID more difficult. To address this problem, we focus\non extracting the shared cross-spectrum features of different modalities. In\nthis paper, a novel multi-spectrum image generation method is proposed and the\ngenerated samples are utilized to help the network to find discriminative\ninformation for re-identifying the same person across modalities. Another\nchallenge of RGB-infrared ReID is that the intra-person (images from the same\nperson) discrepancy is often larger than the inter-person (images from\ndifferent persons) discrepancy, so a dual-subspace pairing strategy is proposed\nto alleviate this problem. Combining those two parts together, we also design a\none-stream neural network combining the aforementioned methods to extract\ncompact representations of person images, called Cross-spectrum Dual-subspace\nPairing (CDP) model. Furthermore, during the training process, we also propose\na Dynamic Hard Spectrum Mining method to automatically mine more hard samples\nfrom hard spectrum based on the current model state to further boost the\nperformance. Extensive experimental results on two public datasets, SYSU-MM01\nwith RGB + near-infrared images and RegDB with RGB + far-infrared images, have\ndemonstrated the efficiency and generality of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 09:01:39 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fan", "Xing", ""], ["Luo", "Hao", ""], ["Zhang", "Chi", ""], ["Jiang", "Wei", ""]]}, {"id": "2003.00214", "submitter": "Wenqi Shao", "authors": "Wenqi Shao, Shitao Tang, Xingang Pan, Ping Tan, Xiaogang Wang, Ping\n  Luo", "title": "Channel Equilibrium Networks for Learning Deep Representation", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are typically constructed by stacking\nmultiple building blocks, each of which contains a normalization layer such as\nbatch normalization (BN) and a rectified linear function such as ReLU. However,\nthis work shows that the combination of normalization and rectified linear\nfunction leads to inhibited channels, which have small magnitude and contribute\nlittle to the learned feature representation, impeding the generalization\nability of CNNs. Unlike prior arts that simply removed the inhibited channels,\nwe propose to \"wake them up\" during training by designing a novel neural\nbuilding block, termed Channel Equilibrium (CE) block, which enables channels\nat the same layer to contribute equally to the learned representation. We show\nthat CE is able to prevent inhibited channels both empirically and\ntheoretically. CE has several appealing benefits. (1) It can be integrated into\nmany advanced CNN architectures such as ResNet and MobileNet, outperforming\ntheir original networks. (2) CE has an interesting connection with the Nash\nEquilibrium, a well-known solution of a non-cooperative game. (3) Extensive\nexperiments show that CE achieves state-of-the-art performance on various\nchallenging benchmarks such as ImageNet and COCO.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 09:02:31 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Shao", "Wenqi", ""], ["Tang", "Shitao", ""], ["Pan", "Xingang", ""], ["Tan", "Ping", ""], ["Wang", "Xiaogang", ""], ["Luo", "Ping", ""]]}, {"id": "2003.00217", "submitter": "Yutao Hu", "authors": "Yutao Hu, Xiaolong Jiang, Xuhui Liu, Baochang Zhang, Jungong Han,\n  Xianbin Cao, David Doermann", "title": "NAS-Count: Counting-by-Density with Neural Architecture Search", "comments": "Accepted to European Conference on Computer Vision(ECCV) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the recent advances in crowd counting have evolved from hand-designed\ndensity estimation networks, where multi-scale features are leveraged to\naddress the scale variation problem, but at the expense of demanding design\nefforts. In this work, we automate the design of counting models with Neural\nArchitecture Search (NAS) and introduce an end-to-end searched encoder-decoder\narchitecture, Automatic Multi-Scale Network (AMSNet). Specifically, we utilize\na counting-specific two-level search space. The encoder and decoder in AMSNet\nare composed of different cells discovered from micro-level search, while the\nmulti-path architecture is explored through macro-level search. To solve the\npixel-level isolation issue in MSE loss, AMSNet is optimized with an\nauto-searched Scale Pyramid Pooling Loss (SPPLoss) that supervises the\nmulti-scale structural information. Extensive experiments on four datasets show\nAMSNet produces state-of-the-art results that outperform hand-designed models,\nfully demonstrating the efficacy of NAS-Count.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 09:18:17 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 03:54:01 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Hu", "Yutao", ""], ["Jiang", "Xiaolong", ""], ["Liu", "Xuhui", ""], ["Zhang", "Baochang", ""], ["Han", "Jungong", ""], ["Cao", "Xianbin", ""], ["Doermann", "David", ""]]}, {"id": "2003.00255", "submitter": "Zhilei Liu", "authors": "Zhilei Liu, Yunpeng Wu, Le Li, Cuicui Zhang, Baoyuan Wu", "title": "Joint Face Completion and Super-resolution using Multi-scale Feature\n  Relation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on face restoration often focused on repairing a specific\ntype of low-quality facial images such as low-resolution (LR) or occluded\nfacial images. However, in the real world, both the above-mentioned forms of\nimage degradation often coexist. Therefore, it is important to design a model\nthat can repair LR occluded images simultaneously. This paper proposes a\nmulti-scale feature graph generative adversarial network (MFG-GAN) to implement\nthe face restoration of images in which both degradation modes coexist, and\nalso to repair images with a single type of degradation. Based on the GAN, the\nMFG-GAN integrates the graph convolution and feature pyramid network to restore\noccluded low-resolution face images to non-occluded high-resolution face\nimages. The MFG-GAN uses a set of customized losses to ensure that high-quality\nimages are generated. In addition, we designed the network in an end-to-end\nformat. Experimental results on the public-domain CelebA and Helen databases\nshow that the proposed approach outperforms state-of-the-art methods in\nperforming face super-resolution (up to 4x or 8x) and face completion\nsimultaneously. Cross-database testing also revealed that the proposed approach\nhas good generalizability.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 13:31:46 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 14:35:13 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Liu", "Zhilei", ""], ["Wu", "Yunpeng", ""], ["Li", "Le", ""], ["Zhang", "Cuicui", ""], ["Wu", "Baoyuan", ""]]}, {"id": "2003.00273", "submitter": "Runfa Chen", "authors": "Runfa Chen, Wenbing Huang, Binghui Huang, Fuchun Sun, Bin Fang", "title": "Reusing Discriminators for Encoding: Towards Unsupervised Image-to-Image\n  Translation", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation is a central task in computer vision.\nCurrent translation frameworks will abandon the discriminator once the training\nprocess is completed. This paper contends a novel role of the discriminator by\nreusing it for encoding the images of the target domain. The proposed\narchitecture, termed as NICE-GAN, exhibits two advantageous patterns over\nprevious approaches: First, it is more compact since no independent encoding\ncomponent is required; Second, this plug-in encoder is directly trained by the\nadversary loss, making it more informative and trained more effectively if a\nmulti-scale discriminator is applied. The main issue in NICE-GAN is the\ncoupling of translation with discrimination along the encoder, which could\nincur training inconsistency when we play the min-max game via GAN. To tackle\nthis issue, we develop a decoupled training strategy by which the encoder is\nonly trained when maximizing the adversary loss while keeping frozen otherwise.\nExtensive experiments on four popular benchmarks demonstrate the superior\nperformance of NICE-GAN over state-of-the-art methods in terms of FID, KID, and\nalso human preference. Comprehensive ablation studies are also carried out to\nisolate the validity of each proposed component. Our codes are available at\nhttps://github.com/alpc91/NICE-GAN-pytorch.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 14:55:43 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 09:01:14 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 16:40:42 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 09:21:26 GMT"}, {"version": "v5", "created": "Sat, 14 Mar 2020 03:19:34 GMT"}, {"version": "v6", "created": "Sat, 28 Mar 2020 14:51:33 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Runfa", ""], ["Huang", "Wenbing", ""], ["Huang", "Binghui", ""], ["Sun", "Fuchun", ""], ["Fang", "Bin", ""]]}, {"id": "2003.00278", "submitter": "Titus Cieslewski", "authors": "Amadeus Oertel, Titus Cieslewski and Davide Scaramuzza", "title": "Augmenting Visual Place Recognition with Structural Cues", "comments": "8 pages, published in RA-L & IROS 2020", "journal-ref": "IEEE Robotics and Automation Letters, 2020", "doi": "10.1109/LRA.2020.3009077", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to augment image-based place recognition with\nstructural cues. Specifically, these structural cues are obtained using\nstructure-from-motion, such that no additional sensors are needed for place\nrecognition. This is achieved by augmenting the 2D convolutional neural network\n(CNN) typically used for image-based place recognition with a 3D CNN that takes\nas input a voxel grid derived from the structure-from-motion point cloud. We\nevaluate different methods for fusing the 2D and 3D features and obtain best\nperformance with global average pooling and simple concatenation. On the Oxford\nRobotCar dataset, the resulting descriptor exhibits superior recognition\nperformance compared to descriptors extracted from only one of the input\nmodalities, including state-of-the-art image-based descriptors. Especially at\nlow descriptor dimensionalities, we outperform state-of-the-art descriptors by\nup to 90%.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 15:28:05 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:06:19 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 15:31:14 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Oertel", "Amadeus", ""], ["Cieslewski", "Titus", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.00287", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Anuj Srivastava", "title": "Representations, Metrics and Statistics For Shape Analysis of Elastic\n  Graphs", "comments": "Visualization improved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past approaches for statistical shape analysis of objects have focused mainly\non objects within the same topological classes, e.g., scalar functions,\nEuclidean curves, or surfaces, etc. For objects that differ in more complex\nways, the current literature offers only topological methods. This paper\nintroduces a far-reaching geometric approach for analyzing shapes of graphical\nobjects, such as road networks, blood vessels, brain fiber tracts, etc. It\nrepresents such objects, exhibiting differences in both geometries and\ntopologies, as graphs made of curves with arbitrary shapes (edges) and\nconnected at arbitrary junctions (nodes). To perform statistical analyses, one\nneeds mathematical representations, metrics and other geometrical tools, such\nas geodesics, means, and covariances. This paper utilizes a quotient structure\nto develop efficient algorithms for computing these quantities, leading to\nuseful statistical tools, including principal component analysis and analytical\nstatistical testing and modeling of graphical shapes. The efficacy of this\nframework is demonstrated using various simulated as well as the real data from\nneurons and brain arterial networks.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:07:48 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 00:34:06 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2003.00293", "submitter": "Paul Irofti", "authors": "Paul Irofti and Andra B\\u{a}ltoiu", "title": "Unsupervised Dictionary Learning for Anomaly Detection", "comments": "in Proceedings of iTWIST'20, Paper-ID: 09, Nantes, France, December,\n  2-4, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the possibilities of employing dictionary learning to address\nthe requirements of most anomaly detection applications, such as absence of\nsupervision, online formulations, low false positive rates. We present new\nresults of our recent semi-supervised online algorithm, TODDLeR, on a\nanti-money laundering application. We also introduce a novel unsupervised\nmethod of using the performance of the learning algorithm as indication of the\nnature of the samples.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:25:56 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 12:49:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Irofti", "Paul", ""], ["B\u0103ltoiu", "Andra", ""]]}, {"id": "2003.00303", "submitter": "Shengyu Zhang", "authors": "Shengyu Zhang, Tan Jiang, Qinghao Huang, Ziqi Tan, Zhou Zhao, Siliang\n  Tang, Jin Yu, Hongxia Yang, Yi Yang, and Fei Wu", "title": "Grounded and Controllable Image Completion by Incorporating Lexical\n  Semantics", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an approach, namely Lexical Semantic Image\nCompletion (LSIC), that may have potential applications in art, design, and\nheritage conservation, among several others. Existing image completion\nprocedure is highly subjective by considering only visual context, which may\ntrigger unpredictable results which are plausible but not faithful to a\ngrounded knowledge. To permit both grounded and controllable completion\nprocess, we advocate generating results faithful to both visual and lexical\nsemantic context, i.e., the description of leaving holes or blank regions in\nthe image (e.g., hole description). One major challenge for LSIC comes from\nmodeling and aligning the structure of visual-semantic context and translating\nacross different modalities. We term this process as structure completion,\nwhich is realized by multi-grained reasoning blocks in our model. Another\nchallenge relates to the unimodal biases, which occurs when the model generates\nplausible results without using the textual description. This can be true since\nthe annotated captions for an image are often semantically equivalent in\nexisting datasets, and thus there is only one paired text for a masked image in\ntraining. We devise an unsupervised unpaired-creation learning path besides the\nover-explored paired-reconstruction path, as well as a multi-stage training\nstrategy to mitigate the insufficiency of labeled data. We conduct extensive\nquantitative and qualitative experiments as well as ablation studies, which\nreveal the efficacy of our proposed LSIC.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:54:21 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhang", "Shengyu", ""], ["Jiang", "Tan", ""], ["Huang", "Qinghao", ""], ["Tan", "Ziqi", ""], ["Zhao", "Zhou", ""], ["Tang", "Siliang", ""], ["Yu", "Jin", ""], ["Yang", "Hongxia", ""], ["Yang", "Yi", ""], ["Wu", "Fei", ""]]}, {"id": "2003.00317", "submitter": "Abhinav K. Jha", "authors": "Ziping Liu, Joyce C. Mhlanga, Richard Laforest, Paul-Robert\n  Derenoncourt, Barry A. Siegel, Abhinav K. Jha", "title": "An estimation-based approach to tumor segmentation in oncological PET", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor segmentation in oncological PET is challenging, a major reason being\nthe partial-volume effects due to the low system resolution and finite voxel\nsize. The latter results in tissue-fraction effects, i.e. voxels contain a\nmixture of tissue classes. Most conventional methods perform segmentation by\nexclusively assigning each voxel in the image as belonging to either the tumor\nor normal tissue classes. Thus, these methods are inherently limited in\nmodeling the tissue-fraction effects. To address this inherent limitation, we\npropose an estimation-based approach to segmentation. Specifically, we develop\na Bayesian method that estimates the posterior mean of fractional volume that\nthe tumor occupies within each image voxel. The proposed method, implemented\nusing an encoder-decoder network, was first evaluated using clinically\nrealistic 2-D simulation studies with known ground truth, in the context of\nsegmenting the primary tumor in PET images of patients with lung cancer. The\nevaluation studies demonstrated that the method accurately estimated the\ntumor-fraction areas and significantly outperformed widely used conventional\nmethods, including a U-net-based method, on the task of segmenting the tumor.\nIn addition, the proposed method was relatively insensitive to partial-volume\neffects and yielded reliable tumor segmentation for different clinical-scanner\nconfigurations. The method was then evaluated using clinical images of patients\nwith stage II and III non-small cell lung cancer from ACRIN 6668/RTOG 0235\nmulti-center clinical trial. Here, the results showed that the proposed method\nsignificantly outperformed all other considered methods and yielded accurate\ntumor segmentation on patient images with dice similarity coefficient of 0.82\n(95% CI: 0.78, 0.86). Overall, this study demonstrates the efficacy of the\nproposed method to accurately segment tumors in PET images.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 17:40:04 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2021 21:56:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Ziping", ""], ["Mhlanga", "Joyce C.", ""], ["Laforest", "Richard", ""], ["Derenoncourt", "Paul-Robert", ""], ["Siegel", "Barry A.", ""], ["Jha", "Abhinav K.", ""]]}, {"id": "2003.00321", "submitter": "Qingjie Meng", "authors": "Qingjie Meng and Daniel Rueckert and Bernhard Kainz", "title": "Learning Cross-domain Generalizable Features by Representation\n  Disentanglement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models exhibit limited generalizability across different\ndomains. Specifically, transferring knowledge from available entangled domain\nfeatures(source/target domain) and categorical features to new unseen\ncategorical features in a target domain is an interesting and difficult problem\nthat is rarely discussed in the current literature. This problem is essential\nfor many real-world applications such as improving diagnostic classification or\nprediction in medical imaging. To address this problem, we propose\nMutual-Information-based Disentangled Neural Networks (MIDNet) to extract\ngeneralizable features that enable transferring knowledge to unseen categorical\nfeatures in target domains. The proposed MIDNet is developed as a\nsemi-supervised learning paradigm to alleviate the dependency on labeled data.\nThis is important for practical applications where data annotation requires\nrare expertise as well as intense time and labor. We demonstrate our method on\nhandwritten digits datasets and a fetal ultrasound dataset for image\nclassification tasks. Experiments show that our method outperforms the\nstate-of-the-art and achieve expected performance with sparsely labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 17:53:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Meng", "Qingjie", ""], ["Rueckert", "Daniel", ""], ["Kainz", "Bernhard", ""]]}, {"id": "2003.00344", "submitter": "Ruwan Wickramarachchi", "authors": "Ruwan Wickramarachchi, Cory Henson, Amit Sheth", "title": "An Evaluation of Knowledge Graph Embeddings for Autonomous Driving Data:\n  Experience and Practice", "comments": "11 pages, To appear in AAAI 2020 Spring Symposium on Combining\n  Machine Learning and Knowledge Engineering in Practice (AAAI-MAKE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autonomous driving (AD) industry is exploring the use of knowledge graphs\n(KGs) to manage the vast amount of heterogeneous data generated from vehicular\nsensors. The various types of equipped sensors include video, LIDAR and RADAR.\nScene understanding is an important topic in AD which requires consideration of\nvarious aspects of a scene, such as detected objects, events, time and\nlocation. Recent work on knowledge graph embeddings (KGEs) - an approach that\nfacilitates neuro-symbolic fusion - has shown to improve the predictive\nperformance of machine learning models. With the expectation that\nneuro-symbolic fusion through KGEs will improve scene understanding, this\nresearch explores the generation and evaluation of KGEs for autonomous driving\ndata. We also present an investigation of the relationship between the level of\ninformational detail in a KG and the quality of its derivative embeddings. By\nsystematically evaluating KGEs along four dimensions -- i.e. quality metrics,\nKG informational detail, algorithms, and datasets -- we show that (1) higher\nlevels of informational detail in KGs lead to higher quality embeddings, (2)\ntype and relation semantics are better captured by the semantic transitional\ndistance-based TransE algorithm, and (3) some metrics, such as coherence\nmeasure, may not be suitable for intrinsically evaluating KGEs in this domain.\nAdditionally, we also present an (early) investigation of the usefulness of\nKGEs for two use-cases in the AD domain.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 20:33:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wickramarachchi", "Ruwan", ""], ["Henson", "Cory", ""], ["Sheth", "Amit", ""]]}, {"id": "2003.00351", "submitter": "Anamaria Radoi", "authors": "Nicolae-Catalin Ristea and Liviu Cristian Dutu and Anamaria Radoi", "title": "Emotion Recognition System from Speech and Visual Information based on\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/SPED.2019.8906538", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion recognition has become an important field of research in the\nhuman-computer interactions domain. The latest advancements in the field show\nthat combining visual with audio information lead to better results if compared\nto the case of using a single source of information separately. From a visual\npoint of view, a human emotion can be recognized by analyzing the facial\nexpression of the person. More precisely, the human emotion can be described\nthrough a combination of several Facial Action Units. In this paper, we propose\na system that is able to recognize emotions with a high accuracy rate and in\nreal time, based on deep Convolutional Neural Networks. In order to increase\nthe accuracy of the recognition system, we analyze also the speech data and\nfuse the information coming from both sources, i.e., visual and audio.\nExperimental results show the effectiveness of the proposed scheme for emotion\nrecognition and the importance of combining visual with audio data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 22:09:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ristea", "Nicolae-Catalin", ""], ["Dutu", "Liviu Cristian", ""], ["Radoi", "Anamaria", ""]]}, {"id": "2003.00378", "submitter": "Xiao Zhang", "authors": "Xiao Zhang, Jinghui Chen, Quanquan Gu, David Evans", "title": "Understanding the Intrinsic Robustness of Image Distributions using\n  Conditional Generative Models", "comments": "14 pages, 2 figures, 5 tables, AISTATS final paper reformatted for\n  readability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with Gilmer et al. (2018), several works have demonstrated the\ninevitability of adversarial examples based on different assumptions about the\nunderlying input probability space. It remains unclear, however, whether these\nresults apply to natural image distributions. In this work, we assume the\nunderlying data distribution is captured by some conditional generative model,\nand prove intrinsic robustness bounds for a general class of classifiers, which\nsolves an open problem in Fawzi et al. (2018). Building upon the\nstate-of-the-art conditional generative models, we study the intrinsic\nrobustness of two common image benchmarks under $\\ell_2$ perturbations, and\nshow the existence of a large gap between the robustness limits implied by our\ntheory and the adversarial robustness achieved by current state-of-the-art\nrobust models. Code for all our experiments is available at\nhttps://github.com/xiaozhanguva/Intrinsic-Rob.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 01:45:04 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhang", "Xiao", ""], ["Chen", "Jinghui", ""], ["Gu", "Quanquan", ""], ["Evans", "David", ""]]}, {"id": "2003.00380", "submitter": "Jieshan Chen", "authors": "Jieshan Chen, Chunyang Chen, Zhenchang Xing, Xiwei Xu, Liming Zhu,\n  Guoqiang Li, and Jinshui Wang", "title": "Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI\n  Components by Deep Learning", "comments": "Accepted to 42nd International Conference on Software Engineering", "journal-ref": null, "doi": "10.1145/3377811.3380327", "report-no": null, "categories": "cs.HC cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization(WHO), it is estimated that\napproximately 1.3 billion people live with some forms of vision impairment\nglobally, of whom 36 million are blind. Due to their disability, engaging these\nminority into the society is a challenging problem. The recent rise of smart\nmobile phones provides a new solution by enabling blind users' convenient\naccess to the information and service for understanding the world. Users with\nvision impairment can adopt the screen reader embedded in the mobile operating\nsystems to read the content of each screen within the app, and use gestures to\ninteract with the phone. However, the prerequisite of using screen readers is\nthat developers have to add natural-language labels to the image-based\ncomponents when they are developing the app. Unfortunately, more than 77% apps\nhave issues of missing labels, according to our analysis of 10,408 Android\napps. Most of these issues are caused by developers' lack of awareness and\nknowledge in considering the minority. And even if developers want to add the\nlabels to UI components, they may not come up with concise and clear\ndescription as most of them are of no visual issues. To overcome these\nchallenges, we develop a deep-learning based model, called LabelDroid, to\nautomatically predict the labels of image-based buttons by learning from\nlarge-scale commercial apps in Google Play. The experimental results show that\nour model can make accurate predictions and the generated labels are of higher\nquality than that from real Android developers.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 02:31:26 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:38:28 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Chen", "Jieshan", ""], ["Chen", "Chunyang", ""], ["Xing", "Zhenchang", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""], ["Li", "Guoqiang", ""], ["Wang", "Jinshui", ""]]}, {"id": "2003.00387", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Qin Jin, Peng Wang, Qi Wu", "title": "Say As You Wish: Fine-grained Control of Image Caption Generation with\n  Abstract Scene Graphs", "comments": "To be appeared in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to describe image contents with coarse to fine details as\nthey wish. However, most image captioning models are intention-agnostic which\ncan not generate diverse descriptions according to different user intentions\ninitiatively. In this work, we propose the Abstract Scene Graph (ASG) structure\nto represent user intention in fine-grained level and control what and how\ndetailed the generated description should be. The ASG is a directed graph\nconsisting of three types of \\textbf{abstract nodes} (object, attribute,\nrelationship) grounded in the image without any concrete semantic labels. Thus\nit is easy to obtain either manually or automatically. From the ASG, we propose\na novel ASG2Caption model, which is able to recognise user intentions and\nsemantics in the graph, and therefore generate desired captions according to\nthe graph structure. Our model achieves better controllability conditioning on\nASGs than carefully designed baselines on both VisualGenome and MSCOCO\ndatasets. It also significantly improves the caption diversity via\nautomatically sampling diverse ASGs as control signals.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 03:34:07 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Shizhe", ""], ["Jin", "Qin", ""], ["Wang", "Peng", ""], ["Wu", "Qi", ""]]}, {"id": "2003.00389", "submitter": "Mingkui Tan", "authors": "JieZhang Cao, Langyuan Mo, Qing Du, Yong Guo, Peilin Zhao, Junzhou\n  Huang, Mingkui Tan", "title": "Joint Wasserstein Distribution Matching", "comments": "This paper is accepted by Chinese Journal of Computers in 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint distribution matching (JDM) problem, which aims to learn bidirectional\nmappings to match joint distributions of two domains, occurs in many machine\nlearning and computer vision applications. This problem, however, is very\ndifficult due to two critical challenges: (i) it is often difficult to exploit\nsufficient information from the joint distribution to conduct the matching;\n(ii) this problem is hard to formulate and optimize. In this paper, relying on\noptimal transport theory, we propose to address JDM problem by minimizing the\nWasserstein distance of the joint distributions in two domains. However, the\nresultant optimization problem is still intractable. We then propose an\nimportant theorem to reduce the intractable problem into a simple optimization\nproblem, and develop a novel method (called Joint Wasserstein Distribution\nMatching (JWDM)) to solve it. In the experiments, we apply our method to\nunsupervised image translation and cross-domain video synthesis. Both\nqualitative and quantitative comparisons demonstrate the superior performance\nof our method over several state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 03:39:00 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Cao", "JieZhang", ""], ["Mo", "Langyuan", ""], ["Du", "Qing", ""], ["Guo", "Yong", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.00392", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Yida Zhao, Qin Jin, Qi Wu", "title": "Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning", "comments": "To be appeared in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal retrieval between videos and texts has attracted growing\nattentions due to the rapid emergence of videos on the web. The current\ndominant approach for this problem is to learn a joint embedding space to\nmeasure cross-modal similarities. However, simple joint embeddings are\ninsufficient to represent complicated visual and textual details, such as\nscenes, objects, actions and their compositions. To improve fine-grained\nvideo-text retrieval, we propose a Hierarchical Graph Reasoning (HGR) model,\nwhich decomposes video-text matching into global-to-local levels. To be\nspecific, the model disentangles texts into hierarchical semantic graph\nincluding three levels of events, actions, entities and relationships across\nlevels. Attention-based graph reasoning is utilized to generate hierarchical\ntextual embeddings, which can guide the learning of diverse and hierarchical\nvideo representations. The HGR model aggregates matchings from different\nvideo-text levels to capture both global and local details. Experimental\nresults on three video-text datasets demonstrate the advantages of our model.\nSuch hierarchical decomposition also enables better generalization across\ndatasets and improves the ability to distinguish fine-grained semantic\ndifferences.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 03:44:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Shizhe", ""], ["Zhao", "Yida", ""], ["Jin", "Qin", ""], ["Wu", "Qi", ""]]}, {"id": "2003.00393", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, Sotaro Tsukizawa", "title": "Deep Active Learning for Biased Datasets via Fisher Kernel\n  Self-Supervision", "comments": "Accepted to CVPR 2020. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning (AL) aims to minimize labeling efforts for data-demanding\ndeep neural networks (DNNs) by selecting the most representative data points\nfor annotation. However, currently used methods are ill-equipped to deal with\nbiased data. The main motivation of this paper is to consider a realistic\nsetting for pool-based semi-supervised AL, where the unlabeled collection of\ntrain data is biased. We theoretically derive an optimal acquisition function\nfor AL in this setting. It can be formulated as distribution shift minimization\nbetween unlabeled train data and weakly-labeled validation dataset. To\nimplement such acquisition function, we propose a low-complexity method for\nfeature density matching using self-supervised Fisher kernel (FK) as well as\nseveral novel pseudo-label estimators. Our FK-based method outperforms\nstate-of-the-art methods on MNIST, SVHN, and ImageNet classification while\nrequiring only 1/10th of processing. The conducted experiments show at least\n40% drop in labeling efforts for the biased class-imbalanced data compared to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 03:56:32 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Hodgkinson", "Alec", ""], ["Yamaguchi", "Takuya", ""], ["Tsukizawa", "Sotaro", ""]]}, {"id": "2003.00397", "submitter": "Qi Wu", "authors": "Qi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, Mingkui Tan", "title": "Intelligent Home 3D: Automatic 3D-House Design from Linguistic\n  Descriptions Only", "comments": "To appear in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home design is a complex task that normally requires architects to finish\nwith their professional skills and tools. It will be fascinating that if one\ncan produce a house plan intuitively without knowing much knowledge about home\ndesign and experience of using complex designing tools, for example, via\nnatural language. In this paper, we formulate it as a language conditioned\nvisual content generation problem that is further divided into a floor plan\ngeneration and an interior texture (such as floor and wall) synthesis task. The\nonly control signal of the generation process is the linguistic expression\ngiven by users that describe the house details. To this end, we propose a House\nPlan Generative Model (HPGM) that first translates the language input to a\nstructural graph representation and then predicts the layout of rooms with a\nGraph Conditioned Layout Prediction Network (GC LPN) and generates the interior\ntexture with a Language Conditioned Texture GAN (LCT-GAN). With some\npost-processing, the final product of this task is a 3D house model. To train\nand evaluate our model, we build the first Text-to-3D House Model dataset.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:28:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Qi", ""], ["Wu", "Qi", ""], ["Tang", "Rui", ""], ["Wang", "Yuhan", ""], ["Wang", "Shuai", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.00402", "submitter": "Ryo Kamoi", "authors": "Ryo Kamoi, Kei Kobayashi", "title": "Why is the Mahalanobis Distance Effective for Anomaly Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mahalanobis distance-based confidence score, a recently proposed anomaly\ndetection method for pre-trained neural classifiers, achieves state-of-the-art\nperformance on both out-of-distribution (OoD) and adversarial examples\ndetection. This work analyzes why this method exhibits such strong performance\nin practical settings while imposing an implausible assumption; namely, that\nclass conditional distributions of pre-trained features have tied covariance.\nAlthough the Mahalanobis distance-based method is claimed to be motivated by\nclassification prediction confidence, we find that its superior performance\nstems from information not useful for classification. This suggests that the\nreason the Mahalanobis confidence score works so well is mistaken, and makes\nuse of different information from ODIN, another popular OoD detection method\nbased on prediction confidence. This perspective motivates us to combine these\ntwo methods, and the combined detector exhibits improved performance and\nrobustness. These findings provide insight into the behavior of neural\nclassifiers in response to anomalous inputs.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:48:36 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 11:42:33 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Kamoi", "Ryo", ""], ["Kobayashi", "Kei", ""]]}, {"id": "2003.00403", "submitter": "Qi Wu", "authors": "Zhenfang Chen, Peng Wang, Lin Ma, Kwan-Yee K. Wong, Qi Wu", "title": "Cops-Ref: A new Dataset and Task on Compositional Referring Expression\n  Comprehension", "comments": "To appear in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension (REF) aims at identifying a particular\nobject in a scene by a natural language expression. It requires joint reasoning\nover the textual and visual domains to solve the problem. Some popular\nreferring expression datasets, however, fail to provide an ideal test bed for\nevaluating the reasoning ability of the models, mainly because 1) their\nexpressions typically describe only some simple distinctive properties of the\nobject and 2) their images contain limited distracting information. To bridge\nthe gap, we propose a new dataset for visual reasoning in context of referring\nexpression comprehension with two main features. First, we design a novel\nexpression engine rendering various reasoning logics that can be flexibly\ncombined with rich visual properties to generate expressions with varying\ncompositionality. Second, to better exploit the full reasoning chain embodied\nin an expression, we propose a new test setting by adding additional\ndistracting images containing objects sharing similar properties with the\nreferent, thus minimising the success rate of reasoning-free cross-domain\nalignment. We evaluate several state-of-the-art REF models, but find none of\nthem can achieve promising performance. A proposed modular hard mining strategy\nperforms the best but still leaves substantial room for improvement. We hope\nthis new dataset and task can serve as a benchmark for deeper visual reasoning\nanalysis and foster the research on referring expression comprehension.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:59:38 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Zhenfang", ""], ["Wang", "Peng", ""], ["Ma", "Lin", ""], ["Wong", "Kwan-Yee K.", ""], ["Wu", "Qi", ""]]}, {"id": "2003.00406", "submitter": "Xiao Wang", "authors": "Sulan Zhai, Shunqiang Liu, Xiao Wang, Jin Tang", "title": "FMT:Fusing Multi-task Convolutional Neural Network for Person Search", "comments": "Published on Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person search is to detect all persons and identify the query persons from\ndetected persons in the image without proposals and bounding boxes, which is\ndifferent from person re-identification. In this paper, we propose a fusing\nmulti-task convolutional neural network(FMT-CNN) to tackle the correlation and\nheterogeneity of detection and re-identification with a single convolutional\nneural network. We focus on how the interplay of person detection and person\nre-identification affects the overall performance. We employ person labels in\nregion proposal network to produce features for person re-identification and\nperson detection network, which can improve the accuracy of detection and\nre-identification simultaneously. We also use a multiple loss to train our\nre-identification network. Experiment results on CUHK-SYSU Person Search\ndataset show that the performance of our proposed method is superior to\nstate-of-the-art approaches in both mAP and top-1.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 05:20:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhai", "Sulan", ""], ["Liu", "Shunqiang", ""], ["Wang", "Xiao", ""], ["Tang", "Jin", ""]]}, {"id": "2003.00410", "submitter": "Xinyi Le", "authors": "Zitian Huang, Yikuan Yu, Jiawen Xu, Feng Ni, and Xinyi Le", "title": "PF-Net: Point Fractal Network for 3D Point Cloud Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Point Fractal Network (PF-Net), a novel\nlearning-based approach for precise and high-fidelity point cloud completion.\nUnlike existing point cloud completion networks, which generate the overall\nshape of the point cloud from the incomplete point cloud and always change\nexisting points and encounter noise and geometrical loss, PF-Net preserves the\nspatial arrangements of the incomplete point cloud and can figure out the\ndetailed geometrical structure of the missing region(s) in the prediction. To\nsucceed at this task, PF-Net estimates the missing point cloud hierarchically\nby utilizing a feature-points-based multi-scale generating network. Further, we\nadd up multi-stage completion loss and adversarial loss to generate more\nrealistic missing region(s). The adversarial loss can better tackle multiple\nmodes in the prediction. Our experiments demonstrate the effectiveness of our\nmethod for several challenging point cloud completion tasks.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 05:40:21 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Huang", "Zitian", ""], ["Yu", "Yikuan", ""], ["Xu", "Jiawen", ""], ["Ni", "Feng", ""], ["Le", "Xinyi", ""]]}, {"id": "2003.00418", "submitter": "Rudrabha Mukhopadhyay", "authors": "Prajwal K R, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay\n  Namboodiri, C.V. Jawahar", "title": "Towards Automatic Face-to-Face Translation", "comments": "9 pages (including references), 5 figures, Published in ACM\n  Multimedia, 2019", "journal-ref": "MM '19: Proceedings of the 27th ACM International Conference on\n  Multimedia; October 2019; Pages 1428-1436", "doi": "10.1145/3343031.3351066", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In light of the recent breakthroughs in automatic machine translation\nsystems, we propose a novel approach that we term as \"Face-to-Face\nTranslation\". As today's digital communication becomes increasingly visual, we\nargue that there is a need for systems that can automatically translate a video\nof a person speaking in language A into a target language B with realistic lip\nsynchronization. In this work, we create an automatic pipeline for this problem\nand demonstrate its impact on multiple real-world applications. First, we build\na working speech-to-speech translation system by bringing together multiple\nexisting modules from speech and language. We then move towards \"Face-to-Face\nTranslation\" by incorporating a novel visual module, LipGAN for generating\nrealistic talking faces from the translated audio. Quantitative evaluation of\nLipGAN on the standard LRW test set shows that it significantly outperforms\nexisting approaches across all standard metrics. We also subject our\nFace-to-Face Translation pipeline, to multiple human evaluations and show that\nit can significantly improve the overall user experience for consuming and\ninteracting with multimodal content across languages. Code, models and demo\nvideo are made publicly available.\n  Demo video: https://www.youtube.com/watch?v=aHG6Oei8jF0\n  Code and models: https://github.com/Rudrabha/LipGAN\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 06:42:43 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["R", "Prajwal K", ""], ["Mukhopadhyay", "Rudrabha", ""], ["Philip", "Jerin", ""], ["Jha", "Abhishek", ""], ["Namboodiri", "Vinay", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2003.00425", "submitter": "Burak Uzkent", "authors": "Burak Uzkent, Stefano Ermon", "title": "Learning When and Where to Zoom with Deep Reinforcement Learning", "comments": "To appear in CVPR 2020 as an Oral Presentation. The code can be found\n  at https://github.com/ermongroup/PatchDrop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high resolution images contain semantically more useful information\nthan their lower resolution counterparts, processing them is computationally\nmore expensive, and in some applications, e.g. remote sensing, they can be much\nmore expensive to acquire. For these reasons, it is desirable to develop an\nautomatic method to selectively use high resolution data when necessary while\nmaintaining accuracy and reducing acquisition/run-time cost. In this direction,\nwe propose PatchDrop a reinforcement learning approach to dynamically identify\nwhen and where to use/acquire high resolution data conditioned on the paired,\ncheap, low resolution images. We conduct experiments on CIFAR10, CIFAR100,\nImageNet and fMoW datasets where we use significantly less high resolution data\nwhile maintaining similar accuracy to models which use full high resolution\nimages.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 07:16:46 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 18:25:16 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Uzkent", "Burak", ""], ["Ermon", "Stefano", ""]]}, {"id": "2003.00429", "submitter": "Xinwei Chen", "authors": "Xinwei Chen, Ali Taleb Zadeh Kasgari and Walid Saad", "title": "Deep Learning for Content-based Personalized Viewport Prediction of\n  360-Degree VR Videos", "comments": null, "journal-ref": null, "doi": "10.1109/LNET.2020.2977124", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the problem of head movement prediction for virtual reality\nvideos is studied. In the considered model, a deep learning network is\nintroduced to leverage position data as well as video frame content to predict\nfuture head movement. For optimizing data input into this neural network, data\nsample rate, reduced data, and long-period prediction length are also explored\nfor this model. Simulation results show that the proposed approach yields\n16.1\\% improvement in terms of prediction accuracy compared to a baseline\napproach that relies only on the position data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 07:31:50 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Xinwei", ""], ["Kasgari", "Ali Taleb Zadeh", ""], ["Saad", "Walid", ""]]}, {"id": "2003.00434", "submitter": "Xiaolin Song", "authors": "Xiaolin Song, Yuyang Zhao, and Jingyu Yang", "title": "STC-Flow: Spatio-temporal Context-aware Optical Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spatio-temporal contextual network, STC-Flow, for\noptical flow estimation. Unlike previous optical flow estimation approaches\nwith local pyramid feature extraction and multi-level correlation, we propose a\ncontextual relation exploration architecture by capturing rich long-range\ndependencies in spatial and temporal dimensions. Specifically, STC-Flow\ncontains three key context modules - pyramidal spatial context module, temporal\ncontext correlation module and recurrent residual contextual upsampling module,\nto build the relationship in each stage of feature extraction, correlation, and\nflow reconstruction, respectively. Experimental results indicate that the\nproposed scheme achieves the state-of-the-art performance of two-frame based\nmethods on the Sintel dataset and the KITTI 2012/2015 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 08:18:57 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 09:11:06 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Song", "Xiaolin", ""], ["Zhao", "Yuyang", ""], ["Yang", "Jingyu", ""]]}, {"id": "2003.00443", "submitter": "Xin Eric Wang", "authors": "Xin Eric Wang, Vihan Jain, Eugene Ie, William Yang Wang, Zornitsa\n  Kozareva, Sujith Ravi", "title": "Environment-agnostic Multitask Learning for Natural Language Grounded\n  Navigation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent research efforts enable study for natural language grounded navigation\nin photo-realistic environments, e.g., following natural language instructions\nor dialog. However, existing methods tend to overfit training data in seen\nenvironments and fail to generalize well in previously unseen environments. To\nclose the gap between seen and unseen environments, we aim at learning a\ngeneralized navigation model from two novel perspectives: (1) we introduce a\nmultitask navigation model that can be seamlessly trained on both\nVision-Language Navigation (VLN) and Navigation from Dialog History (NDH)\ntasks, which benefits from richer natural language guidance and effectively\ntransfers knowledge across tasks; (2) we propose to learn environment-agnostic\nrepresentations for the navigation policy that are invariant among the\nenvironments seen during training, thus generalizing better on unseen\nenvironments. Extensive experiments show that environment-agnostic multitask\nlearning significantly reduces the performance gap between seen and unseen\nenvironments, and the navigation agent trained so outperforms baselines on\nunseen environments by 16% (relative measure on success rate) on VLN and 120%\n(goal progress) on NDH. Our submission to the CVDN leaderboard establishes a\nnew state-of-the-art for the NDH task on the holdout test set. Code is\navailable at https://github.com/google-research/valan.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 09:06:31 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 22:06:54 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2020 18:20:39 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 23:54:02 GMT"}, {"version": "v5", "created": "Tue, 21 Jul 2020 02:54:38 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Xin Eric", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Wang", "William Yang", ""], ["Kozareva", "Zornitsa", ""], ["Ravi", "Sujith", ""]]}, {"id": "2003.00470", "submitter": "Takuya Isomura", "authors": "Takuya Isomura, Taro Toyoizumi", "title": "Dimensionality reduction to maximize prediction generalization\n  capability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops an analytically solvable unsupervised learning scheme that\nextracts the most informative components for predicting future inputs, termed\npredictive principal component analysis (PredPCA). Our scheme can effectively\nremove unpredictable observation noise and globally minimize the test\nprediction error. Mathematical analyses demonstrate that, with sufficiently\nhigh-dimensional observations that are generated by a linear or nonlinear\nsystem, PredPCA can identify the optimal hidden state representation, true\nsystem parameters, and true hidden state dimensionality, with a global\nconvergence guarantee. We demonstrate the performance of PredPCA by using\nsequential visual inputs comprising hand-digits, rotating 3D objects, and\nnatural scenes. It reliably and accurately estimates distinct hidden states and\npredicts future outcomes of previously unseen test input data, even in the\npresence of considerable observation noise. The simple model structure and low\ncomputational cost of PredPCA make it highly desirable as a learning scheme for\nbiological neural networks and neuromorphic chips.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 12:04:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Isomura", "Takuya", ""], ["Toyoizumi", "Taro", ""]]}, {"id": "2003.00482", "submitter": "Xi Chen", "authors": "Xi Chen, Zuoxin Li, Ye Yuan, Gang Yu, Jianxin Shen, Donglian Qi", "title": "State-Aware Tracker for Real-Time Video Object Segmentation", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the task of semi-supervised video object\nsegmentation(VOS) and explore how to make efficient use of video property to\ntackle the challenge of semi-supervision. We propose a novel pipeline called\nState-Aware Tracker(SAT), which can produce accurate segmentation results with\nreal-time speed. For higher efficiency, SAT takes advantage of the inter-frame\nconsistency and deals with each target object as a tracklet. For more stable\nand robust performance over video sequences, SAT gets awareness for each state\nand makes self-adaptation via two feedback loops. One loop assists SAT in\ngenerating more stable tracklets. The other loop helps to construct a more\nrobust and holistic target representation. SAT achieves a promising result of\n72.3% J&F mean with 39 FPS on DAVIS2017-Val dataset, which shows a decent\ntrade-off between efficiency and accuracy. Code will be released at\ngithub.com/MegviiDetection/video_analyst.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 12:48:20 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Xi", ""], ["Li", "Zuoxin", ""], ["Yuan", "Ye", ""], ["Yu", "Gang", ""], ["Shen", "Jianxin", ""], ["Qi", "Donglian", ""]]}, {"id": "2003.00492", "submitter": "Zhen Li", "authors": "Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang and Shuguang Cui", "title": "PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks\n  with Adaptive Sampling", "comments": "To appear in CVPR 2020. Also seen in\n  http://kaldir.vc.in.tum.de/scannet_benchmark/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raw point clouds data inevitably contains outliers or noise through\nacquisition from 3D sensors or reconstruction algorithms. In this paper, we\npresent a novel end-to-end network for robust point clouds processing, named\nPointASNL, which can deal with point clouds with noise effectively. The key\ncomponent in our approach is the adaptive sampling (AS) module. It first\nre-weights the neighbors around the initial sampled points from farthest point\nsampling (FPS), and then adaptively adjusts the sampled points beyond the\nentire point cloud. Our AS module can not only benefit the feature learning of\npoint clouds, but also ease the biased effect of outliers. To further capture\nthe neighbor and long-range dependencies of the sampled point, we proposed a\nlocal-nonlocal (L-NL) module inspired by the nonlocal operation. Such L-NL\nmodule enables the learning process insensitive to noise. Extensive experiments\nverify the robustness and superiority of our approach in point clouds\nprocessing tasks regardless of synthesis data, indoor data, and outdoor data\nwith or without noise. Specifically, PointASNL achieves state-of-the-art robust\nperformance for classification and segmentation tasks on all datasets, and\nsignificantly outperforms previous methods on real-world outdoor SemanticKITTI\ndataset with considerate noise. Our code is released through\nhttps://github.com/yanx27/PointASNL.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 14:04:08 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 14:59:52 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 07:46:18 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Yan", "Xu", ""], ["Zheng", "Chaoda", ""], ["Li", "Zhen", ""], ["Wang", "Sheng", ""], ["Cui", "Shuguang", ""]]}, {"id": "2003.00504", "submitter": "Lei Tai", "authors": "Yongjian Chen and Lei Tai and Kai Sun and Mingyang Li", "title": "MonoPair: Monocular 3D Object Detection Using Pairwise Spatial\n  Relationships", "comments": "CVPR 2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D object detection is an essential component in autonomous driving\nwhile challenging to solve, especially for those occluded samples which are\nonly partially visible. Most detectors consider each 3D object as an\nindependent training target, inevitably resulting in a lack of useful\ninformation for occluded samples. To this end, we propose a novel method to\nimprove the monocular 3D object detection by considering the relationship of\npaired samples. This allows us to encode spatial constraints for\npartially-occluded objects from their adjacent neighbors. Specifically, the\nproposed detector computes uncertainty-aware predictions for object locations\nand 3D distances for the adjacent object pairs, which are subsequently jointly\noptimized by nonlinear least squares. Finally, the one-stage uncertainty-aware\nprediction structure and the post-optimization module are dedicatedly\nintegrated for ensuring the run-time efficiency. Experiments demonstrate that\nour method yields the best performance on KITTI 3D detection benchmark, by\noutperforming state-of-the-art competitors by wide margins, especially for the\nhard samples.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 15:37:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Yongjian", ""], ["Tai", "Lei", ""], ["Sun", "Kai", ""], ["Li", "Mingyang", ""]]}, {"id": "2003.00517", "submitter": "Yifan Chen", "authors": "Yifan Chen, Han Wang, Xiaolu Sun, Bin Fan, Chu Tang", "title": "Deep Attention Aware Feature Learning for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention has proven to be effective in improving the performance of\nperson re-identification. Most existing methods apply visual attention\nheuristically by learning an additional attention map to re-weight the feature\nmaps for person re-identification. However, this kind of methods inevitably\nincrease the model complexity and inference time. In this paper, we propose to\nincorporate the attention learning as additional objectives in a person ReID\nnetwork without changing the original structure, thus maintain the same\ninference time and model size. Two kinds of attentions have been considered to\nmake the learned feature maps being aware of the person and related body parts\nrespectively. Globally, a holistic attention branch (HAB) makes the feature\nmaps obtained by backbone focus on persons so as to alleviate the influence of\nbackground. Locally, a partial attention branch (PAB) makes the extracted\nfeatures be decoupled into several groups and be separately responsible for\ndifferent body parts (i.e., keypoints), thus increasing the robustness to pose\nvariation and partial occlusion. These two kinds of attentions are universal\nand can be incorporated into existing ReID networks. We have tested its\nperformance on two typical networks (TriNet and Bag of Tricks) and observed\nsignificant performance improvement on five widely used datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 16:27:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Yifan", ""], ["Wang", "Han", ""], ["Sun", "Xiaolu", ""], ["Fan", "Bin", ""], ["Tang", "Chu", ""]]}, {"id": "2003.00529", "submitter": "Zhenbo Xu", "authors": "Zhenbo Xu, Wei Zhang, Xiaoqing Ye, Xiao Tan, Wei Yang, Shilei Wen,\n  Errui Ding, Ajin Meng, Liusheng Huang", "title": "ZoomNet: Part-Aware Adaptive Zooming Neural Network for 3D Object\n  Detection", "comments": "Accpeted by AAAI 2020 as Oral presentation; The github page will be\n  updated in March,2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection is an essential task in autonomous driving and robotics.\nThough great progress has been made, challenges remain in estimating 3D pose\nfor distant and occluded objects. In this paper, we present a novel framework\nnamed ZoomNet for stereo imagery-based 3D detection. The pipeline of ZoomNet\nbegins with an ordinary 2D object detection model which is used to obtain pairs\nof left-right bounding boxes. To further exploit the abundant texture cues in\nRGB images for more accurate disparity estimation, we introduce a conceptually\nstraight-forward module -- adaptive zooming, which simultaneously resizes 2D\ninstance bounding boxes to a unified resolution and adjusts the camera\nintrinsic parameters accordingly. In this way, we are able to estimate\nhigher-quality disparity maps from the resized box images then construct dense\npoint clouds for both nearby and distant objects. Moreover, we introduce to\nlearn part locations as complementary features to improve the resistance\nagainst occlusion and put forward the 3D fitting score to better estimate the\n3D detection quality. Extensive experiments on the popular KITTI 3D detection\ndataset indicate ZoomNet surpasses all previous state-of-the-art methods by\nlarge margins (improved by 9.4% on APbv (IoU=0.7) over pseudo-LiDAR). Ablation\nstudy also demonstrates that our adaptive zooming strategy brings an\nimprovement of over 10% on AP3d (IoU=0.7). In addition, since the official\nKITTI benchmark lacks fine-grained annotations like pixel-wise part locations,\nwe also present our KFG dataset by augmenting KITTI with detailed instance-wise\nannotations including pixel-wise part location, pixel-wise disparity, etc..\nBoth the KFG dataset and our codes will be publicly available at\nhttps://github.com/detectRecog/ZoomNet.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 17:18:08 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xu", "Zhenbo", ""], ["Zhang", "Wei", ""], ["Ye", "Xiaoqing", ""], ["Tan", "Xiao", ""], ["Yang", "Wei", ""], ["Wen", "Shilei", ""], ["Ding", "Errui", ""], ["Meng", "Ajin", ""], ["Huang", "Liusheng", ""]]}, {"id": "2003.00535", "submitter": "Liang Du", "authors": "Liang Du, Jingang Tan, Xiangyang Xue, Lili Chen, Hongkai Wen, Jianfeng\n  Feng, Jiamao Li and Xiaolin Zhang", "title": "3DCFS: Fast and Robust Joint 3D Semantic-Instance Segmentation via\n  Coupled Feature Selection", "comments": "icra 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel fast and robust 3D point clouds segmentation framework via\ncoupled feature selection, named 3DCFS, that jointly performs semantic and\ninstance segmentation. Inspired by the human scene perception process, we\ndesign a novel coupled feature selection module, named CFSM, that adaptively\nselects and fuses the reciprocal semantic and instance features from two tasks\nin a coupled manner. To further boost the performance of the instance\nsegmentation task in our 3DCFS, we investigate a loss function that helps the\nmodel learn to balance the magnitudes of the output embedding dimensions during\ntraining, which makes calculating the Euclidean distance more reliable and\nenhances the generalizability of the model. Extensive experiments demonstrate\nthat our 3DCFS outperforms state-of-the-art methods on benchmark datasets in\nterms of accuracy, speed and computational cost.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 17:48:17 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Du", "Liang", ""], ["Tan", "Jingang", ""], ["Xue", "Xiangyang", ""], ["Chen", "Lili", ""], ["Wen", "Hongkai", ""], ["Feng", "Jianfeng", ""], ["Li", "Jiamao", ""], ["Zhang", "Xiaolin", ""]]}, {"id": "2003.00547", "submitter": "Dandan Li", "authors": "Yuan Zhou, Dandan Li, Shuwei Huo, and Sun-Yuan Kung", "title": "Soft-Root-Sign Activation Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation function in deep networks has a significant effect\non the training dynamics and task performance. At present, the most effective\nand widely-used activation function is ReLU. However, because of the non-zero\nmean, negative missing and unbounded output, ReLU is at a potential\ndisadvantage during optimization. To this end, we introduce a novel activation\nfunction to manage to overcome the above three challenges. The proposed\nnonlinearity, namely \"Soft-Root-Sign\" (SRS), is smooth, non-monotonic, and\nbounded. Notably, the bounded property of SRS distinguishes itself from most\nstate-of-the-art activation functions. In contrast to ReLU, SRS can adaptively\nadjust the output by a pair of independent trainable parameters to capture\nnegative information and provide zero-mean property, which leading not only to\nbetter generalization performance, but also to faster learning speed. It also\navoids and rectifies the output distribution to be scattered in the\nnon-negative real number space, making it more compatible with batch\nnormalization (BN) and less sensitive to initialization. In experiments, we\nevaluated SRS on deep networks applied to a variety of tasks, including image\nclassification, machine translation and generative modelling. Our SRS matches\nor exceeds models with ReLU and other state-of-the-art nonlinearities, showing\nthat the proposed activation function is generalized and can achieve high\nperformance across tasks. Ablation study further verified the compatibility\nwith BN and self-adaptability for different initialization.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 18:38:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhou", "Yuan", ""], ["Li", "Dandan", ""], ["Huo", "Shuwei", ""], ["Kung", "Sun-Yuan", ""]]}, {"id": "2003.00559", "submitter": "Sai Ravela", "authors": "Kshitij Bakliwal and Sai Ravela", "title": "The Sloop System for Individual Animal Identification with Deep Learning", "comments": "To appear in WACV 2020 Workshop on Deep Learning for\n  Re-Identification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT Sloop system indexes and retrieves photographs from databases of\nnon-stationary animal population distributions. To do this, it adaptively\nrepresents and matches generic visual feature representations using sparse\nrelevance feedback from experts and crowds. Here, we describe the Sloop system\nand its application, then compare its approach to a standard deep learning\nformulation. We then show that priming with amplitude and deformation features\nrequires very shallow networks to produce superior recognition results. Results\nsuggest that relevance feedback, which enables Sloop's high-recall performance\nmay also be essential for deep learning approaches to individual identification\nto deliver comparable results.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 19:08:06 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Bakliwal", "Kshitij", ""], ["Ravela", "Sai", ""]]}, {"id": "2003.00575", "submitter": "Lukas Hahn", "authors": "Frederik Hasecke and Lukas Hahn and Anton Kummert", "title": "FLIC: Fast Lidar Image Clustering", "comments": "9 pages, 10 figures, accepted to appear in ICPRAM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar sensors are widely used in various applications, ranging from\nscientific fields over industrial use to integration in consumer products. With\nan ever growing number of different driver assistance systems, they have been\nintroduced to automotive series production in recent years and are considered\nan important building block for the practical realisation of autonomous\ndriving. However, due to the potentially large amount of Lidar points per scan,\ntailored algorithms are required to identify objects (e.g. pedestrians or\nvehicles) with high precision in a very short time. In this work, we propose an\nalgorithmic approach for real-time instance segmentation of Lidar sensor data.\nWe show how our method leverages the properties of the Euclidean distance to\nretain three-dimensional measurement information, while being narrowed down to\na two-dimensional representation for fast computation. We further introduce\nwhat we call \"skip connections\", to make our approach robust against\nover-segmentation and improve assignment in cases of partial occlusion. Through\ndetailed evaluation on public data and comparison with established methods, we\nshow how these aspects enable state-of-the-art performance and runtime on a\nsingle CPU core.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 20:21:31 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 14:11:20 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Hasecke", "Frederik", ""], ["Hahn", "Lukas", ""], ["Kummert", "Anton", ""]]}, {"id": "2003.00594", "submitter": "Maike Stern", "authors": "Maike Lorena Stern, Hans Lindberg, Klaus Meyer-Wegener", "title": "Rethinking Fully Convolutional Networks for the Analysis of\n  Photoluminescence Wafer Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The manufacturing of light-emitting diodes is a complex\nsemiconductor-manufacturing process, interspersed with different measurements.\nAmong the employed measurements, photoluminescence imaging has several\nadvantages, namely being a non-destructive, fast and thus cost-effective\nmeasurement. On a photoluminescence measurement image of an LED wafer, every\npixel corresponds to an LED chip's brightness after photo-excitation, revealing\nchip performance information. However, generating a chip-fine defect map of the\nLED wafer, based on photoluminescence images, proves challenging for multiple\nreasons: on the one hand, the measured brightness values vary from image to\nimage, in addition to local spots of differing brightness. On the other hand,\ncertain defect structures may assume multiple shapes, sizes and brightness\ngradients, where salient brightness values may correspond to defective LED\nchips, measurement artefacts or non-defective structures. In this work, we\nrevisit the creation of chip-fine defect maps using fully convolutional\nnetworks and show that the problem of segmenting objects at multiple scales can\nbe improved by the incorporation of densely connected convolutional blocks and\natrous spatial pyramid pooling modules. We also share implementation details\nand our experiences with training networks with small datasets of measurement\nimages. The proposed architecture significantly improves the segmentation\naccuracy of highly variable defect structures over our previous version.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 21:31:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Stern", "Maike Lorena", ""], ["Lindberg", "Hans", ""], ["Meyer-Wegener", "Klaus", ""]]}, {"id": "2003.00601", "submitter": "Siheng Chen", "authors": "Siheng Chen and Baoan Liu and Chen Feng and Carlos Vallespi-Gonzalez\n  and Carl Wellington", "title": "3D Point Cloud Processing and Learning for Autonomous Driving", "comments": "IEEE Signal Processing Magazine, Special Issue on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a review of 3D point cloud processing and learning for autonomous\ndriving. As one of the most important sensors in autonomous vehicles, light\ndetection and ranging (LiDAR) sensors collect 3D point clouds that precisely\nrecord the external surfaces of objects and scenes. The tools for 3D point\ncloud processing and learning are critical to the map creation, localization,\nand perception modules in an autonomous vehicle. While much attention has been\npaid to data collected from cameras, such as images and videos, an increasing\nnumber of researchers have recognized the importance and significance of LiDAR\nin autonomous driving and have proposed processing and learning algorithms to\nexploit 3D point clouds. We review the recent progress in this research area\nand summarize what has been tried and what is needed for practical and safe\nautonomous vehicles. We also offer perspectives on open issues that are needed\nto be solved in the future.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 22:13:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Siheng", ""], ["Liu", "Baoan", ""], ["Feng", "Chen", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl", ""]]}, {"id": "2003.00615", "submitter": "Jia Peng", "authors": "Peng Jia, Xuebo Wu, Yi Huang, Bojun Cai, Dongmei Cai", "title": "PSF--NET: A Non-parametric Point Spread Function Model for Ground Based\n  Optical Telescopes", "comments": "Accepted by AJ. The complete code can be downloaded at\n  DOI:10.12149/101014", "journal-ref": null, "doi": "10.3847/1538-3881/ab7b79", "report-no": null, "categories": "astro-ph.IM astro-ph.SR cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ground based optical telescopes are seriously affected by atmospheric\nturbulence induced aberrations. Understanding properties of these aberrations\nis important both for instruments design and image restoration methods\ndevelopment. Because the point spread function can reflect performance of the\nwhole optic system, it is appropriate to use the point spread function to\ndescribe atmospheric turbulence induced aberrations. Assuming point spread\nfunctions induced by the atmospheric turbulence with the same profile belong to\nthe same manifold space, we propose a non-parametric point spread function --\nPSF-NET. The PSF-NET has a cycle convolutional neural network structure and is\na statistical representation of the manifold space of PSFs induced by the\natmospheric turbulence with the same profile. Testing the PSF-NET with\nsimulated and real observation data, we find that a well trained PSF--NET can\nrestore any short exposure images blurred by atmospheric turbulence with the\nsame profile. Besides, we further use the impulse response of the PSF-NET,\nwhich can be viewed as the statistical mean PSF, to analyze interpretation\nproperties of the PSF-NET. We find that variations of statistical mean PSFs are\ncaused by variations of the atmospheric turbulence profile: as the difference\nof the atmospheric turbulence profile increases, the difference between\nstatistical mean PSFs also increases. The PSF-NET proposed in this paper\nprovides a new way to analyze atmospheric turbulence induced aberrations, which\nwould be benefit to develop new observation methods for ground based optical\ntelescopes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 00:17:25 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Jia", "Peng", ""], ["Wu", "Xuebo", ""], ["Huang", "Yi", ""], ["Cai", "Bojun", ""], ["Cai", "Dongmei", ""]]}, {"id": "2003.00619", "submitter": "Xingtong Liu", "authors": "Xingtong Liu, Yiping Zheng, Benjamin Killeen, Masaru Ishii, Gregory D.\n  Hager, Russell H. Taylor, Mathias Unberath", "title": "Extremely Dense Point Correspondences using a Learned Feature Descriptor", "comments": "The work has been accepted for publication in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-quality 3D reconstructions from endoscopy video play an important role\nin many clinical applications, including surgical navigation where they enable\ndirect video-CT registration. While many methods exist for general multi-view\n3D reconstruction, these methods often fail to deliver satisfactory performance\non endoscopic video. Part of the reason is that local descriptors that\nestablish pair-wise point correspondences, and thus drive reconstruction,\nstruggle when confronted with the texture-scarce surface of anatomy.\nLearning-based dense descriptors usually have larger receptive fields enabling\nthe encoding of global information, which can be used to disambiguate matches.\nIn this work, we present an effective self-supervised training scheme and novel\nloss design for dense descriptor learning. In direct comparison to recent local\nand dense descriptors on an in-house sinus endoscopy dataset, we demonstrate\nthat our proposed dense descriptor can generalize to unseen patients and\nscopes, thereby largely improving the performance of Structure from Motion\n(SfM) in terms of model density and completeness. We also evaluate our method\non a public dense optical flow dataset and a small-scale SfM public dataset to\nfurther demonstrate the effectiveness and generality of our method. The source\ncode is available at\nhttps://github.com/lppllppl920/DenseDescriptorLearning-Pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 00:44:04 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 17:52:40 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Liu", "Xingtong", ""], ["Zheng", "Yiping", ""], ["Killeen", "Benjamin", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""]]}, {"id": "2003.00636", "submitter": "Fang Xu", "authors": "Fang Xu, Shijie Lin, Wen Yang, Lei Yu, Dengxin Dai, Gui-song Xia", "title": "Matching Neuromorphic Events and Color Images via Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The event camera has appealing properties: high dynamic range, low latency,\nlow power consumption and low memory usage, and thus provides complementariness\nto conventional frame-based cameras. It only captures the dynamics of a scene\nand is able to capture almost \"continuous\" motion. However, different from\nframe-based camera that reflects the whole appearance as scenes are, the event\ncamera casts away the detailed characteristics of objects, such as texture and\ncolor. To take advantages of both modalities, the event camera and frame-based\ncamera are combined together for various machine vision tasks. Then the\ncross-modal matching between neuromorphic events and color images plays a vital\nand essential role. In this paper, we propose the Event-Based Image Retrieval\n(EBIR) problem to exploit the cross-modal matching task. Given an event stream\ndepicting a particular object as query, the aim is to retrieve color images\ncontaining the same object. This problem is challenging because there exists a\nlarge modality gap between neuromorphic events and color images. We address the\nEBIR problem by proposing neuromorphic Events-Color image Feature Learning\n(ECFL). Particularly, the adversarial learning is employed to jointly model\nneuromorphic events and color images into a common embedding space. We also\ncontribute to the community N-UKbench and EC180 dataset to promote the\ndevelopment of EBIR problem. Extensive experiments on our datasets show that\nthe proposed method is superior in learning effective modality-invariant\nrepresentation to link two different modalities.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 02:48:56 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xu", "Fang", ""], ["Lin", "Shijie", ""], ["Yang", "Wen", ""], ["Yu", "Lei", ""], ["Dai", "Dengxin", ""], ["Xia", "Gui-song", ""]]}, {"id": "2003.00637", "submitter": "Shunping Ji", "authors": "Jin Liu and Shunping Ji", "title": "A Novel Recurrent Encoder-Decoder Structure for Large-Scale Multi-view\n  Stereo Reconstruction from An Open Aerial Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great deal of research has demonstrated recently that multi-view stereo\n(MVS) matching can be solved with deep learning methods. However, these efforts\nwere focused on close-range objects and only a very few of the deep\nlearning-based methods were specifically designed for large-scale 3D urban\nreconstruction due to the lack of multi-view aerial image benchmarks. In this\npaper, we present a synthetic aerial dataset, called the WHU dataset, we\ncreated for MVS tasks, which, to our knowledge, is the first large-scale\nmulti-view aerial dataset. It was generated from a highly accurate 3D digital\nsurface model produced from thousands of real aerial images with precise camera\nparameters. We also introduce in this paper a novel network, called RED-Net,\nfor wide-range depth inference, which we developed from a recurrent\nencoder-decoder structure to regularize cost maps across depths and a 2D fully\nconvolutional network as framework. RED-Net's low memory requirements and high\nperformance make it suitable for large-scale and highly accurate 3D Earth\nsurface reconstruction. Our experiments confirmed that not only did our method\nexceed the current state-of-the-art MVS methods by more than 50% mean absolute\nerror (MAE) with less memory and computational cost, but its efficiency as\nwell. It outperformed one of the best commercial software programs based on\nconventional methods, improving their efficiency 16 times over. Moreover, we\nproved that our RED-Net model pre-trained on the synthetic WHU dataset can be\nefficiently transferred to very different multi-view aerial image datasets\nwithout any fine-tuning. Dataset are available at http://gpcv.whu.edu.cn/data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 03:04:13 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:11:01 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2020 04:27:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Liu", "Jin", ""], ["Ji", "Shunping", ""]]}, {"id": "2003.00641", "submitter": "Hiroki Kawai", "authors": "Hiroki Kawai, Jiawei Chen, Prakash Ishwar, Janusz Konrad", "title": "VAE/WGAN-Based Image Representation Learning For Pose-Preserving\n  Seamless Identity Replacement In Facial Images", "comments": "6 pages, 5 figures, 2019 IEEE 29th International Workshop on Machine\n  Learning for Signal Processing (MLSP)", "journal-ref": "2019 IEEE 29th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": "10.1109/MLSP.2019.8918926", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel variational generative adversarial network (VGAN) based on\nWasserstein loss to learn a latent representation from a face image that is\ninvariant to identity but preserves head-pose information. This facilitates\nsynthesis of a realistic face image with the same head pose as a given input\nimage, but with a different identity. One application of this network is in\nprivacy-sensitive scenarios; after identity replacement in an image, utility,\nsuch as head pose, can still be recovered. Extensive experimental validation on\nsynthetic and real human-face image datasets performed under 3 threat scenarios\nconfirms the ability of the proposed network to preserve head pose of the input\nimage, mask the input identity, and synthesize a good-quality realistic face\nimage of a desired identity. We also show that our network can be used to\nperform pose-preserving identity morphing and identity-preserving pose\nmorphing. The proposed method improves over a recent state-of-the-art method in\nterms of quantitative metrics as well as synthesized image quality.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 03:35:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kawai", "Hiroki", ""], ["Chen", "Jiawei", ""], ["Ishwar", "Prakash", ""], ["Konrad", "Janusz", ""]]}, {"id": "2003.00651", "submitter": "Zuyao Chen", "authors": "Zuyao Chen, Qianqian Xu, Runmin Cong, Qingming Huang", "title": "Global Context-Aware Progressive Aggregation Network for Salient Object\n  Detection", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have achieved competitive performance in\nsalient object detection, in which how to learn effective and comprehensive\nfeatures plays a critical role. Most of the previous works mainly adopted\nmultiple level feature integration yet ignored the gap between different\nfeatures. Besides, there also exists a dilution process of high-level features\nas they passed on the top-down pathway. To remedy these issues, we propose a\nnovel network named GCPANet to effectively integrate low-level appearance\nfeatures, high-level semantic features, and global context features through\nsome progressive context-aware Feature Interweaved Aggregation (FIA) modules\nand generate the saliency map in a supervised way. Moreover, a Head Attention\n(HA) module is used to reduce information redundancy and enhance the top layers\nfeatures by leveraging the spatial and channel-wise attention, and the Self\nRefinement (SR) module is utilized to further refine and heighten the input\nfeatures. Furthermore, we design the Global Context Flow (GCF) module to\ngenerate the global context information at different stages, which aims to\nlearn the relationship among different salient regions and alleviate the\ndilution effect of high-level features. Experimental results on six benchmark\ndatasets demonstrate that the proposed approach outperforms the\nstate-of-the-art methods both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 04:26:10 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Zuyao", ""], ["Xu", "Qianqian", ""], ["Cong", "Runmin", ""], ["Huang", "Qingming", ""]]}, {"id": "2003.00667", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Michael Milford", "title": "MVP: Unified Motion and Visual Self-Supervised Learning for Large-Scale\n  Robotic Navigation", "comments": "Under review at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Autonomous navigation emerges from both motion and local visual perception in\nreal-world environments. However, most successful robotic motion estimation\nmethods (e.g. VO, SLAM, SfM) and vision systems (e.g. CNN, visual place\nrecognition-VPR) are often separately used for mapping and localization tasks.\nConversely, recent reinforcement learning (RL) based methods for visual\nnavigation rely on the quality of GPS data reception, which may not be reliable\nwhen directly using it as ground truth across multiple, month-spaced traversals\nin large environments. In this paper, we propose a novel motion and visual\nperception approach, dubbed MVP, that unifies these two sensor modalities for\nlarge-scale, target-driven navigation tasks. Our MVP-based method can learn\nfaster, and is more accurate and robust to both extreme environmental changes\nand poor GPS data than corresponding vision-only navigation methods. MVP\ntemporally incorporates compact image representations, obtained using VPR, with\noptimized motion estimation data, including but not limited to those from VO or\noptimized radar odometry (RO), to efficiently learn self-supervised navigation\npolicies via RL. We evaluate our method on two large real-world datasets,\nOxford Robotcar and Nordland Railway, over a range of weather (e.g. overcast,\nnight, snow, sun, rain, clouds) and seasonal (e.g. winter, spring, fall,\nsummer) conditions using the new CityLearn framework; an interactive\nenvironment for efficiently training navigation agents. Our experimental\nresults, on traversals of the Oxford RobotCar dataset with no GPS data, show\nthat MVP can achieve 53% and 93% navigation success rate using VO and RO,\nrespectively, compared to 7% for a vision-only method. We additionally report a\ntrade-off between the RL success rate and the motion estimation precision.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:19:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "2003.00678", "submitter": "Lumin Yang", "authors": "Lumin Yang, Jiajie Zhuang, Hongbo Fu, Xiangzhi Wei, Kun Zhou and Youyi\n  Zheng", "title": "SketchGNN: Semantic Sketch Segmentation with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce SketchGNN, a convolutional graph neural network for semantic\nsegmentation and labeling of freehand vector sketches. We treat an input\nstroke-based sketch as a graph, with nodes representing the sampled points\nalong input strokes and edges encoding the stroke structure information. To\npredict the per-node labels, our SketchGNN uses graph convolution and a\nstatic-dynamic branching network architecture to extract the features at three\nlevels, i.e., point-level, stroke-level, and sketch-level. SketchGNN\nsignificantly improves the accuracy of the state-of-the-art methods for\nsemantic sketch segmentation (by 11.2% in the pixel-based metric and 18.2% in\nthe component-based metric over a large-scale challenging SPG dataset) and has\nmagnitudes fewer parameters than both image-based and sequence-based methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:48:55 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:05:02 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yang", "Lumin", ""], ["Zhuang", "Jiajie", ""], ["Fu", "Hongbo", ""], ["Wei", "Xiangzhi", ""], ["Zhou", "Kun", ""], ["Zheng", "Youyi", ""]]}, {"id": "2003.00682", "submitter": "Prajoy Podder", "authors": "Subrato Bharati, Prajoy Podder, M. Rubaiyat Hossain Mondal", "title": "Hybrid Deep Learning for Detecting Lung Diseases from X-ray Images", "comments": "13 figures", "journal-ref": "Informatics in Medicine Unlocked, 2020", "doi": "10.1016/j.imu.2020.100391", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung disease is common throughout the world. These include chronic\nobstructive pulmonary disease, pneumonia, asthma, tuberculosis, fibrosis, etc.\nTimely diagnosis of lung disease is essential. Many image processing and\nmachine learning models have been developed for this purpose. Different forms\nof existing deep learning techniques including convolutional neural network\n(CNN), vanilla neural network, visual geometry group based neural network\n(VGG), and capsule network are applied for lung disease prediction.The basic\nCNN has poor performance for rotated, tilted, or other abnormal image\norientation. Therefore, we propose a new hybrid deep learning framework by\ncombining VGG, data augmentation and spatial transformer network (STN) with\nCNN. This new hybrid method is termed here as VGG Data STN with CNN (VDSNet).\nAs implementation tools, Jupyter Notebook, Tensorflow, and Keras are used. The\nnew model is applied to NIH chest X-ray image dataset collected from Kaggle\nrepository. Full and sample versions of the dataset are considered. For both\nfull and sample datasets, VDSNet outperforms existing methods in terms of a\nnumber of metrics including precision, recall, F0.5 score and validation\naccuracy. For the case of full dataset, VDSNet exhibits a validation accuracy\nof 73%, while vanilla gray, vanilla RGB, hybrid CNN and VGG, and modified\ncapsule network have accuracy values of 67.8%, 69%, 69.5%, 60.5% and 63.8%,\nrespectively. When sample dataset rather than full dataset is used, VDSNet\nrequires much lower training time at the expense of a slightly lower validation\naccuracy. Hence, the proposed VDSNet framework will simplify the detection of\nlung disease for experts as well as for doctors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 06:07:30 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 17:42:07 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 17:31:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bharati", "Subrato", ""], ["Podder", "Prajoy", ""], ["Mondal", "M. Rubaiyat Hossain", ""]]}, {"id": "2003.00696", "submitter": "Yurui Ren", "authors": "Yurui Ren, Xiaoming Yu, Junming Chen, Thomas H. Li, Ge Li", "title": "Deep Image Spatial Transformation for Person Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose-guided person image generation is to transform a source person image to\na target pose. This task requires spatial manipulations of source data.\nHowever, Convolutional Neural Networks are limited by the lack of ability to\nspatially transform the inputs. In this paper, we propose a differentiable\nglobal-flow local-attention framework to reassemble the inputs at the feature\nlevel. Specifically, our model first calculates the global correlations between\nsources and targets to predict flow fields. Then, the flowed local patch pairs\nare extracted from the feature maps to calculate the local attention\ncoefficients. Finally, we warp the source features using a content-aware\nsampling method with the obtained local attention coefficients. The results of\nboth subjective and objective experiments demonstrate the superiority of our\nmodel. Besides, additional results in video animation and view synthesis show\nthat our model is applicable to other tasks requiring spatial transformation.\nOur source code is available at\nhttps://github.com/RenYurui/Global-Flow-Local-Attention.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 07:31:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 09:42:02 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Ren", "Yurui", ""], ["Yu", "Xiaoming", ""], ["Chen", "Junming", ""], ["Li", "Thomas H.", ""], ["Li", "Ge", ""]]}, {"id": "2003.00697", "submitter": "MyeongAh Cho", "authors": "MyeongAh Cho, Taeoh Kim, Ig-Jae Kim, Kyungjae Lee, and Sangyoun Lee", "title": "Relational Deep Feature Learning for Heterogeneous Face Recognition", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security, vol. 16,\n  pp. 376-388, 2021", "doi": "10.1109/TIFS.2020.3013186", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Face Recognition (HFR) is a task that matches faces across two\ndifferent domains such as visible light (VIS), near-infrared (NIR), or the\nsketch domain. Due to the lack of databases, HFR methods usually exploit the\npre-trained features on a large-scale visual database that contain general\nfacial information. However, these pre-trained features cause performance\ndegradation due to the texture discrepancy with the visual domain. With this\nmotivation, we propose a graph-structured module called Relational Graph Module\n(RGM) that extracts global relational information in addition to general facial\nfeatures. Because each identity's relational information between intra-facial\nparts is similar in any modality, the modeling relationship between features\ncan help cross-domain matching. Through the RGM, relation propagation\ndiminishes texture dependency without losing its advantages from the\npre-trained features. Furthermore, the RGM captures global facial geometrics\nfrom locally correlated convolutional features to identify long-range\nrelationships. In addition, we propose a Node Attention Unit (NAU) that\nperforms node-wise recalibration to concentrate on the more informative nodes\narising from relation-based propagation. Furthermore, we suggest a novel\nconditional-margin loss function (C-softmax) for the efficient projection\nlearning of the embedding vector in HFR. The proposed method outperforms other\nstate-of-the-art methods on five HFR databases. Furthermore, we demonstrate\nperformance improvement on three backbones because our module can be plugged\ninto any pre-trained face recognition backbone to overcome the limitations of a\nsmall HFR database.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 07:35:23 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 11:00:35 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 11:06:22 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Cho", "MyeongAh", ""], ["Kim", "Taeoh", ""], ["Kim", "Ig-Jae", ""], ["Lee", "Kyungjae", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2003.00706", "submitter": "Puneet Kohli", "authors": "Puneet Kohli, Saravana Gunaseelan, Jason Orozco, Yiwen Hua, Edward Li,\n  and Nicolas Dahlquist", "title": "GPU-Accelerated Mobile Multi-view Style Transfer", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An estimated 60% of smartphones sold in 2018 were equipped with multiple rear\ncameras, enabling a wide variety of 3D-enabled applications such as 3D Photos.\nThe success of 3D Photo platforms (Facebook 3D Photo, Holopix, etc) depend on a\nsteady influx of user generated content. These platforms must provide simple\nimage manipulation tools to facilitate content creation, akin to traditional\nphoto platforms. Artistic neural style transfer, propelled by recent\nadvancements in GPU technology, is one such tool for enhancing traditional\nphotos. However, naively extrapolating single-view neural style transfer to the\nmulti-view scenario produces visually inconsistent results and is prohibitively\nslow on mobile devices. We present a GPU-accelerated multi-view style transfer\npipeline which enforces style consistency between views with on-demand\nperformance on mobile platforms. Our pipeline is modular and creates high\nquality depth and parallax effects from a stereoscopic image pair.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:20:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kohli", "Puneet", ""], ["Gunaseelan", "Saravana", ""], ["Orozco", "Jason", ""], ["Hua", "Yiwen", ""], ["Li", "Edward", ""], ["Dahlquist", "Nicolas", ""]]}, {"id": "2003.00707", "submitter": "Jinhong Deng", "authors": "Jinhong Deng, Wen Li, Yuhua Chen, Lixin Duan", "title": "Unbiased Mean Teacher for Cross-domain Object Detection", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain object detection is challenging, because object detection model\nis often vulnerable to data variance, especially to the considerable domain\nshift between two distinctive domains. In this paper, we propose a new Unbiased\nMean Teacher (UMT) model for cross-domain object detection. We reveal that\nthere often exists a considerable model bias for the simple mean teacher (MT)\nmodel in cross-domain scenarios, and eliminate the model bias with several\nsimple yet highly effective strategies. In particular, for the teacher model,\nwe propose a cross-domain distillation method for MT to maximally exploit the\nexpertise of the teacher model. Moreover, for the student model, we alleviate\nits bias by augmenting training samples with pixel-level adaptation. Finally,\nfor the teaching process, we employ an out-of-distribution estimation strategy\nto select samples that most fit the current model to further enhance the\ncross-domain distillation process. By tackling the model bias issue with these\nstrategies, our UMT model achieves mAPs of 44.1%, 58.1%, 41.7%, and 43.1% on\nbenchmark datasets Clipart1k, Watercolor2k, Foggy Cityscapes, and Cityscapes,\nrespectively, which outperforms the existing state-of-the-art results in\nnotable margins. Our implementation is available at\nhttps://github.com/kinredon/umt.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:20:55 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 00:53:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Deng", "Jinhong", ""], ["Li", "Wen", ""], ["Chen", "Yuhua", ""], ["Duan", "Lixin", ""]]}, {"id": "2003.00710", "submitter": "Sascha Wirges", "authors": "Sascha Wirges, Ye Yang, Sven Richter, Haohao Hu, Christoph Stiller", "title": "Learned Enrichment of Top-View Grid Maps Improves Object Detection", "comments": "6 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an object detector for top-view grid maps which is additionally\ntrained to generate an enriched version of its input. Our goal in the joint\nmodel is to improve generalization by regularizing towards structural knowledge\nin form of a map fused from multiple adjacent range sensor measurements. This\ntraining data can be generated in an automatic fashion, thus does not require\nmanual annotations. We present an evidential framework to generate training\ndata, investigate different model architectures and show that predicting\nenriched inputs as an additional task can improve object detection performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:27:54 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 11:57:30 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wirges", "Sascha", ""], ["Yang", "Ye", ""], ["Richter", "Sven", ""], ["Hu", "Haohao", ""], ["Stiller", "Christoph", ""]]}, {"id": "2003.00711", "submitter": "Weibing Huang", "authors": "Sizhang Dai, Weibing Huang", "title": "A-TVSNet: Aggregated Two-View Stereo Network for Multi-View Stereo Depth\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a learning-based network for depth map estimation from multi-view\nstereo (MVS) images. Our proposed network consists of three sub-networks: 1) a\nbase network for initial depth map estimation from an unstructured stereo image\npair, 2) a novel refinement network that leverages both photometric and\ngeometric information, and 3) an attentional multi-view aggregation framework\nthat enables efficient information exchange and integration among different\nstereo image pairs. The proposed network, called A-TVSNet, is evaluated on\nvarious MVS datasets and shows the ability to produce high quality depth map\nthat outperforms competing approaches. Our code is available at\nhttps://github.com/daiszh/A-TVSNet.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 08:29:35 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Dai", "Sizhang", ""], ["Huang", "Weibing", ""]]}, {"id": "2003.00737", "submitter": "Adrian-Stefan Ungureanu", "authors": "Adrian-S. Ungureanu, Saqib Salahuddin and Peter Corcoran", "title": "Towards Unconstrained Palmprint Recognition on Consumer Devices: a\n  Literature Review", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2020.2992219", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a biometric palmprints have been largely under-utilized, but they offer\nsome advantages over fingerprints and facial biometrics. Recent improvements in\nimaging capabilities on handheld and wearable consumer devices have re-awakened\ninterest in the use fo palmprints. The aim of this paper is to provide a\ncomprehensive review of state-of-the-art methods for palmprint recognition\nincluding Region of Interest extraction methods, feature extraction approaches\nand matching algorithms along with overview of available palmprint datasets in\norder to understand the latest trends and research dynamics in the palmprint\nrecognition field.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 09:53:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ungureanu", "Adrian-S.", ""], ["Salahuddin", "Saqib", ""], ["Corcoran", "Peter", ""]]}, {"id": "2003.00739", "submitter": "Liang Jiang", "authors": "Liang Jiang, Zujie Wen, Zhongping Liang, Yafang Wang, Gerard de Melo,\n  Zhe Li, Liangzhuang Ma, Jiaxing Zhang, Xiaolong Li, Yuan Qi", "title": "Long Short-Term Sample Distillation", "comments": "published as a conference paper at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, there has been substantial progress at training\nincreasingly deep neural networks. Recent advances within the teacher--student\ntraining paradigm have established that information about past training updates\nshow promise as a source of guidance during subsequent training steps. Based on\nthis notion, in this paper, we propose Long Short-Term Sample Distillation, a\nnovel training policy that simultaneously leverages multiple phases of the\nprevious training process to guide the later training updates to a neural\nnetwork, while efficiently proceeding in just one single generation pass. With\nLong Short-Term Sample Distillation, the supervision signal for each sample is\ndecomposed into two parts: a long-term signal and a short-term one. The\nlong-term teacher draws on snapshots from several epochs ago in order to\nprovide steadfast guidance and to guarantee teacher--student differences, while\nthe short-term one yields more up-to-date cues with the goal of enabling\nhigher-quality updates. Moreover, the teachers for each sample are unique, such\nthat, overall, the model learns from a very diverse set of teachers.\nComprehensive experimental results across a range of vision and NLP tasks\ndemonstrate the effectiveness of this new training method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 10:03:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Jiang", "Liang", ""], ["Wen", "Zujie", ""], ["Liang", "Zhongping", ""], ["Wang", "Yafang", ""], ["de Melo", "Gerard", ""], ["Li", "Zhe", ""], ["Ma", "Liangzhuang", ""], ["Zhang", "Jiaxing", ""], ["Li", "Xiaolong", ""], ["Qi", "Yuan", ""]]}, {"id": "2003.00752", "submitter": "Antonio Loquercio", "authors": "Antonio Loquercio, Alexey Dosovitskiy, and Davide Scaramuzza", "title": "Learning Depth With Very Sparse Supervision", "comments": "Accepted for Publication at the IEEE Robotics and Automation Letters\n  (RA-L) 2020, and International Conference on Intelligent Robots and Systems\n  (IROS) 2020", "journal-ref": "IEEE Robotics and Automation Letters (RA-L) 2020", "doi": "10.1109/LRA.2020.3009067", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the astonishing capabilities of natural intelligent agents and\ninspired by theories from psychology, this paper explores the idea that\nperception gets coupled to 3D properties of the world via interaction with the\nenvironment. Existing works for depth estimation require either massive amounts\nof annotated training data or some form of hard-coded geometrical constraint.\nThis paper explores a new approach to learning depth perception requiring\nneither of those. Specifically, we train a specialized global-local network\narchitecture with what would be available to a robot interacting with the\nenvironment: from extremely sparse depth measurements down to even a single\npixel per image. From a pair of consecutive images, our proposed network\noutputs a latent representation of the observer's motion between the images and\na dense depth map. Experiments on several datasets show that, when ground truth\nis available even for just one of the image pixels, the proposed network can\nlearn monocular dense depth estimation up to 22.5% more accurately than\nstate-of-the-art approaches. We believe that this work, despite its scientific\ninterest, lays the foundations to learn depth from extremely sparse\nsupervision, which can be valuable to all robotic systems acting under severe\nbandwidth or sensing constraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 10:44:13 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 10:01:55 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Loquercio", "Antonio", ""], ["Dosovitskiy", "Alexey", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.00766", "submitter": "Guangming Wang", "authors": "Guangming Wang, Chi Zhang, Hesheng Wang, Jingchuan Wang, Yong Wang,\n  Xinlei Wang", "title": "Unsupervised Learning of Depth, Optical Flow and Pose with Occlusion\n  from 3D Geometry", "comments": "Published in: IEEE Transactions on Intelligent Transportation\n  Systems. DOI: 10.1109/TITS.2020.3010418", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems, 2020", "doi": "10.1109/TITS.2020.3010418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In autonomous driving, monocular sequences contain lots of information.\nMonocular depth estimation, camera ego-motion estimation and optical flow\nestimation in consecutive frames are high-profile concerns recently. By\nanalyzing tasks above, pixels in the middle frame are modeled into three parts:\nthe rigid region, the non-rigid region, and the occluded region. In joint\nunsupervised training of depth and pose, we can segment the occluded region\nexplicitly. The occlusion information is used in unsupervised learning of\ndepth, pose and optical flow, as the image reconstructed by depth-pose and\noptical flow will be invalid in occluded regions. A less-than-mean mask is\ndesigned to further exclude the mismatched pixels interfered with by motion or\nillumination change in the training of depth and pose networks. This method is\nalso used to exclude some trivial mismatched pixels in the training of the\noptical flow network. Maximum normalization is proposed for depth smoothness\nterm to restrain depth degradation in textureless regions. In the occluded\nregion, as depth and camera motion can provide more reliable motion estimation,\nthey can be used to instruct unsupervised learning of optical flow. Our\nexperiments in KITTI dataset demonstrate that the model based on three regions,\nfull and explicit segmentation of the occlusion region, the rigid region, and\nthe non-rigid region with corresponding unsupervised losses can improve\nperformance on three tasks significantly. The source code is available at:\nhttps://github.com/guangmingw/DOPlearning.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 11:18:13 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 08:47:16 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 05:26:00 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Guangming", ""], ["Zhang", "Chi", ""], ["Wang", "Hesheng", ""], ["Wang", "Jingchuan", ""], ["Wang", "Yong", ""], ["Wang", "Xinlei", ""]]}, {"id": "2003.00768", "submitter": "Mehmet Yamac", "authors": "Mehmet Yamac, Mete Ahishali, Serkan Kiranyaz, Moncef Gabbouj", "title": "Convolutional Sparse Support Estimator Network (CSEN) From energy\n  efficient support estimation to learning-aided Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Support estimation (SE) of a sparse signal refers to finding the location\nindices of the non-zero elements in a sparse representation. Most of the\ntraditional approaches dealing with SE problem are iterative algorithms based\non greedy methods or optimization techniques. Indeed, a vast majority of them\nuse sparse signal recovery techniques to obtain support sets instead of\ndirectly mapping the non-zero locations from denser measurements (e.g.,\nCompressively Sensed Measurements). This study proposes a novel approach for\nlearning such a mapping from a training set. To accomplish this objective, the\nConvolutional Support Estimator Networks (CSENs), each with a compact\nconfiguration, are designed. The proposed CSEN can be a crucial tool for the\nfollowing scenarios: (i) Real-time and low-cost support estimation can be\napplied in any mobile and low-power edge device for anomaly localization,\nsimultaneous face recognition, etc. (ii) CSEN's output can directly be used as\n\"prior information\" which improves the performance of sparse signal recovery\nalgorithms. The results over the benchmark datasets show that state-of-the-art\nperformance levels can be achieved by the proposed approach with a\nsignificantly reduced computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 11:18:35 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 19:41:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yamac", "Mehmet", ""], ["Ahishali", "Mete", ""], ["Kiranyaz", "Serkan", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2003.00770", "submitter": "Maximilian Kraus", "authors": "Michael Hammann, Maximilian Kraus, Sina Shafaei, Alois Knoll", "title": "Identity Recognition in Intelligent Cars with Behavioral Data and\n  LSTM-ResNet Classifier", "comments": "12 Pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity recognition in a car cabin is a critical task nowadays and offers a\ngreat field of applications ranging from personalizing intelligent cars to suit\ndrivers physical and behavioral needs to increasing safety and security.\nHowever, the performance and applicability of published approaches are still\nnot suitable for use in series cars and need to be improved. In this paper, we\ninvestigate Human Identity Recognition in a car cabin with Time Series\nClassification (TSC) and deep neural networks. We use gas and brake pedal\npressure as input to our models. This data is easily collectable during driving\nin everyday situations. Since our classifiers have very little memory\nrequirements and do not require any input data preproccesing, we were able to\ntrain on one Intel i5-3210M processor only. Our classification approach is\nbased on a combination of LSTM and ResNet. The network trained on a subset of\nNUDrive outperforms the ResNet and LSTM models trained solely by 35.9 % and\n53.85 % accuracy respectively. We reach a final accuracy of 79.49 % on a\n10-drivers subset of NUDrive and 96.90 % on a 5-drivers subset of UTDrive.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 11:24:05 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hammann", "Michael", ""], ["Kraus", "Maximilian", ""], ["Shafaei", "Sina", ""], ["Knoll", "Alois", ""]]}, {"id": "2003.00798", "submitter": "Muneeb Aadil", "authors": "Muhammad Yaseen, Muneeb Aadil, Maria Sargsyan", "title": "Preventing Clean Label Poisoning using Gaussian Mixture Loss", "comments": "Preliminary v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2014 when Szegedy et al. showed that carefully designed perturbations\nof the input can lead Deep Neural Networks (DNNs) to wrongly classify its\nlabel, there has been an ongoing research to make DNNs more robust to such\nmalicious perturbations. In this work, we consider a poisoning attack called\nClean Labeling poisoning attack (CLPA). The goal of CLPA is to inject seemingly\nbenign instances which can drastically change decision boundary of the DNNs due\nto which subsequent queries at test time can be mis-classified. We argue that a\nstrong defense against CLPA can be embedded into the model during the training\nby imposing features of the network to follow a Large Margin Gaussian Mixture\ndistribution in the penultimate layer. By having such a prior knowledge, we can\nsystematically evaluate how unusual the example is, given the label it is\nclaiming to be. We demonstrate our builtin defense via experiments on MNIST and\nCIFAR datasets. We train two models on each dataset: one trained via softmax,\nanother via LGM. We show that using LGM can substantially reduce the\neffectiveness of CLPA while having no additional overhead of data sanitization.\nThe code to reproduce our results is available online.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:51:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Yaseen", "Muhammad", ""], ["Aadil", "Muneeb", ""], ["Sargsyan", "Maria", ""]]}, {"id": "2003.00800", "submitter": "Alessandro Betti", "authors": "Alessandro Betti, Benedetto Michelozzi, Andrea Bracci and Andrea\n  Masini", "title": "Real-Time target detection in maritime scenarios based on YOLOv3 model", "comments": "Paper presented at the 9th International Symposium on Optronics in\n  Defence & Security, 28-30 January 2020 (OPTRO2020, Paris). Oral Presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a novel ships dataset is proposed consisting of more than 56k\nimages of marine vessels collected by means of web-scraping and including 12\nship categories. A YOLOv3 single-stage detector based on Keras API is built on\ntop of this dataset. Current results on four categories (cargo ship, naval\nship, oil ship and tug ship) show Average Precision up to 96% for Intersection\nover Union (IoU) of 0.5 and satisfactory detection performances up to IoU of\n0.8. A Data Analytics GUI service based on QT framework and Darknet-53 engine\nis also implemented in order to simplify the deployment process and analyse\nmassive amount of images even for people without Data Science expertise.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 15:25:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Betti", "Alessandro", ""], ["Michelozzi", "Benedetto", ""], ["Bracci", "Andrea", ""], ["Masini", "Andrea", ""]]}, {"id": "2003.00802", "submitter": "Przemys{\\l}aw Spurek", "authors": "Przemys{\\l}aw Spurek, Sebastian Winczowski, Jacek Tabor, Maciej\n  Zamorski, Maciej Zi\\k{e}ba, Tomasz Trzci\\'nski", "title": "Hypernetwork approach to generating point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel method for generating 3D point clouds that\nleverage properties of hyper networks. Contrary to the existing methods that\nlearn only the representation of a 3D object, our approach simultaneously finds\na representation of the object and its 3D surface. The main idea of our\nHyperCloud method is to build a hyper network that returns weights of a\nparticular neural network (target network) trained to map points from a uniform\nunit ball distribution into a 3D shape. As a consequence, a particular 3D shape\ncan be generated using point-by-point sampling from the assumed prior\ndistribution and transforming sampled points with the target network. Since the\nhyper network is based on an auto-encoder architecture trained to reconstruct\nrealistic 3D shapes, the target network weights can be considered a\nparametrization of the surface of a 3D shape, and not a standard representation\nof point cloud usually returned by competitive approaches. The proposed\narchitecture allows finding mesh-based representation of 3D objects in a\ngenerative manner while providing point clouds en pair in quality with the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:09:58 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 19:18:59 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Spurek", "Przemys\u0142aw", ""], ["Winczowski", "Sebastian", ""], ["Tabor", "Jacek", ""], ["Zamorski", "Maciej", ""], ["Zi\u0119ba", "Maciej", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2003.00804", "submitter": "Jialin Liu", "authors": "Jialin Liu, Fei Chao, Chih-Min Lin", "title": "Task Augmentation by Rotating for Meta-Learning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is one of the most effective approaches for improving the\naccuracy of modern machine learning models, and it is also indispensable to\ntrain a deep model for meta-learning. In this paper, we introduce a task\naugmentation method by rotating, which increases the number of classes by\nrotating the original images 90, 180 and 270 degrees, different from\ntraditional augmentation methods which increase the number of images. With a\nlarger amount of classes, we can sample more diverse task instances during\ntraining. Therefore, task augmentation by rotating allows us to train a deep\nnetwork by meta-learning methods with little over-fitting. Experimental results\nshow that our approach is better than the rotation for increasing the number of\nimages and achieves state-of-the-art performance on miniImageNet, CIFAR-FS, and\nFC100 few-shot learning benchmarks. The code is available on\n\\url{www.github.com/AceChuse/TaskLevelAug}.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 07:57:24 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Jialin", ""], ["Chao", "Fei", ""], ["Lin", "Chih-Min", ""]]}, {"id": "2003.00805", "submitter": "Vasileios Mavroeidis Dr.", "authors": "Alexander Egiazarov, Vasileios Mavroeidis, Fabio Massimo Zennaro,\n  Kamer Vishi", "title": "Firearm Detection and Segmentation Using an Ensemble of Semantic Neural\n  Networks", "comments": "8 pages, 8 figures, 2 tables, 2019 European Intelligence and Security\n  Informatics Conference (EISIC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years we have seen an upsurge in terror attacks around the world.\nSuch attacks usually happen in public places with large crowds to cause the\nmost damage possible and get the most attention. Even though surveillance\ncameras are assumed to be a powerful tool, their effect in preventing crime is\nfar from clear due to either limitation in the ability of humans to vigilantly\nmonitor video surveillance or for the simple reason that they are operating\npassively. In this paper, we present a weapon detection system based on an\nensemble of semantic Convolutional Neural Networks that decomposes the problem\nof detecting and locating a weapon into a set of smaller problems concerned\nwith the individual component parts of a weapon. This approach has\ncomputational and practical advantages: a set of simpler neural networks\ndedicated to specific tasks requires less computational resources and can be\ntrained in parallel; the overall output of the system given by the aggregation\nof the outputs of individual networks can be tuned by a user to trade-off false\npositives and false negatives; finally, according to ensemble theory, the\noutput of the overall system will be robust and reliable even in the presence\nof weak individual models. We evaluated our system running simulations aimed at\nassessing the accuracy of individual networks and the whole system. The results\non synthetic data and real-world data are promising, and they suggest that our\napproach may have advantages compared to the monolithic approach based on a\nsingle deep convolutional neural network.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 13:58:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Egiazarov", "Alexander", ""], ["Mavroeidis", "Vasileios", ""], ["Zennaro", "Fabio Massimo", ""], ["Vishi", "Kamer", ""]]}, {"id": "2003.00808", "submitter": "Ammarah Farooq", "authors": "Ammarah Farooq, Muhammad Awais, Fei Yan, Josef Kittler, Ali Akbari,\n  and Syed Safwan Khalid", "title": "A Convolutional Baseline for Person Re-Identification Using Vision and\n  Language Descriptions", "comments": "12 pages including references, currently under review in IEEE\n  transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical person re-identification approaches assume that a person of\ninterest has appeared across different cameras and can be queried by one of the\nexisting images. However, in real-world surveillance scenarios, frequently no\nvisual information will be available about the queried person. In such\nscenarios, a natural language description of the person by a witness will\nprovide the only source of information for retrieval. In this work, person\nre-identification using both vision and language information is addressed under\nall possible gallery and query scenarios. A two stream deep convolutional\nneural network framework supervised by cross entropy loss is presented. The\nweights connecting the second last layer to the last layer with class\nprobabilities, i.e., logits of softmax layer are shared in both networks.\nCanonical Correlation Analysis is performed to enhance the correlation between\nthe two modalities in a joint latent embedding space. To investigate the\nbenefits of the proposed approach, a new testing protocol under a multi modal\nReID setting is proposed for the test split of the CUHK-PEDES and CUHK-SYSU\nbenchmarks. The experimental results verify the merits of the proposed system.\nThe learnt visual representations are more robust and perform 22\\% better\nduring retrieval as compared to a single modality system. The retrieval with a\nmulti modal query greatly enhances the re-identification capability of the\nsystem quantitatively as well as qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 10:12:02 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Farooq", "Ammarah", ""], ["Awais", "Muhammad", ""], ["Yan", "Fei", ""], ["Kittler", "Josef", ""], ["Akbari", "Ali", ""], ["Khalid", "Syed Safwan", ""]]}, {"id": "2003.00809", "submitter": "Indigo Orton", "authors": "Indigo J. D. Orton", "title": "Vision based body gesture meta features for Affective Computing", "comments": "MPhil thesis; 74 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Early detection of psychological distress is key to effective treatment.\nAutomatic detection of distress, such as depression, is an active area of\nresearch. Current approaches utilise vocal, facial, and bodily modalities. Of\nthese, the bodily modality is the least investigated, partially due to the\ndifficulty in extracting bodily representations from videos, and partially due\nto the lack of viable datasets. Existing body modality approaches use automatic\ncategorization of expressions to represent body language as a series of\nspecific expressions, much like words within natural language. In this\ndissertation I present a new type of feature, within the body modality, that\nrepresents meta information of gestures, such as speed, and use it to predict a\nnon-clinical depression label. This differs to existing work by representing\noverall behaviour as a small set of aggregated meta features derived from a\nperson's movement. In my method I extract pose estimation from videos, detect\ngestures within body parts, extract meta information from individual gestures,\nand finally aggregate these features to generate a small feature vector for use\nin prediction tasks. I introduce a new dataset of 65 video recordings of\ninterviews with self-evaluated distress, personality, and demographic labels.\nThis dataset enables the development of features utilising the whole body in\ndistress detection tasks. I evaluate my newly introduced meta-features for\npredicting depression, anxiety, perceived stress, somatic stress, five standard\npersonality measures, and gender. A linear regression based classifier using\nthese features achieves a 82.70% F1 score for predicting depression within my\nnovel dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 14:38:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Orton", "Indigo J. D.", ""]]}, {"id": "2003.00810", "submitter": "Bharath K P", "authors": "Anirudh Itagi, Ritam Sil, Saurav Mohapatra, Subham Rout, Bharath K P,\n  Karthik R, Rajesh Kumar Muthu", "title": "Medicine Strip Identification using 2-D Cepstral Feature Extraction and\n  Multiclass Classification Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Misclassification of medicine is perilous to the health of a patient, more so\nif the said patient is visually impaired or simply did not recognize the color,\nshape or type of medicine strip. This paper proposes a method for\nidentification of medicine strips by 2-D cepstral analysis of their images\nfollowed by performing classification that has been done using the K-Nearest\nNeighbor (KNN), Support Vector Machine (SVM) and Logistic Regression (LR)\nClassifiers. The 2-D cepstral features extracted are extremely distinct to a\nmedicine strip and consequently make identifying them exceptionally accurate.\nThis paper also proposes the Color Gradient and Pill shape Feature (CGPF)\nextraction procedure and discusses the Binary Robust Invariant Scalable\nKeypoints (BRISK) algorithm as well. The mentioned algorithms were implemented\nand their identification results have been compared.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:45:01 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Itagi", "Anirudh", ""], ["Sil", "Ritam", ""], ["Mohapatra", "Saurav", ""], ["Rout", "Subham", ""], ["P", "Bharath K", ""], ["R", "Karthik", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "2003.00813", "submitter": "Bingquan Zhu", "authors": "Bingquan Zhu, Hao Fang, Yanan Sui, Luming Li", "title": "Deepfakes for Medical Video De-Identification: Privacy Protection and\n  Diagnostic Information Preservation", "comments": "Accepted for publication at the AAAI/ACM Conference on Artificial\n  Intelligence, Ethics, and Society (AIES) 2020", "journal-ref": null, "doi": "10.1145/3375627.3375849", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data sharing for medical research has been difficult as open-sourcing\nclinical data may violate patient privacy. Traditional methods for face\nde-identification wipe out facial information entirely, making it impossible to\nanalyze facial behavior. Recent advancements on whole-body keypoints detection\nalso rely on facial input to estimate body keypoints. Both facial and body\nkeypoints are critical in some medical diagnoses, and keypoints invariability\nafter de-identification is of great importance. Here, we propose a solution\nusing deepfake technology, the face swapping technique. While this swapping\nmethod has been criticized for invading privacy and portraiture right, it could\nconversely protect privacy in medical video: patients' faces could be swapped\nto a proper target face and become unrecognizable. However, it remained an open\nquestion that to what extent the swapping de-identification method could affect\nthe automatic detection of body keypoints. In this study, we apply deepfake\ntechnology to Parkinson's disease examination videos to de-identify subjects,\nand quantitatively show that: face-swapping as a de-identification approach is\nreliable, and it keeps the keypoints almost invariant, significantly better\nthan traditional methods. This study proposes a pipeline for video\nde-identification and keypoint preservation, clearing up some ethical\nrestrictions for medical data sharing. This work could make open-source high\nquality medical video datasets more feasible and promote future medical\nresearch that benefits our society.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 22:36:48 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhu", "Bingquan", ""], ["Fang", "Hao", ""], ["Sui", "Yanan", ""], ["Li", "Luming", ""]]}, {"id": "2003.00817", "submitter": "Guangcun Shan", "authors": "Hongyu Wang, Guangcun Shan", "title": "Recognizing Handwritten Mathematical Expressions as LaTex Sequences\n  Using a Multiscale Robust Neural Network", "comments": "6 figures, 5 tables, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a robust multiscale neural network is proposed to recognize\nhandwritten mathematical expressions and output LaTeX sequences, which can\neffectively and correctly focus on where each step of output should be\nconcerned and has a positive effect on analyzing the two-dimensional structure\nof handwritten mathematical expressions and identifying different mathematical\nsymbols in a long expression. With the addition of visualization, the model's\nrecognition process is shown in detail. In addition, our model achieved 49.459%\nand 46.062% ExpRate on the public CROHME 2014 and CROHME 2016 datasets. The\npresent model results suggest that the state-of-the-art model has better\nrobustness, fewer errors, and higher accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 12:39:06 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Hongyu", ""], ["Shan", "Guangcun", ""]]}, {"id": "2003.00820", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Bo Li, Xiangyu Yue, Pengfei Xu, Kurt Keutzer", "title": "MADAN: Multi-source Adversarial Domain Aggregation Network for Domain\n  Adaptation", "comments": "Extension of our previous NeurIPS 2019 paper arXiv:1910.12181 on\n  multi-source domain adaptation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation aims to learn a transferable model to bridge the domain\nshift between one labeled source domain and another sparsely labeled or\nunlabeled target domain. Since the labeled data may be collected from multiple\nsources, multi-source domain adaptation (MDA) has attracted increasing\nattention. Recent MDA methods do not consider the pixel-level alignment between\nsources and target or the misalignment across different sources. In this paper,\nwe propose a novel MDA framework to address these challenges. Specifically, we\ndesign an end-to-end Multi-source Adversarial Domain Aggregation Network\n(MADAN). First, an adapted domain is generated for each source with dynamic\nsemantic consistency while aligning towards the target at the pixel-level\ncycle-consistently. Second, sub-domain aggregation discriminator and\ncross-domain cycle discriminator are proposed to make different adapted domains\nmore closely aggregated. Finally, feature-level alignment is performed between\nthe aggregated domain and the target domain while training the task network.\nFor the segmentation adaptation, we further enforce category-level alignment\nand incorporate context-aware generation, which constitutes MADAN+. We conduct\nextensive MDA experiments on digit recognition, object classification, and\nsimulation-to-real semantic segmentation. The results demonstrate that the\nproposed MADAN and MANDA+ models outperform state-of-the-art approaches by a\nlarge margin.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 21:22:00 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhao", "Sicheng", ""], ["Li", "Bo", ""], ["Yue", "Xiangyu", ""], ["Xu", "Pengfei", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2003.00822", "submitter": "Maximilian Lam", "authors": "Maximilian Lam, Zachary Yedidia, Colby Banbury, Vijay Janapa Reddi", "title": "Quantized Neural Network Inference with Precision Batching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PrecisionBatching, a quantized inference algorithm for speeding up\nneural network execution on traditional hardware platforms at low bitwidths\nwithout the need for retraining or recalibration. PrecisionBatching decomposes\na neural network into individual bitlayers and accumulates them using fast\n1-bit operations while maintaining activations in full precision.\nPrecisionBatching not only facilitates quantized inference at low bitwidths (<\n8 bits) without the need for retraining/recalibration, but also 1) enables\ntraditional hardware platforms the ability to realize inference speedups at a\nfiner granularity of quantization (e.g: 1-16 bit execution) and 2) allows\naccuracy and speedup tradeoffs at runtime by exposing the number of bitlayers\nto accumulate as a tunable parameter. Across a variety of applications (MNIST,\nlanguage modeling, natural language inference) and neural network architectures\n(fully connected, RNN, LSTM), PrecisionBatching yields end-to-end speedups of\nover 8x on a GPU within a < 1% error margin of the full precision baseline,\noutperforming traditional 8-bit quantized inference by over 1.5x-2x at the same\nerror tolerance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 19:34:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lam", "Maximilian", ""], ["Yedidia", "Zachary", ""], ["Banbury", "Colby", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "2003.00823", "submitter": "Dipesh Tamboli", "authors": "Abhijeet Patil, Dipesh Tamboli, Swati Meena, Deepak Anand, Amit Sethi", "title": "Breast Cancer Histopathology Image Classification and Localization using\n  Multiple Instance Learning", "comments": "Accepted in 2019 5th IEEE International WIE Conference on Electrical\n  and Computer Engineering (WIECON-ECE) and Awarded as best paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer has the highest mortality among cancers in women.\nComputer-aided pathology to analyze microscopic histopathology images for\ndiagnosis with an increasing number of breast cancer patients can bring the\ncost and delays of diagnosis down. Deep learning in histopathology has\nattracted attention over the last decade of achieving state-of-the-art\nperformance in classification and localization tasks. The convolutional neural\nnetwork, a deep learning framework, provides remarkable results in tissue\nimages analysis, but lacks in providing interpretation and reasoning behind the\ndecisions. We aim to provide a better interpretation of classification results\nby providing localization on microscopic histopathology images. We frame the\nimage classification problem as weakly supervised multiple instance learning\nproblem where an image is collection of patches i.e. instances. Attention-based\nmultiple instance learning (A-MIL) learns attention on the patches from the\nimage to localize the malignant and normal regions in an image and use them to\nclassify the image. We present classification and localization results on two\npublicly available BreakHIS and BACH dataset. The classification and\nvisualization results are compared with other recent techniques. The proposed\nmethod achieves better localization results without compromising classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 10:29:16 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Patil", "Abhijeet", ""], ["Tamboli", "Dipesh", ""], ["Meena", "Swati", ""], ["Anand", "Deepak", ""], ["Sethi", "Amit", ""]]}, {"id": "2003.00824", "submitter": "Gengchen Mai", "authors": "Gengchen Mai, Krzysztof Janowicz, Bo Yan, Rui Zhu, Ling Cai, Ni Lao", "title": "Multi-Scale Representation Learning for Spatial Feature Distributions\n  using Grid Cells", "comments": "15 pages; Accepted to ICLR 2020 as a spotlight paper", "journal-ref": "ICLR 2020, Apr. 26 - 30, 2020, Addis Ababa, ETHIOPIA", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised text encoding models have recently fueled substantial progress\nin NLP. The key idea is to use neural networks to convert words in texts to\nvector space representations based on word positions in a sentence and their\ncontexts, which are suitable for end-to-end training of downstream tasks. We\nsee a strikingly similar situation in spatial analysis, which focuses on\nincorporating both absolute positions and spatial contexts of geographic\nobjects such as POIs into models. A general-purpose representation model for\nspace is valuable for a multitude of tasks. However, no such general model\nexists to date beyond simply applying discretization or feed-forward nets to\ncoordinates, and little effort has been put into jointly modeling distributions\nwith vastly different characteristics, which commonly emerges from GIS data.\nMeanwhile, Nobel Prize-winning Neuroscience research shows that grid cells in\nmammals provide a multi-scale periodic representation that functions as a\nmetric for location encoding and is critical for recognizing places and for\npath-integration. Therefore, we propose a representation learning model called\nSpace2Vec to encode the absolute positions and spatial relationships of places.\nWe conduct experiments on two real-world geographic data for two different\ntasks: 1) predicting types of POIs given their positions and context, 2) image\nclassification leveraging their geo-locations. Results show that because of its\nmulti-scale representations, Space2Vec outperforms well-established ML\napproaches such as RBF kernels, multi-layer feed-forward nets, and tile\nembedding approaches for location modeling and image classification tasks.\nDetailed analysis shows that all baselines can at most well handle distribution\nat one scale but show poor performances in other scales. In contrast,\nSpace2Vec's multi-scale representation can handle distributions at different\nscales.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 04:22:18 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Mai", "Gengchen", ""], ["Janowicz", "Krzysztof", ""], ["Yan", "Bo", ""], ["Zhu", "Rui", ""], ["Cai", "Ling", ""], ["Lao", "Ni", ""]]}, {"id": "2003.00825", "submitter": "Bilal Hassan", "authors": "Bilal Hassan, Ramsha Ahmed, Taimur Hassan, and Naoufel Werghi", "title": "SIP-SegNet: A Deep Convolutional Encoder-Decoder Network for Joint\n  Semantic Segmentation and Extraction of Sclera, Iris and Pupil based on\n  Periocular Region Suppression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current developments in the field of machine vision have opened new\nvistas towards deploying multimodal biometric recognition systems in various\nreal-world applications. These systems have the ability to deal with the\nlimitations of unimodal biometric systems which are vulnerable to spoofing,\nnoise, non-universality and intra-class variations. In addition, the ocular\ntraits among various biometric traits are preferably used in these recognition\nsystems. Such systems possess high distinctiveness, permanence, and performance\nwhile, technologies based on other biometric traits (fingerprints, voice etc.)\ncan be easily compromised. This work presents a novel deep learning framework\ncalled SIP-SegNet, which performs the joint semantic segmentation of ocular\ntraits (sclera, iris and pupil) in unconstrained scenarios with greater\naccuracy. The acquired images under these scenarios exhibit purkinje reflexes,\nspecular reflections, eye gaze, off-angle shots, low resolution, and various\nocclusions particularly by eyelids and eyelashes. To address these issues,\nSIP-SegNet begins with denoising the pristine image using denoising\nconvolutional neural network (DnCNN), followed by reflection removal and image\nenhancement based on contrast limited adaptive histogram equalization (CLAHE).\nOur proposed framework then extracts the periocular information using adaptive\nthresholding and employs the fuzzy filtering technique to suppress this\ninformation. Finally, the semantic segmentation of sclera, iris and pupil is\nachieved using the densely connected fully convolutional encoder-decoder\nnetwork. We used five CASIA datasets to evaluate the performance of SIP-SegNet\nbased on various evaluation metrics. The simulation results validate the\noptimal segmentation of the proposed SIP-SegNet, with the mean f1 scores of\n93.35, 95.11 and 96.69 for the sclera, iris and pupil classes respectively.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 15:20:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Hassan", "Bilal", ""], ["Ahmed", "Ramsha", ""], ["Hassan", "Taimur", ""], ["Werghi", "Naoufel", ""]]}, {"id": "2003.00826", "submitter": "Muhammed Sit", "authors": "Akshat Gautam, Muhammed Sit and Ibrahim Demir", "title": "Realistic River Image Synthesis using Deep Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrated a practical application of realistic river\nimage generation using deep learning. Specifically, we explored a generative\nadversarial network (GAN) model capable of generating high-resolution and\nrealistic river images that can be used to support modeling and analysis in\nsurface water estimation, river meandering, wetland loss, and other\nhydrological research studies. First, we have created an extensive repository\nof overhead river images to be used in training. Second, we incorporated the\nProgressive Growing GAN (PGGAN), a network architecture that iteratively trains\nsmaller-resolution GANs to gradually build up to a very high resolution to\ngenerate high quality (i.e., 1024x1024) synthetic river imagery. With simpler\nGAN architectures, difficulties arose in terms of exponential increase of\ntraining time and vanishing/exploding gradient issues, which the PGGAN\nimplementation seemed to significantly reduce. The results presented in this\nstudy show great promise in generating high-quality images and capturing the\ndetails of river structure and flow to support hydrological research, which\noften requires extensive imagery for model performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 21:49:33 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 04:46:50 GMT"}, {"version": "v3", "created": "Tue, 27 Jul 2021 21:02:06 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Gautam", "Akshat", ""], ["Sit", "Muhammed", ""], ["Demir", "Ibrahim", ""]]}, {"id": "2003.00827", "submitter": "Laleh Seyyed-Kalantari", "authors": "Laleh Seyyed-Kalantari, Guanxiong Liu, Matthew McDermott, Irene Y.\n  Chen, Marzyeh Ghassemi", "title": "CheXclusion: Fairness gaps in deep chest X-ray classifiers", "comments": "Paper is accepted in Pacific Symposium on Biocomputing 2021\n  (PSB2021). Code can be found at, https://github.com/LalehSeyyed/CheXclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems have received much attention recently for their\nability to achieve expert-level performance on clinical tasks, particularly in\nmedical imaging. Here, we examine the extent to which state-of-the-art deep\nlearning classifiers trained to yield diagnostic labels from X-ray images are\nbiased with respect to protected attributes. We train convolution neural\nnetworks to predict 14 diagnostic labels in 3 prominent public chest X-ray\ndatasets: MIMIC-CXR, Chest-Xray8, CheXpert, as well as a multi-site aggregation\nof all those datasets. We evaluate the TPR disparity -- the difference in true\npositive rates (TPR) -- among different protected attributes such as patient\nsex, age, race, and insurance type as a proxy for socioeconomic status. We\ndemonstrate that TPR disparities exist in the state-of-the-art classifiers in\nall datasets, for all clinical tasks, and all subgroups. A multi-source dataset\ncorresponds to the smallest disparities, suggesting one way to reduce bias. We\nfind that TPR disparities are not significantly correlated with a subgroup's\nproportional disease burden. As clinical models move from papers to products,\nwe encourage clinical decision makers to carefully audit for algorithmic\ndisparities prior to deployment. Our code can be found at,\nhttps://github.com/LalehSeyyed/CheXclusion\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 22:08:12 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:26:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Seyyed-Kalantari", "Laleh", ""], ["Liu", "Guanxiong", ""], ["McDermott", "Matthew", ""], ["Chen", "Irene Y.", ""], ["Ghassemi", "Marzyeh", ""]]}, {"id": "2003.00828", "submitter": "Ines Rieger", "authors": "Ines Rieger, Rene Kollmann, Bettina Finzel, Dominik Seuss, Ute Schmid", "title": "Verifying Deep Learning-based Decisions for Facial Expression\n  Recognition", "comments": "accepted at ESANN 2020", "journal-ref": "Proceedings of the 28th European Symposium on Artificial Neural\n  Networks, Computational Intelligence and Machine Learning (ESANN 2020)\n  139-145", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with high performance can still be biased towards\nnon-relevant features. However, reliability and robustness is especially\nimportant for high-risk fields such as clinical pain treatment. We therefore\npropose a verification pipeline, which consists of three steps. First, we\nclassify facial expressions with a neural network. Next, we apply layer-wise\nrelevance propagation to create pixel-based explanations. Finally, we quantify\nthese visual explanations based on a bounding-box method with respect to facial\nregions. Although our results show that the neural network achieves\nstate-of-the-art results, the evaluation of the visual explanations reveals\nthat relevant facial regions may not be considered.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:59:32 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Rieger", "Ines", ""], ["Kollmann", "Rene", ""], ["Finzel", "Bettina", ""], ["Seuss", "Dominik", ""], ["Schmid", "Ute", ""]]}, {"id": "2003.00830", "submitter": "Mostafa El-Khamy", "authors": "Qingfeng Liu, Mostafa El-Khamy, Dongwoon Bai, Jungwon Lee", "title": "GSANet: Semantic Segmentation with Global and Selective Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel deep learning architecture for semantic\nsegmentation. The proposed Global and Selective Attention Network (GSANet)\nfeatures Atrous Spatial Pyramid Pooling (ASPP) with a novel sparsemax global\nattention and a novel selective attention that deploys a condensation and\ndiffusion mechanism to aggregate the multi-scale contextual information from\nthe extracted deep features. A selective attention decoder is also proposed to\nprocess the GSA-ASPP outputs for optimizing the softmax volume. We are the\nfirst to benchmark the performance of semantic segmentation networks with the\nlow-complexity feature extraction network (FXN) MobileNetEdge, that is\noptimized for low latency on edge devices. We show that GSANet can result in\nmore accurate segmentation with MobileNetEdge, as well as with strong FXNs,\nsuch as Xception. GSANet improves the state-of-art semantic segmentation\naccuracy on both the ADE20k and the Cityscapes datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 00:09:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Liu", "Qingfeng", ""], ["El-Khamy", "Mostafa", ""], ["Bai", "Dongwoon", ""], ["Lee", "Jungwon", ""]]}, {"id": "2003.00831", "submitter": "Kangying Li", "authors": "Kangying Li, Biligsaikhan Batjargal, Akira Maeda", "title": "Character Segmentation in Asian Collector's Seal Imprints: An Attempt to\n  Retrieval Based on Ancient Character Typeface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collector's seals provide important clues about the ownership of a book. They\ncontain much information pertaining to the essential elements of ancient\nmaterials and also show the details of possession, its relation to the book,\nthe identity of the collectors and their social status and wealth, amongst\nothers. Asian collectors have typically used artistic ancient characters rather\nthan modern ones to make their seals. In addition to the owner's name, several\nother words are used to express more profound meanings. A system that\nautomatically recognizes these characters can help enthusiasts and\nprofessionals better understand the background information of these seals.\nHowever, there is a lack of training data and labelled images, as samples of\nsome seals are scarce and most of them are degraded images. It is necessary to\nfind new ways to make full use of such scarce data. While these data are\navailable online, they do not contain information on the characters'position.\nThe goal of this research is to provide retrieval tools assist in obtaining\nmore information from Asian collector's seals imprints without consuming a lot\nof computational resources. In this paper, a character segmentation method is\nproposed to predict the candidate characters'area without any labelled training\ndata that contain character coordinate information. A retrieval-based\nrecognition system that focuses on a single character is also proposed to\nsupport seal retrieval and matching. The experimental results demonstrate that\nthe proposed character segmentation method performs well on Asian collector's\nseals, with 92% of the test data being correctly segmented.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 09:55:20 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Li", "Kangying", ""], ["Batjargal", "Biligsaikhan", ""], ["Maeda", "Akira", ""]]}, {"id": "2003.00832", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Yunsheng Ma, Yang Gu, Jufeng Yang, Tengfei Xing, Pengfei\n  Xu, Runbo Hu, Hua Chai, Kurt Keutzer", "title": "An End-to-End Visual-Audio Attention Network for Emotion Recognition in\n  User-Generated Videos", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition in user-generated videos plays an important role in\nhuman-centered computing. Existing methods mainly employ traditional two-stage\nshallow pipeline, i.e. extracting visual and/or audio features and training\nclassifiers. In this paper, we propose to recognize video emotions in an\nend-to-end manner based on convolutional neural networks (CNNs). Specifically,\nwe develop a deep Visual-Audio Attention Network (VAANet), a novel architecture\nthat integrates spatial, channel-wise, and temporal attentions into a visual 3D\nCNN and temporal attentions into an audio 2D CNN. Further, we design a special\nclassification loss, i.e. polarity-consistent cross-entropy loss, based on the\npolarity-emotion hierarchy constraint to guide the attention generation.\nExtensive experiments conducted on the challenging VideoEmotion-8 and Ekman-6\ndatasets demonstrate that the proposed VAANet outperforms the state-of-the-art\napproaches for video emotion recognition. Our source code is released at:\nhttps://github.com/maysonma/VAANet.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:33:59 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Zhao", "Sicheng", ""], ["Ma", "Yunsheng", ""], ["Gu", "Yang", ""], ["Yang", "Jufeng", ""], ["Xing", "Tengfei", ""], ["Xu", "Pengfei", ""], ["Hu", "Runbo", ""], ["Chai", "Hua", ""], ["Keutzer", "Kurt", ""]]}, {"id": "2003.00833", "submitter": "Diego Lucio", "authors": "Gabriela Y. Kimura, Diego R. Lucio, Alceu S. Britto Jr., David Menotti", "title": "CNN Hyperparameter tuning applied to Iris Liveness Detection", "comments": "Accepted for presentation at the International Conference on Computer\n  Vision Theory and Applications (VISAPP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iris pattern has significantly improved the biometric recognition field\ndue to its high level of stability and uniqueness. Such physical feature has\nplayed an important role in security and other related areas. However,\npresentation attacks, also known as spoofing techniques, can be used to bypass\nthe biometric system with artifacts such as printed images, artificial eyes,\nand textured contact lenses. To improve the security of these systems, many\nliveness detection methods have been proposed, and the first Internacional Iris\nLiveness Detection competition was launched in 2013 to evaluate their\neffectiveness. In this paper, we propose a hyperparameter tuning of the CASIA\nalgorithm, submitted by the Chinese Academy of Sciences to the third\ncompetition of Iris Liveness Detection, in 2017. The modifications proposed\npromoted an overall improvement, with an 8.48% Attack Presentation\nClassification Error Rate (APCER) and 0.18% Bonafide Presentation\nClassification Error Rate (BPCER) for the evaluation of the combined datasets.\nOther threshold values were evaluated in an attempt to reduce the trade-off\nbetween the APCER and the BPCER on the evaluated datasets and worked out\nsuccessfully.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 15:00:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Kimura", "Gabriela Y.", ""], ["Lucio", "Diego R.", ""], ["Britto", "Alceu S.", "Jr."], ["Menotti", "David", ""]]}, {"id": "2003.00834", "submitter": "Yansel Gonzalez Tejeda", "authors": "Yansel Gonzalez Tejeda and Helmut Mayer", "title": "CALVIS: chest, waist and pelvis circumference from 3D human body meshes\n  as ground truth for deep learning", "comments": "14 pages, 6 figures. To appear in the Proceedings of the VIII\n  International Workshop on Representation, analysis and recognition of shape\n  and motion FroM Imaging data (RFMI 2019), 11-13 December 2019, Sidi Bou Said,\n  Tunisia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present CALVIS, a method to calculate $\\textbf{C}$hest,\nw$\\textbf{A}$ist and pe$\\textbf{LVIS}$ circumference from 3D human body meshes.\nOur motivation is to use this data as ground truth for training convolutional\nneural networks (CNN). Previous work had used the large scale CAESAR dataset or\ndetermined these anthropometrical measurements $\\textit{manually}$ from a\nperson or human 3D body meshes. Unfortunately, acquiring these data is a cost\nand time consuming endeavor. In contrast, our method can be used on 3D meshes\nautomatically. We synthesize eight human body meshes and apply CALVIS to\ncalculate chest, waist and pelvis circumference. We evaluate the results\nqualitatively and observe that the measurements can indeed be used to estimate\nthe shape of a person. We then asses the plausibility of our approach by\ngenerating ground truth with CALVIS to train a small CNN. After having trained\nthe network with our data, we achieve competitive validation error.\nFurthermore, we make the implementation of CALVIS publicly available to advance\nthe field.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:36:15 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tejeda", "Yansel Gonzalez", ""], ["Mayer", "Helmut", ""]]}, {"id": "2003.00835", "submitter": "Dong Wang", "authors": "Dong Wang, Feng Zhou, Zheng Yan, Guang Yao, Zongxuan Liu, Wennan Ma\n  and Cewu Lu", "title": "Deep Variational Luenberger-type Observer for Stochastic Video\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the inherent stochasticity and uncertainty, predicting future\nvideo frames is exceptionally challenging. In this work, we study the problem\nof video prediction by combining interpretability of stochastic state space\nmodels and representation learning of deep neural networks. Our model builds\nupon an variational encoder which transforms the input video into a latent\nfeature space and a Luenberger-type observer which captures the dynamic\nevolution of the latent features. This enables the decomposition of videos into\nstatic features and dynamics in an unsupervised manner. By deriving the\nstability theory of the nonlinear Luenberger-type observer, the hidden states\nin the feature space become insensitive with respect to the initial values,\nwhich improves the robustness of the overall model. Furthermore, the\nvariational lower bound on the data log-likelihood can be derived to obtain the\ntractable posterior prediction distribution based on the variational principle.\nFinally, the experiments such as the Bouncing Balls dataset and the Pendulum\ndataset are provided to demonstrate the proposed model outperforms concurrent\nworks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 06:59:04 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wang", "Dong", ""], ["Zhou", "Feng", ""], ["Yan", "Zheng", ""], ["Yao", "Guang", ""], ["Liu", "Zongxuan", ""], ["Ma", "Wennan", ""], ["Lu", "Cewu", ""]]}, {"id": "2003.00836", "submitter": "Adil Rasheed Professor", "authors": "Herman Stavelin, Adil Rasheed, Omer San, Arne Johan Hestnes", "title": "Marine life through You Only Look Once's perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of focus on man made changes to our planet and wildlife\ntherein, more and more emphasis is put on sustainable and responsible gathering\nof resources. In an effort to preserve maritime wildlife the Norwegian\ngovernment has decided that it is necessary to create an overview over the\npresence and abundance of various species of wildlife in the Norwegian fjords\nand oceans. In this paper we apply and analyze an object detection scheme that\ndetects fish in camera images. The data is sampled from a submerged data\nstation at Fulehuk in Norway. We implement You Only Look Once (YOLO) version 3\nand create a dataset consisting of 99,961 images with a mAP of $\\sim 0.88$. We\nalso investigate intermediate results within YOLO, gaining insight into how it\nperforms object detection.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 22:47:45 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Stavelin", "Herman", ""], ["Rasheed", "Adil", ""], ["San", "Omer", ""], ["Hestnes", "Arne Johan", ""]]}, {"id": "2003.00837", "submitter": "Farid Ghareh Mohammadi", "authors": "Farid Ghareh Mohammadi, M. Hadi Amini, and Hamid R. Arabnia", "title": "On Parameter Tuning in Meta-learning for Computer Vision", "comments": "6 pages, 2 algorithms, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to learn plays a pivotal role in meta-learning (MTL) to obtain an\noptimal learning model. In this paper, we investigate mage recognition for\nunseen categories of a given dataset with limited training information. We\ndeploy a zero-shot learning (ZSL) algorithm to achieve this goal. We also\nexplore the effect of parameter tuning on performance of semantic auto-encoder\n(SAE). We further address the parameter tuning problem for meta-learning,\nespecially focusing on zero-shot learning. By combining different embedded\nparameters, we improved the accuracy of tuned-SAE. Advantages and disadvantages\nof parameter tuning and its application in image classification are also\nexplored.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:07:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Mohammadi", "Farid Ghareh", ""], ["Amini", "M. Hadi", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2003.00838", "submitter": "Yunyu Bai", "authors": "Han Fu, Yunyu Bai, Zhuo Li, Jun Shen, Jianling Sun", "title": "A Machine Learning Framework for Data Ingestion in Document Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paper documents are widely used as an irreplaceable channel of information in\nmany fields, especially in financial industry, fostering a great amount of\ndemand for systems which can convert document images into structured data\nrepresentations. In this paper, we present a machine learning framework for\ndata ingestion in document images, which processes the images uploaded by users\nand return fine-grained data in JSON format. Details of model architectures,\ndesign strategies, distinctions with existing solutions and lessons learned\nduring development are elaborated. We conduct abundant experiments on both\nsynthetic and real-world data in State Street. The experimental results\nindicate the effectiveness and efficiency of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 12:02:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Fu", "Han", ""], ["Bai", "Yunyu", ""], ["Li", "Zhuo", ""], ["Shen", "Jun", ""], ["Sun", "Jianling", ""]]}, {"id": "2003.00840", "submitter": "Avichal Rakesh", "authors": "Abhishek Saroha, Avichal Rakesh, Rajiv Kumar Tripathi", "title": "FPGA Implementation of Minimum Mean Brightness Error Bi-Histogram\n  Equalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histogram Equalization (HE) is a popular method for contrast enhancement.\nGenerally, mean brightness is not conserved in Histogram Equalization.\nInitially, Bi-Histogram Equalization (BBHE) was proposed to enhance contrast\nwhile maintaining a the mean brightness. However, when mean brightness is\nprimary concern, Minimum Mean Brightness Error Bi-Histogram Equalization\n(MMBEBHE) is the best technique. There are several implementations of Histogram\nEqualization on FPGA, however to our knowledge MMBEBHE has not been implemented\non FPGAs before. Therefore, we present an implementation of MMBEBHE on FPGA.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 06:42:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Saroha", "Abhishek", ""], ["Rakesh", "Avichal", ""], ["Tripathi", "Rajiv Kumar", ""]]}, {"id": "2003.00845", "submitter": "Samarth Bharadwaj", "authors": "Saneem Ahmed Chemmengath (1), Soumava Paul (2), Samarth Bharadwaj (1),\n  Suranjana Samanta, Karthik Sankaranarayanan ((1) IBM Research, (2) IIT\n  Kharagpur)", "title": "Addressing target shift in zero-shot learning using grouped adversarial\n  learning", "comments": "Under submission at Neurips 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) algorithms typically work by exploiting attribute\ncorrelations to be able to make predictions in unseen classes. However, these\ncorrelations do not remain intact at test time in most practical settings and\nthe resulting change in these correlations lead to adverse effects on zero-shot\nlearning performance. In this paper, we present a new paradigm for ZSL that:\n(i) utilizes the class-attribute mapping of unseen classes to estimate the\nchange in target distribution (target shift), and (ii) propose a novel\ntechnique called grouped Adversarial Learning (gAL) to reduce negative effects\nof this shift. Our approach is widely applicable for several existing ZSL\nalgorithms, including those with implicit attribute predictions. We apply the\nproposed technique ($g$AL) on three popular ZSL algorithms: ALE, SJE, and\nDEVISE, and show performance improvements on 4 popular ZSL datasets: AwA2, aPY,\nCUB and SUN. We obtain SOTA results on SUN and aPY datasets and achieve\ncomparable results on AwA2.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:00:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 11:38:50 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Chemmengath", "Saneem Ahmed", ""], ["Paul", "Soumava", ""], ["Bharadwaj", "Samarth", ""], ["Samanta", "Suranjana", ""], ["Sankaranarayanan", "Karthik", ""]]}, {"id": "2003.00847", "submitter": "Haoyu Chen", "authors": "Chen Haoyu, Teng Minggui, Shi Boxin, Wang YIzhou and Huang Tiejun", "title": "Learning to Deblur and Generate High Frame Rate Video with an Event\n  Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired cameras which can measure the change of\nintensity asynchronously with high temporal resolution. One of the event\ncameras' advantages is that they do not suffer from motion blur when recording\nhigh-speed scenes. In this paper, we formulate the deblurring task on\ntraditional cameras directed by events to be a residual learning one, and we\npropose corresponding network architectures for effective learning of\ndeblurring and high frame rate video generation tasks. We first train a\nmodified U-Net network to restore a sharp image from a blurry image using\ncorresponding events. Then we train another similar network with different\ndownsampling blocks to generate high frame rate video using the restored sharp\nimage and events. Experiment results show that our method can restore sharper\nimages and videos than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:02:05 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 04:09:55 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Haoyu", "Chen", ""], ["Minggui", "Teng", ""], ["Boxin", "Shi", ""], ["YIzhou", "Wang", ""], ["Tiejun", "Huang", ""]]}, {"id": "2003.00851", "submitter": "Seungjun Lee", "authors": "Seungjun Lee", "title": "Deep Learning on Radar Centric 3D Object Detection", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though many existing 3D object detection algorithms rely mostly on\ncamera and LiDAR, camera and LiDAR are prone to be affected by harsh weather\nand lighting conditions. On the other hand, radar is resistant to such\nconditions. However, research has found only recently to apply deep neural\nnetworks on radar data. In this paper, we introduce a deep learning approach to\n3D object detection with radar only. To the best of our knowledge, we are the\nfirst ones to demonstrate a deep learning-based 3D object detection model with\nradar only that was trained on the public radar dataset. To overcome the lack\nof radar labeled data, we propose a novel way of making use of abundant LiDAR\ndata by transforming it into radar-like point cloud data and aggressive radar\naugmentation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:16:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lee", "Seungjun", ""]]}, {"id": "2003.00856", "submitter": "Chenxi Xiao", "authors": "Chenxi Xiao and Juan Wachs", "title": "Triangle-Net: Towards Robustness in Point Cloud Classification", "comments": "Submitted to ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object recognition is becoming a key desired capability for many computer\nvision systems such as autonomous vehicles, service robots and surveillance\ndrones to operate more effectively in unstructured environments. These\nreal-time systems require effective classification methods that are robust to\nsampling resolution, measurement noise, and pose configuration of the objects.\nPrevious research has shown that sparsity, rotation and positional variance of\npoints can lead to a significant drop in the performance of point cloud based\nclassification techniques. In this regard, we propose a novel approach for 3D\nclassification that takes sparse point clouds as input and learns a model that\nis robust to rotational and positional variance as well as point sparsity. To\nthis end, we introduce new feature descriptors which are fed as an input to our\nproposed neural network in order to learn a robust latent representation of the\n3D object. We show that such latent representations can significantly improve\nthe performance of object classification and retrieval. Further, we show that\nour approach outperforms PointNet and 3DmFV by 34.4% and 27.4% respectively in\nclassification tasks using sparse point clouds of only 16 points under\narbitrary SO(3) rotation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:42:32 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xiao", "Chenxi", ""], ["Wachs", "Juan", ""]]}, {"id": "2003.00857", "submitter": "Qiaolin Xia", "authors": "Qiaolin Xia, Xiujun Li, Chunyuan Li, Yonatan Bisk, Zhifang Sui,\n  Jianfeng Gao, Yejin Choi, Noah A. Smith", "title": "Multi-View Learning for Vision-and-Language Navigation", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to navigate in a visual environment following natural language\ninstructions is a challenging task because natural language instructions are\nhighly variable, ambiguous, and under-specified. In this paper, we present a\nnovel training paradigm, Learn from EveryOne (LEO), which leverages multiple\ninstructions (as different views) for the same trajectory to resolve language\nambiguity and improve generalization. By sharing parameters across\ninstructions, our approach learns more effectively from limited training data\nand generalizes better in unseen environments. On the recent Room-to-Room (R2R)\nbenchmark dataset, LEO achieves 16% improvement (absolute) over a greedy agent\nas the base agent (25.3% $\\rightarrow$ 41.4%) in Success Rate weighted by Path\nLength (SPL). Further, LEO is complementary to most existing models for\nvision-and-language navigation, allowing for easy integration with the existing\ntechniques, leading to LEO+, which creates the new state of the art, pushing\nthe R2R benchmark to 62% (9% absolute improvement).\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:07:46 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 02:21:21 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 21:15:55 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Xia", "Qiaolin", ""], ["Li", "Xiujun", ""], ["Li", "Chunyuan", ""], ["Bisk", "Yonatan", ""], ["Sui", "Zhifang", ""], ["Gao", "Jianfeng", ""], ["Choi", "Yejin", ""], ["Smith", "Noah A.", ""]]}, {"id": "2003.00865", "submitter": "Sakshi Udeshi", "authors": "Ezekiel Soremekun, Sakshi Udeshi and Sudipta Chattopadhyay", "title": "Exposing Backdoors in Robust Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The introduction of robust optimisation has pushed the state-of-the-art in\ndefending against adversarial attacks. However, the behaviour of such\noptimisation has not been studied in the light of a fundamentally different\nclass of attacks called backdoors. In this paper, we demonstrate that\nadversarially robust models are susceptible to backdoor attacks. Subsequently,\nwe observe that backdoors are reflected in the feature representation of such\nmodels. Then, this observation is leveraged to detect backdoor-infected models\nvia a detection technique called AEGIS. Specifically, AEGIS uses feature\nclustering to effectively detect backdoor-infected robust Deep Neural Networks\n(DNNs). In our evaluation of several visible and hidden backdoor triggers on\nmajor classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS\neffectively detects robust DNNs infected with backdoors. AEGIS detects a\nbackdoor-infected model with 91.6% accuracy, without any false positives.\nFurthermore, AEGIS detects the targeted class in the backdoor-infected model\nwith a reasonably low (11.1%) false positive rate. Our investigation reveals\nthat salient features of adversarially robust DNNs break the stealthy nature of\nbackdoor attacks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 04:45:26 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 15:15:36 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 07:02:14 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Soremekun", "Ezekiel", ""], ["Udeshi", "Sakshi", ""], ["Chattopadhyay", "Sudipta", ""]]}, {"id": "2003.00867", "submitter": "Myeongjin Kim", "authors": "Myeongjin Kim, Hyeran Byun", "title": "Learning Texture Invariant Representation for Domain Adaptation of\n  Semantic Segmentation", "comments": "2020 CVPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since annotating pixel-level labels for semantic segmentation is laborious,\nleveraging synthetic data is an attractive solution. However, due to the domain\ngap between synthetic domain and real domain, it is challenging for a model\ntrained with synthetic data to generalize to real data. In this paper,\nconsidering the fundamental difference between the two domains as the texture,\nwe propose a method to adapt to the texture of the target domain. First, we\ndiversity the texture of synthetic images using a style transfer algorithm. The\nvarious textures of generated images prevent a segmentation model from\noverfitting to one specific (synthetic) texture. Then, we fine-tune the model\nwith self-training to get direct supervision of the target texture. Our results\nachieve state-of-the-art performance and we analyze the properties of the model\ntrained on the stylized dataset with extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:11:54 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 06:56:11 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kim", "Myeongjin", ""], ["Byun", "Hyeran", ""]]}, {"id": "2003.00872", "submitter": "Yunchao Wei", "authors": "Zilong Huang and Yunchao Wei and Xinggang Wang and Wenyu Liu and\n  Thomas S. Huang and Humphrey Shi", "title": "AlignSeg: Feature-Aligned Segmentation Networks", "comments": "Accepted by TPAMI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating features in terms of different convolutional blocks or contextual\nembeddings has been proven to be an effective way to strengthen feature\nrepresentations for semantic segmentation. However, most of the current popular\nnetwork architectures tend to ignore the misalignment issues during the feature\naggregation process caused by 1) step-by-step downsampling operations, and 2)\nindiscriminate contextual information fusion. In this paper, we explore the\nprinciples in addressing such feature misalignment issues and inventively\npropose Feature-Aligned Segmentation Networks (AlignSeg). AlignSeg consists of\ntwo primary modules, i.e., the Aligned Feature Aggregation (AlignFA) module and\nthe Aligned Context Modeling (AlignCM) module. First, AlignFA adopts a simple\nlearnable interpolation strategy to learn transformation offsets of pixels,\nwhich can effectively relieve the feature misalignment issue caused by\nmultiresolution feature aggregation. Second, with the contextual embeddings in\nhand, AlignCM enables each pixel to choose private custom contextual\ninformation in an adaptive manner, making the contextual embeddings aligned\nbetter to provide appropriate guidance. We validate the effectiveness of our\nAlignSeg network with extensive experiments on Cityscapes and ADE20K, achieving\nnew state-of-the-art mIoU scores of 82.6% and 45.95%, respectively. Our source\ncode will be made available.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:00:58 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 04:34:05 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Huang", "Zilong", ""], ["Wei", "Yunchao", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2003.00874", "submitter": "Jinfu Lin", "authors": "Xiaojian He, Jinfu Lin, Junming Shen", "title": "Weakly-supervised Object Localization for Few-shot Learning and\n  Fine-grained Few-shot Learning", "comments": "8 pages, 6 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims to learn novel visual categories from very few\nsamples, which is a challenging problem in real-world applications. Many\nmethods of few-shot classification work well on general images to learn global\nrepresentation. However, they can not deal with fine-grained categories well at\nthe same time due to a lack of subtle and local information. We argue that\nlocalization is an efficient approach because it directly provides the\ndiscriminative regions, which is critical for both general classification and\nfine-grained classification in a low data regime. In this paper, we propose a\nSelf-Attention Based Complementary Module (SAC Module) to fulfill the\nweakly-supervised object localization, and more importantly produce the\nactivated masks for selecting discriminative deep descriptors for few-shot\nclassification. Based on each selected deep descriptor, Semantic Alignment\nModule (SAM) calculates the semantic alignment distance between the query and\nsupport images to boost classification performance. Extensive experiments show\nour method outperforms the state-of-the-art methods on benchmark datasets under\nvarious settings, especially on the fine-grained few-shot tasks. Besides, our\nmethod achieves superior performance over previous methods when training the\nmodel on miniImageNet and evaluating it on the different datasets,\ndemonstrating its superior generalization capacity. Extra visualization shows\nthe proposed method can localize the key objects more interval.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:07:05 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 13:27:47 GMT"}, {"version": "v3", "created": "Sat, 12 Dec 2020 02:50:57 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["He", "Xiaojian", ""], ["Lin", "Jinfu", ""], ["Shen", "Junming", ""]]}, {"id": "2003.00875", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "Predicting TUG score from gait characteristics with video analysis and\n  machine learning", "comments": "Experimental results and discussion are revised. The code for\n  estimating copula entropy is available at https://github.com/majianthu/copent", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall is a leading cause of death which suffers the elderly and society. Timed\nUp and Go (TUG) test is a common tool for fall risk assessment. In this paper,\nwe propose a method for predicting TUG score from gait characteristics\nextracted from video with computer vision and machine learning technologies.\nFirst, 3D pose is estimated from video captured with 2D and 3D cameras during\nhuman motion and then a group of gait characteristics are computed from 3D pose\nseries. After that, copula entropy is used to select those characteristics\nwhich are mostly associated with TUG score. Finally, the selected\ncharacteristics are fed into the predictive models to predict TUG score.\nExperiments on real world data demonstrated the effectiveness of the proposed\nmethod. As a byproduct, the associations between TUG score and several gait\ncharacteristics are discovered, which laid the scientific foundation of the\nproposed method and make the predictive models such built interpretable to\nclinical users.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 05:27:37 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 11:34:58 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ma", "Jian", ""]]}, {"id": "2003.00877", "submitter": "Chuanxing Geng", "authors": "Chuanxing Geng, Zhenghao Tan, Songcan Chen", "title": "A Multi-view Perspective of Self-supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a newly emerging unsupervised learning paradigm, self-supervised learning\n(SSL) recently gained widespread attention, which usually introduces a pretext\ntask without manual annotation of data. With its help, SSL effectively learns\nthe feature representation beneficial for downstream tasks. Thus the pretext\ntask plays a key role. However, the study of its design, especially its essence\ncurrently is still open. In this paper, we borrow a multi-view perspective to\ndecouple a class of popular pretext tasks into a combination of view data\naugmentation (VDA) and view label classification (VLC), where we attempt to\nexplore the essence of such pretext task while providing some insights into its\ndesign. Specifically, a simple multi-view learning framework is specially\ndesigned (SSL-MV), which assists the feature learning of downstream tasks\n(original view) through the same tasks on the augmented views. SSL-MV focuses\non VDA while abandons VLC, empirically uncovering that it is VDA rather than\ngenerally considered VLC that dominates the performance of such SSL.\nAdditionally, thanks to replacing VLC with VDA tasks, SSL-MV also enables an\nintegrated inference combining the predictions from the augmented views,\nfurther improving the performance. Experiments on several benchmark datasets\ndemonstrate its advantages.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 13:26:00 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 04:20:00 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Geng", "Chuanxing", ""], ["Tan", "Zhenghao", ""], ["Chen", "Songcan", ""]]}, {"id": "2003.00878", "submitter": "Ziyue Xiang", "authors": "Daniel E. Acuna and Ziyue Xiang", "title": "Estimating a Null Model of Scientific Image Reuse to Support Research\n  Integrity Investigations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is a suspicious figure reuse case in science, research integrity\ninvestigators often find it difficult to rebut authors claiming that \"it\nhappened by chance\". In other words, when there is a \"collision\" of image\nfeatures, it is difficult to justify whether it appears rarely or not. In this\narticle, we provide a method to predict the rarity of an image feature by\nstatistically estimating the chance of it randomly occurring across all\nscientific imagery. Our method is based on high-dimensional density estimation\nof ORB features using 7+ million images in the PubMed Open Access Subset\ndataset. We show that this method can lead to meaningful feedback during\nresearch integrity investigations by providing a null hypothesis for scientific\nimage reuse and thus a p-value during deliberations. We apply the model to a\nsample of increasingly complex imagery and confirm that it produces\ndecreasingly smaller p-values as expected. We discuss applications to research\nintegrity investigations as well as future work.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 02:41:13 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Acuna", "Daniel E.", ""], ["Xiang", "Ziyue", ""]]}, {"id": "2003.00880", "submitter": "Stanton Price", "authors": "Stanton R. Price, Steven R. Price, Derek T. Anderson", "title": "Introducing Fuzzy Layers for Deep Learning", "comments": "6 pages, 4 figures, published in 2019 IEEE International Conference\n  on Fuzzy Systems (FUZZ-IEEE)", "journal-ref": "IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), New\n  Orleans, LA, USA, 2019, pp. 1-6", "doi": "10.1109/FUZZ-IEEE.2019.8858790", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many state-of-the-art technologies developed in recent years have been\ninfluenced by machine learning to some extent. Most popular at the time of this\nwriting are artificial intelligence methodologies that fall under the umbrella\nof deep learning. Deep learning has been shown across many applications to be\nextremely powerful and capable of handling problems that possess great\ncomplexity and difficulty. In this work, we introduce a new layer to deep\nlearning: the fuzzy layer. Traditionally, the network architecture of neural\nnetworks is composed of an input layer, some combination of hidden layers, and\nan output layer. We propose the introduction of fuzzy layers into the deep\nlearning architecture to exploit the powerful aggregation properties expressed\nthrough fuzzy methodologies, such as the Choquet and Sugueno fuzzy integrals.\nTo date, fuzzy approaches taken to deep learning have been through the\napplication of various fusion strategies at the decision level to aggregate\noutputs from state-of-the-art pre-trained models, e.g., AlexNet, VGG16,\nGoogLeNet, Inception-v3, ResNet-18, etc. While these strategies have been shown\nto improve accuracy performance for image classification tasks, none have\nexplored the use of fuzzified intermediate, or hidden, layers. Herein, we\npresent a new deep learning strategy that incorporates fuzzy strategies into\nthe deep learning architecture focused on the application of semantic\nsegmentation using per-pixel classification. Experiments are conducted on a\nbenchmark data set as well as a data set collected via an unmanned aerial\nsystem at a U.S. Army test site for the task of automatic road segmentation,\nand preliminary results are promising.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:33:30 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Price", "Stanton R.", ""], ["Price", "Steven R.", ""], ["Anderson", "Derek T.", ""]]}, {"id": "2003.00882", "submitter": "Freddie Bickford Smith", "authors": "Freddie Bickford Smith, Xiaoliang Luo, Brett D. Roads, Bradley C. Love", "title": "The perceptual boost of visual attention is task-dependent in\n  naturalistic settings", "comments": "Published as a workshop paper at \"Bridging AI and Cognitive Science\"\n  (ICLR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down attention allows people to focus on task-relevant visual\ninformation. Is the resulting perceptual boost task-dependent in naturalistic\nsettings? We aim to answer this with a large-scale computational experiment.\nFirst, we design a collection of visual tasks, each consisting of classifying\nimages from a chosen task set (subset of ImageNet categories). The nature of a\ntask is determined by which categories are included in the task set. Second, on\neach task we train an attention-augmented neural network and then compare its\naccuracy to that of a baseline network. We show that the perceptual boost of\nattention is stronger with increasing task-set difficulty, weaker with\nincreasing task-set size and weaker with increasing perceptual similarity\nwithin a task set.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 09:10:24 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 14:30:14 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Smith", "Freddie Bickford", ""], ["Luo", "Xiaoliang", ""], ["Roads", "Brett D.", ""], ["Love", "Bradley C.", ""]]}, {"id": "2003.00883", "submitter": "Camilo Pestana", "authors": "Camilo Pestana, Naveed Akhtar, Wei Liu, David Glance, Ajmal Mian", "title": "Adversarial Perturbations Prevail in the Y-Channel of the YCbCr Color\n  Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning offers state of the art solutions for image recognition.\nHowever, deep models are vulnerable to adversarial perturbations in images that\nare subtle but significantly change the model's prediction. In a white-box\nattack, these perturbations are generally learned for deep models that operate\non RGB images and, hence, the perturbations are equally distributed in the RGB\ncolor space. In this paper, we show that the adversarial perturbations prevail\nin the Y-channel of the YCbCr space. Our finding is motivated from the fact\nthat the human vision and deep models are more responsive to shape and texture\nrather than color. Based on our finding, we propose a defense against\nadversarial images. Our defence, coined ResUpNet, removes perturbations only\nfrom the Y-channel by exploiting ResNet features in an upsampling framework\nwithout the need for a bottleneck. At the final stage, the untouched\nCbCr-channels are combined with the refined Y-channel to restore the clean\nimage. Note that ResUpNet is model agnostic as it does not modify the DNN\nstructure. ResUpNet is trained end-to-end in Pytorch and the results are\ncompared to existing defence techniques in the input transformation category.\nOur results show that our approach achieves the best balance between defence\nagainst adversarial attacks such as FGSM, PGD and DDN and maintaining the\noriginal accuracies of VGG-16, ResNet50 and DenseNet121 on clean images. We\nperform another experiment to show that learning adversarial perturbations only\nfor the Y-channel results in higher fooling rates for the same perturbation\nmagnitude.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 02:41:42 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Pestana", "Camilo", ""], ["Akhtar", "Naveed", ""], ["Liu", "Wei", ""], ["Glance", "David", ""], ["Mian", "Ajmal", ""]]}, {"id": "2003.00888", "submitter": "Guus Engels", "authors": "Guus Engels, Nerea Aranjuelo, Ignacio Arganda-Carreras, Marcos Nieto\n  and Oihana Otaegui", "title": "3D Object Detection From LiDAR Data Using Distance Dependent Feature\n  Extraction", "comments": "10 pages, 8 figures, 6th International Conference on Vehicle\n  Technology and Intelligent Transport Systems (VEHITS 2020)", "journal-ref": null, "doi": "10.5220/0009330402890300", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to 3D object detection that leverages the\nproperties of the data obtained by a LiDAR sensor. State-of-the-art detectors\nuse neural network architectures based on assumptions valid for camera images.\nHowever, point clouds obtained from LiDAR are fundamentally different. Most\ndetectors use shared filter kernels to extract features which do not take into\naccount the range dependent nature of the point cloud features. To show this,\ndifferent detectors are trained on two splits of the KITTI dataset: close range\n(objects up to 25 meters from LiDAR) and long-range. Top view images are\ngenerated from point clouds as input for the networks. Combined results\noutperform the baseline network trained on the full dataset with a single\nbackbone. Additional research compares the effect of using different input\nfeatures when converting the point cloud to image. The results indicate that\nthe network focuses on the shape and structure of the objects, rather than\nexact values of the input. This work proposes an improvement for 3D object\ndetectors by taking into account the properties of LiDAR point clouds over\ndistance. Results show that training separate networks for close-range and\nlong-range objects boosts performance for all KITTI benchmark difficulties.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:16:35 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 07:47:20 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Engels", "Guus", ""], ["Aranjuelo", "Nerea", ""], ["Arganda-Carreras", "Ignacio", ""], ["Nieto", "Marcos", ""], ["Otaegui", "Oihana", ""]]}, {"id": "2003.00891", "submitter": "Steffen Wolf", "authors": "Steffen Wolf, Fred A. Hamprecht, Jan Funke", "title": "Instance Separation Emerges from Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained to inpaint partially occluded images show a deep\nunderstanding of image composition and have even been shown to remove objects\nfrom images convincingly. In this work, we investigate how this implicit\nknowledge of image composition can be leveraged for fully self-supervised\ninstance separation. We propose a measure for the independence of two image\nregions given a fully self-supervised inpainting network and separate objects\nby maximizing this independence. We evaluate our method on two microscopy image\ndatasets and show that it reaches similar segmentation performance to fully\nsupervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 18:05:39 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Wolf", "Steffen", ""], ["Hamprecht", "Fred A.", ""], ["Funke", "Jan", ""]]}, {"id": "2003.00893", "submitter": "Hang Dong", "authors": "Xinyi Zhang, Hang Dong, Zhe Hu, Wei-Sheng Lai, Fei Wang, Ming-Hsuan\n  Yang", "title": "Gated Fusion Network for Degraded Image Super Resolution", "comments": "Accepted by IJCV. The code will be publicly available at\n  https://github.com/BookerDeWitt/GFN-IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image super resolution aims to enhance image quality with respect to\nspatial content, which is a fundamental task in computer vision. In this work,\nwe address the task of single frame super resolution with the presence of image\ndegradation, e.g., blur, haze, or rain streaks. Due to the limitations of frame\ncapturing and formation processes, image degradation is inevitable, and the\nartifacts would be exacerbated by super resolution methods. To address this\nproblem, we propose a dual-branch convolutional neural network to extract base\nfeatures and recovered features separately. The base features contain local and\nglobal information of the input image. On the other hand, the recovered\nfeatures focus on the degraded regions and are used to remove the degradation.\nThose features are then fused through a recursive gate module to obtain sharp\nfeatures for super resolution. By decomposing the feature extraction step into\ntwo task-independent streams, the dual-branch model can facilitate the training\nprocess by avoiding learning the mixed degradation all-in-one and thus enhance\nthe final high-resolution prediction results. We evaluate the proposed method\nin three degradation scenarios. Experiments on these scenarios demonstrate that\nthe proposed method performs more efficiently and favorably against the\nstate-of-the-art approaches on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:28:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 10:47:16 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zhang", "Xinyi", ""], ["Dong", "Hang", ""], ["Hu", "Zhe", ""], ["Lai", "Wei-Sheng", ""], ["Wang", "Fei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2003.00895", "submitter": "Yuxuan Liang", "authors": "Yuxuan Liang, Kun Ouyang, Yiwei Wang, Ye Liu, Junbo Zhang, Yu Zheng,\n  David S. Rosenblum", "title": "Revisiting Convolutional Neural Networks for Citywide Crowd Flow\n  Analytics", "comments": "to appear at ECML-PKDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citywide crowd flow analytics is of great importance to smart city efforts.\nIt aims to model the crowd flow (e.g., inflow and outflow) of each region in a\ncity based on historical observations. Nowadays, Convolutional Neural Networks\n(CNNs) have been widely adopted in raster-based crowd flow analytics by virtue\nof their capability in capturing spatial dependencies. After revisiting\nCNN-based methods for different analytics tasks, we expose two common critical\ndrawbacks in the existing uses: 1) inefficiency in learning global spatial\ndependencies, and 2) overlooking latent region functions. To tackle these\nchallenges, in this paper we present a novel framework entitled DeepLGR that\ncan be easily generalized to address various citywide crowd flow analytics\nproblems. This framework consists of three parts: 1) a local feature extraction\nmodule to learn representations for each region; 2) a global context module to\nextract global contextual priors and upsample them to generate the global\nfeatures; and 3) a region-specific predictor based on tensor decomposition to\nprovide customized predictions for each region, which is very\nparameter-efficient compared to previous methods. Extensive experiments on two\ntypical crowd flow analytics tasks demonstrate the effectiveness, stability,\nand generality of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 12:21:31 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 06:34:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Liang", "Yuxuan", ""], ["Ouyang", "Kun", ""], ["Wang", "Yiwei", ""], ["Liu", "Ye", ""], ["Zhang", "Junbo", ""], ["Zheng", "Yu", ""], ["Rosenblum", "David S.", ""]]}, {"id": "2003.00902", "submitter": "Memo Akten", "authors": "Memo Akten, Rebecca Fiebrink, Mick Grierson", "title": "Learning to See: You Are What You See", "comments": "Presented as an Art Paper at SIGGRAPH 2019", "journal-ref": "ACM SIGGRAPH 2019 Art Gallery July 2019 Article No 13 Pages 1 to 6", "doi": "10.1145/3306211.3320143", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors present a visual instrument developed as part of the creation of\nthe artwork Learning to See. The artwork explores bias in artificial neural\nnetworks and provides mechanisms for the manipulation of specifically trained\nfor real-world representations. The exploration of these representations acts\nas a metaphor for the process of developing a visual understanding and/or\nvisual vocabulary of the world. These representations can be explored and\nmanipulated in real time, and have been produced in such a way so as to reflect\nspecific creative perspectives that call into question the relationship between\nhow both artificial neural networks and humans may construct meaning.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 07:12:52 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akten", "Memo", ""], ["Fiebrink", "Rebecca", ""], ["Grierson", "Mick", ""]]}, {"id": "2003.00908", "submitter": "Felix J\\\"aremo Lawin", "authors": "Andreas Robinson, Felix J\\\"aremo Lawin, Martin Danelljan, Fahad\n  Shahbaz Khan, Michael Felsberg", "title": "Learning Fast and Robust Target Models for Video Object Segmentation", "comments": "CVPR 2020. arXiv admin note: substantial text overlap with\n  arXiv:1904.08630", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation (VOS) is a highly challenging problem since the\ninitial mask, defining the target object, is only given at test-time. The main\ndifficulty is to effectively handle appearance changes and similar background\nobjects, while maintaining accurate segmentation. Most previous approaches\nfine-tune segmentation networks on the first frame, resulting in impractical\nframe-rates and risk of overfitting. More recent methods integrate generative\ntarget appearance models, but either achieve limited robustness or require\nlarge amounts of training data.\n  We propose a novel VOS architecture consisting of two network components. The\ntarget appearance model consists of a light-weight module, which is learned\nduring the inference stage using fast optimization techniques to predict a\ncoarse but robust target segmentation. The segmentation model is exclusively\ntrained offline, designed to process the coarse scores into high quality\nsegmentation masks. Our method is fast, easily trainable and remains highly\neffective in cases of limited training data. We perform extensive experiments\non the challenging YouTube-VOS and DAVIS datasets. Our network achieves\nfavorable performance, while operating at higher frame-rates compared to\nstate-of-the-art. Code and trained models are available at\nhttps://github.com/andr345/frtm-vos.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:58:06 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 09:58:00 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Robinson", "Andreas", ""], ["Lawin", "Felix J\u00e4remo", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "2003.00910", "submitter": "Memo Akten", "authors": "Memo Akten, Rebecca Fiebrink, Mick Grierson", "title": "Deep Meditations: Controlled navigation of latent space", "comments": "Presented at the 2nd Workshop on Machine Learning for Creativity and\n  Design at the Neural Information Processing Systems (NeurIPS) 2018 conference\n  in Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method which allows users to creatively explore and navigate\nthe vast latent spaces of deep generative models. Specifically, our method\nenables users to \\textit{discover} and \\textit{design} \\textit{trajectories} in\nthese high dimensional spaces, to construct stories, and produce time-based\nmedia such as videos---\\textit{with meaningful control over narrative}. Our\ngoal is to encourage and aid the use of deep generative models as a medium for\ncreative expression and story telling with meaningful human control. Our method\nis analogous to traditional video production pipelines in that we use a\nconventional non-linear video editor with proxy clips, and conform with arrays\nof latent space vectors. Examples can be seen at\n\\url{http://deepmeditations.ai}.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 21:19:44 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Akten", "Memo", ""], ["Fiebrink", "Rebecca", ""], ["Grierson", "Mick", ""]]}, {"id": "2003.00943", "submitter": "Adri\\`a Arbu\\'es-Sang\\\"uesa", "authors": "Adri\\`a Arbu\\'es-Sang\\\"uesa, Adri\\'an Mart\\'in, Javier Fern\\'andez,\n  Carlos Rodr\\'iguez, Gloria Haro, Coloma Ballester", "title": "Always Look on the Bright Side of the Field: Merging Pose and Contextual\n  Data to Estimate Orientation of Soccer Players", "comments": "Article accepted in the International Conference on Image Processing\n  (ICIP 2020); Appendix was not included in the original manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although orientation has proven to be a key skill of soccer players in order\nto succeed in a broad spectrum of plays, body orientation is a\nyet-little-explored area in sports analytics' research. Despite being an\ninherently ambiguous concept, player orientation can be defined as the\nprojection (2D) of the normal vector placed in the center of the upper-torso of\nplayers (3D). This research presents a novel technique to obtain player\norientation from monocular video recordings by mapping pose parts (shoulders\nand hips) in a 2D field by combining OpenPose with a super-resolution network,\nand merging the obtained estimation with contextual information (ball\nposition). Results have been validated with players-held EPTS devices,\nobtaining a median error of 27 degrees/player. Moreover, three novel types of\norientation maps are proposed in order to make raw orientation data easy to\nvisualize and understand, thus allowing further analysis at team- or\nplayer-level.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:42:51 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 15:32:43 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Arbu\u00e9s-Sang\u00fcesa", "Adri\u00e0", ""], ["Mart\u00edn", "Adri\u00e1n", ""], ["Fern\u00e1ndez", "Javier", ""], ["Rodr\u00edguez", "Carlos", ""], ["Haro", "Gloria", ""], ["Ballester", "Coloma", ""]]}, {"id": "2003.00951", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Thomas Ledwon, Yao Rong, Neslihan Kose, Gerhard\n  Rigoll", "title": "DriverMHG: A Multi-Modal Dataset for Dynamic Recognition of Driver Micro\n  Hand Gestures and a Real-Time Recognition Framework", "comments": "Accepted to IEEE International Conference on Automatic Face and\n  Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of hand gestures provides a natural alternative to cumbersome\ninterface devices for Human-Computer Interaction (HCI) systems. However,\nreal-time recognition of dynamic micro hand gestures from video streams is\nchallenging for in-vehicle scenarios since (i) the gestures should be performed\nnaturally without distracting the driver, (ii) micro hand gestures occur within\nvery short time intervals at spatially constrained areas, (iii) the performed\ngesture should be recognized only once, and (iv) the entire architecture should\nbe designed lightweight as it will be deployed to an embedded system. In this\nwork, we propose an HCI system for dynamic recognition of driver micro hand\ngestures, which can have a crucial impact in automotive sector especially for\nsafety related issues. For this purpose, we initially collected a dataset named\nDriver Micro Hand Gestures (DriverMHG), which consists of RGB, depth and\ninfrared modalities. The challenges for dynamic recognition of micro hand\ngestures have been addressed by proposing a lightweight convolutional neural\nnetwork (CNN) based architecture which operates online efficiently with a\nsliding window approach. For the CNN model, several 3-dimensional resource\nefficient networks are applied and their performances are analyzed. Online\nrecognition of gestures has been performed with 3D-MobileNetV2, which provided\nthe best offline accuracy among the applied networks with similar computational\ncomplexities. The final architecture is deployed on a driver simulator\noperating in real-time. We make DriverMHG dataset and our source code publicly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:54:19 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Ledwon", "Thomas", ""], ["Rong", "Yao", ""], ["Kose", "Neslihan", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "2003.00953", "submitter": "Yueting Chen", "authors": "Yueting Chen and Xiaohui Yu and Nick Koudas", "title": "Evaluating Temporal Queries Over Video Feeds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Computer Vision and Deep Learning made possible the\nefficient extraction of a schema from frames of streaming video. As such, a\nstream of objects and their associated classes along with unique object\nidentifiers derived via object tracking can be generated, providing unique\nobjects as they are captured across frames. In this paper we initiate a study\nof temporal queries involving objects and their co-occurrences in video feeds.\nFor example, queries that identify video segments during which the same two red\ncars and the same two humans appear jointly for five minutes are of interest to\nmany applications ranging from law enforcement to security and safety. We take\nthe first step and define such queries in a way that they incorporate certain\nphysical aspects of video capture such as object occlusion. We present an\narchitecture consisting of three layers, namely object detection/tracking,\nintermediate data generation and query evaluation. We propose two\ntechniques,MFS and SSG, to organize all detected objects in the intermediate\ndata generation layer, which effectively, given the queries, minimizes the\nnumber of objects and frames that have to be considered during query\nevaluation. We also introduce an algorithm called State Traversal (ST) that\nprocesses incoming frames against the SSG and efficiently prunes objects and\nframes unrelated to query evaluation, while maintaining all states required for\nsuccinct query evaluation. We present the results of a thorough experimental\nevaluation utilizing both real and synthetic data establishing the trade-offs\nbetween MFS and SSG. We stress various parameters of interest in our evaluation\nand demonstrate that the proposed query evaluation methodology coupled with the\nproposed algorithms is capable to evaluate temporal queries over video feeds\nefficiently, achieving orders of magnitude performance benefits.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 14:55:57 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 14:16:18 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 22:22:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Yueting", ""], ["Yu", "Xiaohui", ""], ["Koudas", "Nick", ""]]}, {"id": "2003.00981", "submitter": "Michael Ying Yang", "authors": "Ye Lyu, Michael Ying Yang, George Vosselman, Gui-Song Xia", "title": "Plug & Play Convolutional Regression Tracker for Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object detection targets to simultaneously localize the bounding boxes\nof the objects and identify their classes in a given video. One challenge for\nvideo object detection is to consistently detect all objects across the whole\nvideo. As the appearance of objects may deteriorate in some frames, features or\ndetections from the other frames are commonly used to enhance the prediction.\nIn this paper, we propose a Plug & Play scale-adaptive convolutional regression\ntracker for the video object detection task, which could be easily and\ncompatibly implanted into the current state-of-the-art detection networks. As\nthe tracker reuses the features from the detector, it is a very light-weighted\nincrement to the detection network. The whole network performs at the speed\nclose to a standard object detector. With our new video object detection\npipeline design, image object detectors can be easily turned into efficient\nvideo object detectors without modifying any parameters. The performance is\nevaluated on the large-scale ImageNet VID dataset. Our Plug & Play design\nimproves mAP score for the image detector by around 5% with only little speed\ndrop.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 15:57:55 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lyu", "Ye", ""], ["Yang", "Michael Ying", ""], ["Vosselman", "George", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2003.01041", "submitter": "Ekanayake Mudiyanselage Mevan Bandara Ekanayake", "authors": "E.M.M.B. Ekanayake, B. Rathnayake, D.Y.L. Ranasinghe, S. Herath,\n  G.M.R.I. Godaliyadda, H.M.V.R. Herath, M.P.B. Ekanayake", "title": "Constrained Nonnegative Matrix Factorization for Blind Hyperspectral\n  Unmixing incorporating Endmember Independence", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral unmixing (HU) has become an important technique in exploiting\nhyperspectral data since it decomposes a mixed pixel into a collection of\nendmembers weighted by fractional abundances. The endmembers of a hyperspectral\nimage (HSI) are more likely to be generated by independent sources and be mixed\nin a macroscopic degree before arriving at the sensor element of the imaging\nspectrometer as mixed spectra. Over the past few decades, many attempts have\nfocused on imposing auxiliary constraints on the conventional nonnegative\nmatrix factorization (NMF) framework in order to effectively unmix these mixed\nspectra. As a promising step toward finding an optimum constraint to extract\nendmembers, this paper presents a novel blind HU algorithm, referred to as\nKurtosis-based Smooth Nonnegative Matrix Factorization (KbSNMF) which\nincorporates a novel constraint based on the statistical independence of the\nprobability density functions of endmember spectra. Imposing this constraint on\nthe conventional NMF framework promotes the extraction of independent\nendmembers while further enhancing the parts-based representation of data.\nExperiments conducted on diverse synthetic HSI datasets (with numerous numbers\nof endmembers, spectral bands, pixels, and noise levels) and three standard\nreal HSI datasets demonstrate the validity of the proposed KbSNMF algorithm\ncompared to several state-of-the-art NMF-based HU baselines. The proposed\nalgorithm exhibits superior performance especially in terms of extracting\nendmember spectra from hyperspectral data; therefore, it could uplift the\nperformance of recent deep learning HU methods which utilize the endmember\nspectra as supervisory input data for abundance extraction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:20:04 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 09:17:28 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 19:26:40 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 07:37:53 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ekanayake", "E. M. M. B.", ""], ["Rathnayake", "B.", ""], ["Ranasinghe", "D. Y. L.", ""], ["Herath", "S.", ""], ["Godaliyadda", "G. M. R. I.", ""], ["Herath", "H. M. V. R.", ""], ["Ekanayake", "M. P. B.", ""]]}, {"id": "2003.01060", "submitter": "Nan Yang", "authors": "Nan Yang and Lukas von Stumberg and Rui Wang and Daniel Cremers", "title": "D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual\n  Odometry", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose D3VO as a novel framework for monocular visual odometry that\nexploits deep networks on three levels -- deep depth, pose and uncertainty\nestimation. We first propose a novel self-supervised monocular depth estimation\nnetwork trained on stereo videos without any external supervision. In\nparticular, it aligns the training image pairs into similar lighting condition\nwith predictive brightness transformation parameters. Besides, we model the\nphotometric uncertainties of pixels on the input images, which improves the\ndepth estimation accuracy and provides a learned weighting function for the\nphotometric residuals in direct (feature-less) visual odometry. Evaluation\nresults show that the proposed network outperforms state-of-the-art\nself-supervised depth estimation networks. D3VO tightly incorporates the\npredicted depth, pose and uncertainty into a direct visual odometry method to\nboost both the front-end tracking as well as the back-end non-linear\noptimization. We evaluate D3VO in terms of monocular visual odometry on both\nthe KITTI odometry benchmark and the EuRoC MAV dataset.The results show that\nD3VO outperforms state-of-the-art traditional monocular VO methods by a large\nmargin. It also achieves comparable results to state-of-the-art stereo/LiDAR\nodometry on KITTI and to the state-of-the-art visual-inertial odometry on EuRoC\nMAV, while using only a single camera.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:47:13 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 21:08:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yang", "Nan", ""], ["von Stumberg", "Lukas", ""], ["Wang", "Rui", ""], ["Cremers", "Daniel", ""]]}, {"id": "2003.01063", "submitter": "Marija Jegorova", "authors": "Marija Jegorova, Antti Ilari Karjalainen, Jose Vazquez, Timothy M.\n  Hospedales", "title": "Unlimited Resolution Image Generation with R2D2-GANs", "comments": "Accepted to 2020 IEEE OCEANS (Singapore)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel simulation technique for generating high\nquality images of any predefined resolution. This method can be used to\nsynthesize sonar scans of size equivalent to those collected during a\nfull-length mission, with across track resolutions of any chosen magnitude. In\nessence, our model extends Generative Adversarial Networks (GANs) based\narchitecture into a conditional recursive setting, that facilitates the\ncontinuity of the generated images. The data produced is continuous,\nrealistically-looking, and can also be generated at least two times faster than\nthe real speed of acquisition for the sonars with higher resolutions, such as\nEdgeTech. The seabed topography can be fully controlled by the user. The visual\nassessment tests demonstrate that humans cannot distinguish the simulated\nimages from real. Moreover, experimental results suggest that in the absence of\nreal data the autonomous recognition systems can benefit greatly from training\nwith the synthetic data, produced by the R2D2-GANs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 17:49:32 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Jegorova", "Marija", ""], ["Karjalainen", "Antti Ilari", ""], ["Vazquez", "Jose", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "2003.01090", "submitter": "Ahmadreza Jeddi", "authors": "Ahmadreza Jeddi, Mohammad Javad Shafiee, Michelle Karg, Christian\n  Scharfenberger and Alexander Wong", "title": "Learn2Perturb: an End-to-end Feature Perturbation Learning to Improve\n  Adversarial Robustness", "comments": "13 pages, 6 figures To be published in proceedings of IEEE conference\n  on Computer Vision and Pattern Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have been achieving state-of-the-art performance\nacross a wide variety of applications, their vulnerability to adversarial\nattacks limits their widespread deployment for safety-critical applications.\nAlongside other adversarial defense approaches being investigated, there has\nbeen a very recent interest in improving adversarial robustness in deep neural\nnetworks through the introduction of perturbations during the training process.\nHowever, such methods leverage fixed, pre-defined perturbations and require\nsignificant hyper-parameter tuning that makes them very difficult to leverage\nin a general fashion. In this study, we introduce Learn2Perturb, an end-to-end\nfeature perturbation learning approach for improving the adversarial robustness\nof deep neural networks. More specifically, we introduce novel\nperturbation-injection modules that are incorporated at each layer to perturb\nthe feature space and increase uncertainty in the network. This feature\nperturbation is performed at both the training and the inference stages.\nFurthermore, inspired by the Expectation-Maximization, an alternating\nback-propagation training algorithm is introduced to train the network and\nnoise parameters consecutively. Experimental results on CIFAR-10 and CIFAR-100\ndatasets show that the proposed Learn2Perturb method can result in deep neural\nnetworks which are $4-7\\%$ more robust on $l_{\\infty}$ FGSM and PDG adversarial\nattacks and significantly outperforms the state-of-the-art against $l_2$ $C\\&W$\nattack and a wide range of well-known black-box attacks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 18:27:35 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 16:51:46 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Jeddi", "Ahmadreza", ""], ["Shafiee", "Mohammad Javad", ""], ["Karg", "Michelle", ""], ["Scharfenberger", "Christian", ""], ["Wong", "Alexander", ""]]}, {"id": "2003.01109", "submitter": "Li Xiao", "authors": "Li Xiao, Cheng Zhu, Junjun Liu, Chunlong Luo, Peifang Liu, Yi Zhao", "title": "Learning from Suspected Target: Bootstrapping Performance for Breast\n  Cancer Detection in Mammography", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32226-7_52", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning object detection algorithm has been widely used in medical\nimage analysis. Currently all the object detection tasks are based on the data\nannotated with object classes and their bounding boxes. On the other hand,\nmedical images such as mammography usually contain normal regions or objects\nthat are similar to the lesion region, and may be misclassified in the testing\nstage if they are not taken care of. In this paper, we address such problem by\nintroducing a novel top likelihood loss together with a new sampling procedure\nto select and train the suspected target regions, as well as proposing a\nsimilarity loss to further identify suspected targets from targets. Mean\naverage precision (mAP) according to the predicted targets and specificity,\nsensitivity, accuracy, AUC values according to classification of patients are\nadopted for performance comparisons. We firstly test our proposed method on a\nprivate dense mammogram dataset. Results show that our proposed method greatly\nreduce the false positive rate and the specificity is increased by 0.25 on\ndetecting mass type cancer. It is worth mention that dense breast typically has\na higher risk for developing breast cancers and also are harder for cancer\ndetection in diagnosis, and our method outperforms a reported result from\nperformance of radiologists. Our method is also validated on the public Digital\nDatabase for Screening Mammography (DDSM) dataset, brings significant\nimprovement on mass type cancer detection and outperforms the most\nstate-of-the-art work.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 09:04:24 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xiao", "Li", ""], ["Zhu", "Cheng", ""], ["Liu", "Junjun", ""], ["Luo", "Chunlong", ""], ["Liu", "Peifang", ""], ["Zhao", "Yi", ""]]}, {"id": "2003.01111", "submitter": "Yu Zhang", "authors": "Yu Zhang, Gongbo Liang, Nathan Jacobs, Xiaoqin Wang", "title": "Unsupervised Domain Adaptation for Mammogram Image Classification: A\n  Promising Tool for Model Generalization", "comments": "Accepted by C-MIMI 2019 as a scientific abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization is one of the key challenges in the clinical validation and\napplication of deep learning models to medical images. Studies have shown that\nsuch models trained on publicly available datasets often do not work well on\nreal-world clinical data due to the differences in patient population and image\ndevice configurations. Also, manually annotating clinical images is expensive.\nIn this work, we propose an unsupervised domain adaptation (UDA) method using\nCycle-GAN to improve the generalization ability of the model without using any\nadditional manual annotations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 02:42:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Yu", ""], ["Liang", "Gongbo", ""], ["Jacobs", "Nathan", ""], ["Wang", "Xiaoqin", ""]]}, {"id": "2003.01163", "submitter": "Chen Jiang", "authors": "Chen Jiang, Masood Dehghan, Martin Jagersand", "title": "Understanding Contexts Inside Robot and Human Manipulation Tasks through\n  a Vision-Language Model and Ontology System in a Video Stream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manipulation tasks in daily life, such as pouring water, unfold intentionally\nunder specialized manipulation contexts. Being able to process contextual\nknowledge in these Activities of Daily Living (ADLs) over time can help us\nunderstand manipulation intentions, which are essential for an intelligent\nrobot to transition smoothly between various manipulation actions. In this\npaper, to model the intended concepts of manipulation, we present a vision\ndataset under a strictly constrained knowledge domain for both robot and human\nmanipulations, where manipulation concepts and relations are stored by an\nontology system in a taxonomic manner. Furthermore, we propose a scheme to\ngenerate a combination of visual attentions and an evolving knowledge graph\nfilled with commonsense knowledge. Our scheme works with real-world camera\nstreams and fuses an attention-based Vision-Language model with the ontology\nsystem. The experimental results demonstrate that the proposed scheme can\nsuccessfully represent the evolution of an intended object manipulation\nprocedure for both robots and humans. The proposed scheme allows the robot to\nmimic human-like intentional behaviors by watching real-time videos. We aim to\ndevelop this scheme further for real-world robot intelligence in Human-Robot\nInteraction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 19:48:59 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Jiang", "Chen", ""], ["Dehghan", "Masood", ""], ["Jagersand", "Martin", ""]]}, {"id": "2003.01174", "submitter": "Peng Jiang", "authors": "Peng Jiang and Srikanth Saripalli", "title": "LiDARNet: A Boundary-Aware Domain Adaptation Model for Point Cloud\n  Semantic Segmentation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a boundary-aware domain adaptation model for LiDAR scan full-scene\nsemantic segmentation (LiDARNet). Our model can extract both the domain private\nfeatures and the domain shared features with a two-branch structure. We\nembedded Gated-SCNN into the segmentor component of LiDARNet to learn boundary\ninformation while learning to predict full-scene semantic segmentation labels.\nMoreover, we further reduce the domain gap by inducing the model to learn a\nmapping between two domains using the domain shared and private features.\nAdditionally, we introduce a new dataset (SemanticUSL\\footnote{The access\naddress of\nSemanticUSL:\\url{https://unmannedlab.github.io/research/SemanticUSL}}) for\ndomain adaptation for LiDAR point cloud semantic segmentation. The dataset has\nthe same data format and ontology as SemanticKITTI. We conducted experiments on\nreal-world datasets SemanticKITTI, SemanticPOSS, and SemanticUSL, which have\ndifferences in channel distributions, reflectivity distributions, diversity of\nscenes, and sensors setup. Using our approach, we can get a single\nprojection-based LiDAR full-scene semantic segmentation model working on both\ndomains. Our model can keep almost the same performance on the source domain\nafter adaptation and get an 8\\%-22\\% mIoU performance increase in the target\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:18:31 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 22:31:09 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 21:32:21 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jiang", "Peng", ""], ["Saripalli", "Srikanth", ""]]}, {"id": "2003.01181", "submitter": "Shenyang Huang", "authors": "Stefano Alletto, Shenyang Huang, Vincent Francois-Lavet, Yohei Nakata\n  and Guillaume Rabusseau", "title": "RandomNet: Towards Fully Automatic Neural Architecture Design for\n  Multimodal Learning", "comments": "6 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all neural architecture search methods are evaluated in terms of\nperformance (i.e. test accuracy) of the model structures that it finds. Should\nit be the only metric for a good autoML approach? To examine aspects beyond\nperformance, we propose a set of criteria aimed at evaluating the core of\nautoML problem: the amount of human intervention required to deploy these\nmethods into real world scenarios. Based on our proposed evaluation checklist,\nwe study the effectiveness of a random search strategy for fully automated\nmultimodal neural architecture search. Compared to traditional methods that\nrely on manually crafted feature extractors, our method selects each modality\nfrom a large search space with minimal human supervision. We show that our\nproposed random search strategy performs close to the state of the art on the\nAV-MNIST dataset while meeting the desirable characteristics for a fully\nautomated design process.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 20:41:57 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Alletto", "Stefano", ""], ["Huang", "Shenyang", ""], ["Francois-Lavet", "Vincent", ""], ["Nakata", "Yohei", ""], ["Rabusseau", "Guillaume", ""]]}, {"id": "2003.01196", "submitter": "Haining Zheng", "authors": "Nimish M. Awalgaonkar, Haining Zheng, Christopher S. Gurciullo", "title": "DEEVA: A Deep Learning and IoT Based Computer Vision System to Address\n  Safety and Security of Production Sites in Energy Industry", "comments": "AAAI-20 Workshop on Artificial Intelligence of Things (AIoT) workshop\n  in conjunction with the 34th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to addressing the safety/security related needs at different\nproduction/construction sites, accurate detection of the presence of workers,\nvehicles, equipment important and formed an integral part of computer\nvision-based surveillance systems (CVSS). Traditional CVSS systems focus on the\nuse of different computer vision and pattern recognition algorithms overly\nreliant on manual extraction of features and small datasets, limiting their\nusage because of low accuracy, need for expert knowledge and high computational\ncosts. The main objective of this paper is to provide decision makers at sites\nwith a practical yet comprehensive deep learning and IoT based solution to\ntackle various computer vision related problems such as scene classification,\nobject detection in scenes, semantic segmentation, scene captioning etc. Our\noverarching goal is to address the central question of What is happening at\nthis site and where is it happening in an automated fashion minimizing the need\nfor human resources dedicated to surveillance. We developed Deep ExxonMobil Eye\nfor Video Analysis (DEEVA) package to handle scene classification, object\ndetection, semantic segmentation and captioning of scenes in a hierarchical\napproach. The results reveal that transfer learning with the RetinaNet object\ndetector is able to detect the presence of workers, different types of\nvehicles/construction equipment, safety related objects at a high level of\naccuracy (above 90%). With the help of deep learning to automatically extract\nfeatures and IoT technology to automatic capture, transfer and process vast\namount of realtime images, this framework is an important step towards the\ndevelopment of intelligent surveillance systems aimed at addressing myriads of\nopen ended problems in the realm of security/safety monitoring, productivity\nassessments and future decision making.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 21:26:00 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Awalgaonkar", "Nimish M.", ""], ["Zheng", "Haining", ""], ["Gurciullo", "Christopher S.", ""]]}, {"id": "2003.01204", "submitter": "Priyadarshini Panda", "authors": "Aosong Feng, and Priyadarshini Panda", "title": "Energy-efficient and Robust Cumulative Training with Net2Net\n  Transformation", "comments": "6 pages, 6 figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved state-of-the-art accuracies on several computer\nvision tasks. However, the computational and energy requirements associated\nwith training such deep neural networks can be quite high. In this paper, we\npropose a cumulative training strategy with Net2Net transformation that\nachieves training computational efficiency without incurring large accuracy\nloss, in comparison to a model trained from scratch. We achieve this by first\ntraining a small network (with lesser parameters) on a small subset of the\noriginal dataset, and then gradually expanding the network using Net2Net\ntransformation to train incrementally on larger subsets of the dataset. This\nincremental training strategy with Net2Net utilizes function-preserving\ntransformations that transfers knowledge from each previous small network to\nthe next larger network, thereby, reducing the overall training complexity. Our\nexperiments demonstrate that compared with training from scratch, cumulative\ntraining yields ~2x reduction in computational complexity for training\nTinyImageNet using VGG19 at iso-accuracy. Besides training efficiency, a key\nadvantage of our cumulative training strategy is that we can perform pruning\nduring Net2Net expansion to obtain a final network with optimal configuration\n(~0.4x lower inference compute complexity) compared to conventional training\nfrom scratch. We also demonstrate that the final network obtained from\ncumulative training yields better generalization performance and noise\nrobustness. Further, we show that mutual inference from all the networks\ncreated with cumulative Net2Net expansion enables improved adversarial input\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 21:44:47 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Feng", "Aosong", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2003.01217", "submitter": "Yuhua Chen", "authors": "Yuhua Chen, Anthony G. Christodoulou, Zhengwei Zhou, Feng Shi, Yibin\n  Xie, Debiao Li", "title": "MRI Super-Resolution with GAN and 3D Multi-Level DenseNet: Smaller,\n  Faster, and Better", "comments": "Preprint submitted to Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution (HR) magnetic resonance imaging (MRI) provides detailed\nanatomical information that is critical for diagnosis in the clinical\napplication. However, HR MRI typically comes at the cost of long scan time,\nsmall spatial coverage, and low signal-to-noise ratio (SNR). Recent studies\nshowed that with a deep convolutional neural network (CNN), HR generic images\ncould be recovered from low-resolution (LR) inputs via single image\nsuper-resolution (SISR) approaches. Additionally, previous works have shown\nthat a deep 3D CNN can generate high-quality SR MRIs by using learned image\npriors. However, 3D CNN with deep structures, have a large number of parameters\nand are computationally expensive. In this paper, we propose a novel 3D CNN\narchitecture, namely a multi-level densely connected super-resolution network\n(mDCSRN), which is light-weight, fast and accurate. We also show that with the\ngenerative adversarial network (GAN)-guided training, the mDCSRN-GAN provides\nappealing sharp SR images with rich texture details that are highly comparable\nwith the referenced HR images. Our results from experiments on a large public\ndataset with 1,113 subjects showed that this new architecture outperformed\nother popular deep learning methods in recovering 4x resolution-downgraded\nimages in both quality and speed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:07:56 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 23:46:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Yuhua", ""], ["Christodoulou", "Anthony G.", ""], ["Zhou", "Zhengwei", ""], ["Shi", "Feng", ""], ["Xie", "Yibin", ""], ["Li", "Debiao", ""]]}, {"id": "2003.01223", "submitter": "Anirudh Chandrashekar", "authors": "Anirudh Chandrashekar, Ashok Handa, Natesh Shivakumar, Pierfrancesco\n  Lapolla, Vicente Grau, Regent Lee", "title": "A Deep learning Approach to Generate Contrast-Enhanced Computerised\n  Tomography Angiography without the Use of Intravenous Contrast Agents", "comments": "7 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrast-enhanced computed tomography angiograms (CTAs) are widely used in\ncardiovascular imaging to obtain a non-invasive view of arterial structures.\nHowever, contrast agents are associated with complications at the injection\nsite as well as renal toxicity leading to contrast-induced nephropathy (CIN)\nand renal failure. We hypothesised that the raw data acquired from a\nnon-contrast CT contains sufficient information to differentiate blood and\nother soft tissue components. We utilised deep learning methods to define the\nsubtleties between soft tissue components in order to simulate contrast\nenhanced CTAs without contrast agents. Twenty-six patients with paired\nnon-contrast and CTA images were randomly selected from an approved clinical\nstudy. Non-contrast axial slices within the AAA from 10 patients (n = 100) were\nsampled for the underlying Hounsfield unit (HU) distribution at the lumen,\nintra-luminal thrombus and interface locations. Sampling of HUs in these\nregions revealed significant differences between all regions (p<0.001 for all\ncomparisons), confirming the intrinsic differences in the radiomic signatures\nbetween these regions. To generate a large training dataset, paired axial\nslices from the training set (n=13) were augmented to produce a total of 23,551\n2-D images. We trained a 2-D Cycle Generative Adversarial Network (cycleGAN)\nfor this non-contrast to contrast (NC2C) transformation task. The accuracy of\nthe cycleGAN output was assessed by comparison to the contrast image. This\npipeline is able to differentiate between visually incoherent soft tissue\nregions in non-contrast CT images. The CTAs generated from the non-contrast\nimages bear strong resemblance to the ground truth. Here we describe a novel\napplication of Generative Adversarial Network for CT image processing. This is\npoised to disrupt clinical pathways requiring contrast enhanced CT imaging.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:20:08 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Chandrashekar", "Anirudh", ""], ["Handa", "Ashok", ""], ["Shivakumar", "Natesh", ""], ["Lapolla", "Pierfrancesco", ""], ["Grau", "Vicente", ""], ["Lee", "Regent", ""]]}, {"id": "2003.01234", "submitter": "Jose Bouza", "authors": "Jose J. Bouza, Chun-Hao Yang, David Vaillancourt, Baba C. Vemuri", "title": "MVC-Net: A Convolutional Neural Network Architecture for Manifold-Valued\n  Images With Applications", "comments": "Equal contribution by J. Bouza and CH. Yang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric deep learning has attracted significant attention in recent years,\nin part due to the availability of exotic data types for which traditional\nneural network architectures are not well suited. Our goal in this paper is to\ngeneralize convolutional neural networks (CNN) to the manifold-valued image\ncase which arises commonly in medical imaging and computer vision applications.\nExplicitly, the input data to the network is an image where each pixel value is\na sample from a Riemannian manifold. To achieve this goal, we must generalize\nthe basic building block of traditional CNN architectures, namely, the weighted\ncombinations operation. To this end, we develop a tangent space combination\noperation which is used to define a convolution operation on manifold-valued\nimages that we call, the Manifold-Valued Convolution (MVC). We prove\ntheoretical properties of the MVC operation, including equivariance to the\naction of the isometry group admitted by the manifold and characterizing when\ncompositions of MVC layers collapse to a single layer. We present a detailed\ndescription of how to use MVC layers to build full, multi-layer neural networks\nthat operate on manifold-valued images, which we call the MVC-net. Further, we\nempirically demonstrate superior performance of the MVC-nets in medical imaging\nand computer vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:37:56 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 17:40:23 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Bouza", "Jose J.", ""], ["Yang", "Chun-Hao", ""], ["Vaillancourt", "David", ""], ["Vemuri", "Baba C.", ""]]}, {"id": "2003.01251", "submitter": "Weijing Shi", "authors": "Weijing Shi and Ragunathan (Raj) Rajkumar", "title": "Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a graph neural network to detect objects from a\nLiDAR point cloud. Towards this end, we encode the point cloud efficiently in a\nfixed radius near-neighbors graph. We design a graph neural network, named\nPoint-GNN, to predict the category and shape of the object that each vertex in\nthe graph belongs to. In Point-GNN, we propose an auto-registration mechanism\nto reduce translation variance, and also design a box merging and scoring\noperation to combine detections from multiple vertices accurately. Our\nexperiments on the KITTI benchmark show the proposed approach achieves leading\naccuracy using the point cloud alone and can even surpass fusion-based\nalgorithms. Our results demonstrate the potential of using the graph neural\nnetwork as a new approach for 3D object detection. The code is available\nhttps://github.com/WeijingShi/Point-GNN.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 23:44:12 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Shi", "Weijing", "", "Raj"], ["Ragunathan", "", "", "Raj"], ["Rajkumar", "", ""]]}, {"id": "2003.01267", "submitter": "Murilo Marques Marinho", "authors": "Masakazu Yoshimura, Murilo M. Marinho, Kanako Harada, Mamoru Mitsuishi", "title": "Single-Shot Pose Estimation of Surgical Robot Instruments' Shafts from\n  Monocular Endoscopic Images", "comments": "Accepted on ICRA 2020, 7 pages", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA), Paris, Palais des Congres de Paris, 2020, pp. 9960-9966", "doi": "10.1109/ICRA40945.2020.9196779", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical robots are used to perform minimally invasive surgery and alleviate\nmuch of the burden imposed on surgeons. Our group has developed a surgical\nrobot to aid in the removal of tumors at the base of the skull via access\nthrough the nostrils. To avoid injuring the patients, a collision-avoidance\nalgorithm that depends on having an accurate model for the poses of the\ninstruments' shafts is used. Given that the model's parameters can change over\ntime owing to interactions between instruments and other disturbances, the\nonline estimation of the poses of the instrument's shaft is essential. In this\nwork, we propose a new method to estimate the pose of the surgical instruments'\nshafts using a monocular endoscope. Our method is based on the use of an\nautomatically annotated training dataset and an improved pose-estimation\ndeep-learning architecture. In preliminary experiments, we show that our method\ncan surpass state of the art vision-based marker-less pose estimation\ntechniques (providing an error decrease of 55% in position estimation, 64% in\npitch, and 69% in yaw) by using artificial images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 00:38:48 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Yoshimura", "Masakazu", ""], ["Marinho", "Murilo M.", ""], ["Harada", "Kanako", ""], ["Mitsuishi", "Mamoru", ""]]}, {"id": "2003.01279", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Sarah Adel Bargal, Stan Sclaroff", "title": "Disrupting Deepfakes: Adversarial Attacks Against Conditional Image\n  Translation Networks and Facial Manipulation Systems", "comments": "Accepted at CVPR 2020 Workshop on Adversarial Machine Learning in\n  Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face modification systems using deep learning have become increasingly\npowerful and accessible. Given images of a person's face, such systems can\ngenerate new images of that same person under different expressions and poses.\nSome systems can also modify targeted attributes such as hair color or age.\nThis type of manipulated images and video have been coined Deepfakes. In order\nto prevent a malicious user from generating modified images of a person without\ntheir consent we tackle the new problem of generating adversarial attacks\nagainst such image translation systems, which disrupt the resulting output\nimage. We call this problem disrupting deepfakes. Most image translation\narchitectures are generative models conditioned on an attribute (e.g. put a\nsmile on this person's face). We are first to propose and successfully apply\n(1) class transferable adversarial attacks that generalize to different\nclasses, which means that the attacker does not need to have knowledge about\nthe conditioning class, and (2) adversarial training for generative adversarial\nnetworks (GANs) as a first step towards robust image translation networks.\nFinally, in gray-box scenarios, blurring can mount a successful defense against\ndisruption. We present a spread-spectrum adversarial attack, which evades blur\ndefenses. Our open-source code can be found at\nhttps://github.com/natanielruiz/disrupting-deepfakes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:18:16 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 18:18:14 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 19:58:25 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Bargal", "Sarah Adel", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2003.01285", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Caiming Xiong, Richard Socher, Steven Hoi", "title": "Towards Noise-resistant Object Detection with Noisy Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep object detectors requires significant amount of human-annotated\nimages with accurate object labels and bounding box coordinates, which are\nextremely expensive to acquire. Noisy annotations are much more easily\naccessible, but they could be detrimental for learning. We address the\nchallenging problem of training object detectors with noisy annotations, where\nthe noise contains a mixture of label noise and bounding box noise. We propose\na learning framework which jointly optimizes object labels, bounding box\ncoordinates, and model parameters by performing alternating noise correction\nand model training. To disentangle label noise and bounding box noise, we\npropose a two-step noise correction method. The first step performs\nclass-agnostic bounding box correction by minimizing classifier discrepancy and\nmaximizing region objectness. The second step distils knowledge from dual\ndetection heads for soft label correction and class-specific bounding box\nrefinement. We conduct experiments on PASCAL VOC and MS-COCO dataset with both\nsynthetic noise and machine-generated noise. Our method achieves\nstate-of-the-art performance by effectively cleaning both label noise and\nbounding box noise. Code to reproduce all results will be released.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:32:16 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Li", "Junnan", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""], ["Hoi", "Steven", ""]]}, {"id": "2003.01288", "submitter": "Tetsuo Inoshita", "authors": "Tetsuo Inoshita, Yuichi Nakatani, Katsuhiko Takahashi, Asuka Ishii,\n  Gaku Nakano", "title": "Trained Model Fusion for Object Detection using Gating Network", "comments": "Accepted to ACPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major approaches of transfer learning in computer vision have tried to\nadapt the source domain to the target domain one-to-one. However, this scenario\nis difficult to apply to real applications such as video surveillance systems.\nAs those systems have many cameras installed at each location regarded as\nsource domains, it is difficult to identify the proper source domain. In this\npaper, we introduce a new transfer learning scenario that has various source\ndomains and one target domain, assuming video surveillance system integration.\nAlso, we propose a novel method for automatically producing a high accuracy\nmodel by fusing models trained at various source domains. In particular, we\nshow how to apply a gating network to fuse source domains for object detection\ntasks, which is a new approach. We demonstrate the effectiveness of our method\nthrough experiments on traffic surveillance datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:38:20 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Inoshita", "Tetsuo", ""], ["Nakatani", "Yuichi", ""], ["Takahashi", "Katsuhiko", ""], ["Ishii", "Asuka", ""], ["Nakano", "Gaku", ""]]}, {"id": "2003.01290", "submitter": "Hirohisa Oda", "authors": "Hirohisa Oda, Kohei Nishio, Takayuki Kitasaka, Hizuru Amano, Aitaro\n  Takimoto, Hiroo Uchida, Kojiro Suzuki, Hayato Itoh, Masahiro Oda, Kensaku\n  Mori", "title": "Visualizing intestines for diagnostic assistance of ileus based on\n  intestinal region segmentation from 3D CT images", "comments": null, "journal-ref": "SPIE Medical Imaging 2020, 11314-109", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a visualization method of intestine (the small and large\nintestines) regions and their stenosed parts caused by ileus from CT volumes.\nSince it is difficult for non-expert clinicians to find stenosed parts, the\nintestine and its stenosed parts should be visualized intuitively. Furthermore,\nthe intestine regions of ileus cases are quite hard to be segmented. The\nproposed method segments intestine regions by 3D FCN (3D U-Net). Intestine\nregions are quite difficult to be segmented in ileus cases since the inside the\nintestine is filled with fluids. These fluids have similar intensities with\nintestinal wall on 3D CT volumes. We segment the intestine regions by using 3D\nU-Net trained by a weak annotation approach. Weak-annotation makes possible to\ntrain the 3D U-Net with small manually-traced label images of the intestine.\nThis avoids us to prepare many annotation labels of the intestine that has long\nand winding shape. Each intestine segment is volume-rendered and colored based\non the distance from its endpoint in volume rendering. Stenosed parts (disjoint\npoints of an intestine segment) can be easily identified on such visualization.\nIn the experiments, we showed that stenosed parts were intuitively visualized\nas endpoints of segmented regions, which are colored by red or blue.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:40:51 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Oda", "Hirohisa", ""], ["Nishio", "Kohei", ""], ["Kitasaka", "Takayuki", ""], ["Amano", "Hizuru", ""], ["Takimoto", "Aitaro", ""], ["Uchida", "Hiroo", ""], ["Suzuki", "Kojiro", ""], ["Itoh", "Hayato", ""], ["Oda", "Masahiro", ""], ["Mori", "Kensaku", ""]]}, {"id": "2003.01295", "submitter": "Xiaolu Zhang", "authors": "ZhaoXin Huan, Yulong Wang, Xiaolu Zhang, Lin Shang, Chilin Fu, Jun\n  Zhou", "title": "Data-Free Adversarial Perturbations for Practical Black-Box Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are vulnerable to adversarial examples, which are malicious\ninputs crafted to fool pre-trained models. Adversarial examples often exhibit\nblack-box attacking transferability, which allows that adversarial examples\ncrafted for one model can fool another model. However, existing black-box\nattack methods require samples from the training data distribution to improve\nthe transferability of adversarial examples across different models. Because of\nthe data dependence, the fooling ability of adversarial perturbations is only\napplicable when training data are accessible. In this paper, we present a\ndata-free method for crafting adversarial perturbations that can fool a target\nmodel without any knowledge about the training data distribution. In the\npractical setting of a black-box attack scenario where attackers do not have\naccess to target models and training data, our method achieves high fooling\nrates on target models and outperforms other universal adversarial perturbation\nmethods. Our method empirically shows that current deep learning models are\nstill at risk even when the attackers do not have access to training data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:22:12 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Huan", "ZhaoXin", ""], ["Wang", "Yulong", ""], ["Zhang", "Xiaolu", ""], ["Shang", "Lin", ""], ["Fu", "Chilin", ""], ["Zhou", "Jun", ""]]}, {"id": "2003.01302", "submitter": "Changhao Sun", "authors": "Changhao Sun, Chen Li, Jinghua Zhang, Muhammad Rahaman, Shiliang Ai,\n  Hao Chen, Frank Kulwa, Yixin Li, Xiaoyan Li, Tao Jiang", "title": "Gastric histopathology image segmentation using a hierarchical\n  conditional random field", "comments": null, "journal-ref": "Biocybernetics and Biomedical Engineering, 2020, 40(4): 1535-1555", "doi": "10.1016/j.bbe.2020.09.008", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the Convolutional Neural Networks (CNNs) applied in the intelligent\ndiagnosis of gastric cancer, existing methods mostly focus on individual\ncharacteristics or network frameworks without a policy to depict the integral\ninformation. Mainly, Conditional Random Field (CRF), an efficient and stable\nalgorithm for analyzing images containing complicated contents, can\ncharacterize spatial relation in images. In this paper, a novel Hierarchical\nConditional Random Field (HCRF) based Gastric Histopathology Image Segmentation\n(GHIS) method is proposed, which can automatically localize abnormal (cancer)\nregions in gastric histopathology images obtained by an optical microscope to\nassist histopathologists in medical work. This HCRF model is built up with\nhigher order potentials, including pixel-level and patch-level potentials, and\ngraph-based post-processing is applied to further improve its segmentation\nperformance. Especially, a CNN is trained to build up the pixel-level\npotentials and another three CNNs are fine-tuned to build up the patch-level\npotentials for sufficient spatial segmentation information. In the experiment,\na hematoxylin and eosin (H&E) stained gastric histopathological dataset with\n560 abnormal images are divided into training, validation and test sets with a\nratio of 1 : 1 : 2. Finally, segmentation accuracy, recall and specificity of\n78.91%, 65.59%, and 81.33% are achieved on the test set. Our HCRF model\ndemonstrates high segmentation performance and shows its effectiveness and\nfuture potential in the GHIS field.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:44:31 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 06:14:37 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 09:18:14 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 11:20:50 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 14:03:28 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Sun", "Changhao", ""], ["Li", "Chen", ""], ["Zhang", "Jinghua", ""], ["Rahaman", "Muhammad", ""], ["Ai", "Shiliang", ""], ["Chen", "Hao", ""], ["Kulwa", "Frank", ""], ["Li", "Yixin", ""], ["Li", "Xiaoyan", ""], ["Jiang", "Tao", ""]]}, {"id": "2003.01337", "submitter": "Hanxiao Zhang", "authors": "Hanxiao Zhang, Jingxiong Li, Mali Shen, Yaqi Wang and Guang-Zhong Yang", "title": "DDU-Nets: Distributed Dense Model for 3D MRI Brain Tumor Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of brain tumors and their subregions remains a challenging task\ndue to their weak features and deformable shapes. In this paper, three patterns\n(cross-skip, skip-1 and skip-2) of distributed dense connections (DDCs) are\nproposed to enhance feature reuse and propagation of CNNs by constructing\ntunnels between key layers of the network. For better detecting and segmenting\nbrain tumors from multi-modal 3D MR images, CNN-based models embedded with DDCs\n(DDU-Nets) are trained efficiently from pixel to pixel with a limited number of\nparameters. Postprocessing is then applied to refine the segmentation results\nby reducing the false-positive samples. The proposed method is evaluated on the\nBraTS 2019 dataset with results demonstrating the effectiveness of the DDU-Nets\nwhile requiring less computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 05:08:34 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Hanxiao", ""], ["Li", "Jingxiong", ""], ["Shen", "Mali", ""], ["Wang", "Yaqi", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "2003.01357", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Di Qiu, Lok Ming Lui", "title": "Shape analysis via inconsistent surface registration", "comments": null, "journal-ref": "Proceedings of the Royal Society A, 476(2242), 20200147 (2020)", "doi": "10.1098/rspa.2020.0147", "report-no": null, "categories": "cs.CG cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a framework for shape analysis using inconsistent\nsurface mapping. Traditional landmark-based geometric morphometrics methods\nsuffer from the limited degrees of freedom, while most of the more advanced\nnon-rigid surface mapping methods rely on a strong assumption of the global\nconsistency of two surfaces. From a practical point of view, given two\nanatomical surfaces with prominent feature landmarks, it is more desirable to\nhave a method that automatically detects the most relevant parts of the two\nsurfaces and finds the optimal landmark-matching alignment between those parts,\nwithout assuming any global 1-1 correspondence between the two surfaces. Our\nmethod is capable of solving this problem using inconsistent surface\nregistration based on quasi-conformal theory. It further enables us to quantify\nthe dissimilarity of two shapes using quasi-conformal distortion and\ndifferences in mean and Gaussian curvatures, thereby providing a natural way\nfor shape classification. Experiments on Platyrrhine molars demonstrate the\neffectiveness of our method and shed light on the interplay between function\nand shape in nature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 06:58:16 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Qiu", "Di", ""], ["Lui", "Lok Ming", ""]]}, {"id": "2003.01360", "submitter": "Hualie Jiang", "authors": "Hualie Jiang, Laiyan Ding, Zhenglong Sun and Rui Huang", "title": "DiPE: Deeper into Photometric Errors for Unsupervised Learning of Depth\n  and Ego-motion from Monocular Videos", "comments": "accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of depth and ego-motion from unlabelled monocular\nvideos has recently drawn great attention, which avoids the use of expensive\nground truth in the supervised one. It achieves this by using the photometric\nerrors between the target view and the synthesized views from its adjacent\nsource views as the loss. Despite significant progress, the learning still\nsuffers from occlusion and scene dynamics. This paper shows that carefully\nmanipulating photometric errors can tackle these difficulties better. The\nprimary improvement is achieved by a statistical technique that can mask out\nthe invisible or nonstationary pixels in the photometric error map and thus\nprevents misleading the networks. With this outlier masking approach, the depth\nof objects moving in the opposite direction to the camera can be estimated more\naccurately. To the best of our knowledge, such scenarios have not been\nseriously considered in the previous works, even though they pose a higher risk\nin applications like autonomous driving. We also propose an efficient weighted\nmulti-scale scheme to reduce the artifacts in the predicted depth maps.\nExtensive experiments on the KITTI dataset show the effectiveness of the\nproposed approaches. The overall system achieves state-of-theart performance on\nboth depth and ego-motion estimation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 07:05:15 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 13:21:58 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 06:31:22 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Jiang", "Hualie", ""], ["Ding", "Laiyan", ""], ["Sun", "Zhenglong", ""], ["Huang", "Rui", ""]]}, {"id": "2003.01364", "submitter": "Mohit Lamba", "authors": "Mohit Lamba, Kaushik Mitra", "title": "multi-patch aggregation models for resampling detection", "comments": "6 pages; 6 tables; 4 figures", "journal-ref": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 2967-2971", "doi": "10.1109/ICASSP40776.2020.9053005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images captured nowadays are of varying dimensions with smartphones and\nDSLR's allowing users to choose from a list of available image resolutions. It\nis therefore imperative for forensic algorithms such as resampling detection to\nscale well for images of varying dimensions. However, in our experiments, we\nobserved that many state-of-the-art forensic algorithms are sensitive to image\nsize and their performance quickly degenerates when operated on images of\ndiverse dimensions despite re-training them using multiple image sizes. To\nhandle this issue, we propose a novel pooling strategy called ITERATIVE\nPOOLING. This pooling strategy can dynamically adjust input tensors in a\ndiscrete without much loss of information as in ROI Max-pooling. This pooling\nstrategy can be used with any of the existing deep models and for demonstration\npurposes, we show its utility on Resnet-18 for the case of resampling detection\na fundamental operation for any image sought of image manipulation. Compared to\nexisting strategies and Max-pooling it gives up to 7-8% improvement on public\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 07:19:56 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Lamba", "Mohit", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2003.01367", "submitter": "Samarth Sinha", "authors": "Samarth Sinha, Animesh Garg, Hugo Larochelle", "title": "Curriculum By Smoothing", "comments": "NeurIPS 2020 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown impressive performance in\ncomputer vision tasks such as image classification, detection, and\nsegmentation. Moreover, recent work in Generative Adversarial Networks (GANs)\nhas highlighted the importance of learning by progressively increasing the\ndifficulty of a learning task [26]. When learning a network from scratch, the\ninformation propagated within the network during the earlier stages of training\ncan contain distortion artifacts due to noise which can be detrimental to\ntraining. In this paper, we propose an elegant curriculum based scheme that\nsmoothes the feature embedding of a CNN using anti-aliasing or low-pass\nfilters. We propose to augment the train-ing of CNNs by controlling the amount\nof high frequency information propagated within the CNNs as training\nprogresses, by convolving the output of a CNN feature map of each layer with a\nGaussian kernel. By decreasing the variance of the Gaussian kernel, we\ngradually increase the amount of high-frequency information available within\nthe network for inference. As the amount of information in the feature maps\nincreases during training, the network is able to progressively learn better\nrepresentations of the data. Our proposed augmented training scheme\nsignificantly improves the performance of CNNs on various vision tasks without\neither adding additional trainable parameters or an auxiliary regularization\nobjective. The generality of our method is demonstrated through empirical\nperformance gains in CNN architectures across four different tasks: transfer\nlearning, cross-task transfer learning, and generative models.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 07:27:44 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 23:07:54 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 18:24:41 GMT"}, {"version": "v4", "created": "Sun, 6 Dec 2020 23:18:04 GMT"}, {"version": "v5", "created": "Tue, 5 Jan 2021 04:53:44 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Sinha", "Samarth", ""], ["Garg", "Animesh", ""], ["Larochelle", "Hugo", ""]]}, {"id": "2003.01383", "submitter": "Hao Wu", "authors": "Hao Wu, Jan Paul Siebert and Xiangrong Xu", "title": "Fully Convolutional Networks for Automatically Generating Image Masks to\n  Train Mask R-CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel automatically generating image masks method for\nthe state-of-the-art Mask R-CNN deep learning method. The Mask R-CNN method\nachieves the best results in object detection until now, however, it is very\ntime-consuming and laborious to get the object Masks for training, the proposed\nmethod is composed by a two-stage design, to automatically generating image\nmasks, the first stage implements a fully convolutional networks (FCN) based\nsegmentation network, the second stage network, a Mask R-CNN based object\ndetection network, which is trained on the object image masks from FCN output,\nthe original input image, and additional label information. Through\nexperimentation, our proposed method can obtain the image masks automatically\nto train Mask R-CNN, and it can achieve very high classification accuracy with\nan over 90% mean of average precision (mAP) for segmentation\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 08:09:29 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 06:53:45 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Wu", "Hao", ""], ["Siebert", "Jan Paul", ""], ["Xu", "Xiangrong", ""]]}, {"id": "2003.01395", "submitter": "Priyanto Hidayatullah", "authors": "Priyanto Hidayatullah, Xueting Wang, Toshihiko Yamasaki, Tati L.E.R.\n  Mengko, Rinaldi Munir, Anggraini Barlian, Eros Sukmawati, Supraptono\n  Supraptono", "title": "DeepSperm: A robust and real-time bull sperm-cell detection in densely\n  populated semen videos", "comments": "22 pages, 8 figures, 6 tables, submitted to Computer Methods and\n  Programs in Biomedicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: Object detection is a primary research interest in\ncomputer vision. Sperm-cell detection in a densely populated bull semen\nmicroscopic observation video presents challenges such as partial occlusion,\nvast number of objects in a single video frame, tiny size of the object,\nartifacts, low contrast, and blurry objects because of the rapid movement of\nthe sperm cells. This study proposes an architecture, called DeepSperm, that\nsolves the aforementioned challenges and is more accurate and faster than\nstate-of-the-art architectures. Methods: In the proposed architecture, we use\nonly one detection layer, which is specific for small object detection. For\nhandling overfitting and increasing accuracy, we set a higher network\nresolution, use a dropout layer, and perform data augmentation on hue,\nsaturation, and exposure. Several hyper-parameters are tuned to achieve better\nperformance. We compare our proposed method with those of a conventional image\nprocessing-based object-detection method, you only look once (YOLOv3), and mask\nregion-based convolutional neural network (Mask R-CNN). Results: In our\nexperiment, we achieve 86.91 mAP on the test dataset and a processing speed of\n50.3 fps. In comparison with YOLOv3, we achieve an increase of 16.66 mAP point,\n3.26 x faster on testing, and 1.4 x faster on training with a small training\ndataset, which contains 40 video frames. The weights file size was also reduced\nsignificantly, with 16.94 x smaller than that of YOLOv3. Moreover, it requires\n1.3 x less graphical processing unit (GPU) memory than YOLOv3. Conclusions:\nThis study proposes DeepSperm, which is a simple, effective, and efficient\narchitecture with its hyper-parameters and configuration to detect bull sperm\ncells robustly in real time. In our experiment, we surpass the state of the art\nin terms of accuracy, speed, and resource needs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 09:05:05 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Hidayatullah", "Priyanto", ""], ["Wang", "Xueting", ""], ["Yamasaki", "Toshihiko", ""], ["Mengko", "Tati L. E. R.", ""], ["Munir", "Rinaldi", ""], ["Barlian", "Anggraini", ""], ["Sukmawati", "Eros", ""], ["Supraptono", "Supraptono", ""]]}, {"id": "2003.01413", "submitter": "Hao Ge", "authors": "Hao Ge, Xiaoguang Tu, Yanxiang Gong, Mei Xie, Zheng Ma", "title": "What's the relationship between CNNs and communication systems?", "comments": "Deep learning, adversarial example, interpretability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretability of Convolutional Neural Networks (CNNs) is an important\ntopic in the field of computer vision. In recent years, works in this field\ngenerally adopt a mature model to reveal the internal mechanism of CNNs,\nhelping to understand CNNs thoroughly. In this paper, we argue the working\nmechanism of CNNs can be revealed through a totally different interpretation,\nby comparing the communication systems and CNNs. This paper successfully\nobtained the corresponding relationship between the modules of the two, and\nverified the rationality of the corresponding relationship with experiments.\nFinally, through the analysis of some cutting-edge research on neural networks,\nwe find the inherent relation between these two tasks can be of help in\nexplaining these researches reasonably, as well as helping us discover the\ncorrect research direction of neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 09:50:46 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Ge", "Hao", ""], ["Tu", "Xiaoguang", ""], ["Gong", "Yanxiang", ""], ["Xie", "Mei", ""], ["Ma", "Zheng", ""]]}, {"id": "2003.01446", "submitter": "Chongwei Liu", "authors": "Chongwei Liu, Zhihui Wang, Shijie Wang, Tao Tang, Yulong Tao, Caifei\n  Yang, Haojie Li, Xing Liu, and Xin Fan", "title": "A New Dataset, Poisson GAN and AquaNet for Underwater Object Grabbing", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To boost the object grabbing capability of underwater robots for open-sea\nfarming, we propose a new dataset (UDD) consisting of three categories\n(seacucumber, seaurchin, and scallop) with 2,227 images. To the best of our\nknowledge, it is the first 4K HD dataset collected in a real open-sea farm. We\nalso propose a novel Poisson-blending Generative Adversarial Network (Poisson\nGAN) and an efficient object detection network (AquaNet) to address two common\nissues within related datasets: the class-imbalance problem and the problem of\nmass small object, respectively. Specifically, Poisson GAN combines Poisson\nblending into its generator and employs a new loss called Dual Restriction loss\n(DR loss), which supervises both implicit space features and image-level\nfeatures during training to generate more realistic images. By utilizing\nPoisson GAN, objects of minority class like seacucumber or scallop could be\nadded into an image naturally and annotated automatically, which could increase\nthe loss of minority classes during training detectors to eliminate the\nclass-imbalance problem; AquaNet is a high-efficiency detector to address the\nproblem of detecting mass small objects from cloudy underwater pictures. Within\nit, we design two efficient components: a depth-wise-convolution-based\nMulti-scale Contextual Features Fusion (MFF) block and a Multi-scale\nBlursampling (MBP) module to reduce the parameters of the network to 1.3\nmillion. Both two components could provide multi-scale features of small\nobjects under a short backbone configuration without any loss of accuracy. In\naddition, we construct a large-scale augmented dataset (AUDD) and a\npre-training dataset via Poisson GAN from UDD. Extensive experiments show the\neffectiveness of the proposed Poisson GAN, AquaNet, UDD, AUDD, and pre-training\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 10:57:52 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 01:32:42 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Liu", "Chongwei", ""], ["Wang", "Zhihui", ""], ["Wang", "Shijie", ""], ["Tang", "Tao", ""], ["Tao", "Yulong", ""], ["Yang", "Caifei", ""], ["Li", "Haojie", ""], ["Liu", "Xing", ""], ["Fan", "Xin", ""]]}, {"id": "2003.01450", "submitter": "Andrea Ranieri", "authors": "Katia Lupinetti, Andrea Ranieri, Franca Giannini, Marina Monti", "title": "3D dynamic hand gestures recognition using the Leap Motion sensor and\n  convolutional neural networks", "comments": "Conference paper, 19 pages. UPDATE 20200311: changed in ref [19]\n  'International Journal of Engineering and Technology' -> ' International\n  Journal of Engineering and Technology Innovation'. UPDATE 20200902: changed\n  every LMHGD occurrence to LMDHG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining methods for the automatic understanding of gestures is of paramount\nimportance in many application contexts and in Virtual Reality applications for\ncreating more natural and easy-to-use human-computer interaction methods. In\nthis paper, we present a method for the recognition of a set of non-static\ngestures acquired through the Leap Motion sensor. The acquired gesture\ninformation is converted in color images, where the variation of hand joint\npositions during the gesture are projected on a plane and temporal information\nis represented with color intensity of the projected points. The classification\nof the gestures is performed using a deep Convolutional Neural Network (CNN). A\nmodified version of the popular ResNet-50 architecture is adopted, obtained by\nremoving the last fully connected layer and adding a new layer with as many\nneurons as the considered gesture classes. The method has been successfully\napplied to the existing reference dataset and preliminary tests have already\nbeen performed for the real-time recognition of dynamic gestures performed by\nusers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 11:05:35 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:48:36 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 15:13:22 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Lupinetti", "Katia", ""], ["Ranieri", "Andrea", ""], ["Giannini", "Franca", ""], ["Monti", "Marina", ""]]}, {"id": "2003.01455", "submitter": "Biagio Brattoli", "authors": "Biagio Brattoli, Joseph Tighe, Fedor Zhdanov, Pietro Perona, Krzysztof\n  Chalupka", "title": "Rethinking Zero-shot Video Classification: End-to-end Training for\n  Realistic Applications", "comments": "Accepted for publication at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trained on large datasets, deep learning (DL) can accurately classify videos\ninto hundreds of diverse classes. However, video data is expensive to annotate.\nZero-shot learning (ZSL) proposes one solution to this problem. ZSL trains a\nmodel once, and generalizes to new tasks whose classes are not present in the\ntraining dataset. We propose the first end-to-end algorithm for ZSL in video\nclassification. Our training procedure builds on insights from recent video\nclassification literature and uses a trainable 3D CNN to learn the visual\nfeatures. This is in contrast to previous video ZSL methods, which use\npretrained feature extractors. We also extend the current benchmarking\nparadigm: Previous techniques aim to make the test task unknown at training\ntime but fall short of this goal. We encourage domain shift across training and\ntest data and disallow tailoring a ZSL model to a specific test dataset. We\noutperform the state-of-the-art by a wide margin. Our code, evaluation\nprocedure and model weights are available at\ngithub.com/bbrattoli/ZeroShotVideoClassification.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 11:09:59 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 08:11:41 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 09:06:07 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2020 08:22:45 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Brattoli", "Biagio", ""], ["Tighe", "Joseph", ""], ["Zhdanov", "Fedor", ""], ["Perona", "Pietro", ""], ["Chalupka", "Krzysztof", ""]]}, {"id": "2003.01456", "submitter": "Julian Chibane", "authors": "Julian Chibane, Thiemo Alldieck, Gerard Pons-Moll", "title": "Implicit Functions in Feature Space for 3D Shape Reconstruction and\n  Completion", "comments": "{IEEE} Conference on Computer Vision and Pattern Recognition\n  (CVPR)2020", "journal-ref": "{IEEE} Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many works focus on 3D reconstruction from images, in this paper, we\nfocus on 3D shape reconstruction and completion from a variety of 3D inputs,\nwhich are deficient in some respect: low and high resolution voxels, sparse and\ndense point clouds, complete or incomplete. Processing of such 3D inputs is an\nincreasingly important problem as they are the output of 3D scanners, which are\nbecoming more accessible, and are the intermediate output of 3D computer vision\nalgorithms. Recently, learned implicit functions have shown great promise as\nthey produce continuous reconstructions. However, we identified two limitations\nin reconstruction from 3D inputs: 1) details present in the input data are not\nretained, and 2) poor reconstruction of articulated humans. To solve this, we\npropose Implicit Feature Networks (IF-Nets), which deliver continuous outputs,\ncan handle multiple topologies, and complete shapes for missing or sparse input\ndata retaining the nice properties of recent learned implicit functions, but\ncritically they can also retain detail when it is present in the input data,\nand can reconstruct articulated humans. Our work differs from prior work in two\ncrucial aspects. First, instead of using a single vector to encode a 3D shape,\nwe extract a learnable 3-dimensional multi-scale tensor of deep features, which\nis aligned with the original Euclidean space embedding the shape. Second,\ninstead of classifying x-y-z point coordinates directly, we classify deep\nfeatures extracted from the tensor at a continuous query point. We show that\nthis forces our model to make decisions based on global and local shape\nstructure, as opposed to point coordinates, which are arbitrary under Euclidean\ntransformations. Experiments demonstrate that IF-Nets clearly outperform prior\nwork in 3D object reconstruction in ShapeNet, and obtain significantly more\naccurate 3D human reconstructions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 11:14:29 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 14:47:27 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Chibane", "Julian", ""], ["Alldieck", "Thiemo", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2003.01460", "submitter": "Vincent Le Guen", "authors": "Vincent Le Guen, Nicolas Thome", "title": "Disentangling Physical Dynamics from Unknown Factors for Unsupervised\n  Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": "CVPR 2020", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging physical knowledge described by partial differential equations\n(PDEs) is an appealing way to improve unsupervised video prediction methods.\nSince physics is too restrictive for describing the full visual content of\ngeneric videos, we introduce PhyDNet, a two-branch deep architecture, which\nexplicitly disentangles PDE dynamics from unknown complementary information. A\nsecond contribution is to propose a new recurrent physical cell (PhyCell),\ninspired from data assimilation techniques, for performing PDE-constrained\nprediction in latent space. Extensive experiments conducted on four various\ndatasets show the ability of PhyDNet to outperform state-of-the-art methods.\nAblation studies also highlight the important gain brought out by both\ndisentanglement and PDE-constrained prediction. Finally, we show that PhyDNet\npresents interesting features for dealing with missing data and long-term\nforecasting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 11:26:40 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:51:34 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Guen", "Vincent Le", ""], ["Thome", "Nicolas", ""]]}, {"id": "2003.01473", "submitter": "Qiaolin Xia", "authors": "Qiaolin Xia, Haoyang Huang, Nan Duan, Dongdong Zhang, Lei Ji, Zhifang\n  Sui, Edward Cui, Taroon Bharti, Xin Liu, Ming Zhou", "title": "XGPT: Cross-modal Generative Pre-Training for Image Captioning", "comments": "12 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many BERT-based cross-modal pre-trained models produce excellent\nresults on downstream understanding tasks like image-text retrieval and VQA,\nthey cannot be applied to generation tasks directly. In this paper, we propose\nXGPT, a new method of Cross-modal Generative Pre-Training for Image Captioning\nthat is designed to pre-train text-to-image caption generators through three\nnovel generation tasks, including Image-conditioned Masked Language Modeling\n(IMLM), Image-conditioned Denoising Autoencoding (IDA), and Text-conditioned\nImage Feature Generation (TIFG). As a result, the pre-trained XGPT can be\nfine-tuned without any task-specific architecture modifications to create\nstate-of-the-art models for image captioning. Experiments show that XGPT\nobtains new state-of-the-art results on the benchmark datasets, including COCO\nCaptions and Flickr30k Captions. We also use XGPT to generate new image\ncaptions as data augmentation for the image retrieval task and achieve\nsignificant improvement on all recall metrics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 12:13:06 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 07:56:09 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Xia", "Qiaolin", ""], ["Huang", "Haoyang", ""], ["Duan", "Nan", ""], ["Zhang", "Dongdong", ""], ["Ji", "Lei", ""], ["Sui", "Zhifang", ""], ["Cui", "Edward", ""], ["Bharti", "Taroon", ""], ["Liu", "Xin", ""], ["Zhou", "Ming", ""]]}, {"id": "2003.01474", "submitter": "Adria Ruiz", "authors": "Adria Ruiz and Jakob Verbeek", "title": "Anytime Inference with Distilled Hierarchical Neural Ensembles", "comments": null, "journal-ref": "AAAI Conference on Artificial Intelligence 2021 (AAAI2021)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in deep neural networks can be computationally expensive, and\nnetworks capable of anytime inference are important in mscenarios where the\namount of compute or quantity of input data varies over time. In such networks\nthe inference process can interrupted to provide a result faster, or continued\nto obtain a more accurate result. We propose Hierarchical Neural Ensembles\n(HNE), a novel framework to embed an ensemble of multiple networks in a\nhierarchical tree structure, sharing intermediate layers. In HNE we control the\ncomplexity of inference on-the-fly by evaluating more or less models in the\nensemble. Our second contribution is a novel hierarchical distillation method\nto boost the prediction accuracy of small ensembles. This approach leverages\nthe nested structure of our ensembles, to optimally allocate accuracy and\ndiversity across the individual models. Our experiments show that, compared to\nprevious anytime inference models, HNE provides state-of-the-art\naccuracy-computate trade-offs on the CIFAR-10/100 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 12:13:38 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 08:17:29 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 07:26:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Ruiz", "Adria", ""], ["Verbeek", "Jakob", ""]]}, {"id": "2003.01517", "submitter": "Aneta Neumann", "authors": "Aneta Neumann, Bradley Alexander, Frank Neumann", "title": "Evolutionary Image Transition and Painting Using Random Walks", "comments": "Accepted for the Evolutionary Computation Journal (MIT Press). arXiv\n  admin note: text overlap with arXiv:1604.06187", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study demonstrating how random walk algorithms can be used for\nevolutionary image transition. We design different mutation operators based on\nuniform and biased random walks and study how their combination with a baseline\nmutation operator can lead to interesting image transition processes in terms\nof visual effects and artistic features. Using feature-based analysis we\ninvestigate the evolutionary image transition behaviour with respect to\ndifferent features and evaluate the images constructed during the image\ntransition process. Afterwards, we investigate how modifications of our biased\nrandom walk approaches can be used for evolutionary image painting. We\nintroduce an evolutionary image painting approach whose underlying biased\nrandom walk can be controlled by a parameter influencing the bias of the random\nwalk and thereby creating different artistic painting effects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 10:28:24 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Neumann", "Aneta", ""], ["Alexander", "Bradley", ""], ["Neumann", "Frank", ""]]}, {"id": "2003.01565", "submitter": "Sen Wang", "authors": "Sen Wang, Jiaqi Chen, Xuanliang Deng, Seth Hutchinson, and Frank\n  Dellaert", "title": "Robot Calligraphy using Pseudospectral Optimal Control in Conjunction\n  with a Novel Dynamic Brush Model", "comments": "Update to arXiv:1911.08002 mistakenly submitted as new article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chinese calligraphy is a unique art form with great artistic value but\ndifficult to master. In this paper, we formulate the calligraphy writing\nproblem as a trajectory optimization problem, and propose an improved virtual\nbrush model for simulating the real writing process. Our approach is inspired\nby pseudospectral optimal control in that we parameterize the actuator\ntrajectory for each stroke as a Chebyshev polynomial. The proposed dynamic\nvirtual brush model plays a key role in formulating the objective function to\nbe optimized. Our approach shows excellent performance in drawing aesthetically\npleasing characters, and does so much more efficiently than previous work,\nopening up the possibility to achieve real-time closed-loop control.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 18:26:04 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 02:42:18 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 18:01:02 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Wang", "Sen", ""], ["Chen", "Jiaqi", ""], ["Deng", "Xuanliang", ""], ["Hutchinson", "Seth", ""], ["Dellaert", "Frank", ""]]}, {"id": "2003.01581", "submitter": "Wei Hao Khoong", "authors": "Wei Hao Khoong", "title": "BUSU-Net: An Ensemble U-Net Framework for Medical Image Segmentation", "comments": "GitHub link to the model scripts and trained model weights can be\n  found in the manuscript. Version 2: Added S-UNet's Mi-UNet results for\n  comparison and reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, convolutional neural networks (CNNs) have revolutionized\nmedical image analysis. One of the most well-known CNN architectures in\nsemantic segmentation is the U-net, which has achieved much success in several\nmedical image segmentation applications. Also more recently, with the rise of\nautoML ad advancements in neural architecture search (NAS), methods like\nNAS-Unet have been proposed for NAS in medical image segmentation. In this\npaper, with inspiration from LadderNet, U-Net, autoML and NAS, we propose an\nensemble deep neural network with an underlying U-Net framework consisting of\nbi-directional convolutional LSTMs and dense connections, where the first (from\nleft) U-Net-like network is deeper than the second (from left). We show that\nthis ensemble network outperforms recent state-of-the-art networks in several\nevaluation metrics, and also evaluate a lightweight version of this ensemble\nnetwork, which also outperforms recent state-of-the-art networks in some\nevaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 15:18:01 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 11:51:51 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Khoong", "Wei Hao", ""]]}, {"id": "2003.01587", "submitter": "Dmytro Mishkin", "authors": "Yuhe Jin and Dmytro Mishkin and Anastasiia Mishchuk and Jiri Matas and\n  Pascal Fua and Kwang Moo Yi and Eduard Trulls", "title": "Image Matching across Wide Baselines: From Paper to Practice", "comments": "Added: KeyNet-SOSNet, AffNet-HardNet, TFeat, MKD from kornia", "journal-ref": null, "doi": "10.1007/s11263-020-01385-0", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a comprehensive benchmark for local features and robust\nestimation algorithms, focusing on the downstream task -- the accuracy of the\nreconstructed camera pose -- as our primary metric. Our pipeline's modular\nstructure allows easy integration, configuration, and combination of different\nmethods and heuristics. This is demonstrated by embedding dozens of popular\nalgorithms and evaluating them, from seminal works to the cutting edge of\nmachine learning research. We show that with proper settings, classical\nsolutions may still outperform the perceived state of the art.\n  Besides establishing the actual state of the art, the conducted experiments\nreveal unexpected properties of Structure from Motion (SfM) pipelines that can\nhelp improve their performance, for both algorithmic and learned methods. Data\nand code are online https://github.com/vcg-uvic/image-matching-benchmark,\nproviding an easy-to-use and flexible framework for the benchmarking of local\nfeatures and robust estimation methods, both alongside and against\ntop-performing methods. This work provides a basis for the Image Matching\nChallenge https://vision.uvic.ca/image-matching-challenge.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 15:20:57 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 22:44:58 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 08:38:03 GMT"}, {"version": "v4", "created": "Sat, 6 Feb 2021 15:58:05 GMT"}, {"version": "v5", "created": "Thu, 11 Feb 2021 13:50:17 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Jin", "Yuhe", ""], ["Mishkin", "Dmytro", ""], ["Mishchuk", "Anastasiia", ""], ["Matas", "Jiri", ""], ["Fua", "Pascal", ""], ["Yi", "Kwang Moo", ""], ["Trulls", "Eduard", ""]]}, {"id": "2003.01607", "submitter": "Austin Reiter", "authors": "Austin Reiter, Menglin Jia, Pu Yang, Ser-Nam Lim", "title": "Deep Multi-Modal Sets", "comments": "10 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many vision-related tasks benefit from reasoning over multiple modalities to\nleverage complementary views of data in an attempt to learn robust embedding\nspaces. Most deep learning-based methods rely on a late fusion technique\nwhereby multiple feature types are encoded and concatenated and then a multi\nlayer perceptron (MLP) combines the fused embedding to make predictions. This\nhas several limitations, such as an unnatural enforcement that all features be\npresent at all times as well as constraining only a constant number of\noccurrences of a feature modality at any given time. Furthermore, as more\nmodalities are added, the concatenated embedding grows. To mitigate this, we\npropose Deep Multi-Modal Sets: a technique that represents a collection of\nfeatures as an unordered set rather than one long ever-growing fixed-size\nvector. The set is constructed so that we have invariance both to permutations\nof the feature modalities as well as to the cardinality of the set. We will\nalso show that with particular choices in our model architecture, we can yield\ninterpretable feature performance such that during inference time we can\nobserve which modalities are most contributing to the prediction.With this in\nmind, we demonstrate a scalable, multi-modal framework that reasons over\ndifferent modalities to learn various types of tasks. We demonstrate new\nstate-of-the-art performance on two multi-modal datasets (Ads-Parallelity [34]\nand MM-IMDb [1]).\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 15:48:44 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Reiter", "Austin", ""], ["Jia", "Menglin", ""], ["Yang", "Pu", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2003.01639", "submitter": "Tianyu Ma", "authors": "Tianyu Ma, Ajay Gupta, Mert R. Sabuncu", "title": "Volumetric landmark detection with a multi-scale shift equivariant\n  neural network", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks yield promising results in a wide range of computer\nvision applications, including landmark detection. A major challenge for\naccurate anatomical landmark detection in volumetric images such as clinical CT\nscans is that large-scale data often constrain the capacity of the employed\nneural network architecture due to GPU memory limitations, which in turn can\nlimit the precision of the output. We propose a multi-scale, end-to-end deep\nlearning method that achieves fast and memory-efficient landmark detection in\n3D images. Our architecture consists of blocks of shift-equivariant networks,\neach of which performs landmark detection at a different spatial scale. These\nblocks are connected from coarse to fine-scale, with differentiable resampling\nlayers, so that all levels can be trained together. We also present a noise\ninjection strategy that increases the robustness of the model and allows us to\nquantify uncertainty at test time. We evaluate our method for carotid artery\nbifurcations detection on 263 CT volumes and achieve a better than\nstate-of-the-art accuracy with mean Euclidean distance error of 2.81mm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 17:06:19 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 18:40:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ma", "Tianyu", ""], ["Gupta", "Ajay", ""], ["Sabuncu", "Mert R.", ""]]}, {"id": "2003.01661", "submitter": "Nenglun Chen", "authors": "Nenglun Chen, Lingjie Liu, Zhiming Cui, Runnan Chen, Duygu Ceylan,\n  Changhe Tu, Wenping Wang", "title": "Unsupervised Learning of Intrinsic Structural Representation Points", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning structures of 3D shapes is a fundamental problem in the field of\ncomputer graphics and geometry processing. We present a simple yet\ninterpretable unsupervised method for learning a new structural representation\nin the form of 3D structure points. The 3D structure points produced by our\nmethod encode the shape structure intrinsically and exhibit semantic\nconsistency across all the shape instances with similar structures. This is a\nchallenging goal that has not fully been achieved by other methods.\nSpecifically, our method takes a 3D point cloud as input and encodes it as a\nset of local features. The local features are then passed through a novel point\nintegration module to produce a set of 3D structure points. The chamfer\ndistance is used as reconstruction loss to ensure the structure points lie\nclose to the input point cloud. Extensive experiments have shown that our\nmethod outperforms the state-of-the-art on the semantic shape correspondence\ntask and achieves comparable performance with the state-of-the-art on the\nsegmentation label transfer task. Moreover, the PCA based shape embedding built\nupon consistent structure points demonstrates good performance in preserving\nthe shape structures. Code is available at\nhttps://github.com/NolenChen/3DStructurePoints\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 17:40:00 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 11:54:35 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chen", "Nenglun", ""], ["Liu", "Lingjie", ""], ["Cui", "Zhiming", ""], ["Chen", "Runnan", ""], ["Ceylan", "Duygu", ""], ["Tu", "Changhe", ""], ["Wang", "Wenping", ""]]}, {"id": "2003.01663", "submitter": "Nan Xue", "authors": "Nan Xue and Tianfu Wu and Song Bai and Fu-Dong Wang and Gui-Song Xia\n  and Liangpei Zhang and Philip H.S. Torr", "title": "Holistically-Attracted Wireframe Parsing", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast and parsimonious parsing method to accurately and\nrobustly detect a vectorized wireframe in an input image with a single forward\npass. The proposed method is end-to-end trainable, consisting of three\ncomponents: (i) line segment and junction proposal generation, (ii) line\nsegment and junction matching, and (iii) line segment and junction\nverification. For computing line segment proposals, a novel exact dual\nrepresentation is proposed which exploits a parsimonious geometric\nreparameterization for line segments and forms a holistic 4-dimensional\nattraction field map for an input image. Junctions can be treated as the\n\"basins\" in the attraction field. The proposed method is thus called\nHolistically-Attracted Wireframe Parser (HAWP). In experiments, the proposed\nmethod is tested on two benchmarks, the Wireframe dataset, and the YorkUrban\ndataset. On both benchmarks, it obtains state-of-the-art performance in terms\nof accuracy and efficiency. For example, on the Wireframe dataset, compared to\nthe previous state-of-the-art method L-CNN, it improves the challenging mean\nstructural average precision (msAP) by a large margin ($2.8\\%$ absolute\nimprovements) and achieves 29.5 FPS on single GPU ($89\\%$ relative\nimprovement). A systematic ablation study is performed to further justify the\nproposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 17:43:57 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Xue", "Nan", ""], ["Wu", "Tianfu", ""], ["Bai", "Song", ""], ["Wang", "Fu-Dong", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Liangpei", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "2003.01665", "submitter": "Jaewoo Park", "authors": "Jaewoo Park, Yoon Gyo Jung, Andrew Beng Jin Teoh", "title": "Discriminative Multi-level Reconstruction under Compact Latent Space for\n  One-Class Novelty Detection", "comments": "Accepted to ICPR 2020 Oral (acceptance rate 4.4%)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-class novelty detection, a model learns solely on the in-class data to\nsingle out out-class instances. Autoencoder (AE) variants aim to compactly\nmodel the in-class data to reconstruct it exclusively, thus differentiating the\nin-class from out-class by the reconstruction error. However, compact modeling\nin an improper way might collapse the latent representations of the in-class\ndata and thus their reconstruction, which would lead to performance\ndeterioration. Moreover, to properly measure the reconstruction error of\nhigh-dimensional data, a metric is required that captures high-level semantics\nof the data. To this end, we propose Discriminative Compact AE (DCAE) that\nlearns both compact and collapse-free latent representations of the in-class\ndata, thereby reconstructing them both finely and exclusively. In DCAE, (a) we\nforce a compact latent space to bijectively represent the in-class data by\nreconstructing them through internal discriminative layers of generative\nadversarial nets. (b) Based on the deep encoder's vulnerability to open set\nrisk, out-class instances are encoded into the same compact latent space and\nreconstructed poorly without sacrificing the quality of in-class data\nreconstruction. (c) In inference, the reconstruction error is measured by a\nnovel metric that computes the dissimilarity between a query and its\nreconstruction based on the class semantics captured by the internal\ndiscriminator. Extensive experiments on public image datasets validate the\neffectiveness of our proposed model on both novelty and adversarial example\ndetection, delivering state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 17:45:54 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 13:27:12 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:00:42 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Park", "Jaewoo", ""], ["Jung", "Yoon Gyo", ""], ["Teoh", "Andrew Beng Jin", ""]]}, {"id": "2003.01690", "submitter": "Francesco Croce", "authors": "Francesco Croce, Matthias Hein", "title": "Reliable evaluation of adversarial robustness with an ensemble of\n  diverse parameter-free attacks", "comments": "In ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of defense strategies against adversarial attacks has significantly\ngrown over the last years, but progress is hampered as the evaluation of\nadversarial defenses is often insufficient and thus gives a wrong impression of\nrobustness. Many promising defenses could be broken later on, making it\ndifficult to identify the state-of-the-art. Frequent pitfalls in the evaluation\nare improper tuning of hyperparameters of the attacks, gradient obfuscation or\nmasking. In this paper we first propose two extensions of the PGD-attack\novercoming failures due to suboptimal step size and problems of the objective\nfunction. We then combine our novel attacks with two complementary existing\nones to form a parameter-free, computationally affordable and user-independent\nensemble of attacks to test adversarial robustness. We apply our ensemble to\nover 50 models from papers published at recent top machine learning and\ncomputer vision venues. In all except one of the cases we achieve lower robust\ntest accuracy than reported in these papers, often by more than $10\\%$,\nidentifying several broken defenses.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 18:15:55 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 18:31:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Croce", "Francesco", ""], ["Hein", "Matthias", ""]]}, {"id": "2003.01711", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Brais Martinez and Georgios Tzimiropoulos", "title": "BATS: Binary ArchitecTure Search", "comments": "accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Binary ArchitecTure Search (BATS), a framework that\ndrastically reduces the accuracy gap between binary neural networks and their\nreal-valued counterparts by means of Neural Architecture Search (NAS). We show\nthat directly applying NAS to the binary domain provides very poor results. To\nalleviate this, we describe, to our knowledge, for the first time, the 3 key\ningredients for successfully applying NAS to the binary domain. Specifically,\nwe (1) introduce and design a novel binary-oriented search space, (2) propose a\nnew mechanism for controlling and stabilising the resulting searched\ntopologies, (3) propose and validate a series of new search strategies for\nbinary networks that lead to faster convergence and lower search times.\nExperimental results demonstrate the effectiveness of the proposed approach and\nthe necessity of searching in the binary space directly. Moreover, (4) we set a\nnew state-of-the-art for binary neural networks on CIFAR10, CIFAR100 and\nImageNet datasets. Code will be made available\nhttps://github.com/1adrianb/binary-nas\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 18:57:02 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 21:57:18 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Bulat", "Adrian", ""], ["Martinez", "Brais", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2003.01719", "submitter": "David M\\\"unch", "authors": "Jens Bayer and David M\\\"unch and Michael Arens", "title": "Image-based OoD-Detector Principles on Graph-based Input Data in Human\n  Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Living in a complex world like ours makes it unacceptable that a practical\nimplementation of a machine learning system assumes a closed world. Therefore,\nit is necessary for such a learning-based system in a real world environment,\nto be aware of its own capabilities and limits and to be able to distinguish\nbetween confident and unconfident results of the inference, especially if the\nsample cannot be explained by the underlying distribution. This knowledge is\nparticularly essential in safety-critical environments and tasks e.g.\nself-driving cars or medical applications. Towards this end, we transfer\nimage-based Out-of-Distribution (OoD)-methods to graph-based data and show the\napplicability in action recognition. The contribution of this work is (i) the\nexamination of the portability of recent image-based OoD-detectors for\ngraph-based input data, (ii) a Metric Learning-based approach to detect\nOoD-samples, and (iii) the introduction of a novel semi-synthetic action\nrecognition dataset. The evaluation shows that image-based OoD-methods can be\napplied to graph-based data. Additionally, there is a gap between the\nperformance on intraclass and intradataset results. First methods as the\nexamined baseline or ODIN provide reasonable results. More sophisticated\nnetwork architectures - in contrast to their image-based application - were\nsurpassed in the intradataset comparison and even lead to less classification\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 15:38:43 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Bayer", "Jens", ""], ["M\u00fcnch", "David", ""], ["Arens", "Michael", ""]]}, {"id": "2003.01764", "submitter": "Noam Elron", "authors": "Noam Elron, Shahar S. Yuval, Dmitry Rudoy and Noam Levy", "title": "Blind Image Restoration without Prior Knowledge", "comments": "Submitted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many image restoration techniques are highly dependent on the degradation\nused during training, and their performance declines significantly when applied\nto slightly different input. Blind and universal techniques attempt to mitigate\nthis by producing a trained model that can adapt to varying conditions.\nHowever, blind techniques to date require prior knowledge of the degradation\nprocess, and assumptions regarding its parameter-space. In this paper we\npresent the Self-Normalization Side-Chain (SCNC), a novel approach to blind\nuniversal restoration in which no prior knowledge of the degradation is needed.\nThis module can be added to any existing CNN topology, and is trained along\nwith the rest of the network in an end-to-end manner. The imaging parameters\nrelevant to the task, as well as their dynamics, are deduced from the variety\nin the training data. We apply our solution to several image restoration tasks,\nand demonstrate that the SNSC encodes the degradation-parameters, improving\nrestoration performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 19:57:33 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 18:36:09 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Elron", "Noam", ""], ["Yuval", "Shahar S.", ""], ["Rudoy", "Dmitry", ""], ["Levy", "Noam", ""]]}, {"id": "2003.01768", "submitter": "Xinzheng Zhang Prof.", "authors": "Xinzheng Zhang, Hang Su, Ce Zhang, Peter M. Atkinson, Xiaoheng Tan,\n  Xiaoping Zeng and Xin Jian", "title": "A Robust Imbalanced SAR Image Change Detection Approach Based on Deep\n  Difference Image and PCANet", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, a novel robust change detection approach is presented for\nimbalanced multi-temporal synthetic aperture radar (SAR) image based on deep\nlearning. Our main contribution is to develop a novel method for generating\ndifference image and a parallel fuzzy c-means (FCM) clustering method. The main\nsteps of our proposed approach are as follows: 1) Inspired by convolution and\npooling in deep learning, a deep difference image (DDI) is obtained based on\nparameterized pooling leading to better speckle suppression and feature\nenhancement than traditional difference images. 2) Two different parameter\nSigmoid nonlinear mapping are applied to the DDI to get two mapped DDIs.\nParallel FCM are utilized on these two mapped DDIs to obtain three types of\npseudo-label pixels, namely, changed pixels, unchanged pixels, and intermediate\npixels. 3) A PCANet with support vector machine (SVM) are trained to classify\nintermediate pixels to be changed or unchanged. Three imbalanced multi-temporal\nSAR image sets are used for change detection experiments. The experimental\nresults demonstrate that the proposed approach is effective and robust for\nimbalanced SAR data, and achieve up to 99.52% change detection accuracy\nsuperior to most state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 20:05:49 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zhang", "Xinzheng", ""], ["Su", "Hang", ""], ["Zhang", "Ce", ""], ["Atkinson", "Peter M.", ""], ["Tan", "Xiaoheng", ""], ["Zeng", "Xiaoping", ""], ["Jian", "Xin", ""]]}, {"id": "2003.01782", "submitter": "Takami Sato", "authors": "Takami Sato, Junjie Shen, Ningfei Wang, Yunhan Jack Jia, Xue Lin and\n  Qi Alfred Chen", "title": "Security of Deep Learning based Lane Keeping System under Physical-World\n  Adversarial Attack", "comments": "Project page: https://sites.google.com/view/lane-keeping-adv-attack/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane-Keeping Assistance System (LKAS) is convenient and widely available\ntoday, but also extremely security and safety critical. In this work, we design\nand implement the first systematic approach to attack real-world DNN-based\nLKASes. We identify dirty road patches as a novel and domain-specific threat\nmodel for practicality and stealthiness. We formulate the attack as an\noptimization problem, and address the challenge from the inter-dependencies\namong attacks on consecutive camera frames. We evaluate our approach on a\nstate-of-the-art LKAS and our preliminary results show that our attack can\nsuccessfully cause it to drive off lane boundaries within as short as 1.3\nseconds.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 20:35:25 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Sato", "Takami", ""], ["Shen", "Junjie", ""], ["Wang", "Ningfei", ""], ["Jia", "Yunhan Jack", ""], ["Lin", "Xue", ""], ["Chen", "Qi Alfred", ""]]}, {"id": "2003.01791", "submitter": "James Ren Hou Lee Mr", "authors": "James Ren Hou Lee and Alexander Wong", "title": "TimeConvNets: A Deep Time Windowed Convolution Neural Network Design for\n  Real-time Video Facial Expression Recognition", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core challenge faced by the majority of individuals with Autism Spectrum\nDisorder (ASD) is an impaired ability to infer other people's emotions based on\ntheir facial expressions. With significant recent advances in machine learning,\none potential approach to leveraging technology to assist such individuals to\nbetter recognize facial expressions and reduce the risk of possible loneliness\nand depression due to social isolation is the design of computer vision-driven\nfacial expression recognition systems. Motivated by this social need as well as\nthe low latency requirement of such systems, this study explores a novel deep\ntime windowed convolutional neural network design (TimeConvNets) for the\npurpose of real-time video facial expression recognition. More specifically, we\nexplore an efficient convolutional deep neural network design for\nspatiotemporal encoding of time windowed video frame sub-sequences and study\nthe respective balance between speed and accuracy. Furthermore, to evaluate the\nproposed TimeConvNet design, we introduce a more difficult dataset called\nBigFaceX, composed of a modified aggregation of the extended Cohn-Kanade (CK+),\nBAUM-1, and the eNTERFACE public datasets. Different variants of the proposed\nTimeConvNet design with different backbone network architectures were evaluated\nusing BigFaceX alongside other network designs for capturing spatiotemporal\ninformation, and experimental results demonstrate that TimeConvNets can better\ncapture the transient nuances of facial expressions and boost classification\naccuracy while maintaining a low inference time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 20:58:52 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Lee", "James Ren Hou", ""], ["Wong", "Alexander", ""]]}, {"id": "2003.01811", "submitter": "Bing Han", "authors": "Bing Han, Gopalakrishnan Srinivasan, and Kaushik Roy", "title": "RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper\n  High-Accuracy and Low-Latency Spiking Neural Network", "comments": "to be published in CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) have recently attracted significant research\ninterest as the third generation of artificial neural networks that can enable\nlow-power event-driven data analytics. The best performing SNNs for image\nrecognition tasks are obtained by converting a trained Analog Neural Network\n(ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of\nintegrate-and-fire neurons with \"proper\" firing thresholds. The converted SNNs\ntypically incur loss in accuracy compared to that provided by the original ANN\nand require sizable number of inference time-steps to achieve the best\naccuracy. We find that performance degradation in the converted SNN stems from\nusing \"hard reset\" spiking neuron that is driven to fixed reset potential once\nits membrane potential exceeds the firing threshold, leading to information\nloss during SNN inference. We propose ANN-SNN conversion using \"soft reset\"\nspiking neuron model, referred to as Residual Membrane Potential (RMP) spiking\nneuron, which retains the \"residual\" membrane potential above threshold at the\nfiring instants. We demonstrate near loss-less ANN-SNN conversion using RMP\nneurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets\nincluding CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet\n(73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best\ninference accuracy provided by the converted SNN with \"hard reset\" spiking\nneurons using 2-8 times fewer inference time-steps across network architectures\nand datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:19:12 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:27:05 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Han", "Bing", ""], ["Srinivasan", "Gopalakrishnan", ""], ["Roy", "Kaushik", ""]]}, {"id": "2003.01816", "submitter": "Yizhou Wang", "authors": "Yizhou Wang, Zhongyu Jiang, Xiangyu Gao, Jenq-Neng Hwang, Guanbin\n  Xing, Hui Liu", "title": "RODNet: Radar Object Detection Using Cross-Modal Supervision", "comments": "Accepted by WACV 2021, 10 pages, 9 figures, 3 tables. Proceedings of\n  the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar is usually more robust than the camera in severe driving scenarios,\ne.g., weak/strong lighting and bad weather. However, unlike RGB images captured\nby a camera, the semantic information from the radar signals is noticeably\ndifficult to extract. In this paper, we propose a deep radar object detection\nnetwork (RODNet), to effectively detect objects purely from the carefully\nprocessed radar frequency data in the format of range-azimuth frequency\nheatmaps (RAMaps). Three different 3D autoencoder based architectures are\nintroduced to predict object confidence distribution from each snippet of the\ninput RAMaps. The final detection results are then calculated using our\npost-processing method, called location-based non-maximum suppression (L-NMS).\nInstead of using burdensome human-labeled ground truth, we train the RODNet\nusing the annotations generated automatically by a novel 3D localization method\nusing a camera-radar fusion (CRF) strategy. To train and evaluate our method,\nwe build a new dataset -- CRUW, containing synchronized videos and RAMaps in\nvarious driving scenarios. After intensive experiments, our RODNet shows\nfavorable object detection performance without the presence of the camera.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 22:33:16 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 07:00:42 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Wang", "Yizhou", ""], ["Jiang", "Zhongyu", ""], ["Gao", "Xiangyu", ""], ["Hwang", "Jenq-Neng", ""], ["Xing", "Guanbin", ""], ["Liu", "Hui", ""]]}, {"id": "2003.01822", "submitter": "Qianggong Zhang", "authors": "Qianggong Zhang, Yanyang Gu, Michalkiewicz Mateusz, Mahsa\n  Baktashmotlagh, Anders Eriksson", "title": "Implicitly Defined Layers in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conventional formulations of multilayer feedforward neural networks, the\nindividual layers are customarily defined by explicit functions. In this paper\nwe demonstrate that defining individual layers in a neural network\n\\emph{implicitly} provide much richer representations over the standard\nexplicit one, consequently enabling a vastly broader class of end-to-end\ntrainable architectures. We present a general framework of implicitly defined\nlayers, where much of the theoretical analysis of such layers can be addressed\nthrough the implicit function theorem. We also show how implicitly defined\nlayers can be seamlessly incorporated into existing machine learning libraries.\nIn particular with respect to current automatic differentiation techniques for\nuse in backpropagation based training. Finally, we demonstrate the versatility\nand relevance of our proposed approach on a number of diverse example problems\nwith promising results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 22:44:42 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 01:36:06 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Zhang", "Qianggong", ""], ["Gu", "Yanyang", ""], ["Mateusz", "Michalkiewicz", ""], ["Baktashmotlagh", "Mahsa", ""], ["Eriksson", "Anders", ""]]}, {"id": "2003.01826", "submitter": "Janis Keuper", "authors": "Ricard Durall and Margret Keuper and Janis Keuper", "title": "Watch your Up-Convolution: CNN Based Generative Deep Neural Networks are\n  Failing to Reproduce Spectral Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative convolutional deep neural networks, e.g. popular GAN\narchitectures, are relying on convolution based up-sampling methods to produce\nnon-scalar outputs like images or video sequences. In this paper, we show that\ncommon up-sampling methods, i.e. known as up-convolution or transposed\nconvolution, are causing the inability of such models to reproduce spectral\ndistributions of natural training data correctly. This effect is independent of\nthe underlying architecture and we show that it can be used to easily detect\ngenerated data like deepfakes with up to 100% accuracy on public benchmarks.\n  To overcome this drawback of current generative models, we propose to add a\nnovel spectral regularization term to the training optimization objective. We\nshow that this approach not only allows to train spectral consistent GANs that\nare avoiding high frequency errors. Also, we show that a correct approximation\nof the frequency spectrum has positive effects on the training stability and\noutput quality of generative networks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 23:04:33 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Durall", "Ricard", ""], ["Keuper", "Margret", ""], ["Keuper", "Janis", ""]]}, {"id": "2003.01835", "submitter": "Ashwin Balakrishna", "authors": "Priya Sundaresan, Jennifer Grannen, Brijen Thananjeyan, Ashwin\n  Balakrishna, Michael Laskey, Kevin Stone, Joseph E. Gonzalez, Ken Goldberg", "title": "Learning Rope Manipulation Policies Using Dense Object Descriptors\n  Trained on Synthetic Depth Data", "comments": null, "journal-ref": "2020 International Conference on Robotics and Automation", "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation of deformable 1D objects such as ropes, cables, and\nhoses is challenging due to the lack of high-fidelity analytic models and large\nconfiguration spaces. Furthermore, learning end-to-end manipulation policies\ndirectly from images and physical interaction requires significant time on a\nrobot and can fail to generalize across tasks. We address these challenges\nusing interpretable deep visual representations for rope, extending recent work\non dense object descriptors for robot manipulation. This facilitates the design\nof interpretable and transferable geometric policies built on top of the\nlearned representations, decoupling visual reasoning and control. We present an\napproach that learns point-pair correspondences between initial and goal rope\nconfigurations, which implicitly encodes geometric structure, entirely in\nsimulation from synthetic depth images. We demonstrate that the learned\nrepresentation -- dense depth object descriptors (DDODs) -- can be used to\nmanipulate a real rope into a variety of different arrangements either by\nlearning from demonstrations or using interpretable geometric policies. In 50\ntrials of a knot-tying task with the ABB YuMi Robot, the system achieves a 66%\nknot-tying success rate from previously unseen configurations. See\nhttps://tinyurl.com/rope-learning for supplementary material and videos.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 23:43:05 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Sundaresan", "Priya", ""], ["Grannen", "Jennifer", ""], ["Thananjeyan", "Brijen", ""], ["Balakrishna", "Ashwin", ""], ["Laskey", "Michael", ""], ["Stone", "Kevin", ""], ["Gonzalez", "Joseph E.", ""], ["Goldberg", "Ken", ""]]}, {"id": "2003.01866", "submitter": "Eduardo Pavez", "authors": "Eduardo Pavez, Benjamin Girault, Antonio Ortega and Philip A. Chou", "title": "Region adaptive graph fourier transform for 3d point clouds", "comments": "5 pages, 3 figures, accepted ICIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Region Adaptive Graph Fourier Transform (RA-GFT) for\ncompression of 3D point cloud attributes. The RA-GFT is a multiresolution\ntransform, formed by combining spatially localized block transforms. We assume\nthe points are organized by a family of nested partitions represented by a\nrooted tree. At each resolution level, attributes are processed in clusters\nusing block transforms. Each block transform produces a single approximation\n(DC) coefficient, and various detail (AC) coefficients. The DC coefficients are\npromoted up the tree to the next (lower resolution) level, where the process\ncan be repeated until reaching the root. Since clusters may have a different\nnumbers of points, each block transform must incorporate the relative\nimportance of each coefficient. For this, we introduce the\n$\\mathbf{Q}$-normalized graph Laplacian, and propose using its eigenvectors as\nthe block transform. The RA-GFT achieves better complexity-performance\ntrade-offs than previous approaches. In particular, it outperforms the Region\nAdaptive Haar Transform (RAHT) by up to 2.5 dB, with a small complexity\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 02:47:44 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 21:45:58 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Pavez", "Eduardo", ""], ["Girault", "Benjamin", ""], ["Ortega", "Antonio", ""], ["Chou", "Philip A.", ""]]}, {"id": "2003.01871", "submitter": "Julie Stephany Berrio Perez", "authors": "Julie Stephany Berrio, Mao Shan, Stewart Worrall, James Ward, Eduardo\n  Nebot", "title": "Semantic sensor fusion: from camera to sparse lidar information", "comments": "8 pages, this paper was submitted to ITSC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To navigate through urban roads, an automated vehicle must be able to\nperceive and recognize objects in a three-dimensional environment. A high-level\ncontextual understanding of the surroundings is necessary to plan and execute\naccurate driving maneuvers. This paper presents an approach to fuse different\nsensory information, Light Detection and Ranging (lidar) scans and camera\nimages. The output of a convolutional neural network (CNN) is used as\nclassifier to obtain the labels of the environment. The transference of\nsemantic information between the labelled image and the lidar point cloud is\nperformed in four steps: initially, we use heuristic methods to associate\nprobabilities to all the semantic classes contained in the labelled images.\nThen, the lidar points are corrected to compensate for the vehicle's motion\ngiven the difference between the timestamps of each lidar scan and camera\nimage. In a third step, we calculate the pixel coordinate for the corresponding\ncamera image. In the last step we perform the transfer of semantic information\nfrom the heuristic probability images to the lidar frame, while removing the\nlidar information that is not visible to the camera. We tested our approach in\nthe Usyd Dataset \\cite{usyd_dataset}, obtaining qualitative and quantitative\nresults that demonstrate the validity of our probabilistic sensory fusion\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:09:33 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Berrio", "Julie Stephany", ""], ["Shan", "Mao", ""], ["Worrall", "Stewart", ""], ["Ward", "James", ""], ["Nebot", "Eduardo", ""]]}, {"id": "2003.01872", "submitter": "Chengjin Sun", "authors": "Chengjin Sun, Sizhe Chen, Jia Cai, Xiaolin Huang", "title": "Type I Attack for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are popular tools with a wide range of applications.\nNevertheless, it is as vulnerable to adversarial samples as classifiers. The\nexisting attack methods mainly focus on generating adversarial examples by\nadding imperceptible perturbations to input, which leads to wrong result.\nHowever, we focus on another aspect of attack, i.e., cheating models by\nsignificant changes. The former induces Type II error and the latter causes\nType I error. In this paper, we propose Type I attack to generative models such\nas VAE and GAN. One example given in VAE is that we can change an original\nimage significantly to a meaningless one but their reconstruction results are\nsimilar. To implement the Type I attack, we destroy the original one by\nincreasing the distance in input space while keeping the output similar because\ndifferent inputs may correspond to similar features for the property of deep\nneural network. Experimental results show that our attack method is effective\nto generate Type I adversarial examples for generative models on large-scale\nimage datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:20:59 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Sun", "Chengjin", ""], ["Chen", "Sizhe", ""], ["Cai", "Jia", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2003.01874", "submitter": "Fanyi Xiao", "authors": "Fanyi Xiao, Ling Pei, Lei Chu, Danping Zou, Wenxian Yu, Yifan Zhu, Tao\n  Li", "title": "A Deep Learning Method for Complex Human Activity Recognition Using\n  Virtual Wearable Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor-based human activity recognition (HAR) is now a research hotspot in\nmultiple application areas. With the rise of smart wearable devices equipped\nwith inertial measurement units (IMUs), researchers begin to utilize IMU data\nfor HAR. By employing machine learning algorithms, early IMU-based research for\nHAR can achieve accurate classification results on traditional classical HAR\ndatasets, containing only simple and repetitive daily activities. However,\nthese datasets rarely display a rich diversity of information in real-scene. In\nthis paper, we propose a novel method based on deep learning for complex HAR in\nthe real-scene. Specially, in the off-line training stage, the AMASS dataset,\ncontaining abundant human poses and virtual IMU data, is innovatively adopted\nfor enhancing the variety and diversity. Moreover, a deep convolutional neural\nnetwork with an unsupervised penalty is proposed to automatically extract the\nfeatures of AMASS and improve the robustness. In the on-line testing stage, by\nleveraging advantages of the transfer learning, we obtain the final result by\nfine-tuning the partial neural network (optimizing the parameters in the\nfully-connected layers) using the real IMU data. The experimental results show\nthat the proposed method can surprisingly converge in a few iterations and\nachieve an accuracy of 91.15% on a real IMU dataset, demonstrating the\nefficiency and effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:31:23 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 01:18:54 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Xiao", "Fanyi", ""], ["Pei", "Ling", ""], ["Chu", "Lei", ""], ["Zou", "Danping", ""], ["Yu", "Wenxian", ""], ["Zhu", "Yifan", ""], ["Li", "Tao", ""]]}, {"id": "2003.01875", "submitter": "Li Sun Dr", "authors": "Li Sun, Daniel Adolfsson, Martin Magnusson, Henrik Andreasson, Ingmar\n  Posner, and Tom Duckett", "title": "Localising Faster: Efficient and precise lidar-based robot localisation\n  in large-scale environments", "comments": "7 pages, 5 pages. Accepted by IEEE International Conference on\n  Robotics and Automation (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for global localisation of mobile robots\nin large-scale environments. Our method leverages learning-based localisation\nand filtering-based localisation, to localise the robot efficiently and\nprecisely through seeding Monte Carlo Localisation (MCL) with a deep-learned\ndistribution. In particular, a fast localisation system rapidly estimates the\n6-DOF pose through a deep-probabilistic model (Gaussian Process Regression with\na deep kernel), then a precise recursive estimator refines the estimated robot\npose according to the geometric alignment. More importantly, the Gaussian\nmethod (i.e. deep probabilistic localisation) and non-Gaussian method (i.e.\nMCL) can be integrated naturally via importance sampling. Consequently, the two\nsystems can be integrated seamlessly and mutually benefit from each other. To\nverify the proposed framework, we provide a case study in large-scale\nlocalisation with a 3D lidar sensor. Our experiments on the Michigan NCLT\nlong-term dataset show that the proposed method is able to localise the robot\nin 1.94 s on average (median of 0.8 s) with precision 0.75~m in a large-scale\nenvironment of approximately 0.5 km2.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:39:37 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 18:13:23 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Sun", "Li", ""], ["Adolfsson", "Daniel", ""], ["Magnusson", "Martin", ""], ["Andreasson", "Henrik", ""], ["Posner", "Ingmar", ""], ["Duckett", "Tom", ""]]}, {"id": "2003.01888", "submitter": "Saeed Ghorbani", "authors": "Saeed Ghorbani, Kimia Mahdaviani, Anne Thaler, Konrad Kording, Douglas\n  James Cook, Gunnar Blohm, Nikolaus F. Troje", "title": "MoVi: A Large Multipurpose Motion and Video Dataset", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0253157", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human movements are both an area of intense study and the basis of many\napplications such as character animation. For many applications, it is crucial\nto identify movements from videos or analyze datasets of movements. Here we\nintroduce a new human Motion and Video dataset MoVi, which we make available\npublicly. It contains 60 female and 30 male actors performing a collection of\n20 predefined everyday actions and sports movements, and one self-chosen\nmovement. In five capture rounds, the same actors and movements were recorded\nusing different hardware systems, including an optical motion capture system,\nvideo cameras, and inertial measurement units (IMU). For some of the capture\nrounds, the actors were recorded when wearing natural clothing, for the other\nrounds they wore minimal clothing. In total, our dataset contains 9 hours of\nmotion capture data, 17 hours of video data from 4 different points of view\n(including one hand-held camera), and 6.6 hours of IMU data. In this paper, we\ndescribe how the dataset was collected and post-processed; We present\nstate-of-the-art estimates of skeletal motions and full-body shape deformations\nassociated with skeletal motion. We discuss examples for potential studies this\ndataset could enable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 04:43:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ghorbani", "Saeed", ""], ["Mahdaviani", "Kimia", ""], ["Thaler", "Anne", ""], ["Kording", "Konrad", ""], ["Cook", "Douglas James", ""], ["Blohm", "Gunnar", ""], ["Troje", "Nikolaus F.", ""]]}, {"id": "2003.01894", "submitter": "Amir Hossein Raffiee", "authors": "Amir Hossein Raffiee, Michael Sollami", "title": "GarmentGAN: Photo-realistic Adversarial Fashion Transfer", "comments": "9 pages and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The garment transfer problem comprises two tasks: learning to separate a\nperson's body (pose, shape, color) from their clothing (garment type, shape,\nstyle) and then generating new images of the wearer dressed in arbitrary\ngarments. We present GarmentGAN, a new algorithm that performs image-based\ngarment transfer through generative adversarial methods. The GarmentGAN\nframework allows users to virtually try-on items before purchase and\ngeneralizes to various apparel types. GarmentGAN requires as input only two\nimages, namely, a picture of the target fashion item and an image containing\nthe customer. The output is a synthetic image wherein the customer is wearing\nthe target apparel. In order to make the generated image look photo-realistic,\nwe employ the use of novel generative adversarial techniques. GarmentGAN\nimproves on existing methods in the realism of generated imagery and solves\nvarious problems related to self-occlusions. Our proposed model incorporates\nadditional information during training, utilizing both segmentation maps and\nbody key-point information. We show qualitative and quantitative comparisons to\nseveral other networks to demonstrate the effectiveness of this technique.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 05:01:15 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Raffiee", "Amir Hossein", ""], ["Sollami", "Michael", ""]]}, {"id": "2003.01895", "submitter": "Chengjin Sun", "authors": "Chengjin Sun, Sizhe Chen, and Xiaolin Huang", "title": "Double Backpropagation for Training Autoencoders against Adversarial\n  Attack", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, as widely known, is vulnerable to adversarial samples. This\npaper focuses on the adversarial attack on autoencoders. Safety of the\nautoencoders (AEs) is important because they are widely used as a compression\nscheme for data storage and transmission, however, the current autoencoders are\neasily attacked, i.e., one can slightly modify an input but has totally\ndifferent codes. The vulnerability is rooted the sensitivity of the\nautoencoders and to enhance the robustness, we propose to adopt double\nbackpropagation (DBP) to secure autoencoder such as VAE and DRAW. We restrict\nthe gradient from the reconstruction image to the original one so that the\nautoencoder is not sensitive to trivial perturbation produced by the\nadversarial attack. After smoothing the gradient by DBP, we further smooth the\nlabel by Gaussian Mixture Model (GMM), aiming for accurate and robust\nclassification. We demonstrate in MNIST, CelebA, SVHN that our method leads to\na robust autoencoder resistant to attack and a robust classifier able for image\ntransition and immune to adversarial attack if combined with GMM.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 05:12:27 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Sun", "Chengjin", ""], ["Chen", "Sizhe", ""], ["Huang", "Xiaolin", ""]]}, {"id": "2003.01908", "submitter": "Hadi Salman", "authors": "Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor and J. Zico Kolter", "title": "Denoised Smoothing: A Provable Defense for Pretrained Classifiers", "comments": "10 pages main text; 29 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for provably defending any pretrained image classifier\nagainst $\\ell_p$ adversarial attacks. This method, for instance, allows public\nvision API providers and users to seamlessly convert pretrained non-robust\nclassification services into provably robust ones. By prepending a\ncustom-trained denoiser to any off-the-shelf image classifier and using\nrandomized smoothing, we effectively create a new classifier that is guaranteed\nto be $\\ell_p$-robust to adversarial examples, without modifying the pretrained\nclassifier. Our approach applies to both the white-box and the black-box\nsettings of the pretrained classifier. We refer to this defense as denoised\nsmoothing, and we demonstrate its effectiveness through extensive\nexperimentation on ImageNet and CIFAR-10. Finally, we use our approach to\nprovably defend the Azure, Google, AWS, and ClarifAI image classification APIs.\nOur code replicating all the experiments in the paper can be found at:\nhttps://github.com/microsoft/denoised-smoothing.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 06:15:55 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 02:20:16 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Salman", "Hadi", ""], ["Sun", "Mingjie", ""], ["Yang", "Greg", ""], ["Kapoor", "Ashish", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2003.01913", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Yue Lu, Zhengxing Wu, Junzhi Yu, and Li Wen", "title": "Reveal of Domain Effect: How Visual Restoration Contributes to Object\n  Detection in Aquatic Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater robotic perception usually requires visual restoration and object\ndetection, both of which have been studied for many years. Meanwhile, data\ndomain has a huge impact on modern data-driven leaning process. However,\nexactly indicating domain effect, the relation between restoration and\ndetection remains unclear. In this paper, we generally investigate the relation\nof quality-diverse data domain to detection performance. In the meantime, we\nunveil how visual restoration contributes to object detection in real-world\nunderwater scenes. According to our analysis, five key discoveries are\nreported: 1) Domain quality has an ignorable effect on within-domain\nconvolutional representation and detection accuracy; 2) low-quality domain\nleads to higher generalization ability in cross-domain detection; 3)\nlow-quality domain can hardly be well learned in a domain-mixed learning\nprocess; 4) degrading recall efficiency, restoration cannot improve\nwithin-domain detection accuracy; 5) visual restoration is beneficial to\ndetection in the wild by reducing the domain shift between training data and\nreal-world scenes. Finally, as an illustrative example, we successfully perform\nunderwater object detection with an aquatic robot.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 06:44:19 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Xingyu", ""], ["Lu", "Yue", ""], ["Wu", "Zhengxing", ""], ["Yu", "Junzhi", ""], ["Wen", "Li", ""]]}, {"id": "2003.01920", "submitter": "Jinhyeok Jang", "authors": "Jinhyeok Jang, Dohyung Kim, Cheonshu Park, Minsu Jang, Jaeyeon Lee,\n  Jaehong Kim", "title": "ETRI-Activity3D: A Large-Scale RGB-D Dataset for Robots to Recognize\n  Daily Activities of the Elderly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, based on which many modern algorithms operate, is well known\nto be data-hungry. In particular, the datasets appropriate for the intended\napplication are difficult to obtain. To cope with this situation, we introduce\na new dataset called ETRI-Activity3D, focusing on the daily activities of the\nelderly in robot-view. The major characteristics of the new dataset are as\nfollows: 1) practical action categories that are selected from the close\nobservation of the daily lives of the elderly; 2) realistic data collection,\nwhich reflects the robot's working environment and service situations; and 3) a\nlarge-scale dataset that overcomes the limitations of the current 3D activity\nanalysis benchmark datasets. The proposed dataset contains 112,620 samples\nincluding RGB videos, depth maps, and skeleton sequences. During the data\nacquisition, 100 subjects were asked to perform 55 daily activities.\nAdditionally, we propose a novel network called four-stream adaptive CNN\n(FSA-CNN). The proposed FSA-CNN has three main properties: robustness to\nspatio-temporal variations, input-adaptive activation function, and extension\nof the conventional two-stream approach. In the experiment section, we\nconfirmed the superiority of the proposed FSA-CNN using NTU RGB+D and\nETRI-Activity3D. Further, the domain difference between both groups of age was\nverified experimentally. Finally, the extension of FSA-CNN to deal with the\nmultimodal data was investigated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 07:30:16 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 05:01:07 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Jang", "Jinhyeok", ""], ["Kim", "Dohyung", ""], ["Park", "Cheonshu", ""], ["Jang", "Minsu", ""], ["Lee", "Jaeyeon", ""], ["Kim", "Jaehong", ""]]}, {"id": "2003.01936", "submitter": "Chowdhury Rahman", "authors": "Md. Sadrul Islam Toaha, Sakib Bin Asad, Chowdhury Rafeed Rahman, S.M.\n  Shahriar Haque, Mahfuz Ara Proma, Md. Ahsan Habib Shuvo, Tashin Ahmed, Md.\n  Amimul Basher", "title": "Automatic Signboard Detection and Localization in Densely Populated\n  Developing Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most city establishments of developing cities are unlabeled because of the\nnecessity of manual annotation. Hence location and trajectory services remain\nunder utilized in such cities. Accurate signboard detection and localization in\nnatural scene images is the foremost task for accurate information retrieval\nfrom such city streets. We develop an automated signboard detection system\nsuitable for such cities using Faster R-CNN based localization by incorporating\ntwo specialized pretraining methods and a run time efficient hyperparameter\nvalue selection algorithm. We have taken an incremental approach in reaching\nour final proposed model through detailed evaluation and comparison with\nbaselines using our constructed SVSO signboard dataset containing signboard\nnatural scene images of six developing countries. Our proposed method can\ndetect signboards accurately, even though images contain multiple signboards\nwith diverse shapes and colours in a noisy background. Our proposed model\nachieves 0.91 mAP score on validation set and 0.90 mAP score on an independent\ntest set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:04:03 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 04:31:20 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 20:21:18 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Toaha", "Md. Sadrul Islam", ""], ["Asad", "Sakib Bin", ""], ["Rahman", "Chowdhury Rafeed", ""], ["Haque", "S. M. Shahriar", ""], ["Proma", "Mahfuz Ara", ""], ["Shuvo", "Md. Ahsan Habib", ""], ["Ahmed", "Tashin", ""], ["Basher", "Md. Amimul", ""]]}, {"id": "2003.01941", "submitter": "Chenlin Meng", "authors": "Chenlin Meng, Yang Song, Jiaming Song and Stefano Ermon", "title": "Gaussianization Flows", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative Gaussianization is a fixed-point iteration procedure that can\ntransform any continuous random vector into a Gaussian one. Based on iterative\nGaussianization, we propose a new type of normalizing flow model that enables\nboth efficient computation of likelihoods and efficient inversion for sample\ngeneration. We demonstrate that these models, named Gaussianization flows, are\nuniversal approximators for continuous probability distributions under some\nregularity conditions. Because of this guaranteed expressivity, they can\ncapture multimodal target distributions without compromising the efficiency of\nsample generation. Experimentally, we show that Gaussianization flows achieve\nbetter or comparable performance on several tabular datasets compared to other\nefficiently invertible flow models such as Real NVP, Glow and FFJORD. In\nparticular, Gaussianization flows are easier to initialize, demonstrate better\nrobustness with respect to different transformations of the training data, and\ngeneralize better on small training sets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:15:06 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Meng", "Chenlin", ""], ["Song", "Yang", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "2003.01944", "submitter": "Huy Hoang Nguyen", "authors": "Huy Hoang Nguyen, Simo Saarakkala, Matthew Blaschko, Aleksei Tiulpin", "title": "Semixup: In- and Out-of-Manifold Regularization for Deep Semi-Supervised\n  Knee Osteoarthritis Severity Grading from Plain Radiographs", "comments": "11 main, 03 supplementary pages. The manuscript was accepted to IEEE\n  Transactions on Medical Imaging in August 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee osteoarthritis (OA) is one of the highest disability factors in the\nworld. This musculoskeletal disorder is assessed from clinical symptoms, and\ntypically confirmed via radiographic assessment. This visual assessment done by\na radiologist requires experience, and suffers from moderate to high\ninter-observer variability. The recent literature has shown that deep learning\nmethods can reliably perform the OA severity assessment according to the gold\nstandard Kellgren-Lawrence (KL) grading system. However, these methods require\nlarge amounts of labeled data, which are costly to obtain. In this study, we\npropose the Semixup algorithm, a semi-supervised learning (SSL) approach to\nleverage unlabeled data. Semixup relies on consistency regularization using in-\nand out-of-manifold samples, together with interpolated consistency. On an\nindependent test set, our method significantly outperformed other\nstate-of-the-art SSL methods in most cases. Finally, when compared to a\nwell-tuned fully supervised baseline that yielded a balanced accuracy (BA) of\n$70.9\\pm0.8%$ on the test set, Semixup had comparable performance -- BA of\n$71\\pm0.8%$ $(p=0.368)$ while requiring $6$ times less labeled data. These\nresults show that our proposed SSL method allows building fully automatic OA\nseverity assessment tools with datasets that are available outside research\nsettings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:33:36 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 06:21:31 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 09:44:29 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Nguyen", "Huy Hoang", ""], ["Saarakkala", "Simo", ""], ["Blaschko", "Matthew", ""], ["Tiulpin", "Aleksei", ""]]}, {"id": "2003.01947", "submitter": "Yongsen Zhao", "authors": "Yongsen Zhao, Deming Zhai, Junjun Jiang, Xianming Liu", "title": "ADRN: Attention-based Deep Residual Network for Hyperspectral Image\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) denoising is of crucial importance for many\nsubsequent applications, such as HSI classification and interpretation. In this\npaper, we propose an attention-based deep residual network to directly learn a\nmapping from noisy HSI to the clean one. To jointly utilize the\nspatial-spectral information, the current band and its $K$ adjacent bands are\nsimultaneously exploited as the input. Then, we adopt convolution layer with\ndifferent filter sizes to fuse the multi-scale feature, and use shortcut\nconnection to incorporate the multi-level information for better noise removal.\nIn addition, the channel attention mechanism is employed to make the network\nconcentrate on the most relevant auxiliary information and features that are\nbeneficial to the denoising process best. To ease the training procedure, we\nreconstruct the output through a residual mode rather than a straightforward\nprediction. Experimental results demonstrate that our proposed ADRN scheme\noutperforms the state-of-the-art methods both in quantitative and visual\nevaluations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:36:27 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Zhao", "Yongsen", ""], ["Zhai", "Deming", ""], ["Jiang", "Junjun", ""], ["Liu", "Xianming", ""]]}, {"id": "2003.01960", "submitter": "Jianfeng Li", "authors": "Jianfeng Li, Junqiao Zhao, Tiantian Feng, Chen Ye, Lu Xiong", "title": "Occlusion Aware Unsupervised Learning of Optical Flow From Video", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we proposed an unsupervised learning method for estimating the\noptical flow between video frames, especially to solve the occlusion problem.\nOcclusion is caused by the movement of an object or the movement of the camera,\ndefined as when certain pixels are visible in one video frame but not in\nadjacent frames. Due to the lack of pixel correspondence between frames in the\noccluded area, incorrect photometric loss calculation can mislead the optical\nflow training process. In the video sequence, we found that the occlusion in\nthe forward ($t\\rightarrow t+1$) and backward ($t\\rightarrow t-1$) frame pairs\nare usually complementary. That is, pixels that are occluded in subsequent\nframes are often not occluded in the previous frame and vice versa. Therefore,\nby using this complementarity, a new weighted loss is proposed to solve the\nocclusion problem. In addition, we calculate gradients in multiple directions\nto provide richer supervision information. Our method achieves competitive\noptical flow accuracy compared to the baseline and some supervised methods on\nKITTI 2012 and 2015 benchmarks. This source code has been released at\nhttps://github.com/jianfenglihg/UnOpticalFlow.git.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 09:08:03 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Li", "Jianfeng", ""], ["Zhao", "Junqiao", ""], ["Feng", "Tiantian", ""], ["Ye", "Chen", ""], ["Xiong", "Lu", ""]]}, {"id": "2003.01966", "submitter": "Ren Yang", "authors": "Ren Yang, Fabian Mentzer, Luc Van Gool, Radu Timofte", "title": "Learning for Video Compression with Hierarchical Quality and Recurrent\n  Enhancement", "comments": "Published in CVPR 2020; corrected a minor typo in the footnote of\n  Table 1; corrected Figure 11", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Hierarchical Learned Video Compression (HLVC)\nmethod with three hierarchical quality layers and a recurrent enhancement\nnetwork. The frames in the first layer are compressed by an image compression\nmethod with the highest quality. Using these frames as references, we propose\nthe Bi-Directional Deep Compression (BDDC) network to compress the second layer\nwith relatively high quality. Then, the third layer frames are compressed with\nthe lowest quality, by the proposed Single Motion Deep Compression (SMDC)\nnetwork, which adopts a single motion map to estimate the motions of multiple\nframes, thus saving bits for motion information. In our deep decoder, we\ndevelop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes\nboth compressed frames and the bit stream as inputs. In the recurrent cell of\nWRQE, the memory and update signal are weighted by quality features to\nreasonably leverage multi-frame information for enhancement. In our HLVC\napproach, the hierarchical quality benefits the coding efficiency, since the\nhigh quality information facilitates the compression and enhancement of low\nquality frames at encoder and decoder sides, respectively. Finally, the\nexperiments validate that our HLVC approach advances the state-of-the-art of\ndeep video compression methods, and outperforms the \"Low-Delay P (LDP) very\nfast\" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at\nhttps://github.com/RenYang-home/HLVC.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 09:31:37 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:43:33 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 17:00:03 GMT"}, {"version": "v4", "created": "Mon, 30 Mar 2020 22:38:44 GMT"}, {"version": "v5", "created": "Wed, 1 Apr 2020 18:51:48 GMT"}, {"version": "v6", "created": "Wed, 8 Apr 2020 20:20:36 GMT"}, {"version": "v7", "created": "Mon, 3 Aug 2020 18:35:37 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yang", "Ren", ""], ["Mentzer", "Fabian", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.01989", "submitter": "Fabian Wolf", "authors": "Fabian Wolf and Gernot A. Fink", "title": "Annotation-free Learning of Deep Representations for Word Spotting using\n  Synthetic Data and Self Labeling", "comments": "Accepted to Workshop on Document Analysis Systems (DAS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word spotting is a popular tool for supporting the first exploration of\nhistoric, handwritten document collections. Today, the best performing methods\nrely on machine learning techniques, which require a high amount of annotated\ntraining material. As training data is usually not available in the application\nscenario, annotation-free methods aim at solving the retrieval task without\nrepresentative training samples. In this work, we present an annotation-free\nmethod that still employs machine learning techniques and therefore outperforms\nother learning-free approaches. The weakly supervised training scheme relies on\na lexicon, that does not need to precisely fit the dataset. In combination with\na confidence based selection of pseudo-labeled training samples, we achieve\nstate-of-the-art query-by-example performances. Furthermore, our method allows\nto perform query-by-string, which is usually not the case for other\nannotation-free methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 10:46:25 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 09:45:44 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 07:51:18 GMT"}, {"version": "v4", "created": "Mon, 25 May 2020 08:58:17 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wolf", "Fabian", ""], ["Fink", "Gernot A.", ""]]}, {"id": "2003.01993", "submitter": "Igor Buzhinsky", "authors": "Igor Buzhinsky, Arseny Nerinovsky, Stavros Tripakis", "title": "Metrics and methods for robustness evaluation of neural networks with\n  generative models", "comments": "24 pages, 9 figures; data in Table 3 and Fig. 3 corrected (results\n  unchanged), several typos fixed, references updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that modern deep neural network classifiers are\neasy to fool, assuming that an adversary is able to slightly modify their\ninputs. Many papers have proposed adversarial attacks, defenses and methods to\nmeasure robustness to such adversarial perturbations. However, most commonly\nconsidered adversarial examples are based on $\\ell_p$-bounded perturbations in\nthe input space of the neural network, which are unlikely to arise naturally.\nRecently, especially in computer vision, researchers discovered \"natural\" or\n\"semantic\" perturbations, such as rotations, changes of brightness, or more\nhigh-level changes, but these perturbations have not yet been systematically\nutilized to measure the performance of classifiers. In this paper, we propose\nseveral metrics to measure robustness of classifiers to natural adversarial\nexamples, and methods to evaluate them. These metrics, called latent space\nperformance metrics, are based on the ability of generative models to capture\nprobability distributions, and are defined in their latent spaces. On three\nimage classification case studies, we evaluate the proposed metrics for several\nclassifiers, including ones trained in conventional and robust ways. We find\nthat the latent counterparts of adversarial robustness are associated with the\naccuracy of the classifier rather than its conventional adversarial robustness,\nbut the latter is still reflected on the properties of found latent\nperturbations. In addition, our novel method of finding latent adversarial\nperturbations demonstrates that these perturbations are often perceptually\nsmall.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 10:58:59 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 15:55:23 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Buzhinsky", "Igor", ""], ["Nerinovsky", "Arseny", ""], ["Tripakis", "Stavros", ""]]}, {"id": "2003.01994", "submitter": "Murat Kirtay", "authors": "Murat Kirtay, Ugo Albanese, Lorenzo Vannucci, Guido Schillaci, Cecilia\n  Laschi, Egidio Falotico", "title": "The iCub multisensor datasets for robot and computer vision applications", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document presents novel datasets, constructed by employing the iCub\nrobot equipped with an additional depth sensor and color camera. We used the\nrobot to acquire color and depth information for 210 objects in different\nacquisition scenarios. At this end, the results were large scale datasets for\nrobot and computer vision applications: object representation, object\nrecognition and classification, and action recognition.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 10:59:29 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kirtay", "Murat", ""], ["Albanese", "Ugo", ""], ["Vannucci", "Lorenzo", ""], ["Schillaci", "Guido", ""], ["Laschi", "Cecilia", ""], ["Falotico", "Egidio", ""]]}, {"id": "2003.01995", "submitter": "Benjamin Billot", "authors": "Benjamin Billot, Douglas Greve, Koen Van Leemput, Bruce Fischl, Juan\n  Eugenio Iglesias, Adrian V. Dalca", "title": "A Learning Strategy for Contrast-agnostic MRI Segmentation", "comments": "19 pages, 5 figures", "journal-ref": "Proceedings of the Third Conference on Medical Imaging with Deep\n  Learning (2020), vol.121, pp. 75-93", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning strategy that enables, for the first time,\ncontrast-agnostic semantic segmentation of completely unpreprocessed brain MRI\nscans, without requiring additional training or fine-tuning for new modalities.\nClassical Bayesian methods address this segmentation problem with unsupervised\nintensity models, but require significant computational resources. In contrast,\nlearning-based methods can be fast at test time, but are sensitive to the data\navailable at training. Our proposed learning method, SynthSeg, leverages a set\nof training segmentations (no intensity images required) to generate synthetic\nsample images of widely varying contrasts on the fly during training. These\nsamples are produced using the generative model of the classical Bayesian\nsegmentation framework, with randomly sampled parameters for appearance,\ndeformation, noise, and bias field. Because each mini-batch has a different\nsynthetic contrast, the final network is not biased towards any MRI contrast.\nWe comprehensively evaluate our approach on four datasets comprising over 1,000\nsubjects and four types of MR contrast. The results show that our approach\nsuccessfully segments every contrast in the data, performing slightly better\nthan classical Bayesian segmentation, and three orders of magnitude faster.\nMoreover, even within the same type of MRI contrast, our strategy generalizes\nsignificantly better across datasets, compared to training using real images.\nFinally, we find that synthesizing a broad range of contrasts, even if\nunrealistic, increases the generalization of the neural network. Our code and\nmodel are open source at https://github.com/BBillot/SynthSeg.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 11:00:57 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 14:56:37 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 11:47:24 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Billot", "Benjamin", ""], ["Greve", "Douglas", ""], ["Van Leemput", "Koen", ""], ["Fischl", "Bruce", ""], ["Iglesias", "Juan Eugenio", ""], ["Dalca", "Adrian V.", ""]]}, {"id": "2003.02012", "submitter": "Ze Cui", "authors": "Ze Cui, Jing Wang, Bo Bai, Tiansheng Guo and Yihui Feng", "title": "G-VAE: A Continuously Variable Rate Deep Image Compression Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rate adaption of deep image compression in a single model will become one of\nthe decisive factors competing with the classical image compression codecs.\nHowever, until now, there is no perfect solution that neither increases the\ncomputation nor affects the compression performance. In this paper, we propose\na novel image compression framework G-VAE (Gained Variational Autoencoder),\nwhich could achieve continuously variable rate in a single model. Unlike the\nprevious solutions that encode progressively or change the internal unit of the\nnetwork, G-VAE only adds a pair of gain units at the output of encoder and the\ninput of decoder. It is so concise that G-VAE could be applied to almost all\nthe image compression methods and achieve continuously variable rate with\nnegligible additional parameters and computation. We also propose a new deep\nimage compression framework, which outperforms all the published results on\nKodak datasets in PSNR and MS-SSIM metrics. Experimental results show that\nadding a pair of gain units will not affect the performance of the basic models\nwhile endowing them with continuously variable rate.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 11:42:05 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 01:43:47 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Cui", "Ze", ""], ["Wang", "Jing", ""], ["Bai", "Bo", ""], ["Guo", "Tiansheng", ""], ["Feng", "Yihui", ""]]}, {"id": "2003.02014", "submitter": "Zichao Zhang", "authors": "Juichung Kuo, Manasi Muglikar, Zichao Zhang, Davide Scaramuzza", "title": "Redesigning SLAM for Arbitrary Multi-Camera Systems", "comments": null, "journal-ref": "IEEE Conference on Robotics and Automation (ICRA), Paris, 2020", "doi": "10.1109/ICRA40945.2020.9197553", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adding more cameras to SLAM systems improves robustness and accuracy but\ncomplicates the design of the visual front-end significantly. Thus, most\nsystems in the literature are tailored for specific camera configurations. In\nthis work, we aim at an adaptive SLAM system that works for arbitrary\nmulti-camera setups. To this end, we revisit several common building blocks in\nvisual SLAM. In particular, we propose an adaptive initialization scheme, a\nsensor-agnostic, information-theoretic keyframe selection algorithm, and a\nscalable voxel-based map. These techniques make little assumption about the\nactual camera setups and prefer theoretically grounded methods over heuristics.\nWe adapt a state-of-the-art visual-inertial odometry with these modifications,\nand experimental results show that the modified pipeline can adapt to a wide\nrange of camera setups (e.g., 2 to 6 cameras in one experiment) without the\nneed of sensor-specific modifications or tuning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 11:44:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kuo", "Juichung", ""], ["Muglikar", "Manasi", ""], ["Zhang", "Zichao", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.02027", "submitter": "Mikolaj Jankowski", "authors": "Mikolaj Jankowski, Deniz Gunduz, Krystian Mikolajczyk", "title": "Joint Device-Edge Inference over Wireless Links with Pruning", "comments": null, "journal-ref": null, "doi": "10.1109/SPAWC48557.2020.9154306", "report-no": null, "categories": "cs.IT cs.CV cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a joint feature compression and transmission scheme for efficient\ninference at the wireless network edge. Our goal is to enable efficient and\nreliable inference at the edge server assuming limited computational resources\nat the edge device. Previous work focused mainly on feature compression,\nignoring the computational cost of channel coding. We incorporate the recently\nproposed deep joint source-channel coding (DeepJSCC) scheme, and combine it\nwith novel filter pruning strategies aimed at reducing the redundant complexity\nfrom neural networks. We evaluate our approach on a classification task, and\nshow improved results in both end-to-end reliability and workload reduction at\nthe edge device. This is the first work that combines DeepJSCC with network\npruning, and applies it to image classification over the wireless edge.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 12:06:11 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:32:01 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Jankowski", "Mikolaj", ""], ["Gunduz", "Deniz", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2003.02050", "submitter": "Aymen Mir", "authors": "Aymen Mir, Thiemo Alldieck, Gerard Pons-Moll", "title": "Learning to Transfer Texture from Clothing Images to 3D Humans", "comments": "IEEE Conference on Computer Vision and Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a simple yet effective method to automatically\ntransfer textures of clothing images (front and back) to 3D garments worn on\ntop SMPL, in real time. We first automatically compute training pairs of images\nwith aligned 3D garments using a custom non-rigid 3D to 2D registration method,\nwhich is accurate but slow. Using these pairs, we learn a mapping from pixels\nto the 3D garment surface. Our idea is to learn dense correspondences from\ngarment image silhouettes to a 2D-UV map of a 3D garment surface using shape\ninformation alone, completely ignoring texture, which allows us to generalize\nto the wide range of web images. Several experiments demonstrate that our model\nis more accurate than widely used baselines such as thin-plate-spline warping\nand image-to-image translation networks while being orders of magnitude faster.\nOur model opens the door for applications such as virtual try-on, and allows\nfor generation of 3D humans with varied textures which is necessary for\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 12:53:58 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 23:35:26 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Mir", "Aymen", ""], ["Alldieck", "Thiemo", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2003.02059", "submitter": "Minghui Zheng", "authors": "Wansong Liu, Danyang Luo, Changxu Wu, Minghui Zheng", "title": "Vehicle-Human Interactive Behaviors in Emergency: Data Extraction from\n  Traffic Accident Videos", "comments": "ACC 2020 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, studying the vehicle-human interactive behavior in the emergency\nneeds a large amount of datasets in the actual emergent situations that are\nalmost unavailable. Existing public data sources on autonomous vehicles (AVs)\nmainly focus either on the normal driving scenarios or on emergency situations\nwithout human involvement. To fill this gap and facilitate related research,\nthis paper provides a new yet convenient way to extract the interactive\nbehavior data (i.e., the trajectories of vehicles and humans) from actual\naccident videos that were captured by both the surveillance cameras and driving\nrecorders. The main challenge for data extraction from real-time accident video\nlies in the fact that the recording cameras are un-calibrated and the angles of\nsurveillance are unknown. The approach proposed in this paper employs image\nprocessing to obtain a new perspective which is different from the original\nvideo's perspective. Meanwhile, we manually detect and mark object feature\npoints in each image frame. In order to acquire a gradient of reference ratios,\na geometric model is implemented in the analysis of reference pixel value, and\nthe feature points are then scaled to the object trajectory based on the\ngradient of ratios. The generated trajectories not only restore the object\nmovements completely but also reflect changes in vehicle velocity and rotation\nbased on the feature points distributions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 22:17:46 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 04:10:05 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Liu", "Wansong", ""], ["Luo", "Danyang", ""], ["Wu", "Changxu", ""], ["Zheng", "Minghui", ""]]}, {"id": "2003.02065", "submitter": "Shahine Bouabid", "authors": "Shahine Bouabid and Vincent Delaitre", "title": "Mixup Regularization for Region Proposal based Object Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixup - a neural network regularization technique based on linear\ninterpolation of labeled sample pairs - has stood out by its capacity to\nimprove model's robustness and generalizability through a surprisingly simple\nformalism. However, its extension to the field of object detection remains\nunclear as the interpolation of bounding boxes cannot be naively defined. In\nthis paper, we propose to leverage the inherent region mapping structure of\nanchors to introduce a mixup-driven training regularization for region proposal\nbased object detectors. The proposed method is benchmarked on standard datasets\nwith challenging detection settings. Our experiments show an enhanced\nrobustness to image alterations along with an ability to decontextualize\ndetections, resulting in an improved generalization power.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 13:16:45 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Bouabid", "Shahine", ""], ["Delaitre", "Vincent", ""]]}, {"id": "2003.02068", "submitter": "Chong Liu", "authors": "Chong Liu and Xiaojun Chang and Yi-Dong Shen", "title": "Unity Style Transfer for Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Style variation has been a major challenge for person re-identification,\nwhich aims to match the same pedestrians across different cameras. Existing\nworks attempted to address this problem with camera-invariant descriptor\nsubspace learning. However, there will be more image artifacts when the\ndifference between the images taken by different cameras is larger. To solve\nthis problem, we propose a UnityStyle adaption method, which can smooth the\nstyle disparities within the same camera and across different cameras.\nSpecifically, we firstly create UnityGAN to learn the style changes between\ncameras, producing shape-stable style-unity images for each camera, which is\ncalled UnityStyle images. Meanwhile, we use UnityStyle images to eliminate\nstyle differences between different images, which makes a better match between\nquery and gallery. Then, we apply the proposed method to Re-ID models,\nexpecting to obtain more style-robust depth features for querying. We conduct\nextensive experiments on widely used benchmark datasets to evaluate the\nperformance of the proposed framework, the results of which confirm the\nsuperiority of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 13:22:57 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Liu", "Chong", ""], ["Chang", "Xiaojun", ""], ["Shen", "Yi-Dong", ""]]}, {"id": "2003.02115", "submitter": "Xu Tan", "authors": "Jiale Chen, Xu Tan, Chaowei Shan, Sen Liu and Zhibo Chen", "title": "VESR-Net: The Winning Solution to Youku Video Enhancement and\n  Super-Resolution Challenge", "comments": "The champion of Youku-VESR challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces VESR-Net, a method for video enhancement and\nsuper-resolution (VESR). We design a separate non-local module to explore the\nrelations among video frames and fuse video frames efficiently, and a channel\nattention residual block to capture the relations among feature maps for video\nframe reconstruction in VESR-Net. We conduct experiments to analyze the\neffectiveness of these designs in VESR-Net, which demonstrates the advantages\nof VESR-Net over previous state-of-the-art VESR methods. It is worth to mention\nthat among more than thousands of participants for Youku video enhancement and\nsuper-resolution (Youku-VESR) challenge, our proposed VESR-Net beat other\ncompetitive methods and ranked the first place.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 15:09:17 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Jiale", ""], ["Tan", "Xu", ""], ["Shan", "Chaowei", ""], ["Liu", "Sen", ""], ["Chen", "Zhibo", ""]]}, {"id": "2003.02170", "submitter": "Sanghoon Hong", "authors": "Sanghoon Hong, Hunchul Park, Jonghyuk Park, Sukhyun Cho, Heewoong Park", "title": "HintPose", "comments": "Presented at \"Joint COCO and Mapillary Workshop at ICCV 2019:\n  Keypoint Detection Challenge Track\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the top-down pose estimation models assume that there exists only one\nperson in a bounding box. However, the assumption is not always correct. In\nthis technical report, we introduce two ideas, instance cue and recurrent\nrefinement, to an existing pose estimator so that the model is able to handle\ndetection boxes with multiple persons properly. When we evaluated our model on\nthe COCO17 keypoints dataset, it showed non-negligible improvement compared to\nits baseline model. Our model achieved 76.2 mAP as a single model and 77.3 mAP\nas an ensemble on the test-dev set without additional training data. After\nadditional post-processing with a separate refinement network, our final\npredictions achieved 77.8 mAP on the COCO test-dev set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 16:29:31 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Hong", "Sanghoon", ""], ["Park", "Hunchul", ""], ["Park", "Jonghyuk", ""], ["Cho", "Sukhyun", ""], ["Park", "Heewoong", ""]]}, {"id": "2003.02188", "submitter": "Evgenii Zheltonozhskii", "authors": "Evgenii Zheltonozhskii, Chaim Baskin, Yaniv Nemcovsky, Brian Chmiel,\n  Avi Mendelson, Alex M. Bronstein", "title": "Colored Noise Injection for Training Adversarially Robust Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Even though deep learning has shown unmatched performance on various tasks,\nneural networks have been shown to be vulnerable to small adversarial\nperturbations of the input that lead to significant performance degradation. In\nthis work we extend the idea of adding white Gaussian noise to the network\nweights and activations during adversarial training (PNI) to the injection of\ncolored noise for defense against common white-box and black-box attacks. We\nshow that our approach outperforms PNI and various previous approaches in terms\nof adversarial accuracy on CIFAR-10 and CIFAR-100 datasets. In addition, we\nprovide an extensive ablation study of the proposed method justifying the\nchosen configurations.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:01:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 12:21:56 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Zheltonozhskii", "Evgenii", ""], ["Baskin", "Chaim", ""], ["Nemcovsky", "Yaniv", ""], ["Chmiel", "Brian", ""], ["Mendelson", "Avi", ""], ["Bronstein", "Alex M.", ""]]}, {"id": "2003.02204", "submitter": "Feras Almasri", "authors": "Feras Almasri, Olivier Debeir", "title": "Robust Perceptual Night Vision in Thermal Colorization", "comments": "9 pages, 7 figures, VISAPP2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming a thermal infrared image into a robust perceptual colour Visible\nimage is an ill-posed problem due to the differences in their spectral domains\nand in the objects' representations. Objects appear in one spectrum but not\nnecessarily in the other, and the thermal signature of a single object may have\ndifferent colours in its Visible representation. This makes a direct mapping\nfrom thermal to Visible images impossible and necessitates a solution that\npreserves texture captured in the thermal spectrum while predicting the\npossible colour for certain objects. In this work, a deep learning method to\nmap the thermal signature from the thermal image's spectrum to a Visible\nrepresentation in their low-frequency space is proposed. A pan-sharpening\nmethod is then used to merge the predicted low-frequency representation with\nthe high-frequency representation extracted from the thermal image. The\nproposed model generates colour values consistent with the Visible ground truth\nwhen the object does not vary much in its appearance and generates averaged\ngrey values in other cases. The proposed method shows robust perceptual night\nvision images in preserving the object's appearance and image context compared\nwith the existing state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:17:08 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Almasri", "Feras", ""], ["Debeir", "Olivier", ""]]}, {"id": "2003.02247", "submitter": "Manasi Muglikar Ms.", "authors": "Manasi Muglikar, Zichao Zhang and Davide Scaramuzza", "title": "Voxel Map for Visual SLAM", "comments": null, "journal-ref": "IEEE Conference on Robotics and Automation(ICRA), Paris, 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern visual SLAM systems, it is a standard practice to retrieve\npotential candidate map points from overlapping keyframes for further feature\nmatching or direct tracking. In this work, we argue that keyframes are not the\noptimal choice for this task, due to several inherent limitations, such as weak\ngeometric reasoning and poor scalability. We propose a voxel-map representation\nto efficiently retrieve map points for visual SLAM. In particular, we organize\nthe map points in a regular voxel grid. Visible points from a camera pose are\nqueried by sampling the camera frustum in a raycasting manner, which can be\ndone in constant time using an efficient voxel hashing method. Compared with\nkeyframes, the retrieved points using our method are geometrically guaranteed\nto fall in the camera field-of-view, and occluded points can be identified and\nremoved to a certain extend. This method also naturally scales up to large\nscenes and complicated multicamera configurations. Experimental results show\nthat our voxel map representation is as efficient as a keyframe map with 5\nkeyframes and provides significantly higher localization accuracy (average 46%\nimprovement in RMSE) on the EuRoC dataset. The proposed voxel-map\nrepresentation is a general approach to a fundamental functionality in visual\nSLAM and widely applicable.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:39:14 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Muglikar", "Manasi", ""], ["Zhang", "Zichao", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.02260", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Arian Mehrfard, Tianyu Song, Alex Johnson, Greg Osgood,\n  Mathias Unberath, Mehran Armand, and Nassir Navab", "title": "Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided\n  Therapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suboptimal interaction with patient data and challenges in mastering 3D\nanatomy based on ill-posed 2D interventional images are essential concerns in\nimage-guided therapies. Augmented reality (AR) has been introduced in the\noperating rooms in the last decade; however, in image-guided interventions, it\nhas often only been considered as a visualization device improving traditional\nworkflows. As a consequence, the technology is gaining minimum maturity that it\nrequires to redefine new procedures, user interfaces, and interactions. The\nmain contribution of this paper is to reveal how exemplary workflows are\nredefined by taking full advantage of head-mounted displays when entirely\nco-registered with the imaging system at all times. The proposed AR landscape\nis enabled by co-localizing the users and the imaging devices via the operating\nroom environment and exploiting all involved frustums to move spatial\ninformation between different bodies. The awareness of the system from the\ngeometric and physical characteristics of X-ray imaging allows the redefinition\nof different human-machine interfaces. We demonstrate that this AR paradigm is\ngeneric, and can benefit a wide variety of procedures. Our system achieved an\nerror of $4.76\\pm2.91$ mm for placing K-wire in a fracture management\nprocedure, and yielded errors of $1.57\\pm1.16^\\circ$ and $1.46\\pm1.00^\\circ$ in\nthe abduction and anteversion angles, respectively, for total hip arthroplasty.\nWe hope that our holistic approach towards improving the interface of surgery\nnot only augments the surgeon's capabilities but also augments the surgical\nteam's experience in carrying out an effective intervention with reduced\ncomplications and provide novel approaches of documenting procedures for\ntraining purposes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:59:55 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Fotouhi", "Javad", ""], ["Mehrfard", "Arian", ""], ["Song", "Tianyu", ""], ["Johnson", "Alex", ""], ["Osgood", "Greg", ""], ["Unberath", "Mathias", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "2003.02294", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Giacomo Taylor, Mathias Unberath, Alex Johnson, Sing\n  Chun Lee, Greg Osgood, Mehran Armand, Nassir Navab", "title": "Exploring Partial Intrinsic and Extrinsic Symmetry in 3D Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel methodology to detect imperfect bilateral symmetry in CT\nof human anatomy. In this paper, the structurally symmetric nature of the\npelvic bone is explored and is used to provide interventional image\naugmentation for treatment of unilateral fractures in patients with traumatic\ninjuries. The mathematical basis of our solution is on the incorporation of\nattributes and characteristics that satisfy the properties of intrinsic and\nextrinsic symmetry and are robust to outliers. In the first step, feature\npoints that satisfy intrinsic symmetry are automatically detected in the\nM\\\"obius space defined on the CT data. These features are then pruned via a\ntwo-stage RANSAC to attain correspondences that satisfy also the extrinsic\nsymmetry. Then, a disparity function based on Tukey's biweight robust estimator\nis introduced and minimized to identify a symmetry plane parametrization that\nyields maximum contralateral similarity. Finally, a novel regularization term\nis introduced to enhance similarity between bone density histograms across the\npartial symmetry plane, relying on the important biological observation that,\neven if injured, the dislocated bone segments remain within the body. Our\nextensive evaluations on various cases of common fracture types demonstrate the\nvalidity of the novel concepts and the robustness and accuracy of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 19:08:55 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 00:23:51 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fotouhi", "Javad", ""], ["Taylor", "Giacomo", ""], ["Unberath", "Mathias", ""], ["Johnson", "Alex", ""], ["Lee", "Sing Chun", ""], ["Osgood", "Greg", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""]]}, {"id": "2003.02314", "submitter": "Masood Seyed Mortazavi", "authors": "Masood S. Mortazavi and Ning Yan", "title": "The Impact of Hole Geometry on Relative Robustness of In-Painting\n  Networks: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In-painting networks use existing pixels to generate appropriate pixels to\nfill \"holes\" placed on parts of an image. A 2-D in-painting network's input\nusually consists of (1) a three-channel 2-D image, and (2) an additional\nchannel for the \"holes\" to be in-painted in that image. In this paper, we study\nthe robustness of a given in-painting neural network against variations in hole\ngeometry distributions. We observe that the robustness of an in-painting\nnetwork is dependent on the probability distribution function (PDF) of the hole\ngeometry presented to it during its training even if the underlying image\ndataset used (in training and testing) does not alter. We develop an\nexperimental methodology for testing and evaluating relative robustness of\nin-painting networks against four different kinds of hole geometry PDFs. We\nexamine a number of hypothesis regarding (1) the natural bias of in-painting\nnetworks to the hole distribution used for their training, (2) the underlying\ndataset's ability to differentiate relative robustness as hole distributions\nvary in a train-test (cross-comparison) grid, and (3) the impact of the\ndirectional distribution of edges in the holes and in the image dataset. We\npresent results for L1, PSNR and SSIM quality metrics and develop a specific\nmeasure of relative in-painting robustness to be used in cross-comparison grids\nbased on these quality metrics. (One can incorporate other quality metrics in\nthis relative measure.) The empirical work reported here is an initial step in\na broader and deeper investigation of \"filling the blank\" neural networks'\nsensitivity, robustness and regularization with respect to hole \"geometry\"\nPDFs, and it suggests further research in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:14:14 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Mortazavi", "Masood S.", ""], ["Yan", "Ning", ""]]}, {"id": "2003.02327", "submitter": "Yimeng Li", "authors": "Yimeng Li, Jana Kosecka", "title": "Learning View and Target Invariant Visual Servoing for Navigation", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advances in deep reinforcement learning recently revived interest in\ndata-driven learning based approaches to navigation. In this paper we propose\nto learn viewpoint invariant and target invariant visual servoing for local\nmobile robot navigation; given an initial view and the goal view or an image of\na target, we train deep convolutional network controller to reach the desired\ngoal. We present a new architecture for this task which rests on the ability of\nestablishing correspondences between the initial and goal view and novel reward\nstructure motivated by the traditional feedback control error. The advantage of\nthe proposed model is that it does not require calibration and depth\ninformation and achieves robust visual servoing in a variety of environments\nand targets without any parameter fine tuning. We present comprehensive\nevaluation of the approach and comparison with other deep learning\narchitectures as well as classical visual servoing methods in visually\nrealistic simulation environment. The presented model overcomes the brittleness\nof classical visual servoing based methods and achieves significantly higher\ngeneralization capability compared to the previous learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:36:43 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Li", "Yimeng", ""], ["Kosecka", "Jana", ""]]}, {"id": "2003.02365", "submitter": "David Berthelot", "authors": "David Berthelot, Peyman Milanfar, Ian Goodfellow", "title": "Creating High Resolution Images with a Latent Adversarial Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic images is difficult, and many formulations for this task\nhave been proposed recently. If we restrict the task to that of generating a\nparticular class of images, however, the task becomes more tractable. That is\nto say, instead of generating an arbitrary image as a sample from the manifold\nof natural images, we propose to sample images from a particular \"subspace\" of\nnatural images, directed by a low-resolution image from the same subspace. The\nproblem we address, while close to the formulation of the single-image\nsuper-resolution problem, is in fact rather different. Single image\nsuper-resolution is the task of predicting the image closest to the ground\ntruth from a relatively low resolution image. We propose to produce samples of\nhigh resolution images given extremely small inputs with a new method called\nLatent Adversarial Generator (LAG). In our generative sampling framework, we\nonly use the input (possibly of very low-resolution) to direct what class of\nsamples the network should produce. As such, the output of our algorithm is not\na unique image that relates to the input, but rather a possible se} of related\nimages sampled from the manifold of natural images. Our method learns\nexclusively in the latent space of the adversary using perceptual loss -- it\ndoes not have a pixel loss.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:23:08 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Berthelot", "David", ""], ["Milanfar", "Peyman", ""], ["Goodfellow", "Ian", ""]]}, {"id": "2003.02366", "submitter": "Tongxin Wang", "authors": "Tongxin Wang, Zhengming Ding, Wei Shao, Haixu Tang, Kun Huang", "title": "Towards Fair Cross-Domain Adaptation via Generative Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain Adaptation (DA) targets at adapting a model trained over the\nwell-labeled source domain to the unlabeled target domain lying in different\ndistributions. Existing DA normally assumes the well-labeled source domain is\nclass-wise balanced, which means the size per source class is relatively\nsimilar. However, in real-world applications, labeled samples for some\ncategories in the source domain could be extremely few due to the difficulty of\ndata collection and annotation, which leads to decreasing performance over\ntarget domain on those few-shot categories. To perform fair cross-domain\nadaptation and boost the performance on these minority categories, we develop a\nnovel Generative Few-shot Cross-domain Adaptation (GFCA) algorithm for fair\ncross-domain classification. Specifically, generative feature augmentation is\nexplored to synthesize effective training data for few-shot source classes,\nwhile effective cross-domain alignment aims to adapt knowledge from source to\nfacilitate the target learning. Experimental results on two large cross-domain\nvisual datasets demonstrate the effectiveness of our proposed method on\nimproving both few-shot and overall classification accuracy comparing with the\nstate-of-the-art DA approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:25:09 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 18:44:21 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Tongxin", ""], ["Ding", "Zhengming", ""], ["Shao", "Wei", ""], ["Tang", "Haixu", ""], ["Huang", "Kun", ""]]}, {"id": "2003.02371", "submitter": "Jens Behley", "authors": "Jens Behley and Andres Milioto and Cyrill Stachniss", "title": "A Benchmark for LiDAR-based Panoptic Segmentation based on KITTI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation is the recently introduced task that tackles semantic\nsegmentation and instance segmentation jointly. In this paper, we present an\nextension of SemanticKITTI, which is a large-scale dataset providing dense\npoint-wise semantic labels for all sequences of the KITTI Odometry Benchmark,\nfor training and evaluation of laser-based panoptic segmentation. We provide\nthe data and discuss the processing steps needed to enrich a given semantic\nannotation with temporally consistent instance information, i.e., instance\ninformation that supplements the semantic labels and identifies the same\ninstance over sequences of LiDAR point clouds. Additionally, we present two\nstrong baselines that combine state-of-the-art LiDAR-based semantic\nsegmentation approaches with a state-of-the-art detector enriching the\nsegmentation with instance information and that allow other researchers to\ncompare their approaches against. We hope that our extension of SemanticKITTI\nwith strong baselines enables the creation of novel algorithms for LiDAR-based\npanoptic segmentation as much as it has for the original semantic segmentation\nand semantic scene completion tasks. Data, code, and an online evaluation using\na hidden test set will be published on http://semantic-kitti.org.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:44:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Behley", "Jens", ""], ["Milioto", "Andres", ""], ["Stachniss", "Cyrill", ""]]}, {"id": "2003.02425", "submitter": "Chengxi Li", "authors": "Chengxi Li, Stanley H. Chan and Yi-Ting Chen", "title": "Who Make Drivers Stop? Towards Driver-centric Risk Assessment: Risk\n  Object Identification via Causal Inference", "comments": "Accepted to the International Conference on Intelligent Robots and\n  Systems (IROS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of people die in road accidents due to driver errors. To\nreduce fatalities, developing intelligent driving systems assisting drivers to\nidentify potential risks is in an urgent need. Risky situations are generally\ndefined based on collision prediction in the existing works. However, collision\nis only a source of potential risks, and a more generic definition is required.\nIn this work, we propose a novel driver-centric definition of risk, i.e.,\nobjects influencing drivers' behavior are risky. A new task called risk object\nidentification is introduced. We formulate the task as the cause-effect problem\nand present a novel two-stage risk object identification framework based on\ncausal inference with the proposed object-level manipulable driving model. We\ndemonstrate favorable performance on risk object identification compared with\nstrong baselines on the Honda Research Institute Driving Dataset (HDD). Our\nframework achieves a substantial average performance boost over a strong\nbaseline by 7.5%.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 04:14:35 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 04:28:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Chengxi", ""], ["Chan", "Stanley H.", ""], ["Chen", "Yi-Ting", ""]]}, {"id": "2003.02427", "submitter": "Felix Wolf Hans Erich Von Drigalski", "authors": "Felix von Drigalski, Chisato Nakashima, Yoshiya Shibata, Yoshinori\n  Konishi, Joshua C. Triyonoputro, Kaidi Nie, Damien Petit, Toshio Ueshiba,\n  Ryuichi Takase, Yukiyasu Domae, Taku Yoshioka, Yoshihisa Ijiri, Ixchel G.\n  Ramirez-Alpizar, Weiwei Wan and Kensuke Harada", "title": "Team O2AS at the World Robot Summit 2018: An Approach to Robotic Kitting\n  and Assembly Tasks using General Purpose Grippers and Tools", "comments": null, "journal-ref": null, "doi": "10.1080/01691864.2020.1734481", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a versatile robotic system for kitting and assembly tasks which\nuses no jigs or commercial tool changers. Instead of specialized end effectors,\nit uses its two-finger grippers to grasp and hold tools to perform subtasks\nsuch as screwing and suctioning. A third gripper is used as a precision picking\nand centering tool, and uses in-built passive compliance to compensate for\nsmall position errors and uncertainty. A novel grasp point detection for bin\npicking is described for the kitting task, using a single depth map. Using the\nproposed system we competed in the Assembly Challenge of the Industrial\nRobotics Category of the World Robot Challenge at the World Robot Summit 2018,\nobtaining 4th place and the SICE award for lean design and versatile tool use.\nWe show the effectiveness of our approach through experiments performed during\nthe competition.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 04:34:09 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["von Drigalski", "Felix", ""], ["Nakashima", "Chisato", ""], ["Shibata", "Yoshiya", ""], ["Konishi", "Yoshinori", ""], ["Triyonoputro", "Joshua C.", ""], ["Nie", "Kaidi", ""], ["Petit", "Damien", ""], ["Ueshiba", "Toshio", ""], ["Takase", "Ryuichi", ""], ["Domae", "Yukiyasu", ""], ["Yoshioka", "Taku", ""], ["Ijiri", "Yoshihisa", ""], ["Ramirez-Alpizar", "Ixchel G.", ""], ["Wan", "Weiwei", ""], ["Harada", "Kensuke", ""]]}, {"id": "2003.02437", "submitter": "Pengfei Zhu", "authors": "Pengfei Zhu, Yiming Sun, Longyin Wen, Yu Feng, Qinghua Hu", "title": "Drone Based RGBT Vehicle Detection and Counting: A Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera-equipped drones can capture targets on the ground from a wider field\nof view than static cameras or moving sensors over the ground. In this paper we\npresent a large-scale vehicle detection and counting benchmark, named\nDroneVehicle, aiming at advancing visual analysis tasks on the drone platform.\nThe images in the benchmark were captured over various urban areas, which\ninclude different types of urban roads, residential areas, parking lots,\nhighways, etc., from day to night. Specifically, DroneVehicle consists of\n15,532 pairs of images, i.e., RGB images and infrared images with rich\nannotations, including oriented object bounding boxes, object categories, etc.\nWith intensive amount of effort, our benchmark has 441,642 annotated instances\nin 31,064 images. As a large-scale dataset with both RGB and thermal infrared\n(RGBT) images, the benchmark enables extensive evaluation and investigation of\nvisual analysis algorithms on the drone platform. In particular, we design two\npopular tasks with the benchmark, including object detection and object\ncounting. All these tasks are extremely challenging in the proposed dataset due\nto factors such as illumination, occlusion, and scale variations. We hope the\nbenchmark largely boost the research and development in visual analysis on\ndrone platforms. The DroneVehicle dataset can be download from\nhttps://github.com/VisDrone/DroneVehicle.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 05:29:44 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Zhu", "Pengfei", ""], ["Sun", "Yiming", ""], ["Wen", "Longyin", ""], ["Feng", "Yu", ""], ["Hu", "Qinghua", ""]]}, {"id": "2003.02438", "submitter": "Mohit Lamba", "authors": "Mohit Lamba, Kranthi Kumar, Kaushik Mitra", "title": "Harnessing Multi-View Perspective of Light Fields for Low-Light Imaging", "comments": "Visit our project page at https://mohitlamba94.github.io/L3Fnet/ to\n  download the dataset and the code", "journal-ref": null, "doi": "10.1109/TIP.2020.3045617", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light Field (LF) offers unique advantages such as post-capture refocusing and\ndepth estimation, but low-light conditions limit these capabilities. To restore\nlow-light LFs we should harness the geometric cues present in different LF\nviews, which is not possible using single-frame low-light enhancement\ntechniques. We, therefore, propose a deep neural network for Low-Light Light\nField (L3F) restoration, which we refer to as L3Fnet. The proposed L3Fnet not\nonly performs the necessary visual enhancement of each LF view but also\npreserves the epipolar geometry across views. We achieve this by adopting a\ntwo-stage architecture for L3Fnet. Stage-I looks at all the LF views to encode\nthe LF geometry. This encoded information is then used in Stage-II to\nreconstruct each LF view. To facilitate learning-based techniques for low-light\nLF imaging, we collected a comprehensive LF dataset of various scenes. For each\nscene, we captured four LFs, one with near-optimal exposure and ISO settings\nand the others at different levels of low-light conditions varying from low to\nextreme low-light settings. The effectiveness of the proposed L3Fnet is\nsupported by both visual and numerical comparisons on this dataset. To further\nanalyze the performance of low-light reconstruction methods, we also propose an\nL3F-wild dataset that contains LF captured late at night with almost zero lux\nvalues. No ground truth is available in this dataset. To perform well on the\nL3F-wild dataset, any method must adapt to the light level of the captured\nscene. To do this we propose a novel pre-processing block that makes L3Fnet\nrobust to various degrees of low-light conditions. Lastly, we show that L3Fnet\ncan also be used for low-light enhancement of single-frame images, despite it\nbeing engineered for LF data. We do so by converting the single-frame DSLR\nimage into a form suitable to L3Fnet, which we call as pseudo-LF.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 05:32:44 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 11:16:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Lamba", "Mohit", ""], ["Kumar", "Kranthi", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2003.02445", "submitter": "Jae Kyu Suhr", "authors": "Jae Kyu Suhr and Ho Gi Jung", "title": "End-to-End Trainable One-Stage Parking Slot Detection Integrating Global\n  and Local Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an end-to-end trainable one-stage parking slot detection\nmethod for around view monitor (AVM) images. The proposed method simultaneously\nacquires global information (entrance, type, and occupancy of parking slot) and\nlocal information (location and orientation of junction) by using a\nconvolutional neural network (CNN), and integrates them to detect parking slots\nwith their properties. This method divides an AVM image into a grid and\nperforms a CNN-based feature extraction. For each cell of the grid, the global\nand local information of the parking slot is obtained by applying convolution\nfilters to the extracted feature map. Final detection results are produced by\nintegrating the global and local information of the parking slot through\nnon-maximum suppression (NMS). Since the proposed method obtains most of the\ninformation of the parking slot using a fully convolutional network without a\nregion proposal stage, it is an end-to-end trainable one-stage detector. In\nexperiments, this method was quantitatively evaluated using the public dataset\nand outperforms previous methods by showing both recall and precision of\n99.77%, type classification accuracy of 100%, and occupancy classification\naccuracy of 99.31% while processing 60 frames per second.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 05:57:20 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Suhr", "Jae Kyu", ""], ["Jung", "Ho Gi", ""]]}, {"id": "2003.02449", "submitter": "Chinthaka Gamanayake", "authors": "Chinthaka Gamanayake, Lahiru Jayasinghe, Benny Ng, Chau Yuen", "title": "Cluster Pruning: An Efficient Filter Pruning Method for Edge AI Vision\n  Applications", "comments": null, "journal-ref": "J-STSP-CDNN-00206-2019", "doi": "10.1109/JSTSP.2020.2971418", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the Convolutional Neural Networks (CNN) has shown superior\nresults in the field of computer vision, it is still a challenging task to\nimplement computer vision algorithms in real-time at the edge, especially using\na low-cost IoT device due to high memory consumption and computation\ncomplexities in a CNN. Network compression methodologies such as weight\npruning, filter pruning, and quantization are used to overcome the above\nmentioned problem. Even though filter pruning methodology has shown better\nperformances compared to other techniques, irregularity of the number of\nfilters pruned across different layers of a CNN might not comply with majority\nof the neural computing hardware architectures. In this paper, a novel greedy\napproach called cluster pruning has been proposed, which provides a structured\nway of removing filters in a CNN by considering the importance of filters and\nthe underlying hardware architecture. The proposed methodology is compared with\nthe conventional filter pruning algorithm on Pascal-VOC open dataset, and\nHead-Counting dataset, which is our own dataset developed to detect and count\npeople entering a room. We benchmark our proposed method on three hardware\narchitectures, namely CPU, GPU, and Intel Movidius Neural Computer Stick (NCS)\nusing the popular SSD-MobileNet and SSD-SqueezeNet neural network architectures\nused for edge-AI vision applications. Results demonstrate that our method\noutperforms the conventional filter pruning methodology, using both datasets on\nabove mentioned hardware architectures. Furthermore, a low cost IoT hardware\nsetup consisting of an Intel Movidius-NCS is proposed to deploy an edge-AI\napplication using our proposed pruning methodology.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 06:20:09 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Gamanayake", "Chinthaka", ""], ["Jayasinghe", "Lahiru", ""], ["Ng", "Benny", ""], ["Yuen", "Chau", ""]]}, {"id": "2003.02467", "submitter": "Yong Bai", "authors": "Yong Bai, Yuanfang Guo, Jinjie Wei, Lin Lu, Rui Wang, and Yunhong Wang", "title": "Fake Generated Painting Detection via Frequency Analysis", "comments": "5 pages, 6 figures, accepted by ICIP 2020", "journal-ref": null, "doi": "10.1109/ICIP40778.2020.9190892", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of deep neural networks, digital fake paintings can be\ngenerated by various style transfer algorithms.To detect the fake generated\npaintings, we analyze the fake generated and real paintings in Fourier\nfrequency domain and observe statistical differences and artifacts. Based on\nour observations, we propose Fake Generated Painting Detection via Frequency\nAnalysis (FGPD-FA) by extracting three types of features in frequency domain.\nBesides, we also propose a digital fake painting detection database for\nassessing the proposed method. Experimental results demonstrate the excellence\nof the proposed method in different testing conditions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 07:33:07 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 08:17:37 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bai", "Yong", ""], ["Guo", "Yuanfang", ""], ["Wei", "Jinjie", ""], ["Lu", "Lin", ""], ["Wang", "Rui", ""], ["Wang", "Yunhong", ""]]}, {"id": "2003.02469", "submitter": "Frank Nielsen", "authors": "Frank Nielsen and Richard Nock", "title": "Cumulant-free closed-form formulas for some common (dis)similarities\n  between densities of an exponential family", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.IT cs.LG math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the Bhattacharyya, Hellinger, Kullback-Leibler,\n$\\alpha$-divergences, and Jeffreys' divergences between densities belonging to\na same exponential family have generic closed-form formulas relying on the\nstrictly convex and real-analytic cumulant function characterizing the\nexponential family. In this work, we report (dis)similarity formulas which\nbypass the explicit use of the cumulant function and highlight the role of\nquasi-arithmetic means and their multivariate mean operator extensions. In\npractice, these cumulant-free formulas are handy when implementing these\n(dis)similarities using legacy Application Programming Interfaces (APIs) since\nour method requires only to partially factorize the densities canonically of\nthe considered exponential family.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 07:46:22 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 01:01:42 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 04:11:00 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Nielsen", "Frank", ""], ["Nock", "Richard", ""]]}, {"id": "2003.02484", "submitter": "Saehyung Lee", "authors": "Saehyung Lee, Hyungyu Lee, Sungroh Yoon", "title": "Adversarial Vertex Mixup: Toward Better Adversarially Robust\n  Generalization", "comments": "To appear in CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples cause neural networks to produce incorrect outputs with\nhigh confidence. Although adversarial training is one of the most effective\nforms of defense against adversarial examples, unfortunately, a large gap\nexists between test accuracy and training accuracy in adversarial training. In\nthis paper, we identify Adversarial Feature Overfitting (AFO), which may cause\npoor adversarially robust generalization, and we show that adversarial training\ncan overshoot the optimal point in terms of robust generalization, leading to\nAFO in our simple Gaussian model. Considering these theoretical results, we\npresent soft labeling as a solution to the AFO problem. Furthermore, we propose\nAdversarial Vertex mixup (AVmixup), a soft-labeled data augmentation approach\nfor improving adversarially robust generalization. We complement our\ntheoretical analysis with experiments on CIFAR10, CIFAR100, SVHN, and Tiny\nImageNet, and show that AVmixup significantly improves the robust\ngeneralization performance and that it reduces the trade-off between standard\naccuracy and adversarial robustness.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 08:47:46 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 04:40:09 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 12:26:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Lee", "Saehyung", ""], ["Lee", "Hyungyu", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2003.02488", "submitter": "Pawel Drozdowski", "authors": "P. Drozdowski, C. Rathgeb, A. Dantcheva, N. Damer, C. Busch", "title": "Demographic Bias in Biometrics: A Survey on an Emerging Challenge", "comments": "15 pages, 3 figures, 3 tables. Submitted to IEEE Transactions on\n  Technology and Society. Update after first round of peer review", "journal-ref": "IEEE Transactions on Technology and Society 1, no. 2 (2020):\n  89-103", "doi": "10.1109/TTS.2020.2992344", "report-no": null, "categories": "cs.CY cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems incorporating biometric technologies have become ubiquitous in\npersonal, commercial, and governmental identity management applications. Both\ncooperative (e.g. access control) and non-cooperative (e.g. surveillance and\nforensics) systems have benefited from biometrics. Such systems rely on the\nuniqueness of certain biological or behavioural characteristics of human\nbeings, which enable for individuals to be reliably recognised using automated\nalgorithms.\n  Recently, however, there has been a wave of public and academic concerns\nregarding the existence of systemic bias in automated decision systems\n(including biometrics). Most prominently, face recognition algorithms have\noften been labelled as \"racist\" or \"biased\" by the media, non-governmental\norganisations, and researchers alike.\n  The main contributions of this article are: (1) an overview of the topic of\nalgorithmic bias in the context of biometrics, (2) a comprehensive survey of\nthe existing literature on biometric bias estimation and mitigation, (3) a\ndiscussion of the pertinent technical and social matters, and (4) an outline of\nthe remaining challenges and future work items, both from technological and\nsocial points of view.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 09:07:59 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 08:18:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Drozdowski", "P.", ""], ["Rathgeb", "C.", ""], ["Dantcheva", "A.", ""], ["Damer", "N.", ""], ["Busch", "C.", ""]]}, {"id": "2003.02501", "submitter": "Eunji Chong", "authors": "Eunji Chong, Yongxin Wang, Nataniel Ruiz, and James M. Rehg", "title": "Detecting Attended Visual Targets in Video", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of detecting attention targets in video. Our goal is\nto identify where each person in each frame of a video is looking, and\ncorrectly handle the case where the gaze target is out-of-frame. Our novel\narchitecture models the dynamic interaction between the scene and head features\nand infers time-varying attention targets. We introduce a new annotated\ndataset, VideoAttentionTarget, containing complex and dynamic patterns of\nreal-world gaze behavior. Our experiments show that our model can effectively\ninfer dynamic attention in videos. In addition, we apply our predicted\nattention maps to two social gaze behavior recognition tasks, and show that the\nresulting classifiers significantly outperform existing methods. We achieve\nstate-of-the-art performance on three datasets: GazeFollow (static images),\nVideoAttentionTarget (videos), and VideoCoAtt (videos), and obtain the first\nresults for automatically classifying clinically-relevant gaze behavior without\nwearable cameras or eye trackers.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 09:29:48 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 23:38:48 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chong", "Eunji", ""], ["Wang", "Yongxin", ""], ["Ruiz", "Nataniel", ""], ["Rehg", "James M.", ""]]}, {"id": "2003.02541", "submitter": "Jian Liang", "authors": "Jian Liang, Yunbo Wang, Dapeng Hu, Ran He, and Jiashi Feng", "title": "A Balanced and Uncertainty-aware Approach for Partial Domain Adaptation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the unsupervised domain adaptation problem, especially in\nthe case of class labels in the target domain being only a subset of those in\nthe source domain. Such a partial transfer setting is realistic but challenging\nand existing methods always suffer from two key problems, negative transfer and\nuncertainty propagation. In this paper, we build on domain adversarial learning\nand propose a novel domain adaptation method BA$^3$US with two new techniques\ntermed Balanced Adversarial Alignment (BAA) and Adaptive Uncertainty\nSuppression (AUS), respectively. On one hand, negative transfer results in\nmisclassification of target samples to the classes only present in the source\ndomain. To address this issue, BAA pursues the balance between label\ndistributions across domains in a fairly simple manner. Specifically, it\nrandomly leverages a few source samples to augment the smaller target domain\nduring domain alignment so that classes in different domains are symmetric. On\nthe other hand, a source sample would be denoted as uncertain if there is an\nincorrect class that has a relatively high prediction score, and such\nuncertainty easily propagates to unlabeled target data around it during\nalignment, which severely deteriorates adaptation performance. Thus we present\nAUS that emphasizes uncertain samples and exploits an adaptive weighted\ncomplement entropy objective to encourage incorrect classes to have uniform and\nlow prediction scores. Experimental results on multiple benchmarks demonstrate\nour BA$^3$US surpasses state-of-the-arts for partial domain adaptation tasks.\nCode is available at \\url{https://github.com/tim-learn/BA3US}.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 11:37:06 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:04:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Liang", "Jian", ""], ["Wang", "Yunbo", ""], ["Hu", "Dapeng", ""], ["He", "Ran", ""], ["Feng", "Jiashi", ""]]}, {"id": "2003.02546", "submitter": "ByungSoo Ko", "authors": "Byungsoo Ko, Geonmo Gu", "title": "Embedding Expansion: Augmentation in Embedding Space for Deep Metric\n  Learning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the distance metric between pairs of samples has been studied for\nimage retrieval and clustering. With the remarkable success of pair-based\nmetric learning losses, recent works have proposed the use of generated\nsynthetic points on metric learning losses for augmentation and generalization.\nHowever, these methods require additional generative networks along with the\nmain network, which can lead to a larger model size, slower training speed, and\nharder optimization. Meanwhile, post-processing techniques, such as query\nexpansion and database augmentation, have proposed the combination of feature\npoints to obtain additional semantic information. In this paper, inspired by\nquery expansion and database augmentation, we propose an augmentation method in\nan embedding space for pair-based metric learning losses, called embedding\nexpansion. The proposed method generates synthetic points containing augmented\ninformation by a combination of feature points and performs hard negative pair\nmining to learn with the most informative feature representations. Because of\nits simplicity and flexibility, it can be used for existing metric learning\nlosses without affecting model size, training speed, or optimization\ndifficulty. Finally, the combination of embedding expansion and representative\nmetric learning losses outperforms the state-of-the-art losses and previous\nsample generation methods in both image retrieval and clustering tasks. The\nimplementation is publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 11:43:17 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 08:45:38 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 06:13:11 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ko", "Byungsoo", ""], ["Gu", "Geonmo", ""]]}, {"id": "2003.02567", "submitter": "Lei Kang", "authors": "Lei Kang, Pau Riba, Yaxing Wang, Mar\\c{c}al Rusi\\~nol, Alicia Forn\\'es\n  and Mauricio Villegas", "title": "GANwriting: Content-Conditioned Generation of Styled Handwritten Word\n  Images", "comments": "Accepted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Although current image generation methods have reached impressive quality\nlevels, they are still unable to produce plausible yet diverse images of\nhandwritten words. On the contrary, when writing by hand, a great variability\nis observed across different writers, and even when analyzing words scribbled\nby the same individual, involuntary variations are conspicuous. In this work,\nwe take a step closer to producing realistic and varied artificially rendered\nhandwritten words. We propose a novel method that is able to produce credible\nhandwritten word images by conditioning the generative process with both\ncalligraphic style features and textual content. Our generator is guided by\nthree complementary learning objectives: to produce realistic images, to\nimitate a certain handwriting style and to convey a specific textual content.\nOur model is unconstrained to any predefined vocabulary, being able to render\nwhatever input word. Given a sample writer, it is also able to mimic its\ncalligraphic features in a few-shot setup. We significantly advance over prior\nart and demonstrate with qualitative, quantitative and human-based evaluations\nthe realistic aspect of our synthetically produced images.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 12:37:29 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 19:40:15 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kang", "Lei", ""], ["Riba", "Pau", ""], ["Wang", "Yaxing", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Forn\u00e9s", "Alicia", ""], ["Villegas", "Mauricio", ""]]}, {"id": "2003.02586", "submitter": "David Svitov", "authors": "David Svitov and Sergey Alyamkin", "title": "MarginDistillation: distillation for margin-based softmax", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of convolutional neural networks (CNNs) in conjunction with a\nmargin-based softmax approach demonstrates a state-of-the-art performance for\nthe face recognition problem. Recently, lightweight neural network models\ntrained with the margin-based softmax have been introduced for the face\nidentification task for edge devices. In this paper, we propose a novel\ndistillation method for lightweight neural network architectures that\noutperforms other known methods for the face recognition task on LFW, AgeDB-30\nand Megaface datasets. The idea of the proposed method is to use class centers\nfrom the teacher network for the student network. Then the student network is\ntrained to get the same angles between the class centers and the face\nembeddings, predicted by the teacher network.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 13:03:23 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Svitov", "David", ""], ["Alyamkin", "Sergey", ""]]}, {"id": "2003.02597", "submitter": "Tri Cong Pham", "authors": "Cong Tri Pham, Mai Chi Luong, Dung Van Hoang, Antoine Doucet", "title": "AI outperformed every dermatologist: Improved dermoscopic melanoma\n  diagnosis through customizing batch logic and loss function in an optimized\n  Deep CNN architecture", "comments": "We are submitting the article in the journal and waiting for the\n  review result, so we want to temporarily delete the article. When the article\n  is officially accepted, it will be resubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma, one of most dangerous types of skin cancer, re-sults in a very high\nmortality rate. Early detection and resection are two key points for a\nsuccessful cure. Recent research has used artificial intelligence to classify\nmelanoma and nevus and to compare the assessment of these algorithms to that of\ndermatologists. However, an imbalance of sensitivity and specificity measures\naffected the performance of existing models. This study proposes a method using\ndeep convolutional neural networks aiming to detect melanoma as a binary\nclassification problem. It involves 3 key features, namely customized batch\nlogic, customized loss function and reformed fully connected layers. The\ntraining dataset is kept up to date including 17,302 images of melanoma and\nnevus; this is the largest dataset by far. The model performance is compared to\nthat of 157 dermatologists from 12 university hospitals in Germany based on\nMClass-D dataset. The model outperformed all 157 dermatologists and achieved\nstate-of-the-art performance with AUC at 94.4% with sensitivity of 85.0% and\nspecificity of 95.0% using a prediction threshold of 0.5 on the MClass-D\ndataset of 100 dermoscopic images. Moreover, a threshold of 0.40858 showed the\nmost balanced measure compared to other researches, and is promisingly\napplication to medical diagnosis, with sensitivity of 90.0% and specificity of\n93.8%.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 13:19:13 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 17:11:08 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Pham", "Cong Tri", ""], ["Luong", "Mai Chi", ""], ["Van Hoang", "Dung", ""], ["Doucet", "Antoine", ""]]}, {"id": "2003.02640", "submitter": "Carmelo Sferrazza", "authors": "Carmelo Sferrazza, Thomas Bi and Raffaello D'Andrea", "title": "Learning the sense of touch in simulation: a sim-to-real strategy for\n  vision-based tactile sensing", "comments": "This work has been submitted to the 2020 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) for possible publication.\n  Accompanying video: https://youtu.be/dDTga9PgWS0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches to tactile sensing aim to overcome the complexity of\naccurately modeling contact with soft materials. However, their widespread\nadoption is impaired by concerns about data efficiency and the capability to\ngeneralize when applied to various tasks. This paper focuses on both these\naspects with regard to a vision-based tactile sensor, which aims to reconstruct\nthe distribution of the three-dimensional contact forces applied on its soft\nsurface. Accurate models for the soft materials and the camera projection,\nderived via state-of-the-art techniques in the respective domains, are employed\nto generate a dataset in simulation. A strategy is proposed to train a tailored\ndeep neural network entirely from the simulation data. The resulting learning\narchitecture is directly transferable across multiple tactile sensors without\nfurther training and yields accurate predictions on real data, while showing\npromising generalization capabilities to unseen contact conditions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:17:45 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Sferrazza", "Carmelo", ""], ["Bi", "Thomas", ""], ["D'Andrea", "Raffaello", ""]]}, {"id": "2003.02641", "submitter": "Yuri Gloumakov", "authors": "Yuri Gloumakov, Adam J. Spiers, Aaron M. Dollar", "title": "Dimensionality Reduction and Motion Clustering during Activities of\n  Daily Living: 3, 4, and 7 Degree-of-Freedom Arm Movements", "comments": "11 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide variety of motions performed by the human arm during daily tasks\nmakes it desirable to find representative subsets to reduce the dimensionality\nof these movements for a variety of applications, including the design and\ncontrol of robotic and prosthetic devices. This paper presents a novel method\nand the results of an extensive human subjects study to obtain representative\narm joint angle trajectories that span naturalistic motions during Activities\nof Daily Living (ADLs). In particular, we seek to identify sets of useful\nmotion trajectories of the upper limb that are functions of a single variable,\nallowing, for instance, an entire prosthetic or robotic arm to be controlled\nwith a single input from a user, along with a means to select between motions\nfor different tasks. Data driven approaches are used to obtain clusters as well\nas representative motion averages for the full-arm 7 degree of freedom (DOF),\nelbow-wrist 4 DOF, and wrist-only 3 DOF motions. The proposed method makes use\nof well-known techniques such as dynamic time warping (DTW) to obtain a\ndivergence measure between motion segments, DTW barycenter averaging (DBA) to\nobtain averages, Ward's distance criterion to build hierarchical trees,\nbatch-DTW to simultaneously align multiple motion data, and functional\nprincipal component analysis (fPCA) to evaluate cluster variability. The\nclusters that emerge associate various recorded motions into primarily hand\nstart and end location for the full-arm system, motion direction for the\nwrist-only system, and an intermediate between the two qualities for the\nelbow-wrist system. The proposed clustering methodology is justified by\ncomparing results against alternative approaches.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 04:32:36 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Gloumakov", "Yuri", ""], ["Spiers", "Adam J.", ""], ["Dollar", "Aaron M.", ""]]}, {"id": "2003.02683", "submitter": "Qi Liu", "authors": "Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, Changqing\n  Zou", "title": "SketchyCOCO: Image Generation from Freehand Scene Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first method for automatic image generation from scene-level\nfreehand sketches. Our model allows for controllable image generation by\nspecifying the synthesis goal via freehand sketches. The key contribution is an\nattribute vector bridged Generative Adversarial Network called EdgeGAN, which\nsupports high visual-quality object-level image content generation without\nusing freehand sketches as training data. We have built a large-scale composite\ndataset called SketchyCOCO to support and evaluate the solution. We validate\nour approach on the tasks of both object-level and scene-level image generation\non SketchyCOCO. Through quantitative, qualitative results, human evaluation and\nablation studies, we demonstrate the method's capacity to generate realistic\ncomplex scene-level images from various freehand sketches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 14:54:10 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 08:17:42 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 07:18:49 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2020 08:22:09 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2020 10:15:39 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Gao", "Chengying", ""], ["Liu", "Qi", ""], ["Xu", "Qi", ""], ["Wang", "Limin", ""], ["Liu", "Jianzhuang", ""], ["Zou", "Changqing", ""]]}, {"id": "2003.02692", "submitter": "Hyeon Cho", "authors": "Hyeon Cho, Taehoon Kim, Hyung Jin Chang, Wonjun Hwang", "title": "Self-Supervised Visual Learning by Variable Playback Speeds Prediction\n  of a Video", "comments": "Accepted by IEEE Access on May 19, 2021", "journal-ref": null, "doi": "10.1109/ACCESS.2021.3084840", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a self-supervised visual learning method by predicting the\nvariable playback speeds of a video. Without semantic labels, we learn the\nspatio-temporal visual representation of the video by leveraging the variations\nin the visual appearance according to different playback speeds under the\nassumption of temporal coherence. To learn the spatio-temporal visual\nvariations in the entire video, we have not only predicted a single playback\nspeed but also generated clips of various playback speeds and directions with\nrandomized starting points. Hence the visual representation can be successfully\nlearned from the meta information (playback speeds and directions) of the\nvideo. We also propose a new layer dependable temporal group normalization\nmethod that can be applied to 3D convolutional networks to improve the\nrepresentation learning performance where we divide the temporal features into\nseveral groups and normalize each one using the different corresponding\nparameters. We validate the effectiveness of our method by fine-tuning it to\nthe action recognition and video retrieval tasks on UCF-101 and HMDB-51.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 15:01:08 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 02:39:54 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cho", "Hyeon", ""], ["Kim", "Taehoon", ""], ["Chang", "Hyung Jin", ""], ["Hwang", "Wonjun", ""]]}, {"id": "2003.02750", "submitter": "Thang Dang Duy", "authors": "Dang Duy Thang and Toshihiro Matsui", "title": "Search Space of Adversarial Perturbations against Image Filters", "comments": null, "journal-ref": "Published in International Journal of Advanced Computer Science\n  and Applications(IJACSA), Volume 11 Issue 1, 2020", "doi": "10.14569/IJACSA.2020.0110102", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superiority of deep learning performance is threatened by safety issues\nfor itself. Recent findings have shown that deep learning systems are very weak\nto adversarial examples, an attack form that was altered by the attacker's\nintent to deceive the deep learning system. There are many proposed defensive\nmethods to protect deep learning systems against adversarial examples. However,\nthere is still a lack of principal strategies to deceive those defensive\nmethods. Any time a particular countermeasure is proposed, a new powerful\nadversarial attack will be invented to deceive that countermeasure. In this\nstudy, we focus on investigating the ability to create adversarial patterns in\nsearch space against defensive methods that use image filters. Experimental\nresults conducted on the ImageNet dataset with image classification tasks\nshowed the correlation between the search space of adversarial perturbation and\nfilters. These findings open a new direction for building stronger offensive\nmethods towards deep learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:40:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Thang", "Dang Duy", ""], ["Matsui", "Toshihiro", ""]]}, {"id": "2003.02752", "submitter": "Hongxin Wei", "authors": "Hongxin Wei, Lei Feng, Xiangyu Chen, Bo An", "title": "Combating noisy labels by agreement: A joint training method with\n  co-regularization", "comments": "Accepted by CVPR 2020; Code is available at:\n  https://github.com/hongxin001/JoCoR. arXiv admin note: text overlap with\n  arXiv:1901.04215 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning with noisy labels is a practically challenging problem in\nweakly supervised learning. The state-of-the-art approaches \"Decoupling\" and\n\"Co-teaching+\" claim that the \"disagreement\" strategy is crucial for\nalleviating the problem of learning with noisy labels. In this paper, we start\nfrom a different perspective and propose a robust learning paradigm called\nJoCoR, which aims to reduce the diversity of two networks during training.\nSpecifically, we first use two networks to make predictions on the same\nmini-batch data and calculate a joint loss with Co-Regularization for each\ntraining example. Then we select small-loss examples to update the parameters\nof both two networks simultaneously. Trained by the joint loss, these two\nnetworks would be more and more similar due to the effect of Co-Regularization.\nExtensive experimental results on corrupted data from benchmark datasets\nincluding MNIST, CIFAR-10, CIFAR-100 and Clothing1M demonstrate that JoCoR is\nsuperior to many state-of-the-art approaches for learning with noisy labels.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:42:41 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 07:40:58 GMT"}, {"version": "v3", "created": "Wed, 22 Apr 2020 17:06:32 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Wei", "Hongxin", ""], ["Feng", "Lei", ""], ["Chen", "Xiangyu", ""], ["An", "Bo", ""]]}, {"id": "2003.02790", "submitter": "Mathias Gehrig", "authors": "Mathias Gehrig, Sumit Bam Shrestha, Daniel Mouritzen and Davide\n  Scaramuzza", "title": "Event-Based Angular Velocity Regression with Spiking Networks", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA),\n  Paris, 2020", "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) are bio-inspired networks that process\ninformation conveyed as temporal spikes rather than numeric values. A spiking\nneuron of an SNN only produces a spike whenever a significant number of spikes\noccur within a short period of time. Due to their spike-based computational\nmodel, SNNs can process output from event-based, asynchronous sensors without\nany pre-processing at extremely lower power unlike standard artificial neural\nnetworks. This is possible due to specialized neuromorphic hardware that\nimplements the highly-parallelizable concept of SNNs in silicon. Yet, SNNs have\nnot enjoyed the same rise of popularity as artificial neural networks. This not\nonly stems from the fact that their input format is rather unconventional but\nalso due to the challenges in training spiking networks. Despite their temporal\nnature and recent algorithmic advances, they have been mostly evaluated on\nclassification problems. We propose, for the first time, a temporal regression\nproblem of numerical values given events from an event camera. We specifically\ninvestigate the prediction of the 3-DOF angular velocity of a rotating event\ncamera with an SNN. The difficulty of this problem arises from the prediction\nof angular velocities continuously in time directly from irregular,\nasynchronous event-based input. Directly utilising the output of event cameras\nwithout any pre-processing ensures that we inherit all the benefits that they\nprovide over conventional cameras. That is high-temporal resolution,\nhigh-dynamic range and no motion blur. To assess the performance of SNNs on\nthis task, we introduce a synthetic event camera dataset generated from\nreal-world panoramic images and show that we can successfully train an SNN to\nperform angular velocity regression.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 17:37:16 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Gehrig", "Mathias", ""], ["Shrestha", "Sumit Bam", ""], ["Mouritzen", "Daniel", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.02795", "submitter": "Tao Hu", "authors": "Tao Hu, Lichao Huang, Han Shen", "title": "Multi-object Tracking via End-to-end Tracklet Searching and Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in multiple object tracking use sequence model to calculate the\nsimilarity score between the detections and the previous tracklets. However,\nthe forced exposure to ground-truth in the training stage leads to the\ntraining-inference discrepancy problem, i.e., exposure bias, where association\nerror could accumulate in the inference and make the trajectories drift. In\nthis paper, we propose a novel method for optimizing tracklet consistency,\nwhich directly takes the prediction errors into account by introducing an\nonline, end-to-end tracklet search training process. Notably, our methods\ndirectly optimize the whole tracklet score instead of pairwise affinity. With\nsequence model as appearance encoders of tracklet, our tracker achieves\nremarkable performance gain from conventional tracklet association baseline.\nOur methods have also achieved state-of-the-art in MOT15~17 challenge\nbenchmarks using public detection and online settings.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:46:01 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Hu", "Tao", ""], ["Huang", "Lichao", ""], ["Shen", "Han", ""]]}, {"id": "2003.02822", "submitter": "Danfeng Hong", "authors": "Behnood Rasti, Danfeng Hong, Renlong Hang, Pedram Ghamisi, Xudong\n  Kang, Jocelyn Chanussot, Jon Atli Benediktsson", "title": "Feature Extraction for Hyperspectral Imagery: The Evolution from Shallow\n  to Deep (Overview and Toolbox)", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Magazine, 2020", "doi": "10.1109/MGRS.2020.2979764", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral images provide detailed spectral information through hundreds\nof (narrow) spectral channels (also known as dimensionality or bands) with\ncontinuous spectral information that can accurately classify diverse materials\nof interest. The increased dimensionality of such data makes it possible to\nsignificantly improve data information content but provides a challenge to the\nconventional techniques (the so-called curse of dimensionality) for accurate\nanalysis of hyperspectral images. Feature extraction, as a vibrant field of\nresearch in the hyperspectral community, evolved through decades of research to\naddress this issue and extract informative features suitable for data\nrepresentation and classification. The advances in feature extraction have been\ninspired by two fields of research, including the popularization of image and\nsignal processing as well as machine (deep) learning, leading to two types of\nfeature extraction approaches named shallow and deep techniques. This article\noutlines the advances in feature extraction approaches for hyperspectral\nimagery by providing a technical overview of the state-of-the-art techniques,\nproviding useful entry points for researchers at different levels, including\nstudents, researchers, and senior researchers, willing to explore novel\ninvestigations on this challenging topic. In more detail, this paper provides a\nbird's eye view over shallow (both supervised and unsupervised) and deep\nfeature extraction approaches specifically dedicated to the topic of\nhyperspectral feature extraction and its application on hyperspectral image\nclassification. Additionally, this paper compares 15 advanced techniques with\nan emphasis on their methodological foundations in terms of classification\naccuracies. Furthermore, the codes and libraries are shared at\nhttps://github.com/BehnoodRasti/HyFTech-Hyperspectral-Shallow-Deep-Feature-Extraction-Toolbox.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:45:22 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 11:40:43 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 10:12:21 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 20:38:54 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Rasti", "Behnood", ""], ["Hong", "Danfeng", ""], ["Hang", "Renlong", ""], ["Ghamisi", "Pedram", ""], ["Kang", "Xudong", ""], ["Chanussot", "Jocelyn", ""], ["Benediktsson", "Jon Atli", ""]]}, {"id": "2003.02824", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib, Zsolt Kira", "title": "Action Segmentation with Joint Self-Supervised Temporal Domain\n  Adaptation", "comments": "CVPR 2020. Source code: https://github.com/cmhungsteve/SSTDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress of fully-supervised action segmentation\ntechniques, the performance is still not fully satisfactory. One main challenge\nis the problem of spatiotemporal variations (e.g. different people may perform\nthe same activity in various ways). Therefore, we exploit unlabeled videos to\naddress this problem by reformulating the action segmentation task as a\ncross-domain problem with domain discrepancy caused by spatio-temporal\nvariations. To reduce the discrepancy, we propose Self-Supervised Temporal\nDomain Adaptation (SSTDA), which contains two self-supervised auxiliary tasks\n(binary and sequential domain prediction) to jointly align cross-domain feature\nspaces embedded with local and global temporal dynamics, achieving better\nperformance than other Domain Adaptation (DA) approaches. On three challenging\nbenchmark datasets (GTEA, 50Salads, and Breakfast), SSTDA outperforms the\ncurrent state-of-the-art method by large margins (e.g. for the F1@25 score,\nfrom 59.6% to 69.1% on Breakfast, from 73.4% to 81.5% on 50Salads, and from\n83.6% to 89.1% on GTEA), and requires only 65% of the labeled training data for\ncomparable performance, demonstrating the usefulness of adapting to unlabeled\ntarget videos across variations. The source code is available at\nhttps://github.com/cmhungsteve/SSTDA.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:52:33 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 17:16:37 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 21:09:07 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Chen", "Min-Hung", ""], ["Li", "Baopu", ""], ["Bao", "Yingze", ""], ["AlRegib", "Ghassan", ""], ["Kira", "Zsolt", ""]]}, {"id": "2003.02874", "submitter": "Zhijing Li", "authors": "Zhijing Li, Christopher De Sa, Adrian Sampson", "title": "Optimizing JPEG Quantization for Classification Networks", "comments": "6 pages, 13 figures, Resource-Constrained Machine Learning (ReCoML)\n  Workshop of MLSys 2020 Conference, Austin, TX, USA, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.PF eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning for computer vision depends on lossy image compression: it\nreduces the storage required for training and test data and lowers transfer\ncosts in deployment. Mainstream datasets and imaging pipelines all rely on\nstandard JPEG compression. In JPEG, the degree of quantization of frequency\ncoefficients controls the lossiness: an 8 by 8 quantization table (Q-table)\ndecides both the quality of the encoded image and the compression ratio. While\na long history of work has sought better Q-tables, existing work either seeks\nto minimize image distortion or to optimize for models of the human visual\nsystem. This work asks whether JPEG Q-tables exist that are \"better\" for\nspecific vision networks and can offer better quality--size trade-offs than\nones designed for human perception or minimal distortion. We reconstruct an\nImageNet test set with higher resolution to explore the effect of JPEG\ncompression under novel Q-tables. We attempt several approaches to tune a\nQ-table for a vision task. We find that a simple sorted random sampling method\ncan exceed the performance of the standard JPEG Q-table. We also use\nhyper-parameter tuning techniques including bounded random search, Bayesian\noptimization, and composite heuristic optimization methods. The new Q-tables we\nobtained can improve the compression rate by 10% to 200% when the accuracy is\nfixed, or improve accuracy up to $2\\%$ at the same compression rate.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 19:13:06 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Li", "Zhijing", ""], ["De Sa", "Christopher", ""], ["Sampson", "Adrian", ""]]}, {"id": "2003.02899", "submitter": "Innar Liiv", "authors": "Priit Ulmas and Innar Liiv", "title": "Segmentation of Satellite Imagery using U-Net Models for Land Cover\n  Classification", "comments": "Submitted to IEEE Access; 11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is using a convolutional machine learning model with\na modified U-Net structure for creating land cover classification mapping based\non satellite imagery. The aim of the research is to train and test\nconvolutional models for automatic land cover mapping and to assess their\nusability in increasing land cover mapping accuracy and change detection. To\nsolve these tasks, authors prepared a dataset and trained machine learning\nmodels for land cover classification and semantic segmentation from satellite\nimages. The results were analysed on three different land classification\nlevels. BigEarthNet satellite image archive was selected for the research as\none of two main datasets. This novel and recent dataset was published in 2019\nand includes Sentinel-2 satellite photos from 10 European countries made in\n2017 and 2018. As a second dataset the authors composed an original set\ncontaining a Sentinel-2 image and a CORINE land cover map of Estonia. The\ndeveloped classification model shows a high overall F\\textsubscript{1} score of\n0.749 on multiclass land cover classification with 43 possible image labels.\nThe model also highlights noisy data in the BigEarthNet dataset, where images\nseem to have incorrect labels. The segmentation models offer a solution for\ngenerating automatic land cover mappings based on Sentinel-2 satellite images\nand show a high IoU score for land cover classes such as forests, inland waters\nand arable land. The models show a capability of increasing the accuracy of\nexisting land classification maps and in land cover change detection.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 20:07:48 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Ulmas", "Priit", ""], ["Liiv", "Innar", ""]]}, {"id": "2003.02909", "submitter": "Jia Yuan Yu", "authors": "Mohammad Akif Beg and Jia Yuan Yu", "title": "Generating Embroidery Patterns Using Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scenarios in computer vision, machine learning, and computer\ngraphics, there is a requirement to learn the mapping from an image of one\ndomain to an image of another domain, called Image-to-image translation. For\nexample, style transfer, object transfiguration, visually altering the\nappearance of weather conditions in an image, changing the appearance of a day\nimage into a night image or vice versa, photo enhancement, to name a few. In\nthis paper, we propose two machine learning techniques to solve the embroidery\nimage-to-image translation. Our goal is to generate a preview image which looks\nsimilar to an embroidered image, from a user-uploaded image. Our techniques are\nmodifications of two existing techniques, neural style transfer, and\ncycle-consistent generative-adversarial network. Neural style transfer renders\nthe semantic content of an image from one domain in the style of a different\nimage in another domain, whereas a cycle-consistent generative adversarial\nnetwork learns the mapping from an input image to output image without any\npaired training data, and also learn a loss function to train this mapping.\nFurthermore, the techniques we propose are independent of any embroidery\nattributes, such as elevation of the image, light-source, start, and endpoints\nof a stitch, type of stitch used, fabric type, etc. Given the user image, our\ntechniques can generate a preview image which looks similar to an embroidered\nimage. We train and test our propose techniques on an embroidery dataset which\nconsist of simple 2D images. To do so, we prepare an unpaired embroidery\ndataset with more than 8000 user-uploaded images along with embroidered images.\nEmpirical results show that these techniques successfully generate an\napproximate preview of an embroidered version of a user image, which can help\nusers in decision making.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 20:32:40 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Beg", "Mohammad Akif", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "2003.02920", "submitter": "Ding Xia", "authors": "Xi Yang, Ding Xia, Taichi Kin, Takeo Igarashi", "title": "IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning", "comments": "Accepted by cvpr2020, camera-ready version will be uploaded later", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medicine is an important application area for deep learning models. Research\nin this field is a combination of medical expertise and data science knowledge.\nIn this paper, instead of 2D medical images, we introduce an open-access 3D\nintracranial aneurysm dataset, IntrA, that makes the application of\npoints-based and mesh-based classification and segmentation models available.\nOur dataset can be used to diagnose intracranial aneurysms and to extract the\nneck for a clipping operation in medicine and other areas of deep learning,\nsuch as normal estimation and surface reconstruction. We provide a large-scale\nbenchmark of classification and part segmentation by testing state-of-the-art\nnetworks. We also discuss the performance of each method and demonstrate the\nchallenges of our dataset. The published dataset can be accessed here:\nhttps://github.com/intra3d2019/IntrA.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:21:53 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 08:09:59 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Yang", "Xi", ""], ["Xia", "Ding", ""], ["Kin", "Taichi", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2003.02943", "submitter": "Antong Chen", "authors": "Antong Chen, Jennifer Saouaf, Bo Zhou, Randolph Crawford, Jianda Yuan,\n  Junshui Ma, Richard Baumgartner, Shubing Wang, Gregory Goldmacher", "title": "A deep learning-facilitated radiomics solution for the prediction of\n  lung lesion shrinkage in non-small cell lung cancer trials", "comments": "Accepted by International Symposium on Biomedical Imaging (ISBI) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein we propose a deep learning-based approach for the prediction of lung\nlesion response based on radiomic features extracted from clinical CT scans of\npatients in non-small cell lung cancer trials. The approach starts with the\nclassification of lung lesions from the set of primary and metastatic lesions\nat various anatomic locations. Focusing on the lung lesions, we perform\nautomatic segmentation to extract their 3D volumes. Radiomic features are then\nextracted from the lesion on the pre-treatment scan and the first follow-up\nscan to predict which lesions will shrink at least 30% in diameter during\ntreatment (either Pembrolizumab or combinations of chemotherapy and\nPembrolizumab), which is defined as a partial response by the Response\nEvaluation Criteria In Solid Tumors (RECIST) guidelines. A 5-fold cross\nvalidation on the training set led to an AUC of 0.84 +/- 0.03, and the\nprediction on the testing dataset reached AUC of 0.73 +/- 0.02 for the outcome\nof 30% diameter shrinkage.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:49:42 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Antong", ""], ["Saouaf", "Jennifer", ""], ["Zhou", "Bo", ""], ["Crawford", "Randolph", ""], ["Yuan", "Jianda", ""], ["Ma", "Junshui", ""], ["Baumgartner", "Richard", ""], ["Wang", "Shubing", ""], ["Goldmacher", "Gregory", ""]]}, {"id": "2003.02953", "submitter": "Istv\\'an S\\'ar\\'andi", "authors": "Istv\\'an S\\'ar\\'andi and Timm Linder and Kai O. Arras and Bastian\n  Leibe", "title": "Metric-Scale Truncation-Robust Heatmaps for 3D Human Pose Estimation", "comments": "Accepted for publication at the 2020 IEEE Conference on Automatic\n  Face and Gesture Recognition (FG)", "journal-ref": null, "doi": "10.1109/FG47880.2020.00108", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heatmap representations have formed the basis of 2D human pose estimation\nsystems for many years, but their generalizations for 3D pose have only\nrecently been considered. This includes 2.5D volumetric heatmaps, whose X and Y\naxes correspond to image space and the Z axis to metric depth around the\nsubject. To obtain metric-scale predictions, these methods must include a\nseparate, explicit post-processing step to resolve scale ambiguity. Further,\nthey cannot encode body joint positions outside of the image boundaries,\nleading to incomplete pose estimates in case of image truncation. We address\nthese limitations by proposing metric-scale truncation-robust (MeTRo)\nvolumetric heatmaps, whose dimensions are defined in metric 3D space near the\nsubject, instead of being aligned with image space. We train a\nfully-convolutional network to estimate such heatmaps from monocular RGB in an\nend-to-end manner. This reinterpretation of the heatmap dimensions allows us to\nestimate complete metric-scale poses without test-time knowledge of the focal\nlength or person distance and without relying on anthropometric heuristics in\npost-processing. Furthermore, as the image space is decoupled from the heatmap\nspace, the network can learn to reason about joints beyond the image boundary.\nUsing ResNet-50 without any additional learned layers, we obtain\nstate-of-the-art results on the Human3.6M and MPI-INF-3DHP benchmarks. As our\nmethod is simple and fast, it can become a useful component for real-time\ntop-down multi-person pose estimation systems. We make our code publicly\navailable to facilitate further research (see\nhttps://vision.rwth-aachen.de/metro-pose3d).\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 22:38:13 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["S\u00e1r\u00e1ndi", "Istv\u00e1n", ""], ["Linder", "Timm", ""], ["Arras", "Kai O.", ""], ["Leibe", "Bastian", ""]]}, {"id": "2003.02959", "submitter": "Javad Fotouhi", "authors": "Javad Fotouhi, Xingtong Liu, Mehran Armand, Nassir Navab, Mathias\n  Unberath", "title": "From Perspective X-ray Imaging to Parallax-Robust Orthographic Stitching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stitching images acquired under perspective projective geometry is a relevant\ntopic in computer vision with multiple applications ranging from smartphone\npanoramas to the construction of digital maps. Image stitching is an equally\nprominent challenge in medical imaging, where the limited field-of-view\ncaptured by single images prohibits holistic analysis of patient anatomy. The\nbarrier that prevents straight-forward mosaicing of 2D images is depth mismatch\ndue to parallax. In this work, we leverage the Fourier slice theorem to\naggregate information from multiple transmission images in parallax-free\ndomains using fundamental principles of X-ray image formation. The semantics of\nthe stitched image are restored using a novel deep learning strategy that\nexploits similarity measures designed around frequency, as well as dense and\nsparse spatial image content. Our pipeline, not only stitches images, but also\nprovides orthographic reconstruction that enables metric measurements of\nclinically relevant quantities directly on the 2D image plane.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 23:16:48 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Fotouhi", "Javad", ""], ["Liu", "Xingtong", ""], ["Armand", "Mehran", ""], ["Navab", "Nassir", ""], ["Unberath", "Mathias", ""]]}, {"id": "2003.02960", "submitter": "Aditya Golatkar", "authors": "Aditya Golatkar, Alessandro Achille, Stefano Soatto", "title": "Forgetting Outside the Box: Scrubbing Deep Networks of Information\n  Accessible from Input-Output Observations", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a procedure for removing dependency on a cohort of training data\nfrom a trained deep network that improves upon and generalizes previous methods\nto different readout functions and can be extended to ensure forgetting in the\nactivations of the network. We introduce a new bound on how much information\ncan be extracted per query about the forgotten cohort from a black-box network\nfor which only the input-output behavior is observed. The proposed forgetting\nprocedure has a deterministic part derived from the differential equations of a\nlinearized version of the model, and a stochastic part that ensures information\ndestruction by adding noise tailored to the geometry of the loss landscape. We\nexploit the connections between the activation and weight dynamics of a DNN\ninspired by Neural Tangent Kernels to compute the information in the\nactivations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 23:17:35 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 06:52:59 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 02:23:28 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Golatkar", "Aditya", ""], ["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "2003.02977", "submitter": "Zhisheng Xiao", "authors": "Zhisheng Xiao, Qing Yan, Yali Amit", "title": "Likelihood Regret: An Out-of-Distribution Detection Score For\n  Variational Auto-encoder", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep probabilistic generative models enable modeling the likelihoods of very\nhigh dimensional data. An important application of generative modeling should\nbe the ability to detect out-of-distribution (OOD) samples by setting a\nthreshold on the likelihood. However, some recent studies show that\nprobabilistic generative models can, in some cases, assign higher likelihoods\non certain types of OOD samples, making the OOD detection rules based on\nlikelihood threshold problematic. To address this issue, several OOD detection\nmethods have been proposed for deep generative models. In this paper, we make\nthe observation that many of these methods fail when applied to generative\nmodels based on Variational Auto-encoders (VAE). As an alternative, we propose\nLikelihood Regret, an efficient OOD score for VAEs. We benchmark our proposed\nmethod over existing approaches, and empirical results suggest that our method\nobtains the best overall OOD detection performances when applied to VAEs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 00:30:38 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 03:03:08 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 21:58:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Xiao", "Zhisheng", ""], ["Yan", "Qing", ""], ["Amit", "Yali", ""]]}, {"id": "2003.03000", "submitter": "Essam Rashed", "authors": "Essam A. Rashed and and Mohamed G. Awad", "title": "Neural networks approach for mammography diagnosis using wavelets\n  features", "comments": "Reprint", "journal-ref": "First Canadian Student Conference on Biomedical Computing, 2006", "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A supervised diagnosis system for digital mammogram is developed. The\ndiagnosis processes are done by transforming the data of the images into a\nfeature vector using wavelets multilevel decomposition. This vector is used as\nthe feature tailored toward separating different mammogram classes. The\nsuggested model consists of artificial neural networks designed for classifying\nmammograms according to tumor type and risk level. Results are enhanced from\nour previous study by extracting feature vectors using multilevel\ndecompositions instead of one level of decomposition. Radiologist-labeled\nimages were used to evaluate the diagnosis system. Results are very promising\nand show possible guide for future work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:10:47 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Rashed", "Essam A.", ""], ["Awad", "and Mohamed G.", ""]]}, {"id": "2003.03007", "submitter": "Jicong Fan", "authors": "Dong Yang, Monica Mengqi Li, Hong Fu, Jicong Fan, Howard Leung", "title": "Centrality Graph Convolutional Networks for Skeleton-based Action\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The topological structure of skeleton data plays a significant role in human\naction recognition. Combining the topological structure with graph\nconvolutional networks has achieved remarkable performance. In existing\nmethods, modeling the topological structure of skeleton data only considered\nthe connections between the joints and bones, and directly use physical\ninformation. However, there exists an unknown problem to investigate the key\njoints, bones and body parts in every human action. In this paper, we propose\nthe centrality graph convolutional networks to uncover the overlooked\ntopological information, and best take advantage of the information to\ndistinguish key joints, bones, and body parts. A novel centrality graph\nconvolutional network firstly highlights the effects of the key joints and\nbones to bring a definite improvement. Besides, the topological information of\nthe skeleton sequence is explored and combined to further enhance the\nperformance in a four-channel framework. Moreover, the reconstructed graph is\nimplemented by the adaptive methods on the training process, which further\nyields improvements. Our model is validated by two large-scale datasets,\nNTU-RGB+D and Kinetics, and outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 02:31:26 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Yang", "Dong", ""], ["Li", "Monica Mengqi", ""], ["Fu", "Hong", ""], ["Fan", "Jicong", ""], ["Leung", "Howard", ""]]}, {"id": "2003.03025", "submitter": "Long-fei Chen", "authors": "Chen Long-fei, Yuichi Nakamura, Kazuaki Kondo", "title": "Modeling User Behaviors in Machine Operation Tasks for Adaptive Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adaptive guidance system that supports equipment operators requires a\ncomprehensive model, which involves a variety of user behaviors that considers\ndifferent skill and knowledge levels, as well as rapid-changing task\nsituations. In the present paper, we introduced a novel method for modeling\noperational tasks, aiming to integrate visual operation records provided by\nusers with diverse experience levels and personal characteristics. For this\npurpose, we investigated the relationships between user behavior patterns that\ncould be visually observed and their skill levels under machine operation\nconditions. We considered 144 samples of two sewing tasks performed by 12\noperators using a head-mounted RGB-D camera and a static gaze tracker.\nBehavioral features, such as the operator's gaze and head movements, hand\ninteractions, and hotspots, were observed with significant behavioral trends\nresulting from continuous user skill improvement. We used a two-step method to\nmodel the diversity of user behavior: prototype selection and experience\nintegration based on skill ranking. The experimental results showed that\nseveral features could serve as appropriate indices for user skill evaluation,\nas well as providing valuable clues for revealing personal behavioral\ncharacteristics. The integration of user records with different skills and\noperational habits allowed developing a rich, inclusive task model that could\nbe used flexibly to adapt to diverse user-specific needs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 04:05:08 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 12:30:36 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 05:40:10 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 06:31:23 GMT"}, {"version": "v5", "created": "Wed, 16 Sep 2020 06:57:52 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Long-fei", "Chen", ""], ["Nakamura", "Yuichi", ""], ["Kondo", "Kazuaki", ""]]}, {"id": "2003.03026", "submitter": "Shiyu Song", "authors": "Yao Zhou, Guowei Wan, Shenhua Hou, Li Yu, Gang Wang, Xiaofei Rui,\n  Shiyu Song", "title": "DA4AD: End-to-End Deep Attention-based Visual Localization for\n  Autonomous Driving", "comments": "19 pages, 4 figures, Accepted by ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visual localization framework based on novel deep attention\naware features for autonomous driving that achieves centimeter level\nlocalization accuracy. Conventional approaches to the visual localization\nproblem rely on handcrafted features or human-made objects on the road. They\nare known to be either prone to unstable matching caused by severe appearance\nor lighting changes, or too scarce to deliver constant and robust localization\nresults in challenging scenarios. In this work, we seek to exploit the deep\nattention mechanism to search for salient, distinctive and stable features that\nare good for long-term matching in the scene through a novel end-to-end deep\nneural network. Furthermore, our learned feature descriptors are demonstrated\nto be competent to establish robust matches and therefore successfully estimate\nthe optimal camera poses with high precision. We comprehensively validate the\neffectiveness of our method using a freshly collected dataset with high-quality\nground truth trajectories and hardware synchronization between sensors. Results\ndemonstrate that our method achieves a competitive localization accuracy when\ncompared to the LiDAR-based localization solutions under various challenging\ncircumstances, leading to a potential low-cost localization solution for\nautonomous driving.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 04:34:39 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 17:31:33 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Zhou", "Yao", ""], ["Wan", "Guowei", ""], ["Hou", "Shenhua", ""], ["Yu", "Li", ""], ["Wang", "Gang", ""], ["Rui", "Xiaofei", ""], ["Song", "Shiyu", ""]]}, {"id": "2003.03030", "submitter": "Shihao Zhao", "authors": "Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen,\n  Yu-Gang Jiang", "title": "Clean-Label Backdoor Attacks on Video Recognition Models", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to backdoor attacks which can hide\nbackdoor triggers in DNNs by poisoning training data. A backdoored model\nbehaves normally on clean test images, yet consistently predicts a particular\ntarget class for any test examples that contain the trigger pattern. As such,\nbackdoor attacks are hard to detect, and have raised severe security concerns\nin real-world applications. Thus far, backdoor research has mostly been\nconducted in the image domain with image classification models. In this paper,\nwe show that existing image backdoor attacks are far less effective on videos,\nand outline 4 strict conditions where existing attacks are likely to fail: 1)\nscenarios with more input dimensions (eg. videos), 2) scenarios with high\nresolution, 3) scenarios with a large number of classes and few examples per\nclass (a \"sparse dataset\"), and 4) attacks with access to correct labels (eg.\nclean-label attacks). We propose the use of a universal adversarial trigger as\nthe backdoor trigger to attack video recognition models, a situation where\nbackdoor attacks are likely to be challenged by the above 4 strict conditions.\nWe show on benchmark video datasets that our proposed backdoor attack can\nmanipulate state-of-the-art video models with high success rates by poisoning\nonly a small proportion of training data (without changing the labels). We also\nshow that our proposed backdoor attack is resistant to state-of-the-art\nbackdoor defense/detection methods, and can even be applied to improve image\nbackdoor attacks. Our proposed video backdoor attack not only serves as a\nstrong baseline for improving the robustness of video models, but also provides\na new perspective for more understanding more powerful backdoor attacks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 04:51:48 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 12:13:20 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhao", "Shihao", ""], ["Ma", "Xingjun", ""], ["Zheng", "Xiang", ""], ["Bailey", "James", ""], ["Chen", "Jingjing", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "2003.03040", "submitter": "Bingyao Huang", "authors": "Bingyao Huang and Haibin Ling", "title": "DeProCams: Simultaneous Relighting, Compensation and Shape\n  Reconstruction for Projector-Camera Systems", "comments": "Source code and supplementary material at:\n  https://github.com/BingyaoHuang/DeProCams", "journal-ref": null, "doi": "10.1109/TVCG.2021.3067771", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based relighting, projector compensation and depth/normal\nreconstruction are three important tasks of projector-camera systems (ProCams)\nand spatial augmented reality (SAR). Although they share a similar pipeline of\nfinding projector-camera image mappings, in tradition, they are addressed\nindependently, sometimes with different prerequisites, devices and sampling\nimages. In practice, this may be cumbersome for SAR applications to address\nthem one-by-one. In this paper, we propose a novel end-to-end trainable model\nnamed DeProCams to explicitly learn the photometric and geometric mappings of\nProCams, and once trained, DeProCams can be applied simultaneously to the three\ntasks. DeProCams explicitly decomposes the projector-camera image mappings into\nthree subprocesses: shading attributes estimation, rough direct light\nestimation and photorealistic neural rendering. A particular challenge\naddressed by DeProCams is occlusion, for which we exploit epipolar constraint\nand propose a novel differentiable projector direct light mask. Thus, it can be\nlearned end-to-end along with the other modules. Afterwards, to improve\nconvergence, we apply photometric and geometric constraints such that the\nintermediate results are plausible. In our experiments, DeProCams shows clear\nadvantages over previous arts with promising quality and meanwhile being fully\ndifferentiable. Moreover, by solving the three tasks in a unified model,\nDeProCams waives the need for additional optical devices, radiometric\ncalibrations and structured light.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 05:49:16 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 09:19:14 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Huang", "Bingyao", ""], ["Ling", "Haibin", ""]]}, {"id": "2003.03055", "submitter": "Yuedong Chen", "authors": "Yuedong Chen, Guoxian Song, Zhiwen Shao, Jianfei Cai, Tat-Jen Cham,\n  Jianming Zheng", "title": "GeoConv: Geodesic Guided Convolution for Facial Action Unit Recognition", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic facial action unit (AU) recognition has attracted great attention\nbut still remains a challenging task, as subtle changes of local facial muscles\nare difficult to thoroughly capture. Most existing AU recognition approaches\nleverage geometry information in a straightforward 2D or 3D manner, which\neither ignore 3D manifold information or suffer from high computational costs.\nIn this paper, we propose a novel geodesic guided convolution (GeoConv) for AU\nrecognition by embedding 3D manifold information into 2D convolutions.\nSpecifically, the kernel of GeoConv is weighted by our introduced geodesic\nweights, which are negatively correlated to geodesic distances on a coarsely\nreconstructed 3D face model. Moreover, based on GeoConv, we further develop an\nend-to-end trainable framework named GeoCNN for AU recognition. Extensive\nexperiments on BP4D and DISFA benchmarks show that our approach significantly\noutperforms the state-of-the-art AU recognition methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 07:05:46 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Yuedong", ""], ["Song", "Guoxian", ""], ["Shao", "Zhiwen", ""], ["Cai", "Jianfei", ""], ["Cham", "Tat-Jen", ""], ["Zheng", "Jianming", ""]]}, {"id": "2003.03081", "submitter": "Ying Dai", "authors": "Ying Dai", "title": "CNN-based Repetitive self-revised learning for photos' aesthetics\n  imbalanced classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.08213", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aesthetic assessment is subjective, and the distribution of the aesthetic\nlevels is imbalanced. In order to realize the auto-assessment of photo\naesthetics, we focus on using repetitive self-revised learning (RSRL) to train\nthe CNN-based aesthetics classification network by imbalanced data set. As\nRSRL, the network is trained repetitively by dropping out the low likelihood\nphoto samples at the middle levels of aesthetics from the training data set\nbased on the previously trained network. Further, the retained two networks are\nused in extracting highlight regions of the photos related with the aesthetic\nassessment. Experimental results show that the CNN-based repetitive\nself-revised learning is effective for improving the performances of the\nimbalanced classification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 08:54:53 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 05:08:36 GMT"}, {"version": "v3", "created": "Sun, 15 Mar 2020 08:01:59 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 06:51:43 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dai", "Ying", ""]]}, {"id": "2003.03091", "submitter": "Taiping Zeng", "authors": "Taiping Zeng, Xiaoli Li, and Bailu Si", "title": "StereoNeuroBayesSLAM: A Neurobiologically Inspired Stereo Visual SLAM\n  System Based on Direct Sparse Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neurobiologically inspired visual simultaneous localization and\nmapping (SLAM) system based on direction sparse method to real-time build\ncognitive maps of large-scale environments from a moving stereo camera. The\ncore SLAM system mainly comprises a Bayesian attractor network, which utilizes\nneural responses of head direction (HD) cells in the hippocampus and grid cells\nin the medial entorhinal cortex (MEC) to represent the head direction and the\nposition of the robot in the environment, respectively. Direct sparse method is\nemployed to accurately and robustly estimate velocity information from a stereo\ncamera. Input rotational and translational velocities are integrated by the HD\ncell and grid cell networks, respectively. We demonstrated our\nneurobiologically inspired stereo visual SLAM system on the KITTI odometry\nbenchmark datasets. Our proposed SLAM system is robust to real-time build a\ncoherent semi-metric topological map from a stereo camera. Qualitative\nevaluation on cognitive maps shows that our proposed neurobiologically inspired\nstereo visual SLAM system outperforms our previous brain-inspired algorithms\nand the neurobiologically inspired monocular visual SLAM system both in terms\nof tracking accuracy and robustness, which is closer to the traditional\nstate-of-the-art one.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 09:07:50 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Zeng", "Taiping", ""], ["Li", "Xiaoli", ""], ["Si", "Bailu", ""]]}, {"id": "2003.03107", "submitter": "Fawaz Sammani", "authors": "Fawaz Sammani, Luke Melas-Kyriazi", "title": "Show, Edit and Tell: A Framework for Editing Image Captions", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most image captioning frameworks generate captions directly from images,\nlearning a mapping from visual features to natural language. However, editing\nexisting captions can be easier than generating new ones from scratch.\nIntuitively, when editing captions, a model is not required to learn\ninformation that is already present in the caption (i.e. sentence structure),\nenabling it to focus on fixing details (e.g. replacing repetitive words). This\npaper proposes a novel approach to image captioning based on iterative adaptive\nrefinement of an existing caption. Specifically, our caption-editing model\nconsisting of two sub-modules: (1) EditNet, a language module with an adaptive\ncopy mechanism (Copy-LSTM) and a Selective Copy Memory Attention mechanism\n(SCMA), and (2) DCNet, an LSTM-based denoising auto-encoder. These components\nenable our model to directly copy from and modify existing captions.\nExperiments demonstrate that our new approach achieves state-of-art performance\non the MS COCO dataset both with and without sequence-level training.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 09:52:17 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Sammani", "Fawaz", ""], ["Melas-Kyriazi", "Luke", ""]]}, {"id": "2003.03109", "submitter": "Jevgenij Gamper", "authors": "Jevgenij Gamper, Brandon Chan, Yee Wah Tsang, David Snead, Nasir\n  Rajpoot", "title": "Meta-SVDD: Probabilistic Meta-Learning for One-Class Classification in\n  Cancer Histology Images", "comments": "Accepted to Medical Imaging meets NeurIPS Workshop, Conference on\n  Neural Information Processing Systems 2019, Vancouver", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train a robust deep learning model, one usually needs a balanced set of\ncategories in the training data. The data acquired in a medical domain,\nhowever, frequently contains an abundance of healthy patients, versus a small\nvariety of positive, abnormal cases. Moreover, the annotation of a positive\nsample requires time consuming input from medical domain experts. This scenario\nwould suggest a promise for one-class classification type approaches. In this\nwork we propose a general one-class classification model for histology, that is\nmeta-trained on multiple histology datasets simultaneously, and can be applied\nto new tasks without expensive re-training. This model could be easily used by\npathology domain experts, and potentially be used for screening purposes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 09:59:57 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Gamper", "Jevgenij", ""], ["Chan", "Brandon", ""], ["Tsang", "Yee Wah", ""], ["Snead", "David", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2003.03113", "submitter": "Wei Lin", "authors": "Wei. Lin, Junyu. Gao, Qi. Wang, Xuelong. Li", "title": "Pixel-Level Self-Paced Learning for Super-Resolution", "comments": "5 pages, 5 figures. Accepted by ICASSP 2020, Source code:\n  https://github.com/Elin24/PSPL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, lots of deep networks are proposed to improve the quality of\npredicted super-resolution (SR) images, due to its widespread use in several\nimage-based fields. However, with these networks being constructed deeper and\ndeeper, they also cost much longer time for training, which may guide the\nlearners to local optimization. To tackle this problem, this paper designs a\ntraining strategy named Pixel-level Self-Paced Learning (PSPL) to accelerate\nthe convergence velocity of SISR models. PSPL imitating self-paced learning\ngives each pixel in the predicted SR image and its corresponding pixel in\nground truth an attention weight, to guide the model to a better region in\nparameter space. Extensive experiments proved that PSPL could speed up the\ntraining of SISR models, and prompt several existing models to obtain new\nbetter results. Furthermore, the source code is available at\nhttps://github.com/Elin24/PSPL.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 10:04:50 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 04:32:26 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lin", "Wei.", ""], ["Gao", "Junyu.", ""], ["Wang", "Qi.", ""], ["Li", "Xuelong.", ""]]}, {"id": "2003.03134", "submitter": "Joseph Ortiz", "authors": "Joseph Ortiz, Mark Pupilli, Stefan Leutenegger, Andrew J. Davison", "title": "Bundle Adjustment on a Graph Processor", "comments": "Published in Proceedings of the IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR 2020). Video:\n  https://www.youtube.com/watch?v=TqeN8aQNgd0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph processors such as Graphcore's Intelligence Processing Unit (IPU) are\npart of the major new wave of novel computer architecture for AI, and have a\ngeneral design with massively parallel computation, distributed on-chip memory\nand very high inter-core communication bandwidth which allows breakthrough\nperformance for message passing algorithms on arbitrary graphs. We show for the\nfirst time that the classical computer vision problem of bundle adjustment (BA)\ncan be solved extremely fast on a graph processor using Gaussian Belief\nPropagation. Our simple but fully parallel implementation uses the 1216 cores\non a single IPU chip to, for instance, solve a real BA problem with 125\nkeyframes and 1919 points in under 40ms, compared to 1450ms for the Ceres CPU\nlibrary. Further code optimisation will surely increase this difference on\nstatic problems, but we argue that the real promise of graph processing is for\nflexible in-place optimisation of general, dynamically changing factor graphs\nrepresenting Spatial AI problems. We give indications of this with experiments\nshowing the ability of GBP to efficiently solve incremental SLAM problems, and\ndeal with robust cost functions and different types of factors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 11:05:55 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 16:59:24 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ortiz", "Joseph", ""], ["Pupilli", "Mark", ""], ["Leutenegger", "Stefan", ""], ["Davison", "Andrew J.", ""]]}, {"id": "2003.03136", "submitter": "Oriol Ramos Terrades", "authors": "B. Gautam and O. Ramos Terrades and J. M. Pujades and M. Valls", "title": "Knowledge graph based methods for record linkage", "comments": "the paper is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays, it is common in Historical Demography the use of individual-level\ndata as a consequence of a predominant life-course approach for the\nunderstanding of the demographic behaviour, family transition, mobility, etc.\nRecord linkage advance is key in these disciplines since it allows to increase\nthe volume and the data complexity to be analyzed. However, current methods are\nconstrained to link data coming from the same kind of sources. Knowledge graph\nare flexible semantic representations, which allow to encode data variability\nand semantic relations in a structured manner.\n  In this paper we propose the knowledge graph use to tackle record linkage\ntask. The proposed method, named {\\bf WERL}, takes advantage of the main\nknowledge graph properties and learns embedding vectors to encode census\ninformation. These embeddings are properly weighted to maximize the record\nlinkage performance. We have evaluated this method on benchmark data sets and\nwe have compared it to related methods with stimulating and satisfactory\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 11:09:44 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Gautam", "B.", ""], ["Terrades", "O. Ramos", ""], ["Pujades", "J. M.", ""], ["Valls", "M.", ""]]}, {"id": "2003.03151", "submitter": "Naser Damer", "authors": "Meiling Fang, Naser Damer, Florian Kirchbuchner, Arjan Kuijper", "title": "Demographic Bias in Presentation Attack Detection of Iris Recognition\n  Systems", "comments": "accepted for publication at EUSIPCO2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the widespread use of biometric systems, the demographic bias problem\nraises more attention. Although many studies addressed bias issues in biometric\nverification, there are no works that analyze the bias in presentation attack\ndetection (PAD) decisions. Hence, we investigate and analyze the demographic\nbias in iris PAD algorithms in this paper. To enable a clear discussion, we\nadapt the notions of differential performance and differential outcome to the\nPAD problem. We study the bias in iris PAD using three baselines (hand-crafted,\ntransfer-learning, and training from scratch) using the NDCLD-2013 database.\nThe experimental results point out that female users will be significantly less\nprotected by the PAD, in comparison to males.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:16:19 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 10:02:30 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Fang", "Meiling", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2003.03164", "submitter": "Xuyang Bai Mr.", "authors": "Xuyang Bai, Zixin Luo, Lei Zhou, Hongbo Fu, Long Quan, Chiew-Lan Tai", "title": "D3Feat: Joint Learning of Dense Detection and Description of 3D Local\n  Features", "comments": "Accepted to CVPR 2020, supplementary materials included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful point cloud registration often lies on robust establishment of\nsparse matches through discriminative 3D local features. Despite the fast\nevolution of learning-based 3D feature descriptors, little attention has been\ndrawn to the learning of 3D feature detectors, even less for a joint learning\nof the two tasks. In this paper, we leverage a 3D fully convolutional network\nfor 3D point clouds, and propose a novel and practical learning mechanism that\ndensely predicts both a detection score and a description feature for each 3D\npoint. In particular, we propose a keypoint selection strategy that overcomes\nthe inherent density variations of 3D point clouds, and further propose a\nself-supervised detector loss guided by the on-the-fly feature matching results\nduring training. Finally, our method achieves state-of-the-art results in both\nindoor and outdoor scenarios, evaluated on 3DMatch and KITTI datasets, and\nshows its strong generalization ability on the ETH dataset. Towards practical\nuse, we show that by adopting a reliable feature detector, sampling a smaller\nnumber of features is sufficient to achieve accurate and fast point cloud\nalignment.[code release](https://github.com/XuyangBai/D3Feat)\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:51:09 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Bai", "Xuyang", ""], ["Luo", "Zixin", ""], ["Zhou", "Lei", ""], ["Fu", "Hongbo", ""], ["Quan", "Long", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "2003.03167", "submitter": "Victor Villena-Martinez", "authors": "Victor Villena-Martinez, Sergiu Oprea, Marcelo Saval-Calvo, Jorge\n  Azorin-Lopez, Andres Fuster-Guillo, Robert B. Fisher", "title": "When Deep Learning Meets Data Alignment: A Review on Deep Registration\n  Networks (DRNs)", "comments": "Published in Applied Sciences", "journal-ref": "Appl. Sci. 2020, 10(21), 7524", "doi": "10.3390/app10217524", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Registration is the process that computes the transformation that aligns sets\nof data. Commonly, a registration process can be divided into four main steps:\ntarget selection, feature extraction, feature matching, and transform\ncomputation for the alignment. The accuracy of the result depends on multiple\nfactors, the most significant are the quantity of input data, the presence of\nnoise, outliers and occlusions, the quality of the extracted features,\nreal-time requirements and the type of transformation, especially those ones\ndefined by multiple parameters, like non-rigid deformations. Recent\nadvancements in machine learning could be a turning point in these issues,\nparticularly with the development of deep learning (DL) techniques, which are\nhelping to improve multiple computer vision problems through an abstract\nunderstanding of the input data. In this paper, a review of deep learning-based\nregistration methods is presented. We classify the different papers proposing a\nframework extracted from the traditional registration pipeline to analyse the\nnew learning-based proposal strengths. Deep Registration Networks (DRNs) try to\nsolve the alignment task either replacing part of the traditional pipeline with\na network or fully solving the registration problem. The main conclusions\nextracted are, on the one hand, 1) learning-based registration techniques\ncannot always be clearly classified in the traditional pipeline. 2) These\napproaches allow more complex inputs like conceptual models as well as the\ntraditional 3D datasets. 3) In spite of the generality of learning, the current\nproposals are still ad hoc solutions. Finally, 4) this is a young topic that\nstill requires a large effort to reach general solutions able to cope with the\nproblems that affect traditional approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:56:19 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 12:02:07 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Villena-Martinez", "Victor", ""], ["Oprea", "Sergiu", ""], ["Saval-Calvo", "Marcelo", ""], ["Azorin-Lopez", "Jorge", ""], ["Fuster-Guillo", "Andres", ""], ["Fisher", "Robert B.", ""]]}, {"id": "2003.03186", "submitter": "Elad Amrani", "authors": "Elad Amrani, Rami Ben-Ari, Daniel Rotman and Alex Bronstein", "title": "Noise Estimation Using Density Estimation for Self-Supervised Multimodal\n  Learning", "comments": "Accepted to AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key factors of enabling machine learning models to comprehend and\nsolve real-world tasks is to leverage multimodal data. Unfortunately,\nannotation of multimodal data is challenging and expensive. Recently,\nself-supervised multimodal methods that combine vision and language were\nproposed to learn multimodal representations without annotation. However, these\nmethods often choose to ignore the presence of high levels of noise and thus\nyield sub-optimal results. In this work, we show that the problem of noise\nestimation for multimodal data can be reduced to a multimodal density\nestimation task. Using multimodal density estimation, we propose a noise\nestimation building block for multimodal representation learning that is based\nstrictly on the inherent correlation between different modalities. We\ndemonstrate how our noise estimation can be broadly integrated and achieves\ncomparable results to state-of-the-art performance on five different benchmark\ndatasets for two challenging multimodal tasks: Video Question Answering and\nText-To-Video Retrieval. Furthermore, we provide a theoretical probabilistic\nerror bound substantiating our empirical results and analyze failure cases.\nCode: https://github.com/elad-amrani/ssml.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:25:12 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 09:03:47 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 14:26:22 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Amrani", "Elad", ""], ["Ben-Ari", "Rami", ""], ["Rotman", "Daniel", ""], ["Bronstein", "Alex", ""]]}, {"id": "2003.03192", "submitter": "Muhammad Hamdan", "authors": "Muhammad K.A. Hamdan, Diane T. Rover, Matthew J. Darr, John Just", "title": "Generalizable semi-supervised learning method to estimate mass from\n  sparsely annotated images", "comments": "22 pages, 21 figures, Computers and Electronics in Agriculture. arXiv\n  admin note: text overlap with arXiv:1908.04387", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mass flow estimation is of great importance to several industries, and it can\nbe quite challenging to obtain accurate estimates due to limitation in expense\nor general infeasibility. In the context of agricultural applications, yield\nmonitoring is a key component to precision agriculture and mass flow is the\ncritical factor to measure. Measuring mass flow allows for field productivity\nanalysis, cost minimization, and adjustments to machine efficiency. Methods\nsuch as volume or force-impact have been used to measure mass flow; however,\nthese methods are limited in application and accuracy. In this work, we use\ndeep learning to develop and test a vision system that can accurately estimate\nthe mass of sugarcane while running in real-time on a sugarcane harvester\nduring operation. The deep learning algorithm that is used to estimate mass\nflow is trained using very sparsely annotated images (semi-supervised) using\nonly final load weights (aggregated weights over a certain period of time). The\ndeep neural network (DNN) succeeds in capturing the mass of sugarcane\naccurately and surpasses older volumetric-based methods, despite highly varying\nlighting and material colors in the images. The deep neural network is\ninitially trained to predict mass on laboratory data (bamboo) and then transfer\nlearning is utilized to apply the same methods to estimate mass of sugarcane.\nUsing a vision system with a relatively lightweight deep neural network we are\nable to estimate mass of bamboo with an average error of 4.5% and 5.9% for a\nselect season of sugarcane.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 18:13:07 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Hamdan", "Muhammad K. A.", ""], ["Rover", "Diane T.", ""], ["Darr", "Matthew J.", ""], ["Just", "John", ""]]}, {"id": "2003.03193", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Qi She, Alan F. Smeaton, Tomas E. Ward, Graham Healy", "title": "A Neuro-AI Interface for Evaluating Generative Adversarial Networks", "comments": "Accepted by ICLR 2020 Workshop Bridging AI and Cognitive Science\n  (BAICS). arXiv admin note: substantial text overlap with arXiv:1905.04243", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are increasingly attracting attention\nin the computer vision, natural language processing, speech synthesis and\nsimilar domains. However, evaluating the performance of GANs is still an open\nand challenging problem. Existing evaluation metrics primarily measure the\ndissimilarity between real and generated images using automated statistical\nmethods. They often require large sample sizes for evaluation and do not\ndirectly reflect human perception of image quality. In this work, we introduce\nan evaluation metric called Neuroscore, for evaluating the performance of GANs,\nthat more directly reflects psychoperceptual image quality through the\nutilization of brain signals. Our results show that Neuroscore has superior\nperformance to the current evaluation metrics in that: (1) It is more\nconsistent with human judgment; (2) The evaluation process needs much smaller\nnumbers of samples; and (3) It is able to rank the quality of images on a per\nGAN basis. A convolutional neural network (CNN) based neuro-AI interface is\nproposed to predict Neuroscore from GAN-generated images directly without the\nneed for neural responses. Importantly, we show that including neural responses\nduring the training phase of the network can significantly improve the\nprediction capability of the proposed model. Codes and data can be referred at\nthis link: https://github.com/villawang/Neuro-AI-Interface.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 17:53:43 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 10:42:02 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wang", "Zhengwei", ""], ["She", "Qi", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""], ["Healy", "Graham", ""]]}, {"id": "2003.03206", "submitter": "Yuanhang Zhang", "authors": "Yuanhang Zhang, Shuang Yang, Jingyun Xiao, Shiguang Shan, Xilin Chen", "title": "Can We Read Speech Beyond the Lips? Rethinking RoI Selection for Deep\n  Visual Speech Recognition", "comments": "8 pages; accepted in the 15th IEEE International Conference on\n  Automatic Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in deep learning have heightened interest among researchers\nin the field of visual speech recognition (VSR). Currently, most existing\nmethods equate VSR with automatic lip reading, which attempts to recognise\nspeech by analysing lip motion. However, human experience and psychological\nstudies suggest that we do not always fix our gaze at each other's lips during\na face-to-face conversation, but rather scan the whole face repetitively. This\ninspires us to revisit a fundamental yet somehow overlooked problem: can VSR\nmodels benefit from reading extraoral facial regions, i.e. beyond the lips? In\nthis paper, we perform a comprehensive study to evaluate the effects of\ndifferent facial regions with state-of-the-art VSR models, including the mouth,\nthe whole face, the upper face, and even the cheeks. Experiments are conducted\non both word-level and sentence-level benchmarks with different\ncharacteristics. We find that despite the complex variations of the data,\nincorporating information from extraoral facial regions, even the upper face,\nconsistently benefits VSR performance. Furthermore, we introduce a simple yet\neffective method based on Cutout to learn more discriminative features for\nface-based VSR, hoping to maximise the utility of information encoded in\ndifferent facial regions. Our experiments show obvious improvements over\nexisting state-of-the-art methods that use only the lip region as inputs, a\nresult we believe would probably provide the VSR community with some new and\nexciting insights.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:52:46 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 06:06:20 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Yuanhang", ""], ["Yang", "Shuang", ""], ["Xiao", "Jingyun", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2003.03212", "submitter": "Seong Hyeon Park", "authors": "Seong Hyeon Park, Gyubok Lee, Manoj Bhat, Jimin Seo, Minseok Kang,\n  Jonathan Francis, Ashwin R. Jadhav, Paul Pu Liang and Louis-Philippe Morency", "title": "Diverse and Admissible Trajectory Forecasting through Multimodal Context\n  Understanding", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent trajectory forecasting in autonomous driving requires an agent to\naccurately anticipate the behaviors of the surrounding vehicles and\npedestrians, for safe and reliable decision-making. Due to partial\nobservability in these dynamical scenes, directly obtaining the posterior\ndistribution over future agent trajectories remains a challenging problem. In\nrealistic embodied environments, each agent's future trajectories should be\nboth diverse since multiple plausible sequences of actions can be used to reach\nits intended goals, and admissible since they must obey physical constraints\nand stay in drivable areas. In this paper, we propose a model that synthesizes\nmultiple input signals from the multimodal world|the environment's scene\ncontext and interactions between multiple surrounding agents|to best model all\ndiverse and admissible trajectories. We compare our model with strong baselines\nand ablations across two public datasets and show a significant performance\nimprovement over previous state-of-the-art methods. Lastly, we offer new\nmetrics incorporating admissibility criteria to further study and evaluate the\ndiversity of predictions. Codes are at: https://github.com/kami93/CMU-DATF.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:59:39 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:11:49 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 02:12:55 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 13:57:26 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Park", "Seong Hyeon", ""], ["Lee", "Gyubok", ""], ["Bhat", "Manoj", ""], ["Seo", "Jimin", ""], ["Kang", "Minseok", ""], ["Francis", "Jonathan", ""], ["Jadhav", "Ashwin R.", ""], ["Liang", "Paul Pu", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2003.03229", "submitter": "Radu Tudor Ionescu", "authors": "Mariana-Iuliana Georgescu, Radu Tudor Ionescu, Nicolae-Catalin Ristea,\n  Nicu Sebe", "title": "Non-linear Neurons with Human-like Apical Dendrite Activations", "comments": "Submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to classify linearly non-separable data, neurons are typically\norganized into multi-layer neural networks that are equipped with at least one\nhidden layer. Inspired by some recent discoveries in neuroscience, we propose a\nnew neuron model along with a novel activation function enabling learning of\nnon-linear decision boundaries using a single neuron. We show that a standard\nneuron followed by the novel apical dendrite activation (ADA) can learn the XOR\nlogical function with 100% accuracy. Furthermore, we conduct experiments on\nthree benchmark data sets from computer vision and natural language processing,\ni.e. Fashion-MNIST, UTKFace and MOROCO, showing that the ADA and the leaky ADA\nfunctions provide superior results to Rectified Liner Units (ReLU) and leaky\nReLU, for various neural network architectures, e.g. 1-hidden layer or 2-hidden\nlayers multi-layer perceptrons (MLPs) and convolutional neural networks (CNNs)\nsuch as LeNet, VGG, ResNet and Character-level CNN. We also obtain further\nimprovements when we change the standard model of the neuron with our pyramidal\nneuron with apical dendrite activations (PyNADA).\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 21:09:39 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 06:51:38 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Georgescu", "Mariana-Iuliana", ""], ["Ionescu", "Radu Tudor", ""], ["Ristea", "Nicolae-Catalin", ""], ["Sebe", "Nicu", ""]]}, {"id": "2003.03233", "submitter": "Connah Kendrick", "authors": "Connah Kendrick, David Gillespie, Moi Hoon Yap", "title": "Anysize GAN: A solution to the image-warping problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new type of General Adversarial Network (GAN) to resolve a\ncommon issue with Deep Learning. We develop a novel architecture that can be\napplied to existing latent vector based GAN structures that allows them to\ngenerate on-the-fly images of any size. Existing GAN for image generation\nrequires uniform images of matching dimensions. However, publicly available\ndatasets, such as ImageNet contain thousands of different sizes. Resizing image\ncauses deformations and changing the image data, whereas as our network does\nnot require this preprocessing step. We make significant changes to the\nstandard data loading techniques to enable any size image to be loaded for\ntraining. We also modify the network in two ways, by adding multiple inputs and\na novel dynamic resizing layer. Finally we make adjustments to the\ndiscriminator to work on multiple resolutions. These changes can allow multiple\nresolution datasets to be trained on without any resizing, if memory allows. We\nvalidate our results on the ISIC 2019 skin lesion dataset. We demonstrate our\nmethod can successfully generate realistic images at different sizes without\nissue, preserving and understanding spatial relationships, while maintaining\nfeature relationships. We will release the source codes upon paper acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:18:42 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 21:19:38 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Kendrick", "Connah", ""], ["Gillespie", "David", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "2003.03241", "submitter": "Theodore Papamarkou", "authors": "Theodore Papamarkou, Hayley Guy, Bryce Kroencke, Jordan Miller,\n  Preston Robinette, Daniel Schultz, Jacob Hinkle, Laura Pullum, Catherine\n  Schuman, Jeremy Renshaw, Stylianos Chatzidakis", "title": "Automated detection of corrosion in used nuclear fuel dry storage\n  canisters using residual neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondestructive evaluation methods play an important role in ensuring\ncomponent integrity and safety in many industries. Operator fatigue can play a\ncritical role in the reliability of such methods. This is important for\ninspecting high value assets or assets with a high consequence of failure, such\nas aerospace and nuclear components. Recent advances in convolution neural\nnetworks can support and automate these inspection efforts. This paper proposes\nusing residual neural networks (ResNets) for real-time detection of corrosion,\nincluding iron oxide discoloration, pitting and stress corrosion cracking, in\ndry storage stainless steel canisters housing used nuclear fuel. The proposed\napproach crops nuclear canister images into smaller tiles, trains a ResNet on\nthese tiles, and classifies images as corroded or intact using the per-image\ncount of tiles predicted as corroded by the ResNet. The results demonstrate\nthat such a deep learning approach allows to detect the locus of corrosion via\nsmaller tiles, and at the same time to infer with high accuracy whether an\nimage comes from a corroded canister. Thereby, the proposed approach holds\npromise to automate and speed up nuclear fuel canister inspections, to minimize\ninspection costs, and to partially replace human-conducted onsite inspections,\nthus reducing radiation doses to personnel.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:42:07 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 17:24:21 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 16:06:36 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Papamarkou", "Theodore", ""], ["Guy", "Hayley", ""], ["Kroencke", "Bryce", ""], ["Miller", "Jordan", ""], ["Robinette", "Preston", ""], ["Schultz", "Daniel", ""], ["Hinkle", "Jacob", ""], ["Pullum", "Laura", ""], ["Schuman", "Catherine", ""], ["Renshaw", "Jeremy", ""], ["Chatzidakis", "Stylianos", ""]]}, {"id": "2003.03256", "submitter": "Pavly Salah", "authors": "Pavly Salah Zaki, Marco Magdy William, Bolis Karam Soliman, Kerolos\n  Gamal Alexsan, Keroles Khalil, and Magdy El-Moursy", "title": "Traffic Signs Detection and Recognition System using Deep Learning", "comments": "7 pages, 14 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of technology, automobiles have become an\nessential asset in our day-to-day lives. One of the more important researches\nis Traffic Signs Recognition (TSR) systems. This paper describes an approach\nfor efficiently detecting and recognizing traffic signs in real-time, taking\ninto account the various weather, illumination and visibility challenges\nthrough the means of transfer learning. We tackle the traffic sign detection\nproblem using the state-of-the-art of multi-object detection systems such as\nFaster Recurrent Convolutional Neural Networks (F-RCNN) and Single Shot Multi-\nBox Detector (SSD) combined with various feature extractors such as MobileNet\nv1 and Inception v2, and also Tiny-YOLOv2. However, the focus of this paper is\ngoing to be F-RCNN Inception v2 and Tiny YOLO v2 as they achieved the best\nresults. The aforementioned models were fine-tuned on the German Traffic Signs\nDetection Benchmark (GTSDB) dataset. These models were tested on the host PC as\nwell as Raspberry Pi 3 Model B+ and the TASS PreScan simulation. We will\ndiscuss the results of all the models in the conclusion section.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:54:40 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Zaki", "Pavly Salah", ""], ["William", "Marco Magdy", ""], ["Soliman", "Bolis Karam", ""], ["Alexsan", "Kerolos Gamal", ""], ["Khalil", "Keroles", ""], ["El-Moursy", "Magdy", ""]]}, {"id": "2003.03262", "submitter": "Letizia Mariotti", "authors": "Letizia Mariotti and Ciaran Hughes", "title": "Spherical formulation of moving object geometric constraints for\n  monocular fisheye cameras", "comments": "8 pages, 9 figures, 2 tables Conference ITSC 2019", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 816-823", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a moving object detection algorithm for fisheye\ncameras used in autonomous driving. We reformulate the three commonly used\nconstraints in rectilinear images (epipolar, positive depth and positive height\nconstraints) to spherical coordinates which is invariant to specific camera\nconfiguration once the calibration is known. One of the main challenging use\ncase in autonomous driving is to detect parallel moving objects which suffer\nfrom motion-parallax ambiguity. To alleviate this, we formulate an additional\nfourth constraint, called the anti-parallel constraint, which aids the\ndetection of objects with motion that mirrors the ego-vehicle possible. We\nanalyze the proposed algorithm in different scenarios and demonstrate that it\nworks effectively operating directly on fisheye images.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:59:38 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mariotti", "Letizia", ""], ["Hughes", "Ciaran", ""]]}, {"id": "2003.03271", "submitter": "Roberto L. Castro", "authors": "Roberto L. Castro, Diego Andrade, Basilio Fraguela", "title": "A Hybrid Approach for Tracking Individual Players in Broadcast Match\n  Videos", "comments": "Comments: 25 pages, LaTeX; typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking people in a video sequence is a challenging task that has been\napproached from many perspectives. This task becomes even more complicated when\nthe person to track is a player in a broadcasted sport event, the reasons being\nthe existence of difficulties such as frequent camera movements or switches,\ntotal and partial occlusions between players, and blurry frames due to the\ncodification algorithm of the video. This paper introduces a player tracking\nsolution which is both fast and accurate. This allows to track a player\nprecisely in real-time. The approach combines several models that are executed\nconcurrently in a relatively modest hardware, and whose accuracy has been\nvalidated against hand-labeled broadcast video sequences. Regarding the\naccuracy, the tests show that the area under curve (AUC) of our approach is\naround 0.6, which is similar to generic state of the art solutions. As for\nperformance, our proposal can process high definition videos (1920x1080 px) at\n80 fps.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:16:23 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:09:14 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Castro", "Roberto L.", ""], ["Andrade", "Diego", ""], ["Fraguela", "Basilio", ""]]}, {"id": "2003.03293", "submitter": "Lei Zhang", "authors": "Fuxiang Huang, Lei Zhang, Yang Yang, Xichuan Zhou", "title": "Probability Weighted Compact Feature for Domain Adaptive Retrieval", "comments": "Accepted by CVPR 2020; The source code is available at\n  https://github.com/fuxianghuang1/PWCF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptive image retrieval includes single-domain retrieval and\ncross-domain retrieval. Most of the existing image retrieval methods only focus\non single-domain retrieval, which assumes that the distributions of retrieval\ndatabases and queries are similar. However, in practical application, the\ndiscrepancies between retrieval databases often taken in ideal\nillumination/pose/background/camera conditions and queries usually obtained in\nuncontrolled conditions are very large. In this paper, considering the\npractical application, we focus on challenging cross-domain retrieval. To\naddress the problem, we propose an effective method named Probability Weighted\nCompact Feature Learning (PWCF), which provides inter-domain correlation\nguidance to promote cross-domain retrieval accuracy and learns a series of\ncompact binary codes to improve the retrieval speed. First, we derive our loss\nfunction through the Maximum A Posteriori Estimation (MAP): Bayesian\nPerspective (BP) induced focal-triplet loss, BP induced quantization loss and\nBP induced classification loss. Second, we propose a common manifold structure\nbetween domains to explore the potential correlation across domains.\nConsidering the original feature representation is biased due to the\ninter-domain discrepancy, the manifold structure is difficult to be\nconstructed. Therefore, we propose a new feature named Histogram Feature of\nNeighbors (HFON) from the sample statistics perspective. Extensive experiments\non various benchmark databases validate that our method outperforms many\nstate-of-the-art image retrieval methods for domain adaptive image retrieval.\nThe source code is available at https://github.com/fuxianghuang1/PWCF\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:11:37 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Huang", "Fuxiang", ""], ["Zhang", "Lei", ""], ["Yang", "Yang", ""], ["Zhou", "Xichuan", ""]]}, {"id": "2003.03295", "submitter": "Shubham Goswami", "authors": "Shubham Goswami, Suril Mehta, Dhruva Sahrawat, Anubha Gupta, Ritu\n  Gupta", "title": "Heterogeneity Loss to Handle Intersubject and Intrasubject Variability\n  in Cancer", "comments": "Accepted in ICLR 2020 workshop\n  AI4AH(https://sites.google.com/view/ai4ah-iclr2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing nations lack adequate number of hospitals with modern equipment\nand skilled doctors. Hence, a significant proportion of these nations'\npopulation, particularly in rural areas, is not able to avail specialized and\ntimely healthcare facilities. In recent years, deep learning (DL) models, a\nclass of artificial intelligence (AI) methods, have shown impressive results in\nmedical domain. These AI methods can provide immense support to developing\nnations as affordable healthcare solutions. This work is focused on one such\napplication of blood cancer diagnosis. However, there are some challenges to DL\nmodels in cancer research because of the unavailability of a large data for\nadequate training and the difficulty of capturing heterogeneity in data at\ndifferent levels ranging from acquisition characteristics, session, to\nsubject-level (within subjects and across subjects). These challenges render DL\nmodels prone to overfitting and hence, models lack generalization on\nprospective subjects' data. In this work, we address these problems in the\napplication of B-cell Acute Lymphoblastic Leukemia (B-ALL) diagnosis using deep\nlearning. We propose heterogeneity loss that captures subject-level\nheterogeneity, thereby, forcing the neural network to learn subject-independent\nfeatures. We also propose an unorthodox ensemble strategy that helps us in\nproviding improved classification over models trained on 7-folds giving a\nweighted-$F_1$ score of 95.26% on unseen (test) subjects' data that are, so\nfar, the best results on the C-NMC 2019 dataset for B-ALL classification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:16:23 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:01:11 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Goswami", "Shubham", ""], ["Mehta", "Suril", ""], ["Sahrawat", "Dhruva", ""], ["Gupta", "Anubha", ""], ["Gupta", "Ritu", ""]]}, {"id": "2003.03305", "submitter": "Mikihiro Tanaka", "authors": "Mikihiro Tanaka, Tatsuya Harada", "title": "Captioning Images with Novel Objects via Online Vocabulary Expansion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we introduce a low cost method for generating descriptions\nfrom images containing novel objects. Generally, constructing a model, which\ncan explain images with novel objects, is costly because of the following: (1)\ncollecting a large amount of data for each category, and (2) retraining the\nentire system. If humans see a small number of novel objects, they are able to\nestimate their properties by associating their appearance with known objects.\nAccordingly, we propose a method that can explain images with novel objects\nwithout retraining using the word embeddings of the objects estimated from only\na small number of image features of the objects. The method can be integrated\nwith general image-captioning models. The experimental results show the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:34:15 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Tanaka", "Mikihiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2003.03369", "submitter": "Xiao Luo", "authors": "Xiao Luo, Daqing Wu, Chong Chen, Minghua Deng, Jianqiang Huang,\n  Xian-Sheng Hua", "title": "A Survey on Deep Hashing Methods", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor search is to find the data points in the database such that\nthe distances from them to the query are the smallest, which is a fundamental\nproblem in various domains, such as computer vision, recommendation systems and\nmachine learning. Hashing is one of the most widely used methods for its\ncomputational and storage efficiency. With the development of deep learning,\ndeep hashing methods show more advantages than traditional methods. In this\npaper, we present a comprehensive survey of the deep hashing algorithms.\nSpecifically, we categorize deep supervised hashing methods into pairwise\nsimilarity preserving, multiwise similarity preserving, implicit similarity\npreserving, classification-oriented preserving as well as quantization\naccording to the manners of preserving the similarities. In addition, we also\nintroduce some other topics such as deep unsupervised hashing and multi-modal\ndeep hashing methods. Meanwhile, we also present some commonly used public\ndatasets and the scheme to measure the performance of deep hashing algorithms.\nFinally, we discussed some potential research directions in conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 08:25:15 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:31:45 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Luo", "Xiao", ""], ["Wu", "Daqing", ""], ["Chen", "Chong", ""], ["Deng", "Minghua", ""], ["Huang", "Jianqiang", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2003.03376", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Noha Ghatwary, Barbara Braden, Dominique Lamarque, Adam\n  Bailey, Stefano Realdon, Renato Cannizzaro, Jens Rittscher, Christian Daul,\n  James East", "title": "Endoscopy disease detection challenge 2020", "comments": null, "journal-ref": null, "doi": "10.21227/f8xg-wb80", "report-no": "EDD2020 Dataset", "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Whilst many technologies are built around endoscopy, there is a need to have\na comprehensive dataset collected from multiple centers to address the\ngeneralization issues with most deep learning frameworks. What could be more\nimportant than disease detection and localization? Through our extensive\nnetwork of clinical and computational experts, we have collected, curated and\nannotated gastrointestinal endoscopy video frames. We have released this\ndataset and have launched disease detection and segmentation challenge EDD2020\nhttps://edd2020.grand-challenge.org to address the limitations and explore new\ndirections. EDD2020 is a crowd sourcing initiative to test the feasibility of\nrecent deep learning methods and to promote research for building robust\ntechnologies. In this paper, we provide an overview of the EDD2020 dataset,\nchallenge tasks, evaluation strategies and a short summary of results on test\ndata. A detailed paper will be drafted after the challenge workshop with more\ndetailed analysis of the results.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 00:41:28 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ali", "Sharib", ""], ["Ghatwary", "Noha", ""], ["Braden", "Barbara", ""], ["Lamarque", "Dominique", ""], ["Bailey", "Adam", ""], ["Realdon", "Stefano", ""], ["Cannizzaro", "Renato", ""], ["Rittscher", "Jens", ""], ["Daul", "Christian", ""], ["East", "James", ""]]}, {"id": "2003.03396", "submitter": "Eduardo Carvalho Mr.", "authors": "Eduardo D C Carvalho, Ronald Clark, Andrea Nicastro, Paul H J Kelly", "title": "Scalable Uncertainty for Computer Vision with Functional Variational\n  Inference", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Deep Learning continues to yield successful applications in Computer\nVision, the ability to quantify all forms of uncertainty is a paramount\nrequirement for its safe and reliable deployment in the real-world. In this\nwork, we leverage the formulation of variational inference in function space,\nwhere we associate Gaussian Processes (GPs) to both Bayesian CNN priors and\nvariational family. Since GPs are fully determined by their mean and covariance\nfunctions, we are able to obtain predictive uncertainty estimates at the cost\nof a single forward pass through any chosen CNN architecture and for any\nsupervised learning task. By leveraging the structure of the induced covariance\nmatrices, we propose numerically efficient algorithms which enable fast\ntraining in the context of high-dimensional tasks such as depth estimation and\nsemantic segmentation. Additionally, we provide sufficient conditions for\nconstructing regression loss functions whose probabilistic counterparts are\ncompatible with aleatoric uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 19:09:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Carvalho", "Eduardo D C", ""], ["Clark", "Ronald", ""], ["Nicastro", "Andrea", ""], ["Kelly", "Paul H J", ""]]}, {"id": "2003.03461", "submitter": "Weili Nie", "authors": "Weili Nie, Tero Karras, Animesh Garg, Shoubhik Debnath, Anjul Patney,\n  Ankit B. Patel, Anima Anandkumar", "title": "Semi-Supervised StyleGAN for Disentanglement Learning", "comments": "ICML 2020, 21 pages. Project page:\n  https://sites.google.com/nvidia.com/semi-stylegan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disentanglement learning is crucial for obtaining disentangled\nrepresentations and controllable generation. Current disentanglement methods\nface several inherent limitations: difficulty with high-resolution images,\nprimarily focusing on learning disentangled representations, and\nnon-identifiability due to the unsupervised setting. To alleviate these\nlimitations, we design new architectures and loss functions based on StyleGAN\n(Karras et al., 2019), for semi-supervised high-resolution disentanglement\nlearning. We create two complex high-resolution synthetic datasets for\nsystematic testing. We investigate the impact of limited supervision and find\nthat using only 0.25%~2.5% of labeled data is sufficient for good\ndisentanglement on both synthetic and real datasets. We propose new metrics to\nquantify generator controllability, and observe there may exist a crucial\ntrade-off between disentangled representation learning and controllable\ngeneration. We also consider semantic fine-grained image editing to achieve\nbetter generalization to unseen images.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 22:54:46 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:48:41 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 23:06:53 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Nie", "Weili", ""], ["Karras", "Tero", ""], ["Garg", "Animesh", ""], ["Debnath", "Shoubhik", ""], ["Patney", "Anjul", ""], ["Patel", "Ankit B.", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2003.03472", "submitter": "Jingpei Lu", "authors": "Jingpei Lu, Ambareesh Jayakumari, Florian Richter, Yang Li, Michael C.\n  Yip", "title": "SuPer Deep: A Surgical Perception Framework for Robotic Tissue\n  Manipulation using Deep Learning for Feature Extraction", "comments": "7 pages, 7 figures, ICRA 2021 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic automation in surgery requires precise tracking of surgical tools and\nmapping of deformable tissue. Previous works on surgical perception frameworks\nrequire significant effort in developing features for surgical tool and tissue\ntracking. In this work, we overcome the challenge by exploiting deep learning\nmethods for surgical perception. We integrated deep neural networks, capable of\nefficient feature extraction, into the tissue reconstruction and instrument\npose estimation processes. By leveraging transfer learning, the deep learning\nbased approach requires minimal training data and reduced feature engineering\nefforts to fully perceive a surgical scene. The framework was tested on three\npublicly available datasets, which use the da Vinci Surgical System, for\ncomprehensive analysis. Experimental results show that our framework achieves\nstate-of-the-art tracking performance in a surgical environment by utilizing\ndeep learning for feature extraction.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 00:08:30 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 00:05:33 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 05:41:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lu", "Jingpei", ""], ["Jayakumari", "Ambareesh", ""], ["Richter", "Florian", ""], ["Li", "Yang", ""], ["Yip", "Michael C.", ""]]}, {"id": "2003.03473", "submitter": "Shashank Tripathi", "authors": "Shashank Tripathi, Siddhant Ranade, Ambrish Tyagi, Amit Agrawal", "title": "PoseNet3D: Learning Temporally Consistent 3D Human Pose via Knowledge\n  Distillation", "comments": "Accepted as Oral in 3DV 2020; supplementary material included; added\n  results on 3DPW dataset in revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recovering 3D human pose from 2D joints is a highly unconstrained problem. We\npropose a novel neural network framework, PoseNet3D, that takes 2D joints as\ninput and outputs 3D skeletons and SMPL body model parameters. By casting our\nlearning approach in a student-teacher framework, we avoid using any 3D data\nsuch as paired/unpaired 3D data, motion capture sequences, depth images or\nmulti-view images during training. We first train a teacher network that\noutputs 3D skeletons, using only 2D poses for training. The teacher network\ndistills its knowledge to a student network that predicts 3D pose in SMPL\nrepresentation. Finally, both the teacher and the student networks are jointly\nfine-tuned in an end-to-end manner using temporal, self-consistency and\nadversarial losses, improving the accuracy of each individual network. Results\non Human3.6M dataset for 3D human pose estimation demonstrate that our approach\nreduces the 3D joint prediction error by 18% compared to previous unsupervised\nmethods. Qualitative results on in-the-wild datasets show that the recovered 3D\nposes and meshes are natural, realistic, and flow smoothly over consecutive\nframes.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 00:10:59 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 05:07:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Tripathi", "Shashank", ""], ["Ranade", "Siddhant", ""], ["Tyagi", "Ambrish", ""], ["Agrawal", "Amit", ""]]}, {"id": "2003.03488", "submitter": "Zechun Liu", "authors": "Zechun Liu and Zhiqiang Shen and Marios Savvides and Kwang-Ting Cheng", "title": "ReActNet: Towards Precise Binary Neural Network with Generalized\n  Activation Functions", "comments": "Accepted to ECCV 2020. Code is available at:\n  https://github.com/liuzechun/ReActNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose several ideas for enhancing a binary network to\nclose its accuracy gap from real-valued networks without incurring any\nadditional computational cost. We first construct a baseline network by\nmodifying and binarizing a compact real-valued network with parameter-free\nshortcuts, bypassing all the intermediate convolutional layers including the\ndownsampling layers. This baseline network strikes a good trade-off between\naccuracy and efficiency, achieving superior performance than most of existing\nbinary networks at approximately half of the computational cost. Through\nextensive experiments and analysis, we observed that the performance of binary\nnetworks is sensitive to activation distribution variations. Based on this\nimportant observation, we propose to generalize the traditional Sign and PReLU\nfunctions, denoted as RSign and RPReLU for the respective generalized\nfunctions, to enable explicit learning of the distribution reshape and shift at\nnear-zero extra cost. Lastly, we adopt a distributional loss to further enforce\nthe binary network to learn similar output distributions as those of a\nreal-valued network. We show that after incorporating all these ideas, the\nproposed ReActNet outperforms all the state-of-the-arts by a large margin.\nSpecifically, it outperforms Real-to-Binary Net and MeliusNet29 by 4.0% and\n3.6% respectively for the top-1 accuracy and also reduces the gap to its\nreal-valued counterpart to within 3.0% top-1 accuracy on ImageNet dataset. Code\nand models are available at: https://github.com/liuzechun/ReActNet.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 02:12:02 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 03:05:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Liu", "Zechun", ""], ["Shen", "Zhiqiang", ""], ["Savvides", "Marios", ""], ["Cheng", "Kwang-Ting", ""]]}, {"id": "2003.03489", "submitter": "Yuxin Zhang", "authors": "Yuxin Zhang, Zuquan Zheng, Roland Hu", "title": "Super Resolution Using Segmentation-Prior Self-Attention Generative\n  Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) is intensively implemented to solve super\nresolution (SR) tasks because of its superior performance. However, the problem\nof super resolution is still challenging due to the lack of prior knowledge and\nsmall receptive field of CNN. We propose the Segmentation-Piror Self-Attention\nGenerative Adversarial Network (SPSAGAN) to combine segmentation-priors and\nfeature attentions into a unified framework. This combination is led by a\ncarefully designed weighted addition to balance the influence of feature and\nsegmentation attentions, so that the network can emphasize textures in the same\nsegmentation category and meanwhile focus on the long-distance feature\nrelationship. We also propose a lightweight skip connection architecture called\nResidual-in-Residual Sparse Block (RRSB) to further improve the\nsuper-resolution performance and save computation. Extensive experiments show\nthat SPSAGAN can generate more realistic and visually pleasing textures\ncompared to state-of-the-art SFTGAN and ESRGAN on many SR datasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 02:13:14 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Yuxin", ""], ["Zheng", "Zuquan", ""], ["Hu", "Roland", ""]]}, {"id": "2003.03492", "submitter": "Wensheng Cheng", "authors": "Wensheng Cheng, Yan Zhang, Xu Lei, Wen Yang, Guisong Xia", "title": "Semantic Change Pattern Analysis", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is an important problem in vision field, especially for\naerial images. However, most works focus on traditional change detection, i.e.,\nwhere changes happen, without considering the change type information, i.e.,\nwhat changes happen. Although a few works have tried to apply semantic\ninformation to traditional change detection, they either only give the label of\nemerging objects without taking the change type into consideration, or set some\nkinds of change subjectively without specifying semantic information. To make\nuse of semantic information and analyze change types comprehensively, we\npropose a new task called semantic change pattern analysis for aerial images.\nGiven a pair of co-registered aerial images, the task requires a result\nincluding both where and what changes happen. We then describe the metric\nadopted for the task, which is clean and interpretable. We further provide the\nfirst well-annotated aerial image dataset for this task. Extensive baseline\nexperiments are conducted as reference for following works. The aim of this\nwork is to explore high-level information based on change detection and\nfacilitate the development of this field with the publicly available dataset.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 02:22:19 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Cheng", "Wensheng", ""], ["Zhang", "Yan", ""], ["Lei", "Xu", ""], ["Yang", "Wen", ""], ["Xia", "Guisong", ""]]}, {"id": "2003.03497", "submitter": "Yan Hong", "authors": "Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang", "title": "MatchingGAN: Matching-based Few-shot Image Generation", "comments": "This paper is accepted for oral presentation at ICME\n  2020(http://www.2020.ieeeicme.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To generate new images for a given category, most deep generative models\nrequire abundant training images from this category, which are often too\nexpensive to acquire. To achieve the goal of generation based on only a few\nimages, we propose matching-based Generative Adversarial Network (GAN) for\nfew-shot generation, which includes a matching generator and a matching\ndiscriminator. Matching generator can match random vectors with a few\nconditional images from the same category and generate new images for this\ncategory based on the fused features. The matching discriminator extends\nconventional GAN discriminator by matching the feature of generated image with\nthe fused feature of conditional images. Extensive experiments on three\ndatasets demonstrate the effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:09:06 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 03:17:32 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Hong", "Yan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhang", "Liqing", ""]]}, {"id": "2003.03500", "submitter": "Xiaojie Qi", "authors": "Xiaojie Qi", "title": "Weight mechanism: adding a constant in concatenation of series connect", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a consensus that feature maps in the shallow layer are more related to\nimage attributes such as texture and shape, whereas abstract semantic\nrepresentation exists in the deep layer. Meanwhile, some image information will\nbe lost in the process of the convolution operation. Naturally, the direct\nmethod is combining them together to gain lost detailed information through\nconcatenation or adding. In fact, the image representation flowed in feature\nfusion can not match with the semantic representation completely, and the\nsemantic deviation in different layers also destroy the information\npurification, that leads to useless information being mixed into the fusion\nlayers. Therefore, it is crucial to narrow the gap among the fused layers and\nreduce the impact of noises during fusion. In this paper, we propose a method\nnamed weight mechanism to reduce the gap between feature maps in concatenation\nof series connection, and we get a better result of 0.80% mIoU improvement on\nMassachusetts building dataset by changing the weight of the concatenation of\nseries connection in residual U-Net. Specifically, we design a new architecture\nnamed fused U-Net to test weight mechanism, and it also gains 0.12% mIoU\nimprovement.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:13:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 13:47:43 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Qi", "Xiaojie", ""]]}, {"id": "2003.03501", "submitter": "Palash Goyal", "authors": "Palash Goyal, Saurabh Sahu, Shalini Ghosh, Chul Lee", "title": "Cross-modal Learning for Multi-modal Video Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal machine learning (ML) models can process data in multiple\nmodalities (e.g., video, audio, text) and are useful for video content analysis\nin a variety of problems (e.g., object detection, scene understanding, activity\nrecognition). In this paper, we focus on the problem of video categorization\nusing a multi-modal ML technique. In particular, we have developed a novel\nmulti-modal ML approach that we call \"cross-modal learning\", where one modality\ninfluences another but only when there is correlation between the modalities --\nfor that, we first train a correlation tower that guides the main multi-modal\nvideo categorization tower in the model. We show how this cross-modal principle\ncan be applied to different types of models (e.g., RNN, Transformer, NetVLAD),\nand demonstrate through experiments how our proposed multi-modal video\ncategorization models with cross-modal learning out-perform strong\nstate-of-the-art baseline models.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:21:15 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 23:18:26 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2020 00:36:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Goyal", "Palash", ""], ["Sahu", "Saurabh", ""], ["Ghosh", "Shalini", ""], ["Lee", "Chul", ""]]}, {"id": "2003.03518", "submitter": "Bowen Wen", "authors": "Bowen Wen, Chaitanya Mitash, Sruthi Soorian, Andrew Kimmel, Avishai\n  Sintov and Kostas E. Bekris", "title": "Robust, Occlusion-aware Pose Estimation for Objects Grasped by Adaptive\n  Hands", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Automation (ICRA)\n  2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many manipulation tasks, such as placement or within-hand manipulation,\nrequire the object's pose relative to a robot hand. The task is difficult when\nthe hand significantly occludes the object. It is especially hard for adaptive\nhands, for which it is not easy to detect the finger's configuration. In\naddition, RGB-only approaches face issues with texture-less objects or when the\nhand and the object look similar. This paper presents a depth-based framework,\nwhich aims for robust pose estimation and short response times. The approach\ndetects the adaptive hand's state via efficient parallel search given the\nhighest overlap between the hand's model and the point cloud. The hand's point\ncloud is pruned and robust global registration is performed to generate object\npose hypotheses, which are clustered. False hypotheses are pruned via physical\nreasoning. The remaining poses' quality is evaluated given agreement with\nobserved data. Extensive evaluation on synthetic and real data demonstrates the\naccuracy and computational efficiency of the framework when applied on\nchallenging, highly-occluded scenarios for different object types. An ablation\nstudy identifies how the framework's components help in performance. This work\nalso provides a dataset for in-hand 6D object pose estimation. Code and dataset\nare available at: https://github.com/wenbowen123/icra20-hand-object-pose\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 05:51:03 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wen", "Bowen", ""], ["Mitash", "Chaitanya", ""], ["Soorian", "Sruthi", ""], ["Kimmel", "Andrew", ""], ["Sintov", "Avishai", ""], ["Bekris", "Kostas E.", ""]]}, {"id": "2003.03519", "submitter": "Hanting Chen", "authors": "Hanting Chen, Yunhe Wang, Han Shu, Changyuan Wen, Chunjing Xu, Boxin\n  Shi, Chao Xu, Chang Xu", "title": "Distilling portable Generative Adversarial Networks for Image\n  Translation", "comments": null, "journal-ref": "AAAI 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite Generative Adversarial Networks (GANs) have been widely used in\nvarious image-to-image translation tasks, they can be hardly applied on mobile\ndevices due to their heavy computation and storage cost. Traditional network\ncompression methods focus on visually recognition tasks, but never deal with\ngeneration tasks. Inspired by knowledge distillation, a student generator of\nfewer parameters is trained by inheriting the low-level and high-level\ninformation from the original heavy teacher generator. To promote the\ncapability of student generator, we include a student discriminator to measure\nthe distances between real images, and images generated by student and teacher\ngenerators. An adversarial learning process is therefore established to\noptimize student generator and student discriminator. Qualitative and\nquantitative analysis by conducting experiments on benchmark datasets\ndemonstrate that the proposed method can learn portable generative models with\nstrong performance.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 05:53:01 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Hanting", ""], ["Wang", "Yunhe", ""], ["Shu", "Han", ""], ["Wen", "Changyuan", ""], ["Xu", "Chunjing", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Xu", "Chang", ""]]}, {"id": "2003.03522", "submitter": "Tingbo Hou", "authors": "Tingbo Hou, Adel Ahmadyan, Liangkai Zhang, Jianing Wei, and Matthias\n  Grundmann", "title": "MobilePose: Real-Time Pose Estimation for Unseen Objects with Weak Shape\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of detecting unseen objects from RGB\nimages and estimating their poses in 3D. We propose two mobile friendly\nnetworks: MobilePose-Base and MobilePose-Shape. The former is used when there\nis only pose supervision, and the latter is for the case when shape supervision\nis available, even a weak one. We revisit shape features used in previous\nmethods, including segmentation and coordinate map. We explain when and why\npixel-level shape supervision can improve pose estimation. Consequently, we add\nshape prediction as an intermediate layer in the MobilePose-Shape, and let the\nnetwork learn pose from shape. Our models are trained on mixed real and\nsynthetic data, with weak and noisy shape supervision. They are ultra\nlightweight that can run in real-time on modern mobile devices (e.g. 36 FPS on\nGalaxy S20). Comparing with previous single-shot solutions, our method has\nhigher accuracy, while using a significantly smaller model (2~3% in model size\nor number of parameters).\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 06:23:35 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Hou", "Tingbo", ""], ["Ahmadyan", "Adel", ""], ["Zhang", "Liangkai", ""], ["Wei", "Jianing", ""], ["Grundmann", "Matthias", ""]]}, {"id": "2003.03530", "submitter": "Wen Wang", "authors": "Wen Wang, Xiaojiang Peng, Yanzhou Su, Yu Qiao, Jian Cheng", "title": "TTPP: Temporal Transformer with Progressive Prediction for Efficient\n  Action Anticipation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video action anticipation aims to predict future action categories from\nobserved frames. Current state-of-the-art approaches mainly resort to recurrent\nneural networks to encode history information into hidden states, and predict\nfuture actions from the hidden representations. It is well known that the\nrecurrent pipeline is inefficient in capturing long-term information which may\nlimit its performance in predication task. To address this problem, this paper\nproposes a simple yet efficient Temporal Transformer with Progressive\nPrediction (TTPP) framework, which repurposes a Transformer-style architecture\nto aggregate observed features, and then leverages a light-weight network to\nprogressively predict future features and actions. Specifically, predicted\nfeatures along with predicted probabilities are accumulated into the inputs of\nsubsequent prediction. We evaluate our approach on three action datasets,\nnamely TVSeries, THUMOS-14, and TV-Human-Interaction. Additionally we also\nconduct a comprehensive study for several popular aggregation and prediction\nstrategies. Extensive results show that TTPP not only outperforms the\nstate-of-the-art methods but also more efficient.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 07:59:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wang", "Wen", ""], ["Peng", "Xiaojiang", ""], ["Su", "Yanzhou", ""], ["Qiao", "Yu", ""], ["Cheng", "Jian", ""]]}, {"id": "2003.03537", "submitter": "Bijju Veduruparthi Mr", "authors": "Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das,\n  Moses Arunsingh, Raj Kumar Shrimali, Sriram Prasath, Soumendranath Ray and\n  Sanjay Chatterjee", "title": "Novel Radiomic Feature for Survival Prediction of Lung Cancer Patients\n  using Low-Dose CBCT Images", "comments": "Under review in SPIE Journal of Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Prediction of survivability in a patient for tumor progression is useful to\nestimate the effectiveness of a treatment protocol. In our work, we present a\nmodel to take into account the heterogeneous nature of a tumor to predict\nsurvival. The tumor heterogeneity is measured in terms of its mass by combining\ninformation regarding the radiodensity obtained in images with the gross tumor\nvolume (GTV). We propose a novel feature called Tumor Mass within a GTV (TMG),\nthat improves the prediction of survivability, compared to existing models\nwhich use GTV. Weekly variation in TMG of a patient is computed from the image\ndata and also estimated from a cell survivability model. The parameters\nobtained from the cell survivability model are indicatives of changes in TMG\nover the treatment period. We use these parameters along with other patient\nmetadata to perform survival analysis and regression. Cox's Proportional Hazard\nsurvival regression was performed using these data. Significant improvement in\nthe average concordance index from 0.47 to 0.64 was observed when TMG is used\nin the model instead of GTV. The experiments show that there is a difference in\nthe treatment response in responsive and non-responsive patients and that the\nproposed method can be used to predict patient survivability.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 08:47:26 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Veduruparthi", "Bijju Kranthi", ""], ["Mukherjee", "Jayanta", ""], ["Das", "Partha Pratim", ""], ["Arunsingh", "Moses", ""], ["Shrimali", "Raj Kumar", ""], ["Prasath", "Sriram", ""], ["Ray", "Soumendranath", ""], ["Chatterjee", "Sanjay", ""]]}, {"id": "2003.03545", "submitter": "Zhikang Zou", "authors": "Zhikang Zou and Yifan Liu and Shuangjie Xu and Wei Wei and Shiping Wen\n  and Pan Zhou", "title": "Crowd Counting via Hierarchical Scale Recalibration Network", "comments": "Accepted by ECAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of crowd counting is extremely challenging due to complicated\ndifficulties, especially the huge variation in vision scale. Previous works\ntend to adopt a naive concatenation of multi-scale information to tackle it,\nwhile the scale shifts between the feature maps are ignored. In this paper, we\npropose a novel Hierarchical Scale Recalibration Network (HSRNet), which\naddresses the above issues by modeling rich contextual dependencies and\nrecalibrating multiple scale-associated information. Specifically, a Scale\nFocus Module (SFM) first integrates global context into local features by\nmodeling the semantic inter-dependencies along channel and spatial dimensions\nsequentially. In order to reallocate channel-wise feature responses, a Scale\nRecalibration Module (SRM) adopts a step-by-step fusion to generate final\ndensity maps. Furthermore, we propose a novel Scale Consistency loss to\nconstrain that the scale-associated outputs are coherent with groundtruth of\ndifferent scales. With the proposed modules, our approach can ignore various\nnoises selectively and focus on appropriate crowd scales automatically.\nExtensive experiments on crowd counting datasets (ShanghaiTech, MALL,\nWorldEXPO'10, and UCSD) show that our HSRNet can deliver superior results over\nall state-of-the-art approaches. More remarkably, we extend experiments on an\nextra vehicle dataset, whose results indicate that the proposed model is\ngeneralized to other applications.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 10:06:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zou", "Zhikang", ""], ["Liu", "Yifan", ""], ["Xu", "Shuangjie", ""], ["Wei", "Wei", ""], ["Wen", "Shiping", ""], ["Zhou", "Pan", ""]]}, {"id": "2003.03551", "submitter": "Aihua Mao", "authors": "Aihua Mao, Canglan Dai, Lin Gao, Ying He, Yong-jin Liu", "title": "STD-Net: Structure-preserving and Topology-adaptive Deformation Network\n  for 3D Reconstruction from a Single Image", "comments": "14 pages,5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from a single view image is a long-standing prob-lem in\ncomputer vision. Various methods based on different shape representations(such\nas point cloud or volumetric representations) have been proposed. However,the\n3D shape reconstruction with fine details and complex structures are still\nchal-lenging and have not yet be solved. Thanks to the recent advance of the\ndeepshape representations, it becomes promising to learn the structure and\ndetail rep-resentation using deep neural networks. In this paper, we propose a\nnovel methodcalled STD-Net to reconstruct the 3D models utilizing the mesh\nrepresentationthat is well suitable for characterizing complex structure and\ngeometry details.To reconstruct complex 3D mesh models with fine details, our\nmethod consists of(1) an auto-encoder network for recovering the structure of\nan object with bound-ing box representation from a single image, (2) a\ntopology-adaptive graph CNNfor updating vertex position for meshes of complex\ntopology, and (3) an unifiedmesh deformation block that deforms the structural\nboxes into structure-awaremeshed models. Experimental results on the images\nfrom ShapeNet show that ourproposed STD-Net has better performance than other\nstate-of-the-art methods onreconstructing 3D objects with complex structures\nand fine geometric details.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 11:02:47 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Mao", "Aihua", ""], ["Dai", "Canglan", ""], ["Gao", "Lin", ""], ["He", "Ying", ""], ["Liu", "Yong-jin", ""]]}, {"id": "2003.03570", "submitter": "Bin Zhu", "authors": "Bin Zhu, Qing Song, Lu Yang, Zhihui Wang, Chun Liu, Mengjie Hu", "title": "CPM R-CNN: Calibrating Point-guided Misalignment in Object Detection", "comments": "Accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, offset-guided and point-guided regression dominate\nanchor-based and anchor-free method separately. Recently, point-guided approach\nis introduced to anchor-based method. However, we observe points predicted by\nthis way are misaligned with matched region of proposals and score of\nlocalization, causing a notable gap in performance. In this paper, we propose\nCPM R-CNN which contains three efficient modules to optimize anchor-based\npoint-guided method. According to sufficient evaluations on the COCO dataset,\nCPM R-CNN is demonstrated efficient to improve the localization accuracy by\ncalibrating mentioned misalignment. Compared with Faster R-CNN and Grid R-CNN\nbased on ResNet-101 with FPN, our approach can substantially improve detection\nmAP by 3.3% and 1.5% respectively without whistles and bells. Moreover, our\nbest model achieves improvement by a large margin to 49.9% on COCO test-dev.\nCode and models will be publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 12:29:43 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 09:12:36 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zhu", "Bin", ""], ["Song", "Qing", ""], ["Yang", "Lu", ""], ["Wang", "Zhihui", ""], ["Liu", "Chun", ""], ["Hu", "Mengjie", ""]]}, {"id": "2003.03581", "submitter": "Vladimir Ivashkin", "authors": "Yuri Viazovetskyi, Vladimir Ivashkin, Evgeny Kashin", "title": "StyleGAN2 Distillation for Feed-forward Image Manipulation", "comments": "Camera ready ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StyleGAN2 is a state-of-the-art network in generating realistic images.\nBesides, it was explicitly trained to have disentangled directions in latent\nspace, which allows efficient image manipulation by varying latent factors.\nEditing existing images requires embedding a given image into the latent space\nof StyleGAN2. Latent code optimization via backpropagation is commonly used for\nqualitative embedding of real world images, although it is prohibitively slow\nfor many applications. We propose a way to distill a particular image\nmanipulation of StyleGAN2 into image-to-image network trained in paired way.\nThe resulting pipeline is an alternative to existing GANs, trained on unpaired\ndata. We provide results of human faces' transformation: gender swap,\naging/rejuvenation, style transfer and image morphing. We show that the quality\nof generation using our method is comparable to StyleGAN2 backpropagation and\ncurrent state-of-the-art methods in these particular tasks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 14:02:06 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 14:07:35 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Viazovetskyi", "Yuri", ""], ["Ivashkin", "Vladimir", ""], ["Kashin", "Evgeny", ""]]}, {"id": "2003.03603", "submitter": "Mingkui Tan", "authors": "Shoukai Xu, Haokun Li, Bohan Zhuang, Jing Liu, Jiezhang Cao, Chuangrun\n  Liang, Mingkui Tan", "title": "Generative Low-bitwidth Data Free Quantization", "comments": "eccv2020 code is available at: https://github.com/xushoukai/GDFQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization is an effective way to compress deep models and\nimprove their execution latency and energy efficiency, so that they can be\ndeployed on mobile or embedded devices. Existing quantization methods require\noriginal data for calibration or fine-tuning to get better performance.\nHowever, in many real-world scenarios, the data may not be available due to\nconfidential or private issues, thereby making existing quantization methods\nnot applicable. Moreover, due to the absence of original data, the recently\ndeveloped generative adversarial networks (GANs) cannot be applied to generate\ndata. Although the full-precision model may contain rich data information, such\ninformation alone is hard to exploit for recovering the original data or\ngenerating new meaningful data. In this paper, we investigate a\nsimple-yet-effective method called Generative Low-bitwidth Data Free\nQuantization (GDFQ) to remove the data dependence burden. Specifically, we\npropose a knowledge matching generator to produce meaningful fake data by\nexploiting classification boundary knowledge and distribution information in\nthe pre-trained model. With the help of generated data, we can quantize a model\nby learning knowledge from the pre-trained model. Extensive experiments on\nthree data sets demonstrate the effectiveness of our method. More critically,\nour method achieves much higher accuracy on 4-bit quantization than the\nexisting data free quantization method. Code is available at\nhttps://github.com/xushoukai/GDFQ.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 16:38:34 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 02:47:42 GMT"}, {"version": "v3", "created": "Mon, 10 Aug 2020 12:56:06 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Xu", "Shoukai", ""], ["Li", "Haokun", ""], ["Zhuang", "Bohan", ""], ["Liu", "Jing", ""], ["Cao", "Jiezhang", ""], ["Liang", "Chuangrun", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.03608", "submitter": "Haifeng Li", "authors": "Jie Chen, Ziyang Yuan, Jian Peng, Li Chen, Haozhe Huang, Jiawei Zhu,\n  Yu Liu, Haifeng Li", "title": "DASNet: Dual attentive fully convolutional siamese networks for change\n  detection of high resolution satellite images", "comments": "12 pages, 13 figures, 5 tables", "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing. 2020", "doi": "10.1109/JSTARS.2020.3037893", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection is a basic task of remote sensing image processing. The\nresearch objective is to identity the change information of interest and filter\nout the irrelevant change information as interference factors. Recently, the\nrise of deep learning has provided new tools for change detection, which have\nyielded impressive results. However, the available methods focus mainly on the\ndifference information between multitemporal remote sensing images and lack\nrobustness to pseudo-change information. To overcome the lack of resistance of\ncurrent methods to pseudo-changes, in this paper, we propose a new method,\nnamely, dual attentive fully convolutional Siamese networks (DASNet) for change\ndetection in high-resolution images. Through the dual-attention mechanism,\nlong-range dependencies are captured to obtain more discriminant feature\nrepresentations to enhance the recognition performance of the model. Moreover,\nthe imbalanced sample is a serious problem in change detection, i.e. unchanged\nsamples are much more than changed samples, which is one of the main reasons\nresulting in pseudo-changes. We put forward the weighted double margin\ncontrastive loss to address this problem by punishing the attention to\nunchanged feature pairs and increase attention to changed feature pairs. The\nexperimental results of our method on the change detection dataset (CDD) and\nthe building change detection dataset (BCDD) demonstrate that compared with\nother baseline methods, the proposed method realizes maximum improvements of\n2.1\\% and 3.6\\%, respectively, in the F1 score. Our Pytorch implementation is\navailable at https://github.com/lehaifeng/DASNet.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 16:57:10 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 04:32:22 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Chen", "Jie", ""], ["Yuan", "Ziyang", ""], ["Peng", "Jian", ""], ["Chen", "Li", ""], ["Huang", "Haozhe", ""], ["Zhu", "Jiawei", ""], ["Liu", "Yu", ""], ["Li", "Haifeng", ""]]}, {"id": "2003.03613", "submitter": "Rishab Sharma Mr.", "authors": "Rishab Sharma, Rahul Deora and Anirudha Vishvakarma", "title": "AlphaNet: An Attention Guided Deep Network for Automatic Image Matting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end to end solution for image matting i.e\nhigh-precision extraction of foreground objects from natural images. Image\nmatting and background detection can be achieved easily through chroma keying\nin a studio setting when the background is either pure green or blue.\nNonetheless, image matting in natural scenes with complex and uneven depth\nbackgrounds remains a tedious task that requires human intervention. To achieve\ncomplete automatic foreground extraction in natural scenes, we propose a method\nthat assimilates semantic segmentation and deep image matting processes into a\nsingle network to generate detailed semantic mattes for image composition task.\nThe contribution of our proposed method is two-fold, firstly it can be\ninterpreted as a fully automated semantic image matting method and secondly as\na refinement of existing semantic segmentation models. We propose a novel model\narchitecture as a combination of segmentation and matting that unifies the\nfunction of upsampling and downsampling operators with the notion of attention.\nAs shown in our work, attention guided downsampling and upsampling can extract\nhigh-quality boundary details, unlike other normal downsampling and upsampling\ntechniques. For achieving the same, we utilized an attention guided\nencoder-decoder framework which does unsupervised learning for generating an\nattention map adaptively from the data to serve and direct the upsampling and\ndownsampling operators. We also construct a fashion e-commerce focused dataset\nwith high-quality alpha mattes to facilitate the training and evaluation for\nimage matting.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 17:25:21 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Sharma", "Rishab", ""], ["Deora", "Rahul", ""], ["Vishvakarma", "Anirudha", ""]]}, {"id": "2003.03616", "submitter": "James Murphy", "authors": "Lenore Cowen, Kapil Devkota, Xiaozhe Hu, James M. Murphy, and Kaiyi Wu", "title": "Diffusion State Distances: Multitemporal Analysis, Fast Algorithms, and\n  Applications to Biological Networks", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-dependent metrics are powerful tools for learning the underlying\nstructure of high-dimensional data. This article develops and analyzes a\ndata-dependent metric known as diffusion state distance (DSD), which compares\npoints using a data-driven diffusion process. Unlike related diffusion methods,\nDSDs incorporate information across time scales, which allows for the intrinsic\ndata structure to be inferred in a parameter-free manner. This article develops\na theory for DSD based on the multitemporal emergence of mesoscopic equilibria\nin the underlying diffusion process. New algorithms for denoising and dimension\nreduction with DSD are also proposed and analyzed. These approaches are based\non a weighted spectral decomposition of the underlying diffusion process, and\nexperiments on synthetic datasets and real biological networks illustrate the\nefficacy of the proposed algorithms in terms of both speed and accuracy.\nThroughout, comparisons with related methods are made, in order to illustrate\nthe distinct advantages of DSD for datasets exhibiting multiscale structure.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 17:43:34 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Cowen", "Lenore", ""], ["Devkota", "Kapil", ""], ["Hu", "Xiaozhe", ""], ["Murphy", "James M.", ""], ["Wu", "Kaiyi", ""]]}, {"id": "2003.03622", "submitter": "Quanshi Zhang", "authors": "Xu Cheng, Zhefan Rao, Yilan Chen, Quanshi Zhang", "title": "Explaining Knowledge Distillation by Quantifying the Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to interpret the success of knowledge\ndistillation by quantifying and analyzing task-relevant and task-irrelevant\nvisual concepts that are encoded in intermediate layers of a deep neural\nnetwork (DNN). More specifically, three hypotheses are proposed as follows. 1.\nKnowledge distillation makes the DNN learn more visual concepts than learning\nfrom raw data. 2. Knowledge distillation ensures that the DNN is prone to\nlearning various visual concepts simultaneously. Whereas, in the scenario of\nlearning from raw data, the DNN learns visual concepts sequentially. 3.\nKnowledge distillation yields more stable optimization directions than learning\nfrom raw data. Accordingly, we design three types of mathematical metrics to\nevaluate feature representations of the DNN. In experiments, we diagnosed\nvarious DNNs, and above hypotheses were verified.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:09:17 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Cheng", "Xu", ""], ["Rao", "Zhefan", ""], ["Chen", "Yilan", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2003.03633", "submitter": "Majed El Helou", "authors": "Majed El Helou, Frederike D\\\"umbgen, Sabine S\\\"usstrunk", "title": "AL2: Progressive Activation Loss for Learning General Representations in\n  Classification Neural Networks", "comments": null, "journal-ref": "IEEE International Conference on Acoustics, Speech, and Signal\n  Processing (ICASSP 2020)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large capacity of neural networks enables them to learn complex\nfunctions. To avoid overfitting, networks however require a lot of training\ndata that can be expensive and time-consuming to collect. A common practical\napproach to attenuate overfitting is the use of network regularization\ntechniques. We propose a novel regularization method that progressively\npenalizes the magnitude of activations during training. The combined activation\nsignals produced by all neurons in a given layer form the representation of the\ninput image in that feature space. We propose to regularize this representation\nin the last feature layer before classification layers. Our method's effect on\ngeneralization is analyzed with label randomization tests and cumulative\nablations. Experimental results show the advantages of our approach in\ncomparison with commonly-used regularizers on standard benchmark datasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:38:46 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Helou", "Majed El", ""], ["D\u00fcmbgen", "Frederike", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2003.03644", "submitter": "Zining Wang", "authors": "Zining Wang, Di Feng, Yiyang Zhou, Lars Rosenbaum, Fabian Timm, Klaus\n  Dietmayer, Masayoshi Tomizuka and Wei Zhan", "title": "Inferring Spatial Uncertainty in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of real-world datasets is the prerequisite for developing\nobject detection methods for autonomous driving. While ambiguity exists in\nobject labels due to error-prone annotation process or sensor observation\nnoises, current object detection datasets only provide deterministic\nannotations without considering their uncertainty. This precludes an in-depth\nevaluation among different object detection methods, especially for those that\nexplicitly model predictive probability. In this work, we propose a generative\nmodel to estimate bounding box label uncertainties from LiDAR point clouds, and\ndefine a new representation of the probabilistic bounding box through spatial\ndistribution. Comprehensive experiments show that the proposed model represents\nuncertainties commonly seen in driving scenarios. Based on the spatial\ndistribution, we further propose an extension of IoU, called the Jaccard IoU\n(JIoU), as a new evaluation metric that incorporates label uncertainty.\nExperiments on the KITTI and the Waymo Open Datasets show that JIoU is superior\nto IoU when evaluating probabilistic object detectors.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 19:29:43 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 07:11:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Zining", ""], ["Feng", "Di", ""], ["Zhou", "Yiyang", ""], ["Rosenbaum", "Lars", ""], ["Timm", "Fabian", ""], ["Dietmayer", "Klaus", ""], ["Tomizuka", "Masayoshi", ""], ["Zhan", "Wei", ""]]}, {"id": "2003.03653", "submitter": "Eren Aksoy", "authors": "Tiago Cortinhal, George Tzelepis and Eren Erdal Aksoy", "title": "SalsaNext: Fast, Uncertainty-aware Semantic Segmentation of LiDAR Point\n  Clouds for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce SalsaNext for the uncertainty-aware semantic\nsegmentation of a full 3D LiDAR point cloud in real-time. SalsaNext is the next\nversion of SalsaNet [1] which has an encoder-decoder architecture where the\nencoder unit has a set of ResNet blocks and the decoder part combines upsampled\nfeatures from the residual blocks. In contrast to SalsaNet, we introduce a new\ncontext module, replace the ResNet encoder blocks with a new residual dilated\nconvolution stack with gradually increasing receptive fields and add the\npixel-shuffle layer in the decoder. Additionally, we switch from stride\nconvolution to average pooling and also apply central dropout treatment. To\ndirectly optimize the Jaccard index, we further combine the weighted\ncross-entropy loss with Lovasz-Softmax loss [2]. We finally inject a Bayesian\ntreatment to compute the epistemic and aleatoric uncertainties for each point\nin the cloud. We provide a thorough quantitative evaluation on the\nSemantic-KITTI dataset [3], which demonstrates that the proposed SalsaNext\noutperforms other state-of-the-art semantic segmentation networks and ranks\nfirst on the Semantic-KITTI leaderboard. We also release our source code\nhttps://github.com/TiagoCortinhal/SalsaNext.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 20:17:06 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 15:37:28 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 10:07:58 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Cortinhal", "Tiago", ""], ["Tzelepis", "George", ""], ["Aksoy", "Eren Erdal", ""]]}, {"id": "2003.03669", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Jiajun Deng, Jiebo Luo", "title": "Adaptive Offline Quintuplet Loss for Image-Text Matching", "comments": "Accepted by ECCV 2020. Code is available at\n  https://github.com/sunnychencool/AOQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing image-text matching approaches typically leverage triplet loss with\nonline hard negatives to train the model. For each image or text anchor in a\ntraining mini-batch, the model is trained to distinguish between a positive and\nthe most confusing negative of the anchor mined from the mini-batch (i.e.\nonline hard negative). This strategy improves the model's capacity to discover\nfine-grained correspondences and non-correspondences between image and text\ninputs. However, the above approach has the following drawbacks: (1) the\nnegative selection strategy still provides limited chances for the model to\nlearn from very hard-to-distinguish cases. (2) The trained model has weak\ngeneralization capability from the training set to the testing set. (3) The\npenalty lacks hierarchy and adaptiveness for hard negatives with different\n\"hardness\" degrees. In this paper, we propose solutions by sampling negatives\noffline from the whole training set. It provides \"harder\" offline negatives\nthan online hard negatives for the model to distinguish. Based on the offline\nhard negatives, a quintuplet loss is proposed to improve the model's\ngeneralization capability to distinguish positives and negatives. In addition,\na novel loss function that combines the knowledge of positives, offline hard\nnegatives and online hard negatives is created. It leverages offline hard\nnegatives as the intermediary to adaptively penalize them based on their\ndistance relations to the anchor. We evaluate the proposed training approach on\nthree state-of-the-art image-text models on the MS-COCO and Flickr30K datasets.\nSignificant performance improvements are observed for all the models, proving\nthe effectiveness and generality of our approach. Code is available at\nhttps://github.com/sunnychencool/AOQ\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 22:09:11 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 19:20:03 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 14:58:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Chen", "Tianlang", ""], ["Deng", "Jiajun", ""], ["Luo", "Jiebo", ""]]}, {"id": "2003.03701", "submitter": "Yang Feng", "authors": "Yang Feng, Futang Peng, Xu Zhang, Wei Zhu, Shanfeng Zhang, Howard\n  Zhou, Zhen Li, Tom Duerig, Shih-Fu Chang, Jiebo Luo", "title": "Unifying Specialist Image Embedding into Universal Image Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep image embedding provides a way to measure the semantic similarity of two\nimages. It plays a central role in many applications such as image search, face\nverification, and zero-shot learning. It is desirable to have a universal deep\nembedding model applicable to various domains of images. However, existing\nmethods mainly rely on training specialist embedding models each of which is\napplicable to images from a single domain. In this paper, we study an important\nbut unexplored task: how to train a single universal image embedding model to\nmatch the performance of several specialists on each specialist's domain.\nSimply fusing the training data from multiple domains cannot solve this problem\nbecause some domains become overfitted sooner when trained together using\nexisting methods. Therefore, we propose to distill the knowledge in multiple\nspecialists into a universal embedding to solve this problem. In contrast to\nexisting embedding distillation methods that distill the absolute distances\nbetween images, we transform the absolute distances between images into a\nprobabilistic distribution and minimize the KL-divergence between the\ndistributions of the specialists and the universal embedding. Using several\npublic datasets, we validate that our proposed method accomplishes the goal of\nuniversal image embedding.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 02:51:11 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Feng", "Yang", ""], ["Peng", "Futang", ""], ["Zhang", "Xu", ""], ["Zhu", "Wei", ""], ["Zhang", "Shanfeng", ""], ["Zhou", "Howard", ""], ["Li", "Zhen", ""], ["Duerig", "Tom", ""], ["Chang", "Shih-Fu", ""], ["Luo", "Jiebo", ""]]}, {"id": "2003.03703", "submitter": "Dongxu Li", "authors": "Dongxu Li, Xin Yu, Chenchen Xu, Lars Petersson, Hongdong Li", "title": "Transferring Cross-domain Knowledge for Video Sign Language Recognition", "comments": "CVPR2020 (oral) preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word-level sign language recognition (WSLR) is a fundamental task in sign\nlanguage interpretation. It requires models to recognize isolated sign words\nfrom videos. However, annotating WSLR data needs expert knowledge, thus\nlimiting WSLR dataset acquisition. On the contrary, there are abundant\nsubtitled sign news videos on the internet. Since these videos have no\nword-level annotation and exhibit a large domain gap from isolated signs, they\ncannot be directly used for training WSLR models. We observe that despite the\nexistence of a large domain gap, isolated and news signs share the same visual\nconcepts, such as hand gestures and body movements. Motivated by this\nobservation, we propose a novel method that learns domain-invariant visual\nconcepts and fertilizes WSLR models by transferring knowledge of subtitled news\nsign to them. To this end, we extract news signs using a base WSLR model, and\nthen design a classifier jointly trained on news and isolated signs to coarsely\nalign these two domain features. In order to learn domain-invariant features\nwithin each class and suppress domain-specific features, our method further\nresorts to an external memory to store the class centroids of the aligned news\nsigns. We then design a temporal attention based on the learnt descriptor to\nimprove recognition performance. Experimental results on standard WSLR datasets\nshow that our method outperforms previous state-of-the-art methods\nsignificantly. We also demonstrate the effectiveness of our method on\nautomatically localizing signs from sign news, achieving 28.1 for AP@0.5.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:05:21 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:53:06 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Li", "Dongxu", ""], ["Yu", "Xin", ""], ["Xu", "Chenchen", ""], ["Petersson", "Lars", ""], ["Li", "Hongdong", ""]]}, {"id": "2003.03707", "submitter": "Shuo Yang", "authors": "Shuo Yang, Wei Yu, Ying Zheng, Hongxun Yao, Tao Mei", "title": "Adaptive Semantic-Visual Tree for Hierarchical Embeddings", "comments": null, "journal-ref": "The 27th ACM Multimedia (2019) 2097-2105", "doi": "10.1145/3343031.3350995", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Merchandise categories inherently form a semantic hierarchy with different\nlevels of concept abstraction, especially for fine-grained categories. This\nhierarchy encodes rich correlations among various categories across different\nlevels, which can effectively regularize the semantic space and thus make\npredictions less ambiguous. However, previous studies of fine-grained image\nretrieval primarily focus on semantic similarities or visual similarities. In a\nreal application, merely using visual similarity may not satisfy the need of\nconsumers to search merchandise with real-life images, e.g., given a red coat\nas a query image, we might get a red suit in recall results only based on\nvisual similarity since they are visually similar. But the users actually want\na coat rather than suit even the coat is with different color or texture\nattributes. We introduce this new problem based on photoshopping in real\npractice. That's why semantic information are integrated to regularize the\nmargins to make \"semantic\" prior to \"visual\". To solve this new problem, we\npropose a hierarchical adaptive semantic-visual tree (ASVT) to depict the\narchitecture of merchandise categories, which evaluates semantic similarities\nbetween different semantic levels and visual similarities within the same\nsemantic class simultaneously. The semantic information satisfies the demand of\nconsumers for similar merchandise with the query while the visual information\noptimizes the correlations within the semantic class. At each level, we set\ndifferent margins based on the semantic hierarchy and incorporate them as prior\ninformation to learn a fine-grained feature embedding. To evaluate our\nframework, we propose a new dataset named JDProduct, with hierarchical labels\ncollected from actual image queries and official merchandise images on an\nonline shopping application. Extensive experimental results on the public\nCARS196 and CUB-\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:36:42 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yang", "Shuo", ""], ["Yu", "Wei", ""], ["Zheng", "Ying", ""], ["Yao", "Hongxun", ""], ["Mei", "Tao", ""]]}, {"id": "2003.03710", "submitter": "Da Chen", "authors": "Li Liu, Da Chen, Minglei Shu, Baosheng Li, Huazhong Shu, Michel Paques\n  and Laurent D. Cohen", "title": "Trajectory Grouping with Curvature Regularization for Tubular Structure\n  Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tubular structure tracking is an important task in the fields of computer\nvision and medical image analysis. The minimal paths-based approaches have\nexhibited their powerful ability in tracing tubular structures, by which a\ntubular structure can be naturally treated as a minimal geodesic path computed\nwith a suitable geodesic metric. However, existing minimal paths-based tracing\napproaches still suffer from difficulty, for instances the shortcuts and short\nbranches combination problems, especially when dealing with the images\ninvolving complicated tubular tree structures or background. In this paper, we\nintroduce a new minimal paths-based model for minimally interactive tubular\nstructure centerline extraction in conjunction with a perceptual grouping\nscheme. Basically, we take into account the prescribed tubular trajectories and\ncurvature-penalized geodesic paths to seek favourable shortest paths. The\nproposed approach can benefit from the local smoothness prior on tubular\nstructures and the global optimality of the used graph-based path searching\nscheme. Experimental results on both synthetic and real images prove that the\nproposed model indeed obtains outperformance comparing with the\nstate-of-the-art minimal path-based tubular structure tracing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:40:26 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 13:27:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Li", ""], ["Chen", "Da", ""], ["Shu", "Minglei", ""], ["Li", "Baosheng", ""], ["Shu", "Huazhong", ""], ["Paques", "Michel", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "2003.03711", "submitter": "Shuo Yang", "authors": "Shuo Yang, Min Xu, Haozhe Xie, Stuart Perry, Jiahao Xia", "title": "Single-View 3D Object Reconstruction from Shape Priors in Memory", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for single-view 3D object reconstruction directly learn to\ntransform image features into 3D representations. However, these methods are\nvulnerable to images containing noisy backgrounds and heavy occlusions because\nthe extracted image features do not contain enough information to reconstruct\nhigh-quality 3D shapes. Humans routinely use incomplete or noisy visual cues\nfrom an image to retrieve similar 3D shapes from their memory and reconstruct\nthe 3D shape of an object. Inspired by this, we propose a novel method, named\nMem3D, that explicitly constructs shape priors to supplement the missing\ninformation in the image. Specifically, the shape priors are in the forms of\n\"image-voxel\" pairs in the memory network, which is stored by a well-designed\nwriting strategy during training. We also propose a voxel triplet loss function\nthat helps to retrieve the precise 3D shapes that are highly related to the\ninput image from shape priors. The LSTM-based shape encoder is introduced to\nextract information from the retrieved 3D shapes, which are useful in\nrecovering the 3D shape of an object that is heavily occluded or in complex\nenvironments. Experimental results demonstrate that Mem3D significantly\nimproves reconstruction quality and performs favorably against state-of-the-art\nmethods on the ShapeNet and Pix3D datasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 03:51:07 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 06:18:24 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 10:34:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yang", "Shuo", ""], ["Xu", "Min", ""], ["Xie", "Haozhe", ""], ["Perry", "Stuart", ""], ["Xia", "Jiahao", ""]]}, {"id": "2003.03715", "submitter": "Fangyi Zhu", "authors": "Fangyi Zhu, Jenq-Neng Hwang, Zhanyu Ma, Guang Chen, Jun Guo", "title": "OVC-Net: Object-Oriented Video Captioning with Temporal Graph and Detail\n  Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional video captioning requests a holistic description of the video,\nyet the detailed descriptions of the specific objects may not be available.\nWithout associating the moving trajectories, these image-based data-driven\nmethods cannot understand the activities from the spatio-temporal transitions\nin the inter-object visual features. Besides, adopting ambiguous clip-sentence\npairs in training, it goes against learning the multi-modal functional mappings\nowing to the one-to-many nature. In this paper, we propose a novel task to\nunderstand the videos in object-level, named object-oriented video captioning.\nWe introduce the video-based object-oriented video captioning network (OVC)-Net\nvia temporal graph and detail enhancement to effectively analyze the activities\nalong time and stably capture the vision-language connections under\nsmall-sample condition. The temporal graph provides useful supplement over\nprevious image-based approaches, allowing to reason the activities from the\ntemporal evolution of visual features and the dynamic movement of spatial\nlocations. The detail enhancement helps to capture the discriminative features\namong different objects, with which the subsequent captioning module can yield\nmore informative and precise descriptions. Thereafter, we construct a new\ndataset, providing consistent object-sentence pairs, to facilitate effective\ncross-modal learning. To demonstrate the effectiveness, we conduct experiments\non the new dataset and compare it with the state-of-the-art video captioning\nmethods. From the experimental results, the OVC-Net exhibits the ability of\nprecisely describing the concurrent objects, and achieves the state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 04:34:58 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 11:56:50 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 09:27:41 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 17:21:46 GMT"}, {"version": "v5", "created": "Tue, 14 Jul 2020 16:51:47 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhu", "Fangyi", ""], ["Hwang", "Jenq-Neng", ""], ["Ma", "Zhanyu", ""], ["Chen", "Guang", ""], ["Guo", "Jun", ""]]}, {"id": "2003.03717", "submitter": "Kanata Suzuki", "authors": "Kanata Suzuki, Yasuto Yokota, Yuzi Kanazawa, Tomoyoshi Takebayashi", "title": "Online Self-Supervised Learning for Object Picking: Detecting Optimum\n  Grasping Position using a Metric Learning Approach", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning methods are attractive candidates for automatic\nobject picking. However, the trial samples lack the complete ground truth\nbecause the observable parts of the agent are limited. That is, the information\ncontained in the trial samples is often insufficient to learn the specific\ngrasping position of each object. Consequently, the training falls into a local\nsolution, and the grasp positions learned by the robot are independent of the\nstate of the object. In this study, the optimal grasping position of an\nindividual object is determined from the grasping score, defined as the\ndistance in the feature space obtained using metric learning. The closeness of\nthe solution to the pre-designed optimal grasping position was evaluated in\ntrials. The proposed method incorporates two types of feedback control: one\nfeedback enlarges the grasping score when the grasping position approaches the\noptimum; the other reduces the negative feedback of the potential grasping\npositions among the grasping candidates. The proposed online self-supervised\nlearning method employs two deep neural networks. : SSD that detects the\ngrasping position of an object, and Siamese networks (SNs) that evaluate the\ntrial sample using the similarity of two input data in the feature space. Our\nmethod embeds the relation of each grasping position as feature vectors by\ntraining the trial samples and a few pre-samples indicating the optimum\ngrasping position. By incorporating the grasping score based on the feature\nspace of SNs into the SSD training process, the method preferentially trains\nthe optimum grasping position. In the experiment, the proposed method achieved\na higher success rate than the baseline method using simple teaching signals.\nAnd the grasping scores in the feature space of the SNs accurately represented\nthe grasping positions of the objects.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 04:36:24 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Suzuki", "Kanata", ""], ["Yokota", "Yasuto", ""], ["Kanazawa", "Yuzi", ""], ["Takebayashi", "Tomoyoshi", ""]]}, {"id": "2003.03744", "submitter": "Jinghua Zhang", "authors": "Jinghua Zhang, Chen Li, Frank Kulwa, Xin Zhao, Changhao Sun, Zihan Li,\n  Tao Jiang, Hong Li, and Shouliang Qi", "title": "A Multi-scale CNN-CRF Framework for Environmental Microorganism Image\n  Segmentation", "comments": null, "journal-ref": "BioMed Research International, vol. 2020, Article ID 4621403, 27\n  pages, 2020", "doi": "10.1155/2020/4621403", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assist researchers to identify Environmental Microorganisms (EMs)\neffectively, a Multiscale CNN-CRF (MSCC) framework for the EM image\nsegmentation is proposed in this paper. There are two parts in this framework:\nThe first is a novel pixel-level segmentation approach, using a newly\nintroduced Convolutional Neural Network (CNN), namely, \"mU-Net-B3\", with a\ndense Conditional Random Field (CRF) postprocessing. The second is a VGG-16\nbased patch-level segmentation method with a novel \"buffer\" strategy, which\nfurther improves the segmentation quality of the details of the EMs. In the\nexperiment, compared with the state-of-the-art methods on 420 EM images, the\nproposed MSCC method reduces the memory requirement from 355 MB to 103 MB,\nimproves the overall evaluation indexes (Dice, Jaccard, Recall, Accuracy) from\n85.24%, 77.42%, 82.27%, and 96.76% to 87.13%, 79.74%, 87.12%, and 96.91%,\nrespectively, and reduces the volume overlap error from 22.58% to 20.26%.\nTherefore, the MSCC method shows great potential in the EM segmentation field.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 08:30:30 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 12:32:51 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zhang", "Jinghua", ""], ["Li", "Chen", ""], ["Kulwa", "Frank", ""], ["Zhao", "Xin", ""], ["Sun", "Changhao", ""], ["Li", "Zihan", ""], ["Jiang", "Tao", ""], ["Li", "Hong", ""], ["Qi", "Shouliang", ""]]}, {"id": "2003.03749", "submitter": "Jia Chen", "authors": "Jia Chen, Qin Jin", "title": "Better Captioning with Sequence-Level Exploration", "comments": "accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-level learning objective has been widely used in captioning tasks to\nachieve the state-of-the-art performance for many models. In this objective,\nthe model is trained by the reward on the quality of its generated captions\n(sequence-level). In this work, we show the limitation of the current\nsequence-level learning objective for captioning tasks from both theory and\nempirical result. In theory, we show that the current objective is equivalent\nto only optimizing the precision side of the caption set generated by the model\nand therefore overlooks the recall side. Empirical result shows that the model\ntrained by this objective tends to get lower score on the recall side. We\npropose to add a sequence-level exploration term to the current objective to\nboost recall. It guides the model to explore more plausible captions in the\ntraining. In this way, the proposed objective takes both the precision and\nrecall sides of generated captions into account. Experiments show the\neffectiveness of the proposed method on both video and image captioning\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 09:08:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Jia", ""], ["Jin", "Qin", ""]]}, {"id": "2003.03756", "submitter": "Lone Wong", "authors": "Lone Wong, Deli Zhao, Shaohua Wan, Bo Zhang", "title": "Perceptual Image Super-Resolution with Progressive Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Single Image Super-Resolution (SISR) aims to improve resolution of small-size\nlow-quality image from a single one. With popularity of consumer electronics in\nour daily life, this topic has become more and more attractive. In this paper,\nwe argue that the curse of dimensionality is the underlying reason of limiting\nthe performance of state-of-the-art algorithms. To address this issue, we\npropose Progressive Adversarial Network (PAN) that is capable of coping with\nthis difficulty for domain-specific image super-resolution. The key principle\nof PAN is that we do not apply any distance-based reconstruction errors as the\nloss to be optimized, thus free from the restriction of the curse of\ndimensionality. To maintain faithful reconstruction precision, we resort to\nU-Net and progressive growing of neural architecture. The low-level features in\nencoder can be transferred into decoder to enhance textural details with U-Net.\nProgressive growing enhances image resolution gradually, thereby preserving\nprecision of recovered image. Moreover, to obtain high-fidelity outputs, we\nleverage the framework of the powerful StyleGAN to perform adversarial\nlearning. Without the curse of dimensionality, our model can super-resolve\nlarge-size images with remarkable photo-realistic details and few distortions.\nExtensive experiments demonstrate the superiority of our algorithm over\nstate-of-the-arts both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 10:19:34 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 13:12:28 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 05:10:43 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2020 03:13:50 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wong", "Lone", ""], ["Zhao", "Deli", ""], ["Wan", "Shaohua", ""], ["Zhang", "Bo", ""]]}, {"id": "2003.03759", "submitter": "Elad Plaut", "authors": "Elad Plaut, Erez Ben Yaacov and Bat El Shlomo", "title": "3D Object Detection from a Single Fisheye Image Without a Single Fisheye\n  Training Image", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing monocular 3D object detection methods have been demonstrated on\nrectilinear perspective images and fail in images with alternative projections\nsuch as those acquired by fisheye cameras. Previous works on object detection\nin fisheye images have focused on 2D object detection, partly due to the lack\nof 3D datasets of such images. In this work, we show how to use existing\nmonocular 3D object detection models, trained only on rectilinear images, to\ndetect 3D objects in images from fisheye cameras, without using any fisheye\ntraining data. We outperform the only existing method for monocular 3D object\ndetection in panoramas on a benchmark of synthetic data, despite the fact that\nthe existing method trains on the target non-rectilinear projection whereas we\ntrain only on rectilinear images. We also experiment with an internal dataset\nof real fisheye images.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 11:03:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 10:22:35 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 05:56:56 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Plaut", "Elad", ""], ["Yaacov", "Erez Ben", ""], ["Shlomo", "Bat El", ""]]}, {"id": "2003.03763", "submitter": "Yanlin Qian", "authors": "Yanlin Qian and Jani K\\\"apyl\\\"a and Joni-Kristian K\\\"am\\\"ar\\\"ainen and\n  Samu Koskinen and Jiri Matas", "title": "A Benchmark for Temporal Color Constancy", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Color Constancy (CC) is a recently proposed approach that challenges\nthe conventional single-frame color constancy. The conventional approach is to\nuse a single frame - shot frame - to estimate the scene illumination color. In\ntemporal CC, multiple frames from the view finder sequence are used to estimate\nthe color. However, there are no realistic large scale temporal color constancy\ndatasets for method evaluation. In this work, a new temporal CC benchmark is\nintroduced. The benchmark comprises of (1) 600 real-world sequences recorded\nwith a high-resolution mobile phone camera, (2) a fixed train-test split which\nensures consistent evaluation, and (3) a baseline method which achieves high\naccuracy in the new benchmark and the dataset used in previous works. Results\nfor more than 20 well-known color constancy methods including the recent\nstate-of-the-arts are reported in our experiments.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 11:17:18 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Qian", "Yanlin", ""], ["K\u00e4pyl\u00e4", "Jani", ""], ["K\u00e4m\u00e4r\u00e4inen", "Joni-Kristian", ""], ["Koskinen", "Samu", ""], ["Matas", "Jiri", ""]]}, {"id": "2003.03766", "submitter": "Harish Y V S", "authors": "Y V S Harish, Harit Pandya, Ayush Gaud, Shreya Terupally, Sai Shankar\n  and K. Madhava Krishna", "title": "DFVS: Deep Flow Guided Scene Agnostic Image Based Visual Servoing", "comments": "Accepted in International Conference on Robotics and Automation\n  (ICRA) 2020, IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep learning based visual servoing approaches regress the relative\ncamera pose between a pair of images. Therefore, they require a huge amount of\ntraining data and sometimes fine-tuning for adaptation to a novel scene.\nFurthermore, current approaches do not consider underlying geometry of the\nscene and rely on direct estimation of camera pose. Thus, inaccuracies in\nprediction of the camera pose, especially for distant goals, lead to a\ndegradation in the servoing performance. In this paper, we propose a two-fold\nsolution: (i) We consider optical flow as our visual features, which are\npredicted using a deep neural network. (ii) These flow features are then\nsystematically integrated with depth estimates provided by another neural\nnetwork using interaction matrix. We further present an extensive benchmark in\na photo-realistic 3D simulation across diverse scenes to study the convergence\nand generalisation of visual servoing approaches. We show convergence for over\n3m and 40 degrees while maintaining precise positioning of under 2cm and 1\ndegree on our challenging benchmark where the existing approaches that are\nunable to converge for majority of scenarios for over 1.5m and 20 degrees.\nFurthermore, we also evaluate our approach for a real scenario on an aerial\nrobot. Our approach generalizes to novel scenarios producing precise and robust\nservoing performance for 6 degrees of freedom positioning tasks with even large\ncamera transformations without any retraining or fine-tuning.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 11:42:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Harish", "Y V S", ""], ["Pandya", "Harit", ""], ["Gaud", "Ayush", ""], ["Terupally", "Shreya", ""], ["Shankar", "Sai", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "2003.03771", "submitter": "Haibo Jin", "authors": "Haibo Jin, Shengcai Liao, Ling Shao", "title": "Pixel-in-Pixel Net: Towards Efficient Facial Landmark Detection in the\n  Wild", "comments": "Substantially revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, heatmap regression models have become popular due to their superior\nperformance in locating facial landmarks. However, three major problems still\nexist among these models: (1) they are computationally expensive; (2) they tend\nto lack robustness; (3) domain gaps are commonly present. To address these\nproblems, we propose Pixel-In-Pixel Net (PIPNet) for facial landmark detection.\nThe proposed model is equipped with a novel detection head based on heatmap\nregression, which conducts score and offset predictions simultaneously on\nlow-resolution feature maps. By doing so, repeated upsampling layers are no\nlonger necessary, enabling the inference time to be largely reduced without\nsacrificing model accuracy. Besides, a simple but effective neighbor regression\nmodule is proposed to enforce local constraints by fusing predictions from\nneighboring landmarks, which enhances the robustness of the new detection head.\nTo further improve the cross-domain generalization capability of PIPNet, we\npropose self-training with curriculum as a training strategy. Self-training\nwith curriculum is able to mine more reliable pseudo-labels from unlabeled data\nacross domains by starting with an easier task, then gradually increasing the\ndifficulty to provide more precise labels. Extensive experiments demonstrate\nthe superiority of PIPNet, which obtains new state-of-the-art results on two\nout of four popular benchmarks under the supervised setting. The results on two\ncross-domain test sets are also consistently improved compared to the baseline.\nNotably, our lightweight version of PIPNet runs at 35.7 FPS and 200 FPS on CPU\nand GPU respectively, while still maintaining a competitive accuracy with\nstate-of-the-art methods. The code is available at\nhttps://github.com/jhb86253817/PIPNet.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 12:23:42 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 06:26:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jin", "Haibo", ""], ["Liao", "Shengcai", ""], ["Shao", "Ling", ""]]}, {"id": "2003.03772", "submitter": "Hui Chen", "authors": "Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, Jungong Han", "title": "IMRAM: Iterative Matching with Recurrent Attention Memory for\n  Cross-Modal Image-Text Retrieval", "comments": "9 pages; Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling bi-directional retrieval of images and texts is important for\nunderstanding the correspondence between vision and language. Existing methods\nleverage the attention mechanism to explore such correspondence in a\nfine-grained manner. However, most of them consider all semantics equally and\nthus align them uniformly, regardless of their diverse complexities. In fact,\nsemantics are diverse (i.e. involving different kinds of semantic concepts),\nand humans usually follow a latent structure to combine them into\nunderstandable languages. It may be difficult to optimally capture such\nsophisticated correspondences in existing methods. In this paper, to address\nsuch a deficiency, we propose an Iterative Matching with Recurrent Attention\nMemory (IMRAM) method, in which correspondences between images and texts are\ncaptured with multiple steps of alignments. Specifically, we introduce an\niterative matching scheme to explore such fine-grained correspondence\nprogressively. A memory distillation unit is used to refine alignment knowledge\nfrom early steps to later ones. Experiment results on three benchmark datasets,\ni.e. Flickr8K, Flickr30K, and MS COCO, show that our IMRAM achieves\nstate-of-the-art performance, well demonstrating its effectiveness. Experiments\non a practical business advertisement dataset, named \\Ads{}, further validates\nthe applicability of our method in practical scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 12:24:41 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Hui", ""], ["Ding", "Guiguang", ""], ["Liu", "Xudong", ""], ["Lin", "Zijia", ""], ["Liu", "Ji", ""], ["Han", "Jungong", ""]]}, {"id": "2003.03773", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng and Yi Yang", "title": "Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain\n  Adaptive Semantic Segmentation", "comments": "13 pages, 6 figures, 10 tables, accepted by IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the unsupervised domain adaptation of transferring the\nknowledge from the source domain to the target domain in the context of\nsemantic segmentation. Existing approaches usually regard the pseudo label as\nthe ground truth to fully exploit the unlabeled target-domain data. Yet the\npseudo labels of the target-domain data are usually predicted by the model\ntrained on the source domain. Thus, the generated labels inevitably contain the\nincorrect prediction due to the discrepancy between the training domain and the\ntest domain, which could be transferred to the final adapted model and largely\ncompromises the training process. To overcome the problem, this paper proposes\nto explicitly estimate the prediction uncertainty during training to rectify\nthe pseudo label learning for unsupervised semantic segmentation adaptation.\nGiven the input image, the model outputs the semantic segmentation prediction\nas well as the uncertainty of the prediction. Specifically, we model the\nuncertainty via the prediction variance and involve the uncertainty into the\noptimization objective. To verify the effectiveness of the proposed method, we\nevaluate the proposed method on two prevalent synthetic-to-real semantic\nsegmentation benchmarks, i.e., GTA5 -> Cityscapes and SYNTHIA -> Cityscapes, as\nwell as one cross-city benchmark, i.e., Cityscapes -> Oxford RobotCar. We\ndemonstrate through extensive experiments that the proposed approach (1)\ndynamically sets different confidence thresholds according to the prediction\nvariance, (2) rectifies the learning from noisy pseudo labels, and (3) achieves\nsignificant improvements over the conventional pseudo label learning and yields\ncompetitive performance on all three benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 12:37:19 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 12:07:57 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 06:07:14 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zheng", "Zhedong", ""], ["Yang", "Yi", ""]]}, {"id": "2003.03780", "submitter": "Yonggang Li", "authors": "Yonggang Li and Guosheng Hu and Yongtao Wang and Timothy Hospedales\n  and Neil M. Robertson and Yongxin Yang", "title": "DADA: Differentiable Automatic Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) techniques aim to increase data variability, and thus\ntrain deep networks with better generalisation. The pioneering AutoAugment\nautomated the search for optimal DA policies with reinforcement learning.\nHowever, AutoAugment is extremely computationally expensive, limiting its wide\napplicability. Followup works such as Population Based Augmentation (PBA) and\nFast AutoAugment improved efficiency, but their optimization speed remains a\nbottleneck. In this paper, we propose Differentiable Automatic Data\nAugmentation (DADA) which dramatically reduces the cost. DADA relaxes the\ndiscrete DA policy selection to a differentiable optimization problem via\nGumbel-Softmax. In addition, we introduce an unbiased gradient estimator,\nRELAX, leading to an efficient and effective one-pass optimization strategy to\nlearn an efficient and accurate DA policy. We conduct extensive experiments on\nCIFAR-10, CIFAR-100, SVHN, and ImageNet datasets. Furthermore, we demonstrate\nthe value of Auto DA in pre-training for downstream detection problems. Results\nshow our DADA is at least one order of magnitude faster than the\nstate-of-the-art while achieving very comparable accuracy. The code is\navailable at https://github.com/VDIGPKU/DADA.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 13:23:14 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 11:23:22 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 14:32:24 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Li", "Yonggang", ""], ["Hu", "Guosheng", ""], ["Wang", "Yongtao", ""], ["Hospedales", "Timothy", ""], ["Robertson", "Neil M.", ""], ["Yang", "Yongxin", ""]]}, {"id": "2003.03787", "submitter": "Dongliang Chang", "authors": "Dongliang Chang, Aneeshan Sain, Zhanyu Ma, Yi-Zhe Song, Jun Guo", "title": "Mind the Gap: Enlarging the Domain Gap in Open Set Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation aims to leverage labeled data from a source\ndomain to learn a classifier for an unlabeled target domain. Among its many\nvariants, open set domain adaptation (OSDA) is perhaps the most challenging, as\nit further assumes the presence of unknown classes in the target domain. In\nthis paper, we study OSDA with a particular focus on enriching its ability to\ntraverse across larger domain gaps. Firstly, we show that existing\nstate-of-the-art methods suffer a considerable performance drop in the presence\nof larger domain gaps, especially on a new dataset (PACS) that we re-purposed\nfor OSDA. We then propose a novel framework to specifically address the larger\ndomain gaps. The key insight lies with how we exploit the mutually beneficial\ninformation between two networks; (a) to separate samples of known and unknown\nclasses, (b) to maximize the domain confusion between source and target domain\nwithout the influence of unknown samples. It follows that (a) and (b) will\nmutually supervise each other and alternate until convergence. Extensive\nexperiments are conducted on Office-31, Office-Home, and PACS datasets,\ndemonstrating the superiority of our method in comparison to other\nstate-of-the-arts. Code available at\nhttps://github.com/dongliangchang/Mutual-to-Separate/\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 14:20:24 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 09:15:32 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Chang", "Dongliang", ""], ["Sain", "Aneeshan", ""], ["Ma", "Zhanyu", ""], ["Song", "Yi-Zhe", ""], ["Guo", "Jun", ""]]}, {"id": "2003.03797", "submitter": "Shengke Xue", "authors": "Shengke Xue, Ruiliang Bai, and Xinyu Jin", "title": "2D Probabilistic Undersampling Pattern Optimization for MR Image\n  Reconstruction", "comments": "Manuscript temporarily, will be submitted IEEE Trans. Med. Imag.\n  eventually", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) is mainly limited by long scanning time and\nvulnerable to human tissue motion artifacts, in 3D clinical scenarios. Thus,\nk-space undersampling is used to accelerate the acquisition of MRI while\nleading to visually poor MR images. Recently, some studies 1) use effective\nundersampling patterns, or 2) design deep neural networks to improve the\nquality of resulting images. However, they are considered as two separate\noptimization strategies. In this paper, we propose a cross-domain network for\nMR image reconstruction, in a retrospective data-driven manner, under limited\nsampling rates. Our method can simultaneously obtain the optimal undersampling\npattern (in k-space) and the reconstruction model, which are customized to the\ntype of training data, by using an end-to-end learning strategy. We propose a\n2D probabilistic undersampling layer, to obtain the optimal undersampling\npattern and its probability distribution in a differentiable way. We propose a\n2D inverse Fourier transform layer, which connects the Fourier domain and the\nimage domain during the forward pass and the backpropagation. In addition, by\ntraining 3D fully-sampled k-space data and MR images with the traditional\nEuclidean loss, we discover the universal relationship between the probability\ndistribution of the optimal undersampling pattern and its corresponding\nsampling rate. Experiments show that the quantitative and qualitative results\nof recovered MR images by our 2D probabilistic undersampling pattern obviously\noutperform those of several existing sampling strategies.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 15:15:37 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 08:18:19 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Xue", "Shengke", ""], ["Bai", "Ruiliang", ""], ["Jin", "Xinyu", ""]]}, {"id": "2003.03808", "submitter": "Sachit Menon", "authors": "Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, Cynthia Rudin", "title": "PULSE: Self-Supervised Photo Upsampling via Latent Space Exploration of\n  Generative Models", "comments": "Sachit Menon and Alexandru Damian contributed equally. Computer\n  Vision and Pattern Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary aim of single-image super-resolution is to construct\nhigh-resolution (HR) images from corresponding low-resolution (LR) inputs. In\nprevious approaches, which have generally been supervised, the training\nobjective typically measures a pixel-wise average distance between the\nsuper-resolved (SR) and HR images. Optimizing such metrics often leads to\nblurring, especially in high variance (detailed) regions. We propose an\nalternative formulation of the super-resolution problem based on creating\nrealistic SR images that downscale correctly. We present an algorithm\naddressing this problem, PULSE (Photo Upsampling via Latent Space Exploration),\nwhich generates high-resolution, realistic images at resolutions previously\nunseen in the literature. It accomplishes this in an entirely self-supervised\nfashion and is not confined to a specific degradation operator used during\ntraining, unlike previous methods (which require supervised training on\ndatabases of LR-HR image pairs). Instead of starting with the LR image and\nslowly adding detail, PULSE traverses the high-resolution natural image\nmanifold, searching for images that downscale to the original LR image. This is\nformalized through the \"downscaling loss,\" which guides exploration through the\nlatent space of a generative model. By leveraging properties of\nhigh-dimensional Gaussians, we restrict the search space to guarantee realistic\noutputs. PULSE thereby generates super-resolved images that both are realistic\nand downscale correctly. We show proof of concept of our approach in the domain\nof face super-resolution (i.e., face hallucination). We also present a\ndiscussion of the limitations and biases of the method as currently implemented\nwith an accompanying model card with relevant metrics. Our method outperforms\nstate-of-the-art methods in perceptual quality at higher resolutions and scale\nfactors than previously possible.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 16:44:31 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 21:05:02 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 21:38:32 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Menon", "Sachit", ""], ["Damian", "Alexandru", ""], ["Hu", "Shijia", ""], ["Ravi", "Nikhil", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2003.03824", "submitter": "Siqi Liu", "authors": "Siqi Liu, Arnaud Arindra Adiyoso Setio, Florin C. Ghesu, Eli Gibson,\n  Sasa Grbic, Bogdan Georgescu, Dorin Comaniciu", "title": "No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT\n  Scans by Augmenting with Adversarial Attacks", "comments": "Published on IEEE Trans. on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting malignant pulmonary nodules at an early stage can allow medical\ninterventions which may increase the survival rate of lung cancer patients.\nUsing computer vision techniques to detect nodules can improve the sensitivity\nand the speed of interpreting chest CT for lung cancer screening. Many studies\nhave used CNNs to detect nodule candidates. Though such approaches have been\nshown to outperform the conventional image processing based methods regarding\nthe detection accuracy, CNNs are also known to be limited to generalize on\nunder-represented samples in the training set and prone to imperceptible noise\nperturbations. Such limitations can not be easily addressed by scaling up the\ndataset or the models. In this work, we propose to add adversarial synthetic\nnodules and adversarial attack samples to the training data to improve the\ngeneralization and the robustness of the lung nodule detection systems. To\ngenerate hard examples of nodules from a differentiable nodule synthesizer, we\nuse projected gradient descent (PGD) to search the latent code within a bounded\nneighbourhood that would generate nodules to decrease the detector response. To\nmake the network more robust to unanticipated noise perturbations, we use PGD\nto search for noise patterns that can trigger the network to give\nover-confident mistakes. By evaluating on two different benchmark datasets\ncontaining consensus annotations from three radiologists, we show that the\nproposed techniques can improve the detection performance on real CT data. To\nunderstand the limitations of both the conventional networks and the proposed\naugmented networks, we also perform stress-tests on the false positive\nreduction networks by feeding different types of artificially produced patches.\nWe show that the augmented networks are more robust to both under-represented\nnodules as well as resistant to noise perturbations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 18:32:46 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 23:55:00 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Liu", "Siqi", ""], ["Setio", "Arnaud Arindra Adiyoso", ""], ["Ghesu", "Florin C.", ""], ["Gibson", "Eli", ""], ["Grbic", "Sasa", ""], ["Georgescu", "Bogdan", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2003.03828", "submitter": "Grigorios Chrysos", "authors": "Grigorios G. Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis\n  Panagakis, Jiankang Deng, Stefanos Zafeiriou", "title": "$\\Pi-$nets: Deep Polynomial Neural Networks", "comments": "Accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) is currently the method of choice\nboth for generative, as well as for discriminative learning in computer vision\nand machine learning. The success of DCNNs can be attributed to the careful\nselection of their building blocks (e.g., residual blocks, rectifiers,\nsophisticated normalization schemes, to mention but a few). In this paper, we\npropose $\\Pi$-Nets, a new class of DCNNs. $\\Pi$-Nets are polynomial neural\nnetworks, i.e., the output is a high-order polynomial of the input. $\\Pi$-Nets\ncan be implemented using special kind of skip connections and their parameters\ncan be represented via high-order tensors. We empirically demonstrate that\n$\\Pi$-Nets have better representation power than standard DCNNs and they even\nproduce good results without the use of non-linear activation functions in a\nlarge battery of tasks and signals, i.e., images, graphs, and audio. When used\nin conjunction with activation functions, $\\Pi$-Nets produce state-of-the-art\nresults in challenging tasks, such as image generation. Lastly, our framework\nelucidates why recent generative models, such as StyleGAN, improve upon their\npredecessors, e.g., ProGAN.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 18:48:43 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:25:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chrysos", "Grigorios G.", ""], ["Moschoglou", "Stylianos", ""], ["Bouritsas", "Giorgos", ""], ["Panagakis", "Yannis", ""], ["Deng", "Jiankang", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2003.03836", "submitter": "Ruoyi Du", "authors": "Ruoyi Du, Dongliang Chang, Ayan Kumar Bhunia, Jiyang Xie, Zhanyu Ma,\n  Yi-Zhe Song, Jun Guo", "title": "Fine-Grained Visual Classification via Progressive Multi-Granularity\n  Training of Jigsaw Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual classification (FGVC) is much more challenging than\ntraditional classification tasks due to the inherently subtle intra-class\nobject variations. Recent works mainly tackle this problem by focusing on how\nto locate the most discriminative parts, more complementary parts, and parts of\nvarious granularities. However, less effort has been placed to which\ngranularities are the most discriminative and how to fuse information cross\nmulti-granularity. In this work, we propose a novel framework for fine-grained\nvisual classification to tackle these problems. In particular, we propose: (i)\na progressive training strategy that effectively fuses features from different\ngranularities, and (ii) a random jigsaw patch generator that encourages the\nnetwork to learn features at specific granularities. We obtain state-of-the-art\nperformances on several standard FGVC benchmark datasets, where the proposed\nmethod consistently outperforms existing methods or delivers competitive\nresults. The code will be available at\nhttps://github.com/PRIS-CV/PMG-Progressive-Multi-Granularity-Training.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 19:27:30 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 11:09:49 GMT"}, {"version": "v3", "created": "Sun, 19 Jul 2020 06:56:10 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Du", "Ruoyi", ""], ["Chang", "Dongliang", ""], ["Bhunia", "Ayan Kumar", ""], ["Xie", "Jiyang", ""], ["Ma", "Zhanyu", ""], ["Song", "Yi-Zhe", ""], ["Guo", "Jun", ""]]}, {"id": "2003.03849", "submitter": "Wang Zhihua", "authors": "Zhihua Wang and Kede Ma", "title": "Active Fine-Tuning from gMAD Examples Improves Blind Image Quality\n  Assessment", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research in image quality assessment (IQA) has a long history, and\nsignificant progress has been made by leveraging recent advances in deep neural\nnetworks (DNNs). Despite high correlation numbers on existing IQA datasets,\nDNN-based models may be easily falsified in the group maximum differentiation\n(gMAD) competition with strong counterexamples being identified. Here we show\nthat gMAD examples can be used to improve blind IQA (BIQA) methods.\nSpecifically, we first pre-train a DNN-based BIQA model using multiple noisy\nannotators, and fine-tune it on multiple subject-rated databases of\nsynthetically distorted images, resulting in a top-performing baseline model.\nWe then seek pairs of images by comparing the baseline model with a set of\nfull-reference IQA methods in gMAD. The resulting gMAD examples are most likely\nto reveal the relative weaknesses of the baseline, and suggest potential ways\nfor refinement. We query ground truth quality annotations for the selected\nimages in a well controlled laboratory environment, and further fine-tune the\nbaseline on the combination of human-rated images from gMAD and existing\ndatabases. This process may be iterated, enabling active and progressive\nfine-tuning from gMAD examples for BIQA. We demonstrate the feasibility of our\nactive learning scheme on a large-scale unlabeled image set, and show that the\nfine-tuned method achieves improved generalizability in gMAD, without\ndestroying performance on previously trained databases.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 21:19:01 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:45:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Zhihua", ""], ["Ma", "Kede", ""]]}, {"id": "2003.03856", "submitter": "Nina Wiedemann", "authors": "Nina Wiedemann, Carlos Dietrich, Claudio T. Silva", "title": "A Tracking System For Baseball Game Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The baseball game is often seen as many contests that are performed between\nindividuals. The duel between the pitcher and the batter, for example, is\nconsidered the engine that drives the sport. The pitchers use a variety of\nstrategies to gain competitive advantage against the batter, who does his best\nto figure out the ball trajectory and react in time for a hit. In this work, we\npropose a system that captures the movements of the pitcher, the batter, and\nthe ball in a high level of detail, and discuss several ways how this\ninformation may be processed to compute interesting statistics. We demonstrate\non a large database of videos that our methods achieve comparable results as\nprevious systems, while operating solely on video material. In addition,\nstate-of-the-art AI techniques are incorporated to augment the amount of\ninformation that is made available for players, coaches, teams, and fans.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 22:04:54 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wiedemann", "Nina", ""], ["Dietrich", "Carlos", ""], ["Silva", "Claudio T.", ""]]}, {"id": "2003.03877", "submitter": "Qicheng Lao", "authors": "Qicheng Lao, Mehrzad Mortazavi, Marzieh Tahaei, Francis Dutil, Thomas\n  Fevens, Mohammad Havaei", "title": "FoCL: Feature-Oriented Continual Learning for Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework in continual learning for\ngenerative models: Feature-oriented Continual Learning (FoCL). Unlike previous\nworks that aim to solve the catastrophic forgetting problem by introducing\nregularization in the parameter space or image space, FoCL imposes\nregularization in the feature space. We show in our experiments that FoCL has\nfaster adaptation to distributional changes in sequentially arriving tasks, and\nachieves the state-of-the-art performance for generative models in task\nincremental learning. We discuss choices of combined regularization spaces\ntowards different use case scenarios for boosted performance, e.g., tasks that\nhave high variability in the background. Finally, we introduce a forgetfulness\nmeasure that fairly evaluates the degree to which a model suffers from\nforgetting. Interestingly, the analysis of our proposed forgetfulness score\nalso implies that FoCL tends to have a mitigated forgetting for future tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 00:38:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Lao", "Qicheng", ""], ["Mortazavi", "Mehrzad", ""], ["Tahaei", "Marzieh", ""], ["Dutil", "Francis", ""], ["Fevens", "Thomas", ""], ["Havaei", "Mohammad", ""]]}, {"id": "2003.03879", "submitter": "Sanghyuk Chun", "authors": "Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe,\n  Youngjoon Yoo", "title": "An Empirical Evaluation on Robustness and Uncertainty of Regularization\n  Methods", "comments": "Accepted at ICML 2019 Workshop on Uncertainty and Robustness in Deep\n  Learning. 7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite apparent human-level performances of deep neural networks (DNN), they\nbehave fundamentally differently from humans. They easily change predictions\nwhen small corruptions such as blur and noise are applied on the input (lack of\nrobustness), and they often produce confident predictions on\nout-of-distribution samples (improper uncertainty measure). While a number of\nresearches have aimed to address those issues, proposed solutions are typically\nexpensive and complicated (e.g. Bayesian inference and adversarial training).\nMeanwhile, many simple and cheap regularization methods have been developed to\nenhance the generalization of classifiers. Such regularization methods have\nlargely been overlooked as baselines for addressing the robustness and\nuncertainty issues, as they are not specifically designed for that. In this\npaper, we provide extensive empirical evaluations on the robustness and\nuncertainty estimates of image classifiers (CIFAR-100 and ImageNet) trained\nwith state-of-the-art regularization methods. Furthermore, experimental results\nshow that certain regularization methods can serve as strong baseline methods\nfor robustness and uncertainty estimation of DNNs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 01:15:22 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chun", "Sanghyuk", ""], ["Oh", "Seong Joon", ""], ["Yun", "Sangdoo", ""], ["Han", "Dongyoon", ""], ["Choe", "Junsuk", ""], ["Yoo", "Youngjoon", ""]]}, {"id": "2003.03901", "submitter": "Kun Zhao", "authors": "Can Peng, Kun Zhao and Brian C. Lovell", "title": "Faster ILOD: Incremental Learning for Object Detectors based on Faster\n  RCNN", "comments": "Accepted in Pattern Recognition Letters 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human vision and perception system is inherently incremental where new\nknowledge is continually learned over time whilst existing knowledge is\nretained. On the other hand, deep learning networks are ill-equipped for\nincremental learning. When a well-trained network is adapted to new categories,\nits performance on the old categories will dramatically degrade. To address\nthis problem, incremental learning methods have been explored which preserve\nthe old knowledge of deep learning models. However, the state-of-the-art\nincremental object detector employs an external fixed region proposal method\nthat increases overall computation time and reduces accuracy comparing to\nRegion Proposal Network (RPN) based object detectors such as Faster RCNN. The\npurpose of this paper is to design an efficient end-to-end incremental object\ndetector using knowledge distillation. We first evaluate and analyze the\nperformance of the RPN-based detector with classic distillation on incremental\ndetection tasks. Then, we introduce multi-network adaptive distillation that\nproperly retains knowledge from the old categories when fine-tuning the model\nfor new task. Experiments on the benchmark datasets, PASCAL VOC and COCO,\ndemonstrate that the proposed incremental detector based on Faster RCNN is more\naccurate as well as being 13 times faster than the baseline detector.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 03:09:00 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 00:42:06 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Peng", "Can", ""], ["Zhao", "Kun", ""], ["Lovell", "Brian C.", ""]]}, {"id": "2003.03913", "submitter": "Zhanpeng Zhang", "authors": "Zhanpeng Zhang and Kaipeng Zhang", "title": "FarSee-Net: Real-Time Semantic Segmentation by Efficient Multi-scale\n  Context Aggregation and Feature Space Super-resolution", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic segmentation is desirable in many robotic applications\nwith limited computation resources. One challenge of semantic segmentation is\nto deal with the object scale variations and leverage the context. How to\nperform multi-scale context aggregation within limited computation budget is\nimportant. In this paper, firstly, we introduce a novel and efficient module\ncalled Cascaded Factorized Atrous Spatial Pyramid Pooling (CF-ASPP). It is a\nlightweight cascaded structure for Convolutional Neural Networks (CNNs) to\nefficiently leverage context information. On the other hand, for runtime\nefficiency, state-of-the-art methods will quickly decrease the spatial size of\nthe inputs or feature maps in the early network stages. The final\nhigh-resolution result is usually obtained by non-parametric up-sampling\noperation (e.g. bilinear interpolation). Differently, we rethink this pipeline\nand treat it as a super-resolution process. We use optimized super-resolution\noperation in the up-sampling step and improve the accuracy, especially in\nsub-sampled input image scenario for real-time applications. By fusing the\nabove two improvements, our methods provide better latency-accuracy trade-off\nthan the other state-of-the-art methods. In particular, we achieve 68.4% mIoU\nat 84 fps on the Cityscapes test set with a single Nivida Titan X (Maxwell) GPU\ncard. The proposed module can be plugged into any feature extraction CNN and\nbenefits from the CNN structure development.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 03:53:57 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Zhanpeng", ""], ["Zhang", "Kaipeng", ""]]}, {"id": "2003.03918", "submitter": "Zhicheng Cao", "authors": "Liaojun Pang, Jiong Chen, Fei Guo, Zhicheng Cao, and Heng Zhao", "title": "ROSE: Real One-Stage Effort to Detect the Fingerprint Singular Point\n  Based on Multi-scale Spatial Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the singular point accurately and efficiently is one of the most\nimportant tasks for fingerprint recognition. In recent years, deep learning has\nbeen gradually used in the fingerprint singular point detection. However,\ncurrent deep learning-based singular point detection methods are either\ntwo-stage or multi-stage, which makes them time-consuming. More importantly,\ntheir detection accuracy is yet unsatisfactory, especially in the case of the\nlow-quality fingerprint. In this paper, we make a Real One-Stage Effort to\ndetect fingerprint singular points more accurately and efficiently, and\ntherefore we name the proposed algorithm ROSE for short, in which the\nmulti-scale spatial attention, the Gaussian heatmap and the variant of focal\nloss are applied together to achieve a higher detection rate. Experimental\nresults on the datasets FVC2002 DB1 and NIST SD4 show that our ROSE outperforms\nthe state-of-art algorithms in terms of detection rate, false alarm rate and\ndetection speed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 04:16:31 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Pang", "Liaojun", ""], ["Chen", "Jiong", ""], ["Guo", "Fei", ""], ["Cao", "Zhicheng", ""], ["Zhao", "Heng", ""]]}, {"id": "2003.03923", "submitter": "Xu Yang", "authors": "Xu Yang, Hanwang Zhang, Jianfei Cai", "title": "Deconfounded Image Captioning: A Causal Retrospect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dataset bias in vision-language tasks is becoming one of the main\nproblems that hinder the progress of our community. However, recent studies\nlack a principled analysis of the bias. In this paper, we present a novel\nperspective: Deconfounded Image Captioning (DIC), to find out the cause of the\nbias in image captioning, then retrospect modern neural image captioners, and\nfinally propose a DIC framework: DICv1.0. DIC is based on causal inference,\nwhose two principles: the backdoor and front-door adjustments, help us to\nreview previous works and design the effective models. In particular, we\nshowcase that DICv1.0 can strengthen two prevailing captioning models and\nachieves a single-model 130.7 CIDEr-D and 128.4 c40 CIDEr-D on Karpathy split\nand online split of the challenging MS-COCO dataset, respectively. Last but not\nleast, DICv1.0 is merely a natural derivation from our causal retrospect, which\nopens a promising direction for image captioning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 04:59:05 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yang", "Xu", ""], ["Zhang", "Hanwang", ""], ["Cai", "Jianfei", ""]]}, {"id": "2003.03938", "submitter": "Yi Gao", "authors": "Jinchan He, Xiaxia Yu, Chudong Cai, Yi Gao", "title": "MCMC Guided CNN Training and Segmentation for Pancreas Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient organ segmentation is the precondition of various quantitative\nanalysis. Segmenting the pancreas from abdominal CT images is a challenging\ntask because of its high anatomical variability in shape, size and location.\nWhat's more, the pancreas only occupies a small portion in abdomen, and the\norgan border is very fuzzy. All these factors make the segmentation methods of\nother organs less suitable for the pancreas segmentation. In this report, we\npropose a Markov Chain Monte Carlo (MCMC) sampling guided convolutional neural\nnetwork (CNN) approach, in order to handle such difficulties in morphological\nand photometric variabilities. Specifically, the proposed method mainly\ncontains three steps: First, registration is carried out to mitigate the body\nweight and location variability. Then, an MCMC sampling is employed to guide\nthe sampling of 3D patches, which are fed to the CNN for training. At the same\ntime, the pancreas distribution is also learned for the subsequent\nsegmentation. Third, sampled from the learned distribution, an MCMC process\nguides the segmentation process. Lastly, the patches based segmentation is\nfused using a Bayesian voting scheme. This method is evaluated on the NIH\npancreatic datasets which contains 82 abdominal contrast-enhanced CT volumes.\nFinally, we achieved a competitive result of 78.13% Dice Similarity Coefficient\nvalue and 82.65% Recall value in testing data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 06:27:08 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["He", "Jinchan", ""], ["Yu", "Xiaxia", ""], ["Cai", "Chudong", ""], ["Gao", "Yi", ""]]}, {"id": "2003.03944", "submitter": "Wonchul Son", "authors": "Wonchul Son, Youngbin Kim, Wonseok Song, Youngsu Moon, Wonjun Hwang", "title": "Pacemaker: Intermediate Teacher Knowledge Distillation For On-The-Fly\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for an on-the-fly computational process with very low\nperformance system such as system-on-chip (SoC) and embedded device etc. This\npaper presents pacemaker knowledge distillation as intermediate ensemble\nteacher to use convolutional neural network in these systems. For on-the-fly\nsystem, we consider student model using 1xN shape on-the-fly filter and teacher\nmodel using normal NxN shape filter. We note three points about training\nstudent model, caused by applying on-the-fly filter. First, same depth but\nunavoidable thin model compression. Second, the large capacity gap and\nparameter size gap due to only the horizontal field must be selected not the\nvertical receptive. Third, the performance instability and degradation of\ndirect distilling. To solve these problems, we propose intermediate teacher,\nnamed pacemaker, for an on-the-fly student. So, student can be trained from\npacemaker and original teacher step by step. Experiments prove our proposed\nmethod make significant performance (accuracy) improvements: on CIFAR100, 5.39%\nincreased in WRN-40-4 than conventional knowledge distillation which shows even\nlow performance than baseline. And we solve train instability, occurred when\nconventional knowledge distillation was applied without proposed method, by\nreducing deviation range by applying proposed method pacemaker knowledge\ndistillation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 06:45:44 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Son", "Wonchul", ""], ["Kim", "Youngbin", ""], ["Song", "Wonseok", ""], ["Moon", "Youngsu", ""], ["Hwang", "Wonjun", ""]]}, {"id": "2003.03955", "submitter": "Hao Wang", "authors": "Hao Wang, Doyen Sahoo, Chenghao Liu, Ke Shu, Palakorn Achananuparp,\n  Ee-peng Lim, Steven C. H. Hoi", "title": "Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images\n  and Recipes with Semantic Consistency and Attention Mechanism", "comments": "IEEE Transactions on Multimedia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food retrieval is an important task to perform analysis of food-related\ninformation, where we are interested in retrieving relevant information about\nthe queried food item such as ingredients, cooking instructions, etc. In this\npaper, we investigate cross-modal retrieval between food images and cooking\nrecipes. The goal is to learn an embedding of images and recipes in a common\nfeature space, such that the corresponding image-recipe embeddings lie close to\none another. Two major challenges in addressing this problem are 1) large\nintra-variance and small inter-variance across cross-modal food data; and 2)\ndifficulties in obtaining discriminative recipe representations. To address\nthese two problems, we propose Semantic-Consistent and Attention-based Networks\n(SCAN), which regularize the embeddings of the two modalities through aligning\noutput semantic probabilities. Besides, we exploit a self-attention mechanism\nto improve the embedding of recipes. We evaluate the performance of the\nproposed method on the large-scale Recipe1M dataset, and show that we can\noutperform several state-of-the-art cross-modal retrieval strategies for food\nimages and cooking recipes by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 07:41:17 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 01:10:41 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Hao", ""], ["Sahoo", "Doyen", ""], ["Liu", "Chenghao", ""], ["Shu", "Ke", ""], ["Achananuparp", "Palakorn", ""], ["Lim", "Ee-peng", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2003.03961", "submitter": "Ziwei Wang", "authors": "Ziwei Wang, Ziyi Wu, Jiwen Lu and Jie Zhou", "title": "BiDet: An Efficient Binarized Object Detector", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a binarized neural network learning method called\nBiDet for efficient object detection. Conventional network binarization methods\ndirectly quantize the weights and activations in one-stage or two-stage\ndetectors with constrained representational capacity, so that the information\nredundancy in the networks causes numerous false positives and degrades the\nperformance significantly. On the contrary, our BiDet fully utilizes the\nrepresentational capacity of the binary neural networks for object detection by\nredundancy removal, through which the detection precision is enhanced with\nalleviated false positives. Specifically, we generalize the information\nbottleneck (IB) principle to object detection, where the amount of information\nin the high-level feature maps is constrained and the mutual information\nbetween the feature maps and object detection is maximized. Meanwhile, we learn\nsparse object priors so that the posteriors are concentrated on informative\ndetection prediction with false positive elimination. Extensive experiments on\nthe PASCAL VOC and COCO datasets show that our method outperforms the\nstate-of-the-art binary neural networks by a sizable margin.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 08:16:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wang", "Ziwei", ""], ["Wu", "Ziyi", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2003.03972", "submitter": "Long Chen", "authors": "Long Chen, Haizhou Ai, Rui Chen, Zijie Zhuang, Shuang Liu", "title": "Cross-View Tracking for Multi-Human 3D Pose Estimation at over 100 FPS", "comments": "12 pages with supplementary material; accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D poses of multiple humans in real-time is a classic but still\nchallenging task in computer vision. Its major difficulty lies in the ambiguity\nin cross-view association of 2D poses and the huge state space when there are\nmultiple people in multiple views. In this paper, we present a novel solution\nfor multi-human 3D pose estimation from multiple calibrated camera views. It\ntakes 2D poses in different camera coordinates as inputs and aims for the\naccurate 3D poses in the global coordinate. Unlike previous methods that\nassociate 2D poses among all pairs of views from scratch at every frame, we\nexploit the temporal consistency in videos to match the 2D inputs with 3D poses\ndirectly in 3-space. More specifically, we propose to retain the 3D pose for\neach person and update them iteratively via the cross-view multi-human\ntracking. This novel formulation improves both accuracy and efficiency, as we\ndemonstrated on widely-used public datasets. To further verify the scalability\nof our method, we propose a new large-scale multi-human dataset with 12 to 28\ncamera views. Without bells and whistles, our solution achieves 154 FPS on 12\ncameras and 34 FPS on 28 cameras, indicating its ability to handle large-scale\nreal-world applications. The proposed dataset is released at\nhttps://github.com/longcw/crossview_3d_pose_tracking.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 08:54:00 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:18:14 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 03:02:33 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Long", ""], ["Ai", "Haizhou", ""], ["Chen", "Rui", ""], ["Zhuang", "Zijie", ""], ["Liu", "Shuang", ""]]}, {"id": "2003.03983", "submitter": "Mingshuang Luo", "authors": "Mingshuang Luo, Shuang Yang, Shiguang Shan, Xilin Chen", "title": "Pseudo-Convolutional Policy Gradient for Sequence-to-Sequence\n  Lip-Reading", "comments": "8 pages, Accepted in the 15th IEEE International Conference on\n  Automatic Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip-reading aims to infer the speech content from the lip movement sequence\nand can be seen as a typical sequence-to-sequence (seq2seq) problem which\ntranslates the input image sequence of lip movements to the text sequence of\nthe speech content. However, the traditional learning process of seq2seq models\nalways suffers from two problems: the exposure bias resulted from the strategy\nof \"teacher-forcing\", and the inconsistency between the discriminative\noptimization target (usually the cross-entropy loss) and the final evaluation\nmetric (usually the character/word error rate). In this paper, we propose a\nnovel pseudo-convolutional policy gradient (PCPG) based method to address these\ntwo problems. On the one hand, we introduce the evaluation metric (refers to\nthe character error rate in this paper) as a form of reward to optimize the\nmodel together with the original discriminative target. On the other hand,\ninspired by the local perception property of convolutional operation, we\nperform a pseudo-convolutional operation on the reward and loss dimension, so\nas to take more context around each time step into account to generate a robust\nreward and loss for the whole optimization. Finally, we perform a thorough\ncomparison and evaluation on both the word-level and sentence-level benchmarks.\nThe results show a significant improvement over other related methods, and\nreport either a new state-of-the-art performance or a competitive accuracy on\nall these challenging benchmarks, which clearly proves the advantages of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 09:12:26 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Luo", "Mingshuang", ""], ["Yang", "Shuang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2003.04010", "submitter": "Jinyu Yang", "authors": "Jinyu Yang, Weizhi An, Chaochao Yan, Peilin Zhao, Junzhou Huang", "title": "Context-Aware Domain Adaptation in Semantic Segmentation", "comments": "10 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of unsupervised domain adaptation in\nthe semantic segmentation. There are two primary issues in this field, i.e.,\nwhat and how to transfer domain knowledge across two domains. Existing methods\nmainly focus on adapting domain-invariant features (what to transfer) through\nadversarial learning (how to transfer). Context dependency is essential for\nsemantic segmentation, however, its transferability is still not well\nunderstood. Furthermore, how to transfer contextual information across two\ndomains remains unexplored. Motivated by this, we propose a cross-attention\nmechanism based on self-attention to capture context dependencies between two\ndomains and adapt transferable context. To achieve this goal, we design two\ncross-domain attention modules to adapt context dependencies from both spatial\nand channel views. Specifically, the spatial attention module captures local\nfeature dependencies between each position in the source and target image. The\nchannel attention module models semantic dependencies between each pair of\ncross-domain channel maps. To adapt context dependencies, we further\nselectively aggregate the context information from two domains. The superiority\nof our method over existing state-of-the-art methods is empirically proved on\n\"GTA5 to Cityscapes\" and \"SYNTHIA to Cityscapes\".\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 09:57:24 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yang", "Jinyu", ""], ["An", "Weizhi", ""], ["Yan", "Chaochao", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""]]}, {"id": "2003.04027", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jessen, and Arnt-B{\\o}rre\n  Salberg", "title": "Dense Dilated Convolutions Merging Network for Land Cover Classification", "comments": "Semantic Segmentation, 12 pages, TGRS-2020 early access in IEEE\n  Transactions on Geoscience and Remote Sensing. 2020, Code available at\n  https://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch", "journal-ref": null, "doi": "10.1109/TGRS.2020.2976658", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Land cover classification of remote sensing images is a challenging task due\nto limited amounts of annotated data, highly imbalanced classes, frequent\nincorrect pixel-level annotations, and an inherent complexity in the semantic\nsegmentation task. In this article, we propose a novel architecture called the\ndense dilated convolutions' merging network (DDCM-Net) to address this task.\nThe proposed DDCM-Net consists of dense dilated image convolutions merged with\nvarying dilation rates. This effectively utilizes rich combinations of dilated\nconvolutions that enlarge the network's receptive fields with fewer parameters\nand features compared with the state-of-the-art approaches in the remote\nsensing domain. Importantly, DDCM-Net obtains fused local- and global-context\ninformation, in effect incorporating surrounding discriminative capability for\nmultiscale and complex-shaped objects with similar color and textures in very\nhigh-resolution aerial imagery. We demonstrate the effectiveness, robustness,\nand flexibility of the proposed DDCM-Net on the publicly available ISPRS\nPotsdam and Vaihingen data sets, as well as the DeepGlobe land cover data set.\nOur single model, trained on three-band Potsdam and Vaihingen data sets,\nachieves better accuracy in terms of both mean intersection over union (mIoU)\nand F1-score compared with other published models trained with more than\nthree-band data. We further validate our model on the DeepGlobe data set,\nachieving state-of-the-art result 56.2% mIoU with much fewer parameters and at\na lower computational cost compared with related recent work. Code available at\nhttps://github.com/samleoqh/DDCM-Semantic-Segmentation-PyTorch\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 10:31:38 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jessen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "2003.04030", "submitter": "Yuanhao Cai", "authors": "Yuanhao Cai, Zhicheng Wang, Zhengxiong Luo, Binyi Yin, Angang Du,\n  Haoqian Wang, Xiangyu Zhang, Xinyu Zhou, Erjin Zhou, Jian Sun", "title": "Learning Delicate Local Representations for Multi-Person Pose Estimation", "comments": "ECCV2020 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method called Residual Steps Network (RSN).\nRSN aggregates features with the same spatial size (Intra-level features)\nefficiently to obtain delicate local representations, which retain rich\nlow-level spatial information and result in precise keypoint localization.\nAdditionally, we observe the output features contribute differently to final\nperformance. To tackle this problem, we propose an efficient attention\nmechanism - Pose Refine Machine (PRM) to make a trade-off between local and\nglobal representations in output features and further refine the keypoint\nlocations. Our approach won the 1st place of COCO Keypoint Challenge 2019 and\nachieves state-of-the-art results on both COCO and MPII benchmarks, without\nusing extra training data and pretrained model. Our single model achieves 78.6\non COCO test-dev, 93.0 on MPII test dataset. Ensembled models achieve 79.2 on\nCOCO test-dev, 77.1 on COCO test-challenge dataset. The source code is publicly\navailable for further research at https://github.com/caiyuanhao1998/RSN/\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 10:40:49 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 04:02:33 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 13:09:57 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Cai", "Yuanhao", ""], ["Wang", "Zhicheng", ""], ["Luo", "Zhengxiong", ""], ["Yin", "Binyi", ""], ["Du", "Angang", ""], ["Wang", "Haoqian", ""], ["Zhang", "Xiangyu", ""], ["Zhou", "Xinyu", ""], ["Zhou", "Erjin", ""], ["Sun", "Jian", ""]]}, {"id": "2003.04035", "submitter": "Pauline Luc", "authors": "Pauline Luc, Aidan Clark, Sander Dieleman, Diego de Las Casas, Yotam\n  Doron, Albin Cassirer, Karen Simonyan", "title": "Transformation-based Adversarial Video Prediction on Large-Scale Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in adversarial generative modeling have led to models\ncapable of producing video samples of high quality, even on large and complex\ndatasets of real-world video. In this work, we focus on the task of video\nprediction, where given a sequence of frames extracted from a video, the goal\nis to generate a plausible future sequence. We first improve the state of the\nart by performing a systematic empirical study of discriminator decompositions\nand proposing an architecture that yields faster convergence and higher\nperformance than previous approaches. We then analyze recurrent units in the\ngenerator, and propose a novel recurrent unit which transforms its past hidden\nstate according to predicted motion-like features, and refines it to handle\ndis-occlusions, scene changes and other complex behavior. We show that this\nrecurrent unit consistently outperforms previous designs. Our final model leads\nto a leap in the state-of-the-art performance, obtaining a test set Frechet\nVideo Distance of 25.7, down from 69.2, on the large-scale Kinetics-600\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 10:52:25 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 15:38:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Luc", "Pauline", ""], ["Clark", "Aidan", ""], ["Dieleman", "Sander", ""], ["Casas", "Diego de Las", ""], ["Doron", "Yotam", ""], ["Cassirer", "Albin", ""], ["Simonyan", "Karen", ""]]}, {"id": "2003.04052", "submitter": "Jose Dolz", "authors": "Reza Azad, Abdur R Fayjie, Claude Kauffman, Ismail Ben Ayed, Marco\n  Pedersoli, Jose Dolz", "title": "On the Texture Bias for Few-Shot CNN Segmentation", "comments": "Accepted at WACV'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the initial belief that Convolutional Neural Networks (CNNs) are\ndriven by shapes to perform visual recognition tasks, recent evidence suggests\nthat texture bias in CNNs provides higher performing models when learning on\nlarge labeled training datasets. This contrasts with the perceptual bias in the\nhuman visual cortex, which has a stronger preference towards shape components.\nPerceptual differences may explain why CNNs achieve human-level performance\nwhen large labeled datasets are available, but their performance significantly\ndegrades in lowlabeled data scenarios, such as few-shot semantic segmentation.\nTo remove the texture bias in the context of few-shot learning, we propose a\nnovel architecture that integrates a set of Difference of Gaussians (DoG) to\nattenuate high-frequency local components in the feature space. This produces a\nset of modified feature maps, whose high-frequency components are diminished at\ndifferent standard deviation values of the Gaussian distribution in the spatial\ndomain. As this results in multiple feature maps for a single image, we employ\na bi-directional convolutional long-short-term-memory to efficiently merge the\nmulti scale-space representations. We perform extensive experiments on three\nwell-known few-shot segmentation benchmarks -- Pascal i5, COCO-20i and FSS-1000\n-- and demonstrate that our method outperforms state-of-the-art approaches in\ntwo datasets under the same conditions. The code is available at:\nhttps://github.com/rezazad68/fewshot-segmentation\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 11:55:47 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 02:54:54 GMT"}, {"version": "v3", "created": "Wed, 23 Dec 2020 22:37:09 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Azad", "Reza", ""], ["Fayjie", "Abdur R", ""], ["Kauffman", "Claude", ""], ["Ayed", "Ismail Ben", ""], ["Pedersoli", "Marco", ""], ["Dolz", "Jose", ""]]}, {"id": "2003.04070", "submitter": "Xuelin Qian", "authors": "Fangbin Wan, Yang Wu, Xuelin Qian, Yixiong Chen, Yanwei Fu", "title": "When Person Re-identification Meets Changing Clothes", "comments": "Accepted by CVPRW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is now an active research topic for AI-based\nvideo surveillance applications such as specific person search, but the\npractical issue that the target person(s) may change clothes (clothes\ninconsistency problem) has been overlooked for long. For the first time, this\npaper systematically studies this problem. We first overcome the difficulty of\nlack of suitable dataset, by collecting a small yet representative real dataset\nfor testing whilst building a large realistic synthetic dataset for training\nand deeper studies. Facilitated by our new datasets, we are able to conduct\nvarious interesting new experiments for studying the influence of clothes\ninconsistency. We find that changing clothes makes ReID a much harder problem\nin the sense of bringing difficulties to learning effective representations and\nalso challenges the generalization ability of previous ReID models to identify\npersons with unseen (new) clothes. Representative existing ReID models are\nadopted to show informative results on such a challenging setting, and we also\nprovide some preliminary efforts on improving the robustness of existing models\non handling the clothes inconsistency issue in the data. We believe that this\nstudy can be inspiring and helpful for encouraging more researches in this\ndirection. The dataset is available on the project website:\nhttps://wanfb.github.io/dataset.html.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:32:16 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 04:20:35 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 01:53:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Wan", "Fangbin", ""], ["Wu", "Yang", ""], ["Qian", "Xuelin", ""], ["Chen", "Yixiong", ""], ["Fu", "Yanwei", ""]]}, {"id": "2003.04092", "submitter": "Zitong Yu", "authors": "Zitong Yu, Chenxu Zhao, Zezheng Wang, Yunxiao Qin, Zhuo Su, Xiaobai\n  Li, Feng Zhou, Guoying Zhao", "title": "Searching Central Difference Convolutional Networks for Face\n  Anti-Spoofing", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing (FAS) plays a vital role in face recognition systems. Most\nstate-of-the-art FAS methods 1) rely on stacked convolutions and\nexpert-designed network, which is weak in describing detailed fine-grained\ninformation and easily being ineffective when the environment varies (e.g.,\ndifferent illumination), and 2) prefer to use long sequence as input to extract\ndynamic features, making them difficult to deploy into scenarios which need\nquick response. Here we propose a novel frame level FAS method based on Central\nDifference Convolution (CDC), which is able to capture intrinsic detailed\npatterns via aggregating both intensity and gradient information. A network\nbuilt with CDC, called the Central Difference Convolutional Network (CDCN), is\nable to provide more robust modeling capacity than its counterpart built with\nvanilla convolution. Furthermore, over a specifically designed CDC search\nspace, Neural Architecture Search (NAS) is utilized to discover a more powerful\nnetwork structure (CDCN++), which can be assembled with Multiscale Attention\nFusion Module (MAFM) for further boosting performance. Comprehensive\nexperiments are performed on six benchmark datasets to show that 1) the\nproposed method not only achieves superior performance on intra-dataset testing\n(especially 0.2% ACER in Protocol-1 of OULU-NPU dataset), 2) it also\ngeneralizes well on cross-dataset testing (particularly 6.5% HTER from\nCASIA-MFSD to Replay-Attack datasets). The codes are available at\n\\href{https://github.com/ZitongYu/CDCN}{https://github.com/ZitongYu/CDCN}.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:48:37 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Yu", "Zitong", ""], ["Zhao", "Chenxu", ""], ["Wang", "Zezheng", ""], ["Qin", "Yunxiao", ""], ["Su", "Zhuo", ""], ["Li", "Xiaobai", ""], ["Zhou", "Feng", ""], ["Zhao", "Guoying", ""]]}, {"id": "2003.04094", "submitter": "Jacek Dabrowski", "authors": "Mikolaj Wieczorek (1), Andrzej Michalowski (1), Anna Wroblewska (1 and\n  2), Jacek Dabrowski (1) ((1) Synerise, (2) Warsaw University of Technology)", "title": "A Strong Baseline for Fashion Retrieval with Person Re-Identification\n  Models", "comments": "33 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion retrieval is the challenging task of finding an exact match for\nfashion items contained within an image. Difficulties arise from the\nfine-grained nature of clothing items, very large intra-class and inter-class\nvariance. Additionally, query and source images for the task usually come from\ndifferent domains - street photos and catalogue photos respectively. Due to\nthese differences, a significant gap in quality, lighting, contrast, background\nclutter and item presentation exists between domains. As a result, fashion\nretrieval is an active field of research both in academia and the industry.\n  Inspired by recent advancements in Person Re-Identification research, we\nadapt leading ReID models to be used in fashion retrieval tasks. We introduce a\nsimple baseline model for fashion retrieval, significantly outperforming\nprevious state-of-the-art results despite a much simpler architecture. We\nconduct in-depth experiments on Street2Shop and DeepFashion datasets and\nvalidate our results. Finally, we propose a cross-domain (cross-dataset)\nevaluation method to test the robustness of fashion retrieval models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:50:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wieczorek", "Mikolaj", "", "Synerise"], ["Michalowski", "Andrzej", "", "Synerise"], ["Wroblewska", "Anna", "", "1 and\n  2"], ["Dabrowski", "Jacek", "", "Synerise"]]}, {"id": "2003.04116", "submitter": "M. G. Sarwar Murshed", "authors": "M. G. Sarwar Murshed, Edward Verenich, James J. Carroll, Nazar Khan,\n  Faraz Hussain", "title": "Hazard Detection in Supermarkets using Deep Learning on the Edge", "comments": "6 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supermarkets need to ensure clean and safe environments for both shoppers and\nemployees. Slips, trips, and falls can result in injuries that have a physical\nas well as financial cost. Timely detection of hazardous conditions such as\nspilled liquids or fallen items on supermarket floors can reduce the chances of\nserious injuries. This paper presents EdgeLite, a novel, lightweight deep\nlearning model for easy deployment and inference on resource-constrained\ndevices. We describe the use of EdgeLite on two edge devices for detecting\nsupermarket floor hazards. On a hazard detection dataset that we developed,\nEdgeLite, when deployed on edge devices, outperformed six state-of-the-art\nobject detection models in terms of accuracy while having comparable memory\nusage and inference time.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 18:43:55 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Murshed", "M. G. Sarwar", ""], ["Verenich", "Edward", ""], ["Carroll", "James J.", ""], ["Khan", "Nazar", ""], ["Hussain", "Faraz", ""]]}, {"id": "2003.04117", "submitter": "M. G. Sarwar Murshed", "authors": "Edward Verenich, Alvaro Velasquez, M.G. Sarwar Murshed, Faraz Hussain", "title": "The Utility of Feature Reuse: Transfer Learning in Data-Starved Regimes", "comments": "3 pages, 1 figure, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of transfer learning with deep neural networks has increasingly\nbecome widespread for deploying well-tested computer vision systems to newer\ndomains, especially those with limited datasets. We describe a transfer\nlearning use case for a domain with a data-starved regime, having fewer than\n100 labeled target samples. We evaluate the effectiveness of convolutional\nfeature extraction and fine-tuning of overparameterized models with respect to\nthe size of target training data, as well as their generalization performance\non data with covariate shift, or out-of-distribution (OOD) data. Our\nexperiments show that both overparameterization and feature reuse contribute to\nsuccessful application of transfer learning in training image classifiers in\ndata-starved regimes.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 18:48:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Verenich", "Edward", ""], ["Velasquez", "Alvaro", ""], ["Murshed", "M. G. Sarwar", ""], ["Hussain", "Faraz", ""]]}, {"id": "2003.04132", "submitter": "Weilin Huang", "authors": "Chenfan Zhuang, Xintong Han, Weilin Huang, Matthew R. Scott", "title": "iFAN: Image-Instance Full Alignment Networks for Adaptive Object\n  Detection", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training an object detector on a data-rich domain and applying it to a\ndata-poor one with limited performance drop is highly attractive in industry,\nbecause it saves huge annotation cost. Recent research on unsupervised domain\nadaptive object detection has verified that aligning data distributions between\nsource and target images through adversarial learning is very useful. The key\nis when, where and how to use it to achieve best practice. We propose\nImage-Instance Full Alignment Networks (iFAN) to tackle this problem by\nprecisely aligning feature distributions on both image and instance levels: 1)\nImage-level alignment: multi-scale features are roughly aligned by training\nadversarial domain classifiers in a hierarchically-nested fashion. 2) Full\ninstance-level alignment: deep semantic information and elaborate instance\nrepresentations are fully exploited to establish a strong relationship among\ncategories and domains. Establishing these correlations is formulated as a\nmetric learning problem by carefully constructing instance pairs.\nAbove-mentioned adaptations can be integrated into an object detector (e.g.\nFaster RCNN), resulting in an end-to-end trainable framework where multiple\nalignments can work collaboratively in a coarse-tofine manner. In two domain\nadaptation tasks: synthetic-to-real (SIM10K->Cityscapes) and normal-to-foggy\nweather (Cityscapes->Foggy Cityscapes), iFAN outperforms the state-of-the-art\nmethods with a boost of 10%+ AP over the source-only baseline.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:27:06 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhuang", "Chenfan", ""], ["Han", "Xintong", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "2003.04138", "submitter": "Dimitris Kamilis", "authors": "Dimitris Kamilis, Mario Blatter, Nick Polydorides", "title": "Learned Spectral Computed Tomography", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral Photon-Counting Computed Tomography (SPCCT) is a promising\ntechnology that has shown a number of advantages over conventional X-ray\nComputed Tomography (CT) in the form of material separation, artefact removal\nand enhanced image quality. However, due to the increased complexity and\nnon-linearity of the SPCCT governing equations, model-based reconstruction\nalgorithms typically require handcrafted regularisation terms and meticulous\ntuning of hyperparameters making them impractical to calibrate in variable\nconditions. Additionally, they typically incur high computational costs and in\ncases of limited-angle data, their imaging capability deteriorates\nsignificantly. Recently, Deep Learning has proven to provide state-of-the-art\nreconstruction performance in medical imaging applications while circumventing\nmost of these challenges. Inspired by these advances, we propose a Deep\nLearning imaging method for SPCCT that exploits the expressive power of Neural\nNetworks while also incorporating model knowledge. The method takes the form of\na two-step learned primal-dual algorithm that is trained using case-specific\ndata. The proposed approach is characterised by fast reconstruction capability\nand high imaging performance, even in limited-data cases, while avoiding the\nhand-tuning that is required by other optimisation approaches. We demonstrate\nthe performance of the method in terms of reconstructed images and quality\nmetrics via numerical examples inspired by the application of cardiovascular\nimaging.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:39:12 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Kamilis", "Dimitris", ""], ["Blatter", "Mario", ""], ["Polydorides", "Nick", ""]]}, {"id": "2003.04145", "submitter": "Jialin Gao", "authors": "Jialin Gao, Zhixiang Shi, Jiani Li, Guanshuo Wang, Yufeng Yuan,\n  Shiming Ge, and Xi Zhou", "title": "Accurate Temporal Action Proposal Generation with Relation-Aware Pyramid\n  Network", "comments": "accepted by AAAI-20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate temporal action proposals play an important role in detecting\nactions from untrimmed videos. The existing approaches have difficulties in\ncapturing global contextual information and simultaneously localizing actions\nwith different durations. To this end, we propose a Relation-aware pyramid\nNetwork (RapNet) to generate highly accurate temporal action proposals. In\nRapNet, a novel relation-aware module is introduced to exploit bi-directional\nlong-range relations between local features for context distilling. This\nembedded module enhances the RapNet in terms of its multi-granularity temporal\nproposal generation ability, given predefined anchor boxes. We further\nintroduce a two-stage adjustment scheme to refine the proposal boundaries and\nmeasure their confidence in containing an action with snippet-level actionness.\nExtensive experiments on the challenging ActivityNet and THUMOS14 benchmarks\ndemonstrate our RapNet generates superior accurate proposals over the existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:47:36 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Gao", "Jialin", ""], ["Shi", "Zhixiang", ""], ["Li", "Jiani", ""], ["Wang", "Guanshuo", ""], ["Yuan", "Yufeng", ""], ["Ge", "Shiming", ""], ["Zhou", "Xi", ""]]}, {"id": "2003.04151", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodr\\'iguez, Issam Laradji, Alexandre Drouin, Alexandre Lacoste", "title": "Embedding Propagation: Smoother Manifold for Few-Shot Classification", "comments": "Published at ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot classification is challenging because the data distribution of the\ntraining set can be widely different to the test set as their classes are\ndisjoint. This distribution shift often results in poor generalization.\nManifold smoothing has been shown to address the distribution shift problem by\nextending the decision boundaries and reducing the noise of the class\nrepresentations. Moreover, manifold smoothness is a key factor for\nsemi-supervised learning and transductive learning algorithms. In this work, we\npropose to use embedding propagation as an unsupervised non-parametric\nregularizer for manifold smoothing in few-shot classification. Embedding\npropagation leverages interpolations between the extracted features of a neural\nnetwork based on a similarity graph. We empirically show that embedding\npropagation yields a smoother embedding manifold. We also show that applying\nembedding propagation to a transductive classifier achieves new\nstate-of-the-art results in mini-Imagenet, tiered-Imagenet, Imagenet-FS, and\nCUB. Furthermore, we show that embedding propagation consistently improves the\naccuracy of the models in multiple semi-supervised learning scenarios by up to\n16\\% points. The proposed embedding propagation operation can be easily\nintegrated as a non-parametric layer into a neural network. We provide the\ntraining code and usage examples at\nhttps://github.com/ElementAI/embedding-propagation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:51:09 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 15:14:03 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Rodr\u00edguez", "Pau", ""], ["Laradji", "Issam", ""], ["Drouin", "Alexandre", ""], ["Lacoste", "Alexandre", ""]]}, {"id": "2003.04168", "submitter": "Andres Marrugo", "authors": "Raul Vargas, Andres G. Marrugo, Song Zhang, Lenny A. Romero", "title": "Hybrid calibration procedure for fringe projection profilometry based on\n  stereo-vision and polynomial fitting", "comments": "Accepted for publication in Applied Optics Vol. 59 No. 13, 2020", "journal-ref": null, "doi": "10.1364/AO.383602", "report-no": null, "categories": "physics.ins-det cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The key to accurate 3D shape measurement in Fringe Projection Profilometry\n(FPP) is the proper calibration of the measurement system. Current calibration\ntechniques rely on phase-coordinate mapping (PCM) or back-projection\nstereo-vision (SV) methods. PCM methods are cumbersome to implement as they\nrequire precise positioning of the calibration target relative to the FPP\nsystem but produce highly accurate measurements within the calibration volume.\nSV methods generally do not achieve the same accuracy level. However, the\ncalibration is more flexible in that the calibration target can be arbitrarily\npositioned. In this work, we propose a hybrid calibration method that leverages\nthe SV calibration approach using a PCM method to achieve higher accuracy. The\nmethod has the flexibility of SV methods, is robust to lens distortions, and\nhas a simple relation between the recovered phase and the metric coordinates.\nExperimental results show that the proposed Hybrid method outperforms the SV\nmethod in terms of accuracy and reconstruction time due to its low\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:25:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Vargas", "Raul", ""], ["Marrugo", "Andres G.", ""], ["Zhang", "Song", ""], ["Romero", "Lenny A.", ""]]}, {"id": "2003.04169", "submitter": "Yu Chen", "authors": "Seyed Yahya Nikouei, Yu Chen, Alexander Aved, Erik Blasch", "title": "I-ViSE: Interactive Video Surveillance as an Edge Service using\n  Unsupervised Feature Queries", "comments": "R1 is under review by the IEEE Internet of Things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situation AWareness (SAW) is essential for many mission critical\napplications. However, SAW is very challenging when trying to immediately\nidentify objects of interest or zoom in on suspicious activities from thousands\nof video frames. This work aims at developing a queryable system to instantly\nselect interesting content. While face recognition technology is mature, in\nmany scenarios like public safety monitoring, the features of objects of\ninterest may be much more complicated than face features. In addition, human\noperators may not be always able to provide a descriptive, simple, and accurate\nquery. Actually, it is more often that there are only rough, general\ndescriptions of certain suspicious objects or accidents. This paper proposes an\nInteractive Video Surveillance as an Edge service (I-ViSE) based on\nunsupervised feature queries. Adopting unsupervised methods that do not reveal\nany private information, the I-ViSE scheme utilizes general features of a human\nbody and color of clothes. An I-ViSE prototype is built following the edge-fog\ncomputing paradigm and the experimental results verified the I-ViSE scheme\nmeets the design goal of scene recognition in less than two seconds.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 14:26:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Nikouei", "Seyed Yahya", ""], ["Chen", "Yu", ""], ["Aved", "Alexander", ""], ["Blasch", "Erik", ""]]}, {"id": "2003.04188", "submitter": "Jorge Beltr\\'an", "authors": "Alejandro Barrera, Carlos Guindel, Jorge Beltr\\'an and Fernando\n  Garc\\'ia", "title": "BirdNet+: End-to-End 3D Object Detection in LiDAR Bird's Eye View", "comments": "Submitted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-board 3D object detection in autonomous vehicles often relies on geometry\ninformation captured by LiDAR devices. Albeit image features are typically\npreferred for detection, numerous approaches take only spatial data as input.\nExploiting this information in inference usually involves the use of compact\nrepresentations such as the Bird's Eye View (BEV) projection, which entails a\nloss of information and thus hinders the joint inference of all the parameters\nof the objects' 3D boxes. In this paper, we present a fully end-to-end 3D\nobject detection framework that can infer oriented 3D boxes solely from BEV\nimages by using a two-stage object detector and ad-hoc regression branches,\neliminating the need for a post-processing stage. The method outperforms its\npredecessor (BirdNet) by a large margin and obtains state-of-the-art results on\nthe KITTI 3D Object Detection Benchmark for all the categories in evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:08:40 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Barrera", "Alejandro", ""], ["Guindel", "Carlos", ""], ["Beltr\u00e1n", "Jorge", ""], ["Garc\u00eda", "Fernando", ""]]}, {"id": "2003.04191", "submitter": "Sara Iodice", "authors": "Nima Mohammadi Meshky, Sara Iodice, Krystian Mikolajczyk", "title": "Domain Adversarial Training for Infrared-colour Person Re-Identification", "comments": null, "journal-ref": "ICDP 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is a very active area of research in\ncomputer vision, due to the role it plays in video surveillance. Currently,\nmost methods only address the task of matching between colour images. However,\nin poorly-lit environments CCTV cameras switch to infrared imaging, hence\ndeveloping a system which can correctly perform matching between infrared and\ncolour images is a necessity. In this paper, we propose a part-feature\nextraction network to better focus on subtle, unique signatures on the person\nwhich are visible across both infrared and colour modalities. To train the\nmodel we propose a novel variant of the domain adversarial feature-learning\nframework. Through extensive experimentation, we show that our approach\noutperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:17:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Meshky", "Nima Mohammadi", ""], ["Iodice", "Sara", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "2003.04194", "submitter": "Thai Son Nguyen", "authors": "Thai-Son Nguyen, Sebastian St\\\"uker, Alex Waibel", "title": "Toward Cross-Domain Speech Recognition with End-to-End Models", "comments": "Presented in Life-Long Learning for Spoken Language Systems Workshop\n  - ASRU 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of multi-domain speech recognition, research in the past focused\non hybrid acoustic models to build cross-domain and domain-invariant speech\nrecognition systems. In this paper, we empirically examine the difference in\nbehavior between hybrid acoustic models and neural end-to-end systems when\nmixing acoustic training data from several domains. For these experiments we\ncomposed a multi-domain dataset from public sources, with the different domains\nin the corpus covering a wide variety of topics and acoustic conditions such as\ntelephone conversations, lectures, read speech and broadcast news. We show that\nfor the hybrid models, supplying additional training data from other domains\nwith mismatched acoustic conditions does not increase the performance on\nspecific domains. However, our end-to-end models optimized with sequence-based\ncriterion generalize better than the hybrid models on diverse domains. In term\nof word-error-rate performance, our experimental acoustic-to-word and\nattention-based models trained on multi-domain dataset reach the performance of\ndomain-specific long short-term memory (LSTM) hybrid models, thus resulting in\nmulti-domain speech recognition systems that do not suffer in performance over\ndomain specific ones. Moreover, the use of neural end-to-end models eliminates\nthe need of domain-adapted language models during recognition, which is a great\nadvantage when the input domain is unknown.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:19:53 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Nguyen", "Thai-Son", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "2003.04210", "submitter": "Arun Balajee Vasudevan", "authors": "Arun Balajee Vasudevan, Dengxin Dai, Luc Van Gool", "title": "Semantic Object Prediction and Spatial Sound Super-Resolution with\n  Binaural Sounds", "comments": "Project page:\n  https://www.trace.ethz.ch/publications/2020/sound_perception/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can robustly recognize and localize objects by integrating visual and\nauditory cues. While machines are able to do the same now with images, less\nwork has been done with sounds. This work develops an approach for dense\nsemantic labelling of sound-making objects, purely based on binaural sounds. We\npropose a novel sensor setup and record a new audio-visual dataset of street\nscenes with eight professional binaural microphones and a 360 degree camera.\nThe co-existence of visual and audio cues is leveraged for supervision\ntransfer. In particular, we employ a cross-modal distillation framework that\nconsists of a vision `teacher' method and a sound `student' method -- the\nstudent method is trained to generate the same results as the teacher method.\nThis way, the auditory system can be trained without using human annotations.\nWe also propose two auxiliary tasks namely, a) a novel task on Spatial Sound\nSuper-resolution to increase the spatial resolution of sounds, and b) dense\ndepth prediction of the scene. We then formulate the three tasks into one\nend-to-end trainable multi-tasking network aiming to boost the overall\nperformance. Experimental results on the dataset show that 1) our method\nachieves promising results for semantic prediction and the two auxiliary tasks;\nand 2) the three tasks are mutually beneficial -- training them together\nachieves the best performance and 3) the number and orientations of microphones\nare both important. The data and code will be released to facilitate the\nresearch in this new direction.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 15:49:01 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Vasudevan", "Arun Balajee", ""], ["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "2003.04226", "submitter": "Gal Oren", "authors": "Matan Rusanovsky, Gal Oren, Sigalit Ifergane, Ofer Beeri", "title": "MLography: An Automated Quantitative Metallography Model for Impurities\n  Anomaly Detection using Novel Data Mining and Deep Learning Approach", "comments": "9 pages, 8 figures, 3 algorithms, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The micro-structure of most of the engineering alloys contains some\ninclusions and precipitates, which may affect their properties, therefore it is\ncrucial to characterize them. In this work we focus on the development of a\nstate-of-the-art artificial intelligence model for Anomaly Detection named\nMLography to automatically quantify the degree of anomaly of impurities in\nalloys. For this purpose, we introduce several anomaly detection measures:\nSpatial, Shape and Area anomaly, that successfully detect the most anomalous\nobjects based on their objective, given that the impurities were already\nlabeled. The first two measures quantify the degree of anomaly of each object\nby how each object is distant and big compared to its neighborhood, and by the\nabnormally of its own shape respectively. The last measure, combines the former\ntwo and highlights the most anomalous regions among all input images, for later\n(physical) examination. The performance of the model is presented and analyzed\nbased on few representative cases. We stress that although the models presented\nhere were developed for metallography analysis, most of them can be generalized\nto a wider set of problems in which anomaly detection of geometrical objects is\ndesired. All models as well as the data-set that was created for this work, are\npublicly available at: https://github.com/matanr/MLography.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 15:17:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Rusanovsky", "Matan", ""], ["Oren", "Gal", ""], ["Ifergane", "Sigalit", ""], ["Beeri", "Ofer", ""]]}, {"id": "2003.04232", "submitter": "Srikrishna Karanam", "authors": "Georgios Georgakis, Ren Li, Srikrishna Karanam, Terrence Chen, Jana\n  Kosecka, Ziyan Wu", "title": "Hierarchical Kinematic Human Mesh Recovery", "comments": "17 pages, 8 figures, 5 tables, ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a parametric model of 3D human mesh\nfrom a single image. While there has been substantial recent progress in this\narea with direct regression of model parameters, these methods only implicitly\nexploit the human body kinematic structure, leading to sub-optimal use of the\nmodel prior. In this work, we address this gap by proposing a new technique for\nregression of human parametric model that is explicitly informed by the known\nhierarchical structure, including joint interdependencies of the model. This\nresults in a strong prior-informed design of the regressor architecture and an\nassociated hierarchical optimization that is flexible to be used in conjunction\nwith the current standard frameworks for 3D human mesh recovery. We demonstrate\nthese aspects by means of extensive experiments on standard benchmark datasets,\nshowing how our proposed new design outperforms several existing and popular\nmethods, establishing new state-of-the-art results. By considering joint\ninterdependencies, our method is equipped to infer joints even under data\ncorruptions, which we demonstrate by conducting experiments under varying\ndegrees of occlusion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:15:11 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 17:01:33 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Georgakis", "Georgios", ""], ["Li", "Ren", ""], ["Karanam", "Srikrishna", ""], ["Chen", "Terrence", ""], ["Kosecka", "Jana", ""], ["Wu", "Ziyan", ""]]}, {"id": "2003.04253", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, Ling Shao", "title": "Motion-Attentive Transition for Zero-Shot Video Object Segmentation", "comments": "AAAI 2020. Code: https://github.com/tfzhou/MATNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel Motion-Attentive Transition Network\n(MATNet) for zero-shot video object segmentation, which provides a new way of\nleveraging motion information to reinforce spatio-temporal object\nrepresentation. An asymmetric attention block, called Motion-Attentive\nTransition (MAT), is designed within a two-stream encoder, which transforms\nappearance features into motion-attentive representations at each convolutional\nstage. In this way, the encoder becomes deeply interleaved, allowing for\nclosely hierarchical interactions between object motion and appearance. This is\nsuperior to the typical two-stream architecture, which treats motion and\nappearance separately in each stream and often suffers from overfitting to\nappearance information. Additionally, a bridge network is proposed to obtain a\ncompact, discriminative and scale-sensitive representation for multi-level\nencoder features, which is further fed into a decoder to achieve segmentation\nresults. Extensive experiments on three challenging public benchmarks (i.e.\nDAVIS-16, FBMS and Youtube-Objects) show that our model achieves compelling\nperformance against the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:58:42 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 08:02:47 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 17:34:32 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Zhou", "Tianfei", ""], ["Wang", "Shunzhou", ""], ["Zhou", "Yi", ""], ["Yao", "Yazhou", ""], ["Li", "Jianwu", ""], ["Shao", "Ling", ""]]}, {"id": "2003.04260", "submitter": "Weimin Wang", "authors": "Weimin Wang, Shohei Nobuhara, Ryosuke Nakamura, Ken Sakurada", "title": "SOIC: Semantic Online Initialization and Calibration for LiDAR and\n  Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel semantic-based online extrinsic calibration\napproach, SOIC (so, I see), for Light Detection and Ranging (LiDAR) and camera\nsensors. Previous online calibration methods usually need prior knowledge of\nrough initial values for optimization. The proposed approach removes this\nlimitation by converting the initialization problem to a Perspective-n-Point\n(PnP) problem with the introduction of semantic centroids (SCs). The\nclosed-form solution of this PnP problem has been well researched and can be\nfound with existing PnP methods. Since the semantic centroid of the point cloud\nusually does not accurately match with that of the corresponding image, the\naccuracy of parameters are not improved even after a nonlinear refinement\nprocess. Thus, a cost function based on the constraint of the correspondence\nbetween semantic elements from both point cloud and image data is formulated.\nSubsequently, optimal extrinsic parameters are estimated by minimizing the cost\nfunction. We evaluate the proposed method either with GT or predicted semantics\non KITTI dataset. Experimental results and comparisons with the baseline method\nverify the feasibility of the initialization strategy and the accuracy of the\ncalibration approach. In addition, we release the source code at\nhttps://github.com/--/SOIC.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:02:31 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wang", "Weimin", ""], ["Nobuhara", "Shohei", ""], ["Nakamura", "Ryosuke", ""], ["Sakurada", "Ken", ""]]}, {"id": "2003.04262", "submitter": "Tianfei Zhou", "authors": "Tianfei Zhou, Wenguan Wang, Siyuan Qi, Haibin Ling, Jianbing Shen", "title": "Cascaded Human-Object Interaction Recognition", "comments": "Accepted to CVPR 2020. Winner of the ICCV-2019 PIC Challenge on both\n  HOIW and PIC tracks. Code: https://github.com/tfzhou/C-HOI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid progress has been witnessed for human-object interaction (HOI)\nrecognition, but most existing models are confined to single-stage reasoning\npipelines. Considering the intrinsic complexity of the task, we introduce a\ncascade architecture for a multi-stage, coarse-to-fine HOI understanding. At\neach stage, an instance localization network progressively refines HOI\nproposals and feeds them into an interaction recognition network. Each of the\ntwo networks is also connected to its predecessor at the previous stage,\nenabling cross-stage information propagation. The interaction recognition\nnetwork has two crucial parts: a relation ranking module for high-quality HOI\nproposal selection and a triple-stream classifier for relation prediction. With\nour carefully-designed human-centric relation features, these two modules work\ncollaboratively towards effective interaction understanding. Further beyond\nrelation detection on a bounding-box level, we make our framework flexible to\nperform fine-grained pixel-wise relation segmentation; this provides a new\nglimpse into better relation modeling. Our approach reached the $1^{st}$ place\nin the ICCV2019 Person in Context Challenge, on both relation detection and\nsegmentation tasks. It also shows promising results on V-COCO.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:05:04 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:54:41 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zhou", "Tianfei", ""], ["Wang", "Wenguan", ""], ["Qi", "Siyuan", ""], ["Ling", "Haibin", ""], ["Shen", "Jianbing", ""]]}, {"id": "2003.04276", "submitter": "Kaicheng Yu", "authors": "Kaicheng Yu and Rene Ranftl and Mathieu Salzmann", "title": "How to Train Your Super-Net: An Analysis of Training Heuristics in\n  Weight-Sharing NAS", "comments": "Updated with latest results on NASBench-101, now we achieve 0.48\n  sparse Kendall-Tau on this space", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weight sharing promises to make neural architecture search (NAS) tractable\neven on commodity hardware. Existing methods in this space rely on a diverse\nset of heuristics to design and train the shared-weight backbone network,\na.k.a. the super-net. Since heuristics and hyperparameters substantially vary\nacross different methods, a fair comparison between them can only be achieved\nby systematically analyzing the influence of these factors. In this paper, we\ntherefore provide a systematic evaluation of the heuristics and hyperparameters\nthat are frequently employed by weight-sharing NAS algorithms. Our analysis\nuncovers that some commonly-used heuristics for super-net training negatively\nimpact the correlation between super-net and stand-alone performance, and\nevidences the strong influence of certain hyperparameters and architectural\nchoices. Our code and experiments set a strong and reproducible baseline that\nfuture works can build on.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:34:32 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 13:42:15 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Yu", "Kaicheng", ""], ["Ranftl", "Rene", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "2003.04279", "submitter": "Seunghwan Lee", "authors": "Seunghwan Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim", "title": "Restore from Restored: Video Restoration with Pseudo Clean Video", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a self-supervised video denoising method called\n\"restore-from-restored.\" This method fine-tunes a pre-trained network by using\na pseudo clean video during the test phase. The pseudo clean video is obtained\nby applying a noisy video to the baseline network. By adopting a fully\nconvolutional neural network (FCN) as the baseline, we can improve video\ndenoising performance without accurate optical flow estimation and registration\nsteps, in contrast to many conventional video restoration methods, due to the\ntranslation equivariant property of the FCN. Specifically, the proposed method\ncan take advantage of plentiful similar patches existing across multiple\nconsecutive frames (i.e., patch-recurrence); these patches can boost the\nperformance of the baseline network by a large margin. We analyze the\nrestoration performance of the fine-tuned video denoising networks with the\nproposed self-supervision-based learning algorithm, and demonstrate that the\nFCN can utilize recurring patches without requiring accurate registration among\nadjacent frames. In our experiments, we apply the proposed method to\nstate-of-the-art denoisers and show that our fine-tuned networks achieve a\nconsiderable improvement in denoising performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:37:28 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 06:36:59 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 04:46:32 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lee", "Seunghwan", ""], ["Cho", "Donghyeon", ""], ["Kim", "Jiwon", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2003.04285", "submitter": "Behzad Ghazanfari", "authors": "Behzad Ghazanfari, Fatemeh Afghah", "title": "Deep Inverse Feature Learning: A Representation Learning of Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel perspective about error in machine learning and\nproposes inverse feature learning (IFL) as a representation learning approach\nthat learns a set of high-level features based on the representation of error\nfor classification or clustering purposes. The proposed perspective about error\nrepresentation is fundamentally different from current learning methods, where\nin classification approaches they interpret the error as a function of the\ndifferences between the true labels and the predicted ones or in clustering\napproaches, in which the clustering objective functions such as compactness are\nused. Inverse feature learning method operates based on a deep clustering\napproach to obtain a qualitative form of the representation of error as\nfeatures. The performance of the proposed IFL method is evaluated by applying\nthe learned features along with the original features, or just using the\nlearned features in different classification and clustering techniques for\nseveral data sets. The experimental results show that the proposed method leads\nto promising results in classification and especially in clustering. In\nclassification, the proposed features along with the primary features improve\nthe results of most of the classification methods on several popular data sets.\nIn clustering, the performance of different clustering methods is considerably\nimproved on different data sets. There are interesting results that show some\nfew features of the representation of error capture highly informative aspects\nof primary features. We hope this paper helps to utilize the error\nrepresentation learning in different feature learning domains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:45:44 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Ghazanfari", "Behzad", ""], ["Afghah", "Fatemeh", ""]]}, {"id": "2003.04286", "submitter": "Charles Jin", "authors": "Charles Jin, Martin Rinard", "title": "Manifold Regularization for Locally Stable Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply concepts from manifold regularization to develop new regularization\ntechniques for training locally stable deep neural networks. Our regularizers\nare based on a sparsification of the graph Laplacian which holds with high\nprobability when the data is sparse in high dimensions, as is common in deep\nlearning. Empirically, our networks exhibit stability in a diverse set of\nperturbation models, including $\\ell_2$, $\\ell_\\infty$, and Wasserstein-based\nperturbations; in particular, we achieve 40% adversarial accuracy on CIFAR-10\nagainst an adaptive PGD attack using $\\ell_\\infty$ perturbations of size\n$\\epsilon = 8/255$, and state-of-the-art verified accuracy of 21% in the same\nperturbation model. Furthermore, our techniques are efficient, incurring\noverhead on par with two additional parallel forward passes through the\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:45:44 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 22:53:45 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Jin", "Charles", ""], ["Rinard", "Martin", ""]]}, {"id": "2003.04289", "submitter": "Jing Yang", "authors": "Jing Yang, Brais Martinez, Adrian Bulat, Georgios Tzimiropoulos", "title": "Knowledge distillation via adaptive instance normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of model compression via knowledge\ndistillation. To this end, we propose a new knowledge distillation method based\non transferring feature statistics, specifically the channel-wise mean and\nvariance, from the teacher to the student. Our method goes beyond the standard\nway of enforcing the mean and variance of the student to be similar to those of\nthe teacher through an $L_2$ loss, which we found it to be of limited\neffectiveness. Specifically, we propose a new loss based on adaptive instance\nnormalization to effectively transfer the feature statistics. The main idea is\nto transfer the learned statistics back to the teacher via adaptive instance\nnormalization (conditioned on the student) and let the teacher network\n\"evaluate\" via a loss whether the statistics learned by the student are\nreliably transferred. We show that our distillation method outperforms other\nstate-of-the-art distillation methods over a large set of experimental settings\nincluding different (a) network architectures, (b) teacher-student capacities,\n(c) datasets, and (d) domains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:50:12 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Yang", "Jing", ""], ["Martinez", "Brais", ""], ["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2003.04296", "submitter": "Jun Chen", "authors": "Jun Chen, Yong Liu, Hao Zhang, Shengnan Hou, Jian Yang", "title": "Propagating Asymptotic-Estimated Gradients for Low Bitwidth Quantized\n  Neural Networks", "comments": "This paper has been accepted for publication in the IEEE Journal of\n  Selected Topics in Signal Processing", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing 2020", "doi": "10.1109/JSTSP.2020.2966327", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantized neural networks (QNNs) can be useful for neural network\nacceleration and compression, but during the training process they pose a\nchallenge: how to propagate the gradient of loss function through the graph\nflow with a derivative of 0 almost everywhere. In response to this\nnon-differentiable situation, we propose a novel Asymptotic-Quantized Estimator\n(AQE) to estimate the gradient. In particular, during back-propagation, the\ngraph that relates inputs to output remains smoothness and differentiability.\nAt the end of training, the weights and activations have been quantized to\nlow-precision because of the asymptotic behaviour of AQE. Meanwhile, we propose\na M-bit Inputs and N-bit Weights Network (MINW-Net) trained by AQE, a quantized\nneural network with 1-3 bits weights and activations. In the inference phase,\nwe can use XNOR or SHIFT operations instead of convolution operations to\naccelerate the MINW-Net. Our experiments on CIFAR datasets demonstrate that our\nAQE is well defined, and the QNNs with AQE perform better than that with\nStraight-Through Estimator (STE). For example, in the case of the same ConvNet\nthat has 1-bit weights and activations, our MINW-Net with AQE can achieve a\nprediction accuracy 1.5\\% higher than the Binarized Neural Network (BNN) with\nSTE. The MINW-Net, which is trained from scratch by AQE, can achieve comparable\nclassification accuracy as 32-bit counterparts on CIFAR test sets. Extensive\nexperimental results on ImageNet dataset show great superiority of the proposed\nAQE and our MINW-Net achieves comparable results with other state-of-the-art\nQNNs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:17:47 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Chen", "Jun", ""], ["Liu", "Yong", ""], ["Zhang", "Hao", ""], ["Hou", "Shengnan", ""], ["Yang", "Jian", ""]]}, {"id": "2003.04297", "submitter": "Kaiming He", "authors": "Xinlei Chen and Haoqi Fan and Ross Girshick and Kaiming He", "title": "Improved Baselines with Momentum Contrastive Learning", "comments": "Tech report, 2 pages + references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contrastive unsupervised learning has recently shown encouraging progress,\ne.g., in Momentum Contrast (MoCo) and SimCLR. In this note, we verify the\neffectiveness of two of SimCLR's design improvements by implementing them in\nthe MoCo framework. With simple modifications to MoCo---namely, using an MLP\nprojection head and more data augmentation---we establish stronger baselines\nthat outperform SimCLR and do not require large training batches. We hope this\nwill make state-of-the-art unsupervised learning research more accessible. Code\nwill be made public.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:56:49 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Chen", "Xinlei", ""], ["Fan", "Haoqi", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""]]}, {"id": "2003.04298", "submitter": "Yuki Asano", "authors": "Mandela Patrick, Yuki M. Asano, Polina Kuznetsova, Ruth Fong, Jo\\~ao\n  F. Henriques, Geoffrey Zweig, Andrea Vedaldi", "title": "Multi-modal Self-Supervision from Generalized Data Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of self-supervised learning can be largely attributed to\ncontent-preserving transformations, which can be used to easily induce\ninvariances. While transformations generate positive sample pairs in\ncontrastive loss training, most recent work focuses on developing new objective\nformulations, and pays relatively little attention to the transformations\nthemselves. In this paper, we introduce the framework of Generalized Data\nTransformations to (1) reduce several recent self-supervised learning\nobjectives to a single formulation for ease of comparison, analysis, and\nextension, (2) allow a choice between being invariant or distinctive to data\ntransformations, obtaining different supervisory signals, and (3) derive the\nconditions that combinations of transformations must obey in order to lead to\nwell-posed learning objectives. This framework allows both invariance and\ndistinctiveness to be injected into representations simultaneously, and lets us\nsystematically explore novel contrastive objectives. We apply it to study\nmulti-modal self-supervision for audio-visual representation learning from\nunlabelled videos, improving the state-of-the-art by a large margin, and even\nsurpassing supervised pretraining. We demonstrate results on a variety of\ndownstream video and audio classification and retrieval tasks, on datasets such\nas HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve\nnew state-of-the-art accuracies of 72.8% on HMDB-51 and 95.2% on UCF-101.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:56:49 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 15:24:01 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Patrick", "Mandela", ""], ["Asano", "Yuki M.", ""], ["Kuznetsova", "Polina", ""], ["Fong", "Ruth", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Zweig", "Geoffrey", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "2003.04358", "submitter": "Rahul Sharma", "authors": "Rahul Sharma, Krishna Somandepalli and Shrikanth Narayanan", "title": "Crossmodal learning for audio-visual speech event localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An objective understanding of media depictions, such as about inclusive\nportrayals of how much someone is heard and seen on screen in film and\ntelevision, requires the machines to discern automatically who, when, how and\nwhere someone is talking. Media content is rich in multiple modalities such as\nvisuals and audio which can be used to learn speaker activity in videos. In\nthis work, we present visual representations that have implicit information\nabout when someone is talking and where. We propose a crossmodal neural network\nfor audio speech event detection using the visual frames. We use the learned\nrepresentations for two downstream tasks: i) audio-visual voice activity\ndetection ii) active speaker localization in video frames. We present a\nstate-of-the-art audio-visual voice activity detection system and demonstrate\nthat the learned embeddings can effectively localize to active speakers in the\nvisual frames.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 18:50:50 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sharma", "Rahul", ""], ["Somandepalli", "Krishna", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2003.04367", "submitter": "Bin Kong", "authors": "Quanyu Liao, Xin Wang, Bin Kong, Siwei Lyu, Youbing Yin, Qi Song, Xi\n  Wu", "title": "Category-wise Attack: Transferable Adversarial Examples for Anchor Free\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been demonstrated to be vulnerable to adversarial\nattacks: subtle perturbations can completely change the classification results.\nTheir vulnerability has led to a surge of research in this direction. However,\nmost works dedicated to attacking anchor-based object detection models. In this\nwork, we aim to present an effective and efficient algorithm to generate\nadversarial examples to attack anchor-free object models based on two\napproaches. First, we conduct category-wise instead of instance-wise attacks on\nthe object detectors. Second, we leverage the high-level semantic information\nto generate the adversarial examples. Surprisingly, the generated adversarial\nexamples it not only able to effectively attack the targeted anchor-free object\ndetector but also to be transferred to attack other object detectors, even\nanchor-based detectors such as Faster R-CNN.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 04:49:03 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 16:07:53 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 21:30:41 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 00:14:15 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Liao", "Quanyu", ""], ["Wang", "Xin", ""], ["Kong", "Bin", ""], ["Lyu", "Siwei", ""], ["Yin", "Youbing", ""], ["Song", "Qi", ""], ["Wu", "Xi", ""]]}, {"id": "2003.04377", "submitter": "Olivier Vincent", "authors": "Olivier Vincent, Charley Gros, Joseph Paul Cohen, Julien Cohen-Adad", "title": "Automatic segmentation of spinal multiple sclerosis lesions: How to\n  generalize across MRI contrasts?", "comments": "Presented at OHBM 2020 (v2-3 : corrected typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent improvements in medical image segmentation, the ability to\ngeneralize across imaging contrasts remains an open issue. To tackle this\nchallenge, we implement Feature-wise Linear Modulation (FiLM) to leverage\nphysics knowledge within the segmentation model and learn the characteristics\nof each contrast. Interestingly, a well-optimised U-Net reached the same\nperformance as our FiLMed-Unet on a multi-contrast dataset (0.72 of Dice\nscore), which suggests that there is a bottleneck in spinal MS lesion\nsegmentation different from the generalization across varying contrasts. This\nbottleneck likely stems from inter-rater variability, which is estimated at\n0.61 of Dice score in our dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 19:29:45 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 14:01:34 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 18:33:24 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Vincent", "Olivier", ""], ["Gros", "Charley", ""], ["Cohen", "Joseph Paul", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2003.04382", "submitter": "Qicheng Lao", "authors": "Qicheng Lao, Xiang Jiang, Mohammad Havaei, Yoshua Bengio", "title": "Continuous Domain Adaptation with Variational Domain-Agnostic Feature\n  Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in non-stationary environments is one of the biggest challenges in\nmachine learning. Non-stationarity can be caused by either task drift, i.e.,\nthe drift in the conditional distribution of labels given the input data, or\nthe domain drift, i.e., the drift in the marginal distribution of the input\ndata. This paper aims to tackle this challenge in the context of continuous\ndomain adaptation, where the model is required to learn new tasks adapted to\nnew domains in a non-stationary environment while maintaining previously\nlearned knowledge. To deal with both drifts, we propose variational\ndomain-agnostic feature replay, an approach that is composed of three\ncomponents: an inference module that filters the input data into\ndomain-agnostic representations, a generative module that facilitates knowledge\ntransfer, and a solver module that applies the filtered and transferable\nknowledge to solve the queries. We address the two fundamental scenarios in\ncontinuous domain adaptation, demonstrating the effectiveness of our proposed\napproach for practical usage.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 19:50:24 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Lao", "Qicheng", ""], ["Jiang", "Xiang", ""], ["Havaei", "Mohammad", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2003.04387", "submitter": "Lucas Rouhier", "authors": "Lucas Rouhier, Francisco Perdigon Romero, Joseph Paul Cohen, Julien\n  Cohen-Adad", "title": "Spine intervertebral disc labeling using a fully convolutional redundant\n  counting model", "comments": "MIDL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labeling intervertebral discs is relevant as it notably enables clinicians to\nunderstand the relationship between a patient's symptoms (pain, paralysis) and\nthe exact level of spinal cord injury. However manually labeling those discs is\na tedious and user-biased task which would benefit from automated methods.\nWhile some automated methods already exist for MRI and CT-scan, they are either\nnot publicly available, or fail to generalize across various imaging contrasts.\nIn this paper we combine a Fully Convolutional Network (FCN) with inception\nmodules to localize and label intervertebral discs. We demonstrate a\nproof-of-concept application in a publicly-available multi-center and\nmulti-contrast MRI database (n=235 subjects). The code is publicly available at\nhttps://github.com/neuropoly/vertebral-labeling-deep-learning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 20:02:31 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 14:13:59 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Rouhier", "Lucas", ""], ["Romero", "Francisco Perdigon", ""], ["Cohen", "Joseph Paul", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "2003.04390", "submitter": "Yinbo Chen", "authors": "Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, Trevor Darrell", "title": "A New Meta-Baseline for Few-Shot Learning", "comments": "Code is available on\n  https://github.com/cyvius96/few-shot-meta-baseline", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning has become a popular framework for few-shot learning in recent\nyears, with the goal of learning a model from collections of few-shot\nclassification tasks. While more and more novel meta-learning models are being\nproposed, our research has uncovered simple baselines that have been\noverlooked. We present a Meta-Baseline method, by pre-training a classifier on\nall base classes and meta-learning on a nearest-centroid based few-shot\nclassification algorithm, it outperforms recent state-of-the-art methods by a\nlarge margin. Why does this simple method work so well? In the meta-learning\nstage, we observe that a model generalizing better on unseen tasks from base\nclasses can have a decreasing performance on tasks from novel classes,\nindicating a potential objective discrepancy. We find both pre-training and\ninheriting a good few-shot classification metric from the pre-trained\nclassifier are important for Meta-Baseline, which potentially helps the model\nbetter utilize the pre-trained representations with stronger transferability.\nFurthermore, we investigate when we need meta-learning in this Meta-Baseline.\nOur work sets up a new solid benchmark for this field and sheds light on\nfurther understanding the phenomenons in the meta-learning framework for\nfew-shot learning.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 20:06:36 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 17:53:31 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 15:19:31 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Chen", "Yinbo", ""], ["Wang", "Xiaolong", ""], ["Liu", "Zhuang", ""], ["Xu", "Huijuan", ""], ["Darrell", "Trevor", ""]]}, {"id": "2003.04404", "submitter": "Ruochen Yin", "authors": "Ruochen Yin, Biao Yu, Huapeng Wu, Yutao Song, Runxin Niu", "title": "FusionLane: Multi-Sensor Fusion for Lane Marking Semantic Segmentation\n  Using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a crucial step to achieve effective semantic segmentation of lane\nmarking during the construction of the lane level high-precision map. In recent\nyears, many image semantic segmentation methods have been proposed. These\nmethods mainly focus on the image from camera, due to the limitation of the\nsensor itself, the accurate three-dimensional spatial position of the lane\nmarking cannot be obtained, so the demand for the lane level high-precision map\nconstruction cannot be met. This paper proposes a lane marking semantic\nsegmentation method based on LIDAR and camera fusion deep neural network.\nDifferent from other methods, in order to obtain accurate position information\nof the segmentation results, the semantic segmentation object of this paper is\na bird's eye view converted from a LIDAR points cloud instead of an image\ncaptured by a camera. This method first uses the deeplabv3+ [\\ref{ref:1}]\nnetwork to segment the image captured by the camera, and the segmentation\nresult is merged with the point clouds collected by the LIDAR as the input of\nthe proposed network. In this neural network, we also add a long short-term\nmemory (LSTM) structure to assist the network for semantic segmentation of lane\nmarkings by using the the time series information. The experiments on more than\n14,000 image datasets which we have manually labeled and expanded have shown\nthe proposed method has better performance on the semantic segmentation of the\npoints cloud bird's eye view. Therefore, the automation of high-precision map\nconstruction can be significantly improved. Our code is available at\nhttps://github.com/rolandying/FusionLane.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 20:33:30 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Yin", "Ruochen", ""], ["Yu", "Biao", ""], ["Wu", "Huapeng", ""], ["Song", "Yutao", ""], ["Niu", "Runxin", ""]]}, {"id": "2003.04414", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Yannick Berthoumieu", "title": "Texture Superpixel Clustering from Patch-based Nearest Neighbor Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels are widely used in computer vision applications. Nevertheless,\ndecomposition methods may still fail to efficiently cluster image pixels\naccording to their local texture. In this paper, we propose a new Nearest\nNeighbor-based Superpixel Clustering (NNSC) method to generate texture-aware\nsuperpixels in a limited computational time compared to previous approaches. We\nintroduce a new clustering framework using patch-based nearest neighbor\nmatching, while most existing methods are based on a pixel-wise K-means\nclustering. Therefore, we directly group pixels in the patch space enabling to\ncapture texture information. We demonstrate the efficiency of our method with\nfavorable comparison in terms of segmentation performances on both standard\ncolor and texture datasets. We also show the computational efficiency of NNSC\ncompared to recent texture-aware superpixel methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 21:11:21 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Berthoumieu", "Yannick", ""]]}, {"id": "2003.04428", "submitter": "Remi Giraud", "authors": "R\\'emi Giraud, Merlin Boyer, Micha\\\"el Cl\\'ement", "title": "Multi-Scale Superpatch Matching using Dual Superpixel Descriptors", "comments": null, "journal-ref": "Pattern Recognition Letters 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-segmentation into superpixels is a very effective dimensionality\nreduction strategy, enabling fast dense image processing. The main issue of\nthis approach is the inherent irregularity of the image decomposition compared\nto standard hierarchical multi-resolution schemes, especially when searching\nfor similar neighboring patterns. Several works have attempted to overcome this\nissue by taking into account the region irregularity into their comparison\nmodel. Nevertheless, they remain sub-optimal to provide robust and accurate\nsuperpixel neighborhood descriptors, since they only compute features within\neach region, poorly capturing contour information at superpixel borders. In\nthis work, we address these limitations by introducing the dual superpatch, a\nnovel superpixel neighborhood descriptor. This structure contains features\ncomputed in reduced superpixel regions, as well as at the interfaces of\nmultiple superpixels to explicitly capture contour structure information. A\nfast multi-scale non-local matching framework is also introduced for the search\nof similar descriptors at different resolution levels in an image dataset. The\nproposed dual superpatch enables to more accurately capture similar structured\npatterns at different scales, and we demonstrate the robustness and performance\nof this new strategy on matching and supervised labeling applications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:04:04 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Giraud", "R\u00e9mi", ""], ["Boyer", "Merlin", ""], ["Cl\u00e9ment", "Micha\u00ebl", ""]]}, {"id": "2003.04447", "submitter": "Shivam Gautam", "authors": "Shivam Gautam, Gregory P. Meyer, Carlos Vallespi-Gonzalez and Brian C.\n  Becker", "title": "SDVTracker: Real-Time Multi-Sensor Association and Tracking for\n  Self-Driving Vehicles", "comments": "8 pages, 7 figures, Submitted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate motion state estimation of Vulnerable Road Users (VRUs), is a\ncritical requirement for autonomous vehicles that navigate in urban\nenvironments. Due to their computational efficiency, many traditional autonomy\nsystems perform multi-object tracking using Kalman Filters which frequently\nrely on hand-engineered association. However, such methods fail to generalize\nto crowded scenes and multi-sensor modalities, often resulting in poor state\nestimates which cascade to inaccurate predictions. We present a practical and\nlightweight tracking system, SDVTracker, that uses a deep learned model for\nassociation and state estimation in conjunction with an Interacting Multiple\nModel (IMM) filter. The proposed tracking method is fast, robust and\ngeneralizes across multiple sensor modalities and different VRU classes. In\nthis paper, we detail a model that jointly optimizes both association and state\nestimation with a novel loss, an algorithm for determining ground-truth\nsupervision, and a training procedure. We show this system significantly\noutperforms hand-engineered methods on a real-world urban driving dataset while\nrunning in less than 2.5 ms on CPU for a scene with 100 actors, making it\nsuitable for self-driving applications where low latency and high accuracy is\ncritical.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 23:07:23 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gautam", "Shivam", ""], ["Meyer", "Gregory P.", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Becker", "Brian C.", ""]]}, {"id": "2003.04448", "submitter": "Qian Huang", "authors": "Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser-Nam Lim, Austin\n  Benson", "title": "Better Set Representations For Relational Reasoning", "comments": "Preprint, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating relational reasoning into neural networks has greatly expanded\ntheir capabilities and scope. One defining trait of relational reasoning is\nthat it operates on a set of entities, as opposed to standard vector\nrepresentations. Existing end-to-end approaches typically extract entities from\ninputs by directly interpreting the latent feature representations as a set. We\nshow that these approaches do not respect set permutational invariance and thus\nhave fundamental representational limitations. To resolve this limitation, we\npropose a simple and general network module called a Set Refiner Network (SRN).\nWe first use synthetic image experiments to demonstrate how our approach\neffectively decomposes objects without explicit supervision. Then, we insert\nour module into existing relational reasoning models and show that respecting\nset invariance leads to substantial gains in prediction performance and\nrobustness on several relational reasoning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 23:07:27 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 06:40:23 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Huang", "Qian", ""], ["He", "Horace", ""], ["Singh", "Abhay", ""], ["Zhang", "Yan", ""], ["Lim", "Ser-Nam", ""], ["Benson", "Austin", ""]]}, {"id": "2003.04454", "submitter": "Hyunjun Eun", "authors": "Hyunjun Eun, Daeyeong Kim, Chanho Jung, Changick Kim", "title": "Single-view 2D CNNs with Fully Automatic Non-nodule Categorization for\n  False Positive Reduction in Pulmonary Nodule Detection", "comments": "12 pages, 16 figures", "journal-ref": "Computer Methods and Programs in Biomedicine, vol. 165, Oct. 2018,\n  pp. 215-224", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Objective: In pulmonary nodule detection, the first stage,\ncandidate detection, aims to detect suspicious pulmonary nodules. However,\ndetected candidates include many false positives and thus in the following\nstage, false positive reduction, such false positives are reliably reduced.\nNote that this task is challenging due to 1) the imbalance between the numbers\nof nodules and non-nodules and 2) the intra-class diversity of non-nodules.\nAlthough techniques using 3D convolutional neural networks (CNNs) have shown\npromising performance, they suffer from high computational complexity which\nhinders constructing deep networks. To efficiently address these problems, we\npropose a novel framework using the ensemble of 2D CNNs using single views,\nwhich outperforms existing 3D CNN-based methods.\n  Methods: Our ensemble of 2D CNNs utilizes single-view 2D patches to improve\nboth computational and memory efficiency compared to previous techniques\nexploiting 3D CNNs. We first categorize non-nodules on the basis of features\nencoded by an autoencoder. Then, all 2D CNNs are trained by using the same\nnodule samples, but with different types of non-nodules. By extending the\nlearning capability, this training scheme resolves difficulties of extracting\nrepresentative features from non-nodules with large appearance variations. Note\nthat, instead of manual categorization requiring the heavy workload of\nradiologists, we propose to automatically categorize non-nodules based on the\nautoencoder and k-means clustering.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 23:18:52 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Eun", "Hyunjun", ""], ["Kim", "Daeyeong", ""], ["Jung", "Chanho", ""], ["Kim", "Changick", ""]]}, {"id": "2003.04468", "submitter": "Alexandre Pineault", "authors": "Alexandre Pineault, Guillaume-Alexandre Bilodeau, Gilles Pesant", "title": "Tracking Road Users using Constraint Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at improving the tracking of road users in urban\nscenes. We present a constraint programming (CP) approach for the data\nassociation phase found in the tracking-by-detection paradigm of the multiple\nobject tracking (MOT) problem. Such an approach can solve the data association\nproblem more efficiently than graph-based methods and can handle better the\ncombinatorial explosion occurring when multiple frames are analyzed. Because\nour focus is on the data association problem, our MOT method only uses simple\nimage features, which are the center position and color of detections for each\nframe. Constraints are defined on these two features and on the general MOT\nproblem. For example, we enforce color appearance preservation over\ntrajectories and constrain the extent of motion between frames. Filtering\nlayers are used in order to eliminate detection candidates before using CP and\nto remove dummy trajectories produced by the CP solver. Our proposed method was\ntested on a motorized vehicles tracking dataset and produces results that\noutperform the top methods of the UA-DETRAC benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:04:32 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Pineault", "Alexandre", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Pesant", "Gilles", ""]]}, {"id": "2003.04480", "submitter": "Essam Rashed", "authors": "Essam A. Rashed and M. Samir Abou El Seoud", "title": "Deep learning approach for breast cancer diagnosis", "comments": null, "journal-ref": "ICSIE '19: Proceedings of the 2019 8th International Conference on\n  Software and Information Engineering", "doi": "10.1145/3328833.3328867", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the leading fatal disease worldwide with high risk\ncontrol if early discovered. Conventional method for breast screening is x-ray\nmammography, which is known to be challenging for early detection of cancer\nlesions. The dense breast structure produced due to the compression process\nduring imaging lead to difficulties to recognize small size abnormalities.\nAlso, inter- and intra-variations of breast tissues lead to significant\ndifficulties to achieve high diagnosis accuracy using hand-crafted features.\nDeep learning is an emerging machine learning technology that requires a\nrelatively high computation power. Yet, it proved to be very effective in\nseveral difficult tasks that requires decision making at the level of human\nintelligence. In this paper, we develop a new network architecture inspired by\nthe U-net structure that can be used for effective and early detection of\nbreast cancer. Results indicate a high rate of sensitivity and specificity that\nindicate potential usefulness of the proposed approach in clinical use.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:47:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Rashed", "Essam A.", ""], ["Seoud", "M. Samir Abou El", ""]]}, {"id": "2003.04490", "submitter": "Adam Kortylewski", "authors": "Adam Kortylewski, Ju He, Qing Liu, Alan Yuille", "title": "Compositional Convolutional Neural Networks: A Deep Architecture with\n  Innate Robustness to Partial Occlusion", "comments": "CVPR 2020; Code is available\n  https://github.com/AdamKortylewski/CompositionalNets; Supplementary material:\n  https://adamkortylewski.com/data/compnet_supp.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent findings show that deep convolutional neural networks (DCNNs) do not\ngeneralize well under partial occlusion. Inspired by the success of\ncompositional models at classifying partially occluded objects, we propose to\nintegrate compositional models and DCNNs into a unified deep model with innate\nrobustness to partial occlusion. We term this architecture Compositional\nConvolutional Neural Network. In particular, we propose to replace the fully\nconnected classification head of a DCNN with a differentiable compositional\nmodel. The generative nature of the compositional model enables it to localize\noccluders and subsequently focus on the non-occluded parts of the object. We\nconduct classification experiments on artificially occluded images as well as\nreal images of partially occluded objects from the MS-COCO dataset. The results\nshow that DCNNs do not classify occluded objects robustly, even when trained\nwith data that is strongly augmented with partial occlusions. Our proposed\nmodel outperforms standard DCNNs by a large margin at classifying partially\noccluded objects, even when it has not been exposed to occluded objects during\ntraining. Additional experiments demonstrate that CompositionalNets can also\nlocalize the occluders accurately, despite being trained with class labels\nonly. The code used in this work is publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 01:45:38 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 09:30:33 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 07:23:05 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Kortylewski", "Adam", ""], ["He", "Ju", ""], ["Liu", "Qing", ""], ["Yuille", "Alan", ""]]}, {"id": "2003.04492", "submitter": "Hanchao Yu", "authors": "Hanchao Yu, Shanhui Sun, Haichao Yu, Xiao Chen, Honghui Shi, Thomas\n  Huang, Terrence Chen", "title": "FOAL: Fast Online Adaptive Learning for Cardiac Motion Estimation", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion estimation of cardiac MRI videos is crucial for the evaluation of\nhuman heart anatomy and function. Recent researches show promising results with\ndeep learning-based methods. In clinical deployment, however, they suffer\ndramatic performance drops due to mismatched distributions between training and\ntesting datasets, commonly encountered in the clinical environment. On the\nother hand, it is arguably impossible to collect all representative datasets\nand to train a universal tracker before deployment. In this context, we\nproposed a novel fast online adaptive learning (FOAL) framework: an online\ngradient descent based optimizer that is optimized by a meta-learner. The\nmeta-learner enables the online optimizer to perform a fast and robust\nadaptation. We evaluated our method through extensive experiments on two public\nclinical datasets. The results showed the superior performance of FOAL in\naccuracy compared to the offline-trained tracking method. On average, the FOAL\ntook only $0.4$ second per video for online optimization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 01:51:27 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 18:02:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yu", "Hanchao", ""], ["Sun", "Shanhui", ""], ["Yu", "Haichao", ""], ["Chen", "Xiao", ""], ["Shi", "Honghui", ""], ["Huang", "Thomas", ""], ["Chen", "Terrence", ""]]}, {"id": "2003.04514", "submitter": "Homanga Bharadhwaj", "authors": "Samarth Sinha, Homanga Bharadhwaj, Anirudh Goyal, Hugo Larochelle,\n  Animesh Garg, Florian Shkurti", "title": "Diversity inducing Information Bottleneck in Model Ensembles", "comments": "AAAI 2021. Samarth Sinha* and Homanga Bharadhwaj* contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have achieved state-of-the-art performance on a\nnumber of vision tasks, generalization over high dimensional multi-modal data,\nand reliable predictive uncertainty estimation are still active areas of\nresearch. Bayesian approaches including Bayesian Neural Nets (BNNs) do not\nscale well to modern computer vision tasks, as they are difficult to train, and\nhave poor generalization under dataset-shift. This motivates the need for\neffective ensembles which can generalize and give reliable uncertainty\nestimates. In this paper, we target the problem of generating effective\nensembles of neural networks by encouraging diversity in prediction. We\nexplicitly optimize a diversity inducing adversarial loss for learning the\nstochastic latent variables and thereby obtain diversity in the output\npredictions necessary for modeling multi-modal data. We evaluate our method on\nbenchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and\ncompared to the most competitive baselines show significant improvements in\nclassification accuracy, under a shift in the data distribution and in\nout-of-distribution detection. Code will be released in this url\nhttps://github.com/rvl-lab-utoronto/dibs\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 03:10:41 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 22:57:12 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2020 20:14:08 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Sinha", "Samarth", ""], ["Bharadhwaj", "Homanga", ""], ["Goyal", "Anirudh", ""], ["Larochelle", "Hugo", ""], ["Garg", "Animesh", ""], ["Shkurti", "Florian", ""]]}, {"id": "2003.04534", "submitter": "Palani Thanaraj Krishnan", "authors": "K. Palani Thanaraj, B. Parvathavarthini, U. John Tanik, V.\n  Rajinikanth, Seifedine Kadry, K. Kamalanand", "title": "Implementation of Deep Neural Networks to Classify EEG Signals using\n  Gramian Angular Summation Field for Epilepsy Diagnosis", "comments": "9 pages, 8 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the approach of imaging timeseries data such as EEG in\nthe diagnosis of epilepsy through Deep Neural Network (DNN). EEG signal is\ntransformed into an RGB image using Gramian Angular Summation Field (GASF).\nMany such EEG epochs are transformed into GASF images for the normal and focal\nEEG signals. Then, some of the widely used Deep Neural Networks for image\nclassification problems are used here to detect the focal GASF images. Three\npre-trained DNN such as the AlexNet, VGG16, and VGG19 are validated for\nepilepsy detection based on the transfer learning approach. Furthermore, the\ntextural features are extracted from GASF images, and prominent features are\nselected for a multilayer Artificial Neural Network (ANN) classifier. Lastly, a\nCustom Convolutional Neural Network (CNN) with three CNN layers, Batch\nNormalization, Max-pooling layer, and Dense layers, is proposed for epilepsy\ndiagnosis from GASF images. The results of this paper show that the Custom CNN\nmodel was able to discriminate against the focal and normal GASF images with an\naverage peak Precision of 0.885, Recall of 0.92, and F1-score of 0.90.\nMoreover, the Area Under the Curve (AUC) value of the Receiver Operating\nCharacteristic (ROC) curve is 0.92 for the Custom CNN model. This paper\nsuggests that Deep Learning methods widely used in image classification\nproblems can be an alternative approach for epilepsy detection from EEG signals\nthrough GASF images.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 17:30:14 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Thanaraj", "K. Palani", ""], ["Parvathavarthini", "B.", ""], ["Tanik", "U. John", ""], ["Rajinikanth", "V.", ""], ["Kadry", "Seifedine", ""], ["Kamalanand", "K.", ""]]}, {"id": "2003.04541", "submitter": "Li Xiao", "authors": "Li Xiao, Yufan Luo, Chunlong Luo, Lianhe Zhao, Quanshui Fu, Guoqing\n  Yang, Anpeng Huang, Yi Zhao", "title": "PBRnet: Pyramidal Bounding Box Refinement to Improve Object Localization\n  Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recently developed object detectors focused on coarse-to-fine framework\nwhich contains several stages that classify and regress proposals from\ncoarse-grain to fine-grain, and obtains more accurate detection gradually.\nMulti-resolution models such as Feature Pyramid Network(FPN) integrate\ninformation of different levels of resolution and effectively improve the\nperformance. Previous researches also have revealed that localization can be\nfurther improved by: 1) using fine-grained information which is more\ntranslational variant; 2) refining local areas which is more focused on local\nboundary information. Based on these principles, we designed a novel boundary\nrefinement architecture to improve localization accuracy by combining\ncoarse-to-fine framework with feature pyramid structure, named as Pyramidal\nBounding Box Refinement network(PBRnet), which parameterizes gradually focused\nboundary areas of objects and leverages lower-level feature maps to extract\nfiner local information when refining the predicted bounding boxes. Extensive\nexperiments are performed on the MS-COCO dataset. The PBRnet brings a\nsignificant performance gains by roughly 3 point of $mAP$ when added to FPN or\nLibra R-CNN. Moreover, by treating Cascade R-CNN as a coarse-to-fine detector\nand replacing its localization branch by the regressor of PBRnet, it leads an\nextra performance improvement by 1.5 $mAP$, yielding a total performance\nboosting by as high as 5 point of $mAP$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 05:27:09 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Xiao", "Li", ""], ["Luo", "Yufan", ""], ["Luo", "Chunlong", ""], ["Zhao", "Lianhe", ""], ["Fu", "Quanshui", ""], ["Yang", "Guoqing", ""], ["Huang", "Anpeng", ""], ["Zhao", "Yi", ""]]}, {"id": "2003.04547", "submitter": "Kaixuan Wei", "authors": "Kaixuan Wei, Ying Fu, Hua Huang", "title": "3D Quasi-Recurrent Neural Network for Hyperspectral Image Denoising", "comments": "Accepted by IEEE Transactions on Neural Network and Learning System\n  (TNNLS), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an alternating directional 3D quasi-recurrent\nneural network for hyperspectral image (HSI) denoising, which can effectively\nembed the domain knowledge -- structural spatio-spectral correlation and global\ncorrelation along spectrum. Specifically, 3D convolution is utilized to extract\nstructural spatio-spectral correlation in an HSI, while a quasi-recurrent\npooling function is employed to capture the global correlation along spectrum.\nMoreover, alternating directional structure is introduced to eliminate the\ncausal dependency with no additional computation cost. The proposed model is\ncapable of modeling spatio-spectral dependency while preserving the flexibility\ntowards HSIs with arbitrary number of bands. Extensive experiments on HSI\ndenoising demonstrate significant improvement over state-of-the-arts under\nvarious noise settings, in terms of both restoration accuracy and computation\ntime. Our code is available at https://github.com/Vandermode/QRNN3D.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 06:14:53 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wei", "Kaixuan", ""], ["Fu", "Ying", ""], ["Huang", "Hua", ""]]}, {"id": "2003.04566", "submitter": "Yun Ye", "authors": "Yun Ye, Ganmei You, Jong-Kae Fwu, Xia Zhu, Qing Yang and Yuan Zhu", "title": "Channel Pruning via Optimal Thresholding", "comments": "ICONIP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured pruning, especially channel pruning is widely used for the reduced\ncomputational cost and the compatibility with off-the-shelf hardware devices.\nAmong existing works, weights are typically removed using a predefined global\nthreshold, or a threshold computed from a predefined metric. The predefined\nglobal threshold based designs ignore the variation among different layers and\nweights distribution, therefore, they may often result in sub-optimal\nperformance caused by over-pruning or under-pruning. In this paper, we present\na simple yet effective method, termed Optimal Thresholding (OT), to prune\nchannels with layer dependent thresholds that optimally separate important from\nnegligible channels. By using OT, most negligible or unimportant channels are\npruned to achieve high sparsity while minimizing performance degradation. Since\nmost important weights are preserved, the pruned model can be further\nfine-tuned and quickly converge with very few iterations. Our method\ndemonstrates superior performance, especially when compared to the\nstate-of-the-art designs at high levels of sparsity. On CIFAR-100, a pruned and\nfine-tuned DenseNet-121 by using OT achieves 75.99% accuracy with only 1.46e8\nFLOPs and 0.71M parameters.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:24:24 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 01:55:52 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 05:23:13 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2020 14:31:58 GMT"}, {"version": "v5", "created": "Thu, 10 Sep 2020 05:42:37 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Ye", "Yun", ""], ["You", "Ganmei", ""], ["Fwu", "Jong-Kae", ""], ["Zhu", "Xia", ""], ["Yang", "Qing", ""], ["Zhu", "Yuan", ""]]}, {"id": "2003.04569", "submitter": "Chenjie Wang", "authors": "Chenjie Wang and Bin Luo and Yun Zhang and Qing Zhao and Lu Yin and\n  Wei Wang and Xin Su and Yajun Wang and Chengyuan Li", "title": "DymSLAM:4D Dynamic Scene Reconstruction Based on Geometrical Motion\n  Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most SLAM algorithms are based on the assumption that the scene is static.\nHowever, in practice, most scenes are dynamic which usually contains moving\nobjects, these methods are not suitable. In this paper, we introduce DymSLAM, a\ndynamic stereo visual SLAM system being capable of reconstructing a 4D (3D +\ntime) dynamic scene with rigid moving objects. The only input of DymSLAM is\nstereo video, and its output includes a dense map of the static environment, 3D\nmodel of the moving objects and the trajectories of the camera and the moving\nobjects. We at first detect and match the interesting points between successive\nframes by using traditional SLAM methods. Then the interesting points belonging\nto different motion models (including ego-motion and motion models of rigid\nmoving objects) are segmented by a multi-model fitting approach. Based on the\ninteresting points belonging to the ego-motion, we are able to estimate the\ntrajectory of the camera and reconstruct the static background. The interesting\npoints belonging to the motion models of rigid moving objects are then used to\nestimate their relative motion models to the camera and reconstruct the 3D\nmodels of the objects. We then transform the relative motion to the\ntrajectories of the moving objects in the global reference frame. Finally, we\nthen fuse the 3D models of the moving objects into the 3D map of the\nenvironment by considering their motion trajectories to obtain a 4D (3D+time)\nsequence. DymSLAM obtains information about the dynamic objects instead of\nignoring them and is suitable for unknown rigid objects. Hence, the proposed\nsystem allows the robot to be employed for high-level tasks, such as obstacle\navoidance for dynamic objects. We conducted experiments in a real-world\nenvironment where both the camera and the objects were moving in a wide range.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:25:21 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wang", "Chenjie", ""], ["Luo", "Bin", ""], ["Zhang", "Yun", ""], ["Zhao", "Qing", ""], ["Yin", "Lu", ""], ["Wang", "Wei", ""], ["Su", "Xin", ""], ["Wang", "Yajun", ""], ["Li", "Chengyuan", ""]]}, {"id": "2003.04583", "submitter": "Zhouyingcheng Liao", "authors": "Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll", "title": "TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape\n  and Garment Style", "comments": "Accepted to CVPR 2020. Chaitanya Patel and Zhouyingcheng Liao\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present TailorNet, a neural model which predicts clothing\ndeformation in 3D as a function of three factors: pose, shape and style\n(garment geometry), while retaining wrinkle detail. This goes beyond prior\nmodels, which are either specific to one style and shape, or generalize to\ndifferent shapes producing smooth results, despite being style specific. Our\nhypothesis is that (even non-linear) combinations of examples smooth out high\nfrequency components such as fine-wrinkles, which makes learning the three\nfactors jointly hard. At the heart of our technique is a decomposition of\ndeformation into a high frequency and a low frequency component. While the\nlow-frequency component is predicted from pose, shape and style parameters with\nan MLP, the high-frequency component is predicted with a mixture of shape-style\nspecific pose models. The weights of the mixture are computed with a narrow\nbandwidth kernel to guarantee that only predictions with similar high-frequency\npatterns are combined. The style variation is obtained by computing, in a\ncanonical pose, a subspace of deformation, which satisfies physical constraints\nsuch as inter-penetration, and draping on the body. TailorNet delivers 3D\ngarments which retain the wrinkles from the physics based simulations (PBS) it\nis learned from, while running more than 1000 times faster. In contrast to PBS,\nTailorNet is easy to use and fully differentiable, which is crucial for\ncomputer vision algorithms. Several experiments demonstrate TailorNet produces\nmore realistic results than prior work, and even generates temporally coherent\ndeformations on sequences of the AMASS dataset, despite being trained on static\nposes from a different dataset. To stimulate further research in this\ndirection, we will make a dataset consisting of 55800 frames, as well as our\nmodel publicly available at https://virtualhumans.mpi-inf.mpg.de/tailornet.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 08:49:51 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 16:35:56 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Patel", "Chaitanya", ""], ["Liao", "Zhouyingcheng", ""], ["Pons-Moll", "Gerard", ""]]}, {"id": "2003.04614", "submitter": "Jinyu Yang", "authors": "Jinyu Yang, Weizhi An, Sheng Wang, Xinliang Zhu, Chaochao Yan, Junzhou\n  Huang", "title": "Label-Driven Reconstruction for Domain Adaptation in Semantic\n  Segmentation", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation enables to alleviate the need for pixel-wise\nannotation in the semantic segmentation. One of the most common strategies is\nto translate images from the source domain to the target domain and then align\ntheir marginal distributions in the feature space using adversarial learning.\nHowever, source-to-target translation enlarges the bias in translated images\nand introduces extra computations, owing to the dominant data size of the\nsource domain. Furthermore, consistency of the joint distribution in source and\ntarget domains cannot be guaranteed through global feature alignment. Here, we\npresent an innovative framework, designed to mitigate the image translation\nbias and align cross-domain features with the same category. This is achieved\nby 1) performing the target-to-source translation and 2) reconstructing both\nsource and target images from their predicted labels. Extensive experiments on\nadapting from synthetic to real urban scene understanding demonstrate that our\nframework competes favorably against existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:06:35 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 21:49:02 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 16:23:23 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yang", "Jinyu", ""], ["An", "Weizhi", ""], ["Wang", "Sheng", ""], ["Zhu", "Xinliang", ""], ["Yan", "Chaochao", ""], ["Huang", "Junzhou", ""]]}, {"id": "2003.04618", "submitter": "Songyou Peng", "authors": "Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys,\n  Andreas Geiger", "title": "Convolutional Occupancy Networks", "comments": "ECCV 2020 (Spotlight). Project page with supplementary material and\n  code: https://pengsongyou.github.io/conv_onet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, implicit neural representations have gained popularity for\nlearning-based 3D reconstruction. While demonstrating promising results, most\nimplicit approaches are limited to comparably simple geometry of single objects\nand do not scale to more complicated or large-scale scenes. The key limiting\nfactor of implicit methods is their simple fully-connected network architecture\nwhich does not allow for integrating local information in the observations or\nincorporating inductive biases such as translational equivariance. In this\npaper, we propose Convolutional Occupancy Networks, a more flexible implicit\nrepresentation for detailed reconstruction of objects and 3D scenes. By\ncombining convolutional encoders with implicit occupancy decoders, our model\nincorporates inductive biases, enabling structured reasoning in 3D space. We\ninvestigate the effectiveness of the proposed representation by reconstructing\ncomplex geometry from noisy point clouds and low-resolution voxel\nrepresentations. We empirically find that our method enables the fine-grained\nimplicit 3D reconstruction of single objects, scales to large indoor scenes,\nand generalizes well from synthetic to real data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:17:07 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 20:38:29 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Peng", "Songyou", ""], ["Niemeyer", "Michael", ""], ["Mescheder", "Lars", ""], ["Pollefeys", "Marc", ""], ["Geiger", "Andreas", ""]]}, {"id": "2003.04619", "submitter": "Yong Guo", "authors": "Yong Guo, Yongsheng Luo, Zhenhao He, Jin Huang, Jian Chen", "title": "Hierarchical Neural Architecture Search for Single Image\n  Super-Resolution", "comments": "This paper is accepted by IEEE Signal Processing Letters", "journal-ref": null, "doi": "10.1109/LSP.2020.3003517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR). Most SR models follow a hierarchical architecture that\ncontains both the cell-level design of computational blocks and the\nnetwork-level design of the positions of upsampling blocks. However, designing\nSR models heavily relies on human expertise and is very labor-intensive. More\ncritically, these SR models often contain a huge number of parameters and may\nnot meet the requirements of computation resources in real-world applications.\nTo address the above issues, we propose a Hierarchical Neural Architecture\nSearch (HNAS) method to automatically design promising architectures with\ndifferent requirements of computation cost. To this end, we design a\nhierarchical SR search space and propose a hierarchical controller for\narchitecture search. Such a hierarchical controller is able to simultaneously\nfind promising cell-level blocks and network-level positions of upsampling\nlayers. Moreover, to design compact architectures with promising performance,\nwe build a joint reward by considering both the performance and computation\ncost to guide the search process. Extensive experiments on five benchmark\ndatasets demonstrate the superiority of our method over existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:19:44 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 02:19:54 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 12:43:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Guo", "Yong", ""], ["Luo", "Yongsheng", ""], ["He", "Zhenhao", ""], ["Huang", "Jin", ""], ["Chen", "Jian", ""]]}, {"id": "2003.04626", "submitter": "Roy Sheffer", "authors": "Roy Sheffer, Ami Wiesel", "title": "PnP-Net: A hybrid Perspective-n-Point Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust Perspective-n-Point (PnP) problem using a hybrid\napproach that combines deep learning with model based algorithms. PnP is the\nproblem of estimating the pose of a calibrated camera given a set of 3D points\nin the world and their corresponding 2D projections in the image. In its more\nchallenging robust version, some of the correspondences may be mismatched and\nmust be efficiently discarded. Classical solutions address PnP via iterative\nrobust non-linear least squares method that exploit the problem's geometry but\nare either inaccurate or computationally intensive. In contrast, we propose to\ncombine a deep learning initial phase followed by a model-based fine tuning\nphase. This hybrid approach, denoted by PnP-Net, succeeds in estimating the\nunknown pose parameters under correspondence errors and noise, with low and\nfixed computational complexity requirements. We demonstrate its advantages on\nboth synthetic data and real world data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:43:14 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Sheffer", "Roy", ""], ["Wiesel", "Ami", ""]]}, {"id": "2003.04641", "submitter": "Yuhong Deng", "authors": "Yuhong Deng, Di Guo, Xiaofeng Guo, Naifu Zhang, Huaping Liu, Fuchun\n  Sun", "title": "MQA: Answering the Question via Robotic Manipulation", "comments": "have be accepted by Robotics: Science and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel task, Manipulation Question Answering\n(MQA), where the robot performs manipulation actions to change the environment\nin order to answer a given question. To solve this problem, a framework\nconsisting of a QA module and a manipulation module is proposed. For the QA\nmodule, we adopt the method for the Visual Question Answering (VQA) task. For\nthe manipulation module, a Deep Q Network (DQN) model is designed to generate\nmanipulation actions for the robot to interact with the environment. We\nconsider the situation where the robot continuously manipulating objects inside\na bin until the answer to the question is found. Besides, a novel dataset that\ncontains a variety of object models, scenarios and corresponding\nquestion-answer pairs is established in a simulation environment. Extensive\nexperiments have been conducted to validate the effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:30:09 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 08:46:48 GMT"}, {"version": "v3", "created": "Sun, 27 Jun 2021 13:44:50 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Deng", "Yuhong", ""], ["Guo", "Di", ""], ["Guo", "Xiaofeng", ""], ["Zhang", "Naifu", ""], ["Liu", "Huaping", ""], ["Sun", "Fuchun", ""]]}, {"id": "2003.04645", "submitter": "Johan Vertens", "authors": "Johan Vertens, Jannik Z\\\"urn, Wolfram Burgard", "title": "HeatNet: Bridging the Day-Night Domain Gap in Semantic Segmentation with\n  Thermal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of learning-based semantic segmentation methods are optimized\nfor daytime scenarios and favorable lighting conditions. Real-world driving\nscenarios, however, entail adverse environmental conditions such as nighttime\nillumination or glare which remain a challenge for existing approaches. In this\nwork, we propose a multimodal semantic segmentation model that can be applied\nduring daytime and nighttime. To this end, besides RGB images, we leverage\nthermal images, making our network significantly more robust. We avoid the\nexpensive annotation of nighttime images by leveraging an existing daytime\nRGB-dataset and propose a teacher-student training approach that transfers the\ndataset's knowledge to the nighttime domain. We further employ a domain\nadaptation method to align the learned feature spaces across the domains and\npropose a novel two-stage training scheme. Furthermore, due to a lack of\nthermal data for autonomous driving, we present a new dataset comprising over\n20,000 time-synchronized and aligned RGB-thermal image pairs. In this context,\nwe also present a novel target-less calibration method that allows for\nautomatic robust extrinsic and intrinsic thermal camera calibration. Among\nothers, we employ our new dataset to show state-of-the-art results for\nnighttime semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:36:42 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Vertens", "Johan", ""], ["Z\u00fcrn", "Jannik", ""], ["Burgard", "Wolfram", ""]]}, {"id": "2003.04651", "submitter": "Michael Schelling", "authors": "Michael Schelling, Pedro Hermosilla, Pere-Pau Vazquez, Timo Ropinski", "title": "Enabling Viewpoint Learning through Dynamic Label Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal viewpoint prediction is an essential task in many computer graphics\napplications. Unfortunately, common viewpoint qualities suffer from two major\ndrawbacks: dependency on clean surface meshes, which are not always available,\nand the lack of closed-form expressions, which requires a costly search\ninvolving rendering. To overcome these limitations we propose to separate\nviewpoint selection from rendering through an end-to-end learning approach,\nwhereby we reduce the influence of the mesh quality by predicting viewpoints\nfrom unstructured point clouds instead of polygonal meshes. While this makes\nour approach insensitive to the mesh discretization during evaluation, it only\nbecomes possible when resolving label ambiguities that arise in this context.\nTherefore, we additionally propose to incorporate the label generation into the\ntraining procedure, making the label decision adaptive to the current network\npredictions. We show how our proposed approach allows for learning viewpoint\npredictions for models from different object categories and for different\nviewpoint qualities. Additionally, we show that prediction times are reduced\nfrom several minutes to a fraction of a second, as compared to state-of-the-art\n(SOTA) viewpoint quality evaluation. We will further release the code and\ntraining data, which will to our knowledge be the biggest viewpoint quality\ndataset available.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:49:27 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 14:35:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Schelling", "Michael", ""], ["Hermosilla", "Pedro", ""], ["Vazquez", "Pere-Pau", ""], ["Ropinski", "Timo", ""]]}, {"id": "2003.04655", "submitter": "Yaozong Gao", "authors": "Fei Shan, Yaozong Gao, Jun Wang, Weiya Shi, Nannan Shi, Miaofei Han,\n  Zhong Xue, Dinggang Shen, Yuxin Shi", "title": "Lung Infection Quantification of COVID-19 in CT Images with Deep\n  Learning", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": "10.1002/mp.14609", "report-no": null, "categories": "cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CT imaging is crucial for diagnosis, assessment and staging COVID-19\ninfection. Follow-up scans every 3-5 days are often recommended for disease\nprogression. It has been reported that bilateral and peripheral ground glass\nopacification (GGO) with or without consolidation are predominant CT findings\nin COVID-19 patients. However, due to lack of computerized quantification\ntools, only qualitative impression and rough description of infected areas are\ncurrently used in radiological reports. In this paper, a deep learning\n(DL)-based segmentation system is developed to automatically quantify infection\nregions of interest (ROIs) and their volumetric ratios w.r.t. the lung. The\nperformance of the system was evaluated by comparing the automatically\nsegmented infection regions with the manually-delineated ones on 300 chest CT\nscans of 300 COVID-19 patients. For fast manual delineation of training samples\nand possible manual intervention of automatic results, a human-in-the-loop\n(HITL) strategy has been adopted to assist radiologists for infection region\nsegmentation, which dramatically reduced the total segmentation time to 4\nminutes after 3 iterations of model updating. The average Dice simiarility\ncoefficient showed 91.6% agreement between automatic and manual infaction\nsegmentations, and the mean estimation error of percentage of infection (POI)\nwas 0.3% for the whole lung. Finally, possible applications, including but not\nlimited to analysis of follow-up CT scans and infection distributions in the\nlobes and segments correlated with clinical findings, were discussed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 11:58:40 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 04:18:00 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 08:30:39 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Shan", "Fei", ""], ["Gao", "Yaozong", ""], ["Wang", "Jun", ""], ["Shi", "Weiya", ""], ["Shi", "Nannan", ""], ["Han", "Miaofei", ""], ["Xue", "Zhong", ""], ["Shen", "Dinggang", ""], ["Shi", "Yuxin", ""]]}, {"id": "2003.04668", "submitter": "Juan-Manuel Perez-Rua", "authors": "Juan-Manuel Perez-Rua and Xiatian Zhu and Timothy Hospedales and Tao\n  Xiang", "title": "Incremental Few-Shot Object Detection", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most existing object detection methods rely on the availability of abundant\nlabelled training samples per class and offline model training in a batch mode.\nThese requirements substantially limit their scalability to open-ended\naccommodation of novel classes with limited labelled training data. We present\na study aiming to go beyond these limitations by considering the Incremental\nFew-Shot Detection (iFSD) problem setting, where new classes must be registered\nincrementally (without revisiting base classes) and with few examples. To this\nend we propose OpeN-ended Centre nEt (ONCE), a detector designed for\nincrementally learning to detect novel class objects with few examples. This is\nachieved by an elegant adaptation of the CentreNet detector to the few-shot\nlearning scenario, and meta-learning a class-specific code generator model for\nregistering novel classes. ONCE fully respects the incremental learning\nparadigm, with novel class registration requiring only a single forward pass of\nfew-shot training samples, and no access to base classes -- thus making it\nsuitable for deployment on embedded devices. Extensive experiments conducted on\nboth the standard object detection and fashion landmark detection tasks show\nthe feasibility of iFSD for the first time, opening an interesting and very\nimportant line of research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:56:59 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 20:58:07 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Perez-Rua", "Juan-Manuel", ""], ["Zhu", "Xiatian", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""]]}, {"id": "2003.04671", "submitter": "Xi Li", "authors": "Xi Li, Huimin Ma, Sheng Yi, Yanxian Chen", "title": "Realizing Pixel-Level Semantic Learning in Complex Driving Scenes based\n  on Only One Annotated Pixel per Class", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation tasks based on weakly supervised condition have been\nput forward to achieve a lightweight labeling process. For simple images that\nonly include a few categories, researches based on image-level annotations have\nachieved acceptable performance. However, when facing complex scenes, since\nimage contains a large amount of classes, it becomes difficult to learn visual\nappearance based on image tags. In this case, image-level annotations are not\neffective in providing information. Therefore, we set up a new task in which\nonly one annotated pixel is provided for each category. Based on the more\nlightweight and informative condition, a three step process is built for pseudo\nlabels generation, which progressively implement optimal feature representation\nfor each category, image inference and context-location based refinement. In\nparticular, since high-level semantics and low-level imaging feature have\ndifferent discriminative ability for each class under driving scenes, we divide\neach category into \"object\" or \"scene\" and then provide different operations\nfor the two types separately. Further, an alternate iterative structure is\nestablished to gradually improve segmentation performance, which combines\nCNN-based inter-image common semantic learning and imaging prior based\nintra-image modification process. Experiments on Cityscapes dataset demonstrate\nthat the proposed method provides a feasible way to solve weakly supervised\nsemantic segmentation task under complex driving scenes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:57:55 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Li", "Xi", ""], ["Ma", "Huimin", ""], ["Yi", "Sheng", ""], ["Chen", "Yanxian", ""]]}, {"id": "2003.04676", "submitter": "Kai Zhao", "authors": "Kai Zhao, Qi Han, Chang-Bin Zhang, Jun Xu, and Ming-Ming Cheng", "title": "Deep Hough Transform for Semantic Line Detection", "comments": "https://github.com/Hanqer/deep-hough-transform", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  2021", "doi": "10.1109/TPAMI.2021.3077129", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on a fundamental task of detecting meaningful line structures,\na.k.a. semantic line, in natural scenes. Many previous methods regard this\nproblem as a special case of object detection and adjust existing object\ndetectors for semantic line detection. However, these methods neglect the\ninherent characteristics of lines, leading to sub-optimal performance. Lines\nenjoy much simpler geometric property than complex objects and thus can be\ncompactly parameterized by a few arguments. To better exploit the property of\nlines, in this paper, we incorporate the classical Hough transform technique\ninto deeply learned representations and propose a one-shot end-to-end learning\nframework for line detection. By parameterizing lines with slopes and biases,\nwe perform Hough transform to translate deep representations into the\nparametric domain, in which we perform line detection. Specifically, we\naggregate features along candidate lines on the feature map plane and then\nassign the aggregated features to corresponding locations in the parametric\ndomain. Consequently, the problem of detecting semantic lines in the spatial\ndomain is transformed into spotting individual points in the parametric domain,\nmaking the post-processing steps, i.e. non-maximal suppression, more efficient.\nFurthermore, our method makes it easy to extract contextual line features eg\nfeatures along lines close to a specific line, that are critical for accurate\nline detection. In addition to the proposed method, we design an evaluation\nmetric to assess the quality of line detection and construct a large scale\ndataset for the line detection task. Experimental results on our proposed\ndataset and another public dataset demonstrate the advantages of our method\nover previous state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:08:42 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 04:29:25 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 07:34:22 GMT"}, {"version": "v4", "created": "Sat, 1 May 2021 17:46:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhao", "Kai", ""], ["Han", "Qi", ""], ["Zhang", "Chang-Bin", ""], ["Xu", "Jun", ""], ["Cheng", "Ming-Ming", ""]]}, {"id": "2003.04679", "submitter": "Shen Gao", "authors": "Shen Gao, Xiuying Chen, Chang Liu, Li Liu, Dongyan Zhao and Rui Yan", "title": "Learning to Respond with Stickers: A Framework of Unifying\n  Multi-Modality in Multi-Turn Dialog", "comments": "Accepted by The Web Conference 2020 (WWW 2020). Equal contribution\n  from first two authors. Dataset and code are released at\n  https://github.com/gsh199449/stickerchat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stickers with vivid and engaging expressions are becoming increasingly\npopular in online messaging apps, and some works are dedicated to automatically\nselect sticker response by matching text labels of stickers with previous\nutterances. However, due to their large quantities, it is impractical to\nrequire text labels for the all stickers. Hence, in this paper, we propose to\nrecommend an appropriate sticker to user based on multi-turn dialog context\nhistory without any external labels. Two main challenges are confronted in this\ntask. One is to learn semantic meaning of stickers without corresponding text\nlabels. Another challenge is to jointly model the candidate sticker with the\nmulti-turn dialog context. To tackle these challenges, we propose a sticker\nresponse selector (SRS) model. Specifically, SRS first employs a convolutional\nbased sticker image encoder and a self-attention based multi-turn dialog\nencoder to obtain the representation of stickers and utterances. Next, deep\ninteraction network is proposed to conduct deep matching between the sticker\nwith each utterance in the dialog history. SRS then learns the short-term and\nlong-term dependency between all interaction results by a fusion network to\noutput the the final matching score. To evaluate our proposed method, we\ncollect a large-scale real-world dialog dataset with stickers from one of the\nmost popular online chatting platform. Extensive experiments conducted on this\ndataset show that our model achieves the state-of-the-art performance for all\ncommonly-used metrics. Experiments also verify the effectiveness of each\ncomponent of SRS. To facilitate further research in sticker selection field, we\nrelease this dataset of 340K multi-turn dialog and sticker pairs.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:10:26 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gao", "Shen", ""], ["Chen", "Xiuying", ""], ["Liu", "Chang", ""], ["Liu", "Li", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "2003.04696", "submitter": "Fernando P\\'erez-Garc\\'ia", "authors": "Fernando P\\'erez-Garc\\'ia, Rachel Sparks and S\\'ebastien Ourselin", "title": "TorchIO: a Python library for efficient loading, preprocessing,\n  augmentation and patch-based sampling of medical images in deep learning", "comments": "Submitted to Computer Methods and Programs in Biomedicine. 27 pages,\n  7 figures. Documentation for TorchIO can be found at http://torchio.rtfd.io/", "journal-ref": "Computer Methods and Programs in Biomedicine (June 2021), p.\n  106236. ISSN: 0169-2607", "doi": "10.1016/j.cmpb.2021.106236", "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing of medical images such as MRI or CT presents unique challenges\ncompared to RGB images typically used in computer vision. These include a lack\nof labels for large datasets, high computational costs, and metadata to\ndescribe the physical properties of voxels. Data augmentation is used to\nartificially increase the size of the training datasets. Training with image\npatches decreases the need for computational power. Spatial metadata needs to\nbe carefully taken into account in order to ensure a correct alignment of\nvolumes.\n  We present TorchIO, an open-source Python library to enable efficient\nloading, preprocessing, augmentation and patch-based sampling of medical images\nfor deep learning. TorchIO follows the style of PyTorch and integrates standard\nmedical image processing libraries to efficiently process images during\ntraining of neural networks. TorchIO transforms can be composed, reproduced,\ntraced and extended. We provide multiple generic preprocessing and augmentation\noperations as well as simulation of MRI-specific artifacts.\n  Source code, comprehensive tutorials and extensive documentation for TorchIO\ncan be found at https://github.com/fepegar/torchio. The package can be\ninstalled from the Python Package Index running 'pip install torchio'. It\nincludes a command-line interface which allows users to apply transforms to\nimage files without using Python. Additionally, we provide a graphical\ninterface within a TorchIO extension in 3D Slicer to visualize the effects of\ntransforms.\n  TorchIO was developed to help researchers standardize medical image\nprocessing pipelines and allow them to focus on the deep learning experiments.\nIt encourages open science, as it supports reproducibility and is version\ncontrolled so that the software can be cited precisely. Due to its modularity,\nthe library is compatible with other frameworks for deep learning with medical\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:36:16 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 20:43:32 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 09:09:03 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 10:05:29 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["P\u00e9rez-Garc\u00eda", "Fernando", ""], ["Sparks", "Rachel", ""], ["Ourselin", "S\u00e9bastien", ""]]}, {"id": "2003.04716", "submitter": "Jinshan Pan", "authors": "Jinshan Pan, Songsheng Cheng, Jiawei Zhang, Jinhui Tang", "title": "Deep Blind Video Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video super-resolution (SR) algorithms usually assume that the blur\nkernels in the degradation process are known and do not model the blur kernels\nin the restoration. However, this assumption does not hold for video SR and\nusually leads to over-smoothed super-resolved images. In this paper, we propose\na deep convolutional neural network (CNN) model to solve video SR by a blur\nkernel modeling approach. The proposed deep CNN model consists of motion blur\nestimation, motion estimation, and latent image restoration modules. The motion\nblur estimation module is used to provide reliable blur kernels. With the\nestimated blur kernel, we develop an image deconvolution method based on the\nimage formation model of video SR to generate intermediate latent images so\nthat some sharp image contents can be restored well. However, the generated\nintermediate latent images may contain artifacts. To generate high-quality\nimages, we use the motion estimation module to explore the information from\nadjacent frames, where the motion estimation can constrain the deep CNN model\nfor better image restoration. We show that the proposed algorithm is able to\ngenerate clearer images with finer structural details. Extensive experimental\nresults show that the proposed algorithm performs favorably against\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:43:24 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Pan", "Jinshan", ""], ["Cheng", "Songsheng", ""], ["Zhang", "Jiawei", ""], ["Tang", "Jinhui", ""]]}, {"id": "2003.04719", "submitter": "Junhui Yin", "authors": "Junhui Yin, Siqing Zhang, Dongliang Chang, Zhanyu Ma, Jun Guo", "title": "Dual-attention Guided Dropblock Module for Weakly Supervised Object\n  Localization", "comments": "Accepted by the 25th International Conference on Pattern Recognition\n  (ICPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms is frequently used to learn the discriminative features\nfor better feature representations. In this paper, we extend the attention\nmechanism to the task of weakly supervised object localization (WSOL) and\npropose the dual-attention guided dropblock module (DGDM), which aims at\nlearning the informative and complementary visual patterns for WSOL. This\nmodule contains two key components, the channel attention guided dropout (CAGD)\nand the spatial attention guided dropblock (SAGD). To model channel\ninterdependencies, the CAGD ranks the channel attentions and treats the top-k\nattentions with the largest magnitudes as the important ones. It also keeps\nsome low-valued elements to increase their value if they become important\nduring training. The SAGD can efficiently remove the most discriminative\ninformation by erasing the contiguous regions of feature maps rather than\nindividual pixels. This guides the model to capture the less discriminative\nparts for classification. Furthermore, it can also distinguish the foreground\nobjects from the background regions to alleviate the attention misdirection.\nExperimental results demonstrate that the proposed method achieves new\nstate-of-the-art localization performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 05:07:50 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 13:46:53 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 10:32:06 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yin", "Junhui", ""], ["Zhang", "Siqing", ""], ["Chang", "Dongliang", ""], ["Ma", "Zhanyu", ""], ["Guo", "Jun", ""]]}, {"id": "2003.04721", "submitter": "Seunghwan Lee", "authors": "Seunghwan Lee, Dongkyu Lee, Donghyeon Cho, Jiwon Kim, Tae Hyun Kim", "title": "Restore from Restored: Single Image Denoising with Pseudo Clean Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a simple and effective fine-tuning algorithm called\n\"restore-from-restored\", which can greatly enhance the performance of fully\npre-trained image denoising networks. Many supervised denoising approaches can\nproduce satisfactory results using large external training datasets. However,\nthese methods have limitations in using internal information available in a\ngiven test image. By contrast, recent self-supervised approaches can remove\nnoise in the input image by utilizing information from the specific test input.\nHowever, such methods show relatively lower performance on known noise types\nsuch as Gaussian noise compared to supervised methods. Thus, to combine\nexternal and internal information, we fine-tune the fully pre-trained denoiser\nusing pseudo training set at test time. By exploiting internal self-similar\npatches (i.e., patch-recurrence), the baseline network can be adapted to the\ngiven specific input image. We demonstrate that our method can be easily\nemployed on top of the state-of-the-art denoising networks and further improve\nthe performance on numerous denoising benchmark datasets including real noisy\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 17:35:31 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 09:18:38 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 06:35:28 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Lee", "Seunghwan", ""], ["Lee", "Dongkyu", ""], ["Cho", "Donghyeon", ""], ["Kim", "Jiwon", ""], ["Kim", "Tae Hyun", ""]]}, {"id": "2003.04742", "submitter": "Horia Porav", "authors": "Horia Porav, Valentina-Nicoleta Musat, Tom Bruls, Paul Newman", "title": "Rainy screens: Collecting rainy datasets, indoors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of data with adverse conditions in robotics is a cumbersome task\ndue to the difficulty in guaranteeing proper ground truth and synchronising\nwith desired weather conditions. In this paper, we present a simple method -\nrecording a high resolution screen - for generating diverse rainy images from\nexisting clear ground-truth images that is domain- and source-agnostic, simple\nand scales up. This setup allows us to leverage the diversity of existing\ndatasets with auxiliary task ground-truth data, such as semantic segmentation,\nobject positions etc. We generate rainy images with real adherent droplets and\nrain streaks based on Cityscapes and BDD, and train a de-raining model. We\npresent quantitative results for image reconstruction and semantic\nsegmentation, and qualitative results for an out-of-sample domain, showing that\nmodels trained with our data generalize well.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:57:37 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Porav", "Horia", ""], ["Musat", "Valentina-Nicoleta", ""], ["Bruls", "Tom", ""], ["Newman", "Paul", ""]]}, {"id": "2003.04745", "submitter": "Abir Belaala", "authors": "Abir Belaala (LINFI Laboratory, Biskra University), Labib Sadek\n  (Terrissa LINFI Laboratory, Biskra University), Noureddine Zerhouni (FEMTO-ST\n  Institute, CNRS - UFC / ENSMM / UTBM, Automatic Control and Micro-Mechatronic\n  Systems), Christine Devalland (Service of Anatomy and Pathology Cytology)", "title": "Spitzoid Lesions Diagnosis based on GA feature selection and Random\n  Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spitzoid lesions broadly categorized into Spitz Nevus (SN), Atypical Spitz\nTumors (AST), and Spitz Melanomas (SM). The accurate diagnosis of these lesions\nis one of the most challenges for dermapathologists; this is due to the high\nsimilarities between them. Data mining techniques are successfully applied to\nsituations like these where complexity exists. This study aims to develop an\nartificial intelligence model to support the diagnosis of Spitzoid lesions. A\nprivate spitzoid lesions dataset have been used to evaluate the system proposed\nin this study. The proposed system has three stages. In the first stage, SMOTE\nmethod applied to solve the imbalance data problem, in the second stage, in\norder to eliminate irrelevant features; genetic algorithm is used to select\nsignificant features. This later reduces the computational complexity and speed\nup the data mining process. In the third stage, Random forest classifier is\nemployed to make a decision for two different categories of lesions (Spitz\nnevus or Atypical Spitz Tumors). The performance of our proposed scheme is\nevaluated using accuracy, sensitivity, specificity, G-mean, F- measure, ROC and\nAUC. Results obtained with our SMOTE-GA-RF model with GA-based 16 features show\na great performance with accuracy 0.97, F-measure 0.98, AUC 0.98, and G-mean\n0.97.Results obtained in this study have potential to open new opportunities in\ndiagnosis of spitzoid lesions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:03:28 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 13:23:16 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Belaala", "Abir", "", "LINFI Laboratory, Biskra University"], ["Sadek", "Labib", "", "Terrissa LINFI Laboratory, Biskra University"], ["Zerhouni", "Noureddine", "", "FEMTO-ST\n  Institute, CNRS - UFC / ENSMM / UTBM, Automatic Control and Micro-Mechatronic\n  Systems"], ["Devalland", "Christine", "", "Service of Anatomy and Pathology Cytology"]]}, {"id": "2003.04769", "submitter": "Mobarakol Islam", "authors": "Mobarakol Islam, Vibashan VS, Hongliang Ren", "title": "AP-MTL: Attention Pruned Multi-task Learning Model for Real-time\n  Instrument Detection and Segmentation in Robot-assisted Surgery", "comments": "Accepted in the conference of ICRA 2020", "journal-ref": null, "doi": "10.1109/ICRA40945.2020.9196905", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical scene understanding and multi-tasking learning are crucial for\nimage-guided robotic surgery. Training a real-time robotic system for the\ndetection and segmentation of high-resolution images provides a challenging\nproblem with the limited computational resource. The perception drawn can be\napplied in effective real-time feedback, surgical skill assessment, and\nhuman-robot collaborative surgeries to enhance surgical outcomes. For this\npurpose, we develop a novel end-to-end trainable real-time Multi-Task Learning\n(MTL) model with weight-shared encoder and task-aware detection and\nsegmentation decoders. Optimization of multiple tasks at the same convergence\npoint is vital and presents a complex problem. Thus, we propose an asynchronous\ntask-aware optimization (ATO) technique to calculate task-oriented gradients\nand train the decoders independently. Moreover, MTL models are always\ncomputationally expensive, which hinder real-time applications. To address this\nchallenge, we introduce a global attention dynamic pruning (GADP) by removing\nless significant and sparse parameters. We further design a skip squeeze and\nexcitation (SE) module, which suppresses weak features, excites significant\nfeatures and performs dynamic spatial and channel-wise feature re-calibration.\nValidating on the robotic instrument segmentation dataset of MICCAI endoscopic\nvision challenge, our model significantly outperforms state-of-the-art\nsegmentation and detection models, including best-performed models in the\nchallenge.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:24:51 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 12:30:42 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Islam", "Mobarakol", ""], ["VS", "Vibashan", ""], ["Ren", "Hongliang", ""]]}, {"id": "2003.04772", "submitter": "Beatrice van Amsterdam", "authors": "Beatrice van Amsterdam, Matthew J. Clarkson, Danail Stoyanov", "title": "Multi-Task Recurrent Neural Network for Surgical Gesture Recognition and\n  Progress Prediction", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical gesture recognition is important for surgical data science and\ncomputer-aided intervention. Even with robotic kinematic information,\nautomatically segmenting surgical steps presents numerous challenges because\nsurgical demonstrations are characterized by high variability in style,\nduration and order of actions. In order to extract discriminative features from\nthe kinematic signals and boost recognition accuracy, we propose a multi-task\nrecurrent neural network for simultaneous recognition of surgical gestures and\nestimation of a novel formulation of surgical task progress. To show the\neffectiveness of the presented approach, we evaluate its application on the\nJIGSAWS dataset, that is currently the only publicly available dataset for\nsurgical gesture recognition featuring robot kinematic data. We demonstrate\nthat recognition performance improves in multi-task frameworks with progress\nestimation without any additional manual labelling and training.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:28:02 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["van Amsterdam", "Beatrice", ""], ["Clarkson", "Matthew J.", ""], ["Stoyanov", "Danail", ""]]}, {"id": "2003.04780", "submitter": "Biao Gao", "authors": "Biao Gao, Anran Xu, Yancheng Pan, Xijun Zhao, Wen Yao, Huijing Zhao", "title": "Off-Road Drivable Area Extraction Using 3D LiDAR Data", "comments": "Accepted by IEEE Intelligent Vehicles Symposium (IV2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for off-road drivable area extraction using 3D LiDAR data\nwith the goal of autonomous driving application. A specific deep learning\nframework is designed to deal with the ambiguous area, which is one of the main\nchallenges in the off-road environment. To reduce the considerable demand for\nhuman-annotated data for network training, we utilize the information from vast\nquantities of vehicle paths and auto-generated obstacle labels. Using these\nautogenerated annotations, the proposed network can be trained using weakly\nsupervised or semi-supervised methods, which can achieve better performance\nwith fewer human annotations. The experiments on our dataset illustrate the\nreasonability of our framework and the validity of our weakly and\nsemi-supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:44:45 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Gao", "Biao", ""], ["Xu", "Anran", ""], ["Pan", "Yancheng", ""], ["Zhao", "Xijun", ""], ["Yao", "Wen", ""], ["Zhao", "Huijing", ""]]}, {"id": "2003.04784", "submitter": "Cenek Albl", "authors": "Jingtong Li, Jesse Murray, Dorina Ismaili, Konrad Schindler, Cenek\n  Albl", "title": "Reconstruction of 3D flight trajectories from ad-hoc camera networks", "comments": "IROS 2020 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to reconstruct the 3D trajectory of an airborne robotic\nsystem only from videos recorded with cameras that are unsynchronized, may\nfeature rolling shutter distortion, and whose viewpoints are unknown. Our\napproach enables robust and accurate outside-in tracking of dynamically flying\ntargets, with cheap and easy-to-deploy equipment. We show that, in spite of the\nweakly constrained setting, recent developments in computer vision make it\npossible to reconstruct trajectories in 3D from unsynchronized, uncalibrated\nnetworks of consumer cameras, and validate the proposed method in a realistic\nfield experiment. We make our code available along with the data, including\ncm-accurate ground-truth from differential GNSS navigation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:57:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:40:39 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Li", "Jingtong", ""], ["Murray", "Jesse", ""], ["Ismaili", "Dorina", ""], ["Schindler", "Konrad", ""], ["Albl", "Cenek", ""]]}, {"id": "2003.04797", "submitter": "Rui Tang", "authors": "Rui Tang, Wenlong Song, Xiaoping Guan, Huibin Ge, and Deke Kong", "title": "Dam Burst: A region-merging-based image segmentation method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Until now, all single level segmentation algorithms except CNN-based ones\nlead to over segmentation. And CNN-based segmentation algorithms have their own\nproblems. To avoid over segmentation, multiple thresholds of criteria are\nadopted in region merging process to produce hierarchical segmentation results.\nHowever, there still has extreme over segmentation in the low level of the\nhierarchy, and outstanding tiny objects are merged to their large adjacencies\nin the high level of the hierarchy. This paper proposes a region-merging-based\nimage segmentation method that we call it Dam Burst. As a single level\nsegmentation algorithm, this method avoids over segmentation and retains\ndetails by the same time. It is named because of that it simulates a flooding\nfrom underground destroys dams between water-pools. We treat edge detection\nresults as strengthening structure of a dam if it is on the dam. To simulate a\nflooding from underground, regions are merged by ascending order of the average\ngra-dient inside the region.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 04:07:48 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Tang", "Rui", ""], ["Song", "Wenlong", ""], ["Guan", "Xiaoping", ""], ["Ge", "Huibin", ""], ["Kong", "Deke", ""]]}, {"id": "2003.04811", "submitter": "Junchao Zhang", "authors": "Junchao Zhang", "title": "Weighted Encoding Based Image Interpolation With Nonlocal Linear\n  Regression Model", "comments": null, "journal-ref": null, "doi": "10.1364/AO.397652", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image interpolation is a special case of image super-resolution, where the\nlow-resolution image is directly down-sampled from its high-resolution\ncounterpart without blurring and noise. Therefore, assumptions adopted in\nsuper-resolution models are not valid for image interpolation. To address this\nproblem, we propose a novel image interpolation model based on sparse\nrepresentation. Two widely used priors including sparsity and nonlocal\nself-similarity are used as the regularization terms to enhance the stability\nof interpolation model. Meanwhile, we incorporate the nonlocal linear\nregression into this model since nonlocal similar patches could provide a\nbetter approximation to a given patch. Moreover, we propose a new approach to\nlearn adaptive sub-dictionary online instead of clustering. For each patch,\nsimilar patches are grouped to learn adaptive sub-dictionary, generating a more\nsparse and accurate representation. Finally, the weighted encoding is\nintroduced to suppress tailing of fitting residuals in data fidelity. Abundant\nexperimental results demonstrate that our proposed method outperforms several\nstate-of-the-art methods in terms of quantitative measures and visual quality.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 03:20:21 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhang", "Junchao", ""]]}, {"id": "2003.04820", "submitter": "Amanda Fernandez", "authors": "Richard Tran, David Patrick, Michael Geyer, Amanda Fernandez", "title": "SAD: Saliency-based Defenses Against Adversarial Examples", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise in popularity of machine and deep learning models, there is an\nincreased focus on their vulnerability to malicious inputs. These adversarial\nexamples drift model predictions away from the original intent of the network\nand are a growing concern in practical security. In order to combat these\nattacks, neural networks can leverage traditional image processing approaches\nor state-of-the-art defensive models to reduce perturbations in the data.\nDefensive approaches that take a global approach to noise reduction are\neffective against adversarial attacks, however their lossy approach often\ndistorts important data within the image. In this work, we propose a visual\nsaliency based approach to cleaning data affected by an adversarial attack. Our\nmodel leverages the salient regions of an adversarial image in order to provide\na targeted countermeasure while comparatively reducing loss within the cleaned\nimages. We measure the accuracy of our model by evaluating the effectiveness of\nstate-of-the-art saliency methods prior to attack, under attack, and after\napplication of cleaning methods. We demonstrate the effectiveness of our\nproposed approach in comparison with related defenses and against established\nadversarial attack methods, across two saliency datasets. Our targeted approach\nshows significant improvements in a range of standard statistical and distance\nsaliency metrics, in comparison with both traditional and state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 15:55:23 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Tran", "Richard", ""], ["Patrick", "David", ""], ["Geyer", "Michael", ""], ["Fernandez", "Amanda", ""]]}, {"id": "2003.04845", "submitter": "Wenguan Wang", "authors": "Wenguan Wang, Hailong Zhu, Jifeng Dai, Yanwei Pang, Jianbing Shen, and\n  Ling Shao", "title": "Hierarchical Human Parsing with Typed Part-Relation Reasoning", "comments": "Accepted to CVPR 2020.\n  Code:https://github.com/hlzhu09/Hierarchical-Human-Parsing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human parsing is for pixel-wise human semantic understanding. As human bodies\nare underlying hierarchically structured, how to model human structures is the\ncentral theme in this task. Focusing on this, we seek to simultaneously exploit\nthe representational capacity of deep graph networks and the hierarchical human\nstructures. In particular, we provide following two contributions. First, three\nkinds of part relations, i.e., decomposition, composition, and dependency, are,\nfor the first time, completely and precisely described by three distinct\nrelation networks. This is in stark contrast to previous parsers, which only\nfocus on a portion of the relations and adopt a type-agnostic relation modeling\nstrategy. More expressive relation information can be captured by explicitly\nimposing the parameters in the relation networks to satisfy the specific\ncharacteristics of different relations. Second, previous parsers largely ignore\nthe need for an approximation algorithm over the loopy human hierarchy, while\nwe instead address an iterative reasoning process, by assimilating generic\nmessage-passing networks with their edge-typed, convolutional counterparts.\nWith these efforts, our parser lays the foundation for more sophisticated and\nflexible human relation patterns of reasoning. Comprehensive experiments on\nfive datasets demonstrate that our parser sets a new state-of-the-art on each.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:45:41 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 10:14:43 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Wang", "Wenguan", ""], ["Zhu", "Hailong", ""], ["Dai", "Jifeng", ""], ["Pang", "Yanwei", ""], ["Shen", "Jianbing", ""], ["Shao", "Ling", ""]]}, {"id": "2003.04852", "submitter": "Xueyang Wang", "authors": "Xueyang Wang, Xiya Zhang, Yinheng Zhu, Yuchen Guo, Xiaoyun Yuan, Liuyu\n  Xiang, Zerun Wang, Guiguang Ding, David J Brady, Qionghai Dai, Lu Fang", "title": "PANDA: A Gigapixel-level Human-centric Video Dataset", "comments": "Accepted by IEEE International Conference on Computer Vision and\n  Pattern Recognition (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PANDA, the first gigaPixel-level humAN-centric viDeo dAtaset, for\nlarge-scale, long-term, and multi-object visual analysis. The videos in PANDA\nwere captured by a gigapixel camera and cover real-world scenes with both wide\nfield-of-view (~1 square kilometer area) and high-resolution details\n(~gigapixel-level/frame). The scenes may contain 4k head counts with over 100x\nscale variation. PANDA provides enriched and hierarchical ground-truth\nannotations, including 15,974.6k bounding boxes, 111.8k fine-grained attribute\nlabels, 12.7k trajectories, 2.2k groups and 2.9k interactions. We benchmark the\nhuman detection and tracking tasks. Due to the vast variance of pedestrian\npose, scale, occlusion and trajectory, existing approaches are challenged by\nboth accuracy and efficiency. Given the uniqueness of PANDA with both wide FoV\nand high resolution, a new task of interaction-aware group detection is\nintroduced. We design a 'global-to-local zoom-in' framework, where global\ntrajectories and local interactions are simultaneously encoded, yielding\npromising results. We believe PANDA will contribute to the community of\nartificial intelligence and praxeology by understanding human behaviors and\ninteractions in large-scale real-world scenes. PANDA Website:\nhttp://www.panda-dataset.com.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 16:58:32 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Wang", "Xueyang", ""], ["Zhang", "Xiya", ""], ["Zhu", "Yinheng", ""], ["Guo", "Yuchen", ""], ["Yuan", "Xiaoyun", ""], ["Xiang", "Liuyu", ""], ["Wang", "Zerun", ""], ["Ding", "Guiguang", ""], ["Brady", "David J", ""], ["Dai", "Qionghai", ""], ["Fang", "Lu", ""]]}, {"id": "2003.04857", "submitter": "Yuqian Zhou", "authors": "Yuqian Zhou, David Ren, Neil Emerton, Sehoon Lim, Timothy Large", "title": "Image Restoration for Under-Display Camera", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The new trend of full-screen devices encourages us to position a camera\nbehind a screen. Removing the bezel and centralizing the camera under the\nscreen brings larger display-to-body ratio and enhances eye contact in video\nchat, but also causes image degradation. In this paper, we focus on a\nnewly-defined Under-Display Camera (UDC), as a novel real-world single image\nrestoration problem. First, we take a 4k Transparent OLED (T-OLED) and a phone\nPentile OLED (P-OLED) and analyze their optical systems to understand the\ndegradation. Second, we design a Monitor-Camera Imaging System (MCIS) for\neasier real pair data acquisition, and a model-based data synthesizing pipeline\nto generate Point Spread Function (PSF) and UDC data only from display pattern\nand camera measurements. Finally, we resolve the complicated degradation using\ndeconvolution-based pipeline and learning-based methods. Our model demonstrates\na real-time high-quality restoration. The presented methods and results reveal\nthe promising research values and directions of UDC.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:09:00 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 07:54:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhou", "Yuqian", ""], ["Ren", "David", ""], ["Emerton", "Neil", ""], ["Lim", "Sehoon", ""], ["Large", "Timothy", ""]]}, {"id": "2003.04858", "submitter": "Yihao Zhao", "authors": "Yihao Zhao, Ruihai Wu, Hao Dong", "title": "Unpaired Image-to-Image Translation using Adversarial Consistency Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image translation is a class of vision problems whose goal\nis to find the mapping between different image domains using unpaired training\ndata. Cycle-consistency loss is a widely used constraint for such problems.\nHowever, due to the strict pixel-level constraint, it cannot perform geometric\nchanges, remove large objects, or ignore irrelevant texture. In this paper, we\npropose a novel adversarial-consistency loss for image-to-image translation.\nThis loss does not require the translated image to be translated back to be a\nspecific source image but can encourage the translated images to retain\nimportant features of the source images and overcome the drawbacks of\ncycle-consistency loss noted above. Our method achieves state-of-the-art\nresults on three challenging tasks: glasses removal, male-to-female\ntranslation, and selfie-to-anime translation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:10:38 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 05:04:22 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 11:50:27 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2020 14:44:59 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 12:48:57 GMT"}, {"version": "v6", "created": "Fri, 18 Dec 2020 02:56:53 GMT"}, {"version": "v7", "created": "Mon, 18 Jan 2021 12:13:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhao", "Yihao", ""], ["Wu", "Ruihai", ""], ["Dong", "Hao", ""]]}, {"id": "2003.04865", "submitter": "Yutaro Shigeto", "authors": "Yutaro Shigeto, Yuya Yoshikawa, Jiaqing Lin, Akikazu Takeuchi", "title": "Video Caption Dataset for Describing Human Actions in Japanese", "comments": "Accepted for LREC 2020. Dataset available at\n  https://actions.stair.center/captions.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, automatic video caption generation has attracted\nconsiderable attention. This paper focuses on the generation of Japanese\ncaptions for describing human actions. While most currently available video\ncaption datasets have been constructed for English, there is no equivalent\nJapanese dataset. To address this, we constructed a large-scale Japanese video\ncaption dataset consisting of 79,822 videos and 399,233 captions. Each caption\nin our dataset describes a video in the form of \"who does what and where.\" To\ndescribe human actions, it is important to identify the details of a person,\nplace, and action. Indeed, when we describe human actions, we usually mention\nthe scene, person, and action. In our experiments, we evaluated two caption\ngeneration methods to obtain benchmark results. Further, we investigated\nwhether those generation methods could specify \"who does what and where.\"\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:15:48 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Shigeto", "Yutaro", ""], ["Yoshikawa", "Yuya", ""], ["Lin", "Jiaqing", ""], ["Takeuchi", "Akikazu", ""]]}, {"id": "2003.04883", "submitter": "Veronica Mattioli", "authors": "Veronica Mattioli, Davide Alinovi and Riccardo Raheli", "title": "A Maximum Likelihood Approach to Speed Estimation of Foreground Objects\n  in Video Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion and speed estimation play a key role in computer vision and video\nprocessing for various application scenarios. Existing algorithms are mainly\nbased on projected and apparent motion models and are currently used in many\ncontexts, such as automotive security and driver assistance, industrial\nautomation and inspection systems, video surveillance, human activity tracking\ntechniques and biomedical solutions, including monitoring of vital signs. In\nthis paper, a general Maximum Likelihood (ML) approach to speed estimation of\nforeground objects in video streams is proposed. Application examples are\npresented and the performance of the proposed algorithms is discussed and\ncompared with more conventional solutions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:56:50 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Mattioli", "Veronica", ""], ["Alinovi", "Davide", ""], ["Raheli", "Riccardo", ""]]}, {"id": "2003.04888", "submitter": "Xin Liu", "authors": "Xin Liu, Yongbin Sun, Ziwei Liu, and Dahua Lin", "title": "Learning Diverse Fashion Collocation by Neural Graph Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion recommendation systems are highly desired by customers to find\nvisually-collocated fashion items, such as clothes, shoes, bags, etc. While\nexisting methods demonstrate promising results, they remain lacking in\nflexibility and diversity, e.g. assuming a fixed number of items or favoring\nsafe but boring recommendations. In this paper, we propose a novel fashion\ncollocation framework, Neural Graph Filtering, that models a flexible set of\nfashion items via a graph neural network. Specifically, we consider the visual\nembeddings of each garment as a node in the graph, and describe the\ninter-garment relationship as the edge between nodes. By applying symmetric\noperations on the edge vectors, this framework allows varying numbers of\ninputs/outputs and is invariant to their ordering. We further include a style\nclassifier augmented with focal loss to enable the collocation of significantly\ndiverse styles, which are inherently imbalanced in the training set. To\nfacilitate a comprehensive study on diverse fashion collocation, we reorganize\nAmazon Fashion dataset with carefully designed evaluation protocols. We\nevaluate the proposed approach on three popular benchmarks, the Polyvore\ndataset, the Polyvore-D dataset, and our reorganized Amazon Fashion dataset.\nExtensive experimental results show that our approach significantly outperforms\nthe state-of-the-art methods with over 10% improvements on the standard AUC\nmetric on the established tasks. More importantly, 82.5% of the users prefer\nour diverse-style recommendations over other alternatives in a real-world\nperception study.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 16:17:08 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Liu", "Xin", ""], ["Sun", "Yongbin", ""], ["Liu", "Ziwei", ""], ["Lin", "Dahua", ""]]}, {"id": "2003.04894", "submitter": "Kun Zhou", "authors": "Kun Zhou, Xiaoguang Han, Nianjuan Jiang, Kui Jia, Jiangbo Lu", "title": "HEMlets PoSh: Learning Part-Centric Heatmap Triplets for 3D Human Pose\n  and Shape Estimation", "comments": "14 pages, 14figures, To appear in TPAMI2021. arXiv admin note:\n  substantial text overlap with arXiv:1910.12032", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D human pose from a single image is a challenging task. This work\nattempts to address the uncertainty of lifting the detected 2D joints to the 3D\nspace by introducing an intermediate state-Part-Centric Heatmap Triplets\n(HEMlets), which shortens the gap between the 2D observation and the 3D\ninterpretation. The HEMlets utilize three joint-heatmaps to represent the\nrelative depth information of the end-joints for each skeletal body part. In\nour approach, a Convolutional Network (ConvNet) is first trained to predict\nHEMlets from the input image, followed by a volumetric joint-heatmap\nregression. We leverage on the integral operation to extract the joint\nlocations from the volumetric heatmaps, guaranteeing end-to-end learning.\nDespite the simplicity of the network design, the quantitative comparisons show\na significant performance improvement over the best-of-grade methods (e.g.\n$20\\%$ on Human3.6M). The proposed method naturally supports training with\n\"in-the-wild\" images, where only weakly-annotated relative depth information of\nskeletal joints is available. This further improves the generalization ability\nof our model, as validated by qualitative comparisons on outdoor images.\nLeveraging the strength of the HEMlets pose estimation, we further design and\nappend a shallow yet effective network module to regress the SMPL parameters of\nthe body pose and shape. We term the entire HEMlets-based human pose and shape\nrecovery pipeline HEMlets PoSh. Extensive quantitative and qualitative\nexperiments on the existing human body recovery benchmarks justify the\nstate-of-the-art results obtained with our HEMlets PoSh approach.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 04:03:45 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 08:21:32 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 07:01:23 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Zhou", "Kun", ""], ["Han", "Xiaoguang", ""], ["Jiang", "Nianjuan", ""], ["Jia", "Kui", ""], ["Lu", "Jiangbo", ""]]}, {"id": "2003.04942", "submitter": "Navyasri Reddy", "authors": "Navyasri Reddy, Samyak Jain, Pradeep Yarlagadda, Vineet Gandhi", "title": "Tidying Deep Saliency Prediction Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning computational models for visual attention (saliency estimation) is\nan effort to inch machines/robots closer to human visual cognitive abilities.\nData-driven efforts have dominated the landscape since the introduction of deep\nneural network architectures. In deep learning research, the choices in\narchitecture design are often empirical and frequently lead to more complex\nmodels than necessary. The complexity, in turn, hinders the application\nrequirements. In this paper, we identify four key components of saliency\nmodels, i.e., input features, multi-level integration, readout architecture,\nand loss functions. We review the existing state of the art models on these\nfour components and propose novel and simpler alternatives. As a result, we\npropose two novel end-to-end architectures called SimpleNet and MDNSal, which\nare neater, minimal, more interpretable and achieve state of the art\nperformance on public saliency benchmarks. SimpleNet is an optimized\nencoder-decoder architecture and brings notable performance gains on the\nSALICON dataset (the largest saliency benchmark). MDNSal is a parametric model\nthat directly predicts parameters of a GMM distribution and is aimed to bring\nmore interpretability to the prediction maps. The proposed saliency models can\nbe inferred at 25fps, making them suitable for real-time applications. Code and\npre-trained models are available at https://github.com/samyak0210/saliency.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 19:34:49 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Reddy", "Navyasri", ""], ["Jain", "Samyak", ""], ["Yarlagadda", "Pradeep", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2003.04949", "submitter": "Shan Lin", "authors": "Shan Lin, Fangbo Qin, Yangming Li, Randall A. Bly, Kris S. Moe, Blake\n  Hannaford", "title": "LC-GAN: Image-to-image Translation Based on Generative Adversarial\n  Network for Endoscopic Images", "comments": "Accepted by 2020 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent vision is appealing in computer-assisted and robotic surgeries.\nVision-based analysis with deep learning usually requires large labeled\ndatasets, but manual data labeling is expensive and time-consuming in medical\nproblems. We investigate a novel cross-domain strategy to reduce the need for\nmanual data labeling by proposing an image-to-image translation model\nlive-cadaver GAN (LC-GAN) based on generative adversarial networks (GANs). We\nconsider a situation when a labeled cadaveric surgery dataset is available\nwhile the task is instrument segmentation on an unlabeled live surgery dataset.\nWe train LC-GAN to learn the mappings between the cadaveric and live images.\nFor live image segmentation, we first translate the live images to\nfake-cadaveric images with LC-GAN and then perform segmentation on the\nfake-cadaveric images with models trained on the real cadaveric dataset. The\nproposed method fully makes use of the labeled cadaveric dataset for live image\nsegmentation without the need to label the live dataset. LC-GAN has two\ngenerators with different architectures that leverage the deep feature\nrepresentation learned from the cadaveric image based segmentation task.\nMoreover, we propose the structural similarity loss and segmentation\nconsistency loss to improve the semantic consistency during translation. Our\nmodel achieves better image-to-image translation and leads to improved\nsegmentation performance in the proposed cross-domain segmentation task.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 19:59:25 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 21:24:33 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Lin", "Shan", ""], ["Qin", "Fangbo", ""], ["Li", "Yangming", ""], ["Bly", "Randall A.", ""], ["Moe", "Kris S.", ""], ["Hannaford", "Blake", ""]]}, {"id": "2003.04981", "submitter": "Xinyi Zhou", "authors": "Xinyi Zhou, Jindi Wu, Reza Zafarani", "title": "SAFE: Similarity-Aware Multi-Modal Fake News Detection", "comments": "To be published in The 24th Pacific-Asia Conference on Knowledge\n  Discovery and Data Mining (PAKDD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective detection of fake news has recently attracted significant\nattention. Current studies have made significant contributions to predicting\nfake news with less focus on exploiting the relationship (similarity) between\nthe textual and visual information in news articles. Attaching importance to\nsuch similarity helps identify fake news stories that, for example, attempt to\nuse irrelevant images to attract readers' attention. In this work, we propose a\n$\\mathsf{S}$imilarity-$\\mathsf{A}$ware $\\mathsf{F}$ak$\\mathsf{E}$ news\ndetection method ($\\mathsf{SAFE}$) which investigates multi-modal (textual and\nvisual) information of news articles. First, neural networks are adopted to\nseparately extract textual and visual features for news representation. We\nfurther investigate the relationship between the extracted features across\nmodalities. Such representations of news textual and visual information along\nwith their relationship are jointly learned and used to predict fake news. The\nproposed method facilitates recognizing the falsity of news articles based on\ntheir text, images, or their \"mismatches.\" We conduct extensive experiments on\nlarge-scale real-world data, which demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:51:04 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Zhou", "Xinyi", ""], ["Wu", "Jindi", ""], ["Zafarani", "Reza", ""]]}, {"id": "2003.04989", "submitter": "Daniel Otero Baguer", "authors": "Daniel Otero Baguer, Johannes Leuschner, Maximilian Schmidt", "title": "Computed Tomography Reconstruction Using Deep Image Prior and Learned\n  Reconstruction Methods", "comments": null, "journal-ref": "Inverse Problems, Volume 36, Number 9 (2020)", "doi": "10.1088/1361-6420/aba415", "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the application of deep learning methods for\ncomputed tomography in the context of having a low-data regime. As motivation,\nwe review some of the existing approaches and obtain quantitative results after\ntraining them with different amounts of data. We find that the learned\nprimal-dual has an outstanding performance in terms of reconstruction quality\nand data efficiency. However, in general, end-to-end learned methods have two\nissues: a) lack of classical guarantees in inverse problems and b) lack of\ngeneralization when not trained with enough data. To overcome these issues, we\nbring in the deep image prior approach in combination with classical\nregularization. The proposed methods improve the state-of-the-art results in\nthe low data-regime.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 21:03:34 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 12:09:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Baguer", "Daniel Otero", ""], ["Leuschner", "Johannes", ""], ["Schmidt", "Maximilian", ""]]}, {"id": "2003.05005", "submitter": "Shreyank N Gowda", "authors": "Shreyank N Gowda, Chun Yuan", "title": "Using an ensemble color space model to tackle adversarial examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minute pixel changes in an image drastically change the prediction that the\ndeep learning model makes. One of the most significant problems that could\narise due to this, for instance, is autonomous driving. Many methods have been\nproposed to combat this with varying amounts of success. We propose a 3 step\nmethod for defending such attacks. First, we denoise the image using\nstatistical methods. Second, we show that adopting multiple color spaces in the\nsame model can help us to fight these adversarial attacks further as each color\nspace detects certain features explicit to itself. Finally, the feature maps\ngenerated are enlarged and sent back as an input to obtain even smaller\nfeatures. We show that the proposed model does not need to be trained to defend\nan particular type of attack and is inherently more robust to black-box,\nwhite-box, and grey-box adversarial attack techniques. In particular, the model\nis 56.12 percent more robust than compared models in case of white box attacks\nwhen the models are not subject to adversarial example training.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 21:20:53 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gowda", "Shreyank N", ""], ["Yuan", "Chun", ""]]}, {"id": "2003.05015", "submitter": "Kathl\\'en Kohn", "authors": "Timothy Duff, Kathl\\'en Kohn, Anton Leykin, Tomas Pajdla", "title": "PL${}_{1}$P -- Point-line Minimal Problems under Partial Visibility in\n  Three Views", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete classification of minimal problems for generic\narrangements of points and lines in space observed partially by three\ncalibrated perspective cameras when each line is incident to at most one point.\nThis is a large class of interesting minimal problems that allows missing\nobservations in images due to occlusions and missed detections. There is an\ninfinite number of such minimal problems; however, we show that they can be\nreduced to 140616 equivalence classes by removing superfluous features and\nrelabeling the cameras. We also introduce camera-minimal problems, which are\npractical for designing minimal solvers, and show how to pick a simplest\ncamera-minimal problem for each minimal problem. This simplification results in\n74575 equivalence classes. Only 76 of these were known; the rest are new. In\norder to identify problems that have potential for practical solving of image\nmatching and 3D reconstruction, we present several smaller natural subfamilies\nof camera-minimal problems as well as compute solution counts for all\ncamera-minimal problems which have less than 300 solutions for generic data.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 21:50:52 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Duff", "Timothy", ""], ["Kohn", "Kathl\u00e9n", ""], ["Leykin", "Anton", ""], ["Pajdla", "Tomas", ""]]}, {"id": "2003.05020", "submitter": "Wenguan Wang", "authors": "Xiankai Lu, Wenguan Wang, Jianbing Shen, Yu-Wing Tai, David Crandall,\n  and Steven C. H. Hoi", "title": "Learning Video Object Segmentation from Unlabeled Videos", "comments": "Accepted to CVPR 2020. Code: https://github.com/carrierlxk/MuG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for video object segmentation (VOS) that addresses\nobject pattern learning from unlabeled videos, unlike most existing methods\nwhich rely heavily on extensive annotated data. We introduce a unified\nunsupervised/weakly supervised learning framework, called MuG, that\ncomprehensively captures intrinsic properties of VOS at multiple granularities.\nOur approach can help advance understanding of visual patterns in VOS and\nsignificantly reduce annotation burden. With a carefully-designed architecture\nand strong representation learning ability, our learned model can be applied to\ndiverse VOS settings, including object-level zero-shot VOS, instance-level\nzero-shot VOS, and one-shot VOS. Experiments demonstrate promising performance\nin these settings, as well as the potential of MuG in leveraging unlabeled data\nto further improve the segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 22:12:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Lu", "Xiankai", ""], ["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""], ["Tai", "Yu-Wing", ""], ["Crandall", "David", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2003.05034", "submitter": "Ali Dabouei", "authors": "Ali Dabouei, Sobhan Soleymani, Fariborz Taherkhani, Nasser M.\n  Nasrabadi", "title": "SuperMix: Supervising the Mixing Data Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a supervised mixing augmentation method, termed\nSuperMix, which exploits the knowledge of a teacher to mix images based on\ntheir salient regions. SuperMix optimizes a mixing objective that considers: i)\nforcing the class of input images to appear in the mixed image, ii) preserving\nthe local structure of images, and iii) reducing the risk of suppressing\nimportant features. To make the mixing suitable for large-scale applications,\nwe develop an optimization technique, $65\\times$ faster than gradient descent\non the same problem. We validate the effectiveness of SuperMix through\nextensive evaluations and ablation studies on two tasks of object\nclassification and knowledge distillation. On the classification task, SuperMix\nprovides the same performance as the advanced augmentation methods, such as\nAutoAugment. On the distillation task, SuperMix sets a new state-of-the-art\nwith a significantly simplified distillation method. Particularly, in six out\nof eight teacher-student setups from the same architectures, the students\ntrained on the mixed data surpass their teachers with a notable margin.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 23:24:38 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Dabouei", "Ali", ""], ["Soleymani", "Sobhan", ""], ["Taherkhani", "Fariborz", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "2003.05037", "submitter": "Hayit Greenspan", "authors": "Ophir Gozes, Maayan Frid-Adar, Hayit Greenspan, Patrick D. Browning,\n  Huangqi Zhang, Wenbin Ji, Adam Bernheim, Eliot Siegel", "title": "Rapid AI Development Cycle for the Coronavirus (COVID-19) Pandemic:\n  Initial Results for Automated Detection & Patient Monitoring using Deep\n  Learning CT Image Analysis", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Develop AI-based automated CT image analysis tools for detection,\nquantification, and tracking of Coronavirus; demonstrate they can differentiate\ncoronavirus patients from non-patients. Materials and Methods: Multiple\ninternational datasets, including from Chinese disease-infected areas were\nincluded. We present a system that utilizes robust 2D and 3D deep learning\nmodels, modifying and adapting existing AI models and combining them with\nclinical understanding. We conducted multiple retrospective experiments to\nanalyze the performance of the system in the detection of suspected COVID-19\nthoracic CT features and to evaluate evolution of the disease in each patient\nover time using a 3D volume review, generating a Corona score. The study\nincludes a testing set of 157 international patients (China and U.S). Results:\nClassification results for Coronavirus vs Non-coronavirus cases per thoracic CT\nstudies were 0.996 AUC (95%CI: 0.989-1.00) ; on datasets of Chinese control and\ninfected patients. Possible working point: 98.2% sensitivity, 92.2%\nspecificity. For time analysis of Coronavirus patients, the system output\nenables quantitative measurements for smaller opacities (volume, diameter) and\nvisualization of the larger opacities in a slice-based heat map or a 3D volume\ndisplay. Our suggested Corona score measures the progression of disease over\ntime. Conclusion: This initial study, which is currently being expanded to a\nlarger population, demonstrated that rapidly developed AI-based image analysis\ncan achieve high accuracy in detection of Coronavirus as well as quantification\nand tracking of disease burden.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 23:37:21 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 07:44:00 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 08:20:12 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Gozes", "Ophir", ""], ["Frid-Adar", "Maayan", ""], ["Greenspan", "Hayit", ""], ["Browning", "Patrick D.", ""], ["Zhang", "Huangqi", ""], ["Ji", "Wenbin", ""], ["Bernheim", "Adam", ""], ["Siegel", "Eliot", ""]]}, {"id": "2003.05056", "submitter": "Maryam Asadi", "authors": "Maryam Asadi-Aghbolaghi, Reza Azad, Mahmood Fathy, and Sergio Escalera", "title": "Multi-level Context Gating of Embedded Collective Knowledge for Medical\n  Image Segmentation", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.00166", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation has been very challenging due to the large\nvariation of anatomy across different cases. Recent advances in deep learning\nframeworks have exhibited faster and more accurate performance in image\nsegmentation. Among the existing networks, U-Net has been successfully applied\non medical image segmentation. In this paper, we propose an extension of U-Net\nfor medical image segmentation, in which we take full advantages of U-Net,\nSqueeze and Excitation (SE) block, bi-directional ConvLSTM (BConvLSTM), and the\nmechanism of dense convolutions. (I) We improve the segmentation performance by\nutilizing SE modules within the U-Net, with a minor effect on model complexity.\nThese blocks adaptively recalibrate the channel-wise feature responses by\nutilizing a self-gating mechanism of the global information embedding of the\nfeature maps. (II) To strengthen feature propagation and encourage feature\nreuse, we use densely connected convolutions in the last convolutional layer of\nthe encoding path. (III) Instead of a simple concatenation in the skip\nconnection of U-Net, we employ BConvLSTM in all levels of the network to\ncombine the feature maps extracted from the corresponding encoding path and the\nprevious decoding up-convolutional layer in a non-linear way. The proposed\nmodel is evaluated on six datasets DRIVE, ISIC 2017 and 2018, lung\nsegmentation, $PH^2$, and cell nuclei segmentation, achieving state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:29:59 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Asadi-Aghbolaghi", "Maryam", ""], ["Azad", "Reza", ""], ["Fathy", "Mahmood", ""], ["Escalera", "Sergio", ""]]}, {"id": "2003.05065", "submitter": "Tom Runia", "authors": "Tom F.H. Runia, Kirill Gavrilyuk, Cees G.M. Snoek, Arnold W.M.\n  Smeulders", "title": "Cloth in the Wind: A Case Study of Physical Measurement through\n  Simulation", "comments": "CVPR 2020. arXiv admin note: substantial text overlap with\n  arXiv:1910.07861", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many of the physical phenomena around us, we have developed sophisticated\nmodels explaining their behavior. Nevertheless, measuring physical properties\nfrom visual observations is challenging due to the high number of causally\nunderlying physical parameters -- including material properties and external\nforces. In this paper, we propose to measure latent physical properties for\ncloth in the wind without ever having seen a real example before. Our solution\nis an iterative refinement procedure with simulation at its core. The algorithm\ngradually updates the physical model parameters by running a simulation of the\nobserved phenomenon and comparing the current simulation to a real-world\nobservation. The correspondence is measured using an embedding function that\nmaps physically similar examples to nearby points. We consider a case study of\ncloth in the wind, with curling flags as our leading example -- a seemingly\nsimple phenomena but physically highly involved. Based on the physics of cloth\nand its visual manifestation, we propose an instantiation of the embedding\nfunction. For this mapping, modeled as a deep network, we introduce a spectral\nlayer that decomposes a video volume into its temporal spectral power and\ncorresponding frequencies. Our experiments demonstrate that the proposed method\ncompares favorably to prior work on the task of measuring cloth material\nproperties and external wind force from a real-world video.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 21:32:23 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Runia", "Tom F. H.", ""], ["Gavrilyuk", "Kirill", ""], ["Snoek", "Cees G. M.", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "2003.05078", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Jean-Baptiste Alayrac, Aida Nematzadeh, Lucas\n  Smaira, Mateusz Malinowski, Jo\\~ao Carreira, Phil Blunsom, Andrew Zisserman", "title": "Visual Grounding in Video for Unsupervised Word Translation", "comments": "CVPR 2020", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are thousands of actively spoken languages on Earth, but a single\nvisual world. Grounding in this visual world has the potential to bridge the\ngap between all these languages. Our goal is to use visual grounding to improve\nunsupervised word mapping between languages. The key idea is to establish a\ncommon visual representation between two languages by learning embeddings from\nunpaired instructional videos narrated in the native language. Given this\nshared embedding we demonstrate that (i) we can map words between the\nlanguages, particularly the 'visual' words; (ii) that the shared embedding\nprovides a good initialization for existing unsupervised text-based word\ntranslation techniques, forming the basis for our proposed hybrid visual-text\nmapping algorithm, MUVE; and (iii) our approach achieves superior performance\nby addressing the shortcomings of text-based methods -- it is more robust,\nhandles datasets with less commonality, and is applicable to low-resource\nlanguages. We apply these methods to translate words from English to French,\nKorean, and Japanese -- all without any parallel corpora and simply by watching\nmany videos of people speaking while doing things.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 02:03:37 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 15:20:44 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Alayrac", "Jean-Baptiste", ""], ["Nematzadeh", "Aida", ""], ["Smaira", "Lucas", ""], ["Malinowski", "Mateusz", ""], ["Carreira", "Jo\u00e3o", ""], ["Blunsom", "Phil", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2003.05080", "submitter": "Sam Maksoud", "authors": "Sam Maksoud, Kun Zhao, Peter Hobson, Anthony Jennings, and Brian\n  Lovell", "title": "SOS: Selective Objective Switch for Rapid Immunofluorescence Whole Slide\n  Image Classification", "comments": "Accepted for publication at CVPR2020", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00392", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulty of processing gigapixel whole slide images (WSIs) in clinical\nmicroscopy has been a long-standing barrier to implementing computer aided\ndiagnostic systems. Since modern computing resources are unable to perform\ncomputations at this extremely large scale, current state of the art methods\nutilize patch-based processing to preserve the resolution of WSIs. However,\nthese methods are often resource intensive and make significant compromises on\nprocessing time. In this paper, we demonstrate that conventional patch-based\nprocessing is redundant for certain WSI classification tasks where high\nresolution is only required in a minority of cases. This reflects what is\nobserved in clinical practice; where a pathologist may screen slides using a\nlow power objective and only switch to a high power in cases where they are\nuncertain about their findings. To eliminate these redundancies, we propose a\nmethod for the selective use of high resolution processing based on the\nconfidence of predictions on downscaled WSIs --- we call this the Selective\nObjective Switch (SOS). Our method is validated on a novel dataset of 684\nLiver-Kidney-Stomach immunofluorescence WSIs routinely used in the\ninvestigation of autoimmune liver disease. By limiting high resolution\nprocessing to cases which cannot be classified confidently at low resolution,\nwe maintain the accuracy of patch-level analysis whilst reducing the inference\ntime by a factor of 7.74.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 02:08:46 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Maksoud", "Sam", ""], ["Zhao", "Kun", ""], ["Hobson", "Peter", ""], ["Jennings", "Anthony", ""], ["Lovell", "Brian", ""]]}, {"id": "2003.05093", "submitter": "Wonjik Kim", "authors": "Wonjik Kim, Masayuki Tanaka, Masatoshi Okutomi, Yoko Sasaki", "title": "Learning-Based Human Segmentation and Velocity Estimation Using\n  Automatic Labeled LiDAR Sequence for Training", "comments": "Please check the following URL for more information.\n  http://www.ok.sc.e.titech.ac.jp/res/LHD/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automatic labeled sequential data generation\npipeline for human segmentation and velocity estimation with point clouds.\nConsidering the impact of deep neural networks, state-of-the-art network\narchitectures have been proposed for human recognition using point clouds\ncaptured by Light Detection and Ranging (LiDAR). However, one disadvantage is\nthat legacy datasets may only cover the image domain without providing\nimportant label information and this limitation has disturbed the progress of\nresearch to date. Therefore, we develop an automatic labeled sequential data\ngeneration pipeline, in which we can control any parameter or data generation\nenvironment with pixel-wise and per-frame ground truth segmentation and\npixel-wise velocity information for human recognition. Our approach uses a\nprecise human model and reproduces a precise motion to generate realistic\nartificial data. We present more than 7K video sequences which consist of 32\nframes generated by the proposed pipeline. With the proposed sequence\ngenerator, we confirm that human segmentation performance is improved when\nusing the video domain compared to when using the image domain. We also\nevaluate our data by comparing with data generated under different conditions.\nIn addition, we estimate pedestrian velocity with LiDAR by only utilizing data\ngenerated by the proposed pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 03:14:52 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kim", "Wonjik", ""], ["Tanaka", "Masayuki", ""], ["Okutomi", "Masatoshi", ""], ["Sasaki", "Yoko", ""]]}, {"id": "2003.05102", "submitter": "Tianwei Zhang", "authors": "Tianwei Zhang, Huayan Zhang, Yang Li, Yoshihiko Nakamura and Lei Zhang", "title": "FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow", "comments": "To be published in ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic environments are challenging for visual SLAM since the moving objects\nocclude the static environment features and lead to wrong camera motion\nestimation. In this paper, we present a novel dense RGB-D SLAM solution that\nsimultaneously accomplishes the dynamic/static segmentation and camera\nego-motion estimation as well as the static background reconstructions. Our\nnovelty is using optical flow residuals to highlight the dynamic semantics in\nthe RGB-D point clouds and provide more accurate and efficient dynamic/static\nsegmentation for camera tracking and background reconstruction. The dense\nreconstruction results on public datasets and real dynamic scenes indicate that\nthe proposed approach achieved accurate and efficient performances in both\ndynamic and static environments compared to state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 04:00:49 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Zhang", "Tianwei", ""], ["Zhang", "Huayan", ""], ["Li", "Yang", ""], ["Nakamura", "Yoshihiko", ""], ["Zhang", "Lei", ""]]}, {"id": "2003.05112", "submitter": "Wei-Ta Chu", "authors": "Sian-Yao Huang and Wei-Ta Chu", "title": "PONAS: Progressive One-shot Neural Architecture Search for Very\n  Efficient Deployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We achieve very efficient deep learning model deployment that designs neural\nnetwork architectures to fit different hardware constraints. Given a\nconstraint, most neural architecture search (NAS) methods either sample a set\nof sub-networks according to a pre-trained accuracy predictor, or adopt the\nevolutionary algorithm to evolve specialized networks from the supernet. Both\napproaches are time consuming. Here our key idea for very efficient deployment\nis, when searching the architecture space, constructing a table that stores the\nvalidation accuracy of all candidate blocks at all layers. For a stricter\nhardware constraint, the architecture of a specialized network can be very\nefficiently determined based on this table by picking the best candidate blocks\nthat yield the least accuracy loss. To accomplish this idea, we propose\nProgressive One-shot Neural Architecture Search (PONAS) that combines\nadvantages of progressive NAS and one-shot methods. In PONAS, we propose a\ntwo-stage training scheme, including the meta training stage and the\nfine-tuning stage, to make the search process efficient and stable. During\nsearch, we evaluate candidate blocks in different layers and construct the\naccuracy table that is to be used in deployment. Comprehensive experiments\nverify that PONAS is extremely flexible, and is able to find architecture of a\nspecialized network in around 10 seconds. In ImageNet classification, 75.2%\ntop-1 accuracy can be obtained, which is comparable with the state of the arts.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 05:00:31 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 05:27:40 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Huang", "Sian-Yao", ""], ["Chu", "Wei-Ta", ""]]}, {"id": "2003.05122", "submitter": "Gruber Tobias", "authors": "Stefanie Walz and Tobias Gruber and Werner Ritter and Klaus Dietmayer", "title": "Uncertainty depth estimation with gated images for 3D reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gated imaging is an emerging sensor technology for self-driving cars that\nprovides high-contrast images even under adverse weather influence. It has been\nshown that this technology can even generate high-fidelity dense depth maps\nwith accuracy comparable to scanning LiDAR systems. In this work, we extend the\nrecent Gated2Depth framework with aleatoric uncertainty providing an additional\nconfidence measure for the depth estimates. This confidence can help to filter\nout uncertain estimations in regions without any illumination. Moreover, we\nshow that training on dense depth maps generated by LiDAR depth completion\nalgorithms can further improve the performance.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 06:00:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Walz", "Stefanie", ""], ["Gruber", "Tobias", ""], ["Ritter", "Werner", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "2003.05128", "submitter": "Sungha Choi", "authors": "Sungha Choi, Joanne T. Kim, Jaegul Choo", "title": "Cars Can't Fly up in the Sky: Improving Urban-Scene Segmentation via\n  Height-driven Attention Networks", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exploits the intrinsic features of urban-scene images and proposes\na general add-on module, called height-driven attention networks (HANet), for\nimproving semantic segmentation for urban-scene images. It emphasizes\ninformative features or classes selectively according to the vertical position\nof a pixel. The pixel-wise class distributions are significantly different from\neach other among horizontally segmented sections in the urban-scene images.\nLikewise, urban-scene images have their own distinct characteristics, but most\nsemantic segmentation networks do not reflect such unique attributes in the\narchitecture. The proposed network architecture incorporates the capability\nexploiting the attributes to handle the urban scene dataset effectively. We\nvalidate the consistent performance (mIoU) increase of various semantic\nsegmentation models on two datasets when HANet is adopted. This extensive\nquantitative analysis demonstrates that adding our module to existing models is\neasy and cost-effective. Our method achieves a new state-of-the-art performance\non the Cityscapes benchmark with a large margin among ResNet-101 based\nsegmentation models. Also, we show that the proposed model is coherent with the\nfacts observed in the urban scene by visualizing and interpreting the attention\nmap. Our code and trained models are publicly available at\nhttps://github.com/shachoi/HANet\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 06:22:12 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 12:06:05 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 02:34:31 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Choi", "Sungha", ""], ["Kim", "Joanne T.", ""], ["Choo", "Jaegul", ""]]}, {"id": "2003.05136", "submitter": "Jun Wan", "authors": "Ajian Li, Zichang Tan, Xuan Li, Jun Wan, Sergio Escalera, Guodong Guo,\n  Stan Z. Li", "title": "CASIA-SURF CeFA: A Benchmark for Multi-modal Cross-ethnicity Face\n  Anti-spoofing", "comments": "17 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1912.02340", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethnic bias has proven to negatively affect the performance of face\nrecognition systems, and it remains an open research problem in face\nanti-spoofing. In order to study the ethnic bias for face anti-spoofing, we\nintroduce the largest up to date CASIA-SURF Cross-ethnicity Face Anti-spoofing\n(CeFA) dataset (briefly named CeFA), covering $3$ ethnicities, $3$ modalities,\n$1,607$ subjects, and 2D plus 3D attack types. Four protocols are introduced to\nmeasure the affect under varied evaluation conditions, such as cross-ethnicity,\nunknown spoofs or both of them. To the best of our knowledge, CeFA is the first\ndataset including explicit ethnic labels in current published/released datasets\nfor face anti-spoofing. Then, we propose a novel multi-modal fusion method as a\nstrong baseline to alleviate these bias, namely, the static-dynamic fusion\nmechanism applied in each modality (i.e., RGB, Depth and infrared image).\nLater, a partially shared fusion strategy is proposed to learn complementary\ninformation from multiple modalities. Extensive experiments demonstrate that\nthe proposed method achieves state-of-the-art results on the CASIA-SURF,\nOULU-NPU, SiW and the CeFA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 06:58:54 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Ajian", ""], ["Tan", "Zichang", ""], ["Li", "Xuan", ""], ["Wan", "Jun", ""], ["Escalera", "Sergio", ""], ["Guo", "Guodong", ""], ["Li", "Stan Z.", ""]]}, {"id": "2003.05145", "submitter": "Hyeongmin Lee", "authors": "Hyeongmin Lee, Taeoh Kim, Hanbin Son, Sangwook Baek, Minsu Cheon,\n  Sangyoun Lee", "title": "Regularized Adaptation for Stable and Efficient Continuous-Level\n  Learning on Image Processing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Convolutional Neural Network (CNN) based image processing, most of the\nstudies propose networks that are optimized for a single-level (or a\nsingle-objective); thus, they underperform on other levels and must be\nretrained for delivery of optimal performance. Using multiple models to cover\nmultiple levels involves very high computational costs. To solve these\nproblems, recent approaches train the networks on two different levels and\npropose their own interpolation methods to enable the arbitrary intermediate\nlevels. However, many of them fail to adapt hard tasks or interpolate smoothly,\nor the others still require large memory and computational cost. In this paper,\nwe propose a novel continuous-level learning framework using a Filter\nTransition Network (FTN) which is a non-linear module that easily adapt to new\nlevels, and is regularized to prevent undesirable side-effects. Additionally,\nfor stable learning of FTN, we newly propose a method to initialize non-linear\nCNNs with identity mappings. Furthermore, FTN is extremely lightweight module\nsince it is a data-independent module, which means it is not affected by the\nspatial resolution of the inputs. Extensive results for various image\nprocessing tasks indicate that the performance of FTN is stable in terms of\nadaptation and interpolation, and comparable to that of the other heavy\nframeworks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 07:46:57 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 03:52:48 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Lee", "Hyeongmin", ""], ["Kim", "Taeoh", ""], ["Son", "Hanbin", ""], ["Baek", "Sangwook", ""], ["Cheon", "Minsu", ""], ["Lee", "Sangyoun", ""]]}, {"id": "2003.05162", "submitter": "Tejas Gokhale", "authors": "Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou\n  Yang", "title": "Video2Commonsense: Generating Commonsense Descriptions to Enrich Video\n  Captioning", "comments": "Accepted to EMNLP, Long Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Captioning is a crucial and challenging task for video understanding. In\nvideos that involve active agents such as humans, the agent's actions can bring\nabout myriad changes in the scene. Observable changes such as movements,\nmanipulations, and transformations of the objects in the scene, are reflected\nin conventional video captioning. Unlike images, actions in videos are also\ninherently linked to social aspects such as intentions (why the action is\ntaking place), effects (what changes due to the action), and attributes that\ndescribe the agent. Thus for video understanding, such as when captioning\nvideos or when answering questions about videos, one must have an understanding\nof these commonsense aspects. We present the first work on generating\ncommonsense captions directly from videos, to describe latent aspects such as\nintentions, effects, and attributes. We present a new dataset\n\"Video-to-Commonsense (V2C)\" that contains $\\sim9k$ videos of human agents\nperforming various actions, annotated with 3 types of commonsense descriptions.\nAdditionally we explore the use of open-ended video-based commonsense question\nanswering (V2C-QA) as a way to enrich our captions. Both the generation task\nand the QA task can be used to enrich video captions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:42:57 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 05:16:13 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 02:08:26 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Gokhale", "Tejas", ""], ["Banerjee", "Pratyay", ""], ["Baral", "Chitta", ""], ["Yang", "Yezhou", ""]]}, {"id": "2003.05176", "submitter": "Quanquan Li", "authors": "Jingru Tan, Changbao Wang, Buyu Li, Quanquan Li, Wanli Ouyang,\n  Changqing Yin, Junjie Yan", "title": "Equalization Loss for Long-Tailed Object Recognition", "comments": "CVPR 2020. Winner of LVIS Challenge 2019. Code has been available at\n  https: //github.com/tztztztztz/eql.detectron2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object recognition techniques using convolutional neural networks (CNN) have\nachieved great success. However, state-of-the-art object detection methods\nstill perform poorly on large vocabulary and long-tailed datasets, e.g. LVIS.\nIn this work, we analyze this problem from a novel perspective: each positive\nsample of one category can be seen as a negative sample for other categories,\nmaking the tail categories receive more discouraging gradients. Based on it, we\npropose a simple but effective loss, named equalization loss, to tackle the\nproblem of long-tailed rare categories by simply ignoring those gradients for\nrare categories. The equalization loss protects the learning of rare categories\nfrom being at a disadvantage during the network parameter updating. Thus the\nmodel is capable of learning better discriminative features for objects of rare\nclasses. Without any bells and whistles, our method achieves AP gains of 4.1%\nand 4.8% for the rare and common categories on the challenging LVIS benchmark,\ncompared to the Mask R-CNN baseline. With the utilization of the effective\nequalization loss, we finally won the 1st place in the LVIS Challenge 2019.\nCode has been made available at: https: //github.com/tztztztztz/eql.detectron2\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 09:14:53 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 15:31:25 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tan", "Jingru", ""], ["Wang", "Changbao", ""], ["Li", "Buyu", ""], ["Li", "Quanquan", ""], ["Ouyang", "Wanli", ""], ["Yin", "Changqing", ""], ["Yan", "Junjie", ""]]}, {"id": "2003.05182", "submitter": "Dominique Beaini", "authors": "Dominique Beaini, Sofiane Achiche, Maxime Raison", "title": "Improving Convolutional Neural Networks Via Conservative Field\n  Regularisation and Integration", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current research in convolutional neural networks (CNN) focuses mainly on\nchanging the architecture of the networks, optimizing the hyper-parameters and\nimproving the gradient descent. However, most work use only 3 standard families\nof operations inside the CNN, the convolution, the activation function, and the\npooling. In this work, we propose a new family of operations based on the\nGreen's function of the Laplacian, which allows the network to solve the\nLaplacian, to integrate any vector field and to regularize the field by forcing\nit to be conservative. Hence, the Green's function (GF) is the first operation\nthat regularizes the 2D or 3D feature space by forcing it to be conservative\nand physically interpretable, instead of regularizing the norm of the weights.\nOur results show that such regularization allows the network to learn faster,\nto have smoother training curves and to better generalize, without any\nadditional parameter. The current manuscript presents early results, more work\nis required to benchmark the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 09:29:48 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Beaini", "Dominique", ""], ["Achiche", "Sofiane", ""], ["Raison", "Maxime", ""]]}, {"id": "2003.05209", "submitter": "Asma Khatun Dr.", "authors": "Asma Khatun and Sk. Golam Sarowar Hossain", "title": "A Fourier Domain Feature Approach for Human Activity Recognition & Fall\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elder people consequence a variety of problems while living Activities of\nDaily Living (ADL) for the reason of age, sense, loneliness and cognitive\nchanges. These cause the risk to ADL which leads to several falls. Getting real\nlife fall data is a difficult process and are not available whereas simulated\nfalls become ubiquitous to evaluate the proposed methodologies. From the\nliterature review, it is investigated that most of the researchers used raw and\nenergy features (time domain features) of the signal data as those are most\ndiscriminating. However, in real life situations fall signal may be noisy than\nthe current simulated data. Hence the result using raw feature may dramatically\nchanges when using in a real life scenario. This research is using frequency\ndomain Fourier coefficient features to differentiate various human activities\nof daily life. The feature vector constructed using those Fast Fourier\nTransform are robust to noise and rotation invariant. Two different supervised\nclassifiers kNN and SVM are used for evaluating the method. Two standard\npublicly available datasets are used for benchmark analysis. In this research,\nmore discriminating results are obtained applying kNN classifier than the SVM\nclassifier. Various standard measure including Standard Accuracy (SA), Macro\nAverage Accuracy (MAA), Sensitivity (SE) and Specificity (SP) has been\naccounted. In all cases, the proposed method outperforms energy features\nwhereas competitive results are shown with raw features. It is also noticed\nthat the proposed method performs better than the recently risen deep learning\napproach in which data augmentation method were not used.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 10:49:11 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Khatun", "Asma", ""], ["Hossain", "Sk. Golam Sarowar", ""]]}, {"id": "2003.05212", "submitter": "Hongzhuo Liang", "authors": "Shuang Li, Jiaxi Jiang, Philipp Ruppel, Hongzhuo Liang, Xiaojian Ma,\n  Norman Hendrich, Fuchun Sun, Jianwei Zhang", "title": "A Mobile Robot Hand-Arm Teleoperation System by Vision and IMU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multimodal mobile teleoperation system that\nconsists of a novel vision-based hand pose regression network (Transteleop) and\nan IMU-based arm tracking method. Transteleop observes the human hand through a\nlow-cost depth camera and generates not only joint angles but also depth images\nof paired robot hand poses through an image-to-image translation process. A\nkeypoint-based reconstruction loss explores the resemblance in appearance and\nanatomy between human and robotic hands and enriches the local features of\nreconstructed images. A wearable camera holder enables simultaneous hand-arm\ncontrol and facilitates the mobility of the whole teleoperation system. Network\nevaluation results on a test dataset and a variety of complex manipulation\ntasks that go beyond simple pick-and-place operations show the efficiency and\nstability of our multimodal teleoperation system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 10:57:24 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Shuang", ""], ["Jiang", "Jiaxi", ""], ["Ruppel", "Philipp", ""], ["Liang", "Hongzhuo", ""], ["Ma", "Xiaojian", ""], ["Hendrich", "Norman", ""], ["Sun", "Fuchun", ""], ["Zhang", "Jianwei", ""]]}, {"id": "2003.05218", "submitter": "Yiming Li", "authors": "Yiming Li, Changhong Fu, Ziyuan Huang, Yinqiang Zhang, Jia Pan", "title": "Keyfilter-Aware Real-Time UAV Object Tracking", "comments": "2020 International Conference on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter-based tracking has been widely applied in unmanned aerial\nvehicle (UAV) with high efficiency. However, it has two imperfections, i.e.,\nboundary effect and filter corruption. Several methods enlarging the search\narea can mitigate boundary effect, yet introducing undesired background\ndistraction. Existing frame-by-frame context learning strategies for repressing\nbackground distraction nevertheless lower the tracking speed. Inspired by\nkeyframe-based simultaneous localization and mapping, keyfilter is proposed in\nvisual tracking for the first time, in order to handle the above issues\nefficiently and effectively. Keyfilters generated by periodically selected\nkeyframes learn the context intermittently and are used to restrain the\nlearning of filters, so that 1) context awareness can be transmitted to all the\nfilters via keyfilter restriction, and 2) filter corruption can be repressed.\nCompared to the state-of-the-art results, our tracker performs better on two\nchallenging benchmarks, with enough speed for UAV real-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:11:16 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Yiming", ""], ["Fu", "Changhong", ""], ["Huang", "Ziyuan", ""], ["Zhang", "Yinqiang", ""], ["Pan", "Jia", ""]]}, {"id": "2003.05235", "submitter": "Weilin Huang", "authors": "Yu Gao, Xintong Han, Xun Wang, Weilin Huang, Matthew R. Scott", "title": "Channel Interaction Networks for Fine-Grained Image Categorization", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image categorization is challenging due to the subtle\ninter-class differences.We posit that exploiting the rich relationships between\nchannels can help capture such differences since different channels correspond\nto different semantics. In this paper, we propose a channel interaction network\n(CIN), which models the channel-wise interplay both within an image and across\nimages. For a single image, a self-channel interaction (SCI) module is proposed\nto explore channel-wise correlation within the image. This allows the model to\nlearn the complementary features from the correlated channels, yielding\nstronger fine-grained features. Furthermore, given an image pair, we introduce\na contrastive channel interaction (CCI) module to model the cross-sample\nchannel interaction with a metric learning framework, allowing the CIN to\ndistinguish the subtle visual differences between images. Our model can be\ntrained efficiently in an end-to-end fashion without the need of multi-stage\ntraining and testing. Finally, comprehensive experiments are conducted on three\npublicly available benchmarks, where the proposed method consistently\noutperforms the state-of-theart approaches, such as DFL-CNN (Wang, Morariu, and\nDavis 2018) and NTS (Yang et al. 2018).\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:51:51 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Gao", "Yu", ""], ["Han", "Xintong", ""], ["Wang", "Xun", ""], ["Huang", "Weilin", ""], ["Scott", "Matthew R.", ""]]}, {"id": "2003.05242", "submitter": "Yang Dongming", "authors": "Dongming Yang, YueXian Zou, Jian Zhang, Ge Li", "title": "GID-Net: Detecting Human-Object Interaction with Global and Instance\n  Dependency", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since detecting and recognizing individual human or object are not adequate\nto understand the visual world, learning how humans interact with surrounding\nobjects becomes a core technology. However, convolution operations are weak in\ndepicting visual interactions between the instances since they only build\nblocks that process one local neighborhood at a time. To address this problem,\nwe learn from human perception in observing HOIs to introduce a two-stage\ntrainable reasoning mechanism, referred to as GID block. GID block breaks\nthrough the local neighborhoods and captures long-range dependency of pixels\nboth in global-level and instance-level from the scene to help detecting\ninteractions between instances. Furthermore, we conduct a multi-stream network\ncalled GID-Net, which is a human-object interaction detection framework\nconsisting of a human branch, an object branch and an interaction branch.\nSemantic information in global-level and local-level are efficiently reasoned\nand aggregated in each of the branches. We have compared our proposed GID-Net\nwith existing state-of-the-art methods on two public benchmarks, including\nV-COCO and HICO-DET. The results have showed that GID-Net outperforms the\nexisting best-performing methods on both the above two benchmarks, validating\nits efficacy in detecting human-object interactions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:58:43 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Yang", "Dongming", ""], ["Zou", "YueXian", ""], ["Zhang", "Jian", ""], ["Li", "Ge", ""]]}, {"id": "2003.05257", "submitter": "Dan Levi", "authors": "Netalee Efrat, Max Bluvstein, Noa Garnett, Dan Levi, Shaul Oron, Bat\n  El Shlomo", "title": "Semi-Local 3D Lane Detection and Uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel camera-based DNN method for 3D lane detection with\nuncertainty estimation. Our method is based on a semi-local, BEV, tile\nrepresentation that breaks down lanes into simple lane segments. It combines\nlearning a parametric model for the segments along with a deep feature\nembedding that is then used to cluster segment together into full lanes. This\ncombination allows our method to generalize to complex lane topologies,\ncurvatures and surface geometries. Additionally, our method is the first to\noutput a learning based uncertainty estimation for the lane detection task. The\nefficacy of our method is demonstrated in extensive experiments achieving\nstate-of-the-art results for camera-based 3D lane detection, while also showing\nour ability to generalize to complex topologies, curvatures and road geometries\nas well as to different cameras. We also demonstrate how our uncertainty\nestimation aligns with the empirical error statistics indicating that it is\nwell calibrated and truly reflects the detection noise.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 12:35:24 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Efrat", "Netalee", ""], ["Bluvstein", "Max", ""], ["Garnett", "Noa", ""], ["Levi", "Dan", ""], ["Oron", "Shaul", ""], ["Shlomo", "Bat El", ""]]}, {"id": "2003.05326", "submitter": "Yiming Li", "authors": "Fan Li, Changhong Fu, Fuling Lin, Yiming Li, Peng Lu", "title": "Training-Set Distillation for Real-Time UAV Object Tracking", "comments": "2020 IEEE International Conference on Robotics and Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) has recently exhibited promising performance in\nvisual object tracking for unmanned aerial vehicle (UAV). Such online learning\nmethod heavily depends on the quality of the training-set, yet complicated\naerial scenarios like occlusion or out of view can reduce its reliability. In\nthis work, a novel time slot-based distillation approach is proposed to\nefficiently and effectively optimize the training-set's quality on the fly. A\ncooperative energy minimization function is established to score the historical\nsamples adaptively. To accelerate the scoring process, frames with high\nconfident tracking results are employed as the keyframes to divide the tracking\nprocess into multiple time slots. After the establishment of a new slot, the\nweighted fusion of the previous samples generates one key-sample, in order to\nreduce the number of samples to be scored. Besides, when the current time slot\nexceeds the maximum frame number, which can be scored, the sample with the\nlowest score will be discarded. Consequently, the training-set can be\nefficiently and reliably distilled. Comprehensive tests on two well-known UAV\nbenchmarks prove the effectiveness of our method with real-time speed on a\nsingle CPU.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 14:28:09 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Li", "Fan", ""], ["Fu", "Changhong", ""], ["Lin", "Fuling", ""], ["Li", "Yiming", ""], ["Lu", "Peng", ""]]}, {"id": "2003.05328", "submitter": "Song Bian", "authors": "Song Bian, Tianchen Wang, Masayuki Hiromoto, Yiyu Shi, Takashi Sato", "title": "ENSEI: Efficient Secure Inference via Frequency-Domain Homomorphic\n  Convolution for Privacy-Preserving Visual Recognition", "comments": "10 pages, 3 figures, in Proceedings of Conference on Computer Vision\n  and Pattern Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose ENSEI, a secure inference (SI) framework based on\nthe frequency-domain secure convolution (FDSC) protocol for the efficient\nexecution of privacy-preserving visual recognition. Our observation is that,\nunder the combination of homomorphic encryption and secret sharing, homomorphic\nconvolution can be obliviously carried out in the frequency domain,\nsignificantly simplifying the related computations. We provide protocol designs\nand parameter derivations for number-theoretic transform (NTT) based FDSC. In\nthe experiment, we thoroughly study the accuracy-efficiency trade-offs between\ntime- and frequency-domain homomorphic convolution. With ENSEI, compared to the\nbest known works, we achieve 5--11x online time reduction, up to 33x setup time\nreduction, and up to 10x reduction in the overall inference time. A further 33%\nof bandwidth reductions can be obtained on binary neural networks with only 1%\nof accuracy degradation on the CIFAR-10 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 14:35:48 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Bian", "Song", ""], ["Wang", "Tianchen", ""], ["Hiromoto", "Masayuki", ""], ["Shi", "Yiyu", ""], ["Sato", "Takashi", ""]]}, {"id": "2003.05379", "submitter": "Anjaneya Teja Sarma Kalvakolanu", "authors": "Anjaneya Teja Sarma Kalvakolanu", "title": "Plant Disease Detection from Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant disease detection is a huge problem and often require professional help\nto detect the disease. This research focuses on creating a deep learning model\nthat detects the type of disease that affected the plant from the images of the\nleaves of the plants. The deep learning is done with the help of Convolutional\nNeural Network by performing transfer learning. The model is created using\ntransfer learning and is experimented with both resnet 34 and resnet 50 to\ndemonstrate that discriminative learning gives better results. This method\nachieved state of art results for the dataset used. The main goal is to lower\nthe professional help to detect the plant diseases and make this model\naccessible to as many people as possible.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 02:17:36 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kalvakolanu", "Anjaneya Teja Sarma", ""]]}, {"id": "2003.05383", "submitter": "Yu-Sheng Lin", "authors": "Yu-Sheng Lin, Zhe-Yu Liu, Yu-An Chen, Yu-Siang Wang, Ya-Liang Chang,\n  and Winston H. Hsu", "title": "xCos: An Explainable Cosine Metric for Face Verification Task", "comments": "ACM Transactions on Multimedia Computing Communications and\n  Applications (TOMM). 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the XAI (explainable AI) on the face recognition task, particularly\nthe face verification here. Face verification is a crucial task in recent days\nand it has been deployed to plenty of applications, such as access control,\nsurveillance, and automatic personal log-on for mobile devices. With the\nincreasing amount of data, deep convolutional neural networks can achieve very\nhigh accuracy for the face verification task. Beyond exceptional performances,\ndeep face verification models need more interpretability so that we can trust\nthe results they generate. In this paper, we propose a novel similarity metric,\ncalled explainable cosine ($xCos$), that comes with a learnable module that can\nbe plugged into most of the verification models to provide meaningful\nexplanations. With the help of $xCos$, we can see which parts of the two input\nfaces are similar, where the model pays its attention to, and how the local\nsimilarities are weighted to form the output $xCos$ score. We demonstrate the\neffectiveness of our proposed method on LFW and various competitive benchmarks,\nresulting in not only providing novel and desiring model interpretability for\nface verification but also ensuring the accuracy as plugging into existing face\nrecognition models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 16:03:44 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 14:24:32 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 13:38:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lin", "Yu-Sheng", ""], ["Liu", "Zhe-Yu", ""], ["Chen", "Yu-An", ""], ["Wang", "Yu-Siang", ""], ["Chang", "Ya-Liang", ""], ["Hsu", "Winston H.", ""]]}, {"id": "2003.05408", "submitter": "Bijju Veduruparthi Mr", "authors": "Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das,\n  Mandira Saha, Sanjoy Chatterjee, Raj Kumar Shrimali, Soumendranath Ray and\n  Sriram Prasath", "title": "Early Response Assessment in Lung Cancer Patients using Spatio-temporal\n  CBCT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We report a model to predict patient's radiological response to curative\nradiation therapy (RT) for non-small-cell lung cancer (NSCLC).\n  Cone-Beam Computed Tomography images acquired weekly during the six-week\ncourse of RT were contoured with the Gross Tumor Volume (GTV) by senior\nradiation oncologists for 53 patients (7 images per patient).\n  Deformable registration of the images yielded six deformation fields for each\npair of consecutive images per patient.\n  Jacobian of a field provides a measure of local expansion/contraction and is\nused in our model.\n  Delineations were compared post-registration to compute unchanged ($U$),\nnewly grown ($G$), and reduced ($R$) regions within GTV.\n  The mean Jacobian of these regions $\\mu_U$, $\\mu_G$ and $\\mu_R$ are\nstatistically compared and a response assessment model is proposed.\n  A good response is hypothesized if $\\mu_R < 1.0$, $\\mu_R < \\mu_U$, and $\\mu_G\n< \\mu_U$.\n  For early prediction of post-treatment response, first, three weeks' images\nare used.\n  Our model predicted clinical response with a precision of $74\\%$.\n  Using reduction in CT numbers (CTN) and percentage GTV reduction as features\nin logistic regression, yielded an area-under-curve of 0.65 with p=0.005.\n  Combining logistic regression model with the proposed hypothesis yielded an\nodds ratio of 20.0 (p=0.0).\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 08:20:22 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Veduruparthi", "Bijju Kranthi", ""], ["Mukherjee", "Jayanta", ""], ["Das", "Partha Pratim", ""], ["Saha", "Mandira", ""], ["Chatterjee", "Sanjoy", ""], ["Shrimali", "Raj Kumar", ""], ["Ray", "Soumendranath", ""], ["Prasath", "Sriram", ""]]}, {"id": "2003.05410", "submitter": "Pradeep Kumar Jayaraman", "authors": "Aditya Sanghi, Pradeep Kumar Jayaraman", "title": "How Powerful Are Randomly Initialized Pointcloud Set Functions?", "comments": "6 pages, 2 figures, NeurIPS 2019 Sets & Partitions Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study random embeddings produced by untrained neural set functions, and\nshow that they are powerful representations which well capture the input\nfeatures for downstream tasks such as classification, and are often linearly\nseparable. We obtain surprising results that show that random set functions can\noften obtain close to or even better accuracy than fully trained models. We\ninvestigate factors that affect the representative power of such embeddings\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 16:54:43 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Sanghi", "Aditya", ""], ["Jayaraman", "Pradeep Kumar", ""]]}, {"id": "2003.05420", "submitter": "Peng Jiang Dr.", "authors": "Guangnan Wu and Zhiyi Pan and Peng Jiang and Changhe Tu", "title": "Bi-Directional Attention for Joint Instance and Semantic Segmentation in\n  Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Instance segmentation in point clouds is one of the most fine-grained ways to\nunderstand the 3D scene. Due to its close relationship to semantic\nsegmentation, many works approach these two tasks simultaneously and leverage\nthe benefits of multi-task learning. However, most of them only considered\nsimple strategies such as element-wise feature fusion, which may not lead to\nmutual promotion. In this work, we build a Bi-Directional Attention module on\nbackbone neural networks for 3D point cloud perception, which uses similarity\nmatrix measured from features for one task to help aggregate non-local\ninformation for the other task, avoiding the potential feature exclusion and\ntask conflict. From comprehensive experiments and ablation studies on the S3DIS\ndataset and the PartNet dataset, the superiority of our method is verified.\nMoreover, the mechanism of how bi-directional attention module helps joint\ninstance and semantic segmentation is also analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 17:16:07 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Wu", "Guangnan", ""], ["Pan", "Zhiyi", ""], ["Jiang", "Peng", ""], ["Tu", "Changhe", ""]]}, {"id": "2003.05425", "submitter": "Pim de Haan", "authors": "Pim de Haan, Maurice Weiler, Taco Cohen and Max Welling", "title": "Gauge Equivariant Mesh CNNs: Anisotropic convolutions on geometric\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A common approach to define convolutions on meshes is to interpret them as a\ngraph and apply graph convolutional networks (GCNs). Such GCNs utilize\nisotropic kernels and are therefore insensitive to the relative orientation of\nvertices and thus to the geometry of the mesh as a whole. We propose Gauge\nEquivariant Mesh CNNs which generalize GCNs to apply anisotropic gauge\nequivariant kernels. Since the resulting features carry orientation\ninformation, we introduce a geometric message passing scheme defined by\nparallel transporting features over mesh edges. Our experiments validate the\nsignificantly improved expressivity of the proposed model over conventional\nGCNs and other methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 17:21:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["de Haan", "Pim", ""], ["Weiler", "Maurice", ""], ["Cohen", "Taco", ""], ["Welling", "Max", ""]]}, {"id": "2003.05436", "submitter": "Wilson Yan", "authors": "Wilson Yan, Ashwin Vangipuram, Pieter Abbeel, Lerrel Pinto", "title": "Learning Predictive Representations for Deformable Objects Using\n  Contrastive Estimation", "comments": "Project website:\n  https://sites.google.com/view/contrastive-predictive-model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using visual model-based learning for deformable object manipulation is\nchallenging due to difficulties in learning plannable visual representations\nalong with complex dynamic models. In this work, we propose a new learning\nframework that jointly optimizes both the visual representation model and the\ndynamics model using contrastive estimation. Using simulation data collected by\nrandomly perturbing deformable objects on a table, we learn latent dynamics\nmodels for these objects in an offline fashion. Then, using the learned models,\nwe use simple model-based planning to solve challenging deformable object\nmanipulation tasks such as spreading ropes and cloths. Experimentally, we show\nsubstantial improvements in performance over standard model-based learning\ntechniques across our rope and cloth manipulation suite. Finally, we transfer\nour visual manipulation policies trained on data purely collected in simulation\nto a real PR2 robot through domain randomization.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 17:55:15 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Yan", "Wilson", ""], ["Vangipuram", "Ashwin", ""], ["Abbeel", "Pieter", ""], ["Pinto", "Lerrel", ""]]}, {"id": "2003.05438", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Zhuang Liu and Marios Savvides and\n  Trevor Darrell and Eric Xing", "title": "Un-Mix: Rethinking Image Mixtures for Unsupervised Visual Representation\n  Learning", "comments": "12 pages. Code is available at: https://github.com/szq0214/Un-Mix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning, smoothing label or prediction distribution in neural\nnetwork training has been proven useful in preventing the model from being\nover-confident, and is crucial for learning more robust visual representations.\nThis observation motivates us to explore ways to make predictions flattened in\nunsupervised learning. Considering that human-annotated labels are not adopted\nin unsupervised learning, we introduce a straightforward approach to perturb\ninput image space in order to soften the output prediction space indirectly,\nmeanwhile, assigning new label values in the unsupervised frameworks\naccordingly. Despite its conceptual simplicity, we show empirically that with\nthe simple solution -- Unsupervised image mixtures (Un-Mix), we can learn more\nrobust visual representations from the transformed input. Extensive experiments\nare conducted on CIFAR-10, CIFAR-100, STL-10, Tiny ImageNet and standard\nImageNet with popular unsupervised methods SimCLR, BYOL, MoCo V1&V2, etc. Our\nproposed image mixture and label assignment strategy can obtain consistent\nimprovement by 1~3% following exactly the same hyperparameters and training\nprocedures of the base methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 17:59:04 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 17:44:46 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 18:32:44 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Liu", "Zhuang", ""], ["Savvides", "Marios", ""], ["Darrell", "Trevor", ""], ["Xing", "Eric", ""]]}, {"id": "2003.05471", "submitter": "Vage Egiazarian", "authors": "Vage Egiazarian, Oleg Voynov, Alexey Artemov, Denis Volkhonskiy,\n  Aleksandr Safin, Maria Taktasheva, Denis Zorin, Evgeny Burnaev", "title": "Deep Vectorization of Technical Drawings", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58601-0_35", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for vectorization of technical line drawings, such as\nfloor plans, architectural drawings, and 2D CAD images. Our method includes (1)\na deep learning-based cleaning stage to eliminate the background and\nimperfections in the image and fill in missing parts, (2) a transformer-based\nnetwork to estimate vector primitives, and (3) optimization procedure to obtain\nthe final primitive configurations. We train the networks on synthetic data,\nrenderings of vector line drawings, and manually vectorized scans of line\ndrawings. Our method quantitatively and qualitatively outperforms a number of\nexisting techniques on a collection of representative technical drawings.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:19:00 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 20:54:55 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 14:32:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Egiazarian", "Vage", ""], ["Voynov", "Oleg", ""], ["Artemov", "Alexey", ""], ["Volkhonskiy", "Denis", ""], ["Safin", "Aleksandr", ""], ["Taktasheva", "Maria", ""], ["Zorin", "Denis", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2003.05477", "submitter": "Richard Droste", "authors": "Richard Droste, Jianbo Jiao, J. Alison Noble", "title": "Unified Image and Video Saliency Modeling", "comments": "Presented at the European Conference on Computer Vision (ECCV) 2020.\n  R. Droste and J. Jiao contributed equally to this work. v3: Updated Fig. 5a)\n  and added new MTI300 benchmark results to supp. material", "journal-ref": "In: ECCV 2020, Springer LNCS 12350, pp. 419-435", "doi": "10.1007/978-3-030-58558-7_25", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual saliency modeling for images and videos is treated as two independent\ntasks in recent computer vision literature. While image saliency modeling is a\nwell-studied problem and progress on benchmarks like SALICON and MIT300 is\nslowing, video saliency models have shown rapid gains on the recent DHF1K\nbenchmark. Here, we take a step back and ask: Can image and video saliency\nmodeling be approached via a unified model, with mutual benefit? We identify\ndifferent sources of domain shift between image and video saliency data and\nbetween different video saliency datasets as a key challenge for effective\njoint modelling. To address this we propose four novel domain adaptation\ntechniques - Domain-Adaptive Priors, Domain-Adaptive Fusion, Domain-Adaptive\nSmoothing and Bypass-RNN - in addition to an improved formulation of learned\nGaussian priors. We integrate these techniques into a simple and lightweight\nencoder-RNN-decoder-style network, UNISAL, and train it jointly with image and\nvideo saliency data. We evaluate our method on the video saliency datasets\nDHF1K, Hollywood-2 and UCF-Sports, and the image saliency datasets SALICON and\nMIT300. With one set of parameters, UNISAL achieves state-of-the-art\nperformance on all video saliency datasets and is on par with the\nstate-of-the-art for image saliency datasets, despite faster runtime and a 5 to\n20-fold smaller model size compared to all competing deep methods. We provide\nretrospective analyses and ablation studies which confirm the importance of the\ndomain shift modeling. The code is available at\nhttps://github.com/rdroste/unisal\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:28:29 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 00:48:35 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 13:43:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Droste", "Richard", ""], ["Jiao", "Jianbo", ""], ["Noble", "J. Alison", ""]]}, {"id": "2003.05505", "submitter": "Chengyao Li", "authors": "Chengyao Li, Jason Ku and Steven L. Waslander", "title": "Confidence Guided Stereo 3D Object Detection with Split Depth Estimation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and reliable 3D object detection is vital to safe autonomous\ndriving. Despite recent developments, the performance gap between stereo-based\nmethods and LiDAR-based methods is still considerable. Accurate depth\nestimation is crucial to the performance of stereo-based 3D object detection\nmethods, particularly for those pixels associated with objects in the\nforeground. Moreover, stereo-based methods suffer from high variance in the\ndepth estimation accuracy, which is often not considered in the object\ndetection pipeline. To tackle these two issues, we propose CG-Stereo, a\nconfidence-guided stereo 3D object detection pipeline that uses separate\ndecoders for foreground and background pixels during depth estimation, and\nleverages the confidence estimation from the depth estimation network as a soft\nattention mechanism in the 3D object detector. Our approach outperforms all\nstate-of-the-art stereo-based 3D detectors on the KITTI benchmark.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 20:00:11 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Li", "Chengyao", ""], ["Ku", "Jason", ""], ["Waslander", "Steven L.", ""]]}, {"id": "2003.05523", "submitter": "Bharat Joshi", "authors": "Bharat Joshi, Md Modasshir, Travis Manderson, Hunter Damron, Marios\n  Xanthidis, Alberto Quattrini Li, Ioannis Rekleitis, Gregory Dudek", "title": "DeepURL: Deep Pose Estimation Framework for Underwater Relative\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a real-time deep learning approach for determining\nthe 6D relative pose of Autonomous Underwater Vehicles (AUV) from a single\nimage. A team of autonomous robots localizing themselves in a\ncommunication-constrained underwater environment is essential for many\napplications such as underwater exploration, mapping, multi-robot convoying,\nand other multi-robot tasks. Due to the profound difficulty of collecting\nground truth images with accurate 6D poses underwater, this work utilizes\nrendered images from the Unreal Game Engine simulation for training. An\nimage-to-image translation network is employed to bridge the gap between the\nrendered and the real images producing synthetic images for training. The\nproposed method predicts the 6D pose of an AUV from a single image as 2D image\nkeypoints representing 8 corners of the 3D model of the AUV, and then the 6D\npose in the camera coordinates is determined using RANSAC-based PnP.\nExperimental results in real-world underwater environments (swimming pool and\nocean) with different cameras demonstrate the robustness and accuracy of the\nproposed technique in terms of translation error and orientation error over the\nstate-of-the-art methods. The code is publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 21:11:05 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 00:58:03 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 16:22:03 GMT"}, {"version": "v4", "created": "Thu, 21 Jan 2021 18:10:20 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Joshi", "Bharat", ""], ["Modasshir", "Md", ""], ["Manderson", "Travis", ""], ["Damron", "Hunter", ""], ["Xanthidis", "Marios", ""], ["Li", "Alberto Quattrini", ""], ["Rekleitis", "Ioannis", ""], ["Dudek", "Gregory", ""]]}, {"id": "2003.05534", "submitter": "Simon Niklaus", "authors": "Simon Niklaus, Feng Liu", "title": "Softmax Splatting for Video Frame Interpolation", "comments": "CVPR 2020, http://sniklaus.com/softsplat", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable image sampling in the form of backward warping has seen broad\nadoption in tasks like depth estimation and optical flow prediction. In\ncontrast, how to perform forward warping has seen less attention, partly due to\nadditional challenges such as resolving the conflict of mapping multiple pixels\nto the same target location in a differentiable way. We propose softmax\nsplatting to address this paradigm shift and show its effectiveness on the\napplication of frame interpolation. Specifically, given two input frames, we\nforward-warp the frames and their feature pyramid representations based on an\noptical flow estimate using softmax splatting. In doing so, the softmax\nsplatting seamlessly handles cases where multiple source pixels map to the same\ntarget location. We then use a synthesis network to predict the interpolation\nresult from the warped representations. Our softmax splatting allows us to not\nonly interpolate frames at an arbitrary time but also to fine tune the feature\npyramid and the optical flow. We show that our synthesis approach, empowered by\nsoftmax splatting, achieves new state-of-the-art results for video frame\ninterpolation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 21:38:56 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Niklaus", "Simon", ""], ["Liu", "Feng", ""]]}, {"id": "2003.05541", "submitter": "A S M Iftekhar", "authors": "Oytun Ulutan, A S M Iftekhar, B.S. Manjunath", "title": "VSGNet: Spatial Attention Network for Detecting Human Object\n  Interactions Using Graph Convolutions", "comments": "Accepted in IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensive visual understanding requires detection frameworks that can\neffectively learn and utilize object interactions while analyzing objects\nindividually. This is the main objective in Human-Object Interaction (HOI)\ndetection task. In particular, relative spatial reasoning and structural\nconnections between objects are essential cues for analyzing interactions,\nwhich is addressed by the proposed Visual-Spatial-Graph Network (VSGNet)\narchitecture. VSGNet extracts visual features from the human-object pairs,\nrefines the features with spatial configurations of the pair, and utilizes the\nstructural connections between the pair via graph convolutions. The performance\nof VSGNet is thoroughly evaluated using the Verbs in COCO (V-COCO) and HICO-DET\ndatasets. Experimental results indicate that VSGNet outperforms\nstate-of-the-art solutions by 8% or 4 mAP in V-COCO and 16% or 3 mAP in\nHICO-DET.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 22:23:51 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Ulutan", "Oytun", ""], ["Iftekhar", "A S M", ""], ["Manjunath", "B. S.", ""]]}, {"id": "2003.05549", "submitter": "Yingpeng Deng", "authors": "Yingpeng Deng and Lina J. Karam", "title": "Frequency-Tuned Universal Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have shown that the predictions of a convolutional neural network\n(CNN) for an image set can be severely distorted by one single image-agnostic\nperturbation, or universal perturbation, usually with an empirically fixed\nthreshold in the spatial domain to restrict its perceivability. However, by\nconsidering the human perception, we propose to adopt JND thresholds to guide\nthe perceivability of universal adversarial perturbations. Based on this, we\npropose a frequency-tuned universal attack method to compute universal\nperturbations and show that our method can realize a good balance between\nperceivability and effectiveness in terms of fooling rate by adapting the\nperturbations to the local frequency content. Compared with existing universal\nadversarial attack techniques, our frequency-tuned attack method can achieve\ncutting-edge quantitative results. We demonstrate that our approach can\nsignificantly improve the performance of the baseline on both white-box and\nblack-box attacks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 22:52:19 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 18:37:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Deng", "Yingpeng", ""], ["Karam", "Lina J.", ""]]}, {"id": "2003.05551", "submitter": "Michael Kellman", "authors": "Michael Kellman, Kevin Zhang, Jon Tamir, Emrah Bostan, Michael Lustig,\n  Laura Waller", "title": "Memory-efficient Learning for Large-scale Computational Imaging", "comments": "9 pages, 8 figures. See also relate NeurIPS 2019 presentation\n  arXiv:1912.05098", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical aspects of computational imaging systems, such as experimental\ndesign and image priors, can be optimized through deep networks formed by the\nunrolled iterations of classical model-based reconstructions (termed\nphysics-based networks). However, for real-world large-scale inverse problems,\ncomputing gradients via backpropagation is infeasible due to memory limitations\nof graphics processing units. In this work, we propose a memory-efficient\nlearning procedure that exploits the reversibility of the network's layers to\nenable data-driven design for large-scale computational imaging systems. We\ndemonstrate our method on a small-scale compressed sensing example, as well as\ntwo large-scale real-world systems: multi-channel magnetic resonance imaging\nand super-resolution optical microscopy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 23:08:04 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Kellman", "Michael", ""], ["Zhang", "Kevin", ""], ["Tamir", "Jon", ""], ["Bostan", "Emrah", ""], ["Lustig", "Michael", ""], ["Waller", "Laura", ""]]}, {"id": "2003.05559", "submitter": "Zhizhong Han", "authors": "Zhizhong Han, Guanhui Qiao, Yu-Shen Liu, and Matthias Zwicker", "title": "SeqXY2SeqZ: Structure Learning for 3D Shapes by Sequentially Predicting\n  1D Occupancy Segments From 2D Coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning for 3D shapes is vital for 3D computer vision.\nState-of-the-art methods show promising results by representing shapes using\nimplicit functions in 3D that are learned using discriminative neural networks.\nHowever, learning implicit functions requires dense and irregular sampling in\n3D space, which also makes the sampling methods affect the accuracy of shape\nreconstruction during test. To avoid dense and irregular sampling in 3D, we\npropose to represent shapes using 2D functions, where the output of the\nfunction at each 2D location is a sequence of line segments inside the shape.\nOur approach leverages the power of functional representations, but without the\ndisadvantage of 3D sampling. Specifically, we use a voxel tubelization to\nrepresent a voxel grid as a set of tubes along any one of the X, Y, or Z axes.\nEach tube can be indexed by its 2D coordinates on the plane spanned by the\nother two axes. We further simplify each tube into a sequence of occupancy\nsegments. Each occupancy segment consists of successive voxels occupied by the\nshape, which leads to a simple representation of its 1D start and end location.\nGiven the 2D coordinates of the tube and a shape feature as condition, this\nrepresentation enables us to learn 3D shape structures by sequentially\npredicting the start and end locations of each occupancy segment in the tube.\nWe implement this approach using a Seq2Seq model with attention, called\nSeqXY2SeqZ, which learns the mapping from a sequence of 2D coordinates along\ntwo arbitrary axes to a sequence of 1D locations along the third axis.\nSeqXY2SeqZ not only benefits from the regularity of voxel grids in training and\ntesting, but also achieves high memory efficiency. Our experiments show that\nSeqXY2SeqZ outperforms the state-ofthe-art methods under widely used\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 00:24:36 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 15:06:39 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Han", "Zhizhong", ""], ["Qiao", "Guanhui", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2003.05569", "submitter": "Luo Chunjie", "authors": "Chunjie Luo, Jianfeng Zhan, Lei Wang, Wanling Gao", "title": "Extended Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) has become a standard technique for training the\nmodern deep networks. However, its effectiveness diminishes when the batch size\nbecomes smaller, since the batch statistics estimation becomes inaccurate. That\nhinders batch normalization's usage for 1) training larger model which requires\nsmall batches constrained by memory consumption, 2) training on mobile or\nembedded devices of which the memory resource is limited. In this paper, we\npropose a simple but effective method, called extended batch normalization\n(EBN). For NCHW format feature maps, extended batch normalization computes the\nmean along the (N, H, W) dimensions, as the same as batch normalization, to\nmaintain the advantage of batch normalization. To alleviate the problem caused\nby small batch size, extended batch normalization computes the standard\ndeviation along the (N, C, H, W) dimensions, thus enlarges the number of\nsamples from which the standard deviation is computed. We compare extended\nbatch normalization with batch normalization and group normalization on the\ndatasets of MNIST, CIFAR-10/100, STL-10, and ImageNet, respectively. The\nexperiments show that extended batch normalization alleviates the problem of\nbatch normalization with small batch size while achieving close performances to\nbatch normalization with large batch size.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 01:53:15 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Gao", "Wanling", ""]]}, {"id": "2003.05583", "submitter": "Zhang Lingling", "authors": "Lingling Zhang, Xiaojun Chang, Jun Liu, Minnan Luo, Sen Wang, Zongyuan\n  Ge, Alexander Hauptmann", "title": "ZSTAD: Zero-Shot Temporal Activity Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integral part of video analysis and surveillance is temporal activity\ndetection, which means to simultaneously recognize and localize activities in\nlong untrimmed videos. Currently, the most effective methods of temporal\nactivity detection are based on deep learning, and they typically perform very\nwell with large scale annotated videos for training. However, these methods are\nlimited in real applications due to the unavailable videos about certain\nactivity classes and the time-consuming data annotation. To solve this\nchallenging problem, we propose a novel task setting called zero-shot temporal\nactivity detection (ZSTAD), where activities that have never been seen in\ntraining can still be detected. We design an end-to-end deep network based on\nR-C3D as the architecture for this solution. The proposed network is optimized\nwith an innovative loss function that considers the embeddings of activity\nlabels and their super-classes while learning the common semantics of seen and\nunseen activities. Experiments on both the THUMOS14 and the Charades datasets\nshow promising performance in terms of detecting unseen activities.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 02:40:36 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zhang", "Lingling", ""], ["Chang", "Xiaojun", ""], ["Liu", "Jun", ""], ["Luo", "Minnan", ""], ["Wang", "Sen", ""], ["Ge", "Zongyuan", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "2003.05586", "submitter": "Pongpisit Thanasutives", "authors": "Pongpisit Thanasutives, Ken-ichi Fukui, Masayuki Numao, Boonserm\n  Kijsirikul", "title": "Encoder-Decoder Based Convolutional Neural Networks with\n  Multi-Scale-Aware Modules for Crowd Counting", "comments": "Accepted at ICPR 2020", "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9413286", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two modified neural networks based on dual path\nmulti-scale fusion networks (SFANet) and SegNet for accurate and efficient\ncrowd counting. Inspired by SFANet, the first model, which is named M-SFANet,\nis attached with atrous spatial pyramid pooling (ASPP) and context-aware module\n(CAN). The encoder of M-SFANet is enhanced with ASPP containing parallel atrous\nconvolutional layers with different sampling rates and hence able to extract\nmulti-scale features of the target object and incorporate larger context. To\nfurther deal with scale variation throughout an input image, we leverage the\nCAN module which adaptively encodes the scales of the contextual information.\nThe combination yields an effective model for counting in both dense and sparse\ncrowd scenes. Based on the SFANet decoder structure, M-SFANet's decoder has\ndual paths, for density map and attention map generation. The second model is\ncalled M-SegNet, which is produced by replacing the bilinear upsampling in\nSFANet with max unpooling that is used in SegNet. This change provides a faster\nmodel while providing competitive counting performance. Designed for high-speed\nsurveillance applications, M-SegNet has no additional multi-scale-aware module\nin order to not increase the complexity. Both models are encoder-decoder based\narchitectures and are end-to-end trainable. We conduct extensive experiments on\nfive crowd counting datasets and one vehicle counting dataset to show that\nthese modifications yield algorithms that could improve state-of-the-art crowd\ncounting methods. Codes are available at\nhttps://github.com/Pongpisit-Thanasutives/Variations-of-SFANet-for-Crowd-Counting.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 03:00:26 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 15:05:18 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 11:22:19 GMT"}, {"version": "v4", "created": "Mon, 13 Apr 2020 15:18:44 GMT"}, {"version": "v5", "created": "Wed, 25 Nov 2020 12:35:21 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Thanasutives", "Pongpisit", ""], ["Fukui", "Ken-ichi", ""], ["Numao", "Masayuki", ""], ["Kijsirikul", "Boonserm", ""]]}, {"id": "2003.05593", "submitter": "Ziming Zhang", "authors": "Yecheng Lyu and Xinming Huang and Ziming Zhang", "title": "Learning to Segment 3D Point Clouds in 2D Image Space", "comments": "Accepted to CVPR 2020 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to the literature where local patterns in 3D point clouds are\ncaptured by customized convolutional operators, in this paper we study the\nproblem of how to effectively and efficiently project such point clouds into a\n2D image space so that traditional 2D convolutional neural networks (CNNs) such\nas U-Net can be applied for segmentation. To this end, we are motivated by\ngraph drawing and reformulate it as an integer programming problem to learn the\ntopology-preserving graph-to-grid mapping for each individual point cloud. To\naccelerate the computation in practice, we further propose a novel hierarchical\napproximate algorithm. With the help of the Delaunay triangulation for graph\nconstruction from point clouds and a multi-scale U-Net for segmentation, we\nmanage to demonstrate the state-of-the-art performance on ShapeNet and PartNet,\nrespectively, with significant improvement over the literature. Code is\navailable at https://github.com/Zhang-VISLab.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 03:18:59 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2020 16:43:17 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 17:57:13 GMT"}, {"version": "v4", "created": "Wed, 7 Oct 2020 23:27:31 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lyu", "Yecheng", ""], ["Huang", "Xinming", ""], ["Zhang", "Ziming", ""]]}, {"id": "2003.05597", "submitter": "Xue Yang", "authors": "Xue Yang, Junchi Yan and Tao He", "title": "On the Arbitrary-Oriented Object Detection: Classification based\n  Approaches Revisited", "comments": "16 pages, 14 figures, 10 tables, journal version of CSL (ECCV2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary-oriented object detection has been a building block for rotation\nsensitive tasks. We first show that the problem of discontinuous boundaries\nsuffered in existing dominant regression-based rotation detectors, is caused by\nangular periodicity or corner ordering, according to the parameterization\nprotocol. We also show that the root cause is that the ideal predictions can be\nout of the defined range. Accordingly, we transform the angular prediction task\nfrom a regression problem to a classification one. For the resulting circularly\ndistributed angle classification problem, we first devise a Circular Smooth\nLabel (CSL) technique to handle the periodicity of angle and increase the error\ntolerance to adjacent angles. To reduce the excessive model parameters by CSL,\nwe further design a Gray Coded Label (GCL), which greatly reduces the length of\nthe encoding. Finally, we further develop an object heading detection module,\nwhich can be useful when the exact heading orientation information is needed\ne.g. for ship and plane heading detection. We release our OHD-SJTU dataset and\nOHDet detector for heading detection. Results on three large-scale public\ndatasets for aerial images i.e. DOTA, HRSC2016, OHD-SJTU, as well as scene text\ndataset ICDAR2015 and MLT, show the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 03:23:54 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 12:59:20 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 06:25:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Xue", ""], ["Yan", "Junchi", ""], ["He", "Tao", ""]]}, {"id": "2003.05614", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Abhinav Gupta, Cordelia Schmid, Karteek Alahari", "title": "Beyond the Camera: Neural Networks in World Coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movement and strategic placement of the visual field onto the retina,\ngives animals increased resolution of the scene and suppresses distracting\ninformation. This fundamental system has been missing from video understanding\nwith deep networks, typically limited to 224 by 224 pixel content locked to the\ncamera frame. We propose a simple idea, WorldFeatures, where each feature at\nevery layer has a spatial transformation, and the feature map is only\ntransformed as needed. We show that a network built with these WorldFeatures,\ncan be used to model eye movements, such as saccades, fixation, and smooth\npursuit, even in a batch setting on pre-recorded video. That is, the network\ncan for example use all 224 by 224 pixels to look at a small detail one moment,\nand the whole scene the next. We show that typical building blocks, such as\nconvolutions and pooling, can be adapted to support WorldFeatures using\navailable tools. Experiments are presented on the Charades, Olympic Sports, and\nCaltech-UCSD Birds-200-2011 datasets, exploring action recognition,\nfine-grained recognition, and video stabilization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 04:29:34 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Gupta", "Abhinav", ""], ["Schmid", "Cordelia", ""], ["Alahari", "Karteek", ""]]}, {"id": "2003.05626", "submitter": "Shreetam Behera", "authors": "Shreetam Behera, Debi Prosad Dogra, Malay Kumar Bandyopadhyay, and\n  Partha Pratim Roy", "title": "Understanding Crowd Flow Movements Using Active-Langevin Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd flow describes the elementary group behavior of crowds. Understanding\nthe dynamics behind these movements can help to identify various abnormalities\nin crowds. However, developing a crowd model describing these flows is a\nchallenging task. In this paper, a physics-based model is proposed to describe\nthe movements in dense crowds. The crowd model is based on active Langevin\nequation where the motion points are assumed to be similar to active colloidal\nparticles in fluids. The model is further augmented with computer-vision\ntechniques to segment both linear and non-linear motion flows in a dense crowd.\nThe evaluation of the active Langevin equation-based crowd segmentation has\nbeen done on publicly available crowd videos and on our own videos. The\nproposed method is able to segment the flow with lesser optical flow error and\nbetter accuracy in comparison to existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 05:32:59 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 04:36:58 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 07:31:57 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Behera", "Shreetam", ""], ["Dogra", "Debi Prosad", ""], ["Bandyopadhyay", "Malay Kumar", ""], ["Roy", "Partha Pratim", ""]]}, {"id": "2003.05643", "submitter": "Shang-Hua Gao", "authors": "Shang-Hua Gao, Yong-Qiang Tan, Ming-Ming Cheng, Chengze Lu, Yunpeng\n  Chen, Shuicheng Yan", "title": "Highly Efficient Salient Object Detection with 100K Parameters", "comments": "Accepted by ECCV 2020. Source code: https://mmcheng.net/sod100k/", "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection models often demand a considerable amount of\ncomputation cost to make precise prediction for each pixel, making them hardly\napplicable on low-power devices. In this paper, we aim to relieve the\ncontradiction between computation cost and model performance by improving the\nnetwork efficiency to a higher degree. We propose a flexible convolutional\nmodule, namely generalized OctConv (gOctConv), to efficiently utilize both\nin-stage and cross-stages multi-scale features, while reducing the\nrepresentation redundancy by a novel dynamic weight decay scheme. The effective\ndynamic weight decay scheme stably boosts the sparsity of parameters during\ntraining, supports learnable number of channels for each scale in gOctConv,\nallowing 80% of parameters reduce with negligible performance drop. Utilizing\ngOctConv, we build an extremely light-weighted model, namely CSNet, which\nachieves comparable performance with about 0.2% parameters (100k) of large\nmodels on popular salient object detection benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 07:00:46 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 01:36:34 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gao", "Shang-Hua", ""], ["Tan", "Yong-Qiang", ""], ["Cheng", "Ming-Ming", ""], ["Lu", "Chengze", ""], ["Chen", "Yunpeng", ""], ["Yan", "Shuicheng", ""]]}, {"id": "2003.05653", "submitter": "Jiangke Lin", "authors": "Jiangke Lin, Yi Yuan, Tianjia Shao, Kun Zhou", "title": "Towards High-Fidelity 3D Face Reconstruction from In-the-Wild Images\n  Using Graph Convolutional Networks", "comments": "Accepted to CVPR 2020. The source code is available at\n  https://github.com/FuxiCV/3D-Face-GCNs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Morphable Model (3DMM) based methods have achieved great success in\nrecovering 3D face shapes from single-view images. However, the facial textures\nrecovered by such methods lack the fidelity as exhibited in the input images.\nRecent work demonstrates high-quality facial texture recovering with generative\nnetworks trained from a large-scale database of high-resolution UV maps of face\ntextures, which is hard to prepare and not publicly available. In this paper,\nwe introduce a method to reconstruct 3D facial shapes with high-fidelity\ntextures from single-view images in-the-wild, without the need to capture a\nlarge-scale face texture database. The main idea is to refine the initial\ntexture generated by a 3DMM based method with facial details from the input\nimage. To this end, we propose to use graph convolutional networks to\nreconstruct the detailed colors for the mesh vertices instead of reconstructing\nthe UV map. Experiments show that our method can generate high-quality results\nand outperforms state-of-the-art methods in both qualitative and quantitative\ncomparisons.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 08:06:04 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 03:28:55 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 08:41:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lin", "Jiangke", ""], ["Yuan", "Yi", ""], ["Shao", "Tianjia", ""], ["Zhou", "Kun", ""]]}, {"id": "2003.05654", "submitter": "Ratnesh Madaan", "authors": "Ratnesh Madaan, Nicholas Gyde, Sai Vemprala, Matthew Brown, Keiko\n  Nagami, Tim Taubner, Eric Cristofalo, Davide Scaramuzza, Mac Schwager, Ashish\n  Kapoor", "title": "AirSim Drone Racing Lab", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous drone racing is a challenging research problem at the intersection\nof computer vision, planning, state estimation, and control. We introduce\nAirSim Drone Racing Lab, a simulation framework for enabling fast prototyping\nof algorithms for autonomy and enabling machine learning research in this\ndomain, with the goal of reducing the time, money, and risks associated with\nfield robotics. Our framework enables generation of racing tracks in multiple\nphoto-realistic environments, orchestration of drone races, comes with a suite\nof gate assets, allows for multiple sensor modalities (monocular, depth,\nneuromorphic events, optical flow), different camera models, and benchmarking\nof planning, control, computer vision, and learning-based algorithms. We used\nour framework to host a simulation based drone racing competition at NeurIPS\n2019. The competition binaries are available at our github repository.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 08:06:06 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Madaan", "Ratnesh", ""], ["Gyde", "Nicholas", ""], ["Vemprala", "Sai", ""], ["Brown", "Matthew", ""], ["Nagami", "Keiko", ""], ["Taubner", "Tim", ""], ["Cristofalo", "Eric", ""], ["Scaramuzza", "Davide", ""], ["Schwager", "Mac", ""], ["Kapoor", "Ashish", ""]]}, {"id": "2003.05656", "submitter": "Han Wang", "authors": "Han Wang, Chen Wang and Lihua Xie", "title": "Intensity Scan Context: Coding Intensity and Geometry Relations for Loop\n  Closure Detection", "comments": "Accepted in International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": "2020 IEEE International Conference on Robotics and Automation\n  (ICRA)", "doi": "10.1109/ICRA40945.2020.9196764", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop closure detection is an essential and challenging problem in\nsimultaneous localization and mapping (SLAM). It is often tackled with light\ndetection and ranging (LiDAR) sensor due to its view-point and illumination\ninvariant properties. Existing works on 3D loop closure detection often\nleverage the matching of local or global geometrical-only descriptors, but\nwithout considering the intensity reading. In this paper we explore the\nintensity property from LiDAR scan and show that it can be effective for place\nrecognition. Concretely, we propose a novel global descriptor, intensity scan\ncontext (ISC), that explores both geometry and intensity characteristics. To\nimprove the efficiency for loop closure detection, an efficient two-stage\nhierarchical re-identification process is proposed, including a\nbinary-operation based fast geometric relation retrieval and an intensity\nstructure re-identification. Thorough experiments including both local\nexperiment and public datasets test have been conducted to evaluate the\nperformance of the proposed method. Our method achieves higher recall rate and\nrecall precision than existing geometric-only methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 08:11:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Han", ""], ["Wang", "Chen", ""], ["Xie", "Lihua", ""]]}, {"id": "2003.05660", "submitter": "Aliaksei Petsiuk", "authors": "Aliaksei L. Petsiuk, Joshua M. Pearce", "title": "Open Source Computer Vision-based Layer-wise 3D Printing Analysis", "comments": "29 pages, 19 figures", "journal-ref": null, "doi": "10.1016/j.addma.2020.101473", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper describes an open source computer vision-based hardware structure\nand software algorithm, which analyzes layer-wise the 3-D printing processes,\ntracks printing errors, and generates appropriate printer actions to improve\nreliability. This approach is built upon multiple-stage monocular image\nexamination, which allows monitoring both the external shape of the printed\nobject and internal structure of its layers. Starting with the side-view height\nvalidation, the developed program analyzes the virtual top view for outer shell\ncontour correspondence using the multi-template matching and iterative closest\npoint algorithms, as well as inner layer texture quality clustering the\nspatial-frequency filter responses with Gaussian mixture models and segmenting\nstructural anomalies with the agglomerative hierarchical clustering algorithm.\nThis allows evaluation of both global and local parameters of the printing\nmodes. The experimentally-verified analysis time per layer is less than one\nminute, which can be considered a quasi-real-time process for large prints. The\nsystems can work as an intelligent printing suspension tool designed to save\ntime and material. However, the results show the algorithm provides a means to\nsystematize in situ printing data as a first step in a fully open source\nfailure correction algorithm for additive manufacturing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 08:33:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Petsiuk", "Aliaksei L.", ""], ["Pearce", "Joshua M.", ""]]}, {"id": "2003.05664", "submitter": "Chunhua Shen", "authors": "Zhi Tian and Chunhua Shen and Hao Chen", "title": "Conditional Convolutions for Instance Segmentation", "comments": "Accepted to Proc. European Conf. Computer Vision (ECCV) 2020 as oral\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a simple yet effective instance segmentation framework, termed\nCondInst (conditional convolutions for instance segmentation). Top-performing\ninstance segmentation methods such as Mask R-CNN rely on ROI operations\n(typically ROIPool or ROIAlign) to obtain the final instance masks. In\ncontrast, we propose to solve instance segmentation from a new perspective.\nInstead of using instance-wise ROIs as inputs to a network of fixed weights, we\nemploy dynamic instance-aware networks, conditioned on instances. CondInst\nenjoys two advantages: 1) Instance segmentation is solved by a fully\nconvolutional network, eliminating the need for ROI cropping and feature\nalignment. 2) Due to the much improved capacity of dynamically-generated\nconditional convolutions, the mask head can be very compact (e.g., 3 conv.\nlayers, each having only 8 channels), leading to significantly faster\ninference. We demonstrate a simpler instance segmentation method that can\nachieve improved performance in both accuracy and inference speed. On the COCO\ndataset, we outperform a few recent methods including well-tuned Mask RCNN\nbaselines, without longer training schedules needed.\n  Code is available: https://github.com/aim-uofa/adet\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 08:42:36 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 00:50:35 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 13:38:31 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 02:18:32 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["Chen", "Hao", ""]]}, {"id": "2003.05669", "submitter": "Mohammadreza Salehi Dehnavi", "authors": "Mohammadreza Salehi, Atrin Arya, Barbod Pajoum, Mohammad Otoofi,\n  Amirreza Shaeiri, Mohammad Hossein Rohban, Hamid R. Rabiee", "title": "ARAE: Adversarially Robust Training of Autoencoders Improves Novelty\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders (AE) have recently been widely employed to approach the novelty\ndetection problem. Trained only on the normal data, the AE is expected to\nreconstruct the normal data effectively while fail to regenerate the anomalous\ndata, which could be utilized for novelty detection. However, in this paper, it\nis demonstrated that this does not always hold. AE often generalizes so\nperfectly that it can also reconstruct the anomalous data well. To address this\nproblem, we propose a novel AE that can learn more semantically meaningful\nfeatures. Specifically, we exploit the fact that adversarial robustness\npromotes learning of meaningful features. Therefore, we force the AE to learn\nsuch features by penalizing networks with a bottleneck layer that is unstable\nagainst adversarial perturbations. We show that despite using a much simpler\narchitecture in comparison to the prior methods, the proposed AE outperforms or\nis competitive to state-of-the-art on three benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 09:06:41 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 19:42:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Salehi", "Mohammadreza", ""], ["Arya", "Atrin", ""], ["Pajoum", "Barbod", ""], ["Otoofi", "Mohammad", ""], ["Shaeiri", "Amirreza", ""], ["Rohban", "Mohammad Hossein", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "2003.05684", "submitter": "Zhize Wu", "authors": "Zhize Wu, Thomas Weise, Le Zou, Fei Sun, Ming Tan", "title": "Skeleton Based Action Recognition using a Stacked Denoising Autoencoder\n  with Constraints of Privileged Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the availability of cost-effective depth cameras coupled with\nreal-time skeleton estimation, the interest in skeleton-based human action\nrecognition is renewed. Most of the existing skeletal representation approaches\nuse either the joint location or the dynamics model. Differing from the\nprevious studies, we propose a new method called Denoising Autoencoder with\nTemporal and Categorical Constraints (DAE_CTC)} to study the skeletal\nrepresentation in a view of skeleton reconstruction. Based on the concept of\nlearning under privileged information, we integrate action categories and\ntemporal coordinates into a stacked denoising autoencoder in the training\nphase, to preserve category and temporal feature, while learning the hidden\nrepresentation from a skeleton. Thus, we are able to improve the discriminative\nvalidity of the hidden representation. In order to mitigate the variation\nresulting from temporary misalignment, a new method of temporal registration,\ncalled Locally-Warped Sequence Registration (LWSR), is proposed for registering\nthe sequences of inter- and intra-class actions. We finally represent the\nsequences using a Fourier Temporal Pyramid (FTP) representation and perform\nclassification using a combination of LWSR registration, FTP representation,\nand a linear Support Vector Machine (SVM). The experimental results on three\naction data sets, namely MSR-Action3D, UTKinect-Action, and Florence3D-Action,\nshow that our proposal performs better than many existing methods and\ncomparably to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 09:56:22 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Wu", "Zhize", ""], ["Weise", "Thomas", ""], ["Zou", "Le", ""], ["Sun", "Fei", ""], ["Tan", "Ming", ""]]}, {"id": "2003.05698", "submitter": "Pawan Goyal", "authors": "Pawan Goyal, Hussam Al Daas, and Peter Benner", "title": "Low-Rank and Total Variation Regularization and Its Application to Image\n  Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of image recovery from given partial\n(corrupted) observations. Recovering an image using a low-rank model has been\nan active research area in data analysis and machine learning. But often,\nimages are not only of low-rank but they also exhibit sparsity in a transformed\nspace. In this work, we propose a new problem formulation in such a way that we\nseek to recover an image that is of low-rank and has sparsity in a transformed\ndomain. We further discuss various non-convex non-smooth surrogates of the rank\nfunction, leading to a relaxed problem. Then, we present an efficient iterative\nscheme to solve the relaxed problem that essentially employs the (weighted)\nsingular value thresholding at each iteration. Furthermore, we discuss the\nconvergence properties of the proposed iterative method. We perform extensive\nexperiments, showing that the proposed algorithm outperforms state-of-the-art\nmethodologies in recovering images.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:37:49 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Goyal", "Pawan", ""], ["Daas", "Hussam Al", ""], ["Benner", "Peter", ""]]}, {"id": "2003.05707", "submitter": "Mhd Hasan Sarhan", "authors": "Mhd Hasan Sarhan, Nassir Navab, Abouzar Eslami, Shadi Albarqouni", "title": "Fairness by Learning Orthogonal Disentangled Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative powerful representations is a crucial step for\nmachine learning systems. Introducing invariance against arbitrary nuisance or\nsensitive attributes while performing well on specific tasks is an important\nproblem in representation learning. This is mostly approached by purging the\nsensitive information from learned representations. In this paper, we propose a\nnovel disentanglement approach to invariant representation problem. We\ndisentangle the meaningful and sensitive representations by enforcing\northogonality constraints as a proxy for independence. We explicitly enforce\nthe meaningful representation to be agnostic to sensitive information by\nentropy maximization. The proposed approach is evaluated on five publicly\navailable datasets and compared with state of the art methods for learning\nfairness and invariance achieving the state of the art performance on three\ndatasets and comparable performance on the rest. Further, we perform an\nablative study to evaluate the effect of each component.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:09:15 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 14:56:46 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 09:04:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Sarhan", "Mhd Hasan", ""], ["Navab", "Nassir", ""], ["Eslami", "Abouzar", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2003.05709", "submitter": "Jingyun Xiao", "authors": "Jingyun Xiao, Shuang Yang, Yuanhang Zhang, Shiguang Shan, Xilin Chen", "title": "Deformation Flow Based Two-Stream Network for Lip Reading", "comments": "7 pages, FG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading is the task of recognizing the speech content by analyzing\nmovements in the lip region when people are speaking. Observing on the\ncontinuity in adjacent frames in the speaking process, and the consistency of\nthe motion patterns among different speakers when they pronounce the same\nphoneme, we model the lip movements in the speaking process as a sequence of\napparent deformations in the lip region. Specifically, we introduce a\nDeformation Flow Network (DFN) to learn the deformation flow between adjacent\nframes, which directly captures the motion information within the lip region.\nThe learned deformation flow is then combined with the original grayscale\nframes with a two-stream network to perform lip reading. Different from\nprevious two-stream networks, we make the two streams learn from each other in\nthe learning process by introducing a bidirectional knowledge distillation loss\nto train the two branches jointly. Owing to the complementary cues provided by\ndifferent branches, the two-stream network shows a substantial improvement over\nusing either single branch. A thorough experimental evaluation on two\nlarge-scale lip reading benchmarks is presented with detailed analysis. The\nresults accord with our motivation, and show that our method achieves\nstate-of-the-art or comparable performance on these two challenging datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:13:44 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 00:54:46 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Xiao", "Jingyun", ""], ["Yang", "Shuang", ""], ["Zhang", "Yuanhang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2003.05710", "submitter": "Nibaran Das", "authors": "Somenath Kuiry, Nibaran Das, Alaka Das, Mita Nasipuri", "title": "EDC3: Ensemble of Deep-Classifiers using Class-specific Copula functions\n  to Improve Semantic Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the literature, many fusion techniques are registered for the segmentation\nof images, but they primarily focus on observed output or belief score or\nprobability score of the output classes. In the present work, we have utilized\ninter source statistical dependency among different classifiers for ensembling\nof different deep learning techniques for semantic segmentation of images. For\nthis purpose, in the present work, a class-wise Copula-based ensembling method\nis newly proposed for solving the multi-class segmentation problem.\nExperimentally, it is observed that the performance has improved more for\nsemantic image segmentation using the proposed class-specific Copula function\nthan the traditionally used single Copula function for the problem. The\nperformance is also compared with three state-of-the-art ensembling methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:18:45 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Kuiry", "Somenath", ""], ["Das", "Nibaran", ""], ["Das", "Alaka", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2003.05712", "submitter": "Nibaran Das", "authors": "Soumyajyoti Dey, Soham Das, Swarnendu Ghosh, Shyamali Mitra, Sukanta\n  Chakrabarty and Nibaran Das", "title": "SynCGAN: Using learnable class specific priors to generate synthetic\n  data for improving classifier performance on cytological images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging aspects of medical image analysis is the lack of\na high quantity of annotated data. This makes it difficult for deep learning\nalgorithms to perform well due to a lack of variations in the input space.\nWhile generative adversarial networks have shown promise in the field of\nsynthetic data generation, but without a carefully designed prior the\ngeneration procedure can not be performed well. In the proposed approach we\nhave demonstrated the use of automatically generated segmentation masks as\nlearnable class-specific priors to guide a conditional GAN for the generation\nof patho-realistic samples for cytology image. We have observed that\naugmentation of data using the proposed pipeline called \"SynCGAN\" improves the\nperformance of state of the art classifiers such as ResNet-152, DenseNet-161,\nInception-V3 significantly.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:23:23 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Dey", "Soumyajyoti", ""], ["Das", "Soham", ""], ["Ghosh", "Swarnendu", ""], ["Mitra", "Shyamali", ""], ["Chakrabarty", "Sukanta", ""], ["Das", "Nibaran", ""]]}, {"id": "2003.05748", "submitter": "Sean Saito", "authors": "Sean Saito, Jin Wang", "title": "Explaining Away Attacks Against Neural Networks", "comments": "2 pages, 2 figures; Accepted at MLSys 2020 First Workshop on Secure\n  and Resilient Autonomy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of identifying adversarial attacks on image-based\nneural networks. We present intriguing experimental results showing significant\ndiscrepancies between the explanations generated for the predictions of a model\non clean and adversarial data. Utilizing this intuition, we propose a framework\nwhich can identify whether a given input is adversarial based on the\nexplanations given by the model. Code for our experiments can be found here:\nhttps://github.com/seansaito/Explaining-Away-Attacks-Against-Neural-Networks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 15:32:30 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Saito", "Sean", ""], ["Wang", "Jin", ""]]}, {"id": "2003.05787", "submitter": "Zuheng Ming", "authors": "Zuheng Ming, Jean-Christophe Burie, Muhammad Muzzamil Luqman", "title": "Cross-modal Multi-task Learning for Graphic Recognition of Caricature\n  Face", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.03341", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition of realistic visual images has been well studied and made a\nsignificant progress in the recent decade. Unlike the realistic visual images,\nthe face recognition of the caricatures is far from the performance of the\nvisual images. This is largely due to the extreme non-rigid distortions of the\ncaricatures introduced by exaggerating the facial features to strengthen the\ncharacters. The heterogeneous modalities of the caricatures and the visual\nimages result the caricature-visual face recognition is a cross-modal problem.\nIn this paper, we propose a method to conduct caricature-visual face\nrecognition via multi-task learning. Rather than the conventional multi-task\nlearning with fixed weights of tasks, this work proposes an approach to learn\nthe weights of tasks according to the importance of tasks. The proposed\nmulti-task learning with dynamic tasks weights enables to appropriately train\nthe hard task and easy task instead of being stuck in the over-training easy\ntask as conventional methods. The experimental results demonstrate the\neffectiveness of the proposed dynamic multi-task learning for cross-modal\ncaricature-visual face recognition. The performances on the datasets CaVI and\nWebCaricature show the superiority over the state-of-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 18:08:19 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Ming", "Zuheng", ""], ["Burie", "Jean-Christophe", ""], ["Luqman", "Muhammad Muzzamil", ""]]}, {"id": "2003.05837", "submitter": "Hao Shao", "authors": "Manyuan Zhang, Hao Shao, Guanglu Song, Yu Liu, Junjie Yan", "title": "Top-1 Solution of Multi-Moments in Time Challenge 2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this technical report, we briefly introduce the solutions of our team\n'Efficient' for the Multi-Moments in Time challenge in ICCV 2019. We first\nconduct several experiments with popular Image-Based action recognition methods\nTRN, TSN, and TSM. Then a novel temporal interlacing network is proposed\ntowards fast and accurate recognition. Besides, the SlowFast network and its\nvariants are explored. Finally, we ensemble all the above models and achieve\n67.22\\% on the validation set and 60.77\\% on the test set, which ranks 1st on\nthe final leaderboard. In addition, we release a new code repository for video\nunderstanding which unifies state-of-the-art 2D and 3D methods based on\nPyTorch. The solution of the challenge is also included in the repository,\nwhich is available at https://github.com/Sense-X/X-Temporal.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:11:38 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 11:53:24 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Manyuan", ""], ["Shao", "Hao", ""], ["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Yan", "Junjie", ""]]}, {"id": "2003.05848", "submitter": "Fabian Manhardt", "authors": "Fabian Manhardt and Gu Wang and Benjamin Busam and Manuel Nickel and\n  Sven Meier and Luca Minciullo and Xiangyang Ji and Nassir Navab", "title": "CPS++: Improving Class-level 6D Pose and Shape Estimation From Monocular\n  Images With Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary monocular 6D pose estimation methods can only cope with a\nhandful of object instances. This naturally hampers possible applications as,\nfor instance, robots seamlessly integrated in everyday processes necessarily\nrequire the ability to work with hundreds of different objects. To tackle this\nproblem of immanent practical relevance, we propose a novel method for\nclass-level monocular 6D pose estimation, coupled with metric shape retrieval.\nUnfortunately, acquiring adequate annotations is very time-consuming and labor\nintensive. This is especially true for class-level 6D pose estimation, as one\nis required to create a highly detailed reconstruction for all objects and then\nannotate each object and scene using these models. To overcome this\nshortcoming, we additionally propose the idea of synthetic-to-real domain\ntransfer for class-level 6D poses by means of self-supervised learning, which\nremoves the burden of collecting numerous manual annotations. In essence, after\ntraining our proposed method fully supervised with synthetic data, we leverage\nrecent advances in differentiable rendering to self-supervise the model with\nunannotated real RGB-D data to improve latter inference. We experimentally\ndemonstrate that we can retrieve precise 6D poses and metric shapes from a\nsingle RGB image.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:28:13 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 15:20:02 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 10:20:19 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Manhardt", "Fabian", ""], ["Wang", "Gu", ""], ["Busam", "Benjamin", ""], ["Nickel", "Manuel", ""], ["Meier", "Sven", ""], ["Minciullo", "Luca", ""], ["Ji", "Xiangyang", ""], ["Navab", "Nassir", ""]]}, {"id": "2003.05855", "submitter": "Lei Li", "authors": "Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, Chiew-Lan Tai", "title": "End-to-End Learning Local Multi-view Descriptors for 3D Point Clouds", "comments": "CVPR 2020. Webpage:\n  https://github.com/craigleili/3DLocalMultiViewDesc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an end-to-end framework to learn local multi-view\ndescriptors for 3D point clouds. To adopt a similar multi-view representation,\nexisting studies use hand-crafted viewpoints for rendering in a preprocessing\nstage, which is detached from the subsequent descriptor learning stage. In our\nframework, we integrate the multi-view rendering into neural networks by using\na differentiable renderer, which allows the viewpoints to be optimizable\nparameters for capturing more informative local context of interest points. To\nobtain discriminative descriptors, we also design a soft-view pooling module to\nattentively fuse convolutional features across views. Extensive experiments on\nexisting 3D registration benchmarks show that our method outperforms existing\nlocal descriptors both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:41:34 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 14:32:05 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Lei", ""], ["Zhu", "Siyu", ""], ["Fu", "Hongbo", ""], ["Tan", "Ping", ""], ["Tai", "Chiew-Lan", ""]]}, {"id": "2003.05863", "submitter": "Han Yang", "authors": "Han Yang, Ruimao Zhang, Xiaobao Guo, Wei Liu, Wangmeng Zuo, Ping Luo", "title": "Towards Photo-Realistic Virtual Try-On by Adaptively\n  Generating$\\leftrightarrow$Preserving Image Content", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image visual try-on aims at transferring a target clothing image onto a\nreference person, and has become a hot topic in recent years. Prior arts\nusually focus on preserving the character of a clothing image (e.g. texture,\nlogo, embroidery) when warping it to arbitrary human pose. However, it remains\na big challenge to generate photo-realistic try-on images when large occlusions\nand human poses are presented in the reference person. To address this issue,\nwe propose a novel visual try-on network, namely Adaptive Content Generating\nand Preserving Network (ACGPN). In particular, ACGPN first predicts semantic\nlayout of the reference image that will be changed after try-on (e.g. long\nsleeve shirt$\\rightarrow$arm, arm$\\rightarrow$jacket), and then determines\nwhether its image content needs to be generated or preserved according to the\npredicted semantic layout, leading to photo-realistic try-on and rich clothing\ndetails. ACGPN generally involves three major modules. First, a semantic layout\ngeneration module utilizes semantic segmentation of the reference image to\nprogressively predict the desired semantic layout after try-on. Second, a\nclothes warping module warps clothing images according to the generated\nsemantic layout, where a second-order difference constraint is introduced to\nstabilize the warping process during training. Third, an inpainting module for\ncontent fusion integrates all information (e.g. reference image, semantic\nlayout, warped clothes) to adaptively produce each semantic part of human body.\nIn comparison to the state-of-the-art methods, ACGPN can generate\nphoto-realistic images with much better perceptual quality and richer\nfine-details.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:55:39 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Yang", "Han", ""], ["Zhang", "Ruimao", ""], ["Guo", "Xiaobao", ""], ["Liu", "Wei", ""], ["Zuo", "Wangmeng", ""], ["Luo", "Ping", ""]]}, {"id": "2003.05886", "submitter": "Christopher Zach", "authors": "Christopher Zach, Huu Le", "title": "Truncated Inference for Latent Variable Optimization Problems:\n  Application to Robust Estimation and Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with an auxiliary latent variable structure in addition\nto the main model parameters occur frequently in computer vision and machine\nlearning. The additional latent variables make the underlying optimization task\nexpensive, either in terms of memory (by maintaining the latent variables), or\nin terms of runtime (repeated exact inference of latent variables). We aim to\nremove the need to maintain the latent variables and propose two formally\njustified methods, that dynamically adapt the required accuracy of latent\nvariable inference. These methods have applications in large scale robust\nestimation and in learning energy-based models from labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:32:06 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Zach", "Christopher", ""], ["Le", "Huu", ""]]}, {"id": "2003.05891", "submitter": "Jun Shi", "authors": "Jun Shi, Jianfeng Xu, Kazuyuki Tasaka, Zhibo Chen", "title": "SASL: Saliency-Adaptive Sparsity Learning for Neural Network\n  Acceleration", "comments": "Accepted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating the inference speed of CNNs is critical to their deployment in\nreal-world applications. Among all the pruning approaches, those implementing a\nsparsity learning framework have shown to be effective as they learn and prune\nthe models in an end-to-end data-driven manner. However, these works impose the\nsame sparsity regularization on all filters indiscriminately, which can hardly\nresult in an optimal structure-sparse network. In this paper, we propose a\nSaliency-Adaptive Sparsity Learning (SASL) approach for further optimization. A\nnovel and effective estimation of each filter, i.e., saliency, is designed,\nwhich is measured from two aspects: the importance for the prediction\nperformance and the consumed computational resources. During sparsity learning,\nthe regularization strength is adjusted according to the saliency, so our\noptimized format can better preserve the prediction performance while zeroing\nout more computation-heavy filters. The calculation for saliency introduces\nminimum overhead to the training process, which means our SASL is very\nefficient. During the pruning phase, in order to optimize the proposed\ndata-dependent criterion, a hard sample mining strategy is utilized, which\nshows higher effectiveness and efficiency. Extensive experiments demonstrate\nthe superior performance of our method. Notably, on ILSVRC-2012 dataset, our\napproach can reduce 49.7% FLOPs of ResNet-50 with very negligible 0.39% top-1\nand 0.05% top-5 accuracy degradation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:49:37 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 11:44:59 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 02:40:13 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Shi", "Jun", ""], ["Xu", "Jianfeng", ""], ["Tasaka", "Kazuyuki", ""], ["Chen", "Zhibo", ""]]}, {"id": "2003.05905", "submitter": "Rongliang Wu", "authors": "Rongliang Wu, Gongjie Zhang, Shijian Lu, Tao Chen", "title": "Cascade EF-GAN: Progressive Facial Expression Editing with Local Focuses", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Nets (GANs) have shown remarkable\nimprovements for facial expression editing. However, current methods are still\nprone to generate artifacts and blurs around expression-intensive regions, and\noften introduce undesired overlapping artifacts while handling large-gap\nexpression transformations such as transformation from furious to laughing. To\naddress these limitations, we propose Cascade Expression Focal GAN (Cascade\nEF-GAN), a novel network that performs progressive facial expression editing\nwith local expression focuses. The introduction of the local focus enables the\nCascade EF-GAN to better preserve identity-related features and details around\neyes, noses and mouths, which further helps reduce artifacts and blurs within\nthe generated facial images. In addition, an innovative cascade transformation\nstrategy is designed by dividing a large facial expression transformation into\nmultiple small ones in cascade, which helps suppress overlapping artifacts and\nproduce more realistic editing while dealing with large-gap expression\ntransformations. Extensive experiments over two publicly available facial\nexpression datasets show that our proposed Cascade EF-GAN achieves superior\nperformance for facial expression editing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 17:11:44 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 15:08:06 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wu", "Rongliang", ""], ["Zhang", "Gongjie", ""], ["Lu", "Shijian", ""], ["Chen", "Tao", ""]]}, {"id": "2003.05907", "submitter": "Pradyumna Chari", "authors": "Pradyumna Chari, Anil Kumar Vadathya, Kaushik Mitra", "title": "Optimal HDR and Depth from Dual Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual camera systems have assisted in the proliferation of various\napplications, such as optical zoom, low-light imaging and High Dynamic Range\n(HDR) imaging. In this work, we explore an optimal method for capturing the\nscene HDR and disparity map using dual camera setups. Hasinoff et al. (2010)\nhave developed a noise optimal framework for HDR capture from a single camera.\nWe generalize this to the dual camera set-up for estimating both HDR and\ndisparity map. It may seem that dual camera systems can capture HDR in a\nshorter time. However, disparity estimation is a necessary step, which requires\noverlap among the images captured by the two cameras. This may lead to an\nincrease in the capture time. To address this conflicting requirement, we\npropose a novel framework to find the optimal exposure and ISO sequence by\nminimizing the capture time under the constraints of an upper bound on the\ndisparity error and a lower bound on the per-exposure SNR. We show that the\nresulting optimization problem is non-convex in general and propose an\nappropriate initialization technique. To obtain the HDR and disparity map from\nthe optimal capture sequence, we propose a pipeline which alternates between\nestimating the camera ICRFs and the scene disparity map. We demonstrate that\nour optimal capture sequence leads to better results than other possible\ncapture sequences. Our results are also close to those obtained by capturing\nthe full stereo stack spanning the entire dynamic range. Finally, we present\nfor the first time a stereo HDR dataset consisting of dense ISO and exposure\nstack captured from a smartphone dual camera. The dataset consists of 6 scenes,\nwith an average of 142 exposure-ISO image sequence per scene.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 17:14:08 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Chari", "Pradyumna", ""], ["Vadathya", "Anil Kumar", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2003.05961", "submitter": "Ruofan Zhou", "authors": "Ruofan Zhou, Majed El Helou, Daniel Sage, Thierry Laroche, Arne Seitz,\n  Sabine S\\\"usstrunk", "title": "W2S: Microscopy Data with Joint Denoising and Super-Resolution for\n  Widefield to SIM Mapping", "comments": "ECCVW 2020. Project page: \\<https://github.com/ivrl/w2s>", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fluorescence microscopy live-cell imaging, there is a critical trade-off\nbetween the signal-to-noise ratio and spatial resolution on one side, and the\nintegrity of the biological sample on the other side. To obtain clean\nhigh-resolution (HR) images, one can either use microscopy techniques, such as\nstructured-illumination microscopy (SIM), or apply denoising and\nsuper-resolution (SR) algorithms. However, the former option requires multiple\nshots that can damage the samples, and although efficient deep learning based\nalgorithms exist for the latter option, no benchmark exists to evaluate these\nalgorithms on the joint denoising and SR (JDSR) tasks. To study JDSR on\nmicroscopy data, we propose such a novel JDSR dataset, Widefield2SIM (W2S),\nacquired using a conventional fluorescence widefield and SIM imaging. W2S\nincludes 144,000 real fluorescence microscopy images, resulting in a total of\n360 sets of images. A set is comprised of noisy low-resolution (LR) widefield\nimages with different noise levels, a noise-free LR image, and a corresponding\nhigh-quality HR SIM image. W2S allows us to benchmark the combinations of 6\ndenoising methods and 6 SR methods. We show that state-of-the-art SR networks\nperform very poorly on noisy inputs. Our evaluation also reveals that applying\nthe best denoiser in terms of reconstruction error followed by the best SR\nmethod does not necessarily yield the best final result. Both quantitative and\nqualitative results show that SR networks are sensitive to noise and the\nsequential application of denoising and SR algorithms is sub-optimal. Lastly,\nwe demonstrate that SR networks retrained end-to-end for JDSR outperform any\ncombination of state-of-the-art deep denoising and SR networks\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 18:15:09 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 11:17:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zhou", "Ruofan", ""], ["Helou", "Majed El", ""], ["Sage", "Daniel", ""], ["Laroche", "Thierry", ""], ["Seitz", "Arne", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2003.05970", "submitter": "Aasheesh Singh", "authors": "Aasheesh Singh, Aditya Kamireddypalli, Vineet Gandhi, K Madhava\n  Krishna", "title": "LiDAR guided Small obstacle Segmentation", "comments": "8 pages, Submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Detecting small obstacles on the road is critical for autonomous driving. In\nthis paper, we present a method to reliably detect such obstacles through a\nmulti-modal framework of sparse LiDAR(VLP-16) and Monocular vision. LiDAR is\nemployed to provide additional context in the form of confidence maps to\nmonocular segmentation networks. We show significant performance gains when the\ncontext is fed as an additional input to monocular semantic segmentation\nframeworks. We further present a new semantic segmentation dataset to the\ncommunity, comprising of over 3000 image frames with corresponding LiDAR\nobservations. The images come with pixel-wise annotations of three classes\noff-road, road, and small obstacle. We stress that precise calibration between\nLiDAR and camera is crucial for this task and thus propose a novel Hausdorff\ndistance based calibration refinement method over extrinsic parameters. As a\nfirst benchmark over this dataset, we report our results with 73% instance\ndetection up to a distance of 50 meters on challenging scenarios. Qualitatively\nby showcasing accurate segmentation of obstacles less than 15 cms at 50m depth\nand quantitatively through favourable comparisons vis a vis prior art, we\nvindicate the method's efficacy. Our project-page and Dataset is hosted at\nhttps://small-obstacle-dataset.github.io/\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 18:34:46 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Singh", "Aasheesh", ""], ["Kamireddypalli", "Aditya", ""], ["Gandhi", "Vineet", ""], ["Krishna", "K Madhava", ""]]}, {"id": "2003.05982", "submitter": "Gregory Meyer", "authors": "Gregory P. Meyer, Jake Charland, Shreyash Pandey, Ankit Laddha, Shivam\n  Gautam, Carlos Vallespi-Gonzalez, Carl K. Wellington", "title": "LaserFlow: Efficient and Probabilistic Object Detection and Motion\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present LaserFlow, an efficient method for 3D object\ndetection and motion forecasting from LiDAR. Unlike the previous work, our\napproach utilizes the native range view representation of the LiDAR, which\nenables our method to operate at the full range of the sensor in real-time\nwithout voxelization or compression of the data. We propose a new multi-sweep\nfusion architecture, which extracts and merges temporal features directly from\nthe range images. Furthermore, we propose a novel technique for learning a\nprobability distribution over future trajectories inspired by curriculum\nlearning. We evaluate LaserFlow on two autonomous driving datasets and\ndemonstrate competitive results when compared to the existing state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:13:12 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 20:55:26 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 16:56:13 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 20:57:25 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Meyer", "Gregory P.", ""], ["Charland", "Jake", ""], ["Pandey", "Shreyash", ""], ["Laddha", "Ankit", ""], ["Gautam", "Shivam", ""], ["Vallespi-Gonzalez", "Carlos", ""], ["Wellington", "Carl K.", ""]]}, {"id": "2003.05991", "submitter": "Dor Bank", "authors": "Dor Bank, Noam Koenigstein, Raja Giryes", "title": "Autoencoders", "comments": "Book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autoencoder is a specific type of a neural network, which is mainly\ndesigned to encode the input into a compressed and meaningful representation,\nand then decode it back such that the reconstructed input is similar as\npossible to the original one. This chapter surveys the different types of\nautoencoders that are mainly used today. It also describes various applications\nand use-cases of autoencoders.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:38:47 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 11:18:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Bank", "Dor", ""], ["Koenigstein", "Noam", ""], ["Giryes", "Raja", ""]]}, {"id": "2003.05993", "submitter": "Erik Wijmans", "authors": "Erik Wijmans, Julian Straub, Dhruv Batra, Irfan Essa, Judy Hoffman,\n  Ari Morcos", "title": "Analyzing Visual Representations in Embodied Navigation Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep reinforcement learning require a large amount of\ntraining data and generally result in representations that are often over\nspecialized to the target task. In this work, we present a methodology to study\nthe underlying potential causes for this specialization. We use the recently\nproposed projection weighted Canonical Correlation Analysis (PWCCA) to measure\nthe similarity of visual representations learned in the same environment by\nperforming different tasks.\n  We then leverage our proposed methodology to examine the task dependence of\nvisual representations learned on related but distinct embodied navigation\ntasks. Surprisingly, we find that slight differences in task have no measurable\neffect on the visual representation for both SqueezeNet and ResNet\narchitectures. We then empirically demonstrate that visual representations\nlearned on one task can be effectively transferred to a different task.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:43:59 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Wijmans", "Erik", ""], ["Straub", "Julian", ""], ["Batra", "Dhruv", ""], ["Essa", "Irfan", ""], ["Hoffman", "Judy", ""], ["Morcos", "Ari", ""]]}, {"id": "2003.06000", "submitter": "Wei Yang", "authors": "Wei Yang, Chris Paxton, Maya Cakmak, Dieter Fox", "title": "Human Grasp Classification for Reactive Human-to-Robot Handovers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transfer of objects between humans and robots is a critical capability for\ncollaborative robots. Although there has been a recent surge of interest in\nhuman-robot handovers, most prior research focus on robot-to-human handovers.\nFurther, work on the equally critical human-to-robot handovers often assumes\nhumans can place the object in the robot's gripper. In this paper, we propose\nan approach for human-to-robot handovers in which the robot meets the human\nhalfway, by classifying the human's grasp of the object and quickly planning a\ntrajectory accordingly to take the object from the human's hand according to\ntheir intent. To do this, we collect a human grasp dataset which covers typical\nways of holding objects with various hand shapes and poses, and learn a deep\nmodel on this dataset to classify the hand grasps into one of these categories.\nWe present a planning and execution approach that takes the object from the\nhuman hand according to the detected grasp and hand position, and replans as\nnecessary when the handover is interrupted. Through a systematic evaluation, we\ndemonstrate that our system results in more fluent handovers versus two\nbaselines. We also present findings from a user study (N = 9) demonstrating the\neffectiveness and usability of our approach with naive users in different\nscenarios. More results and videos can be found at http://wyang.me/handovers.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:58:03 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Yang", "Wei", ""], ["Paxton", "Chris", ""], ["Cakmak", "Maya", ""], ["Fox", "Dieter", ""]]}, {"id": "2003.06045", "submitter": "Zehua Zhang", "authors": "Zehua Zhang, Ashish Tawari, Sujitha Martin, David Crandall", "title": "Interaction Graphs for Object Importance Estimation in On-road Driving\n  Videos", "comments": "Accepted by ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vehicle driving along the road is surrounded by many objects, but only a\nsmall subset of them influence the driver's decisions and actions. Learning to\nestimate the importance of each object on the driver's real-time\ndecision-making may help better understand human driving behavior and lead to\nmore reliable autonomous driving systems. Solving this problem requires models\nthat understand the interactions between the ego-vehicle and the surrounding\nobjects. However, interactions among other objects in the scene can potentially\nalso be very helpful, e.g., a pedestrian beginning to cross the road between\nthe ego-vehicle and the car in front will make the car in front less important.\nWe propose a novel framework for object importance estimation using an\ninteraction graph, in which the features of each object node are updated by\ninteracting with others through graph convolution. Experiments show that our\nmodel outperforms state-of-the-art baselines with much less input and\npre-processing.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 22:28:56 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Zehua", ""], ["Tawari", "Ashish", ""], ["Martin", "Sujitha", ""], ["Crandall", "David", ""]]}, {"id": "2003.06054", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Timothy Hospedales, Tao Xiang", "title": "Deep Domain-Adversarial Image Generation for Domain Generalisation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models typically suffer from the domain shift problem when\ntrained on a source dataset and evaluated on a target dataset of different\ndistribution. To overcome this problem, domain generalisation (DG) methods aim\nto leverage data from multiple source domains so that a trained model can\ngeneralise to unseen domains. In this paper, we propose a novel DG approach\nbased on \\emph{Deep Domain-Adversarial Image Generation} (DDAIG). Specifically,\nDDAIG consists of three components, namely a label classifier, a domain\nclassifier and a domain transformation network (DoTNet). The goal for DoTNet is\nto map the source training data to unseen domains. This is achieved by having a\nlearning objective formulated to ensure that the generated data can be\ncorrectly classified by the label classifier while fooling the domain\nclassifier. By augmenting the source training data with the generated unseen\ndomain data, we can make the label classifier more robust to unknown domain\nchanges. Extensive experiments on four DG datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 23:17:47 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""]]}, {"id": "2003.06082", "submitter": "Bernadette Bucher", "authors": "Bernadette Bucher, Karl Schmeckpeper, Nikolai Matni, Kostas Daniilidis", "title": "An Adversarial Objective for Scalable Exploration", "comments": "Additional visualizations of our results are available on our website\n  at https://sites.google.com/view/action-for-better-prediction . Bernadette\n  Bucher and Karl Schmeckpeper contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based curiosity combines active learning approaches to optimal sampling\nwith the information gain based incentives for exploration presented in the\ncuriosity literature. Existing model-based curiosity methods look to\napproximate prediction uncertainty with approaches which struggle to scale to\nmany prediction-planning pipelines used in robotics tasks. We address these\nscalability issues with an adversarial curiosity method minimizing a score\ngiven by a discriminator network. This discriminator is optimized jointly with\na prediction model and enables our active learning approach to sample sequences\nof observations and actions which result in predictions considered the least\nrealistic by the discriminator. We demonstrate progressively increasing\nadvantages as compute is restricted of our adversarial curiosity approach over\nleading model-based exploration strategies in simulated environments. We\nfurther demonstrate the ability of our adversarial curiosity method to scale to\na robotic manipulation prediction-planning pipeline where we improve sample\nefficiency and prediction performance for a domain transfer problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 02:03:05 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 17:11:18 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 01:21:48 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 18:39:43 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Bucher", "Bernadette", ""], ["Schmeckpeper", "Karl", ""], ["Matni", "Nikolai", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "2003.06105", "submitter": "Kai Qiao", "authors": "Kai Qiao, Jian Chen, Linyuan Wang, Chi Zhang, Li Tong, Bin Yan", "title": "BigGAN-based Bayesian reconstruction of natural images from human brain\n  activity", "comments": "brain decoding; visual reconstruction; generative adversarial network\n  (GAN); Bayesian framework. under review in Neuroscience of Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the visual decoding domain, visually reconstructing presented images given\nthe corresponding human brain activity monitored by functional magnetic\nresonance imaging (fMRI) is difficult, especially when reconstructing viewed\nnatural images. Visual reconstruction is a conditional image generation on fMRI\ndata and thus generative adversarial network (GAN) for natural image generation\nis recently introduced for this task. Although GAN-based methods have greatly\nimproved, the fidelity and naturalness of reconstruction are still\nunsatisfactory due to the small number of fMRI data samples and the instability\nof GAN training. In this study, we proposed a new GAN-based Bayesian visual\nreconstruction method (GAN-BVRM) that includes a classifier to decode\ncategories from fMRI data, a pre-trained conditional generator to generate\nnatural images of specified categories, and a set of encoding models and\nevaluator to evaluate generated images. GAN-BVRM employs the pre-trained\ngenerator of the prevailing BigGAN to generate masses of natural images, and\nselects the images that best matches with the corresponding brain activity\nthrough the encoding models as the reconstruction of the image stimuli. In this\nprocess, the semantic and detailed contents of reconstruction are controlled by\ndecoded categories and encoding models, respectively. GAN-BVRM used the\nBayesian manner to avoid contradiction between naturalness and fidelity from\ncurrent GAN-based methods and thus can improve the advantages of GAN.\nExperimental results revealed that GAN-BVRM improves the fidelity and\nnaturalness, that is, the reconstruction is natural and similar to the\npresented image stimuli.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 04:32:11 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Qiao", "Kai", ""], ["Chen", "Jian", ""], ["Wang", "Linyuan", ""], ["Zhang", "Chi", ""], ["Tong", "Li", ""], ["Yan", "Bin", ""]]}, {"id": "2003.06107", "submitter": "Conghao Wang", "authors": "Beihao Xia, Conghao Wang, Qinmu Peng, Xinge You and Dacheng Tao", "title": "A Spatial-Temporal Attentive Network with Spatial Continuity for\n  Trajectory Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It remains challenging to automatically predict the multi-agent trajectory\ndue to multiple interactions including agent to agent interaction and scene to\nagent interaction. Although recent methods have achieved promising performance,\nmost of them just consider spatial influence of the interactions and ignore the\nfact that temporal influence always accompanies spatial influence. Moreover,\nthose methods based on scene information always require extra segmented scene\nimages to generate multiple socially acceptable trajectories. To solve these\nlimitations, we propose a novel model named spatial-temporal attentive network\nwith spatial continuity (STAN-SC). First, spatial-temporal attention mechanism\nis presented to explore the most useful and important information. Second, we\nconduct a joint feature sequence based on the sequence and instant state\ninformation to make the generative trajectories keep spatial continuity.\nExperiments are performed on the two widely used ETH-UCY datasets and\ndemonstrate that the proposed model achieves state-of-the-art prediction\naccuracy and handles more complex scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 04:35:50 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 00:48:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xia", "Beihao", ""], ["Wang", "Conghao", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""], ["Tao", "Dacheng", ""]]}, {"id": "2003.06124", "submitter": "Chao Jiang", "authors": "Jiang Chao, Liang Huawei, Wang Zhiling", "title": "A High-Performance Object Proposals based on Horizontal High Frequency\n  Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of object proposal as a preprocessing step for\ntarget detection to improve computational efficiency has become an effective\nmethod. Good object proposal methods should have high object detection recall\nrate and low computational cost, as well as good localization quality and\nrepeatability. However, it is difficult for current advanced algorithms to\nachieve a good balance in the above performance. For this problem, we propose a\nclass-independent object proposal algorithm BIHL. It combines the advantages of\nwindow scoring and superpixel merging, which not only improves the localization\nquality but also speeds up the computational efficiency. The experimental\nresults on the VOC2007 data set show that when the IOU is 0.5 and 10,000 budget\nproposals, our method can achieve the highest detection recall and an mean\naverage best overlap of 79.5%, and the computational efficiency is nearly three\ntimes faster than the current fastest method. Moreover, our method is the\nmethod with the highest average repeatability among the methods that achieve\ngood repeatability to various disturbances.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 05:41:17 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 03:12:32 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Chao", "Jiang", ""], ["Huawei", "Liang", ""], ["Zhiling", "Wang", ""]]}, {"id": "2003.06125", "submitter": "Long Wang", "authors": "Kaihua Zhang, Long Wang, Dong Liu, Bo Liu, Qingshan Liu and Zhu Li", "title": "Dual Temporal Memory Network for Efficient Video Object Segmentation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video Object Segmentation (VOS) is typically formulated in a semi-supervised\nsetting. Given the ground-truth segmentation mask on the first frame, the task\nof VOS is to track and segment the single or multiple objects of interests in\nthe rest frames of the video at the pixel level. One of the fundamental\nchallenges in VOS is how to make the most use of the temporal information to\nboost the performance. We present an end-to-end network which stores short- and\nlong-term video sequence information preceding the current frame as the\ntemporal memories to address the temporal modeling in VOS. Our network consists\nof two temporal sub-networks including a short-term memory sub-network and a\nlong-term memory sub-network. The short-term memory sub-network models the\nfine-grained spatial-temporal interactions between local regions across\nneighboring frames in video via a graph-based learning framework, which can\nwell preserve the visual consistency of local regions over time. The long-term\nmemory sub-network models the long-range evolution of object via a\nSimplified-Gated Recurrent Unit (S-GRU), making the segmentation be robust\nagainst occlusions and drift errors. In our experiments, we show that our\nproposed method achieves a favorable and competitive performance on three\nfrequently-used VOS datasets, including DAVIS 2016, DAVIS 2017 and Youtube-VOS\nin terms of both speed and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:07:45 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Kaihua", ""], ["Wang", "Long", ""], ["Liu", "Dong", ""], ["Liu", "Bo", ""], ["Liu", "Qingshan", ""], ["Li", "Zhu", ""]]}, {"id": "2003.06129", "submitter": "Alexander Carballo", "authors": "Alexander Carballo, Jacob Lambert, Abraham Monrroy-Cano, David Robert\n  Wong, Patiphon Narksri, Yuki Kitsukawa, Eijiro Takeuchi, Shinpei Kato, and\n  Kazuya Takeda", "title": "LIBRE: The Multiple 3D LiDAR Dataset", "comments": "Accepted for oral presentation at IEEE Intelligent Vehicles Symposium\n  (IV2020), https://2020.ieee-iv.org/ LIBRE dataset available at\n  https://sites.google.com/g.sp.m.is.nagoya-u.ac.jp/libre-dataset/ Reference\n  video available at https://youtu.be/rWyecoCtKcQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present LIBRE: LiDAR Benchmarking and Reference, a\nfirst-of-its-kind dataset featuring 10 different LiDAR sensors, covering a\nrange of manufacturers, models, and laser configurations. Data captured\nindependently from each sensor includes three different environments and\nconfigurations: static targets, where objects were placed at known distances\nand measured from a fixed position within a controlled environment; adverse\nweather, where static obstacles were measured from a moving vehicle, captured\nin a weather chamber where LiDARs were exposed to different conditions (fog,\nrain, strong light); and finally, dynamic traffic, where dynamic objects were\ncaptured from a vehicle driven on public urban roads, multiple times at\ndifferent times of the day, and including supporting sensors such as cameras,\ninfrared imaging, and odometry devices. LIBRE will contribute to the research\ncommunity to (1) provide a means for a fair comparison of currently available\nLiDARs, and (2) facilitate the improvement of existing self-driving vehicles\nand robotics-related software, in terms of development and tuning of\nLiDAR-based perception algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:17:39 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:00:54 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Carballo", "Alexander", ""], ["Lambert", "Jacob", ""], ["Monrroy-Cano", "Abraham", ""], ["Wong", "David Robert", ""], ["Narksri", "Patiphon", ""], ["Kitsukawa", "Yuki", ""], ["Takeuchi", "Eijiro", ""], ["Kato", "Shinpei", ""], ["Takeda", "Kazuya", ""]]}, {"id": "2003.06131", "submitter": "Xiufeng Xie", "authors": "Xiufeng Xie, Kyu-Han Kim", "title": "Partial Weight Adaptation for Robust DNN Inference", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream video analytics uses a pre-trained DNN model with an assumption\nthat inference input and training data follow the same probability\ndistribution. However, this assumption does not always hold in the wild:\nautonomous vehicles may capture video with varying brightness; unstable\nwireless bandwidth calls for adaptive bitrate streaming of video; and,\ninference servers may serve inputs from heterogeneous IoT devices/cameras. In\nsuch situations, the level of input distortion changes rapidly, thus reshaping\nthe probability distribution of the input.\n  We present GearNN, an adaptive inference architecture that accommodates\nheterogeneous DNN inputs. GearNN employs an optimization algorithm to identify\na small set of \"distortion-sensitive\" DNN parameters, given a memory budget.\nBased on the distortion level of the input, GearNN then adapts only the\ndistortion-sensitive parameters, while reusing the rest of constant parameters\nacross all input qualities. In our evaluation of DNN inference with dynamic\ninput distortions, GearNN improves the accuracy (mIoU) by an average of 18.12%\nover a DNN trained with the undistorted dataset and 4.84% over stability\ntraining from Google, with only 1.8% extra memory overhead.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 06:25:45 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Xie", "Xiufeng", ""], ["Kim", "Kyu-Han", ""]]}, {"id": "2003.06141", "submitter": "Dong Liu", "authors": "Haochen Zhang and Dong Liu and Zhiwei Xiong", "title": "Is There Tradeoff between Spatial and Temporal in Video\n  Super-Resolution?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of deep learning lead to great success of image and video\nsuper-resolution (SR) methods that are based on convolutional neural networks\n(CNN). For video SR, advanced algorithms have been proposed to exploit the\ntemporal correlation between low-resolution (LR) video frames, and/or to\nsuper-resolve a frame with multiple LR frames. These methods pursue higher\nquality of super-resolved frames, where the quality is usually measured frame\nby frame in e.g. PSNR. However, frame-wise quality may not reveal the\nconsistency between frames. If an algorithm is applied to each frame\nindependently (which is the case of most previous methods), the algorithm may\ncause temporal inconsistency, which can be observed as flickering. It is a\nnatural requirement to improve both frame-wise fidelity and between-frame\nconsistency, which are termed spatial quality and temporal quality,\nrespectively. Then we may ask, is a method optimized for spatial quality also\noptimized for temporal quality? Can we optimize the two quality metrics\njointly?\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 07:49:05 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Haochen", ""], ["Liu", "Dong", ""], ["Xiong", "Zhiwei", ""]]}, {"id": "2003.06148", "submitter": "Lu Qi", "authors": "Lu Qi and Yi Wang and Yukang Chen and Yingcong Chen and Xiangyu Zhang\n  and Jian Sun and Jiaya Jia", "title": "PointINS: Point-based Instance Segmentation", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the mask representation in instance segmentation\nwith Point-of-Interest (PoI) features. Differentiating multiple potential\ninstances within a single PoI feature is challenging because learning a\nhigh-dimensional mask feature for each instance using vanilla convolution\ndemands a heavy computing burden. To address this challenge, we propose an\ninstance-aware convolution. It decomposes this mask representation learning\ntask into two tractable modules as instance-aware weights and instance-agnostic\nfeatures. The former is to parametrize convolution for producing mask features\ncorresponding to different instances, improving mask learning efficiency by\navoiding employing several independent convolutions. Meanwhile, the latter\nserves as mask templates in a single point. Together, instance-aware mask\nfeatures are computed by convolving the template with dynamic weights, used for\nthe mask prediction. Along with instance-aware convolution, we propose\nPointINS, a simple and practical instance segmentation approach, building upon\ndense one-stage detectors. Through extensive experiments, we evaluated the\neffectiveness of our framework built upon RetinaNet and FCOS. PointINS in\nResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO\ndataset, outperforming existing point-based methods by a large margin. It gives\na comparable performance to the region-based Mask R-CNN with faster inference.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 08:24:58 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 09:07:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Qi", "Lu", ""], ["Wang", "Yi", ""], ["Chen", "Yukang", ""], ["Chen", "Yingcong", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""], ["Jia", "Jiaya", ""]]}, {"id": "2003.06156", "submitter": "Raphael Memmesheimer", "authors": "Raphael Memmesheimer, Nick Theisen, Dietrich Paulus", "title": "Gimme Signals: Discriminative signal encoding for multimodal activity\n  recognition", "comments": "8 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simple, yet effective and flexible method for action recognition\nsupporting multiple sensor modalities. Multivariate signal sequences are\nencoded in an image and are then classified using a recently proposed\nEfficientNet CNN architecture. Our focus was to find an approach that\ngeneralizes well across different sensor modalities without specific adaptions\nwhile still achieving good results. We apply our method to 4 action recognition\ndatasets containing skeleton sequences, inertial and motion capturing\nmeasurements as well as \\wifi fingerprints that range up to 120 action classes.\nOur method defines the current best CNN-based approach on the NTU RGB+D 120\ndataset, lifts the state of the art on the ARIL Wi-Fi dataset by +6.78%,\nimproves the UTD-MHAD inertial baseline by +14.4%, the UTD-MHAD skeleton\nbaseline by 1.13% and achieves 96.11% on the Simitate motion capturing data\n(80/20 split). We further demonstrate experiments on both, modality fusion on a\nsignal level and signal reduction to prevent the representation from\noverloading.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 08:58:15 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 13:10:04 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Memmesheimer", "Raphael", ""], ["Theisen", "Nick", ""], ["Paulus", "Dietrich", ""]]}, {"id": "2003.06158", "submitter": "Nikolas Lessmann", "authors": "Nikolas Lessmann and Bram van Ginneken", "title": "Random smooth gray value transformations for cross modality learning\n  with gray value invariant networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Random transformations are commonly used for augmentation of the training\ndata with the goal of reducing the uniformity of the training samples. These\ntransformations normally aim at variations that can be expected in images from\nthe same modality. Here, we propose a simple method for transforming the gray\nvalues of an image with the goal of reducing cross modality differences. This\napproach enables segmentation of the lumbar vertebral bodies in CT images using\na network trained exclusively with MR images. The source code is made available\nat https://github.com/nlessmann/rsgt\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 09:01:08 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Lessmann", "Nikolas", ""], ["van Ginneken", "Bram", ""]]}, {"id": "2003.06167", "submitter": "Li Tengpeng -", "authors": "Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen, Qingshan Liu", "title": "Adaptive Graph Convolutional Network with Attention Graph Clustering for\n  Co-saliency Detection", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-saliency detection aims to discover the common and salient foregrounds\nfrom a group of relevant images. For this task, we present a novel adaptive\ngraph convolutional network with attention graph clustering (GCAGC). Three\nmajor contributions have been made, and are experimentally shown to have\nsubstantial practical merits. First, we propose a graph convolutional network\ndesign to extract information cues to characterize the intra- and interimage\ncorrespondence. Second, we develop an attention graph clustering algorithm to\ndiscriminate the common objects from all the salient foreground objects in an\nunsupervised fashion. Third, we present a unified framework with\nencoder-decoder structure to jointly train and optimize the graph convolutional\nnetwork, attention graph cluster, and co-saliency detection decoder in an\nend-to-end manner. We evaluate our proposed GCAGC method on three cosaliency\ndetection benchmark datasets (iCoseg, Cosal2015 and COCO-SEG). Our GCAGC method\nobtains significant improvements over the state-of-the-arts on most of them.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 09:35:59 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Zhang", "Kaihua", ""], ["Li", "Tengpeng", ""], ["Shen", "Shiwen", ""], ["Liu", "Bo", ""], ["Chen", "Jin", ""], ["Liu", "Qingshan", ""]]}, {"id": "2003.06171", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Enrico Motta, Enrico Daga", "title": "Towards a Framework for Visual Intelligence in Service Robotics:\n  Epistemic Requirements and Gap Analysis", "comments": null, "journal-ref": "In Proceedings of the 17th International Conference on Principles\n  of Knowledge Representation and Reasoning (KR 2020), Special Session on KR\n  and Robotics", "doi": "10.24963/kr.2020/93", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key capability required by service robots operating in real-world, dynamic\nenvironments is that of Visual Intelligence, i.e., the ability to use their\nvision system, reasoning components and background knowledge to make sense of\ntheir environment. In this paper, we analyze the epistemic requirements for\nVisual Intelligence, both in a top-down fashion, using existing frameworks for\nhuman-like Visual Intelligence in the literature, and from the bottom up, based\non the errors emerging from object recognition trials in a real-world robotic\nscenario. Finally, we use these requirements to evaluate current knowledge\nbases for Service Robotics and to identify gaps in the support they provide for\nVisual Intelligence. These gaps provide the basis of a research agenda for\ndeveloping more effective knowledge representations for Visual Intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 09:41:05 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chiatti", "Agnese", ""], ["Motta", "Enrico", ""], ["Daga", "Enrico", ""]]}, {"id": "2003.06205", "submitter": "Eva Blanco", "authors": "E. Blanco-Mallo, B. Remeseiro, V. Bol\\'on-Canedo, A. Alonso-Betanzos", "title": "On the effectiveness of convolutional autoencoders on image-based\n  personalized recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems (RS) are increasingly present in our daily lives,\nespecially since the advent of Big Data, which allows for storing all kinds of\ninformation about users' preferences. Personalized RS are successfully applied\nin platforms such as Netflix, Amazon or YouTube. However, they are missing in\ngastronomic platforms such as TripAdvisor, where moreover we can find millions\nof images tagged with users' tastes. This paper explores the potential of using\nthose images as sources of information for modeling users' tastes and proposes\nan image-based classification system to obtain personalized recommendations,\nusing a convolutional autoencoder as feature extractor. The proposed\narchitecture will be applied to TripAdvisor data, using users' reviews that can\nbe defined as a triad composed by a user, a restaurant, and an image of it\ntaken by the user. Since the dataset is highly unbalanced, the use of data\naugmentation on the minority class is also considered in the experimentation.\nResults on data from three cities of different sizes (Santiago de Compostela,\nBarcelona and New York) demonstrate the effectiveness of using a convolutional\nautoencoder as feature extractor, instead of the standard deep features\ncomputed with convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 11:19:02 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Blanco-Mallo", "E.", ""], ["Remeseiro", "B.", ""], ["Bol\u00f3n-Canedo", "V.", ""], ["Alonso-Betanzos", "A.", ""]]}, {"id": "2003.06216", "submitter": "Honey Gupta", "authors": "Honey Gupta and Kaushik Mitra", "title": "Pyramidal Edge-maps and Attention based Guided Thermal Super-resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided super-resolution (GSR) of thermal images using visible range images is\nchallenging because of the difference in the spectral-range between the images.\nThis in turn means that there is significant texture-mismatch between the\nimages, which manifests as blur and ghosting artifacts in the super-resolved\nthermal image. To tackle this, we propose a novel algorithm for GSR based on\npyramidal edge-maps extracted from the visible image. Our proposed network has\ntwo sub-networks. The first sub-network super-resolves the low-resolution\nthermal image while the second obtains edge-maps from the visible image at a\ngrowing perceptual scale and integrates them into the super-resolution\nsub-network with the help of attention-based fusion. Extraction and integration\nof multi-level edges allows the super-resolution network to process\ntexture-to-object level information progressively, enabling more\nstraightforward identification of overlapping edges between the input images.\nExtensive experiments show that our model outperforms the state-of-the-art GSR\nmethods, both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 12:11:26 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 05:50:14 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Gupta", "Honey", ""], ["Mitra", "Kaushik", ""]]}, {"id": "2003.06221", "submitter": "Assaf Shocher", "authors": "Assaf Shocher, Yossi Gandelsman, Inbar Mosseri, Michal Yarom, Michal\n  Irani, William T. Freeman and Tali Dekel", "title": "Semantic Pyramid for Image Generation", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2020.\n  CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel GAN-based model that utilizes the space of deep features\nlearned by a pre-trained classification model. Inspired by classical image\npyramid representations, we construct our model as a Semantic Generation\nPyramid -- a hierarchical framework which leverages the continuum of semantic\ninformation encapsulated in such deep features; this ranges from low level\ninformation contained in fine features to high level, semantic information\ncontained in deeper features. More specifically, given a set of features\nextracted from a reference image, our model generates diverse image samples,\neach with matching features at each semantic level of the classification model.\nWe demonstrate that our model results in a versatile and flexible framework\nthat can be used in various classic and novel image generation tasks. These\ninclude: generating images with a controllable extent of semantic similarity to\na reference image, and different manipulation tasks such as\nsemantically-controlled inpainting and compositing; all achieved with the same\nmodel, with no further training.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 12:23:37 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 14:31:49 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Shocher", "Assaf", ""], ["Gandelsman", "Yossi", ""], ["Mosseri", "Inbar", ""], ["Yarom", "Michal", ""], ["Irani", "Michal", ""], ["Freeman", "William T.", ""], ["Dekel", "Tali", ""]]}, {"id": "2003.06227", "submitter": "Ashish Shrivastava", "authors": "Ting-Yao Hu, Ashish Shrivastava, Oncel Tuzel, Chandra Dhir", "title": "Unsupervised Style and Content Separation by Minimizing Mutual\n  Information for Speech Synthesis", "comments": "Accepted at ICASSP 2020 (for presentation in a lecture session)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.IT cs.LG cs.SD math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to generate speech from input text and a style vector\nthat is extracted from a reference speech signal in an unsupervised manner,\ni.e., no style annotation, such as speaker information, is required. Existing\nunsupervised methods, during training, generate speech by computing style from\nthe corresponding ground truth sample and use a decoder to combine the style\nvector with the input text. Training the model in such a way leaks content\ninformation into the style vector. The decoder can use the leaked content and\nignore some of the input text to minimize the reconstruction loss. At inference\ntime, when the reference speech does not match the content input, the output\nmay not contain all of the content of the input text. We refer to this problem\nas \"content leakage\", which we address by explicitly estimating and minimizing\nthe mutual information between the style and the content through an adversarial\ntraining formulation. We call our method MIST - Mutual Information based Style\nContent Separation. The main goal of the method is to preserve the input\ncontent in the synthesized speech signal, which we measure by the word error\nrate (WER) and show substantial improvements over state-of-the-art unsupervised\nspeech synthesis methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 23:47:41 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hu", "Ting-Yao", ""], ["Shrivastava", "Ashish", ""], ["Tuzel", "Oncel", ""], ["Dhir", "Chandra", ""]]}, {"id": "2003.06233", "submitter": "Chenyang Zhu", "authors": "Jiazhao Zhang, Chenyang Zhu, Lintao Zheng, Kai Xu", "title": "Fusion-Aware Point Convolution for Online Semantic 3D Scene Segmentation", "comments": "To be presented at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online semantic 3D segmentation in company with real-time RGB-D\nreconstruction poses special challenges such as how to perform 3D convolution\ndirectly over the progressively fused 3D geometric data, and how to smartly\nfuse information from frame to frame. We propose a novel fusion-aware 3D point\nconvolution which operates directly on the geometric surface being\nreconstructed and exploits effectively the inter-frame correlation for high\nquality 3D feature learning. This is enabled by a dedicated dynamic data\nstructure which organizes the online acquired point cloud with global-local\ntrees. Globally, we compile the online reconstructed 3D points into an\nincrementally growing coordinate interval tree, enabling fast point insertion\nand neighborhood query. Locally, we maintain the neighborhood information for\neach point using an octree whose construction benefits from the fast query of\nthe global tree.Both levels of trees update dynamically and help the 3D\nconvolution effectively exploits the temporal coherence for effective\ninformation fusion across RGB-D frames.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 12:32:24 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 15:46:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhang", "Jiazhao", ""], ["Zhu", "Chenyang", ""], ["Zheng", "Lintao", ""], ["Xu", "Kai", ""]]}, {"id": "2003.06254", "submitter": "Luke Darlow", "authors": "Luke Nicholas Darlow, Amos Storkey", "title": "What Information Does a ResNet Compress?", "comments": "10 pages + appendices; submitted to ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The information bottleneck principle (Shwartz-Ziv & Tishby, 2017) suggests\nthat SGD-based training of deep neural networks results in optimally compressed\nhidden layers, from an information theoretic perspective. However, this claim\nwas established on toy data. The goal of the work we present here is to test\nwhether the information bottleneck principle is applicable to a realistic\nsetting using a larger and deeper convolutional architecture, a ResNet model.\nWe trained PixelCNN++ models as inverse representation decoders to measure the\nmutual information between hidden layers of a ResNet and input image data, when\ntrained for (1) classification and (2) autoencoding. We find that two stages of\nlearning happen for both training regimes, and that compression does occur,\neven for an autoencoder. Sampling images by conditioning on hidden layers'\nactivations offers an intuitive visualisation to understand what a ResNets\nlearns to forget.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:02:11 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Darlow", "Luke Nicholas", ""], ["Storkey", "Amos", ""]]}, {"id": "2003.06258", "submitter": "Patrick Kn\\\"obelreiter", "authors": "Patrick Kn\\\"obelreiter and Christian Sormann and Alexander Shekhovtsov\n  and Friedrich Fraundorfer and Thomas Pock", "title": "Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proposed by many researchers that combining deep neural networks\nwith graphical models can create more efficient and better regularized\ncomposite models. The main difficulties in implementing this in practice are\nassociated with a discrepancy in suitable learning objectives as well as with\nthe necessity of approximations for the inference. In this work we take one of\nthe simplest inference methods, a truncated max-product Belief Propagation, and\nadd what is necessary to make it a proper component of a deep learning model:\nWe connect it to learning formulations with losses on marginals and compute the\nbackprop operation. This BP-Layer can be used as the final or an intermediate\nblock in convolutional neural networks (CNNs), allowing us to design a\nhierarchical model composing BP inference and CNNs at different scale levels.\nThe model is applicable to a range of dense prediction problems, is\nwell-trainable and provides parameter-efficient and robust solutions in stereo,\noptical flow and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:11:35 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Kn\u00f6belreiter", "Patrick", ""], ["Sormann", "Christian", ""], ["Shekhovtsov", "Alexander", ""], ["Fraundorfer", "Friedrich", ""], ["Pock", "Thomas", ""]]}, {"id": "2003.06276", "submitter": "Muhammad Ali Farooq", "authors": "Muhammad Ali Farooq, Muhammad Aatif Mobeen Azhar, Rana Hammad Raza", "title": "Automatic Lesion Detection System (ALDS) for Skin Cancer Classification\n  Using SVM and Neural Classifiers", "comments": null, "journal-ref": null, "doi": "10.1109/BIBE.2016.53", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology aided platforms provide reliable tools in almost every field these\ndays. These tools being supported by computational power are significant for\napplications that need sensitive and precise data analysis. One such important\napplication in the medical field is Automatic Lesion Detection System (ALDS)\nfor skin cancer classification. Computer aided diagnosis helps physicians and\ndermatologists to obtain a second opinion for proper analysis and treatment of\nskin cancer. Precise segmentation of the cancerous mole along with surrounding\narea is essential for proper analysis and diagnosis. This paper is focused\ntowards the development of improved ALDS framework based on probabilistic\napproach that initially utilizes active contours and watershed merged mask for\nsegmenting out the mole and later SVM and Neural Classifier are applied for the\nclassification of the segmented mole. After lesion segmentation, the selected\nfeatures are classified to ascertain that whether the case under consideration\nis melanoma or non-melanoma. The approach is tested for varying datasets and\ncomparative analysis is performed that reflects the effectiveness of the\nproposed system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:31:35 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Farooq", "Muhammad Ali", ""], ["Azhar", "Muhammad Aatif Mobeen", ""], ["Raza", "Rana Hammad", ""]]}, {"id": "2003.06297", "submitter": "Chaoqi Chen", "authors": "Chaoqi Chen, Zebiao Zheng, Xinghao Ding, Yue Huang, Qi Dou", "title": "Harmonizing Transferability and Discriminability for Adapting Object\n  Detectors", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in adaptive object detection have achieved compelling results\nin virtue of adversarial feature adaptation to mitigate the distributional\nshifts along the detection pipeline. Whilst adversarial adaptation\nsignificantly enhances the transferability of feature representations, the\nfeature discriminability of object detectors remains less investigated.\nMoreover, transferability and discriminability may come at a contradiction in\nadversarial adaptation given the complex combinations of objects and the\ndifferentiated scene layouts between domains. In this paper, we propose a\nHierarchical Transferability Calibration Network (HTCN) that hierarchically\n(local-region/image/instance) calibrates the transferability of feature\nrepresentations for harmonizing transferability and discriminability. The\nproposed model consists of three components: (1) Importance Weighted\nAdversarial Training with input Interpolation (IWAT-I), which strengthens the\nglobal discriminability by re-weighting the interpolated image-level features;\n(2) Context-aware Instance-Level Alignment (CILA) module, which enhances the\nlocal discriminability by capturing the underlying complementary effect between\nthe instance-level feature and the global context information for the\ninstance-level feature alignment; (3) local feature masks that calibrate the\nlocal transferability to provide semantic guidance for the following\ndiscriminative pattern alignment. Experimental results show that HTCN\nsignificantly outperforms the state-of-the-art methods on benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:47:48 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Chen", "Chaoqi", ""], ["Zheng", "Zebiao", ""], ["Ding", "Xinghao", ""], ["Huang", "Yue", ""], ["Dou", "Qi", ""]]}, {"id": "2003.06310", "submitter": "Pai-Yu Tan", "authors": "Pai-Yu Tan, Po-Yao Chuang, Yen-Ting Lin, Cheng-Wen Wu, and Juin-Ming\n  Lu", "title": "A Power-Efficient Binary-Weight Spiking Neural Network Architecture for\n  Real-Time Object Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network hardware is considered an essential part of future edge\ndevices. In this paper, we propose a binary-weight spiking neural network\n(BW-SNN) hardware architecture for low-power real-time object classification on\nedge platforms. This design stores a full neural network on-chip, and hence\nrequires no off-chip bandwidth. The proposed systolic array maximizes data\nreuse for a typical convolutional layer. A 5-layer convolutional BW-SNN\nhardware is implemented in 90nm CMOS. Compared with state-of-the-art designs,\nthe area cost and energy per classification are reduced by 7$\\times$ and\n23$\\times$, respectively, while also achieving a higher accuracy on the MNIST\nbenchmark. This is also a pioneering SNN hardware architecture that supports\nadvanced CNN architectures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 11:25:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Tan", "Pai-Yu", ""], ["Chuang", "Po-Yao", ""], ["Lin", "Yen-Ting", ""], ["Wu", "Cheng-Wen", ""], ["Lu", "Juin-Ming", ""]]}, {"id": "2003.06327", "submitter": "Hazrat Ali", "authors": "Waqar Ahmad, Misbah Kazmi, Hazrat Ali", "title": "Human Activity Recognition using Multi-Head CNN followed by LSTM", "comments": "IEEE ICET 2019", "journal-ref": null, "doi": "10.1109/ICET48972.2019.8994412", "report-no": null, "categories": "eess.SP cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study presents a novel method to recognize human physical activities\nusing CNN followed by LSTM. Achieving high accuracy by traditional machine\nlearning algorithms, (such as SVM, KNN and random forest method) is a\nchallenging task because the data acquired from the wearable sensors like\naccelerometer and gyroscope is a time-series data. So, to achieve high\naccuracy, we propose a multi-head CNN model comprising of three CNNs to extract\nfeatures for the data acquired from different sensors and all three CNNs are\nthen merged, which are followed by an LSTM layer and a dense layer. The\nconfiguration of all three CNNs is kept the same so that the same number of\nfeatures are obtained for every input to CNN. By using the proposed method, we\nachieve state-of-the-art accuracy, which is comparable to traditional machine\nlearning algorithms and other deep neural network algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:29:59 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Ahmad", "Waqar", ""], ["Kazmi", "Misbah", ""], ["Ali", "Hazrat", ""]]}, {"id": "2003.06336", "submitter": "Renato Martins", "authors": "Renato Martins, Dhiego Bersan, Mario F. M. Campos and Erickson R.\n  Nascimento", "title": "Extending Maps with Semantic and Contextual Object Information for Robot\n  Navigation: a Learning-Based Framework using Visual and Depth Cues", "comments": "Preprint version of the article to appear at Journal of Intelligent &\n  Robotic Systems (2020)", "journal-ref": null, "doi": "10.1007/s10846-019-01136-5", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of building augmented metric representations\nof scenes with semantic information from RGB-D images. We propose a complete\nframework to create an enhanced map representation of the environment with\nobject-level information to be used in several applications such as human-robot\ninteraction, assistive robotics, visual navigation, or in manipulation tasks.\nOur formulation leverages a CNN-based object detector (Yolo) with a 3D\nmodel-based segmentation technique to perform instance semantic segmentation,\nand to localize, identify, and track different classes of objects in the scene.\nThe tracking and positioning of semantic classes is done with a dictionary of\nKalman filters in order to combine sensor measurements over time and then\nproviding more accurate maps. The formulation is designed to identify and to\ndisregard dynamic objects in order to obtain a medium-term invariant map\nrepresentation. The proposed method was evaluated with collected and publicly\navailable RGB-D data sequences acquired in different indoor scenes.\nExperimental results show the potential of the technique to produce augmented\nsemantic maps containing several objects (notably doors). We also provide to\nthe community a dataset composed of annotated object classes (doors, fire\nextinguishers, benches, water fountains) and their positioning, as well as the\nsource code as ROS packages.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 15:05:23 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Martins", "Renato", ""], ["Bersan", "Dhiego", ""], ["Campos", "Mario F. M.", ""], ["Nascimento", "Erickson R.", ""]]}, {"id": "2003.06356", "submitter": "Muhammad Ali Farooq", "authors": "Muhammad Ali Farooq, Asma Khatoon, Viktor Varkarakis, Peter Corcoran", "title": "Advanced Deep Learning Methodologies for Skin Cancer Classification in\n  Prodromal Stages", "comments": "Paper Published in AICS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology-assisted platforms provide reliable solutions in almost every\nfield these days. One such important application in the medical field is the\nskin cancer classification in preliminary stages that need sensitive and\nprecise data analysis. For the proposed study the Kaggle skin cancer dataset is\nutilized. The proposed study consists of two main phases. In the first phase,\nthe images are preprocessed to remove the clutters thus producing a refined\nversion of training images. To achieve that, a sharpening filter is applied\nfollowed by a hair removal algorithm. Different image quality measurement\nmetrics including Peak Signal to Noise (PSNR), Mean Square Error (MSE), Maximum\nAbsolute Squared Deviation (MXERR) and Energy Ratio/ Ratio of Squared Norms\n(L2RAT) are used to compare the overall image quality before and after applying\npreprocessing operations. The results from the aforementioned image quality\nmetrics prove that image quality is not compromised however it is upgraded by\napplying the preprocessing operations. The second phase of the proposed\nresearch work incorporates deep learning methodologies that play an imperative\nrole in accurate, precise and robust classification of the lesion mole. This\nhas been reflected by using two state of the art deep learning models:\nInception-v3 and MobileNet. The experimental results demonstrate notable\nimprovement in train and validation accuracy by using the refined version of\nimages of both the networks, however, the Inception-v3 network was able to\nachieve better validation accuracy thus it was finally selected to evaluate it\non test data. The final test accuracy using state of art Inception-v3 network\nwas 86%.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 16:07:00 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Farooq", "Muhammad Ali", ""], ["Khatoon", "Asma", ""], ["Varkarakis", "Viktor", ""], ["Corcoran", "Peter", ""]]}, {"id": "2003.06409", "submitter": "Anthony Hu", "authors": "Anthony Hu, Fergal Cotter, Nikhil Mohan, Corina Gurau, Alex Kendall", "title": "Probabilistic Future Prediction for Video Scene Understanding", "comments": "Accepted as a conference paper at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning architecture for probabilistic future\nprediction from video. We predict the future semantics, geometry and motion of\ncomplex real-world urban scenes and use this representation to control an\nautonomous vehicle. This work is the first to jointly predict ego-motion,\nstatic scene, and the motion of dynamic agents in a probabilistic manner, which\nallows sampling consistent, highly probable futures from a compact latent\nspace. Our model learns a representation from RGB video with a spatio-temporal\nconvolutional module. The learned representation can be explicitly decoded to\nfuture semantic segmentation, depth, and optical flow, in addition to being an\ninput to a learnt driving policy. To model the stochasticity of the future, we\nintroduce a conditional variational approach which minimises the divergence\nbetween the present distribution (what could happen given what we have seen)\nand the future distribution (what we observe actually happens). During\ninference, diverse futures are generated by sampling from the present\ndistribution.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 17:48:21 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 10:07:40 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hu", "Anthony", ""], ["Cotter", "Fergal", ""], ["Mohan", "Nikhil", ""], ["Gurau", "Corina", ""], ["Kendall", "Alex", ""]]}, {"id": "2003.06417", "submitter": "Deepak Pathak", "authors": "Scott Emmons, Ajay Jain, Michael Laskin, Thanard Kurutach, Pieter\n  Abbeel, Deepak Pathak", "title": "Sparse Graphical Memory for Robust Planning", "comments": "Accepted at NeurIPS 2020. Video and code at\n  https://mishalaskin.github.io/sgm/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To operate effectively in the real world, agents should be able to act from\nhigh-dimensional raw sensory input such as images and achieve diverse goals\nacross long time-horizons. Current deep reinforcement and imitation learning\nmethods can learn directly from high-dimensional inputs but do not scale well\nto long-horizon tasks. In contrast, classical graphical methods like A* search\nare able to solve long-horizon tasks, but assume that the state space is\nabstracted away from raw sensory input. Recent works have attempted to combine\nthe strengths of deep learning and classical planning; however, dominant\nmethods in this domain are still quite brittle and scale poorly with the size\nof the environment. We introduce Sparse Graphical Memory (SGM), a new data\nstructure that stores states and feasible transitions in a sparse memory. SGM\naggregates states according to a novel two-way consistency objective, adapting\nclassic state aggregation criteria to goal-conditioned RL: two states are\nredundant when they are interchangeable both as goals and as starting states.\nTheoretically, we prove that merging nodes according to two-way consistency\nleads to an increase in shortest path lengths that scales only linearly with\nthe merging threshold. Experimentally, we show that SGM significantly\noutperforms current state of the art methods on long horizon, sparse-reward\nvisual navigation tasks. Project video and code are available at\nhttps://mishalaskin.github.io/sgm/\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 17:59:32 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 18:55:04 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 21:37:49 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Emmons", "Scott", ""], ["Jain", "Ajay", ""], ["Laskin", "Michael", ""], ["Kurutach", "Thanard", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2003.06430", "submitter": "Jacopo Cavazza", "authors": "Ruggero Ragonesi, Riccardo Volpi, Jacopo Cavazza and Vittorio Murino", "title": "Learning Unbiased Representations via Mutual Information Backpropagation", "comments": "Code publicly available at https://github.com/rugrag/learn-unbiased", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in learning data-driven representations that can generalize\nwell, even when trained on inherently biased data. In particular, we face the\ncase where some attributes (bias) of the data, if learned by the model, can\nseverely compromise its generalization properties. We tackle this problem\nthrough the lens of information theory, leveraging recent findings for a\ndifferentiable estimation of mutual information. We propose a novel end-to-end\noptimization strategy, which simultaneously estimates and minimizes the mutual\ninformation between the learned representation and the data attributes. When\napplied on standard benchmarks, our model shows comparable or superior\nclassification performance with respect to state-of-the-art approaches.\nMoreover, our method is general enough to be applicable to the problem of\n``algorithmic fairness'', with competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 18:06:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ragonesi", "Ruggero", ""], ["Volpi", "Riccardo", ""], ["Cavazza", "Jacopo", ""], ["Murino", "Vittorio", ""]]}, {"id": "2003.06434", "submitter": "Shane Sims", "authors": "Shane Sims and Cristina Conati", "title": "A Neural Architecture for Detecting Confusion in Eye-tracking Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraged by the success of deep learning in a variety of domains, we\ninvestigate a novel application of its methods on the effectiveness of\ndetecting user confusion in eye-tracking data. We introduce an architecture\nthat uses RNN and CNN sub-models in parallel to take advantage of the temporal\nand visuospatial aspects of our data. Experiments with a dataset of user\ninteractions with the ValueChart visualization tool show that our model\noutperforms an existing model based on Random Forests resulting in a 22%\nimprovement in combined sensitivity & specificity.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 18:20:39 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sims", "Shane", ""], ["Conati", "Cristina", ""]]}, {"id": "2003.06439", "submitter": "Xing Zhao", "authors": "Xing Zhao and Shuang Yang and Shiguang Shan and Xilin Chen", "title": "Mutual Information Maximization for Effective Lip Reading", "comments": "8 pages, Accepted in the 15th IEEE International Conference on\n  Automatic Face and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lip reading has received an increasing research interest in recent years due\nto the rapid development of deep learning and its widespread potential\napplications. One key point to obtain good performance for the lip reading task\ndepends heavily on how effective the representation can be to capture the lip\nmovement information and meanwhile to resist the noises resulted from the\nchange of pose, lighting conditions, speaker's appearance and so on. Towards\nthis target, we propose to introduce the mutual information constraints on both\nthe local feature's level and the global sequence's level to enhance the\nrelations of the features with the speech content. On the one hand, we\nconstraint the features generated at each time step to enable them carry a\nstrong relation with the speech content by imposing the local mutual\ninformation maximization constraint (LMIM), leading to improvements over the\nmodel's ability to discover fine-grained lip movements and the fine-grained\ndifferences among words with similar pronunciation, such as ``spend'' and\n``spending''. On the other hand, we introduce the mutual information\nmaximization constraint on the global sequence's level (GMIM), to make the\nmodel be able to pay more attention to discriminate key frames related with the\nspeech content, and less to various noises appeared in the speaking process. By\ncombining these two advantages together, the proposed method is expected to be\nboth discriminative and robust for effective lip reading. To verify this\nmethod, we evaluate on two large-scale benchmark. We perform a detailed\nanalysis and comparison on several aspects, including the comparison of the\nLMIM and GMIM with the baseline, the visualization of the learned\nrepresentation and so on. The results not only prove the effectiveness of the\nproposed method but also report new state-of-the-art performance on both the\ntwo benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 18:47:42 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zhao", "Xing", ""], ["Yang", "Shuang", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2003.06451", "submitter": "Angelica I. Aviles-Rivero", "authors": "Marianne de Vriendt, Philip Sellars, Angelica I Aviles-Rivero", "title": "The GraphNet Zoo: An All-in-One Graph Based Deep Semi-Supervised\n  Framework for Medical Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classifying a medical image dataset when we have a\nlimited amounts of labels. This is very common yet challenging setting as\nlabelled data is expensive, time consuming to collect and may require expert\nknowledge. The current classification go-to of deep supervised learning is\nunable to cope with such a problem setup. However, using semi-supervised\nlearning, one can produce accurate classifications using a significantly\nreduced amount of labelled data. Therefore, semi-supervised learning is\nperfectly suited for medical image classification. However, there has almost\nbeen no uptake of semi-supervised methods in the medical domain. In this work,\nwe propose an all-in-one framework for deep semi-supervised classification\nfocusing on graph based approaches, which up to our knowledge it is the first\ntime that an approach with minimal labels has been shown to such an\nunprecedented scale with medical data. We introduce the concept of hybrid\nmodels by defining a classifier as a combination between an energy-based model\nand a deep net. Our energy functional is built on the Dirichlet energy based on\nthe graph p-Laplacian. Our framework includes energies based on the $\\ell_1$\nand $\\ell_2$ norms. We then connected this energy model to a deep net to\ngenerate a much richer feature space to construct a stronger graph. Our\nframework can be set to be adapted to any complex dataset. We demonstrate,\nthrough extensive numerical comparisons, that our approach readily compete with\nfully-supervised state-of-the-art techniques for the applications of Malaria\nCells, Mammograms and Chest X-ray classification whilst using only 20% of\nlabels.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:18:21 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 18:19:25 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["de Vriendt", "Marianne", ""], ["Sellars", "Philip", ""], ["Aviles-Rivero", "Angelica I", ""]]}, {"id": "2003.06468", "submitter": "Ali Rahmati", "authors": "Ali Rahmati, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard, and\n  Huaiyu Dai", "title": "GeoDA: a geometric framework for black-box adversarial attacks", "comments": "In Proceedings of IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are known as carefully perturbed images fooling image\nclassifiers. We propose a geometric framework to generate adversarial examples\nin one of the most challenging black-box settings where the adversary can only\ngenerate a small number of queries, each of them returning the top-$1$ label of\nthe classifier. Our framework is based on the observation that the decision\nboundary of deep networks usually has a small mean curvature in the vicinity of\ndata samples. We propose an effective iterative algorithm to generate\nquery-efficient black-box perturbations with small $\\ell_p$ norms for $p \\ge\n1$, which is confirmed via experimental evaluations on state-of-the-art natural\nimage classifiers. Moreover, for $p=2$, we theoretically show that our\nalgorithm actually converges to the minimal $\\ell_2$-perturbation when the\ncurvature of the decision boundary is bounded. We also obtain the optimal\ndistribution of the queries over the iterations of the algorithm. Finally,\nexperimental results confirm that our principled black-box attack algorithm\nperforms better than state-of-the-art algorithms as it generates smaller\nperturbations with a reduced number of queries.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 20:03:01 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Rahmati", "Ali", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""], ["Dai", "Huaiyu", ""]]}, {"id": "2003.06472", "submitter": "Binod Bhattarai", "authors": "Binod Bhattarai and Tae-Kyun Kim", "title": "Inducing Optimal Attribute Representations for Conditional GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional GANs are widely used in translating an image from one category to\nanother. Meaningful conditions to GANs provide greater flexibility and control\nover the nature of the target domain synthetic data. Existing conditional GANs\ncommonly encode target domain label information as hard-coded categorical\nvectors in the form of 0s and 1s. The major drawbacks of such representations\nare inability to encode the high-order semantic information of target\ncategories and their relative dependencies. We propose a novel end-to-end\nlearning framework with Graph Convolutional Networks to learn the attribute\nrepresentations to condition on the generator. The GAN losses, i.e. the\ndiscriminator and attribute classification losses, are fed back to the Graph\nresulting in the synthetic images that are more natural and clearer in\nattributes. Moreover, prior-arts are given priorities to condition on the\ngenerator side, not on the discriminator side of GANs. We apply the conditions\nto the discriminator side as well via multi-task learning. We enhanced the four\nstate-of-the art cGANs architectures: Stargan, Stargan-JNT, AttGAN and STGAN.\nOur extensive qualitative and quantitative evaluations on challenging face\nattributes manipulation data set, CelebA, LFWA, and RaFD, show that the cGANs\nenhanced by our methods outperform by a large margin, compared to their\ncounter-parts and other conditioning methods, in terms of both target\nattributes recognition rates and quality measures such as PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 20:24:07 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 13:36:11 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2003.06473", "submitter": "Sifei Liu", "authors": "Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani,\n  Ming-Hsuan Yang, Jan Kautz", "title": "Self-supervised Single-view 3D Reconstruction via Semantic Consistency", "comments": "Codes and other resources will be released at:\n  https://sites.google.com/view/unsup-mesh/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn a self-supervised, single-view 3D reconstruction model that predicts\nthe 3D mesh shape, texture and camera pose of a target object with a collection\nof 2D images and silhouettes. The proposed method does not necessitate 3D\nsupervision, manually annotated keypoints, multi-view images of an object or a\nprior 3D template. The key insight of our work is that objects can be\nrepresented as a collection of deformable parts, and each part is semantically\ncoherent across different instances of the same category (e.g., wings on birds\nand wheels on cars). Therefore, by leveraging self-supervisedly learned part\nsegmentation of a large collection of category-specific images, we can\neffectively enforce semantic consistency between the reconstructed meshes and\nthe original images. This significantly reduces ambiguities during joint\nprediction of shape and camera pose of an object, along with texture. To the\nbest of our knowledge, we are the first to try and solve the single-view\nreconstruction problem without a category-specific template mesh or semantic\nkeypoints. Thus our model can easily generalize to various object categories\nwithout such labels, e.g., horses, penguins, etc. Through a variety of\nexperiments on several categories of deformable and rigid objects, we\ndemonstrate that our unsupervised method performs comparably if not better than\nexisting category-specific reconstruction methods learned with supervision.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 20:29:01 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Xueting", ""], ["Liu", "Sifei", ""], ["Kim", "Kihwan", ""], ["De Mello", "Shalini", ""], ["Jampani", "Varun", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "2003.06486", "submitter": "Bingjiang Qiu", "authors": "Bingjiang Qiu, Jiapan Guo, Joep Kraeima, Haye H. Glas, Ronald J. H.\n  Borra, Max J. H. Witjes, Peter M. A. van Ooijen", "title": "Recurrent convolutional neural networks for mandible segmentation from\n  computed tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, accurate mandible segmentation in CT scans based on deep learning\nmethods has attracted much attention. However, there still exist two major\nchallenges, namely, metal artifacts among mandibles and large variations in\nshape or size among individuals. To address these two challenges, we propose a\nrecurrent segmentation convolutional neural network (RSegCNN) that embeds\nsegmentation convolutional neural network (SegCNN) into the recurrent neural\nnetwork (RNN) for robust and accurate segmentation of the mandible. Such a\ndesign of the system takes into account the similarity and continuity of the\nmandible shapes captured in adjacent image slices in CT scans. The RSegCNN\ninfers the mandible information based on the recurrent structure with the\nembedded encoder-decoder segmentation (SegCNN) components. The recurrent\nstructure guides the system to exploit relevant and important information from\nadjacent slices, while the SegCNN component focuses on the mandible shapes from\na single CT slice. We conducted extensive experiments to evaluate the proposed\nRSegCNN on two head and neck CT datasets. The experimental results show that\nthe RSegCNN is significantly better than the state-of-the-art models for\naccurate mandible segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 21:11:28 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Qiu", "Bingjiang", ""], ["Guo", "Jiapan", ""], ["Kraeima", "Joep", ""], ["Glas", "Haye H.", ""], ["Borra", "Ronald J. H.", ""], ["Witjes", "Max J. H.", ""], ["van Ooijen", "Peter M. A.", ""]]}, {"id": "2003.06498", "submitter": "Andrea Zunino", "authors": "Andrea Zunino, Sarah Adel Bargal, Riccardo Volpi, Mehrnoosh Sameki,\n  Jianming Zhang, Stan Sclaroff, Vittorio Murino, Kate Saenko", "title": "Explainable Deep Classification Models for Domain Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventionally, AI models are thought to trade off explainability for lower\naccuracy. We develop a training strategy that not only leads to a more\nexplainable AI system for object classification, but as a consequence, suffers\nno perceptible accuracy degradation. Explanations are defined as regions of\nvisual evidence upon which a deep classification network makes a decision. This\nis represented in the form of a saliency map conveying how much each pixel\ncontributed to the network's decision. Our training strategy enforces a\nperiodic saliency-based feedback to encourage the model to focus on the image\nregions that directly correspond to the ground-truth object. We quantify\nexplainability using an automated metric, and using human judgement. We propose\nexplainability as a means for bridging the visual-semantic gap between\ndifferent domains where model explanations are used as a means of disentagling\ndomain specific information from otherwise relevant features. We demonstrate\nthat this leads to improved generalization to new domains without hindering\nperformance on the original domain.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 22:22:15 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zunino", "Andrea", ""], ["Bargal", "Sarah Adel", ""], ["Volpi", "Riccardo", ""], ["Sameki", "Mehrnoosh", ""], ["Zhang", "Jianming", ""], ["Sclaroff", "Stan", ""], ["Murino", "Vittorio", ""], ["Saenko", "Kate", ""]]}, {"id": "2003.06513", "submitter": "Yifan Gong", "authors": "Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao\n  Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, and Yanzhi Wang", "title": "A Privacy-Preserving-Oriented DNN Pruning and Mobile Acceleration\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning of deep neural networks (DNNs) has been proposed to satisfy\nthe limited storage and computing capability of mobile edge devices. However,\nprevious pruning methods mainly focus on reducing the model size and/or\nimproving performance without considering the privacy of user data. To mitigate\nthis concern, we propose a privacy-preserving-oriented pruning and mobile\nacceleration framework that does not require the private training dataset. At\nthe algorithm level of the proposed framework, a systematic weight pruning\ntechnique based on the alternating direction method of multipliers (ADMM) is\ndesigned to iteratively solve the pattern-based pruning problem for each layer\nwith randomly generated synthetic data. In addition, corresponding\noptimizations at the compiler level are leveraged for inference accelerations\non devices. With the proposed framework, users could avoid the time-consuming\npruning process for non-experts and directly benefit from compressed models.\nExperimental results show that the proposed framework outperforms three\nstate-of-art end-to-end DNN frameworks, i.e., TensorFlow-Lite, TVM, and MNN,\nwith speedup up to 4.2X, 2.5X, and 2.0X, respectively, with almost no accuracy\nloss, while preserving data privacy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 23:52:03 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 00:45:39 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Gong", "Yifan", ""], ["Zhan", "Zheng", ""], ["Li", "Zhengang", ""], ["Niu", "Wei", ""], ["Ma", "Xiaolong", ""], ["Wang", "Wenhao", ""], ["Ren", "Bin", ""], ["Ding", "Caiwen", ""], ["Lin", "Xue", ""], ["Xu", "Xiaolin", ""], ["Wang", "Yanzhi", ""]]}, {"id": "2003.06515", "submitter": "Deeksha Dixit", "authors": "Deeksha Dixit, Surabhi Verma, Pratap Tokekar", "title": "Evaluation of Cross-View Matching to Improve Ground Vehicle Localization\n  with Aerial Perception", "comments": "8 pages, 12 figures, submitted to IEEE Robotics and Automation\n  Letters (RA-L) with the International Conference on Robotics and Automation\n  (ICRA 2021) presentation option", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-view matching refers to the problem of finding the closest match for a\ngiven query ground view image to one from a database of aerial images. If the\naerial images are geotagged, then the closest matching aerial image can be used\nto localize the query ground view image. Due to the recent success of deep\nlearning methods, several cross-view matching techniques have been proposed.\nThese approaches perform well for the matching of isolated query images.\nHowever, their evaluation over a trajectory is limited. In this paper, we\nevaluate cross-view matching for the task of localizing a ground vehicle over a\nlonger trajectory. We treat these cross-view matches as sensor measurements\nthat are fused using a particle filter. We evaluate the performance of this\nmethod using a city-wide dataset collected in a photorealistic simulation by\nvarying four parameters: height of aerial images, the pitch of the aerial\ncamera mount, FOV of the ground camera, and the methodology of fusing\ncross-view measurements in the particle filter. We also report the results\nobtained using our pipeline on a real-world dataset collected using Google\nStreet View and satellite view APIs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 23:59:07 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 00:56:39 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 21:26:47 GMT"}, {"version": "v4", "created": "Sun, 15 Nov 2020 22:20:51 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dixit", "Deeksha", ""], ["Verma", "Surabhi", ""], ["Tokekar", "Pratap", ""]]}, {"id": "2003.06518", "submitter": "Jie Ying Wu", "authors": "Jie Ying Wu, Peter Kazanzides, Mathias Unberath", "title": "Leveraging Vision and Kinematics Data to Improve Realism of Biomechanic\n  Soft-tissue Simulation for Robotic Surgery", "comments": "12 pages, 4 figures, to be published in IJCARS IPCAI special edition\n  2020", "journal-ref": null, "doi": "10.1007/s11548-020-02139-6", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose Surgical simulations play an increasingly important role in surgeon\neducation and developing algorithms that enable robots to perform surgical\nsubtasks. To model anatomy, Finite Element Method (FEM) simulations have been\nheld as the gold standard for calculating accurate soft-tissue deformation.\nUnfortunately, their accuracy is highly dependent on the simulation parameters,\nwhich can be difficult to obtain.\n  Methods In this work, we investigate how live data acquired during any\nrobotic endoscopic surgical procedure may be used to correct for inaccurate FEM\nsimulation results. Since FEMs are calculated from initial parameters and\ncannot directly incorporate observations, we propose to add a correction factor\nthat accounts for the discrepancy between simulation and observations. We train\na network to predict this correction factor.\n  Results To evaluate our method, we use an open-source da Vinci Surgical\nSystem to probe a soft-tissue phantom and replay the interaction in simulation.\nWe train the network to correct for the difference between the predicted mesh\nposition and the measured point cloud. This results in 15-30% improvement in\nthe mean distance, demonstrating the effectiveness of our approach across a\nlarge range of simulation parameters.\n  Conclusion We show a first step towards a framework that synergistically\ncombines the benefits of model-based simulation and real-time observations. It\ncorrects discrepancies between simulation and the scene that results from\ninaccurate modeling parameters. This can provide a more accurate simulation\nenvironment for surgeons and better data with which to train algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 00:16:08 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wu", "Jie Ying", ""], ["Kazanzides", "Peter", ""], ["Unberath", "Mathias", ""]]}, {"id": "2003.06520", "submitter": "Zhelun Wu", "authors": "Zhelun Wu, Hongyan Jiang, Siyun He", "title": "Symmetry Detection of Occluded Point Cloud Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry detection has been a classical problem in computer graphics, many of\nwhich using traditional geometric methods. In recent years, however, we have\nwitnessed the arising deep learning changed the landscape of computer graphics.\nIn this paper, we aim to solve the symmetry detection of the occluded point\ncloud in a deep-learning fashion. To the best of our knowledge, we are the\nfirst to utilize deep learning to tackle such a problem. In such a deep\nlearning framework, double supervisions: points on the symmetry plane and\nnormal vectors are employed to help us pinpoint the symmetry plane. We\nconducted experiments on the YCB- video dataset and demonstrate the efficacy of\nour method.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 00:23:58 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wu", "Zhelun", ""], ["Jiang", "Hongyan", ""], ["He", "Siyun", ""]]}, {"id": "2003.06523", "submitter": "Riccardo Marin", "authors": "Riccardo Marin, Arianna Rampini, Umberto Castellani, Emanuele\n  Rodol\\`a, Maks Ovsjanikov, Simone Melzi", "title": "Instant recovery of shape from spectrum via latent space connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first learning-based method for recovering shapes from\nLaplacian spectra. Given an auto-encoder, our model takes the form of a\ncycle-consistent module to map latent vectors to sequences of eigenvalues. This\nmodule provides an efficient and effective linkage between spectrum and\ngeometry of a given shape. Our data-driven approach replaces the need for\nad-hoc regularizers required by prior methods, while providing more accurate\nresults at a fraction of the computational cost. Our learning model applies\nwithout modifications across different dimensions (2D and 3D shapes alike),\nrepresentations (meshes, contours and point clouds), as well as across\ndifferent shape classes, and admits arbitrary resolution of the input spectrum\nwithout affecting complexity. The increased flexibility allows us to provide a\nproxy to differentiable eigendecomposition and to address notoriously difficult\ntasks in 3D vision and geometry processing within a unified framework,\nincluding shape generation from spectrum, mesh super-resolution, shape\nexploration, style transfer, spectrum estimation from point clouds,\nsegmentation transfer and point-to-point matching.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 00:48:34 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 08:49:17 GMT"}, {"version": "v3", "created": "Sun, 19 Apr 2020 12:48:38 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 21:53:40 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Marin", "Riccardo", ""], ["Rampini", "Arianna", ""], ["Castellani", "Umberto", ""], ["Rodol\u00e0", "Emanuele", ""], ["Ovsjanikov", "Maks", ""], ["Melzi", "Simone", ""]]}, {"id": "2003.06529", "submitter": "Qian Zhang", "authors": "Xinyi Zeng, Qian Zhang, Jia Chen, Guixu Zhang, Aimin Zhou and Yiqin\n  Wang", "title": "Boundary Guidance Hierarchical Network for Real-Time Tongue Segmentation", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated tongue image segmentation in tongue images is a challenging task\nfor two reasons: 1) there are many pathological details on the tongue surface,\nwhich affect the extraction of the boundary; 2) the shapes of the tongues\ncaptured from various persons (with different diseases) are quite different. To\ndeal with the challenge, a novel end-to-end Boundary Guidance Hierarchical\nNetwork (BGHNet) with a new hybrid loss is proposed in this paper. In the new\napproach, firstly Context Feature Encoder Module (CFEM) is built upon the\nbottomup pathway to confront with the shrinkage of the receptive field.\nSecondly, a novel hierarchical recurrent feature fusion module (HRFFM) is adopt\nto progressively and hierarchically refine object maps to recover image details\nby integrating local context information. Finally, the proposed hybrid loss in\na four hierarchy-pixel, patch, map and boundary guides the network to\neffectively segment the tongue regions and accurate tongue boundaries. BGHNet\nis applied to a set of tongue images. The experimental results suggest that the\nproposed approach can achieve the latest tongue segmentation performance. And\nin the meantime, the lightweight network contains only 15.45M parameters and\nperforms only 11.22GFLOPS.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 01:25:46 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zeng", "Xinyi", ""], ["Zhang", "Qian", ""], ["Chen", "Jia", ""], ["Zhang", "Guixu", ""], ["Zhou", "Aimin", ""], ["Wang", "Yiqin", ""]]}, {"id": "2003.06534", "submitter": "Liang Lin", "authors": "Junfan Lin and Ziliang Chen and Xiaodan Liang and Keze Wang and Liang\n  Lin", "title": "Learning Reinforced Agents with Counterfactual Simulation for Medical\n  Automatic Diagnosis", "comments": "Submitted to TPAMI 2020. In the experiments, our trained agent\n  achieves the new state-of-the-art under various experimental settings and\n  possesses the advantage of sample-efficiency and robustness compared to other\n  existing MAD methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical automatic diagnosis (MAD) aims to learn an agent that mimics the\nbehavior of human doctors, i.e. inquiring symptoms and informing diseases. Due\nto medical ethics concerns, it is impractical to directly apply reinforcement\nlearning techniques to MAD, e.g., training a reinforced agent with human\npatients. Developing a patient simulator by using the collected patient-doctor\ndialogue records has been proposed as a promising workaround to MAD. However,\nmost of these existing works overlook the causal relationship between patient\nsymptoms and diseases. For example, these simulators simply generate the\n\"not-sure\" response to the symptom inquiry if the symptom was not observed in\nthe dialogue record. Consequently, the MAD agent is usually trained without\nexploiting the counterfactual reasoning beyond the factual observations. To\naddress this problem, this paper presents a propensity-based patient simulator\n(PBPS), which is capable of facilitating the training of MAD agents by\ngenerating informative counterfactual answers along with the disease diagnosis.\nSpecifically, our PBPS estimates the propensity score of each record with the\npatient-doctor dialogue reasoning, and can thus generate the counterfactual\nanswers by searching across records. That is, the unrecorded symptom for one\npatient can be found in the records of other patients according to the\npropensity score matching. The informative and causal-aware responses from PBPS\nare beneficial for modeling diagnostic confidence. To this end, we also propose\na progressive assurance agent~(P2A) trained with PBPS, which includes two\nseparate yet cooperative branches accounting for the execution of\nsymptom-inquiry and disease-diagnosis actions, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 02:05:54 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:54:26 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Lin", "Junfan", ""], ["Chen", "Ziliang", ""], ["Liang", "Xiaodan", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "2003.06535", "submitter": "Bingtao Ma", "authors": "Bingtao Ma and Hongsen Liu and Liangliang Nan and Yang Cong", "title": "An End-to-End Geometric Deficiency Elimination Algorithm for 3D Meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 3D mesh is an important representation of geometric data. In the\ngeneration of mesh data, geometric deficiencies (e.g., duplicate elements,\ndegenerate faces, isolated vertices, self-intersection, and inner faces) are\nunavoidable and may violate the topology structure of an object. In this paper,\nwe propose an effective and efficient geometric deficiency elimination\nalgorithm for 3D meshes. Specifically, duplicate elements can be eliminated by\nassessing the occurrence times of vertices or faces; degenerate faces can be\nremoved according to the outer product of two edges; since isolated vertices do\nnot appear in any face vertices, they can be deleted directly;\nself-intersecting faces are detected using an AABB tree and remeshed afterward;\nby simulating whether multiple random rays that shoot from a face can reach\ninfinity, we can judge whether the surface is an inner face, then decide to\ndelete it or not. Experiments on ModelNet40 dataset illustrate that our method\ncan eliminate the deficiencies of the 3D mesh thoroughly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 02:31:58 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 02:55:59 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Ma", "Bingtao", ""], ["Liu", "Hongsen", ""], ["Nan", "Liangliang", ""], ["Cong", "Yang", ""]]}, {"id": "2003.06537", "submitter": "Tian Zheng", "authors": "Lei Han, Tian Zheng, Lan Xu, Lu Fang", "title": "OccuSeg: Occupancy-aware 3D Instance Segmentation", "comments": "CVPR 2020, video this https URL https://youtu.be/co7y6LQ7Kqc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D instance segmentation, with a variety of applications in robotics and\naugmented reality, is in large demands these days. Unlike 2D images that are\nprojective observations of the environment, 3D models provide metric\nreconstruction of the scenes without occlusion or scale ambiguity. In this\npaper, we define \"3D occupancy size\", as the number of voxels occupied by each\ninstance. It owns advantages of robustness in prediction, on which basis,\nOccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our\nmulti-task learning produces both occupancy signal and embedding\nrepresentations, where the training of spatial and feature embeddings varies\nwith their difference in scale-aware. Our clustering scheme benefits from the\nreliable comparison between the predicted occupancy size and the clustered\noccupancy size, which encourages hard samples being correctly clustered and\navoids over segmentation. The proposed approach achieves state-of-the-art\nperformance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while\nmaintaining high efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 02:48:55 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 12:55:01 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 07:29:53 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Han", "Lei", ""], ["Zheng", "Tian", ""], ["Xu", "Lan", ""], ["Fang", "Lu", ""]]}, {"id": "2003.06555", "submitter": "Xiaogang Xu Mr.", "authors": "Xiaogang Xu, Hengshuang Zhao, Jiaya Jia", "title": "Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic\n  Segmentation", "comments": "in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training is promising for improving robustness of deep neural\nnetworks towards adversarial perturbation, especially on classification tasks.\nEffect of this type of training on semantic segmentation, contrarily, just\ncommences. We make the initial attempt to explore the defense strategy on\nsemantic segmentation by formulating a general adversarial training procedure\nthat can perform decently on both adversarial and clean samples. We propose a\ndynamic divide-and-conquer adversarial training (DDC-AT) strategy to enhance\nthe defense effect, by setting additional branches in the target model during\ntraining, and dealing with pixels with diverse properties towards adversarial\nperturbation. Our dynamical division mechanism divides pixels into multiple\nbranches automatically, achieved by unsupervised learning. Note all these\nadditional branches can be abandoned during inference and thus leave no extra\nparameter and computation cost. Extensive experiments with various segmentation\nmodels are conducted on PASCAL VOC 2012 and Cityscapes datasets, in which\nDDC-AT yields satisfying performance under both white- and black-box attack.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 05:06:49 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xu", "Xiaogang", ""], ["Zhao", "Hengshuang", ""], ["Jia", "Jiaya", ""]]}, {"id": "2003.06566", "submitter": "Puneet Mangla", "authors": "Puneet Mangla, Vedant Singh, Shreyas Jayant Havaldar, Vineeth N\n  Balasubramanian", "title": "On the benefits of defining vicinal distributions in latent space", "comments": "$\\textbf{Best Paper Award}$ at CVPR 2021 Workshop on\n  $\\textbf{Adversarial Machine Learning in Real-World Computer Vision\n  (AML-CV)}$. Also accepted at ICLR 2021 Workshops on $\\textbf{Robust-Reliable\n  Machine Learning}$ (Oral) and $\\textbf{Generalization beyond the training\n  distribution}$ (Abstract)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vicinal risk minimization (VRM) principle is an empirical risk\nminimization (ERM) variant that replaces Dirac masses with vicinal functions.\nThere is strong numerical and theoretical evidence showing that VRM outperforms\nERM in terms of generalization if appropriate vicinal functions are chosen.\nMixup Training (MT), a popular choice of vicinal distribution, improves the\ngeneralization performance of models by introducing globally linear behavior in\nbetween training examples. Apart from generalization, recent works have shown\nthat mixup trained models are relatively robust to input\nperturbations/corruptions and at the same time are calibrated better than their\nnon-mixup counterparts. In this work, we investigate the benefits of defining\nthese vicinal distributions like mixup in latent space of generative models\nrather than in input space itself. We propose a new approach - \\textit{VarMixup\n(Variational Mixup)} - to better sample mixup images by using the latent\nmanifold underlying the data. Our empirical studies on CIFAR-10, CIFAR-100, and\nTiny-ImageNet demonstrate that models trained by performing mixup in the latent\nmanifold learned by VAEs are inherently more robust to various input\ncorruptions/perturbations, are significantly better calibrated, and exhibit\nmore local-linear loss landscapes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 06:45:26 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 06:47:02 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 05:19:45 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mangla", "Puneet", ""], ["Singh", "Vedant", ""], ["Havaldar", "Shreyas Jayant", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2003.06567", "submitter": "Quanming Yao", "authors": "Hui Zhang, Quanming Yao, Mingkun Yang, Yongchao Xu, Xiang Bai", "title": "AutoSTR: Efficient Backbone Search for Scene Text Recognition", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition (STR) is very challenging due to the diversity of text\ninstances and the complexity of scenes. The community has paid increasing\nattention to boost the performance by improving the pre-processing image\nmodule, like rectification and deblurring, or the sequence translator. However,\nanother critical module, i.e., the feature sequence extractor, has not been\nextensively explored. In this work, inspired by the success of neural\narchitecture search (NAS), which can identify better architectures than\nhuman-designed ones, we propose automated STR (AutoSTR) to search\ndata-dependent backbones to boost text recognition performance. First, we\ndesign a domain-specific search space for STR, which contains both choices on\noperations and constraints on the downsampling path. Then, we propose a\ntwo-step search algorithm, which decouples operations and downsampling path,\nfor an efficient search in the given space. Experiments demonstrate that, by\nsearching data-dependent backbones, AutoSTR can outperform the state-of-the-art\napproaches on standard benchmarks with much fewer FLOPS and model parameters.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 06:51:04 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 16:36:10 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhang", "Hui", ""], ["Yao", "Quanming", ""], ["Yang", "Mingkun", ""], ["Xu", "Yongchao", ""], ["Bai", "Xiang", ""]]}, {"id": "2003.06576", "submitter": "Long Chen", "authors": "Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, Yueting\n  Zhuang", "title": "Counterfactual Samples Synthesizing for Robust Visual Question Answering", "comments": "Appear in CVPR 2020; Codes in https://github.com/yanxinzju/CSS-VQA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite Visual Question Answering (VQA) has realized impressive progress over\nthe last few years, today's VQA models tend to capture superficial linguistic\ncorrelations in the train set and fail to generalize to the test set with\ndifferent QA distributions. To reduce the language biases, several recent works\nintroduce an auxiliary question-only model to regularize the training of\ntargeted VQA model, and achieve dominating performance on VQA-CP. However,\nsince the complexity of design, current methods are unable to equip the\nensemble-based models with two indispensable characteristics of an ideal VQA\nmodel: 1) visual-explainable: the model should rely on the right visual regions\nwhen making decisions. 2) question-sensitive: the model should be sensitive to\nthe linguistic variations in question. To this end, we propose a model-agnostic\nCounterfactual Samples Synthesizing (CSS) training scheme. The CSS generates\nnumerous counterfactual training samples by masking critical objects in images\nor words in questions, and assigning different ground-truth answers. After\ntraining with the complementary samples (ie, the original and generated\nsamples), the VQA models are forced to focus on all critical objects and words,\nwhich significantly improves both visual-explainable and question-sensitive\nabilities. In return, the performance of these models is further boosted.\nExtensive ablations have shown the effectiveness of CSS. Particularly, by\nbuilding on top of the model LMH, we achieve a record-breaking performance of\n58.95% on VQA-CP v2, with 6.5% gains.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 08:34:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Long", ""], ["Yan", "Xin", ""], ["Xiao", "Jun", ""], ["Zhang", "Hanwang", ""], ["Pu", "Shiliang", ""], ["Zhuang", "Yueting", ""]]}, {"id": "2003.06583", "submitter": "Qingjie Liu", "authors": "Bin Hou, Qingjie Liu, Heng Wang, and Yunhong Wang", "title": "From W-Net to CDGAN: Bi-temporal Change Detection via Deep Learning\n  Techniques", "comments": "Accept to TGRS", "journal-ref": null, "doi": "10.1109/TGRS.2019.2948659", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional change detection methods usually follow the image differencing,\nchange feature extraction and classification framework, and their performance\nis limited by such simple image domain differencing and also the hand-crafted\nfeatures. Recently, the success of deep convolutional neural networks (CNNs)\nhas widely spread across the whole field of computer vision for their powerful\nrepresentation abilities. In this paper, we therefore address the remote\nsensing image change detection problem with deep learning techniques. We\nfirstly propose an end-to-end dual-branch architecture, termed as the W-Net,\nwith each branch taking as input one of the two bi-temporal images as in the\ntraditional change detection models. In this way, CNN features with more\npowerful representative abilities can be obtained to boost the final detection\nperformance. Also, W-Net performs differencing in the feature domain rather\nthan in the traditional image domain, which greatly alleviates loss of useful\ninformation for determining the changes. Furthermore, by reformulating change\ndetection as an image translation problem, we apply the recently popular\nGenerative Adversarial Network (GAN) in which our W-Net serves as the\nGenerator, leading to a new GAN architecture for change detection which we call\nCDGAN. To train our networks and also facilitate future research, we construct\na large scale dataset by collecting images from Google Earth and provide\ncarefully manually annotated ground truths. Experiments show that our proposed\nmethods can provide fine-grained change detection results superior to the\nexisting state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 09:24:08 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hou", "Bin", ""], ["Liu", "Qingjie", ""], ["Wang", "Heng", ""], ["Wang", "Yunhong", ""]]}, {"id": "2003.06592", "submitter": "Vladimir Ivashkin", "authors": "Vladimir Ivashkin", "title": "Image-to-image Neural Network for Addition and Subtraction of a Pair of\n  Not Very Large Numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking back at the history of calculators, one can see that they become less\nfunctional and more computationally expensive over time. A modern calculator\nruns on a personal computer and is drawn at 60 fps only to help us click a few\ndigits with a mouse pointer. A search engine is often used as a calculator,\nwhich means that nowadays we need the Internet just to add two numbers. In this\npaper, we propose to go further and train a convolutional neural network that\ntakes an image of a simple mathematical expression and generates an image of an\nanswer. This neural calculator works only with pairs of double-digit numbers\nand supports only addition and subtraction. Also, sometimes it makes mistakes.\nWe promise that the proposed calculator is a small step for man, but one giant\nleap for mankind.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 09:59:17 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ivashkin", "Vladimir", ""]]}, {"id": "2003.06594", "submitter": "Yue Hu", "authors": "Yue Hu, Siheng Chen, Ya Zhang, and Xiao Gu", "title": "Collaborative Motion Prediction via Neural Motion Message Passing", "comments": "Accepted by CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion prediction is essential and challenging for autonomous vehicles and\nsocial robots. One challenge of motion prediction is to model the interaction\namong traffic actors, which could cooperate with each other to avoid collisions\nor form groups. To address this challenge, we propose neural motion message\npassing (NMMP) to explicitly model the interaction and learn representations\nfor directed interactions between actors. Based on the proposed NMMP, we design\nthe motion prediction systems for two settings: the pedestrian setting and the\njoint pedestrian and vehicle setting. Both systems share a common pattern: we\nuse an individual branch to model the behavior of a single actor and an\ninteractive branch to model the interaction between actors, while with\ndifferent wrappers to handle the varied input formats and characteristics. The\nexperimental results show that both systems outperform the previous\nstate-of-the-art methods on several existing benchmarks. Besides, we provide\ninterpretability for interaction learning.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 10:12:54 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hu", "Yue", ""], ["Chen", "Siheng", ""], ["Zhang", "Ya", ""], ["Gu", "Xiao", ""]]}, {"id": "2003.06606", "submitter": "Canjie Luo", "authors": "Canjie Luo, Yuanzhi Zhu, Lianwen Jin, Yongpan Wang", "title": "Learn to Augment: Joint Data Augmentation and Network Optimization for\n  Text Recognition", "comments": "Accepted to Proc. IEEE Conf. Comp. Vis. Pattern Recogn. (CVPR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwritten text and scene text suffer from various shapes and distorted\npatterns. Thus training a robust recognition model requires a large amount of\ndata to cover diversity as much as possible. In contrast to data collection and\nannotation, data augmentation is a low cost way. In this paper, we propose a\nnew method for text image augmentation. Different from traditional augmentation\nmethods such as rotation, scaling and perspective transformation, our proposed\naugmentation method is designed to learn proper and efficient data augmentation\nwhich is more effective and specific for training a robust recognizer. By using\na set of custom fiducial points, the proposed augmentation method is flexible\nand controllable. Furthermore, we bridge the gap between the isolated processes\nof data augmentation and network optimization by joint learning. An agent\nnetwork learns from the output of the recognition network and controls the\nfiducial points to generate more proper training samples for the recognition\nnetwork. Extensive experiments on various benchmarks, including regular scene\ntext, irregular scene text and handwritten text, show that the proposed\naugmentation and the joint learning methods significantly boost the performance\nof the recognition networks. A general toolkit for geometric augmentation is\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 11:18:22 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Luo", "Canjie", ""], ["Zhu", "Yuanzhi", ""], ["Jin", "Lianwen", ""], ["Wang", "Yongpan", ""]]}, {"id": "2003.06615", "submitter": "Sakshi Patel", "authors": "Sakshi Patel, Bharath K P and Rajesh Kumar Muthu", "title": "Medical Image Enhancement Using Histogram Processing and Feature\n  Extraction for Cancer Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  MRI (Magnetic Resonance Imaging) is a technique used to analyze and diagnose\nthe problem defined by images like cancer or tumor in a brain. Physicians\nrequire good contrast images for better treatment purpose as it contains\nmaximum information of the disease. MRI images are low contrast images which\nmake diagnoses difficult; hence better localization of image pixels is\nrequired. Histogram Equalization techniques help to enhance the image so that\nit gives an improved visual quality and a well defined problem. The contrast\nand brightness is enhanced in such a way that it does not lose its original\ninformation and the brightness is preserved. We compare the different\nequalization techniques in this paper; the techniques are critically studied\nand elaborated. They are also tabulated to compare various parameters present\nin the image. In addition we have also segmented and extracted the tumor part\nout of the brain using K-means algorithm. For classification and feature\nextraction the method used is Support Vector Machine (SVM). The main goal of\nthis research work is to help the medical field with a light of image\nprocessing.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 12:11:23 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Patel", "Sakshi", ""], ["P", "Bharath K", ""], ["Muthu", "Rajesh Kumar", ""]]}, {"id": "2003.06620", "submitter": "Yang Tang", "authors": "Chaoqiang Zhao, Qiyu Sun, Chongzhen Zhang, Yang Tang, Feng Qian", "title": "Monocular Depth Estimation Based On Deep Learning: An Overview", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": "10.1007/s11431-020-1582-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth information is important for autonomous systems to perceive\nenvironments and estimate their own state. Traditional depth estimation\nmethods, like structure from motion and stereo vision matching, are built on\nfeature correspondences of multiple viewpoints. Meanwhile, the predicted depth\nmaps are sparse. Inferring depth information from a single image (monocular\ndepth estimation) is an ill-posed problem. With the rapid development of deep\nneural networks, monocular depth estimation based on deep learning has been\nwidely studied recently and achieved promising performance in accuracy.\nMeanwhile, dense depth maps are estimated from single images by deep neural\nnetworks in an end-to-end manner. In order to improve the accuracy of depth\nestimation, different kinds of network frameworks, loss functions and training\nstrategies are proposed subsequently. Therefore, we survey the current\nmonocular depth estimation methods based on deep learning in this review.\nInitially, we conclude several widely used datasets and evaluation indicators\nin deep learning-based depth estimation. Furthermore, we review some\nrepresentative existing methods according to different training manners:\nsupervised, unsupervised and semi-supervised. Finally, we discuss the\nchallenges and provide some ideas for future researches in monocular depth\nestimation.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 12:35:34 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 11:41:20 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Zhao", "Chaoqiang", ""], ["Sun", "Qiyu", ""], ["Zhang", "Chongzhen", ""], ["Tang", "Yang", ""], ["Qian", "Feng", ""]]}, {"id": "2003.06630", "submitter": "Li Qiang", "authors": "Qiang Li, Xianming Liu, Kaige Han, Cheng Guo, Xiangyang Ji, and\n  Xiaolin Wu", "title": "Rapid Whole Slide Imaging via Learning-based Two-shot Virtual\n  Autofocusing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole slide imaging (WSI) is an emerging technology for digital pathology.\nThe process of autofocusing is the main influence of the performance of WSI.\nTraditional autofocusing methods either are time-consuming due to repetitive\nmechanical motions, or require additional hardware and thus are not compatible\nto current WSI systems. In this paper, we propose the concept of\n\\textit{virtual autofocusing}, which does not rely on mechanical adjustment to\nconduct refocusing but instead recovers in-focus images in an offline\nlearning-based manner. With the initial focal position, we only perform\ntwo-shot imaging, in contrast traditional methods commonly need to conduct as\nmany as 21 times image shooting in each tile scanning. Considering that the two\ncaptured out-of-focus images retain pieces of partial information about the\nunderlying in-focus image, we propose a U-Net-inspired deep neural network\nbased approach for fusing them into a recovered in-focus image. The proposed\nscheme is fast in tissue slides scanning, enabling a high-throughput generation\nof digital pathology images. Experimental results demonstrate that our scheme\nachieves satisfactory refocusing performance.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 13:40:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Qiang", ""], ["Liu", "Xianming", ""], ["Han", "Kaige", ""], ["Guo", "Cheng", ""], ["Ji", "Xiangyang", ""], ["Wu", "Xiaolin", ""]]}, {"id": "2003.06631", "submitter": "Ruihui Li", "authors": "Chao Huang, Ruihui Li, Xianzhi Li, and Chi-Wing Fu", "title": "Non-Local Part-Aware Point Cloud Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel non-local part-aware deep neural network to\ndenoise point clouds by exploring the inherent non-local self-similarity in 3D\nobjects and scenes. Different from existing works that explore small local\npatches, we design the non-local learning unit (NLU) customized with a graph\nattention module to adaptively capture non-local semantically-related features\nover the entire point cloud. To enhance the denoising performance, we cascade a\nseries of NLUs to progressively distill the noise features from the noisy\ninputs. Further, besides the conventional surface reconstruction loss, we\nformulate a semantic part loss to regularize the predictions towards the\nrelevant parts and enable denoising in a part-aware manner. Lastly, we\nperformed extensive experiments to evaluate our method, both quantitatively and\nqualitatively, and demonstrate its superiority over the state-of-the-arts on\nboth synthetic and real-scanned noisy inputs.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 13:51:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Huang", "Chao", ""], ["Li", "Ruihui", ""], ["Li", "Xianzhi", ""], ["Fu", "Chi-Wing", ""]]}, {"id": "2003.06635", "submitter": "Guansong Lu", "authors": "Guansong Lu, Zhiming Zhou, Jian Shen, Cheng Chen, Weinan Zhang, Yong\n  Yu", "title": "Large-Scale Optimal Transport via Adversarial Training with\n  Cycle-Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in large-scale optimal transport have greatly extended its\napplication scenarios in machine learning. However, existing methods either not\nexplicitly learn the transport map or do not support general cost function. In\nthis paper, we propose an end-to-end approach for large-scale optimal\ntransport, which directly solves the transport map and is compatible with\ngeneral cost function. It models the transport map via stochastic neural\nnetworks and enforces the constraint on the marginal distributions via\nadversarial training. The proposed framework can be further extended towards\nlearning Monge map or optimal bijection via adopting cycle-consistency\nconstraint(s). We verify the effectiveness of the proposed method and\ndemonstrate its superior performance against existing methods with large-scale\nreal-world applications, including domain adaptation, image-to-image\ntranslation, and color transfer.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 14:06:46 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lu", "Guansong", ""], ["Zhou", "Zhiming", ""], ["Shen", "Jian", ""], ["Chen", "Cheng", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "2003.06637", "submitter": "Nantheera Anantrasirichai", "authors": "Nantheera Anantrasirichai and Majid Geravand and David Braendler and\n  David R. Bull", "title": "Fast Depth Estimation for View Synthesis", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disparity/depth estimation from sequences of stereo images is an important\nelement in 3D vision. Owing to occlusions, imperfect settings and homogeneous\nluminance, accurate estimate of depth remains a challenging problem. Targetting\nview synthesis, we propose a novel learning-based framework making use of\ndilated convolution, densely connected convolutional modules, compact decoder\nand skip connections. The network is shallow but dense, so it is fast and\naccurate. Two additional contributions -- a non-linear adjustment of the depth\nresolution and the introduction of a projection loss, lead to reduction of\nestimation error by up to 20% and 25% respectively. The results show that our\nnetwork outperforms state-of-the-art methods with an average improvement in\naccuracy of depth estimation and view synthesis by approximately 45% and 34%\nrespectively. Where our method generates comparable quality of estimated depth,\nit performs 10 times faster than those methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 14:10:42 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Anantrasirichai", "Nantheera", ""], ["Geravand", "Majid", ""], ["Braendler", "David", ""], ["Bull", "David R.", ""]]}, {"id": "2003.06646", "submitter": "Subhajit Chaudhury", "authors": "Subhajit Chaudhury, Toshihiko Yamasaki", "title": "Investigating Generalization in Neural Networks under Optimally Evolved\n  Training Perturbations", "comments": "Accepted at IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the generalization properties of neural networks\nunder input perturbations and show that minimal training data corruption by a\nfew pixel modifications can cause drastic overfitting. We propose an\nevolutionary algorithm to search for optimal pixel perturbations using novel\ncost function inspired from literature in domain adaptation that explicitly\nmaximizes the generalization gap and domain divergence between clean and\ncorrupted images. Our method outperforms previous pixel-based data distribution\nshift methods on state-of-the-art Convolutional Neural Networks (CNNs)\narchitectures. Interestingly, we find that the choice of optimization plays an\nimportant role in generalization robustness due to the empirical observation\nthat SGD is resilient to such training data corruption unlike adaptive\noptimization techniques (ADAM). Our source code is available at\nhttps://github.com/subhajitchaudhury/evo-shift.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 14:38:07 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Yamasaki", "Toshihiko", ""]]}, {"id": "2003.06650", "submitter": "Yixiao Ge", "authors": "Yixiao Ge, Feng Zhu, Rui Zhao, Hongsheng Li", "title": "Structured Domain Adaptation with Online Relation Regularization for\n  Unsupervised Person Re-ID", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) aims at adapting the model trained on a\nlabeled source-domain dataset to an unlabeled target-domain dataset. The task\nof UDA on open-set person re-identification (re-ID) is even more challenging as\nthe identities (classes) do not overlap between the two domains. One major\nresearch direction was based on domain translation, which, however, has fallen\nout of favor in recent years due to inferior performance compared to\npseudo-label-based methods. We argue that translation-based methods have great\npotential on exploiting the valuable source-domain data but they did not\nprovide proper regularization on the translation process. Specifically, these\nmethods only focus on maintaining the identities of the translated images while\nignoring the inter-sample relation during translation. To tackle the challenge,\nwe propose an end-to-end structured domain adaptation framework with an online\nrelation-consistency regularization term. During training, the person feature\nencoder is optimized to model inter-sample relations on-the-fly for supervising\nrelation-consistency domain translation, which in turn, improves the encoder\nwith informative translated images. An improved pseudo-label-based encoder can\ntherefore be obtained by jointly training the source-to-target translated\nimages with ground-truth identities and target-domain images with pseudo\nidentities. In the experiments, our proposed framework is shown to outperform\nstate-of-the-art methods on multiple UDA tasks of person re-ID. Code is\navailable at https://github.com/yxgeee/SDA.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 14:45:18 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 14:00:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ge", "Yixiao", ""], ["Zhu", "Feng", ""], ["Zhao", "Rui", ""], ["Li", "Hongsheng", ""]]}, {"id": "2003.06659", "submitter": "Thomas Kerdreux", "authors": "Thomas Kerdreux and Louis Thiry and Erwan Kerdreux", "title": "Interactive Neural Style Transfer with Artists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present interactive painting processes in which a painter and various\nneural style transfer algorithms interact on a real canvas. Understanding what\nthese algorithms' outputs achieve is then paramount to describe the creative\nagency in our interactive experiments. We gather a set of paired\npainting-pictures images and present a new evaluation methodology based on the\npredictivity of neural style transfer algorithms. We point some algorithms'\ninstabilities and show that they can be used to enlarge the diversity and\npleasing oddity of the images synthesized by the numerous existing neural style\ntransfer algorithms. This diversity of images was perceived as a source of\ninspiration for human painters, portraying the machine as a computational\ncatalyst.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 15:27:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kerdreux", "Thomas", ""], ["Thiry", "Louis", ""], ["Kerdreux", "Erwan", ""]]}, {"id": "2003.06670", "submitter": "Leonid Karlinsky", "authors": "Moshe Lichtenstein and Prasanna Sattigeri and Rogerio Feris and Raja\n  Giryes and Leonid Karlinsky", "title": "TAFSSL: Task-Adaptive Feature Sub-Space Learning for few-shot\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Few-Shot Learning (FSL), or learning from very few (typically\n$1$ or $5$) examples per novel class (unseen during training), has received a\nlot of attention and significant performance advances in the recent literature.\nWhile number of techniques have been proposed for FSL, several factors have\nemerged as most important for FSL performance, awarding SOTA even to the\nsimplest of techniques. These are: the backbone architecture (bigger is\nbetter), type of pre-training on the base classes (meta-training vs regular\nmulti-class, currently regular wins), quantity and diversity of the base\nclasses set (the more the merrier, resulting in richer and better adaptive\nfeatures), and the use of self-supervised tasks during pre-training (serving as\na proxy for increasing the diversity of the base set). In this paper we propose\nyet another simple technique that is important for the few shot learning\nperformance - a search for a compact feature sub-space that is discriminative\nfor a given few-shot test task. We show that the Task-Adaptive Feature\nSub-Space Learning (TAFSSL) can significantly boost the performance in FSL\nscenarios when some additional unlabeled data accompanies the novel few-shot\ntask, be it either the set of unlabeled queries (transductive FSL) or some\nadditional set of unlabeled data samples (semi-supervised FSL). Specifically,\nwe show that on the challenging miniImageNet and tieredImageNet benchmarks,\nTAFSSL can improve the current state-of-the-art in both transductive and\nsemi-supervised FSL settings by more than $5\\%$, while increasing the benefit\nof using unlabeled data in FSL to above $10\\%$ performance gain.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 16:59:17 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lichtenstein", "Moshe", ""], ["Sattigeri", "Prasanna", ""], ["Feris", "Rogerio", ""], ["Giryes", "Raja", ""], ["Karlinsky", "Leonid", ""]]}, {"id": "2003.06692", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Pooja Guhan, Uttaran Bhattacharya, Rohan Chandra,\n  Aniket Bera and Dinesh Manocha", "title": "EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege's\n  Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EmotiCon, a learning-based algorithm for context-aware perceived\nhuman emotion recognition from videos and images. Motivated by Frege's Context\nPrinciple from psychology, our approach combines three interpretations of\ncontext for emotion recognition. Our first interpretation is based on using\nmultiple modalities(e.g. faces and gaits) for emotion recognition. For the\nsecond interpretation, we gather semantic context from the input image and use\na self-attention-based CNN to encode this information. Finally, we use depth\nmaps to model the third interpretation related to socio-dynamic interactions\nand proximity among agents. We demonstrate the efficiency of our network\nthrough experiments on EMOTIC, a benchmark dataset. We report an Average\nPrecision (AP) score of 35.48 across 26 classes, which is an improvement of 7-8\nover prior methods. We also introduce a new dataset, GroupWalk, which is a\ncollection of videos captured in multiple real-world settings of people\nwalking. We report an AP of 65.83 across 4 categories on GroupWalk, which is\nalso an improvement over prior methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 19:55:21 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Mittal", "Trisha", ""], ["Guhan", "Pooja", ""], ["Bhattacharya", "Uttaran", ""], ["Chandra", "Rohan", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2003.06705", "submitter": "Djordje Batic", "authors": "Djordje Batic, Dubravko Culibrk", "title": "Identifying Individual Dogs in Social Media Images", "comments": "Presented at BMVC 2019: Workshop on Visual AI and Entrepreneurship,\n  Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of an initial study focused on developing a visual AI\nsolution able to recognize individual dogs in unconstrained (wild) images\noccurring on social media.\n  The work described here is part of joint project done with Pet2Net, a social\nnetwork focused on pets and their owners. In order to detect and recognize\nindividual dogs we combine transfer learning and object detection approaches on\nInception v3 and SSD Inception v2 architectures respectively and evaluate the\nproposed pipeline using a new data set containing real data that the users\nuploaded to Pet2Net platform. We show that it can achieve 94.59% accuracy in\nidentifying individual dogs. Our approach has been designed with simplicity in\nmind and the goal of easy deployment on all the images uploaded to Pet2Net\nplatform.\n  A purely visual approach to identifying dogs in images, will enhance Pet2Net\nfeatures aimed at finding lost dogs, as well as form the basis of future work\nfocused on identifying social relationships between dogs, which cannot be\ninferred from other data collected by the platform.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 21:11:02 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Batic", "Djordje", ""], ["Culibrk", "Dubravko", ""]]}, {"id": "2003.06711", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera,\n  Dinesh Manocha", "title": "Emotions Don't Lie: An Audio-Visual Deepfake Detection Method Using\n  Affective Cues", "comments": "Accepted to ACMMM-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning-based method for detecting real and fake deepfake\nmultimedia content. To maximize information for learning, we extract and\nanalyze the similarity between the two audio and visual modalities from within\nthe same video. Additionally, we extract and compare affective cues\ncorresponding to perceived emotion from the two modalities within a video to\ninfer whether the input video is \"real\" or \"fake\". We propose a deep learning\nnetwork, inspired by the Siamese network architecture and the triplet loss. To\nvalidate our model, we report the AUC metric on two large-scale deepfake\ndetection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach\nwith several SOTA deepfake detection methods and report per-video AUC of 84.4%\non the DFDC and 96.6% on the DF-TIMIT datasets, respectively. To the best of\nour knowledge, ours is the first approach that simultaneously exploits audio\nand video modalities and also perceived emotions from the two modalities for\ndeepfake detection.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 22:07:26 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 16:46:06 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 20:43:34 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Mittal", "Trisha", ""], ["Bhattacharya", "Uttaran", ""], ["Chandra", "Rohan", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2003.06722", "submitter": "Fariba Zohrizadeh", "authors": "Mohsen Kheirandishfard, Fariba Zohrizadeh, Farhad Kamangar", "title": "Class Conditional Alignment for Partial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial adaptation models have demonstrated significant progress towards\ntransferring knowledge from a labeled source dataset to an unlabeled target\ndataset. Partial domain adaptation (PDA) investigates the scenarios in which\nthe source domain is large and diverse, and the target label space is a subset\nof the source label space. The main purpose of PDA is to identify the shared\nclasses between the domains and promote learning transferable knowledge from\nthese classes. In this paper, we propose a multi-class adversarial architecture\nfor PDA. The proposed approach jointly aligns the marginal and\nclass-conditional distributions in the shared label space by minimaxing a novel\nmulti-class adversarial loss function. Furthermore, we incorporate effective\nregularization terms to encourage selecting the most relevant subset of source\ndomain classes. In the absence of target labels, the proposed approach is able\nto effectively learn domain-invariant feature representations, which in turn\ncan enhance the classification performance in the target domain. Comprehensive\nexperiments on three benchmark datasets Office-31, Office-Home, and\nCaltech-Office corroborate the effectiveness of the proposed approach in\naddressing different partial transfer learning tasks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 23:51:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kheirandishfard", "Mohsen", ""], ["Zohrizadeh", "Fariba", ""], ["Kamangar", "Farhad", ""]]}, {"id": "2003.06729", "submitter": "I. Zeki Yalniz", "authors": "Karishma Sharma, Pinar Donmez, Enming Luo, Yan Liu, I. Zeki Yalniz", "title": "NoiseRank: Unsupervised Label Noise Reduction with Dependence Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise is increasingly prevalent in datasets acquired from noisy\nchannels. Existing approaches that detect and remove label noise generally rely\non some form of supervision, which is not scalable and error-prone. In this\npaper, we propose NoiseRank, for unsupervised label noise reduction using\nMarkov Random Fields (MRF). We construct a dependence model to estimate the\nposterior probability of an instance being incorrectly labeled given the\ndataset, and rank instances based on their estimated probabilities. Our method\n1) Does not require supervision from ground-truth labels, or priors on label or\nnoise distribution. 2) It is interpretable by design, enabling transparency in\nlabel noise removal. 3) It is agnostic to classifier architecture/optimization\nframework and content modality. These advantages enable wide applicability in\nreal noise settings, unlike prior works constrained by one or more conditions.\nNoiseRank improves state-of-the-art classification on Food101-N (~20% noise),\nand is effective on high noise Clothing-1M (~40% noise).\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 01:10:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sharma", "Karishma", ""], ["Donmez", "Pinar", ""], ["Luo", "Enming", ""], ["Liu", "Yan", ""], ["Yalniz", "I. Zeki", ""]]}, {"id": "2003.06731", "submitter": "Sudarshan Ramenahalli", "authors": "Sudarshan Ramenahalli", "title": "A model of figure ground organization incorporating local and global\n  cues", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Figure Ground Organization (FGO) -- inferring spatial depth ordering of\nobjects in a visual scene -- involves determining which side of an occlusion\nboundary is figure (closer to the observer) and which is ground (further away\nfrom the observer). A combination of global cues, like convexity, and local\ncues, like T-junctions are involved in this process. We present a biologically\nmotivated, feed forward computational model of FGO incorporating convexity,\nsurroundedness, parallelism as global cues and Spectral Anisotropy (SA),\nT-junctions as local cues. While SA is computed in a biologically plausible\nmanner, the inclusion of T-Junctions is biologically motivated. The model\nconsists of three independent feature channels, Color, Intensity and\nOrientation, but SA and T-Junctions are introduced only in the Orientation\nchannel as these properties are specific to that feature of objects. We study\nthe effect of adding each local cue independently and both of them\nsimultaneously to the model with no local cues. We evaluate model performance\nbased on figure-ground classification accuracy (FGCA) at every border location\nusing the BSDS 300 figure-ground dataset. Each local cue, when added alone,\ngives statistically significant improvement in the FGCA of the model suggesting\nits usefulness as an independent FGO cue. The model with both local cues\nachieves higher FGCA than the models with individual cues, indicating SA and\nT-Junctions are not mutually contradictory. Compared to the model with no local\ncues, the feed-forward model with both local cues achieves $\\geq 8.78$%\nimprovement in terms of FGCA.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 01:18:40 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ramenahalli", "Sudarshan", ""]]}, {"id": "2003.06734", "submitter": "Youssef Zaky", "authors": "Youssef Zaky, Gaurav Paruthi, Bryan Tripp, James Bergstra", "title": "Active Perception and Representation for Robotic Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of visual animals actively control their eyes, heads,\nand/or bodies to direct their gaze toward different parts of their environment.\nIn contrast, recent applications of reinforcement learning in robotic\nmanipulation employ cameras as passive sensors. These are carefully placed to\nview a scene from a fixed pose. Active perception allows animals to gather the\nmost relevant information about the world and focus their computational\nresources where needed. It also enables them to view objects from different\ndistances and viewpoints, providing a rich visual experience from which to\nlearn abstract representations of the environment. Inspired by the primate\nvisual-motor system, we present a framework that leverages the benefits of\nactive perception to accomplish manipulation tasks. Our agent uses viewpoint\nchanges to localize objects, to learn state representations in a\nself-supervised manner, and to perform goal-directed actions. We apply our\nmodel to a simulated grasping task with a 6-DoF action space. Compared to its\npassive, fixed-camera counterpart, the active model achieves 8% better\nperformance in targeted grasping. Compared to vanilla deep Q-learning\nalgorithms, our model is at least four times more sample-efficient,\nhighlighting the benefits of both active perception and representation\nlearning.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 01:43:51 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zaky", "Youssef", ""], ["Paruthi", "Gaurav", ""], ["Tripp", "Bryan", ""], ["Bergstra", "James", ""]]}, {"id": "2003.06745", "submitter": "Yi Zhu", "authors": "Yi Zhu, Fengda Zhu, Zhaohuan Zhan, Bingqian Lin, Jianbin Jiao, Xiaojun\n  Chang, Xiaodan Liang", "title": "Vision-Dialog Navigation by Exploring Cross-modal Memory", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-dialog navigation posed as a new holy-grail task in vision-language\ndisciplinary targets at learning an agent endowed with the capability of\nconstant conversation for help with natural language and navigating according\nto human responses. Besides the common challenges faced in visual language\nnavigation, vision-dialog navigation also requires to handle well with the\nlanguage intentions of a series of questions about the temporal context from\ndialogue history and co-reasoning both dialogs and visual scenes. In this\npaper, we propose the Cross-modal Memory Network (CMN) for remembering and\nunderstanding the rich information relevant to historical navigation actions.\nOur CMN consists of two memory modules, the language memory module (L-mem) and\nthe visual memory module (V-mem). Specifically, L-mem learns latent\nrelationships between the current language interaction and a dialog history by\nemploying a multi-head attention mechanism. V-mem learns to associate the\ncurrent visual views and the cross-modal memory about the previous navigation\nactions. The cross-modal memory is generated via a vision-to-language attention\nand a language-to-vision attention. Benefiting from the collaborative learning\nof the L-mem and the V-mem, our CMN is able to explore the memory about the\ndecision making of historical navigation actions which is for the current step.\nExperiments on the CVDN dataset show that our CMN outperforms the previous\nstate-of-the-art model by a significant margin on both seen and unseen\nenvironments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 03:08:06 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zhu", "Yi", ""], ["Zhu", "Fengda", ""], ["Zhan", "Zhaohuan", ""], ["Lin", "Bingqian", ""], ["Jiao", "Jianbin", ""], ["Chang", "Xiaojun", ""], ["Liang", "Xiaodan", ""]]}, {"id": "2003.06746", "submitter": "Yan Hong", "authors": "Yan Hong, Li Niu, Jianfu Zhang, Liqing Zhang", "title": "Beyond without Forgetting: Multi-Task Learning for Classification with\n  Disjoint Datasets", "comments": "This paper is accepted by ICME 2020(http://www.2020.ieeeicme.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task Learning (MTL) for classification with disjoint datasets aims to\nexplore MTL when one task only has one labeled dataset. In existing methods,\nfor each task, the unlabeled datasets are not fully exploited to facilitate\nthis task. Inspired by semi-supervised learning, we use unlabeled datasets with\npseudo labels to facilitate each task. However, there are two major issues: 1)\nthe pseudo labels are very noisy; 2) the unlabeled datasets and the labeled\ndataset for each task has considerable data distribution mismatch. To address\nthese issues, we propose our MTL with Selective Augmentation (MTL-SA) method to\nselect the training samples in unlabeled datasets with confident pseudo labels\nand close data distribution to the labeled dataset. Then, we use the selected\ntraining samples to add information and use the remaining training samples to\npreserve information. Extensive experiments on face-centric and human-centric\napplications demonstrate the effectiveness of our MTL-SA method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 03:19:18 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hong", "Yan", ""], ["Niu", "Li", ""], ["Zhang", "Jianfu", ""], ["Zhang", "Liqing", ""]]}, {"id": "2003.06748", "submitter": "Qingchao Zhang", "authors": "Qingchao Zhang, Xiaojing Ye, Hongcheng Liu, and Yunmei Chen", "title": "A Novel Learnable Gradient Descent Type Algorithm for Non-convex\n  Non-smooth Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization algorithms for solving nonconvex inverse problem have attracted\nsignificant interests recently. However, existing methods require the nonconvex\nregularization to be smooth or simple to ensure convergence. In this paper, we\npropose a novel gradient descent type algorithm, by leveraging the idea of\nresidual learning and Nesterov's smoothing technique, to solve inverse problems\nconsisting of general nonconvex and nonsmooth regularization with provable\nconvergence. Moreover, we develop a neural network architecture intimating this\nalgorithm to learn the nonlinear sparsity transformation adaptively from\ntraining data, which also inherits the convergence to accommodate the general\nnonconvex structure of this learned transformation. Numerical results\ndemonstrate that the proposed network outperforms the state-of-the-art methods\non a variety of different image reconstruction problems in terms of efficiency\nand accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 03:44:43 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 23:39:46 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Zhang", "Qingchao", ""], ["Ye", "Xiaojing", ""], ["Liu", "Hongcheng", ""], ["Chen", "Yunmei", ""]]}, {"id": "2003.06749", "submitter": "Anwesan Pal", "authors": "Yiding Qiu, Anwesan Pal, Henrik I. Christensen", "title": "Learning hierarchical relationships for object-goal navigation", "comments": "Paper accepted at CoRL-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct search for objects as part of navigation poses a challenge for small\nitems. Utilizing context in the form of object-object relationships enable\nhierarchical search for targets efficiently. Most of the current approaches\ntend to directly incorporate sensory input into a reward-based learning\napproach, without learning about object relationships in the natural\nenvironment, and thus generalize poorly across domains. We present\nMemory-utilized Joint hierarchical Object Learning for Navigation in Indoor\nRooms (MJOLNIR), a target-driven navigation algorithm, which considers the\ninherent relationship between target objects, and the more salient contextual\nobjects occurring in its surrounding. Extensive experiments conducted across\nmultiple environment settings show an $82.9\\%$ and $93.5\\%$ gain over existing\nstate-of-the-art navigation methods in terms of the success rate (SR), and\nsuccess weighted by path length (SPL), respectively. We also show that our\nmodel learns to converge much faster than other algorithms, without suffering\nfrom the well-known overfitting problem. Additional details regarding the\nsupplementary material and code are available at\nhttps://sites.google.com/eng.ucsd.edu/mjolnir.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 04:01:09 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 22:22:11 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Qiu", "Yiding", ""], ["Pal", "Anwesan", ""], ["Christensen", "Henrik I.", ""]]}, {"id": "2003.06752", "submitter": "Liu Liu", "authors": "Liu Liu, Dylan Campbell, Hongdong Li, Dingfu Zhou, Xibin Song and\n  Ruigang Yang", "title": "Learning 2D-3D Correspondences To Solve The Blind Perspective-n-Point\n  Problem", "comments": "A blind-PnP solver", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional absolute camera pose via a Perspective-n-Point (PnP) solver\noften assumes that the correspondences between 2D image pixels and 3D points\nare given. When the correspondences between 2D and 3D points are not known a\npriori, the task becomes the much more challenging blind PnP problem. This\npaper proposes a deep CNN model which simultaneously solves for both the 6-DoF\nabsolute camera pose and 2D--3D correspondences. Our model comprises three\nneural modules connected in sequence. First, a two-stream PointNet-inspired\nnetwork is applied directly to both the 2D image keypoints and the 3D scene\npoints in order to extract discriminative point-wise features harnessing both\nlocal and contextual information. Second, a global feature matching module is\nemployed to estimate a matchability matrix among all 2D--3D pairs. Third, the\nobtained matchability matrix is fed into a classification module to\ndisambiguate inlier matches. The entire network is trained end-to-end, followed\nby a robust model fitting (P3P-RANSAC) at test time only to recover the 6-DoF\ncamera pose. Extensive tests on both real and simulated data have shown that\nour method substantially outperforms existing approaches, and is capable of\nprocessing thousands of points a second with the state-of-the-art accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 04:17:30 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Liu", "Liu", ""], ["Campbell", "Dylan", ""], ["Li", "Hongdong", ""], ["Zhou", "Dingfu", ""], ["Song", "Xibin", ""], ["Yang", "Ruigang", ""]]}, {"id": "2003.06754", "submitter": "Pengxiang Wu", "authors": "Pengxiang Wu, Siheng Chen, Dimitris Metaxas", "title": "MotionNet: Joint Perception and Motion Prediction for Autonomous Driving\n  Based on Bird's Eye View Maps", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to reliably perceive the environmental states, particularly the\nexistence of objects and their motion behavior, is crucial for autonomous\ndriving. In this work, we propose an efficient deep model, called MotionNet, to\njointly perform perception and motion prediction from 3D point clouds.\nMotionNet takes a sequence of LiDAR sweeps as input and outputs a bird's eye\nview (BEV) map, which encodes the object category and motion information in\neach grid cell. The backbone of MotionNet is a novel spatio-temporal pyramid\nnetwork, which extracts deep spatial and temporal features in a hierarchical\nfashion. To enforce the smoothness of predictions over both space and time, the\ntraining of MotionNet is further regularized with novel spatial and temporal\nconsistency losses. Extensive experiments show that the proposed method overall\noutperforms the state-of-the-arts, including the latest scene-flow- and\n3D-object-detection-based methods. This indicates the potential value of the\nproposed method serving as a backup to the bounding-box-based system, and\nproviding complementary information to the motion planner in autonomous\ndriving. Code is available at https://github.com/pxiangwu/MotionNet.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 04:37:12 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wu", "Pengxiang", ""], ["Chen", "Siheng", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "2003.06757", "submitter": "Jinyang Guo", "authors": "Jinyang Guo, Wanli Ouyang, Dong Xu", "title": "Channel Pruning Guided by Classification Loss and Feature Importance", "comments": "AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new layer-by-layer channel pruning method called\nChannel Pruning guided by classification Loss and feature Importance (CPLI). In\ncontrast to the existing layer-by-layer channel pruning approaches that only\nconsider how to reconstruct the features from the next layer, our approach\nadditionally take the classification loss into account in the channel pruning\nprocess. We also observe that some reconstructed features will be removed at\nthe next pruning stage. So it is unnecessary to reconstruct these features. To\nthis end, we propose a new strategy to suppress the influence of unimportant\nfeatures (i.e., the features will be removed at the next pruning stage). Our\ncomprehensive experiments on three benchmark datasets, i.e., CIFAR-10,\nImageNet, and UCF-101, demonstrate the effectiveness of our CPLI method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 05:08:47 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Guo", "Jinyang", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dong", ""]]}, {"id": "2003.06761", "submitter": "Zedu Chen", "authors": "Zedu Chen, Bineng Zhong, Guorong Li, Shengping Zhang, Rongrong Ji", "title": "Siamese Box Adaptive Network for Visual Tracking", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing trackers usually rely on either a multi-scale searching\nscheme or pre-defined anchor boxes to accurately estimate the scale and aspect\nratio of a target. Unfortunately, they typically call for tedious and heuristic\nconfigurations. To address this issue, we propose a simple yet effective visual\ntracking framework (named Siamese Box Adaptive Network, SiamBAN) by exploiting\nthe expressive power of the fully convolutional network (FCN). SiamBAN views\nthe visual tracking problem as a parallel classification and regression\nproblem, and thus directly classifies objects and regresses their bounding\nboxes in a unified FCN. The no-prior box design avoids hyper-parameters\nassociated with the candidate boxes, making SiamBAN more flexible and general.\nExtensive experiments on visual tracking benchmarks including VOT2018, VOT2019,\nOTB100, NFS, UAV123, and LaSOT demonstrate that SiamBAN achieves\nstate-of-the-art performance and runs at 40 FPS, confirming its effectiveness\nand efficiency. The code will be available at https://github.com/hqucv/siamban.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 05:58:12 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 09:59:30 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Chen", "Zedu", ""], ["Zhong", "Bineng", ""], ["Li", "Guorong", ""], ["Zhang", "Shengping", ""], ["Ji", "Rongrong", ""]]}, {"id": "2003.06777", "submitter": "Chi Zhang", "authors": "Chi Zhang, Yujun Cai, Guosheng Lin, Chunhua Shen", "title": "DeepEMD: Differentiable Earth Mover's Distance for Few-Shot Learning", "comments": "Extended version of DeepEMD in CVPR2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has proved to be very effective in learning with a large amount\nof labelled data. Few-shot learning in contrast attempts to learn with only a\nfew labelled data. In this work, we develop methods for few-shot image\nclassification from a new perspective of optimal matching between image\nregions. We employ the Earth Mover's Distance (EMD) as a metric to compute a\nstructural distance between dense image representations to determine image\nrelevance. The EMD generates the optimal matching flows between structural\nelements that have the minimum matching cost, which is used to calculate the\nimage distance for classification. To generate the important weights of\nelements in the EMD formulation, we design a cross-reference mechanism, which\ncan effectively alleviate the adverse impact caused by the cluttered background\nand large intra-class appearance variations. To handle $k$-shot classification,\nwe propose to learn a structured fully connected layer that can directly\nclassify dense image representations with the proposed EMD. Based on the\nimplicit function theorem, the EMD can be inserted as a layer into the network\nfor end-to-end training. Our extensive experiments validate the effectiveness\nof our algorithm which outperforms state-of-the-art methods by a significant\nmargin on four widely used few-shot classification benchmarks, namely,\nminiImageNet, tieredImageNet, Fewshot-CIFAR100 (FC100) and Caltech-UCSD\nBirds-200-2011 (CUB).\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 08:13:16 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 12:56:06 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 02:27:17 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Zhang", "Chi", ""], ["Cai", "Yujun", ""], ["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""]]}, {"id": "2003.06779", "submitter": "Sudarshan Ramenahalli", "authors": "Sudarshan Ramenahalli", "title": "A proto-object based audiovisual saliency map", "comments": "50 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural environment and our interaction with it is essentially multisensory,\nwhere we may deploy visual, tactile and/or auditory senses to perceive, learn\nand interact with our environment. Our objective in this study is to develop a\nscene analysis algorithm using multisensory information, specifically vision\nand audio. We develop a proto-object based audiovisual saliency map (AVSM) for\nthe analysis of dynamic natural scenes. A specialized audiovisual camera with\n$360 \\degree$ Field of View, capable of locating sound direction, is used to\ncollect spatiotemporally aligned audiovisual data. We demonstrate that the\nperformance of proto-object based audiovisual saliency map in detecting and\nlocalizing salient objects/events is in agreement with human judgment. In\naddition, the proto-object based AVSM that we compute as a linear combination\nof visual and auditory feature conspicuity maps captures a higher number of\nvalid salient events compared to unisensory saliency maps. Such an algorithm\ncan be useful in surveillance, robotic navigation, video compression and\nrelated applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 08:34:35 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ramenahalli", "Sudarshan", ""]]}, {"id": "2003.06780", "submitter": "Chunhua Shen", "authors": "Guansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, Xiao Bai", "title": "Self-trained Deep Ordinal Regression for End-to-End Video Anomaly\n  Detection", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video anomaly detection is of critical practical importance to a variety of\nreal applications because it allows human attention to be focused on events\nthat are likely to be of interest, in spite of an otherwise overwhelming volume\nof video. We show that applying self-trained deep ordinal regression to video\nanomaly detection overcomes two key limitations of existing methods, namely, 1)\nbeing highly dependent on manually labeled normal training data; and 2)\nsub-optimal feature learning. By formulating a surrogate two-class ordinal\nregression task we devise an end-to-end trainable video anomaly detection\napproach that enables joint representation learning and anomaly scoring without\nmanually labeled normal/abnormal data. Experiments on eight real-world video\nscenes show that our proposed method outperforms state-of-the-art methods that\nrequire no labeled training data by a substantial margin, and enables easy and\naccurate localization of the identified anomalies. Furthermore, we demonstrate\nthat our method offers effective human-in-the-loop anomaly detection which can\nbe critical in applications where anomalies are rare and the false-negative\ncost is high.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 08:44:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Pang", "Guansong", ""], ["Yan", "Cheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Bai", "Xiao", ""]]}, {"id": "2003.06788", "submitter": "Yahui Liu", "authors": "Yahui Liu, Marco De Nadai, Jian Yao, Nicu Sebe, Bruno Lepri, Xavier\n  Alameda-Pineda", "title": "GMM-UNIT: Unsupervised Multi-Domain and Multi-Modal Image-to-Image\n  Translation via Attribute Gaussian Mixture Modeling", "comments": "27 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised image-to-image translation (UNIT) aims at learning a mapping\nbetween several visual domains by using unpaired training images. Recent\nstudies have shown remarkable success for multiple domains but they suffer from\ntwo main limitations: they are either built from several two-domain mappings\nthat are required to be learned independently, or they generate low-diversity\nresults, a problem known as mode collapse. To overcome these limitations, we\npropose a method named GMM-UNIT, which is based on a content-attribute\ndisentangled representation where the attribute space is fitted with a GMM.\nEach GMM component represents a domain, and this simple assumption has two\nprominent advantages. First, it can be easily extended to most multi-domain and\nmulti-modal image-to-image translation tasks. Second, the continuous domain\nencoding allows for interpolation between domains and for extrapolation to\nunseen domains and translations. Additionally, we show how GMM-UNIT can be\nconstrained down to different methods in the literature, meaning that GMM-UNIT\nis a unifying framework for unsupervised image-to-image translation.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 10:18:56 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 22:15:26 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Yahui", ""], ["De Nadai", "Marco", ""], ["Yao", "Jian", ""], ["Sebe", "Nicu", ""], ["Lepri", "Bruno", ""], ["Alameda-Pineda", "Xavier", ""]]}, {"id": "2003.06792", "submitter": "Syed Waqas Zamir", "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad\n  Shahbaz Khan, Ming-Hsuan Yang, Ling Shao", "title": "Learning Enriched Features for Real Image Restoration and Enhancement", "comments": "Accepted for publication at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the goal of recovering high-quality image content from its degraded\nversion, image restoration enjoys numerous applications, such as in\nsurveillance, computational photography, medical imaging, and remote sensing.\nRecently, convolutional neural networks (CNNs) have achieved dramatic\nimprovements over conventional approaches for image restoration task. Existing\nCNN-based methods typically operate either on full-resolution or on\nprogressively low-resolution representations. In the former case, spatially\nprecise but contextually less robust results are achieved, while in the latter\ncase, semantically reliable but spatially less accurate outputs are generated.\nIn this paper, we present a novel architecture with the collective goals of\nmaintaining spatially-precise high-resolution representations through the\nentire network and receiving strong contextual information from the\nlow-resolution representations. The core of our approach is a multi-scale\nresidual block containing several key elements: (a) parallel multi-resolution\nconvolution streams for extracting multi-scale features, (b) information\nexchange across the multi-resolution streams, (c) spatial and channel attention\nmechanisms for capturing contextual information, and (d) attention based\nmulti-scale feature aggregation. In a nutshell, our approach learns an enriched\nset of features that combines contextual information from multiple scales,\nwhile simultaneously preserving the high-resolution spatial details. Extensive\nexperiments on five real image benchmark datasets demonstrate that our method,\nnamed as MIRNet, achieves state-of-the-art results for a variety of image\nprocessing tasks, including image denoising, super-resolution, and image\nenhancement. The source code and pre-trained models are available at\nhttps://github.com/swz30/MIRNet.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:04:30 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 12:58:28 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""], ["Shao", "Ling", ""]]}, {"id": "2003.06794", "submitter": "Muhammad Ali Farooq", "authors": "Moazam Soomro, Muhammad Ali Farooq, Rana Hammad Raza", "title": "Performance Evaluation of Advanced Deep Learning Architectures for\n  Offline Handwritten Character Recognition", "comments": "FIT 2017 Paper Published in IEEE FIT 2017", "journal-ref": null, "doi": "10.1109/FIT.2017.00071", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a hand-written character recognition comparison and\nperformance evaluation for robust and precise classification of different\nhand-written characters. The system utilizes advanced multilayer deep neural\nnetwork by collecting features from raw pixel values. The hidden layers stack\ndeep hierarchies of non-linear features since learning complex features from\nconventional neural networks is very challenging. Two state of the art deep\nlearning architectures were used which includes Caffe AlexNet and GoogleNet\nmodels in NVIDIA DIGITS.The frameworks were trained and tested on two different\ndatasets for incorporating diversity and complexity. One of them is the\npublicly available dataset i.e. Chars74K comprising of 7705 characters and has\nupper and lowercase English alphabets, along with numerical digits. While the\nother dataset created locally consists of 4320 characters. The local dataset\nconsists of 62 classes and was created by 40 subjects. It also consists upper\nand lowercase English alphabets, along with numerical digits. The overall\ndataset is divided in the ratio of 80% for training and 20% for testing phase.\nThe time required for training phase is approximately 90 minutes. For\nvalidation part, the results obtained were compared with the groundtruth. The\naccuracy level achieved with AlexNet was 77.77% and 88.89% with Google Net. The\nhigher accuracy level of GoogleNet is due to its unique combination of\ninception modules, each including pooling, convolutions at various scales and\nconcatenation procedures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:17:16 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Soomro", "Moazam", ""], ["Farooq", "Muhammad Ali", ""], ["Raza", "Rana Hammad", ""]]}, {"id": "2003.06798", "submitter": "Leonid Karlinsky", "authors": "Leonid Karlinsky and Joseph Shtok and Amit Alfassy and Moshe\n  Lichtenstein and Sivan Harary and Eli Schwartz and Sivan Doveh and Prasanna\n  Sattigeri and Rogerio Feris and Alexander Bronstein and Raja Giryes", "title": "StarNet: towards Weakly Supervised Few-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot detection and classification have advanced significantly in recent\nyears. Yet, detection approaches require strong annotation (bounding boxes)\nboth for pre-training and for adaptation to novel classes, and classification\napproaches rarely provide localization of objects in the scene. In this paper,\nwe introduce StarNet - a few-shot model featuring an end-to-end differentiable\nnon-parametric star-model detection and classification head. Through this head,\nthe backbone is meta-trained using only image-level labels to produce good\nfeatures for jointly localizing and classifying previously unseen categories of\nfew-shot test tasks using a star-model that geometrically matches between the\nquery and support images (to find corresponding object instances). Being a\nfew-shot detector, StarNet does not require any bounding box annotations,\nneither during pre-training nor for novel classes adaptation. It can thus be\napplied to the previously unexplored and challenging task of Weakly Supervised\nFew-Shot Object Detection (WS-FSOD), where it attains significant improvements\nover the baselines. In addition, StarNet shows significant gains on few-shot\nclassification benchmarks that are less cropped around the objects (where\nobject localization is key).\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:35:28 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 10:22:45 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 11:37:25 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Karlinsky", "Leonid", ""], ["Shtok", "Joseph", ""], ["Alfassy", "Amit", ""], ["Lichtenstein", "Moshe", ""], ["Harary", "Sivan", ""], ["Schwartz", "Eli", ""], ["Doveh", "Sivan", ""], ["Sattigeri", "Prasanna", ""], ["Feris", "Rogerio", ""], ["Bronstein", "Alexander", ""], ["Giryes", "Raja", ""]]}, {"id": "2003.06800", "submitter": "Anton Osokin", "authors": "Anton Osokin, Denis Sumin, Vasily Lomakin", "title": "OS2D: One-Stage One-Shot Object Detection by Matching Anchor Features", "comments": "Published at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the task of one-shot object detection, which\nconsists in detecting objects defined by a single demonstration. Differently\nfrom the standard object detection, the classes of objects used for training\nand testing do not overlap. We build the one-stage system that performs\nlocalization and recognition jointly. We use dense correlation matching of\nlearned local features to find correspondences, a feed-forward geometric\ntransformation model to align features and bilinear resampling of the\ncorrelation tensor to compute the detection score of the aligned features. All\nthe components are differentiable, which allows end-to-end training.\nExperimental evaluation on several challenging domains (retail products, 3D\nobjects, buildings and logos) shows that our method can detect unseen classes\n(e.g., toothpaste when trained on groceries) and outperforms several baselines\nby a significant margin. Our code is available online:\nhttps://github.com/aosokin/os2d .\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:39:47 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 13:59:00 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Osokin", "Anton", ""], ["Sumin", "Denis", ""], ["Lomakin", "Vasily", ""]]}, {"id": "2003.06801", "submitter": "Ioannis Apostolopoulos", "authors": "Ioannis D. Apostolopoulos", "title": "Experimenting with Convolutional Neural Network Architectures for the\n  automatic characterization of Solitary Pulmonary Nodules' malignancy rating", "comments": "22 pages. arXiv admin note: text overlap with arXiv:1409.4842,\n  arXiv:1512.07108 by other authors", "journal-ref": null, "doi": "10.5281/zenodo.3711239", "report-no": null, "categories": "eess.IV cs.CV physics.data-an physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lung Cancer is the most common cause of cancer-related death worldwide. Early\nand automatic diagnosis of Solitary Pulmonary Nodules (SPN) in Computer\nTomography (CT) chest scans can provide early treatment as well as doctor\nliberation from time-consuming procedures. Deep Learning has been proven as a\npopular and influential method in many medical imaging diagnosis areas. In this\nstudy, we consider the problem of diagnostic classification between benign and\nmalignant lung nodules in CT images derived from a PET/CT scanner. More\nspecifically, we intend to develop experimental Convolutional Neural Network\n(CNN) architectures and conduct experiments, by tuning their parameters, to\ninvestigate their behavior, and to define the optimal setup for the accurate\nclassification. For the experiments, we utilize PET/CT images obtained from the\nLaboratory of Nuclear Medicine of the University of Patras, and the publically\navailable database called Lung Image Database Consortium Image Collection\n(LIDC-IDRI). Furthermore, we apply simple data augmentation to generate new\ninstances and to inspect the performance of the developed networks.\nClassification accuracy of 91% and 93% on the PET/CT dataset and on a selection\nof nodule images form the LIDC-IDRI dataset, is achieved accordingly. The\nresults demonstrate that CNNs are a trustworth method for nodule\nclassification. Also, the experiment confirms that data augmentation enhances\nthe robustness of the CNNs.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 11:46:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Apostolopoulos", "Ioannis D.", ""]]}, {"id": "2003.06812", "submitter": "Thierry Dumas", "authors": "Thierry Dumas, Franck Galpin and Philippe Bordes", "title": "Iterative training of neural networks for intra prediction", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": "10.1109/TIP.2020.3038348", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an iterative training of neural networks for intra\nprediction in a block-based image and video codec. First, the neural networks\nare trained on blocks arising from the codec partitioning of images, each\npaired with its context. Then, iteratively, blocks are collected from the\npartitioning of images via the codec including the neural networks trained at\nthe previous iteration, each paired with its context, and the neural networks\nare retrained on the new pairs. Thanks to this training, the neural networks\ncan learn intra prediction functions that both stand out from those already in\nthe initial codec and boost the codec in terms of rate-distortion. Moreover,\nthe iterative process allows the design of training data cleansings essential\nfor the neural network training. When the iteratively trained neural networks\nare put into H.265 (HM-16.15), -4.2% of mean dB-rate reduction is obtained. By\nmoving them into H.266 (VTM-5.0), the mean dB-rate reduction reaches -1.9%.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 12:29:51 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 07:46:42 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Dumas", "Thierry", ""], ["Galpin", "Franck", ""], ["Bordes", "Philippe", ""]]}, {"id": "2003.06814", "submitter": "ShawnXY Yang", "authors": "Xiao Yang, Yinpeng Dong, Tianyu Pang, Jun Zhu, Hang Su", "title": "Towards Privacy Protection by Generating Adversarial Identity Masks", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As billions of personal data such as photos are shared through social media\nand network, the privacy and security of data have drawn an increasing\nattention. Several attempts have been made to alleviate the leakage of identity\ninformation with the aid of image obfuscation techniques. However, most of the\npresent results are either perceptually unsatisfactory or ineffective against\nreal-world recognition systems. In this paper, we argue that an algorithm for\nprivacy protection must block the ability of automatic inference of the\nidentity and at the same time, make the resultant image natural from the users'\npoint of view. To achieve this, we propose a targeted identity-protection\niterative method (TIP-IM), which can generate natural face images by adding\nadversarial identity masks to conceal ones' identity against a recognition\nsystem. Extensive experiments on various state-of-the-art face recognition\nmodels demonstrate the effectiveness of our proposed method on alleviating the\nidentity leakage of face images, without sacrificing? the visual quality of the\nprotected images.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 12:45:10 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yang", "Xiao", ""], ["Dong", "Yinpeng", ""], ["Pang", "Tianyu", ""], ["Zhu", "Jun", ""], ["Su", "Hang", ""]]}, {"id": "2003.06816", "submitter": "Yi Wang", "authors": "Yi Wang, Ying-Cong Chen, Xin Tao, Jiaya Jia", "title": "VCNet: A Robust Approach to Blind Image Inpainting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind inpainting is a task to automatically complete visual contents without\nspecifying masks for missing areas in an image. Previous works assume missing\nregion patterns are known, limiting its application scope. In this paper, we\nrelax the assumption by defining a new blind inpainting setting, making\ntraining a blind inpainting neural system robust against various unknown\nmissing region patterns. Specifically, we propose a two-stage visual\nconsistency network (VCN), meant to estimate where to fill (via masks) and\ngenerate what to fill. In this procedure, the unavoidable potential mask\nprediction errors lead to severe artifacts in the subsequent repairing. To\naddress it, our VCN predicts semantically inconsistent regions first, making\nmask prediction more tractable. Then it repairs these estimated missing regions\nusing a new spatial normalization, enabling VCN to be robust to the mask\nprediction errors. In this way, semantically convincing and visually compelling\ncontent is thus generated. Extensive experiments are conducted, showing our\nmethod is effective and robust in blind image inpainting. And our VCN allows\nfor a wide spectrum of applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 12:47:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Yi", ""], ["Chen", "Ying-Cong", ""], ["Tao", "Xin", ""], ["Jia", "Jiaya", ""]]}, {"id": "2003.06820", "submitter": "Amir Rahimi", "authors": "Amir Rahimi, Amirreza Shaban, Ching-An Cheng, Richard Hartley, Byron\n  Boots", "title": "Intra Order-preserving Functions for Calibration of Multi-Class Neural\n  Networks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting calibrated confidence scores for multi-class deep networks is\nimportant for avoiding rare but costly mistakes. A common approach is to learn\na post-hoc calibration function that transforms the output of the original\nnetwork into calibrated confidence scores while maintaining the network's\naccuracy. However, previous post-hoc calibration techniques work only with\nsimple calibration functions, potentially lacking sufficient representation to\ncalibrate the complex function landscape of deep networks. In this work, we aim\nto learn general post-hoc calibration functions that can preserve the top-k\npredictions of any deep network. We call this family of functions intra\norder-preserving functions. We propose a new neural network architecture that\nrepresents a class of intra order-preserving functions by combining common\nneural network components. Additionally, we introduce order-invariant and\ndiagonal sub-families, which can act as regularization for better\ngeneralization when the training data size is small. We show the effectiveness\nof the proposed method across a wide range of datasets and classifiers. Our\nmethod outperforms state-of-the-art post-hoc calibration methods, namely\ntemperature scaling and Dirichlet calibration, in several evaluation metrics\nfor the task.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 12:57:21 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 06:59:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Rahimi", "Amir", ""], ["Shaban", "Amirreza", ""], ["Cheng", "Ching-An", ""], ["Hartley", "Richard", ""], ["Boots", "Byron", ""]]}, {"id": "2003.06832", "submitter": "Kaiyan Chen", "authors": "Kaiyan Chen, Ming Wu, Jiaming Liu, Chuang Zhang", "title": "FGSD: A Dataset for Fine-Grained Ship Detection in High Resolution\n  Satellite Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ship detection using high-resolution remote sensing images is an important\ntask, which contribute to sea surface regulation. The complex background and\nspecial visual angle make ship detection relies in high quality datasets to a\ncertain extent. However, there is few works on giving both precise\nclassification and accurate location of ships in existing ship detection\ndatasets. To further promote the research of ship detection, we introduced a\nnew fine-grained ship detection datasets, which is named as FGSD. The dataset\ncollects high-resolution remote sensing images that containing ship samples\nfrom multiple large ports around the world. Ship samples were fine categorized\nand annotated with both horizontal and rotating bounding boxes. To further\ndetailed the information of the dataset, we put forward a new representation\nmethod of ships' orientation. For future research, the dock as a new class was\nannotated in the dataset. Besides, rich information of images were provided in\nFGSD, including the source port, resolution and corresponding GoogleEarth' s\nresolution level of each image. As far as we know, FGSD is the most\ncomprehensive ship detection dataset currently and it'll be available soon.\nSome baselines for FGSD are also provided in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 13:54:20 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Kaiyan", ""], ["Wu", "Ming", ""], ["Liu", "Jiaming", ""], ["Zhang", "Chuang", ""]]}, {"id": "2003.06838", "submitter": "Yonghao Dang", "authors": "Jianqin Yin and Yanchun Wu and Huaping Liu and Yonghao Dang and Zhiyi\n  Liu and Jun Liu", "title": "Energy-based Periodicity Mining with Deep Features for Action Repetition\n  Counting in Unconstrained Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action repetition counting is to estimate the occurrence times of the\nrepetitive motion in one action, which is a relatively new, important but\nchallenging measurement problem. To solve this problem, we propose a new method\nsuperior to the traditional ways in two aspects, without preprocessing and\napplicable for arbitrary periodicity actions. Without preprocessing, the\nproposed model makes our method convenient for real applications; processing\nthe arbitrary periodicity action makes our model more suitable for the actual\ncircumstance. In terms of methodology, firstly, we analyze the movement\npatterns of the repetitive actions based on the spatial and temporal features\nof actions extracted by deep ConvNets; Secondly, the Principal Component\nAnalysis algorithm is used to generate the intuitive periodic information from\nthe chaotic high-dimensional deep features; Thirdly, the periodicity is mined\nbased on the high-energy rule using Fourier transform; Finally, the inverse\nFourier transform with a multi-stage threshold filter is proposed to improve\nthe quality of the mined periodicity, and peak detection is introduced to\nfinish the repetition counting. Our work features two-fold: 1) An important\ninsight that deep features extracted for action recognition can well model the\nself-similarity periodicity of the repetitive action is presented. 2) A\nhigh-energy based periodicity mining rule using deep features is presented,\nwhich can process arbitrary actions without preprocessing. Experimental results\nshow that our method achieves comparable results on the public datasets YT\nSegments and QUVA.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 14:21:18 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yin", "Jianqin", ""], ["Wu", "Yanchun", ""], ["Liu", "Huaping", ""], ["Dang", "Yonghao", ""], ["Liu", "Zhiyi", ""], ["Liu", "Jun", ""]]}, {"id": "2003.06841", "submitter": "Zipeng Ye", "authors": "Zipeng Ye, Ran Yi, Minjing Yu, Juyong Zhang, Yu-Kun Lai, Yong-jin Liu", "title": "3D-CariGAN: An End-to-End Solution to 3D Caricature Generation from Face\n  Photos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Caricature is a kind of artistic style of human faces that attracts\nconsiderable research in computer vision. So far all existing 3D caricature\ngeneration methods require some information related to caricature as input,\ne.g., a caricature sketch or 2D caricature. However, this kind of input is\ndifficult to provide by non-professional users. In this paper, we propose an\nend-to-end deep neural network model to generate high-quality 3D caricature\nwith a simple face photo as input. The most challenging issue in our system is\nthat the source domain of face photos (characterized by 2D normal faces) is\nsignificantly different from the target domain of 3D caricatures (characterized\nby 3D exaggerated face shapes and texture). To address this challenge, we (1)\nbuild a large dataset of 6,100 3D caricature meshes and use it to establish a\nPCA model in the 3D caricature shape space and (2) detect landmarks in the\ninput face photo and use them to set up correspondence between 2D caricature\nand 3D caricature shape. Our system can automatically generate high-quality 3D\ncaricatures. In many situations, users want to control the output by a simple\nand intuitive way, so we further introduce a simple-to-use interactive control\nwith three horizontal and one vertical lines. Experiments and user studies show\nthat our system is easy to use and can generate high-quality 3D caricatures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 14:42:15 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ye", "Zipeng", ""], ["Yi", "Ran", ""], ["Yu", "Minjing", ""], ["Zhang", "Juyong", ""], ["Lai", "Yu-Kun", ""], ["Liu", "Yong-jin", ""]]}, {"id": "2003.06845", "submitter": "Fan Ma", "authors": "Fan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt\n  Feiszli, Zheng Shou", "title": "SF-Net: Single-Frame Supervision for Temporal Action Localization", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an intermediate form of supervision, i.e.,\nsingle-frame supervision, for temporal action localization (TAL). To obtain the\nsingle-frame supervision, the annotators are asked to identify only a single\nframe within the temporal window of an action. This can significantly reduce\nthe labor cost of obtaining full supervision which requires annotating the\naction boundary. Compared to the weak supervision that only annotates the\nvideo-level label, the single-frame supervision introduces extra temporal\naction signals while maintaining low annotation overhead. To make full use of\nsuch single-frame supervision, we propose a unified system called SF-Net.\nFirst, we propose to predict an actionness score for each video frame. Along\nwith a typical category score, the actionness score can provide comprehensive\ninformation about the occurrence of a potential action and aid the temporal\nboundary refinement during inference. Second, we mine pseudo action and\nbackground frames based on the single-frame annotations. We identify pseudo\naction frames by adaptively expanding each annotated single frame to its\nnearby, contextual frames and we mine pseudo background frames from all the\nunannotated frames across multiple videos. Together with the ground-truth\nlabeled frames, these pseudo-labeled frames are further used for training the\nclassifier. In extensive experiments on THUMOS14, GTEA, and BEOID, SF-Net\nsignificantly improves upon state-of-the-art weakly-supervised methods in terms\nof both segment localization and single-frame localization. Notably, SF-Net\nachieves comparable results to its fully-supervised counterpart which requires\nmuch more resource intensive annotations. The code is available at\nhttps://github.com/Flowerfan/SF-Net.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 15:06:01 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 00:56:59 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 00:30:18 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 02:11:16 GMT"}, {"version": "v5", "created": "Fri, 24 Jul 2020 00:48:24 GMT"}, {"version": "v6", "created": "Sat, 15 Aug 2020 04:20:57 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ma", "Fan", ""], ["Zhu", "Linchao", ""], ["Yang", "Yi", ""], ["Zha", "Shengxin", ""], ["Kundu", "Gourab", ""], ["Feiszli", "Matt", ""], ["Shou", "Zheng", ""]]}, {"id": "2003.06849", "submitter": "Xingqian Xu", "authors": "Xingqian Xu, Mang Tik Chiu, Thomas S. Huang, Honghui Shi", "title": "Deep Affinity Net: Instance Segmentation via Affinity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the modern instance segmentation approaches fall into two categories:\nregion-based approaches in which object bounding boxes are detected first and\nlater used in cropping and segmenting instances; and keypoint-based approaches\nin which individual instances are represented by a set of keypoints followed by\na dense pixel clustering around those keypoints. Despite the maturity of these\ntwo paradigms, we would like to report an alternative affinity-based paradigm\nwhere instances are segmented based on densely predicted affinities and graph\npartitioning algorithms. Such affinity-based approaches indicate that\nhigh-level graph features other than regions or keypoints can be directly\napplied in the instance segmentation task. In this work, we propose Deep\nAffinity Net, an effective affinity-based approach accompanied with a new graph\npartitioning algorithm Cascade-GAEC. Without bells and whistles, our end-to-end\nmodel results in 32.4% AP on Cityscapes val and 27.5% AP on test. It achieves\nthe best single-shot result as well as the fastest running time among all\naffinity-based models. It also outperforms the region-based method Mask R-CNN.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 15:22:56 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xu", "Xingqian", ""], ["Chiu", "Mang Tik", ""], ["Huang", "Thomas S.", ""], ["Shi", "Honghui", ""]]}, {"id": "2003.06872", "submitter": "Nibaran Das", "authors": "Tanmoy Dasgupta and Nibaran Das and Mita Nasipuri", "title": "Multistage Curvilinear Coordinate Transform Based Document Image\n  Dewarping using a Novel Quality Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work demonstrates a fast and improved technique for dewarping\nnonlinearly warped document images. The images are first dewarped at the\npage-level by estimating optimum inverse projections using curvilinear\nhomography. The quality of the process is then estimated by evaluating a set of\nmetrics related to the characteristics of the text lines and rectilinear\nobjects for measuring parallelism, orthogonality, etc. These are designed\nspecifically to estimate the quality of the dewarping process without the need\nof any ground truth. If the quality is estimated to be unsatisfactory, the\npage-level dewarping process is repeated with finer approximations. This is\nfollowed by a line-level dewarping process that makes granular corrections to\nthe warps in individual text-lines. The methodology has been tested on the\nCBDAR 2007 / IUPR 2011 document image dewarping dataset and is seen to yield\nthe best OCR accuracy in the shortest amount of time, till date. The usefulness\nof the methodology has also been evaluated on the DocUNet 2018 dataset with\nsome minor tweaks, and is seen to produce comparable results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:17:53 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Dasgupta", "Tanmoy", ""], ["Das", "Nibaran", ""], ["Nasipuri", "Mita", ""]]}, {"id": "2003.06877", "submitter": "Liang Liao", "authors": "Liang Liao, Jing Xiao, Zheng Wang, Chia-Wen Lin, Shin'ichi Satoh", "title": "Guidance and Evaluation: Semantic-Aware Image Inpainting for Mixed\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completing a corrupted image with correct structures and reasonable textures\nfor a mixed scene remains an elusive challenge. Since the missing hole in a\nmixed scene of a corrupted image often contains various semantic information,\nconventional two-stage approaches utilizing structural information often lead\nto the problem of unreliable structural prediction and ambiguous image texture\ngeneration. In this paper, we propose a Semantic Guidance and Evaluation\nNetwork (SGE-Net) to iteratively update the structural priors and the inpainted\nimage in an interplay framework of semantics extraction and image inpainting.\nIt utilizes semantic segmentation map as guidance in each scale of inpainting,\nunder which location-dependent inferences are re-evaluated, and, accordingly,\npoorly-inferred regions are refined in subsequent scales. Extensive experiments\non real-world images of mixed scenes demonstrated the superiority of our\nproposed method over state-of-the-art approaches, in terms of clear boundaries\nand photo-realistic textures.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:49:20 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 03:39:48 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 10:58:49 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Liao", "Liang", ""], ["Xiao", "Jing", ""], ["Wang", "Zheng", ""], ["Lin", "Chia-Wen", ""], ["Satoh", "Shin'ichi", ""]]}, {"id": "2003.06878", "submitter": "Yusuke Tashiro", "authors": "Yusuke Tashiro, Yang Song, Stefano Ermon", "title": "Diversity can be Transferred: Output Diversification for White- and\n  Black-box Attacks", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks often involve random perturbations of the inputs drawn\nfrom uniform or Gaussian distributions, e.g., to initialize optimization-based\nwhite-box attacks or generate update directions in black-box attacks. These\nsimple perturbations, however, could be sub-optimal as they are agnostic to the\nmodel being attacked. To improve the efficiency of these attacks, we propose\nOutput Diversified Sampling (ODS), a novel sampling strategy that attempts to\nmaximize diversity in the target model's outputs among the generated samples.\nWhile ODS is a gradient-based strategy, the diversity offered by ODS is\ntransferable and can be helpful for both white-box and black-box attacks via\nsurrogate models. Empirically, we demonstrate that ODS significantly improves\nthe performance of existing white-box and black-box attacks. In particular, ODS\nreduces the number of queries needed for state-of-the-art black-box attacks on\nImageNet by a factor of two.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 17:49:25 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:44:53 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 00:12:48 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Tashiro", "Yusuke", ""], ["Song", "Yang", ""], ["Ermon", "Stefano", ""]]}, {"id": "2003.06883", "submitter": "Xin Tan", "authors": "Xin Tan and Yiheng Zhang and Ying Cao and Lizhuang Ma and Rynson W.H.\n  Lau", "title": "Night-time Semantic Segmentation with a Large Real Dataset", "comments": "13 pages, 10 figures. The dataset will be made publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although huge progress has been made on semantic segmentation in recent\nyears, most existing works assume that the input images are captured in\nday-time with good lighting conditions. In this work, we aim to address the\nsemantic segmentation problem of night-time scenes, which has two main\nchallenges: 1) labeled night-time data are scarce, and 2) over- and\nunder-exposures may co-occur in the input night-time images and are not\nexplicitly modeled in existing semantic segmentation pipelines. To tackle the\nscarcity of night-time data, we collect a novel labeled dataset (named\nNightCity) of 4,297 real night-time images with ground truth pixel-level\nsemantic annotations. To our knowledge, NightCity is the largest dataset for\nnight-time semantic segmentation. In addition, we also propose an\nexposure-aware framework to address the night-time segmentation problem through\naugmenting the segmentation process with explicitly learned exposure features.\nExtensive experiments show that training on NightCity can significantly improve\nthe performance of night-time semantic segmentation and that our exposure-aware\nmodel outperforms the state-of-the-art segmentation methods, yielding top\nperformances on our benchmark dataset.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 18:11:34 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Tan", "Xin", ""], ["Zhang", "Yiheng", ""], ["Cao", "Ying", ""], ["Ma", "Lizhuang", ""], ["Lau", "Rynson W. H.", ""]]}, {"id": "2003.06885", "submitter": "Olivier Rukundo", "authors": "Olivier Rukundo", "title": "Evaluation of Rounding Functions in Nearest-Neighbor Interpolation", "comments": "9 pages, 8 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel evaluation study of the most appropriate round function for\nnearest-neighbor (NN) image interpolation is presented. Evaluated rounding\nfunctions are selected among the five rounding rules defined by the Institute\nof Electrical and Electronics Engineers (IEEE) 754-2008 standard. Both full-\nand no-reference image quality assessment (IQA) metrics are used to study and\nevaluate the influence of rounding functions on NN interpolation image quality.\nThe concept of achieved occurrences over targeted occurrences is used to\ndetermine the percentage of achieved occurrences based on the number of test\nimages used. Inferential statistical analysis is applied to deduce from a small\nnumber of images and draw a conclusion of the behavior of each rounding\nfunction on a bigger number of images. Under the normal distribution and at the\nlevel of confidence equals to 95%, the maximum and minimum achievable\noccurrences by each evaluated rounding function are both provided based on the\ninferential analysis-based experiments.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 18:17:36 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 19:10:17 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rukundo", "Olivier", ""]]}, {"id": "2003.06932", "submitter": "Qinghui Liu", "authors": "Qinghui Liu, Michael Kampffmeyer, Robert Jenssen, Arnt-B{\\o}rre\n  Salberg", "title": "Self-Constructing Graph Convolutional Networks for Semantic Labeling", "comments": "IGARSS-2020, code at: github.com/samleoqh/MSCG-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) have received increasing attention in many\nfields. However, due to the lack of prior graphs, their use for semantic\nlabeling has been limited. Here, we propose a novel architecture called the\nSelf-Constructing Graph (SCG), which makes use of learnable latent variables to\ngenerate embeddings and to self-construct the underlying graphs directly from\nthe input features without relying on manually built prior knowledge graphs.\nSCG can automatically obtain optimized non-local context graphs from\ncomplex-shaped objects in aerial imagery. We optimize SCG via an adaptive\ndiagonal enhancement method and a variational lower bound that consists of a\ncustomized graph reconstruction term and a Kullback-Leibler divergence\nregularization term. We demonstrate the effectiveness and flexibility of the\nproposed SCG on the publicly available ISPRS Vaihingen dataset and our model\nSCG-Net achieves competitive results in terms of F1-score with much fewer\nparameters and at a lower computational cost compared to related pure-CNN based\nwork. Our code will be made public soon.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 21:55:24 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 13:44:08 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Liu", "Qinghui", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""], ["Salberg", "Arnt-B\u00f8rre", ""]]}, {"id": "2003.06944", "submitter": "David Mayerich", "authors": "Nguyen Tran, Rupali Mankar, David Mayerich, Zhu Han", "title": "Hyperspectral-Multispectral Image Fusion with Weighted LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral imaging enables spatially-resolved identification of materials in\nremote sensing, biomedicine, and astronomy. However, acquisition times require\nbalancing spectral and spatial resolution with signal-to-noise. Hyperspectral\nimaging provides superior material specificity, while multispectral images are\nfaster to collect at greater fidelity. We propose an approach for fusing\nhyperspectral and multispectral images to provide high-quality hyperspectral\noutput. The proposed optimization leverages the least absolute shrinkage and\nselection operator (LASSO) to perform variable selection and regularization.\nComputational time is reduced by applying the alternating direction method of\nmultipliers (ADMM), as well as initializing the fusion image by estimating it\nusing maximum a posteriori (MAP) based on Hardie's method. We demonstrate that\nthe proposed sparse fusion and reconstruction provides quantitatively superior\nresults when compared to existing methods on publicly available images.\nFinally, we show how the proposed method can be practically applied in\nbiomedical infrared spectroscopic microscopy.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 23:07:56 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Tran", "Nguyen", ""], ["Mankar", "Rupali", ""], ["Mayerich", "David", ""], ["Han", "Zhu", ""]]}, {"id": "2003.06945", "submitter": "Cho-Ying Wu", "authors": "Cho-Ying Wu, Ulrich Neumann", "title": "Scene Completeness-Aware Lidar Depth Completion for Driving Scenario", "comments": "Present at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces Scene Completeness-Aware Depth Completion (SCADC) to\ncomplete raw lidar scans into dense depth maps with fine and complete scene\nstructures. Recent sparse depth completion for lidars only focuses on the lower\nscenes and produces irregular estimations on the upper because existing\ndatasets, such as KITTI, do not provide groundtruth for upper areas. These\nareas are considered less important since they are usually sky or trees of less\nscene understanding interest. However, we argue that in several driving\nscenarios such as large trucks or cars with loads, objects could extend to the\nupper parts of scenes. Thus depth maps with structured upper scene estimation\nare important for RGBD algorithms. SCADC adopts stereo images that produce\ndisparities with better scene completeness but are generally less precise than\nlidars, to help sparse lidar depth completion. To our knowledge, we are the\nfirst to focus on scene completeness of sparse depth completion. We validate\nour SCADC on both depth estimate precision and scene-completeness on KITTI.\nMoreover, we experiment on less-explored outdoor RGBD semantic segmentation\nwith scene completeness-aware D-input to validate our method.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 23:23:26 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 03:46:18 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 23:50:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Wu", "Cho-Ying", ""], ["Neumann", "Ulrich", ""]]}, {"id": "2003.06951", "submitter": "Chang Chen", "authors": "Chang Chen, Zhiwei Xiong, Xiaoming Liu, Feng Wu", "title": "Camera Trace Erasing", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera trace is a unique noise produced in digital imaging process. Most\nexisting forensic methods analyze camera trace to identify image origins. In\nthis paper, we address a new low-level vision problem, camera trace erasing, to\nreveal the weakness of trace-based forensic methods. A comprehensive\ninvestigation on existing anti-forensic methods reveals that it is non-trivial\nto effectively erase camera trace while avoiding the destruction of content\nsignal. To reconcile these two demands, we propose Siamese Trace Erasing\n(SiamTE), in which a novel hybrid loss is designed on the basis of Siamese\narchitecture for network training. Specifically, we propose embedded\nsimilarity, truncated fidelity, and cross identity to form the hybrid loss.\nCompared with existing anti-forensic methods, SiamTE has a clear advantage for\ncamera trace erasing, which is demonstrated in three representative tasks. Code\nand dataset are available at https://github.com/ngchc/CameraTE.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:09:55 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Chen", "Chang", ""], ["Xiong", "Zhiwei", ""], ["Liu", "Xiaoming", ""], ["Wu", "Feng", ""]]}, {"id": "2003.06957", "submitter": "Xin Wang", "authors": "Xin Wang, Thomas E. Huang, Trevor Darrell, Joseph E. Gonzalez, Fisher\n  Yu", "title": "Frustratingly Simple Few-Shot Object Detection", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting rare objects from a few examples is an emerging problem. Prior\nworks show meta-learning is a promising approach. But, fine-tuning techniques\nhave drawn scant attention. We find that fine-tuning only the last layer of\nexisting detectors on rare classes is crucial to the few-shot object detection\ntask. Such a simple approach outperforms the meta-learning methods by roughly\n2~20 points on current benchmarks and sometimes even doubles the accuracy of\nthe prior methods. However, the high variance in the few samples often leads to\nthe unreliability of existing benchmarks. We revise the evaluation protocols by\nsampling multiple groups of training examples to obtain stable comparisons and\nbuild new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again,\nour fine-tuning approach establishes a new state of the art on the revised\nbenchmarks. The code as well as the pretrained models are available at\nhttps://github.com/ucbdrive/few-shot-object-detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:29:14 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Xin", ""], ["Huang", "Thomas E.", ""], ["Darrell", "Trevor", ""], ["Gonzalez", "Joseph E.", ""], ["Yu", "Fisher", ""]]}, {"id": "2003.06958", "submitter": "Chi Nhan Duong", "authors": "Chi Nhan Duong, Thanh-Dat Truong, Kha Gia Quach, Hung Bui, Kaushik\n  Roy, Khoa Luu", "title": "Vec2Face: Unveil Human Faces from their Blackbox Features in Face\n  Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unveiling face images of a subject given his/her high-level representations\nextracted from a blackbox Face Recognition engine is extremely challenging. It\nis because the limitations of accessible information from that engine including\nits structure and uninterpretable extracted features. This paper presents a\nnovel generative structure with Bijective Metric Learning, namely Bijective\nGenerative Adversarial Networks in a Distillation framework (DiBiGAN), for\nsynthesizing faces of an identity given that person's features. In order to\neffectively address this problem, this work firstly introduces a bijective\nmetric so that the distance measurement and metric learning process can be\ndirectly adopted in image domain for an image reconstruction task. Secondly, a\ndistillation process is introduced to maximize the information exploited from\nthe blackbox face recognition engine. Then a Feature-Conditional Generator\nStructure with Exponential Weighting Strategy is presented for a more robust\ngenerator that can synthesize realistic faces with ID preservation. Results on\nseveral benchmarking datasets including CelebA, LFW, AgeDB, CFP-FP against\nmatching engines have demonstrated the effectiveness of DiBiGAN on both image\nrealism and ID preservation properties.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:30:54 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Duong", "Chi Nhan", ""], ["Truong", "Thanh-Dat", ""], ["Quach", "Kha Gia", ""], ["Bui", "Hung", ""], ["Roy", "Kaushik", ""], ["Luu", "Khoa", ""]]}, {"id": "2003.06965", "submitter": "Akhil Padmanabha", "authors": "Akhil Padmanabha, Frederik Ebert, Stephen Tian, Roberto Calandra,\n  Chelsea Finn, Sergey Levine", "title": "OmniTact: A Multi-Directional High Resolution Touch Sensor", "comments": "Accepted at International Conference on Robotics and Automation\n  (ICRA) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating touch as a sensing modality for robots can enable finer and\nmore robust manipulation skills. Existing tactile sensors are either flat, have\nsmall sensitive fields or only provide low-resolution signals. In this paper,\nwe introduce OmniTact, a multi-directional high-resolution tactile sensor.\nOmniTact is designed to be used as a fingertip for robotic manipulation with\nrobotic hands, and uses multiple micro-cameras to detect multi-directional\ndeformations of a gel-based skin. This provides a rich signal from which a\nvariety of different contact state variables can be inferred using modern image\nprocessing and computer vision methods. We evaluate the capabilities of\nOmniTact on a challenging robotic control task that requires inserting an\nelectrical connector into an outlet, as well as a state estimation problem that\nis representative of those typically encountered in dexterous robotic\nmanipulation, where the goal is to infer the angle of contact of a curved\nfinger pressing against an object. Both tasks are performed using only touch\nsensing and deep convolutional neural networks to process images from the\nsensor's cameras. We compare with a state-of-the-art tactile sensor that is\nonly sensitive on one side, as well as a state-of-the-art multi-directional\ntactile sensor, and find that OmniTact's combination of high-resolution and\nmulti-directional sensing is crucial for reliably inserting the electrical\nconnector and allows for higher accuracy in the state estimation task. Videos\nand supplementary material can be found at\nhttps://sites.google.com/berkeley.edu/omnitact\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 01:31:29 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Padmanabha", "Akhil", ""], ["Ebert", "Frederik", ""], ["Tian", "Stephen", ""], ["Calandra", "Roberto", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "2003.06974", "submitter": "Yiming Li", "authors": "Yiming Li, Baoyuan Wu, Yan Feng, Yanbo Fan, Yong Jiang, Zhifeng Li,\n  Shutao Xia", "title": "Toward Adversarial Robustness via Semi-supervised Robust Training", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples have been shown to be the severe threat to deep neural\nnetworks (DNNs). One of the most effective adversarial defense methods is\nadversarial training (AT) through minimizing the adversarial risk $R_{adv}$,\nwhich encourages both the benign example $x$ and its adversarially perturbed\nneighborhoods within the $\\ell_{p}$-ball to be predicted as the ground-truth\nlabel. In this work, we propose a novel defense method, the robust training\n(RT), by jointly minimizing two separated risks ($R_{stand}$ and $R_{rob}$),\nwhich is with respect to the benign example and its neighborhoods respectively.\nThe motivation is to explicitly and jointly enhance the accuracy and the\nadversarial robustness. We prove that $R_{adv}$ is upper-bounded by $R_{stand}\n+ R_{rob}$, which implies that RT has similar effect as AT. Intuitively,\nminimizing the standard risk enforces the benign example to be correctly\npredicted, and the robust risk minimization encourages the predictions of the\nneighbor examples to be consistent with the prediction of the benign example.\nBesides, since $R_{rob}$ is independent of the ground-truth label, RT is\nnaturally extended to the semi-supervised mode ($i.e.$, SRT), to further\nenhance the adversarial robustness. Moreover, we extend the $\\ell_{p}$-bounded\nneighborhood to a general case, which covers different types of perturbations,\nsuch as the pixel-wise ($i.e.$, $x + \\delta$) or the spatial perturbation\n($i.e.$, $ AX + b$). Extensive experiments on benchmark datasets not only\nverify the superiority of the proposed SRT method to state-of-the-art methods\nfor defensing pixel-wise or spatial perturbations separately, but also\ndemonstrate its robustness to both perturbations simultaneously. The code for\nreproducing main results is available at\n\\url{https://github.com/THUYimingLi/Semi-supervised_Robust_Training}.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 02:14:08 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 10:14:20 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 01:12:53 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Li", "Yiming", ""], ["Wu", "Baoyuan", ""], ["Feng", "Yan", ""], ["Fan", "Yanbo", ""], ["Jiang", "Yong", ""], ["Li", "Zhifeng", ""], ["Xia", "Shutao", ""]]}, {"id": "2003.06975", "submitter": "Pedro F. Proen\\c{c}a", "authors": "Pedro F Proen\\c{c}a and Pedro Sim\\~oes", "title": "TACO: Trash Annotations in Context for Litter Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TACO is an open image dataset for litter detection and segmentation, which is\ngrowing through crowdsourcing. Firstly, this paper describes this dataset and\nthe tools developed to support it. Secondly, we report instance segmentation\nperformance using Mask R-CNN on the current version of TACO. Despite its small\nsize (1500 images and 4784 annotations), our results are promising on this\nchallenging problem. However, to achieve satisfactory trash detection in the\nwild for deployment, TACO still needs much more manual annotations. These can\nbe contributed using: http://tacodataset.org/\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 02:17:07 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 04:09:13 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Proen\u00e7a", "Pedro F", ""], ["Sim\u00f5es", "Pedro", ""]]}, {"id": "2003.06979", "submitter": "Bhavya Kailkhura", "authors": "Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K. Varshney, Dawn\n  Song", "title": "Anomalous Example Detection in Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning (DL) is vulnerable to out-of-distribution and adversarial\nexamples resulting in incorrect outputs. To make DL more robust, several\nposthoc (or runtime) anomaly detection techniques to detect (and discard) these\nanomalous samples have been proposed in the recent past. This survey tries to\nprovide a structured and comprehensive overview of the research on anomaly\ndetection for DL based applications. We provide a taxonomy for existing\ntechniques based on their underlying assumptions and adopted approaches. We\ndiscuss various techniques in each of the categories and provide the relative\nstrengths and weaknesses of the approaches. Our goal in this survey is to\nprovide an easier yet better understanding of the techniques belonging to\ndifferent categories in which research has been done on this topic. Finally, we\nhighlight the unsolved research challenges while applying anomaly detection\ntechniques in DL systems and present some high-impact future research\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 02:47:23 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 21:59:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bulusu", "Saikiran", ""], ["Kailkhura", "Bhavya", ""], ["Li", "Bo", ""], ["Varshney", "Pramod K.", ""], ["Song", "Dawn", ""]]}, {"id": "2003.06986", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou, Mingzhu Long, Xuyang Wang, Xiang Xie, Guolin Li, Zhihua\n  Wang", "title": "A CNN-Based Blind Denoising Method for Endoscopic Images", "comments": "accepted in BioCAS2019 conference", "journal-ref": null, "doi": "10.1109/BIOCAS.2019.8918994", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of images captured by wireless capsule endoscopy (WCE) is key for\ndoctors to diagnose diseases of gastrointestinal (GI) tract. However, there\nexist many low-quality endoscopic images due to the limited illumination and\ncomplex environment in GI tract. After an enhancement process, the severe noise\nbecome an unacceptable problem. The noise varies with different cameras, GI\ntract environments and image enhancement. And the noise model is hard to be\nobtained. This paper proposes a convolutional blind denoising network for\nendoscopic images. We apply Deep Image Prior (DIP) method to reconstruct a\nclean image iteratively using a noisy image without a specific noise model and\nground truth. Then we design a blind image quality assessment network based on\nMobileNet to estimate the quality of the reconstructed images. The estimated\nquality is used to stop the iterative operation in DIP method. The number of\niterations is reduced about 36% by using transfer learning in our DIP process.\nExperimental results on endoscopic images and real-world noisy images\ndemonstrate the superiority of our proposed method over the state-of-the-art\nmethods in terms of visual quality and quantitative metrics.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:11:11 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zou", "Shaofeng", ""], ["Long", "Mingzhu", ""], ["Wang", "Xuyang", ""], ["Xie", "Xiang", ""], ["Li", "Guolin", ""], ["Wang", "Zhihua", ""]]}, {"id": "2003.06988", "submitter": "Nelson Nauata Junior", "authors": "Nelson Nauata, Kai-Hung Chang, Chin-Yi Cheng, Greg Mori, Yasutaka\n  Furukawa", "title": "House-GAN: Relational Generative Adversarial Networks for\n  Graph-constrained House Layout Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel graph-constrained generative adversarial network,\nwhose generator and discriminator are built upon relational architecture. The\nmain idea is to encode the constraint into the graph structure of its\nrelational networks. We have demonstrated the proposed architecture for a new\nhouse layout generation problem, whose task is to take an architectural\nconstraint as a graph (i.e., the number and types of rooms with their spatial\nadjacency) and produce a set of axis-aligned bounding boxes of rooms. We\nmeasure the quality of generated house layouts with the three metrics: the\nrealism, the diversity, and the compatibility with the input graph constraint.\nOur qualitative and quantitative evaluations over 117,000 real floorplan images\ndemonstrate that the proposed approach outperforms existing methods and\nbaselines. We will publicly share all our code and data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:16:12 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Nauata", "Nelson", ""], ["Chang", "Kai-Hung", ""], ["Cheng", "Chin-Yi", ""], ["Mori", "Greg", ""], ["Furukawa", "Yasutaka", ""]]}, {"id": "2003.06994", "submitter": "Pengfei Zhu", "authors": "Pengfei Zhu, Jiayu Zheng, Dawei Du, Longyin Wen, Yiming Sun, Qinghua\n  Hu", "title": "Multi-Drone based Single Object Tracking with Agent Sharing Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drone equipped with cameras can dynamically track the target in the air from\na broader view compared with static cameras or moving sensors over the ground.\nHowever, it is still challenging to accurately track the target using a single\ndrone due to several factors such as appearance variations and severe\nocclusions. In this paper, we collect a new Multi-Drone single Object Tracking\n(MDOT) dataset that consists of 92 groups of video clips with 113,918 high\nresolution frames taken by two drones and 63 groups of video clips with 145,875\nhigh resolution frames taken by three drones. Besides, two evaluation metrics\nare specially designed for multi-drone single object tracking, i.e. automatic\nfusion score (AFS) and ideal fusion score (IFS). Moreover, an agent sharing\nnetwork (ASNet) is proposed by self-supervised template sharing and view-aware\nfusion of the target from multiple drones, which can improve the tracking\naccuracy significantly compared with single drone tracking. Extensive\nexperiments on MDOT show that our ASNet significantly outperforms recent\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:27:04 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Zhu", "Pengfei", ""], ["Zheng", "Jiayu", ""], ["Du", "Dawei", ""], ["Wen", "Longyin", ""], ["Sun", "Yiming", ""], ["Hu", "Qinghua", ""]]}, {"id": "2003.06999", "submitter": "Chixiang Ma", "authors": "Chixiang Ma, Lei Sun, Zhuoyao Zhong, Qiang Huo", "title": "ReLaText: Exploiting Visual Relationships for Arbitrary-Shaped Scene\n  Text Detection with Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new arbitrary-shaped text detection approach named ReLaText by\nformulating text detection as a visual relationship detection problem. To\ndemonstrate the effectiveness of this new formulation, we start from using a\n\"link\" relationship to address the challenging text-line grouping problem\nfirstly. The key idea is to decompose text detection into two subproblems,\nnamely detection of text primitives and prediction of link relationships\nbetween nearby text primitive pairs. Specifically, an anchor-free region\nproposal network based text detector is first used to detect text primitives of\ndifferent scales from different feature maps of a feature pyramid network, from\nwhich a text primitive graph is constructed by linking each pair of nearby text\nprimitives detected from a same feature map with an edge. Then, a Graph\nConvolutional Network (GCN) based link relationship prediction module is used\nto prune wrongly-linked edges in the text primitive graph to generate a number\nof disjoint subgraphs, each representing a detected text instance. As GCN can\neffectively leverage context information to improve link prediction accuracy,\nour GCN based text-line grouping approach can achieve better text detection\naccuracy than previous text-line grouping methods, especially when dealing with\ntext instances with large inter-character or very small inter-line spacings.\nConsequently, the proposed ReLaText achieves state-of-the-art performance on\nfive public text detection benchmarks, namely RCTW-17, MSRA-TD500, Total-Text,\nCTW1500 and DAST1500.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:33:48 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ma", "Chixiang", ""], ["Sun", "Lei", ""], ["Zhong", "Zhuoyao", ""], ["Huo", "Qiang", ""]]}, {"id": "2003.07003", "submitter": "Shafin Rahman", "authors": "Shafin Rahman and Salman Khan and Nick Barnes and Fahad Shahbaz Khan", "title": "Any-Shot Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous work on novel object detection considers zero or few-shot settings\nwhere none or few examples of each category are available for training. In real\nworld scenarios, it is less practical to expect that 'all' the novel classes\nare either unseen or {have} few-examples. Here, we propose a more realistic\nsetting termed 'Any-shot detection', where totally unseen and few-shot\ncategories can simultaneously co-occur during inference. Any-shot detection\noffers unique challenges compared to conventional novel object detection such\nas, a high imbalance between unseen, few-shot and seen object classes,\nsusceptibility to forget base-training while learning novel classes and\ndistinguishing novel classes from the background. To address these challenges,\nwe propose a unified any-shot detection model, that can concurrently learn to\ndetect both zero-shot and few-shot object classes. Our core idea is to use\nclass semantics as prototypes for object detection, a formulation that\nnaturally minimizes knowledge forgetting and mitigates the class-imbalance in\nthe label space. Besides, we propose a rebalanced loss function that emphasizes\ndifficult few-shot cases but avoids overfitting on the novel classes to allow\ndetection of totally unseen classes. Without bells and whistles, our framework\ncan also be used solely for Zero-shot detection and Few-shot detection tasks.\nWe report extensive experiments on Pascal VOC and MS-COCO datasets where our\napproach is shown to provide significant improvements.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 03:43:15 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Rahman", "Shafin", ""], ["Khan", "Salman", ""], ["Barnes", "Nick", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2003.07012", "submitter": "Qinzhe Xiao", "authors": "Jianming Lv, Qinzhe Xiao, Jiajie Zhong", "title": "AVR: Attention based Salient Visual Relationship Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual relationship detection aims to locate objects in images and recognize\nthe relationships between objects. Traditional methods treat all observed\nrelationships in an image equally, which causes a relatively poor performance\nin the detection tasks on complex images with abundant visual objects and\nvarious relationships. To address this problem, we propose an attention based\nmodel, namely AVR, to achieve salient visual relationships based on both local\nand global context of the relationships. Specifically, AVR recognizes\nrelationships and measures the attention on the relationships in the local\ncontext of an input image by fusing the visual features, semantic and spatial\ninformation of the relationships. AVR then applies the attention to assign\nimportant relationships with larger salient weights for effective information\nfiltering. Furthermore, AVR is integrated with the priori knowledge in the\nglobal context of image datasets to improve the precision of relationship\nprediction, where the context is modeled as a heterogeneous graph to measure\nthe priori probability of relationships based on the random walk algorithm.\nComprehensive experiments are conducted to demonstrate the effectiveness of AVR\nin several real-world image datasets, and the results show that AVR outperforms\nstate-of-the-art visual relationship detection methods significantly by up to\n$87.5\\%$ in terms of recall.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 04:12:39 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lv", "Jianming", ""], ["Xiao", "Qinzhe", ""], ["Zhong", "Jiajie", ""]]}, {"id": "2003.07018", "submitter": "Mingkui Tan", "authors": "Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang Cao, Zeshuai\n  Deng, Yanwu Xu, Mingkui Tan", "title": "Closed-loop Matters: Dual Regression Networks for Single Image\n  Super-Resolution", "comments": "This paper is accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have exhibited promising performance in image\nsuper-resolution (SR) by learning a nonlinear mapping function from\nlow-resolution (LR) images to high-resolution (HR) images. However, there are\ntwo underlying limitations to existing SR methods. First, learning the mapping\nfunction from LR to HR images is typically an ill-posed problem, because there\nexist infinite HR images that can be downsampled to the same LR image. As a\nresult, the space of the possible functions can be extremely large, which makes\nit hard to find a good solution. Second, the paired LR-HR data may be\nunavailable in real-world applications and the underlying degradation method is\noften unknown. For such a more general case, existing SR models often incur the\nadaptation problem and yield poor performance. To address the above issues, we\npropose a dual regression scheme by introducing an additional constraint on LR\ndata to reduce the space of the possible functions. Specifically, besides the\nmapping from LR to HR images, we learn an additional dual regression mapping\nestimates the down-sampling kernel and reconstruct LR images, which forms a\nclosed-loop to provide additional supervision. More critically, since the dual\nregression process does not depend on HR images, we can directly learn from LR\nimages. In this sense, we can easily adapt SR models to real-world data, e.g.,\nraw video frames from YouTube. Extensive experiments with paired training data\nand unpaired real-world data demonstrate our superiority over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 04:23:42 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 10:19:46 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 09:58:07 GMT"}, {"version": "v4", "created": "Fri, 22 May 2020 15:57:57 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Jian", ""], ["Wang", "Jingdong", ""], ["Chen", "Qi", ""], ["Cao", "Jiezhang", ""], ["Deng", "Zeshuai", ""], ["Xu", "Yanwu", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.07021", "submitter": "Chunfang Deng", "authors": "Chunfang Deng, Mengmeng Wang, Liang Liu, Yong Liu", "title": "Extended Feature Pyramid Network for Small Object Detection", "comments": "With the agreement of all authors, we would like to withdraw the\n  manuscript. For lack of some experiments, a part of important claims cannot\n  stand solidly. We need to further carry out experiments, and reconsider the\n  rationality of these claims", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small object detection remains an unsolved challenge because it is hard to\nextract information of small objects with only a few pixels. While scale-level\ncorresponding detection in feature pyramid network alleviates this problem, we\nfind feature coupling of various scales still impairs the performance of small\nobjects. In this paper, we propose extended feature pyramid network (EFPN) with\nan extra high-resolution pyramid level specialized for small object detection.\nSpecifically, we design a novel module, named feature texture transfer (FTT),\nwhich is used to super-resolve features and extract credible regional details\nsimultaneously. Moreover, we design a foreground-background-balanced loss\nfunction to alleviate area imbalance of foreground and background. In our\nexperiments, the proposed EFPN is efficient on both computation and memory, and\nyields state-of-the-art results on small traffic-sign dataset Tsinghua-Tencent\n100K and small category of general object detection dataset MS COCO.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 04:27:54 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 12:49:50 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Deng", "Chunfang", ""], ["Wang", "Mengmeng", ""], ["Liu", "Liang", ""], ["Liu", "Yong", ""]]}, {"id": "2003.07042", "submitter": "Kaito Imai", "authors": "Kaito Imai and Takamichi Miyata", "title": "Gated Texture CNN for Efficient and Configurable Image Denoising", "comments": "code is available: https://github.com/mdipcit/GTCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN)-based image denoising methods typically\nestimate the noise component contained in a noisy input image and restore a\nclean image by subtracting the estimated noise from the input. However,\nprevious denoising methods tend to remove high-frequency information (e.g.,\ntextures) from the input. It caused by intermediate feature maps of CNN\ncontains texture information. A straightforward approach to this problem is\nstacking numerous layers, which leads to a high computational cost. To achieve\nhigh performance and computational efficiency, we propose a gated texture CNN\n(GTCNN), which is designed to carefully exclude the texture information from\neach intermediate feature map of the CNN by incorporating gating mechanisms.\nOur GTCNN achieves state-of-the-art performance with 4.8 times fewer parameters\nthan previous state-of-the-art methods. Furthermore, the GTCNN allows us to\ninteractively control the texture strength in the output image without any\nadditional modules, training, or computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 06:37:07 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 01:59:52 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Imai", "Kaito", ""], ["Miyata", "Takamichi", ""]]}, {"id": "2003.07048", "submitter": "Yijun Song", "authors": "Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, Jun Yu", "title": "Weakly-Supervised Multi-Level Attentional Reconstruction Network for\n  Grounding Textual Queries in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of temporally grounding textual queries in videos is to localize one\nvideo segment that semantically corresponds to the given query. Most of the\nexisting approaches rely on segment-sentence pairs (temporal annotations) for\ntraining, which are usually unavailable in real-world scenarios. In this work\nwe present an effective weakly-supervised model, named as Multi-Level\nAttentional Reconstruction Network (MARN), which only relies on video-sentence\npairs during the training stage. The proposed method leverages the idea of\nattentional reconstruction and directly scores the candidate segments with the\nlearnt proposal-level attentions. Moreover, another branch learning clip-level\nattention is exploited to refine the proposals at both the training and testing\nstage. We develop a novel proposal sampling mechanism to leverage\nintra-proposal information for learning better proposal representation and\nadopt 2D convolution to exploit inter-proposal clues for learning reliable\nattention map. Experiments on Charades-STA and ActivityNet-Captions datasets\ndemonstrate the superiority of our MARN over the existing weakly-supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 07:01:01 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Song", "Yijun", ""], ["Wang", "Jingwen", ""], ["Ma", "Lin", ""], ["Yu", "Zhou", ""], ["Yu", "Jun", ""]]}, {"id": "2003.07064", "submitter": "Osman Semih Kayhan", "authors": "Osman Semih Kayhan and Jan C. van Gemert", "title": "On Translation Invariance in CNNs: Convolutional Layers can Exploit\n  Absolute Spatial Location", "comments": "CVPR 2020. (Minor revision on Figure 4, arguments unchanged.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we challenge the common assumption that convolutional layers in\nmodern CNNs are translation invariant. We show that CNNs can and will exploit\nthe absolute spatial location by learning filters that respond exclusively to\nparticular absolute locations by exploiting image boundary effects. Because\nmodern CNNs filters have a huge receptive field, these boundary effects operate\neven far from the image boundary, allowing the network to exploit absolute\nspatial location all over the image. We give a simple solution to remove\nspatial location encoding which improves translation invariance and thus gives\na stronger visual inductive bias which particularly benefits small data sets.\nWe broadly demonstrate these benefits on several architectures and various\napplications such as image classification, patch matching, and two video\nclassification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 08:00:06 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 14:59:07 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Kayhan", "Osman Semih", ""], ["van Gemert", "Jan C.", ""]]}, {"id": "2003.07069", "submitter": "Wenjie Shi", "authors": "Wenjie Shi and Gao Huang and Shiji Song and Zhuoyuan Wang and Tingyu\n  Lin and Cheng Wu", "title": "Self-Supervised Discovering of Interpretable Features for Reinforcement\n  Learning", "comments": "Accepted as a Regular Paper in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has recently led to many breakthroughs on a\nrange of complex control tasks. However, the agent's decision-making process is\ngenerally not transparent. The lack of interpretability hinders the\napplicability of RL in safety-critical scenarios. While several methods have\nattempted to interpret vision-based RL, most come without detailed explanation\nfor the agent's behavior. In this paper, we propose a self-supervised\ninterpretable framework, which can discover interpretable features to enable\neasy understanding of RL agents even for non-experts. Specifically, a\nself-supervised interpretable network (SSINet) is employed to produce\nfine-grained attention masks for highlighting task-relevant information, which\nconstitutes most evidence for the agent's decisions. We verify and evaluate our\nmethod on several Atari 2600 games as well as Duckietown, which is a\nchallenging self-driving car simulator environment. The results show that our\nmethod renders empirical evidences about how the agent makes decisions and why\nthe agent performs well or badly, especially when transferred to novel scenes.\nOverall, our method provides valuable insight into the internal decision-making\nprocess of vision-based RL. In addition, our method does not use any external\nlabelled data, and thus demonstrates the possibility to learn high-quality mask\nthrough a self-supervised manner, which may shed light on new paradigms for\nlabel-free vision learning such as self-supervised segmentation and detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 08:26:17 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 00:30:19 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 09:18:11 GMT"}, {"version": "v4", "created": "Fri, 19 Mar 2021 08:01:48 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Shi", "Wenjie", ""], ["Huang", "Gao", ""], ["Song", "Shiji", ""], ["Wang", "Zhuoyuan", ""], ["Lin", "Tingyu", ""], ["Wu", "Cheng", ""]]}, {"id": "2003.07071", "submitter": "Peng Su", "authors": "Peng Su, Kun Wang, Xingyu Zeng, Shixiang Tang, Dapeng Chen, Di Qiu,\n  Xiaogang Wang", "title": "Adapting Object Detectors with Conditional Domain Normalization", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world object detectors are often challenged by the domain gaps between\ndifferent datasets. In this work, we present the Conditional Domain\nNormalization (CDN) to bridge the domain gap. CDN is designed to encode\ndifferent domain inputs into a shared latent space, where the features from\ndifferent domains carry the same domain attribute. To achieve this, we first\ndisentangle the domain-specific attribute out of the semantic features from one\ndomain via a domain embedding module, which learns a domain-vector to\ncharacterize the corresponding domain attribute information. Then this\ndomain-vector is used to encode the features from another domain through a\nconditional normalization, resulting in different domains' features carrying\nthe same domain attribute. We incorporate CDN into various convolution stages\nof an object detector to adaptively address the domain shifts of different\nlevel's representation. In contrast to existing adaptation works that conduct\ndomain confusion learning on semantic features to remove domain-specific\nfactors, CDN aligns different domain distributions by modulating the semantic\nfeatures of one domain conditioned on the learned domain-vector of another\ndomain. Extensive experiments show that CDN outperforms existing methods\nremarkably on both real-to-real and synthetic-to-real adaptation benchmarks,\nincluding 2D image detection and 3D point cloud detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 08:27:29 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 04:18:13 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Su", "Peng", ""], ["Wang", "Kun", ""], ["Zeng", "Xingyu", ""], ["Tang", "Shixiang", ""], ["Chen", "Dapeng", ""], ["Qiu", "Di", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2003.07072", "submitter": "Shilei Cao", "authors": "Shuxin Wang, Shilei Cao, Dong Wei, Renzhen Wang, Kai Ma, Liansheng\n  Wang, Deyu Meng, and Yefeng Zheng", "title": "LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence\n  for One-shot Medical Image Segmentation", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a one-shot segmentation method to alleviate the burden of manual\nannotation for medical images. The main idea is to treat one-shot segmentation\nas a classical atlas-based segmentation problem, where voxel-wise\ncorrespondence from the atlas to the unlabelled data is learned. Subsequently,\nsegmentation label of the atlas can be transferred to the unlabelled data with\nthe learned correspondence. However, since ground truth correspondence between\nimages is usually unavailable, the learning system must be well-supervised to\navoid mode collapse and convergence failure. To overcome this difficulty, we\nresort to the forward-backward consistency, which is widely used in\ncorrespondence problems, and additionally learn the backward correspondences\nfrom the warped atlases back to the original atlas. This cycle-correspondence\nlearning design enables a variety of extra, cycle-consistency-based supervision\nsignals to make the training process stable, while also boost the performance.\nWe demonstrate the superiority of our method over both deep learning-based\none-shot segmentation methods and a classical multi-atlas segmentation method\nvia thorough experiments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 08:36:17 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 10:47:57 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 04:50:48 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Shuxin", ""], ["Cao", "Shilei", ""], ["Wei", "Dong", ""], ["Wang", "Renzhen", ""], ["Ma", "Kai", ""], ["Wang", "Liansheng", ""], ["Meng", "Deyu", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2003.07080", "submitter": "Zheng Ge", "authors": "Zheng Ge, Zequn Jie, Xin Huang, Rong Xu and Osamu Yoshie", "title": "PS-RCNN: Detecting Secondary Human Instances in a Crowd via Primary\n  Object Suppression", "comments": "6pages, accepted by ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting human bodies in highly crowded scenes is a challenging problem. Two\nmain reasons result in such a problem: 1). weak visual cues of heavily occluded\ninstances can hardly provide sufficient information for accurate detection; 2).\nheavily occluded instances are easier to be suppressed by\nNon-Maximum-Suppression (NMS). To address these two issues, we introduce a\nvariant of two-stage detectors called PS-RCNN. PS-RCNN first detects\nslightly/none occluded objects by an R-CNN module (referred as P-RCNN), and\nthen suppress the detected instances by human-shaped masks so that the features\nof heavily occluded instances can stand out. After that, PS-RCNN utilizes\nanother R-CNN module specialized in heavily occluded human detection (referred\nas S-RCNN) to detect the rest missed objects by P-RCNN. Final results are the\nensemble of the outputs from these two R-CNNs. Moreover, we introduce a High\nResolution RoI Align (HRRA) module to retain as much of fine-grained features\nof visible parts of the heavily occluded humans as possible. Our PS-RCNN\nsignificantly improves recall and AP by 4.49% and 2.92% respectively on\nCrowdHuman, compared to the baseline. Similar improvements on Widerperson are\nalso achieved by the PS-RCNN.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 09:03:45 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ge", "Zheng", ""], ["Jie", "Zequn", ""], ["Huang", "Xin", ""], ["Xu", "Rong", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2003.07098", "submitter": "Hina Shakir", "authors": "Hina Shakir, Haroon Rasheed and Tariq Mairaj Rasool Khan", "title": "Radiomic feature selection for lung cancer classifiers", "comments": null, "journal-ref": "Journal of Intelligent & Fuzzy Systems, vol. Pre-press, no.\n  Pre-press, pp. 1-9, 2020", "doi": "10.3233/JIFS-179672", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods with quantitative imaging features integration have\nrecently gained a lot of attention for lung nodule classification. However,\nthere is a dearth of studies in the literature on effective features ranking\nmethods for classification purpose. Moreover, optimal number of features\nrequired for the classification task also needs to be evaluated. In this study,\nwe investigate the impact of supervised and unsupervised feature selection\ntechniques on machine learning methods for nodule classification in Computed\nTomography (CT) images. The research work explores the classification\nperformance of Naive Bayes and Support Vector Machine(SVM) when trained with 2,\n4, 8, 12, 16 and 20 highly ranked features from supervised and unsupervised\nranking approaches. The best classification results were achieved using SVM\ntrained with 8 radiomic features selected from supervised feature ranking\nmethods and the accuracy was 100%. The study further revealed that very good\nnodule classification can be achieved by training any of the SVM or Naive Bayes\nwith a fewer radiomic features. A periodic increment in the number of radiomic\nfeatures from 2 to 20 did not improve the classification results whether the\nselection was made using supervised or unsupervised ranking approaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 10:20:24 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Shakir", "Hina", ""], ["Rasheed", "Haroon", ""], ["Khan", "Tariq Mairaj Rasool", ""]]}, {"id": "2003.07101", "submitter": "Moritz Kampelm\\\"uhler", "authors": "Moritz Kampelm\\\"uhler and Axel Pinz", "title": "Synthesizing human-like sketches from natural images using a conditional\n  convolutional decoder", "comments": "In IEEE Winter Conference on Applications of Computer Vision (WACV)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to precisely communicate diverse concepts by employing\nsketches, a highly reduced and abstract shape based representation of visual\ncontent. We propose, for the first time, a fully convolutional end-to-end\narchitecture that is able to synthesize human-like sketches of objects in\nnatural images with potentially cluttered background. To enable an architecture\nto learn this highly abstract mapping, we employ the following key components:\n(1) a fully convolutional encoder-decoder structure, (2) a perceptual\nsimilarity loss function operating in an abstract feature space and (3)\nconditioning of the decoder on the label of the object that shall be sketched.\nGiven the combination of these architectural concepts, we can train our\nstructure in an end-to-end supervised fashion on a collection of sketch-image\npairs. The generated sketches of our architecture can be classified with 85.6%\nTop-5 accuracy and we verify their visual quality via a user study. We find\nthat deep features as a perceptual similarity metric enable image translation\nwith large domain gaps and our findings further show that convolutional neural\nnetworks trained on image classification tasks implicitly learn to encode shape\ninformation. Code is available under\nhttps://github.com/kampelmuehler/synthesizing_human_like_sketches\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 10:42:53 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Kampelm\u00fchler", "Moritz", ""], ["Pinz", "Axel", ""]]}, {"id": "2003.07111", "submitter": "Marcus Valtonen \\\"Ornhag", "authors": "Marcus Valtonen \\\"Ornhag and Patrik Persson and M{\\aa}rten Wadenb\\\"ack\n  and Kalle {\\AA}str\\\"om and Anders Heyden", "title": "Minimal Solvers for Indoor UAV Positioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a collection of relative pose problems which arise\nnaturally in applications for visual indoor UAV navigation. We focus on cases\nwhere additional information from an onboard IMU is available and thus provides\na partial extrinsic calibration through the gravitational vector. The solvers\nare designed for a partially calibrated camera, for a variety of realistic\nindoor scenarios, which makes it possible to navigate using images of the\nground floor. Current state-of-the-art solvers use more general assumptions,\nsuch as using arbitrary planar structures; however, these solvers do not yield\nadequate reconstructions for real scenes, nor do they perform fast enough to be\nincorporated in real-time systems.\n  We show that the proposed solvers enjoy better numerical stability, are\nfaster, and require fewer point correspondences, compared to state-of-the-art\nsolvers. These properties are vital components for robust navigation in\nreal-time systems, and we demonstrate on both synthetic and real data that our\nmethod outperforms other methods, and yields superior motion estimation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:07:38 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["\u00d6rnhag", "Marcus Valtonen", ""], ["Persson", "Patrik", ""], ["Wadenb\u00e4ck", "M\u00e5rten", ""], ["\u00c5str\u00f6m", "Kalle", ""], ["Heyden", "Anders", ""]]}, {"id": "2003.07119", "submitter": "Ruofan Zhou", "authors": "Majed El Helou, Ruofan Zhou, Sabine S\\\"usstrunk", "title": "Stochastic Frequency Masking to Improve Super-Resolution and Denoising\n  Networks", "comments": "ECCV 2020. Project page: https://github.com/majedelhelou/SFM", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution and denoising are ill-posed yet fundamental image\nrestoration tasks. In blind settings, the degradation kernel or the noise level\nare unknown. This makes restoration even more challenging, notably for\nlearning-based methods, as they tend to overfit to the degradation seen during\ntraining. We present an analysis, in the frequency domain, of\ndegradation-kernel overfitting in super-resolution and introduce a conditional\nlearning perspective that extends to both super-resolution and denoising.\nBuilding on our formulation, we propose a stochastic frequency masking of\nimages used in training to regularize the networks and address the overfitting\nproblem. Our technique improves state-of-the-art methods on blind\nsuper-resolution with different synthetic kernels, real super-resolution, blind\nGaussian denoising, and real-image denoising.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:21:20 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 13:47:18 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 15:26:52 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Helou", "Majed El", ""], ["Zhou", "Ruofan", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2003.07137", "submitter": "Pedro Miraldo", "authors": "Romulo T. Rodrigues, Pedro Miraldo, Dimos V. Dimarogonas, and A. Pedro\n  Aguiar", "title": "Active Depth Estimation: Stability Analysis and its Applications", "comments": "7 pages, 3 figures, conference", "journal-ref": "International Conference on Robotics and Automation (ICRA), 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering the 3D structure of the surrounding environment is an essential\ntask in any vision-controlled Structure-from-Motion (SfM) scheme. This paper\nfocuses on the theoretical properties of the SfM, known as the incremental\nactive depth estimation. The term incremental stands for estimating the 3D\nstructure of the scene over a chronological sequence of image frames. Active\nmeans that the camera actuation is such that it improves estimation\nperformance. Starting from a known depth estimation filter, this paper presents\nthe stability analysis of the filter in terms of the control inputs of the\ncamera. By analyzing the convergence of the estimator using the Lyapunov\ntheory, we relax the constraints on the projection of the 3D point in the image\nplane when compared to previous results. Nonetheless, our method is capable of\ndealing with the cameras' limited field-of-view constraints. The main results\nare validated through experiments with simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 12:12:24 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Rodrigues", "Romulo T.", ""], ["Miraldo", "Pedro", ""], ["Dimarogonas", "Dimos V.", ""], ["Aguiar", "A. Pedro", ""]]}, {"id": "2003.07139", "submitter": "Huibing Wang", "authors": "Huibing Wang, Jinjia Peng, Guangqi Jiang, Fengqiang Xu, Xianping Fu", "title": "Discriminative Feature and Dictionary Learning with Part-aware Model for\n  Vehicle Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of smart cities, urban surveillance video analysis will\nplay a further significant role in intelligent transportation systems.\nIdentifying the same target vehicle in large datasets from non-overlapping\ncameras should be highlighted, which has grown into a hot topic in promoting\nintelligent transportation systems. However, vehicle re-identification (re-ID)\ntechnology is a challenging task since vehicles of the same design or\nmanufacturer show similar appearance. To fill these gaps, we tackle this\nchallenge by proposing Triplet Center Loss based Part-aware Model (TCPM) that\nleverages the discriminative features in part details of vehicles to refine the\naccuracy of vehicle re-identification. TCPM base on part discovery is that\npartitions the vehicle from horizontal and vertical directions to strengthen\nthe details of the vehicle and reinforce the internal consistency of the parts.\nIn addition, to eliminate intra-class differences in local regions of the\nvehicle, we propose external memory modules to emphasize the consistency of\neach part to learn the discriminating features, which forms a global dictionary\nover all categories in dataset. In TCPM, triplet-center loss is introduced to\nensure each part of vehicle features extracted has intra-class consistency and\ninter-class separability. Experimental results show that our proposed TCPM has\nan enormous preference over the existing state-of-the-art methods on benchmark\ndatasets VehicleID and VeRi-776.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 12:15:31 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Wang", "Huibing", ""], ["Peng", "Jinjia", ""], ["Jiang", "Guangqi", ""], ["Xu", "Fengqiang", ""], ["Fu", "Xianping", ""]]}, {"id": "2003.07167", "submitter": "Cai Shaofeng", "authors": "Chengxin Wang, Shaofeng Cai, Gary Tan", "title": "GraphTCN: Spatio-Temporal Interaction Modeling for Human Trajectory\n  Prediction", "comments": "10 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future paths of an agent's neighbors accurately and in a\ntimely manner is central to the autonomous applications for collision\navoidance. Conventional approaches, e.g., LSTM-based models, take considerable\ncomputational costs in the prediction, especially for the long sequence\nprediction. To support more efficient and accurate trajectory predictions, we\npropose a novel CNN-based spatial-temporal graph framework GraphTCN, which\nmodels the spatial interactions as social graphs and captures the\nspatio-temporal interactions with a modified temporal convolutional network. In\ncontrast to conventional models, both the spatial and temporal modeling of our\nmodel are computed within each local time window. Therefore, it can be executed\nin parallel for much higher efficiency, and meanwhile with accuracy comparable\nto best-performing approaches. Experimental results confirm that our model\nachieves better performance in terms of both efficiency and accuracy as\ncompared with state-of-the-art models on various trajectory prediction\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 12:56:12 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 16:28:11 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 15:05:04 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 12:44:11 GMT"}, {"version": "v5", "created": "Sun, 8 Nov 2020 08:03:27 GMT"}, {"version": "v6", "created": "Wed, 10 Mar 2021 06:21:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Chengxin", ""], ["Cai", "Shaofeng", ""], ["Tan", "Gary", ""]]}, {"id": "2003.07177", "submitter": "Piao Huang", "authors": "Piao Huang, Shoudong Han, Jun Zhao, Donghaisheng Liu, Hongwei Wang, En\n  Yu, and Alex ChiChung Kot", "title": "Refinements in Motion and Appearance for Online Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern multi-object tracking (MOT) system usually involves separated modules,\nsuch as motion model for location and appearance model for data association.\nHowever, the compatible problems within both motion and appearance models are\nalways ignored. In this paper, a general architecture named as MIF is presented\nby seamlessly blending the Motion integration, three-dimensional(3D) Integral\nimage and adaptive appearance feature Fusion. Since the uncertain pedestrian\nand camera motions are usually handled separately, the integrated motion model\nis designed using our defined intension of camera motion. Specifically, a 3D\nintegral image based spatial blocking method is presented to efficiently cut\nuseless connections between trajectories and candidates with spatial\nconstraints. Then the appearance model and visibility prediction are jointly\nbuilt. Considering scale, pose and visibility, the appearance features are\nadaptively fused to overcome the feature misalignment problem. Our MIF based\ntracker (MIFT) achieves the state-of-the-art accuracy with 60.1 MOTA on both\nMOT16&17 challenges.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 13:12:37 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 08:15:51 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Huang", "Piao", ""], ["Han", "Shoudong", ""], ["Zhao", "Jun", ""], ["Liu", "Donghaisheng", ""], ["Wang", "Hongwei", ""], ["Yu", "En", ""], ["Kot", "Alex ChiChung", ""]]}, {"id": "2003.07212", "submitter": "Sheng He", "authors": "Sheng He, Lambert Schomaker", "title": "FragNet: Writer Identification using Deep Fragment Networks", "comments": null, "journal-ref": "IEEE Trans. on Information Forensic and Security, 2020", "doi": "10.1109/TIFS.2020.2981236", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writer identification based on a small amount of text is a challenging\nproblem. In this paper, we propose a new benchmark study for writer\nidentification based on word or text block images which approximately contain\none word. In order to extract powerful features on these word images, a deep\nneural network, named FragNet, is proposed. The FragNet has two pathways:\nfeature pyramid which is used to extract feature maps and fragment pathway\nwhich is trained to predict the writer identity based on fragments extracted\nfrom the input image and the feature maps on the feature pyramid. We conduct\nexperiments on four benchmark datasets, which show that our proposed method can\ngenerate efficient and robust deep representations for writer identification\nbased on both word and page images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 13:42:28 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:16:25 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["He", "Sheng", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2003.07216", "submitter": "Matteo Figini", "authors": "Matteo Figini (1), Hongxiang Lin (1), Godwin Ogbole (2), Felice D Arco\n  (3), Stefano B. Blumberg (1), David W. Carmichael (4 and 5), Ryutaro Tanno (1\n  and 6), Enrico Kaden (1 and 4), Biobele J. Brown (7), Ikeoluwa Lagunju (7),\n  Helen J. Cross (3 and 4), Delmiro Fernandez-Reyes (1 and 7), Daniel C.\n  Alexander (1) ((1) Centre for Medical Image Computing and Department of\n  Computer Science - University College London - UK, (2) Department of\n  Radiology - College of Medicine - University of Ibadan - Nigeria, (3) Great\n  Ormond Street Hospital for Children - London - UK, (4) UCL Great Ormond\n  Street Institute of Child Health - London - UK, (5) Department of Biomedical\n  Engineering - Kings College London - UK, (6) Machine Intelligence and\n  Perception Group - Microsoft Research Cambridge - UK, (7) Department of\n  Paediatrics - College of Medicine - University of Ibadan - Nigeria)", "title": "Image Quality Transfer Enhances Contrast and Resolution of Low-Field\n  Brain MRI in African Paediatric Epilepsy Patients", "comments": "6 pages, 3 figures, accepted at ICLR 2020 workshop on Artificial\n  Intelligence for Affordable Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1.5T or 3T scanners are the current standard for clinical MRI, but low-field\n(<1T) scanners are still common in many lower- and middle-income countries for\nreasons of cost and robustness to power failures. Compared to modern high-field\nscanners, low-field scanners provide images with lower signal-to-noise ratio at\nequivalent resolution, leaving practitioners to compensate by using large slice\nthickness and incomplete spatial coverage. Furthermore, the contrast between\ndifferent types of brain tissue may be substantially reduced even at equal\nsignal-to-noise ratio, which limits diagnostic value. Recently the paradigm of\nImage Quality Transfer has been applied to enhance 0.36T structural images\naiming to approximate the resolution, spatial coverage, and contrast of typical\n1.5T or 3T images. A variant of the neural network U-Net was trained using\nlow-field images simulated from the publicly available 3T Human Connectome\nProject dataset. Here we present qualitative results from real and simulated\nclinical low-field brain images showing the potential value of IQT to enhance\nthe clinical utility of readily accessible low-field MRIs in the management of\nepilepsy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 13:46:58 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 19:52:52 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Figini", "Matteo", "", "4 and 5"], ["Lin", "Hongxiang", "", "4 and 5"], ["Ogbole", "Godwin", "", "4 and 5"], ["Arco", "Felice D", "", "4 and 5"], ["Blumberg", "Stefano B.", "", "4 and 5"], ["Carmichael", "David W.", "", "4 and 5"], ["Tanno", "Ryutaro", "", "1\n  and 6"], ["Kaden", "Enrico", "", "1 and 4"], ["Brown", "Biobele J.", "", "3 and 4"], ["Lagunju", "Ikeoluwa", "", "3 and 4"], ["Cross", "Helen J.", "", "3 and 4"], ["Fernandez-Reyes", "Delmiro", "", "1 and 7"], ["Alexander", "Daniel C.", ""]]}, {"id": "2003.07232", "submitter": "Feng Zhang", "authors": "Hanbin Dai, Liangbo Zhou, Feng Zhang, Zhengyu Zhang, Hong Hu, Xiatian\n  Zhu, Mao Ye", "title": "Joint COCO and Mapillary Workshop at ICCV 2019 Keypoint Detection\n  Challenge Track Technical Report: Distribution-Aware Coordinate\n  Representation for Human Pose Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.06278", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the coordinate representation in human pose\nestimation. While being the standard choice, heatmap based representation has\nnot been systematically investigated. We found that the process of coordinate\ndecoding (i.e. transforming the predicted heatmaps to the coordinates) is\nsurprisingly significant for human pose estimation performance, which\nnevertheless was not recognised before. In light of the discovered importance,\nwe further probe the design limitations of the standard coordinate decoding\nmethod and propose a principled distribution-aware decoding method. Meanwhile,\nwe improve the standard coordinate encoding process (i.e. transforming\nground-truth coordinates to heatmaps) by generating accurate heatmap\ndistributions for unbiased model training. Taking them together, we formulate a\nnovel Distribution-Aware coordinate Representation for Keypoint (DARK) method.\nServing as a model-agnostic plug-in, DARK significantly improves the\nperformance of a variety of state-of-the-art human pose estimation models.\nExtensive experiments show that DARK yields the best results on COCO keypoint\ndetection challenge, validating the usefulness and effectiveness of our novel\ncoordinate representation idea. The project page containing more details is at\nhttps://ilovepose.github.io/coco\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 10:22:36 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Dai", "Hanbin", ""], ["Zhou", "Liangbo", ""], ["Zhang", "Feng", ""], ["Zhang", "Zhengyu", ""], ["Hu", "Hong", ""], ["Zhu", "Xiatian", ""], ["Ye", "Mao", ""]]}, {"id": "2003.07238", "submitter": "Xianzhi Li", "authors": "Xianzhi Li and Ruihui Li and Guangyong Chen and Chi-Wing Fu and Daniel\n  Cohen-Or and Pheng-Ann Heng", "title": "A Rotation-Invariant Framework for Deep Point Cloud Analysis", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2021.3092570", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many deep neural networks were designed to process 3D point clouds,\nbut a common drawback is that rotation invariance is not ensured, leading to\npoor generalization to arbitrary orientations. In this paper, we introduce a\nnew low-level purely rotation-invariant representation to replace common 3D\nCartesian coordinates as the network inputs. Also, we present a network\narchitecture to embed these representations into features, encoding local\nrelations between points and their neighbors, and the global shape structure.\nTo alleviate inevitable global information loss caused by the\nrotation-invariant representations, we further introduce a region relation\nconvolution to encode local and non-local information. We evaluate our method\non multiple point cloud analysis tasks, including shape classification, part\nsegmentation, and shape retrieval. Experimental results show that our method\nachieves consistent, and also the best performance, on inputs at arbitrary\norientations, compared with the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 14:04:45 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 09:36:16 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Li", "Xianzhi", ""], ["Li", "Ruihui", ""], ["Chen", "Guangyong", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2003.07254", "submitter": "Chao Wen", "authors": "Jiashun Wang, Chao Wen, Yanwei Fu, Haitao Lin, Tianyun Zou, Xiangyang\n  Xue, Yinda Zhang", "title": "Neural Pose Transfer by Spatially Adaptive Instance Normalization", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose transfer has been studied for decades, in which the pose of a source\nmesh is applied to a target mesh. Particularly in this paper, we are interested\nin transferring the pose of source human mesh to deform the target human mesh,\nwhile the source and target meshes may have different identity information.\nTraditional studies assume that the paired source and target meshes are existed\nwith the point-wise correspondences of user annotated landmarks/mesh points,\nwhich requires heavy labelling efforts. On the other hand, the generalization\nability of deep models is limited, when the source and target meshes have\ndifferent identities. To break this limitation, we proposes the first neural\npose transfer model that solves the pose transfer via the latest technique for\nimage style transfer, leveraging the newly proposed component -- spatially\nadaptive instance normalization. Our model does not require any correspondences\nbetween the source and target meshes. Extensive experiments show that the\nproposed model can effectively transfer deformation from source to target\nmeshes, and has good generalization ability to deal with unseen identities or\nposes of meshes. Code is available at\nhttps://github.com/jiashunwang/Neural-Pose-Transfer .\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 14:33:59 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 17:08:21 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Wang", "Jiashun", ""], ["Wen", "Chao", ""], ["Fu", "Yanwei", ""], ["Lin", "Haitao", ""], ["Zou", "Tianyun", ""], ["Xue", "Xiangyang", ""], ["Zhang", "Yinda", ""]]}, {"id": "2003.07258", "submitter": "Wojciech Samek", "authors": "Leila Arras, Ahmed Osman, Wojciech Samek", "title": "Ground Truth Evaluation of Neural Network Explanations with CLEVR-XAI", "comments": "37 pages, 9 tables, 2 figures (plus appendix 14 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of deep learning in today's applications entailed an increasing need\nin explaining the model's decisions beyond prediction performances in order to\nfoster trust and accountability. Recently, the field of explainable AI (XAI)\nhas developed methods that provide such explanations for already trained neural\nnetworks. In computer vision tasks such explanations, termed heatmaps,\nvisualize the contributions of individual pixels to the prediction. So far XAI\nmethods along with their heatmaps were mainly validated qualitatively via\nhuman-based assessment, or evaluated through auxiliary proxy tasks such as\npixel perturbation, weak object localization or randomization tests. Due to the\nlack of an objective and commonly accepted quality measure for heatmaps, it was\ndebatable which XAI method performs best and whether explanations can be\ntrusted at all. In the present work, we tackle the problem by proposing a\nground truth based evaluation framework for XAI methods based on the CLEVR\nvisual question answering task. Our framework provides a (1) selective, (2)\ncontrolled and (3) realistic testbed for the evaluation of neural network\nexplanations. We compare ten different explanation methods, resulting in new\ninsights about the quality and properties of XAI methods, sometimes\ncontradicting with conclusions from previous comparative studies. The CLEVR-XAI\ndataset and the benchmarking code can be found at\nhttps://github.com/ahmedmagdiosman/clevr-xai.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 14:43:33 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 16:18:05 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Arras", "Leila", ""], ["Osman", "Ahmed", ""], ["Samek", "Wojciech", ""]]}, {"id": "2003.07289", "submitter": "Kaichen Zhou", "authors": "Kaichen Zhou, Changhao Chen, Bing Wang, Muhamad Risqi U. Saputra, Niki\n  Trigoni, Andrew Markham", "title": "VMLoc: Variational Fusion For Learning-Based Multimodal Camera\n  Localization", "comments": null, "journal-ref": "The Thirty-Fifth AAAI Conference on Artificial Intelligence\n  (AAAI-2021)", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent learning-based approaches have achieved impressive results in the\nfield of single-shot camera localization. However, how best to fuse multiple\nmodalities (e.g., image and depth) and to deal with degraded or missing input\nare less well studied. In particular, we note that previous approaches towards\ndeep fusion do not perform significantly better than models employing a single\nmodality. We conjecture that this is because of the naive approaches to feature\nspace fusion through summation or concatenation which do not take into account\nthe different strengths of each modality. To address this, we propose an\nend-to-end framework, termed VMLoc, to fuse different sensor inputs into a\ncommon latent space through a variational Product-of-Experts (PoE) followed by\nattention-based fusion. Unlike previous multimodal variational works directly\nadapting the objective function of vanilla variational auto-encoder, we show\nhow camera localization can be accurately estimated through an unbiased\nobjective function based on importance weighting. Our model is extensively\nevaluated on RGB-D datasets and the results prove the efficacy of our model.\nThe source code is available at https://github.com/Zalex97/VMLoc.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 14:52:10 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 12:39:06 GMT"}, {"version": "v3", "created": "Sun, 16 Aug 2020 05:39:18 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 00:46:12 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhou", "Kaichen", ""], ["Chen", "Changhao", ""], ["Wang", "Bing", ""], ["Saputra", "Muhamad Risqi U.", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2003.07304", "submitter": "Ze Yang", "authors": "Ze Yang (1), Yali Wang (1), Xianyu Chen (1), Jianzhuang Liu (2), Yu\n  Qiao (1 and 3) ((1) ShenZhen Key Lab of Computer Vision and Pattern\n  Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of Advanced\n  Technology, Chinese Academy of Sciences, (2) Huawei Noah's Ark Lab, (3) SIAT\n  Branch, Shenzhen Institute of Artificial Intelligence and Robotics for\n  Society)", "title": "Context-Transformer: Tackling Object Confusion for Few-Shot Detection", "comments": "Accepted by AAAI-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection is a challenging but realistic scenario, where only\na few annotated training images are available for training detectors. A popular\napproach to handle this problem is transfer learning, i.e., fine-tuning a\ndetector pretrained on a source-domain benchmark. However, such transferred\ndetector often fails to recognize new objects in the target domain, due to low\ndata diversity of training samples. To tackle this problem, we propose a novel\nContext-Transformer within a concise deep transfer framework. Specifically,\nContext-Transformer can effectively leverage source-domain object knowledge as\nguidance, and automatically exploit contexts from only a few training images in\nthe target domain. Subsequently, it can adaptively integrate these relational\nclues to enhance the discriminative power of detector, in order to reduce\nobject confusion in few-shot scenarios. Moreover, Context-Transformer is\nflexibly embedded in the popular SSD-style detectors, which makes it a\nplug-and-play module for end-to-end few-shot learning. Finally, we evaluate\nContext-Transformer on the challenging settings of few-shot detection and\nincremental few-shot detection. The experimental results show that, our\nframework outperforms the recent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:17:11 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Yang", "Ze", "", "1 and 3"], ["Wang", "Yali", "", "1 and 3"], ["Chen", "Xianyu", "", "1 and 3"], ["Liu", "Jianzhuang", "", "1 and 3"], ["Qiao", "Yu", "", "1 and 3"]]}, {"id": "2003.07311", "submitter": "Suprosanna Shit", "authors": "Suprosanna Shit, Johannes C. Paetzold, Anjany Sekuboyina, Ivan Ezhov,\n  Alexander Unger, Andrey Zhylka, Josien P. W. Pluim, Ulrich Bauer, Bjoern H.\n  Menze", "title": "clDice -- a Novel Topology-Preserving Loss Function for Tubular\n  Structure Segmentation", "comments": "* The authors Suprosanna Shit and Johannes C. Paetzold contributed\n  equally to the work", "journal-ref": null, "doi": null, "report-no": "Accepted in CVPR 2021", "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of tubular, network-like structures, such as vessels,\nneurons, or roads, is relevant to many fields of research. For such structures,\nthe topology is their most important characteristic; particularly preserving\nconnectedness: in the case of vascular networks, missing a connected vessel\nentirely alters the blood-flow dynamics. We introduce a novel similarity\nmeasure termed centerlineDice (short clDice), which is calculated on the\nintersection of the segmentation masks and their (morphological) skeleta. We\ntheoretically prove that clDice guarantees topology preservation up to homotopy\nequivalence for binary 2D and 3D segmentation. Extending this, we propose a\ncomputationally efficient, differentiable loss function (soft-clDice) for\ntraining arbitrary neural segmentation networks. We benchmark the soft-clDice\nloss on five public datasets, including vessels, roads and neurons (2D and 3D).\nTraining on soft-clDice leads to segmentation with more accurate connectivity\ninformation, higher graph similarity, and better volumetric scores.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:27:49 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 20:45:16 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 22:46:43 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 19:53:43 GMT"}, {"version": "v5", "created": "Mon, 29 Mar 2021 13:36:28 GMT"}, {"version": "v6", "created": "Tue, 30 Mar 2021 11:51:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shit", "Suprosanna", ""], ["Paetzold", "Johannes C.", ""], ["Sekuboyina", "Anjany", ""], ["Ezhov", "Ivan", ""], ["Unger", "Alexander", ""], ["Zhylka", "Andrey", ""], ["Pluim", "Josien P. W.", ""], ["Bauer", "Ulrich", ""], ["Menze", "Bjoern H.", ""]]}, {"id": "2003.07325", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Yongxin Yang, Yu Qiao, Tao Xiang", "title": "Domain Adaptive Ensemble Learning", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of generalizing deep neural networks from multiple source domains\nto a target one is studied under two settings: When unlabeled target data is\navailable, it is a multi-source unsupervised domain adaptation (UDA) problem,\notherwise a domain generalization (DG) problem. We propose a unified framework\ntermed domain adaptive ensemble learning (DAEL) to address both problems. A\nDAEL model is composed of a CNN feature extractor shared across domains and\nmultiple classifier heads each trained to specialize in a particular source\ndomain. Each such classifier is an expert to its own domain and a non-expert to\nothers. DAEL aims to learn these experts collaboratively so that when forming\nan ensemble, they can leverage complementary information from each other to be\nmore effective for an unseen target domain. To this end, each source domain is\nused in turn as a pseudo-target-domain with its own expert providing\nsupervisory signal to the ensemble of non-experts learned from the other\nsources. For unlabeled target data under the UDA setting where real expert does\nnot exist, DAEL uses pseudo-label to supervise the ensemble learning. Extensive\nexperiments on three multi-source UDA datasets and two DG datasets show that\nDAEL improves the state of the art on both problems, often by significant\nmargins. The code is released at\n\\url{https://github.com/KaiyangZhou/Dassl.pytorch}.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:54:15 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 15:43:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Yang", "Yongxin", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""]]}, {"id": "2003.07326", "submitter": "Le Yang", "authors": "Le Yang, Yizeng Han, Xi Chen, Shiji Song, Jifeng Dai, Gao Huang", "title": "Resolution Adaptive Networks for Efficient Inference", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive inference is an effective mechanism to achieve a dynamic tradeoff\nbetween accuracy and computational cost in deep networks. Existing works mainly\nexploit architecture redundancy in network depth or width. In this paper, we\nfocus on spatial redundancy of input samples and propose a novel Resolution\nAdaptive Network (RANet), which is inspired by the intuition that\nlow-resolution representations are sufficient for classifying \"easy\" inputs\ncontaining large objects with prototypical features, while only some \"hard\"\nsamples need spatially detailed information. In RANet, the input images are\nfirst routed to a lightweight sub-network that efficiently extracts\nlow-resolution representations, and those samples with high prediction\nconfidence will exit early from the network without being further processed.\nMeanwhile, high-resolution paths in the network maintain the capability to\nrecognize the \"hard\" samples. Therefore, RANet can effectively reduce the\nspatial redundancy involved in inferring high-resolution inputs. Empirically,\nwe demonstrate the effectiveness of the proposed RANet on the CIFAR-10,\nCIFAR-100 and ImageNet datasets in both the anytime prediction setting and the\nbudgeted batch classification setting.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 16:54:36 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 17:17:32 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 05:00:06 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 11:14:36 GMT"}, {"version": "v5", "created": "Mon, 18 May 2020 04:49:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Yang", "Le", ""], ["Han", "Yizeng", ""], ["Chen", "Xi", ""], ["Song", "Shiji", ""], ["Dai", "Jifeng", ""], ["Huang", "Gao", ""]]}, {"id": "2003.07333", "submitter": "Sylvain Lobry", "authors": "Sylvain Lobry, Diego Marcos, Jesse Murray, Devis Tuia", "title": "RSVQA: Visual Question Answering for Remote Sensing Data", "comments": "12 pages, Published in IEEE Transactions on Geoscience and Remote\n  Sensing. Added one experiment and authors' biographies", "journal-ref": null, "doi": "10.1109/TGRS.2020.2988782", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the task of visual question answering for remote\nsensing data (RSVQA). Remote sensing images contain a wealth of information\nwhich can be useful for a wide range of tasks including land cover\nclassification, object counting or detection. However, most of the available\nmethodologies are task-specific, thus inhibiting generic and easy access to the\ninformation contained in remote sensing data. As a consequence, accurate remote\nsensing product generation still requires expert knowledge. With RSVQA, we\npropose a system to extract information from remote sensing data that is\naccessible to every user: we use questions formulated in natural language and\nuse them to interact with the images. With the system, images can be queried to\nobtain high level information specific to the image content or relational\ndependencies between objects visible in the images. Using an automatic method\nintroduced in this article, we built two datasets (using low and high\nresolution data) of image/question/answer triplets. The information required to\nbuild the questions and answers is queried from OpenStreetMap (OSM). The\ndatasets can be used to train (when using supervised methods) and evaluate\nmodels to solve the RSVQA task. We report the results obtained by applying a\nmodel based on Convolutional Neural Networks (CNNs) for the visual part and on\na Recurrent Neural Network (RNN) for the natural language part to this task.\nThe model is trained on the two datasets, yielding promising results in both\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:09:31 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 14:05:28 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lobry", "Sylvain", ""], ["Marcos", "Diego", ""], ["Murray", "Jesse", ""], ["Tuia", "Devis", ""]]}, {"id": "2003.07335", "submitter": "Sarah Ostadabbas", "authors": "Behnaz Rezaei, Amirreza Farnoosh, and Sarah Ostadabbas", "title": "G-LBM:Generative Low-dimensional Background Model Estimation from Video\n  Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a computationally tractable and theoretically\nsupported non-linear low-dimensional generative model to represent real-world\ndata in the presence of noise and sparse outliers. The non-linear\nlow-dimensional manifold discovery of data is done through describing a joint\ndistribution over observations, and their low-dimensional representations (i.e.\nmanifold coordinates). Our model, called generative low-dimensional background\nmodel (G-LBM) admits variational operations on the distribution of the manifold\ncoordinates and simultaneously generates a low-rank structure of the latent\nmanifold given the data. Therefore, our probabilistic model contains the\nintuition of the non-probabilistic low-dimensional manifold learning. G-LBM\nselects the intrinsic dimensionality of the underling manifold of the\nobservations, and its probabilistic nature models the noise in the observation\ndata. G-LBM has direct application in the background scenes model estimation\nfrom video sequences and we have evaluated its performance on SBMnet-2016 and\nBMC2012 datasets, where it achieved a performance higher or comparable to other\nstate-of-the-art methods while being agnostic to the background scenes in\nvideos. Besides, in challenges such as camera jitter and background motion,\nG-LBM is able to robustly estimate the background by effectively modeling the\nuncertainties in video observations in these scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:10:09 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 16:18:47 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Rezaei", "Behnaz", ""], ["Farnoosh", "Amirreza", ""], ["Ostadabbas", "Sarah", ""]]}, {"id": "2003.07340", "submitter": "Yu-Jhe Li", "authors": "Yu-Jhe Li, Zhengyi Luo, Xinshuo Weng, Kris M. Kitani", "title": "Learning Shape Representations for Clothing Variations in Person\n  Re-Identification", "comments": "11 pages, 8 figures. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) aims to recognize instances of the same\nperson contained in multiple images taken across different cameras. Existing\nmethods for re-ID tend to rely heavily on the assumption that both query and\ngallery images of the same person have the same clothing. Unfortunately, this\nassumption may not hold for datasets captured over long periods of time (e.g.,\nweeks, months or years). To tackle the re-ID problem in the context of clothing\nchanges, we propose a novel representation learning model which is able to\ngenerate a body shape feature representation without being affected by clothing\ncolor or patterns. We call our model the Color Agnostic Shape Extraction\nNetwork (CASE-Net). CASE-Net learns a representation of identity that depends\nonly on body shape via adversarial learning and feature disentanglement. Due to\nthe lack of large-scale re-ID datasets which contain clothing changes for the\nsame person, we propose two synthetic datasets for evaluation. We create a\nrendered dataset SMPL-reID with different clothes patterns and a synthesized\ndataset Div-Market with different clothing color to simulate two types of\nclothing changes. The quantitative and qualitative results across 5 datasets\n(SMPL-reID, Div-Market, two benchmark re-ID datasets, a cross-modality re-ID\ndataset) confirm the robustness and superiority of our approach against several\nstate-of-the-art approaches\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:23:50 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Li", "Yu-Jhe", ""], ["Luo", "Zhengyi", ""], ["Weng", "Xinshuo", ""], ["Kitani", "Kris M.", ""]]}, {"id": "2003.07341", "submitter": "Mazlum Ferhat Arslan", "authors": "M. Ferhat Arslan (1), Sibel Tari (1) ((1) Middle East Technical\n  University)", "title": "Complexity of Shapes Embedded in ${\\mathbb Z^n}$ with a Bias Towards\n  Squares", "comments": "13 pages, 14 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Shape complexity is a hard-to-quantify quality, mainly due to its relative\nnature. Biased by Euclidean thinking, circles are commonly considered as the\nsimplest. However, their constructions as digital images are only\napproximations to the ideal form. Consequently, complexity orders computed in\nreference to circle are unstable. Unlike circles which lose their circleness in\ndigital images, squares retain their qualities. Hence, we consider squares\n(hypercubes in $\\mathbb Z^n$) to be the simplest shapes relative to which\ncomplexity orders are constructed. Using the connection between $L^\\infty$ norm\nand squares we effectively encode squareness-adapted simplification through\nwhich we obtain multi-scale complexity measure, where scale determines the\nlevel of interest to the boundary. The emergent scale above which the effect of\na boundary feature (appendage) disappears is related to the ratio of the\ncontacting width of the appendage to that of the main body. We discuss what\nzero complexity implies in terms of information repetition and constructibility\nand what kind of shapes in addition to squares have zero complexity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:24:22 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Arslan", "M. Ferhat", ""], ["Tari", "Sibel", ""]]}, {"id": "2003.07344", "submitter": "Karan Sikka", "authors": "Karan Sikka, Andrew Silberfarb, John Byrnes, Indranil Sur, Ed Chow,\n  Ajay Divakaran, Richard Rohwer", "title": "Deep Adaptive Semantic Logic (DASL): Compiling Declarative Knowledge\n  into Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Adaptive Semantic Logic (DASL), a novel framework for\nautomating the generation of deep neural networks that incorporates\nuser-provided formal knowledge to improve learning from data. We provide formal\nsemantics that demonstrate that our knowledge representation captures all of\nfirst order logic and that finite sampling from infinite domains converges to\ncorrect truth values. DASL's representation improves on prior neural-symbolic\nwork by avoiding vanishing gradients, allowing deeper logical structure, and\nenabling richer interactions between the knowledge and learning components. We\nillustrate DASL through a toy problem in which we add structure to an image\nclassification problem and demonstrate that knowledge of that structure reduces\ndata requirements by a factor of $1000$. We then evaluate DASL on a visual\nrelationship detection task and demonstrate that the addition of commonsense\nknowledge improves performance by $10.7\\%$ in a data scarce setting.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:37:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sikka", "Karan", ""], ["Silberfarb", "Andrew", ""], ["Byrnes", "John", ""], ["Sur", "Indranil", ""], ["Chow", "Ed", ""], ["Divakaran", "Ajay", ""], ["Rohwer", "Richard", ""]]}, {"id": "2003.07356", "submitter": "Ameya Phalak", "authors": "Ameya Phalak, Vijay Badrinarayanan, Andrew Rabinovich", "title": "Scan2Plan: Efficient Floorplan Generation from 3D Scans of Indoor Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Scan2Plan, a novel approach for accurate estimation of a\nfloorplan from a 3D scan of the structural elements of indoor environments. The\nproposed method incorporates a two-stage approach where the initial stage\nclusters an unordered point cloud representation of the scene into room\ninstances and wall instances using a deep neural network based voting approach.\nThe subsequent stage estimates a closed perimeter, parameterized by a simple\npolygon, for each individual room by finding the shortest path along the\npredicted room and wall keypoints. The final floorplan is simply an assembly of\nall such room perimeters in the global co-ordinate system. The Scan2Plan\npipeline produces accurate floorplans for complex layouts, is highly\nparallelizable and extremely efficient compared to existing methods. The voting\nmodule is trained only on synthetic data and evaluated on publicly available\nStructured3D and BKE datasets to demonstrate excellent qualitative and\nquantitative results outperforming state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 17:59:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Phalak", "Ameya", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "2003.07441", "submitter": "Gustav Grund Pihlgren", "authors": "Gustav Grund Pihlgren (1), Fredrik Sandin (1), Marcus Liwicki (1) ((1)\n  Lule\\r{a} University of Technology)", "title": "Pretraining Image Encoders without Reconstruction via Feature Prediction\n  Loss", "comments": null, "journal-ref": null, "doi": "10.1109/ICPR48806.2021.9412239", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates three methods for calculating loss for\nautoencoder-based pretraining of image encoders: The commonly used\nreconstruction loss, the more recently introduced deep perceptual similarity\nloss, and a feature prediction loss proposed here; the latter turning out to be\nthe most efficient choice. Standard auto-encoder pretraining for deep learning\ntasks is done by comparing the input image and the reconstructed image. Recent\nwork shows that predictions based on embeddings generated by image autoencoders\ncan be improved by training with perceptual loss, i.e., by adding a loss\nnetwork after the decoding step. So far the autoencoders trained with loss\nnetworks implemented an explicit comparison of the original and reconstructed\nimages using the loss network. However, given such a loss network we show that\nthere is no need for the time-consuming task of decoding the entire image.\nInstead, we propose to decode the features of the loss network, hence the name\n\"feature prediction loss\". To evaluate this method we perform experiments on\nthree standard publicly available datasets (LunarLander-v2, STL-10, and SVHN)\nand compare six different procedures for training image encoders (pixel-wise,\nperceptual similarity, and feature prediction losses; combined with two\nvariations of image and feature encoding/decoding). The embedding-based\nprediction results show that encoders trained with feature prediction loss is\nas good or better than those trained with the other two losses. Additionally,\nthe encoder is significantly faster to train using feature prediction loss in\ncomparison to the other losses. The method implementation used in this work is\navailable online: https://github.com/guspih/Perceptual-Autoencoders\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 21:08:43 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 15:54:22 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Pihlgren", "Gustav Grund", ""], ["Sandin", "Fredrik", ""], ["Liwicki", "Marcus", ""]]}, {"id": "2003.07442", "submitter": "Al-Akhir Nayan", "authors": "Al-Akhir Nayan, Joyeta Saha, Ahamad Nokib Mozumder, Khan Raqib Mahmud,\n  Abul Kalam Al Azad", "title": "Real Time Multi-Class Object Detection and Recognition Using Vision\n  Augmentation Algorithm", "comments": null, "journal-ref": "International Journal of Advanced Science and Technology, Vol. 29,\n  No. 5, (2020), pp. 14070 - 14083", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this research is to detect small objects with low resolution and\nnoise. The existing real time object detection algorithm is based on the deep\nneural network of convolution need to perform multilevel convolution and\npooling operations on the entire image to extract a deep semantic\ncharacteristic of the image. The detection models perform better for large\nobjects. The features of existing models do not fully represent the essential\nfeatures of small objects after repeated convolution operations. We have\nintroduced a novel real time detection algorithm which employs upsampling and\nskip connection to extract multiscale features at different convolution levels\nin a learning task resulting a remarkable performance in detecting small\nobjects. The detection precision of the model is shown to be higher and faster\nthan that of the state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 01:08:24 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 00:42:06 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 06:32:23 GMT"}, {"version": "v4", "created": "Wed, 11 Nov 2020 18:22:22 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Nayan", "Al-Akhir", ""], ["Saha", "Joyeta", ""], ["Mozumder", "Ahamad Nokib", ""], ["Mahmud", "Khan Raqib", ""], ["Azad", "Abul Kalam Al", ""]]}, {"id": "2003.07443", "submitter": "Gustavo De Rosa", "authors": "Mateus Roder, Gustavo Henrique de Rosa, Jo\\~ao Paulo Papa", "title": "Learnergy: Energy-based Machine Learners", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the last years, machine learning techniques have been broadly\nencouraged in the context of deep learning architectures. An exciting algorithm\ndenoted as Restricted Boltzmann Machine relies on energy- and\nprobabilistic-based nature to tackle the most diverse applications, such as\nclassification, reconstruction, and generation of images and signals.\nNevertheless, one can see they are not adequately renowned compared to other\nwell-known deep learning techniques, e.g., Convolutional Neural Networks. Such\nbehavior promotes the lack of researches and implementations around the\nliterature, coping with the challenge of sufficiently comprehending these\nenergy-based systems. Therefore, in this paper, we propose a Python-inspired\nframework in the context of energy-based architectures, denoted as Learnergy.\nEssentially, Learnergy is built upon PyTorch to provide a more friendly\nenvironment and a faster prototyping workspace and possibly the usage of CUDA\ncomputations, speeding up their computational time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 21:14:32 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 15:39:03 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Roder", "Mateus", ""], ["de Rosa", "Gustavo Henrique", ""], ["Papa", "Jo\u00e3o Paulo", ""]]}, {"id": "2003.07449", "submitter": "Tristan Sylvain", "authors": "Tristan Sylvain and Pengchuan Zhang and Yoshua Bengio and R Devon\n  Hjelm and Shikhar Sharma", "title": "Object-Centric Image Generation from Layouts", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent impressive results on single-object and single-domain image\ngeneration, the generation of complex scenes with multiple objects remains\nchallenging. In this paper, we start with the idea that a model must be able to\nunderstand individual objects and relationships between objects in order to\ngenerate complex scenes well. Our layout-to-image-generation method, which we\ncall Object-Centric Generative Adversarial Network (or OC-GAN), relies on a\nnovel Scene-Graph Similarity Module (SGSM). The SGSM learns representations of\nthe spatial relationships between objects in the scene, which lead to our\nmodel's improved layout-fidelity. We also propose changes to the conditioning\nmechanism of the generator that enhance its object instance-awareness. Apart\nfrom improving image quality, our contributions mitigate two failure modes in\nprevious approaches: (1) spurious objects being generated without corresponding\nbounding boxes in the layout, and (2) overlapping bounding boxes in the layout\nleading to merged objects in images. Extensive quantitative evaluation and\nablation studies demonstrate the impact of our contributions, with our model\noutperforming previous state-of-the-art approaches on both the COCO-Stuff and\nVisual Genome datasets. Finally, we address an important limitation of\nevaluation metrics used in previous works by introducing SceneFID -- an\nobject-centric adaptation of the popular Fr{\\'e}chet Inception Distance metric,\nthat is better suited for multi-object images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 21:40:09 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 16:02:11 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sylvain", "Tristan", ""], ["Zhang", "Pengchuan", ""], ["Bengio", "Yoshua", ""], ["Hjelm", "R Devon", ""], ["Sharma", "Shikhar", ""]]}, {"id": "2003.07469", "submitter": "Jiaxiong Qiu", "authors": "Jiaxiong Qiu, Cai Chen, Shuaicheng Liu, Bing Zeng", "title": "SlimConv: Reducing Channel Redundancy in Convolutional Neural Networks\n  by Weights Flipping", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The channel redundancy in feature maps of convolutional neural networks\n(CNNs) results in the large consumption of memories and computational\nresources. In this work, we design a novel Slim Convolution (SlimConv) module\nto boost the performance of CNNs by reducing channel redundancies. Our SlimConv\nconsists of three main steps: Reconstruct, Transform and Fuse, through which\nthe features are splitted and reorganized in a more efficient way, such that\nthe learned weights can be compressed effectively. In particular, the core of\nour model is a weight flipping operation which can largely improve the feature\ndiversities, contributing to the performance crucially. Our SlimConv is a\nplug-and-play architectural unit which can be used to replace convolutional\nlayers in CNNs directly. We validate the effectiveness of SlimConv by\nconducting comprehensive experiments on ImageNet, MS COCO2014, Pascal VOC2012\nsegmentation, and Pascal VOC2007 detection datasets. The experiments show that\nSlimConv-equipped models can achieve better performances consistently, less\nconsumption of memory and computation resources than non-equipped conterparts.\nFor example, the ResNet-101 fitted with SlimConv achieves 77.84% top-1\nclassification accuracy with 4.87 GFLOPs and 27.96M parameters on ImageNet,\nwhich shows almost 0.5% better performance with about 3 GFLOPs and 38%\nparameters reduced.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 23:23:10 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Qiu", "Jiaxiong", ""], ["Chen", "Cai", ""], ["Liu", "Shuaicheng", ""], ["Zeng", "Bing", ""]]}, {"id": "2003.07474", "submitter": "Bas Peters", "authors": "Bas Peters, Eldad Haber, Keegan Lensink", "title": "Fully reversible neural networks for large-scale surface and sub-surface\n  characterization via remote sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large spatial/frequency scale of hyperspectral and airborne magnetic and\ngravitational data causes memory issues when using convolutional neural\nnetworks for (sub-) surface characterization. Recently developed fully\nreversible networks can mostly avoid memory limitations by virtue of having a\nlow and fixed memory requirement for storing network states, as opposed to the\ntypical linear memory growth with depth. Fully reversible networks enable the\ntraining of deep neural networks that take in entire data volumes, and create\nsemantic segmentations in one go. This approach avoids the need to work in\nsmall patches or map a data patch to the class of just the central pixel. The\ncross-entropy loss function requires small modifications to work in conjunction\nwith a fully reversible network and learn from sparsely sampled labels without\never seeing fully labeled ground truth. We show examples from land-use change\ndetection from hyperspectral time-lapse data, and regional aquifer mapping from\nairborne geophysical and geological data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 23:54:22 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Peters", "Bas", ""], ["Haber", "Eldad", ""], ["Lensink", "Keegan", ""]]}, {"id": "2003.07493", "submitter": "Shi-Xue Zhang", "authors": "Shi-Xue Zhang, Xiaobin Zhu, Jie-Bo Hou, Chang Liu, Chun Yang, Hongfa\n  Wang, Xu-Cheng Yin", "title": "Deep Relational Reasoning Graph Network for Arbitrary Shape Text\n  Detection", "comments": "10 pages, Accepted by CVPR2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary shape text detection is a challenging task due to the high variety\nand complexity of scenes texts. In this paper, we propose a novel unified\nrelational reasoning graph network for arbitrary shape text detection. In our\nmethod, an innovative local graph bridges a text proposal model via\nConvolutional Neural Network (CNN) and a deep relational reasoning network via\nGraph Convolutional Network (GCN), making our network end-to-end trainable. To\nbe concrete, every text instance will be divided into a series of small\nrectangular components, and the geometry attributes (e.g., height, width, and\norientation) of the small components will be estimated by our text proposal\nmodel. Given the geometry attributes, the local graph construction model can\nroughly establish linkages between different text components. For further\nreasoning and deducing the likelihood of linkages between the component and its\nneighbors, we adopt a graph-based network to perform deep relational reasoning\non local graphs. Experiments on public available datasets demonstrate the\nstate-of-the-art performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 01:50:07 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 07:36:56 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhang", "Shi-Xue", ""], ["Zhu", "Xiaobin", ""], ["Hou", "Jie-Bo", ""], ["Liu", "Chang", ""], ["Yang", "Chun", ""], ["Wang", "Hongfa", ""], ["Yin", "Xu-Cheng", ""]]}, {"id": "2003.07496", "submitter": "Jie Song", "authors": "Jie Song, Yixin Chen, Jingwen Ye, Xinchao Wang, Chengchao Shen, Feng\n  Mao, Mingli Song", "title": "DEPARA: Deep Attribution Graph for Deep Knowledge Transferability", "comments": "Accepted by CVPR2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the intrinsic interconnections between the knowledge encoded in\nPRe-trained Deep Neural Networks (PR-DNNs) of heterogeneous tasks sheds light\non their mutual transferability, and consequently enables knowledge transfer\nfrom one task to another so as to reduce the training effort of the latter. In\nthis paper, we propose the DEeP Attribution gRAph (DEPARA) to investigate the\ntransferability of knowledge learned from PR-DNNs. In DEPARA, nodes correspond\nto the inputs and are represented by their vectorized attribution maps with\nregards to the outputs of the PR-DNN. Edges denote the relatedness between\ninputs and are measured by the similarity of their features extracted from the\nPR-DNN. The knowledge transferability of two PR-DNNs is measured by the\nsimilarity of their corresponding DEPARAs. We apply DEPARA to two important yet\nunder-studied problems in transfer learning: pre-trained model selection and\nlayer selection. Extensive experiments are conducted to demonstrate the\neffectiveness and superiority of the proposed method in solving both these\nproblems. Code, data and models reproducing the results in this paper are\navailable at \\url{https://github.com/zju-vipa/DEPARA}.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:07:50 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Song", "Jie", ""], ["Chen", "Yixin", ""], ["Ye", "Jingwen", ""], ["Wang", "Xinchao", ""], ["Shen", "Chengchao", ""], ["Mao", "Feng", ""], ["Song", "Mingli", ""]]}, {"id": "2003.07514", "submitter": "Jongmin Yu", "authors": "Jongmin Yu, Yongsang Yoon, and Moongu Jeon", "title": "Predictively Encoded Graph Convolutional Network for Noise-Robust\n  Skeleton-based Action Recognition", "comments": "Submitted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In skeleton-based action recognition, graph convolutional networks (GCNs),\nwhich model human body skeletons using graphical components such as nodes and\nconnections, have achieved remarkable performance recently. However, current\nstate-of-the-art methods for skeleton-based action recognition usually work on\nthe assumption that the completely observed skeletons will be provided. This\nmay be problematic to apply this assumption in real scenarios since there is\nalways a possibility that captured skeletons are incomplete or noisy. In this\nwork, we propose a skeleton-based action recognition method which is robust to\nnoise information of given skeleton features. The key insight of our approach\nis to train a model by maximizing the mutual information between normal and\nnoisy skeletons using a predictive coding manner. We have conducted\ncomprehensive experiments about skeleton-based action recognition with defected\nskeletons using NTU-RGB+D and Kinetics-Skeleton datasets. The experimental\nresults demonstrate that our approach achieves outstanding performance when\nskeleton samples are noised compared with existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 03:37:36 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Yu", "Jongmin", ""], ["Yoon", "Yongsang", ""], ["Jeon", "Moongu", ""]]}, {"id": "2003.07516", "submitter": "Luanxuan Hou", "authors": "Luanxuan Hou and Jie Cao and Yuan Zhao and Haifeng Shen and Yiping\n  Meng and Ran He and Jieping Ye", "title": "Augmented Parallel-Pyramid Net for Attention Guided Pose-Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target of human pose estimation is to determine body part or joint\nlocations of each person from an image. This is a challenging problems with\nwide applications. To address this issue, this paper proposes an augmented\nparallel-pyramid net with attention partial module and differentiable auto-data\naugmentation. Technically, a parallel pyramid structure is proposed to\ncompensate the loss of information. We take the design of parallel structure\nfor reverse compensation. Meanwhile, the overall computational complexity does\nnot increase. We further define an Attention Partial Module (APM) operator to\nextract weighted features from different scale feature maps generated by the\nparallel pyramid structure. Compared with refining through upsampling operator,\nAPM can better capture the relationship between channels. At last, we proposed\na differentiable auto data augmentation method to further improve estimation\naccuracy. We define a new pose search space where the sequences of data\naugmentations are formulated as a trainable and operational CNN component.\nExperiments corroborate the effectiveness of our proposed method. Notably, our\nmethod achieves the top-1 accuracy on the challenging COCO keypoint benchmark\nand the state-of-the-art results on the MPII datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 03:52:17 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hou", "Luanxuan", ""], ["Cao", "Jie", ""], ["Zhao", "Yuan", ""], ["Shen", "Haifeng", ""], ["Meng", "Yiping", ""], ["He", "Ran", ""], ["Ye", "Jieping", ""]]}, {"id": "2003.07525", "submitter": "Hamid Hosseinianfar", "authors": "Hamid Hosseinianfar, Maite Brandt-Pearce", "title": "Cooperative Object Detection and Parameter Estimation Using Visible\n  Light Communications", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.1109/OJCOMS.2020.3020574", "report-no": null, "categories": "cs.IT cs.CV math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visible light communication (VLC) systems are promising candidates for future\nindoor access and peer-to-peer networks. The performance of these systems,\nhowever, is vulnerable to the line of sight (LOS) link blockage due to objects\ninside the room. In this paper, we develop a probabilistic object detection\nmethod that takes advantage of the blockage status of the LOS links between the\nuser devices and transceivers on the ceiling to locate those objects. The\ntarget objects are modeled as cylinders with random radii. The location and\nsize of an object can be estimated by using a quadratic programming approach.\nSimulation results show that the root-mean-squared error can be less than $1$\ncm and $8$ cm for estimating the center and the radius of the object,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:40:33 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Hosseinianfar", "Hamid", ""], ["Brandt-Pearce", "Maite", ""]]}, {"id": "2003.07526", "submitter": "Sunho Kim", "authors": "Sunho Kim, Byungjai Kim, HyunWook Park", "title": "Synthesis of Brain Tumor MR Images for Learning Data Augmentation", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": "10.1002/mp.14701", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image analysis using deep neural networks has been actively studied.\nDeep neural networks are trained by learning data. For accurate training of\ndeep neural networks, the learning data should be sufficient, of good quality,\nand should have a generalized property. However, in medical images, it is\ndifficult to acquire sufficient patient data because of the difficulty of\npatient recruitment, the burden of annotation of lesions by experts, and the\ninvasion of patients' privacy. In comparison, the medical images of healthy\nvolunteers can be easily acquired. Using healthy brain images, the proposed\nmethod synthesizes multi-contrast magnetic resonance images of brain tumors.\nBecause tumors have complex features, the proposed method simplifies them into\nconcentric circles that are easily controllable. Then it converts the\nconcentric circles into various realistic shapes of tumors through deep neural\nnetworks. Because numerous healthy brain images are easily available, our\nmethod can synthesize a huge number of the brain tumor images with various\nconcentric circles. We performed qualitative and quantitative analysis to\nassess the usefulness of augmented data from the proposed method. Intuitive and\ninteresting experimental results are available online at\nhttps://github.com/KSH0660/BrainTumor\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:43:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Kim", "Sunho", ""], ["Kim", "Byungjai", ""], ["Park", "HyunWook", ""]]}, {"id": "2003.07529", "submitter": "Nibaran Das", "authors": "Shyamali Mitra, Nibaran Das, Soumyajyoti Dey, Sukanta Chakrabarty,\n  Mita Nasipuri, Mrinal Kanti Naskar", "title": "Cytology Image Analysis Techniques Towards Automation: Systematically\n  Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cytology is the branch of pathology which deals with the microscopic\nexamination of cells for diagnosis of carcinoma or inflammatory conditions.\nAutomation in cytology started in the early 1950s with the aim to reduce manual\nefforts in diagnosis of cancer. The inflush of intelligent technological units\nwith high computational power and improved specimen collection techniques\nhelped to achieve its technological heights. In the present survey, we focus on\nsuch image processing techniques which put steps forward towards the automation\nof cytology. We take a short tour to 17 types of cytology and explore various\nsegmentation and/or classification techniques which evolved during last three\ndecades boosting the concept of automation in cytology. It is observed, that\nmost of the works are aligned towards three types of cytology: Cervical, Breast\nand Lung, which are discussed elaborately in this paper. The user-end systems\ndeveloped during that period are summarized to comprehend the overall growth in\nthe respective domains. To be precise, we discuss the diversity of the\nstate-of-the-art methodologies, their challenges to provide prolific and\ncompetent future research directions inbringing the cytology-based commercial\nsystems into the mainstream.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:56:19 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Mitra", "Shyamali", ""], ["Das", "Nibaran", ""], ["Dey", "Soumyajyoti", ""], ["Chakrabarty", "Sukanta", ""], ["Nasipuri", "Mita", ""], ["Naskar", "Mrinal Kanti", ""]]}, {"id": "2003.07540", "submitter": "Yu Liu", "authors": "Guanglu Song, Yu Liu, Xiaogang Wang", "title": "Revisiting the Sibling Head in Object Detector", "comments": "Accept to CVPR 2020 & Method of Champion of OpenImage Challenge 2019,\n  detection track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ``shared head for classification and localization'' (sibling head),\nfirstly denominated in Fast RCNN~\\cite{girshick2015fast}, has been leading the\nfashion of the object detection community in the past five years. This paper\nprovides the observation that the spatial misalignment between the two object\nfunctions in the sibling head can considerably hurt the training process, but\nthis misalignment can be resolved by a very simple operator called task-aware\nspatial disentanglement (TSD). Considering the classification and regression,\nTSD decouples them from the spatial dimension by generating two disentangled\nproposals for them, which are estimated by the shared proposal. This is\ninspired by the natural insight that for one instance, the features in some\nsalient area may have rich information for classification while these around\nthe boundary may be good at bounding box regression. Surprisingly, this simple\ndesign can boost all backbones and models on both MS COCO and Google OpenImage\nconsistently by ~3% mAP. Further, we propose a progressive constraint to\nenlarge the performance margin between the disentangled and the shared\nproposals, and gain ~1% more mAP. We show the \\algname{} breaks through the\nupper bound of nowadays single-model detector by a large margin (mAP 49.4 with\nResNet-101, 51.2 with SENet154), and is the core model of our 1st place\nsolution on the Google OpenImage Challenge 2019.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 05:21:54 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2003.07543", "submitter": "Yu Liu", "authors": "Guanglu Song, Yu Liu, Yuhang Zang, Xiaogang Wang, Biao Leng, Qingsheng\n  Yuan", "title": "KPNet: Towards Minimal Face Detector", "comments": "AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The small receptive field and capacity of minimal neural networks limit their\nperformance when using them to be the backbone of detectors. In this work, we\nfind that the appearance feature of a generic face is discriminative enough for\na tiny and shallow neural network to verify from the background. And the\nessential barriers behind us are 1) the vague definition of the face bounding\nbox and 2) tricky design of anchor-boxes or receptive field. Unlike most\ntop-down methods for joint face detection and alignment, the proposed KPNet\ndetects small facial keypoints instead of the whole face by in a bottom-up\nmanner. It first predicts the facial landmarks from a low-resolution image via\nthe well-designed fine-grained scale approximation and scale adaptive\nsoft-argmax operator. Finally, the precise face bounding boxes, no matter how\nwe define it, can be inferred from the keypoints. Without any complex head\narchitecture or meticulous network designing, the KPNet achieves\nstate-of-the-art accuracy on generic face detection and alignment benchmarks\nwith only $\\sim1M$ parameters, which runs at 1000fps on GPU and is easy to\nperform real-time on most modern front-end chips.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 05:37:45 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Song", "Guanglu", ""], ["Liu", "Yu", ""], ["Zang", "Yuhang", ""], ["Wang", "Xiaogang", ""], ["Leng", "Biao", ""], ["Yuan", "Qingsheng", ""]]}, {"id": "2003.07557", "submitter": "Yu Liu", "authors": "Yu Liu, Guanglu Song, Yuhang Zang, Yan Gao, Enze Xie, Junjie Yan, Chen\n  Change Loy, Xiaogang Wang", "title": "1st Place Solutions for OpenImage2019 -- Object Detection and Instance\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the solutions of the two champion teams, `MMfruit'\nfor the detection track and `MMfruitSeg' for the segmentation track, in\nOpenImage Challenge 2019. It is commonly known that for an object detector, the\nshared feature at the end of the backbone is not appropriate for both\nclassification and regression, which greatly limits the performance of both\nsingle stage detector and Faster RCNN \\cite{ren2015faster} based detector. In\nthis competition, we observe that even with a shared feature, different\nlocations in one object has completely inconsistent performances for the two\ntasks. \\textit{E.g. the features of salient locations are usually good for\nclassification, while those around the object edge are good for regression.}\nInspired by this, we propose the Decoupling Head (DH) to disentangle the object\nclassification and regression via the self-learned optimal feature extraction,\nwhich leads to a great improvement. Furthermore, we adjust the soft-NMS\nalgorithm to adj-NMS to obtain stable performance improvement. Finally, a\nwell-designed ensemble strategy via voting the bounding box location and\nconfidence is proposed. We will also introduce several training/inferencing\nstrategies and a bag of tricks that give minor improvement. Given those masses\nof details, we train and aggregate 28 global models with various backbones,\nheads and 3+2 expert models, and achieves the 1st place on the OpenImage 2019\nObject Detection Challenge on the both public and private leadboards. Given\nsuch good instance bounding box, we further design a simple instance-level\nsemantic segmentation pipeline and achieve the 1st place on the segmentation\nchallenge.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 06:45:07 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Liu", "Yu", ""], ["Song", "Guanglu", ""], ["Zang", "Yuhang", ""], ["Gao", "Yan", ""], ["Xie", "Enze", ""], ["Yan", "Junjie", ""], ["Loy", "Chen Change", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2003.07560", "submitter": "Yiren Li", "authors": "Yiren Li, Zheng Huang, Junchi Yan, Yi Zhou, Fan Ye and Xianhui Liu", "title": "GFTE: Graph-based Financial Table Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tabular data is a crucial form of information expression, which can organize\ndata in a standard structure for easy information retrieval and comparison.\nHowever, in financial industry and many other fields tables are often disclosed\nin unstructured digital files, e.g. Portable Document Format (PDF) and images,\nwhich are difficult to be extracted directly. In this paper, to facilitate deep\nlearning based table extraction from unstructured digital files, we publish a\nstandard Chinese dataset named FinTab, which contains more than 1,600 financial\ntables of diverse kinds and their corresponding structure representation in\nJSON. In addition, we propose a novel graph-based convolutional neural network\nmodel named GFTE as a baseline for future comparison. GFTE integrates image\nfeature, position feature and textual feature together for precise edge\nprediction and reaches overall good results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:10:05 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Li", "Yiren", ""], ["Huang", "Zheng", ""], ["Yan", "Junchi", ""], ["Zhou", "Yi", ""], ["Ye", "Fan", ""], ["Liu", "Xianhui", ""]]}, {"id": "2003.07561", "submitter": "Di Wu", "authors": "Di Wu, Yihao Chen, Xianbiao Qi, Yongjian Yu, Weixuan Chen, and Rong\n  Xiao", "title": "Neural Mesh Refiner for 6-DoF Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we effectively utilise the 2D monocular image information for\nrecovering the 6D pose (6-DoF) of the visual objects? Deep learning has shown\nto be effective for robust and real-time monocular pose estimation. Oftentimes,\nthe network learns to regress the 6-DoF pose using a naive loss function.\nHowever, due to a lack of geometrical scene understanding from the directly\nregressed pose estimation, there are misalignments between the rendered mesh\nfrom the 3D object and the 2D instance segmentation result, e.g., bounding\nboxes and masks prediction. This paper bridges the gap between 2D mask\ngeneration and 3D location prediction via a differentiable neural mesh\nrenderer. We utilise the overlay between the accurate mask prediction and less\naccurate mesh prediction to iteratively optimise the direct regressed 6D pose\ninformation with a focus on translation estimation. By leveraging geometry, we\ndemonstrate that our technique significantly improves direct regression\nperformance on the difficult task of translation estimation and achieve the\nstate of the art results on Peking University/Baidu - Autonomous Driving\ndataset and the ApolloScape 3D Car Instance dataset. The code can be found at\n\\url{https://bit.ly/2IRihfU}.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:12:22 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 02:11:23 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 10:14:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wu", "Di", ""], ["Chen", "Yihao", ""], ["Qi", "Xianbiao", ""], ["Yu", "Yongjian", ""], ["Chen", "Weixuan", ""], ["Xiao", "Rong", ""]]}, {"id": "2003.07564", "submitter": "Hao Yang", "authors": "Hao Yang, Dan Yan, Li Zhang, Dong Li, YunDa Sun, ShaoDi You, Stephen\n  J. Maybank", "title": "Feedback Graph Convolutional Network for Skeleton-based Action\n  Recognition", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skeleton-based action recognition has attracted considerable attention in\ncomputer vision since skeleton data is more robust to the dynamic circumstance\nand complicated background than other modalities. Recently, many researchers\nhave used the Graph Convolutional Network (GCN) to model spatial-temporal\nfeatures of skeleton sequences by an end-to-end optimization. However,\nconventional GCNs are feedforward networks which are impossible for low-level\nlayers to access semantic information in the high-level layers. In this paper,\nwe propose a novel network, named Feedback Graph Convolutional Network (FGCN).\nThis is the first work that introduces the feedback mechanism into GCNs and\naction recognition. Compared with conventional GCNs, FGCN has the following\nadvantages: (1) a multi-stage temporal sampling strategy is designed to extract\nspatial-temporal features for action recognition in a coarse-to-fine\nprogressive process; (2) A dense connections based Feedback Graph Convolutional\nBlock (FGCB) is proposed to introduce feedback connections into the GCNs. It\ntransmits the high-level semantic features to the low-level layers and flows\ntemporal information stage by stage to progressively model global\nspatial-temporal features for action recognition; (3) The FGCN model provides\nearly predictions. In the early stages, the model receives partial information\nabout actions. Naturally, its predictions are relatively coarse. The coarse\npredictions are treated as the prior to guide the feature learning of later\nstages for a accurate prediction. Extensive experiments on the datasets,\nNTU-RGB+D, NTU-RGB+D120 and Northwestern-UCLA, demonstrate that the proposed\nFGCN is effective for action recognition. It achieves the state-of-the-art\nperformance on the three datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:20:47 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Yang", "Hao", ""], ["Yan", "Dan", ""], ["Zhang", "Li", ""], ["Li", "Dong", ""], ["Sun", "YunDa", ""], ["You", "ShaoDi", ""], ["Maybank", "Stephen J.", ""]]}, {"id": "2003.07573", "submitter": "Tal Grinshpoun", "authors": "Haya Brama and Tal Grinshpoun", "title": "Heat and Blur: An Effective and Fast Defense Against Adversarial\n  Examples", "comments": "Submitted to IJCAI 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing incorporation of artificial neural networks (NNs) into many\nfields, and especially into life-critical systems, is restrained by their\nvulnerability to adversarial examples (AEs). Some existing defense methods can\nincrease NNs' robustness, but they often require special architecture or\ntraining procedures and are irrelevant to already trained models. In this\npaper, we propose a simple defense that combines feature visualization with\ninput modification, and can, therefore, be applicable to various pre-trained\nnetworks. By reviewing several interpretability methods, we gain new insights\nregarding the influence of AEs on NNs' computation. Based on that, we\nhypothesize that information about the \"true\" object is preserved within the\nNN's activity, even when the input is adversarial, and present a feature\nvisualization version that can extract that information in the form of\nrelevance heatmaps. We then use these heatmaps as a basis for our defense, in\nwhich the adversarial effects are corrupted by massive blurring. We also\nprovide a new evaluation metric that can capture the effects of both attacks\nand defenses more thoroughly and descriptively, and demonstrate the\neffectiveness of the defense and the utility of the suggested evaluation\nmeasurement with VGG19 results on the ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 08:11:18 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Brama", "Haya", ""], ["Grinshpoun", "Tal", ""]]}, {"id": "2003.07581", "submitter": "Umar Iqbal", "authors": "Umar Iqbal and Pavlo Molchanov and Jan Kautz", "title": "Weakly-Supervised 3D Human Pose Learning via Multi-view Images in the\n  Wild", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major challenge for monocular 3D human pose estimation in-the-wild is the\nacquisition of training data that contains unconstrained images annotated with\naccurate 3D poses. In this paper, we address this challenge by proposing a\nweakly-supervised approach that does not require 3D annotations and learns to\nestimate 3D poses from unlabeled multi-view data, which can be acquired easily\nin in-the-wild environments. We propose a novel end-to-end learning framework\nthat enables weakly-supervised training using multi-view consistency. Since\nmulti-view consistency is prone to degenerated solutions, we adopt a 2.5D pose\nrepresentation and propose a novel objective function that can only be\nminimized when the predictions of the trained model are consistent and\nplausible across all camera views. We evaluate our proposed approach on two\nlarge scale datasets (Human3.6M and MPII-INF-3DHP) where it achieves\nstate-of-the-art performance among semi-/weakly-supervised methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 08:47:16 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Iqbal", "Umar", ""], ["Molchanov", "Pavlo", ""], ["Kautz", "Jan", ""]]}, {"id": "2003.07584", "submitter": "Yihao Luo", "authors": "Yihao Luo, Min Xu, Caihong Yuan, Xiang Cao, Liangqi Zhang, Yan Xu,\n  Tianjiang Wang and Qi Feng", "title": "SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object\n  Tracking", "comments": "Accepted by ICANN2021, 12 pages, 5figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently spiking neural networks (SNNs), the third-generation of neural\nnetworks has shown remarkable capabilities of energy-efficient computing, which\nis a promising alternative for deep neural networks (DNNs) with high energy\nconsumption. SNNs have reached competitive results compared to DNNs in\nrelatively simple tasks and small datasets such as image classification and\nMNIST/CIFAR, while few studies on more challenging vision tasks on complex\ndatasets. In this paper, we focus on extending deep SNNs to object tracking, a\nmore advanced vision task with embedded applications and energy-saving\nrequirements, and present a spike-based Siamese network called SiamSNN.\nSpecifically, we propose an optimized hybrid similarity estimation method to\nexploit temporal information in the SNNs, and introduce a novel two-status\ncoding scheme to optimize the temporal distribution of output spike trains for\nfurther improvements. SiamSNN is the first deep SNN tracker that achieves short\nlatency and low precision loss on the visual object tracking benchmarks\nOTB2013/2015, VOT2016/2018, and GOT-10k. Moreover, SiamSNN achieves notably low\nenergy consumption and real-time on Neuromorphic chip TrueNorth.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 08:49:51 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 11:41:09 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 05:25:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Luo", "Yihao", ""], ["Xu", "Min", ""], ["Yuan", "Caihong", ""], ["Cao", "Xiang", ""], ["Zhang", "Liangqi", ""], ["Xu", "Yan", ""], ["Wang", "Tianjiang", ""], ["Feng", "Qi", ""]]}, {"id": "2003.07596", "submitter": "Tomas Teijeiro", "authors": "Tomas Teijeiro and Paulo Felix", "title": "Construe: a software solution for the explanation-based interpretation\n  of time series", "comments": "Original Software Publication. 10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a software implementation of a general framework for time\nseries interpretation based on abductive reasoning. The software provides a\ndata model and a set of algorithms to make inference to the best explanation of\na time series, resulting in a description in multiple abstraction levels of the\nprocesses underlying the time series. As a proof of concept, a comprehensive\nknowledge base for the electrocardiogram (ECG) domain is provided, so it can be\nused directly as a tool for ECG analysis. This tool has been successfully\nvalidated in several noteworthy problems, such as heartbeat classification or\natrial fibrillation detection.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 09:26:55 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Teijeiro", "Tomas", ""], ["Felix", "Paulo", ""]]}, {"id": "2003.07603", "submitter": "Ruifeng Shi", "authors": "Ruifeng Shi, Deming Zhai, Xianming Liu, Junjun Jiang, Wen Gao", "title": "Rectified Meta-Learning from Noisy Labels for Robust Image-based Plant\n  Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant diseases serve as one of main threats to food security and crop\nproduction. It is thus valuable to exploit recent advances of artificial\nintelligence to assist plant disease diagnosis. One popular approach is to\ntransform this problem as a leaf image classification task, which can be then\naddressed by the powerful convolutional neural networks (CNNs). However, the\nperformance of CNN-based classification approach depends on a large amount of\nhigh-quality manually labeled training data, which are inevitably introduced\nnoise on labels in practice, leading to model overfitting and performance\ndegradation. To overcome this problem, we propose a novel framework that\nincorporates rectified meta-learning module into common CNN paradigm to train a\nnoise-robust deep network without using extra supervision information. The\nproposed method enjoys the following merits: i) A rectified meta-learning is\ndesigned to pay more attention to unbiased samples, leading to accelerated\nconvergence and improved classification accuracy. ii) Our method is free on\nassumption of label noise distribution, which works well on various kinds of\nnoise. iii) Our method serves as a plug-and-play module, which can be embedded\ninto any deep models optimized by gradient descent based method. Extensive\nexperiments are conducted to demonstrate the superior performance of our\nalgorithm over the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 09:51:30 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 03:01:25 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Shi", "Ruifeng", ""], ["Zhai", "Deming", ""], ["Liu", "Xianming", ""], ["Jiang", "Junjun", ""], ["Gao", "Wen", ""]]}, {"id": "2003.07618", "submitter": "Vladislav Sovrasov", "authors": "Vladislav Sovrasov and Dmitry Sidnev", "title": "Building Computationally Efficient and Well-Generalizing Person\n  Re-Identification Models with Metric Learning", "comments": "Submitted to International Conference on Pattern Recognition (ICPR\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the problem of domain shift in person\nre-identification.Being trained on one dataset, a re-identification model\nusually performs much worse on unseen data. Partially this gap is caused by the\nrelatively small scale of person re-identification datasets (compared to face\nrecognition ones, for instance), but it is also related to training objectives.\nWe propose to use the metric learning objective, namely AM-Softmax loss, and\nsome additional training practices to build well-generalizing, yet,\ncomputationally efficient models. We use recently proposed Omni-Scale Network\n(OSNet) architecture combined with several training tricks and architecture\nadjustments to obtain state-of-the art results in cross-domain generalization\nproblem on a large-scale MSMT17 dataset in three setups: MSMT17-all->DukeMTMC,\nMSMT17-train->Market1501 and MSMT17-all->Market1501.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:24:58 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 12:23:15 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Sovrasov", "Vladislav", ""], ["Sidnev", "Dmitry", ""]]}, {"id": "2003.07619", "submitter": "Clara Fernandez Labrador", "authors": "Clara Fernandez-Labrador, Ajad Chhatkuli, Danda Pani Paudel, Jose J.\n  Guerrero, C\\'edric Demonceaux and Luc Van Gool", "title": "Unsupervised Learning of Category-Specific Symmetric 3D Keypoints from\n  Point Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic discovery of category-specific 3D keypoints from a collection of\nobjects of some category is a challenging problem. One reason is that not all\nobjects in a category necessarily have the same semantic parts. The level of\ndifficulty adds up further when objects are represented by 3D point clouds,\nwith variations in shape and unknown coordinate frames. We define keypoints to\nbe category-specific, if they meaningfully represent objects' shape and their\ncorrespondences can be simply established order-wise across all objects. This\npaper aims at learning category-specific 3D keypoints, in an unsupervised\nmanner, using a collection of misaligned 3D point clouds of objects from an\nunknown category. In order to do so, we model shapes defined by the keypoints,\nwithin a category, using the symmetric linear basis shapes without assuming the\nplane of symmetry to be known. The usage of symmetry prior leads us to learn\nstable keypoints suitable for higher misalignments. To the best of our\nknowledge, this is the first work on learning such keypoints directly from 3D\npoint clouds. Using categories from four benchmark datasets, we demonstrate the\nquality of our learned keypoints by quantitative and qualitative evaluations.\nOur experiments also show that the keypoints discovered by our method are\ngeometrically and semantically consistent.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:28:02 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 16:30:56 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 09:56:02 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fernandez-Labrador", "Clara", ""], ["Chhatkuli", "Ajad", ""], ["Paudel", "Danda Pani", ""], ["Guerrero", "Jose J.", ""], ["Demonceaux", "C\u00e9dric", ""], ["Van Gool", "Luc", ""]]}, {"id": "2003.07623", "submitter": "Damian Campo", "authors": "Giulia Slavic, Damian Campo, Mohamad Baydoun, Pablo Marin, David\n  Martin, Lucio Marcenaro, Carlo Regazzoni", "title": "Anomaly Detection in Video Data Based on Probabilistic Latent Space\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for detecting anomalies in video data. A\nVariational Autoencoder (VAE) is used for reducing the dimensionality of video\nframes, generating latent space information that is comparable to\nlow-dimensional sensory data (e.g., positioning, steering angle), making\nfeasible the development of a consistent multi-modal architecture for\nautonomous vehicles. An Adapted Markov Jump Particle Filter defined by discrete\nand continuous inference levels is employed to predict the following frames and\ndetecting anomalies in new video sequences. Our method is evaluated on\ndifferent video scenarios where a semi-autonomous vehicle performs a set of\ntasks in a closed environment.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:32:22 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Slavic", "Giulia", ""], ["Campo", "Damian", ""], ["Baydoun", "Mohamad", ""], ["Marin", "Pablo", ""], ["Martin", "David", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "2003.07631", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Gr\\'egoire Montavon, Sebastian Lapuschkin, Christopher\n  J. Anders, Klaus-Robert M\\\"uller", "title": "Explaining Deep Neural Networks and Beyond: A Review of Methods and\n  Applications", "comments": "30 pages, 20 figures", "journal-ref": null, "doi": "10.1109/JPROC.2021.3060483", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the broader and highly successful usage of machine learning in industry\nand the sciences, there has been a growing demand for Explainable AI.\nInterpretability and explanation methods for gaining a better understanding\nabout the problem solving abilities and strategies of nonlinear Machine\nLearning, in particular, deep neural networks, are therefore receiving\nincreased attention. In this work we aim to (1) provide a timely overview of\nthis active emerging field, with a focus on 'post-hoc' explanations, and\nexplain its theoretical foundations, (2) put interpretability algorithms to a\ntest both from a theory and comparative evaluation perspective using extensive\nsimulations, (3) outline best practice aspects i.e. how to best include\ninterpretation methods into the standard usage of machine learning and (4)\ndemonstrate successful usage of explainable AI in a representative selection of\napplication scenarios. Finally, we discuss challenges and possible future\ndirections of this exciting foundational field of machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:45:51 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 12:39:41 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Samek", "Wojciech", ""], ["Montavon", "Gr\u00e9goire", ""], ["Lapuschkin", "Sebastian", ""], ["Anders", "Christopher J.", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "2003.07637", "submitter": "Hu Zhang", "authors": "Hu Zhang, Linchao Zhu, Yi Zhu and Yi Yang", "title": "Motion-Excited Sampler: Video Adversarial Attack with Sparked Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are known to be susceptible to adversarial noise, which\nare tiny and imperceptible perturbations. Most of previous work on adversarial\nattack mainly focus on image models, while the vulnerability of video models is\nless explored. In this paper, we aim to attack video models by utilizing\nintrinsic movement pattern and regional relative motion among video frames. We\npropose an effective motion-excited sampler to obtain motion-aware noise prior,\nwhich we term as sparked prior. Our sparked prior underlines frame correlations\nand utilizes video dynamics via relative motion. By using the sparked prior in\ngradient estimation, we can successfully attack a variety of video\nclassification models with fewer number of queries. Extensive experimental\nresults on four benchmark datasets validate the efficacy of our proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:54:12 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 01:37:47 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhang", "Hu", ""], ["Zhu", "Linchao", ""], ["Zhu", "Yi", ""], ["Yang", "Yi", ""]]}, {"id": "2003.07640", "submitter": "Lin Wang", "authors": "Lin Wang, Tae-Kyun Kim, Kuk-Jin Yoon", "title": "EventSR: From Asynchronous Events to Image Reconstruction, Restoration,\n  and Super-Resolution via End-to-End Adversarial Learning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Event cameras sense intensity changes and have many advantages over\nconventional cameras. To take advantage of event cameras, some methods have\nbeen proposed to reconstruct intensity images from event streams. However, the\noutputs are still in low resolution (LR), noisy, and unrealistic. The\nlow-quality outputs stem broader applications of event cameras, where high\nspatial resolution (HR) is needed as well as high temporal resolution, dynamic\nrange, and no motion blur. We consider the problem of reconstructing and\nsuper-resolving intensity images from LR events, when no ground truth (GT) HR\nimages and down-sampling kernels are available. To tackle the challenges, we\npropose a novel end-to-end pipeline that reconstructs LR images from event\nstreams, enhances the image qualities and upsamples the enhanced images, called\nEventSR. For the absence of real GT images, our method is primarily\nunsupervised, deploying adversarial learning. To train EventSR, we create an\nopen dataset including both real-world and simulated scenes. The use of both\ndatasets boosts up the network performance, and the network architectures and\nvarious loss functions in each phase help improve the image qualities. The\nwhole pipeline is trained in three phases. While each phase is mainly for one\nof the three tasks, the networks in earlier phases are fine-tuned by respective\nloss functions in an end-to-end manner. Experimental results show that EventSR\nreconstructs high-quality SR images from events for both simulated and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 10:58:10 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Wang", "Lin", ""], ["Kim", "Tae-Kyun", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "2003.07650", "submitter": "Chenglong Li", "authors": "Zhengzheng Tu, Chun Lin, Chenglong Li, Jin Tang and Bin Luo", "title": "M$^5$L: Multi-Modal Multi-Margin Metric Learning for RGBT Tracking", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying the confusing samples in the course of RGBT tracking is a quite\nchallenging problem, which hasn't got satisfied solution. Existing methods only\nfocus on enlarging the boundary between positive and negative samples, however,\nthe structured information of samples might be harmed, e.g., confusing positive\nsamples are closer to the anchor than normal positive samples.To handle this\nproblem, we propose a novel Multi-Modal Multi-Margin Metric Learning framework,\nnamed M$^5$L for RGBT tracking in this paper. In particular, we design a\nmulti-margin structured loss to distinguish the confusing samples which play a\nmost critical role in tracking performance boosting. To alleviate this problem,\nwe additionally enlarge the boundaries between confusing positive samples and\nnormal ones, between confusing negative samples and normal ones with predefined\nmargins, by exploiting the structured information of all samples in each\nmodality.Moreover, a cross-modality constraint is employed to reduce the\ndifference between modalities and push positive samples closer to the anchor\nthan negative ones from two modalities.In addition, to achieve quality-aware\nRGB and thermal feature fusion, we introduce the modality attentions and learn\nthem using a feature fusion module in our network. Extensive experiments on\nlarge-scale datasets testify that our framework clearly improves the tracking\nperformance and outperforms the state-of-the-art RGBT trackers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 11:37:56 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Tu", "Zhengzheng", ""], ["Lin", "Chun", ""], ["Li", "Chenglong", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""]]}, {"id": "2003.07685", "submitter": "Yuchao Dai Dr.", "authors": "Jing Zhang, Xin Yu, Aixuan Li, Peipei Song, Bowen Liu and Yuchao Dai", "title": "Weakly-Supervised Salient Object Detection via Scribble Annotations", "comments": "Accepted by IEEE/CVF CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with laborious pixel-wise dense labeling, it is much easier to label\ndata by scribbles, which only costs 1$\\sim$2 seconds to label one image.\nHowever, using scribble labels to learn salient object detection has not been\nexplored. In this paper, we propose a weakly-supervised salient object\ndetection model to learn saliency from such annotations. In doing so, we first\nrelabel an existing large-scale salient object detection dataset with\nscribbles, namely S-DUTS dataset. Since object structure and detail information\nis not identified by scribbles, directly training with scribble labels will\nlead to saliency maps of poor boundary localization. To mitigate this problem,\nwe propose an auxiliary edge detection task to localize object edges\nexplicitly, and a gated structure-aware loss to place constraints on the scope\nof structure to be recovered. Moreover, we design a scribble boosting scheme to\niteratively consolidate our scribble annotations, which are then employed as\nsupervision to learn high-quality saliency maps. As existing saliency\nevaluation metrics neglect to measure structure alignment of the predictions,\nthe saliency map ranking metric may not comply with human perception. We\npresent a new metric, termed saliency structure measure, to measure the\nstructure alignment of the predicted saliency maps, which is more consistent\nwith human perception. Extensive experiments on six benchmark datasets\ndemonstrate that our method not only outperforms existing\nweakly-supervised/unsupervised methods, but also is on par with several\nfully-supervised state-of-the-art models. Our code and data is publicly\navailable at https://github.com/JingZhang617/Scribble_Saliency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 12:59:50 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zhang", "Jing", ""], ["Yu", "Xin", ""], ["Li", "Aixuan", ""], ["Song", "Peipei", ""], ["Liu", "Bowen", ""], ["Dai", "Yuchao", ""]]}, {"id": "2003.07694", "submitter": "Siyu Huang", "authors": "Siyu Huang, Haoyi Xiong, Tianyang Wang, Qingzhong Wang, Zeyu Chen, Jun\n  Huan, Dejing Dou", "title": "Parameter-Free Style Projection for Arbitrary Style Transfer", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arbitrary image style transfer is a challenging task which aims to stylize a\ncontent image conditioned on an arbitrary style image. In this task the\ncontent-style feature transformation is a critical component for a proper\nfusion of features. Existing feature transformation algorithms often suffer\nfrom unstable learning, loss of content and style details, and non-natural\nstroke patterns. To mitigate these issues, this paper proposes a parameter-free\nalgorithm, Style Projection, for fast yet effective content-style\ntransformation. To leverage the proposed Style Projection~component, this paper\nfurther presents a real-time feed-forward model for arbitrary style transfer,\nincluding a regularization for matching the content semantics between inputs\nand outputs. Extensive experiments have demonstrated the effectiveness and\nefficiency of the proposed method in terms of qualitative analysis,\nquantitative evaluation, and user study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 13:07:41 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Huang", "Siyu", ""], ["Xiong", "Haoyi", ""], ["Wang", "Tianyang", ""], ["Wang", "Qingzhong", ""], ["Chen", "Zeyu", ""], ["Huan", "Jun", ""], ["Dou", "Dejing", ""]]}, {"id": "2003.07711", "submitter": "Marco Forte", "authors": "Marco Forte and Fran\\c{c}ois Piti\\'e", "title": "$F$, $B$, Alpha Matting", "comments": "Submitted to ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting out an object and estimating its opacity mask, known as image\nmatting, is a key task in many image editing applications. Deep learning\napproaches have made significant progress by adapting the encoder-decoder\narchitecture of segmentation networks. However, most of the existing networks\nonly predict the alpha matte and post-processing methods must then be used to\nrecover the original foreground and background colours in the transparent\nregions. Recently, two methods have shown improved results by also estimating\nthe foreground colours, but at a significant computational and memory cost.\n  In this paper, we propose a low-cost modification to alpha matting networks\nto also predict the foreground and background colours. We study variations of\nthe training regime and explore a wide range of existing and novel loss\nfunctions for the joint prediction.\n  Our method achieves the state of the art performance on the Adobe\nComposition-1k dataset for alpha matte and composite colour quality. It is also\nthe current best performing method on the alphamatting.com online evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 13:27:51 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Forte", "Marco", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "2003.07717", "submitter": "Rundi Wu", "authors": "Rundi Wu, Xuelin Chen, Yixin Zhuang, Baoquan Chen", "title": "Multimodal Shape Completion via Conditional Generative Adversarial\n  Networks", "comments": "Accepted to ECCV 2020 (spotlight). Project page at\n  https://chriswu1997.github.io/files/multimodal-pc/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep learning methods have been proposed for completing partial data\nfrom shape acquisition setups, i.e., filling the regions that were missing in\nthe shape. These methods, however, only complete the partial shape with a\nsingle output, ignoring the ambiguity when reasoning the missing geometry.\nHence, we pose a multi-modal shape completion problem, in which we seek to\ncomplete the partial shape with multiple outputs by learning a one-to-many\nmapping. We develop the first multimodal shape completion method that completes\nthe partial shape via conditional generative modeling, without requiring paired\ntraining data. Our approach distills the ambiguity by conditioning the\ncompletion on a learned multimodal distribution of possible results. We\nextensively evaluate the approach on several datasets that contain varying\nforms of shape incompleteness, and compare among several baseline methods and\nvariants of our methods qualitatively and quantitatively, demonstrating the\nmerit of our method in completing partial shapes with both diversity and\nquality.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 13:37:52 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 08:28:56 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 13:38:29 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Wu", "Rundi", ""], ["Chen", "Xuelin", ""], ["Zhuang", "Yixin", ""], ["Chen", "Baoquan", ""]]}, {"id": "2003.07725", "submitter": "Yuting Hu", "authors": "Yuting Hu, Zhiling Long, Anirudha Sundaresan, Motaz Alfarraj, Ghassan\n  AlRegib, Sungmee Park, and Sundaresan Jayaraman", "title": "Fabric Surface Characterization: Assessment of Deep Learning-based\n  Texture Representations Using a Challenging Dataset", "comments": "arXiv admin note: text overlap with arXiv:1905.09907", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile sensing or fabric hand plays a critical role in an individual's\ndecision to buy a certain fabric from the range of available fabrics for a\ndesired application. Therefore, textile and clothing manufacturers have long\nbeen in search of an objective method for assessing fabric hand, which can then\nbe used to engineer fabrics with a desired hand. Recognizing textures and\nmaterials in real-world images has played an important role in object\nrecognition and scene understanding. In this paper, we explore how to\ncomputationally characterize apparent or latent properties (e.g., surface\nsmoothness) of materials, i.e., computational material surface\ncharacterization, which moves a step further beyond material recognition. We\nformulate the problem as a very fine-grained texture classification problem,\nand study how deep learning-based texture representation techniques can help\ntackle the task. We introduce a new, large-scale challenging microscopic\nmaterial surface dataset (CoMMonS), geared towards an automated fabric quality\nassessment mechanism in an intelligent manufacturing system. We then conduct a\ncomprehensive evaluation of state-of-the-art deep learning-based methods for\ntexture classification using CoMMonS. Additionally, we propose a multi-level\ntexture encoding and representation network (MuLTER), which simultaneously\nleverages low- and high-level features to maintain both texture details and\nspatial information in the texture representation. Our results show that, in\ncomparison with the state-of-the-art deep texture descriptors, MuLTER yields\nhigher accuracy not only on our CoMMonS dataset for material characterization,\nbut also on established datasets such as MINC-2500 and GTOS-mobile for material\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 05:37:06 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hu", "Yuting", ""], ["Long", "Zhiling", ""], ["Sundaresan", "Anirudha", ""], ["Alfarraj", "Motaz", ""], ["AlRegib", "Ghassan", ""], ["Park", "Sungmee", ""], ["Jayaraman", "Sundaresan", ""]]}, {"id": "2003.07733", "submitter": "Jianzhu Guo", "authors": "Jianzhu Guo, Xiangyu Zhu, Chenxu Zhao, Dong Cao, Zhen Lei and Stan Z.\n  Li", "title": "Learning Meta Face Recognition in Unseen Domains", "comments": "Accepted to CVPR2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition systems are usually faced with unseen domains in real-world\napplications and show unsatisfactory performance due to their poor\ngeneralization. For example, a well-trained model on webface data cannot deal\nwith the ID vs. Spot task in surveillance scenario. In this paper, we aim to\nlearn a generalized model that can directly handle new unseen domains without\nany model updating. To this end, we propose a novel face recognition method via\nmeta-learning named Meta Face Recognition (MFR). MFR synthesizes the\nsource/target domain shift with a meta-optimization objective, which requires\nthe model to learn effective representations not only on synthesized source\ndomains but also on synthesized target domains. Specifically, we build\ndomain-shift batches through a domain-level sampling strategy and get\nback-propagated gradients/meta-gradients on synthesized source/target domains\nby optimizing multi-domain distributions. The gradients and meta-gradients are\nfurther combined to update the model to improve generalization. Besides, we\npropose two benchmarks for generalized face recognition evaluation. Experiments\non our benchmarks validate the generalization of our method compared to several\nbaselines and other state-of-the-arts. The proposed benchmarks will be\navailable at https://github.com/cleardusk/MFR.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:10:30 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 04:50:08 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Guo", "Jianzhu", ""], ["Zhu", "Xiangyu", ""], ["Zhao", "Chenxu", ""], ["Cao", "Dong", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "2003.07734", "submitter": "Nam-Gyu Cho", "authors": "Da-Hye Yoon, Nam-Gyu Cho, Seong-Whan Lee", "title": "A Novel Online Action Detection Framework from Untrimmed Video Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online temporal action localization from an untrimmed video stream is a\nchallenging problem in computer vision. It is challenging because of i) in an\nuntrimmed video stream, more than one action instance may appear, including\nbackground scenes, and ii) in online settings, only past and current\ninformation is available. Therefore, temporal priors, such as the average\naction duration of training data, which have been exploited by previous action\ndetection methods, are not suitable for this task because of the high\nintra-class variation in human actions. We propose a novel online action\ndetection framework that considers actions as a set of temporally ordered\nsubclasses and leverages a future frame generation network to cope with the\nlimited information issue associated with the problem outlined above.\nAdditionally, we augment our data by varying the lengths of videos to allow the\nproposed method to learn about the high intra-class variation in human actions.\nWe evaluate our method using two benchmark datasets, THUMOS'14 and ActivityNet,\nfor an online temporal action localization scenario and demonstrate that the\nperformance is comparable to state-of-the-art methods that have been proposed\nfor offline settings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:11:24 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Yoon", "Da-Hye", ""], ["Cho", "Nam-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2003.07740", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Gyutaek Oh, Jong Chul Ye", "title": "Geometric Approaches to Increase the Expressivity of Deep Neural\n  Networks for MR Reconstruction", "comments": "Accepted for IEEE JSTSP Special Issue on Domain Enriched Learning for\n  Medical Imaging", "journal-ref": null, "doi": "10.1109/JSTSP.2020.2982777", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning approaches have been extensively investigated to\nreconstruct images from accelerated magnetic resonance image (MRI) acquisition.\nAlthough these approaches provide significant performance gain compared to\ncompressed sensing MRI (CS-MRI), it is not clear how to choose a suitable\nnetwork architecture to balance the trade-off between network complexity and\nperformance. Recently, it was shown that an encoder-decoder convolutional\nneural network (CNN) can be interpreted as a piecewise linear basis-like\nrepresentation, whose specific representation is determined by the ReLU\nactivation patterns for a given input image. Thus, the expressivity or the\nrepresentation power is determined by the number of piecewise linear regions.\nAs an extension of this geometric understanding, this paper proposes a\nsystematic geometric approach using bootstrapping and subnetwork aggregation\nusing an attention module to increase the expressivity of the underlying neural\nnetwork. Our method can be implemented in both k-space domain and image domain\nthat can be trained in an end-to-end manner. Experimental results show that the\nproposed schemes significantly improve reconstruction performance with\nnegligible complexity increases.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:18:37 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Cha", "Eunju", ""], ["Oh", "Gyutaek", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2003.07758", "submitter": "Vladimir Iashin", "authors": "Vladimir Iashin and Esa Rahtu", "title": "Multi-modal Dense Video Captioning", "comments": "To appear in the proceedings of CVPR Workshops 2020; Code:\n  https://github.com/v-iashin/MDVC Project Page:\n  https://v-iashin.github.io/mdvc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dense video captioning is a task of localizing interesting events from an\nuntrimmed video and producing textual description (captions) for each localized\nevent. Most of the previous works in dense video captioning are solely based on\nvisual information and completely ignore the audio track. However, audio, and\nspeech, in particular, are vital cues for a human observer in understanding an\nenvironment. In this paper, we present a new dense video captioning approach\nthat is able to utilize any number of modalities for event description.\nSpecifically, we show how audio and speech modalities may improve a dense video\ncaptioning model. We apply automatic speech recognition (ASR) system to obtain\na temporally aligned textual description of the speech (similar to subtitles)\nand treat it as a separate input alongside video frames and the corresponding\naudio track. We formulate the captioning task as a machine translation problem\nand utilize recently proposed Transformer architecture to convert multi-modal\ninput data into textual descriptions. We demonstrate the performance of our\nmodel on ActivityNet Captions dataset. The ablation studies indicate a\nconsiderable contribution from audio and speech components suggesting that\nthese modalities contain substantial complementary information to video frames.\nFurthermore, we provide an in-depth analysis of the ActivityNet Caption results\nby leveraging the category tags obtained from original YouTube videos. Code is\npublicly available: github.com/v-iashin/MDVC\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 15:15:17 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 18:12:10 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Iashin", "Vladimir", ""], ["Rahtu", "Esa", ""]]}, {"id": "2003.07761", "submitter": "Syed Waqas Zamir", "authors": "Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad\n  Shahbaz Khan, Ming-Hsuan Yang, Ling Shao", "title": "CycleISP: Real Image Restoration via Improved Data Synthesis", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The availability of large-scale datasets has helped unleash the true\npotential of deep convolutional neural networks (CNNs). However, for the\nsingle-image denoising problem, capturing a real dataset is an unacceptably\nexpensive and cumbersome procedure. Consequently, image denoising algorithms\nare mostly developed and evaluated on synthetic data that is usually generated\nwith a widespread assumption of additive white Gaussian noise (AWGN). While the\nCNNs achieve impressive results on these synthetic datasets, they do not\nperform well when applied on real camera images, as reported in recent\nbenchmark datasets. This is mainly because the AWGN is not adequate for\nmodeling the real camera noise which is signal-dependent and heavily\ntransformed by the camera imaging pipeline. In this paper, we present a\nframework that models camera imaging pipeline in forward and reverse\ndirections. It allows us to produce any number of realistic image pairs for\ndenoising both in RAW and sRGB spaces. By training a new image denoising\nnetwork on realistic synthetic data, we achieve the state-of-the-art\nperformance on real camera benchmark datasets. The parameters in our model are\n~5 times lesser than the previous best method for RAW denoising. Furthermore,\nwe demonstrate that the proposed framework generalizes beyond image denoising\nproblem e.g., for color matching in stereoscopic cinema. The source code and\npre-trained models are available at https://github.com/swz30/CycleISP.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 15:20:25 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Zamir", "Syed Waqas", ""], ["Arora", "Aditya", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Yang", "Ming-Hsuan", ""], ["Shao", "Ling", ""]]}, {"id": "2003.07770", "submitter": "Wenyan Lin", "authors": "Wen-Yan Lin", "title": "Hierarchical Models: Intrinsic Separability in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been noticed that high dimension data exhibits strange patterns.\nThis has been variously interpreted as either a \"blessing\" or a \"curse\",\ncausing uncomfortable inconsistencies in the literature. We propose that these\npatterns arise from an intrinsically hierarchical generative process. Modeling\nthe process creates a web of constraints that reconcile many different theories\nand results. The model also implies high dimensional data posses an innate\nseparability that can be exploited for machine learning. We demonstrate how\nthis permits the open-set learning problem to be defined mathematically,\nleading to qualitative and quantitative improvements in performance.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 09:27:24 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Lin", "Wen-Yan", ""]]}, {"id": "2003.07784", "submitter": "Pourya Shamsolmoali", "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Ruili Wang, Huiyu Zhou, Jie\n  Yang", "title": "A novel Deep Structure U-Net for Sea-Land Segmentation in Remote Sensing\n  Images", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sea-land segmentation is an important process for many key applications in\nremote sensing. Proper operative sea-land segmentation for remote sensing\nimages remains a challenging issue due to complex and diverse transition\nbetween sea and lands. Although several Convolutional Neural Networks (CNNs)\nhave been developed for sea-land segmentation, the performance of these CNNs is\nfar from the expected target. This paper presents a novel deep neural network\nstructure for pixel-wise sea-land segmentation, a Residual Dense U-Net\n(RDU-Net), in complex and high-density remote sensing images. RDU-Net is a\ncombination of both down-sampling and up-sampling paths to achieve satisfactory\nresults. In each down- and up-sampling path, in addition to the convolution\nlayers, several densely connected residual network blocks are proposed to\nsystematically aggregate multi-scale contextual information. Each dense network\nblock contains multilevel convolution layers, short-range connections and an\nidentity mapping connection which facilitates features re-use in the network\nand makes full use of the hierarchical features from the original images. These\nproposed blocks have a certain number of connections that are designed with\nshorter distance backpropagation between the layers and can significantly\nimprove segmentation results whilst minimizing computational costs. We have\nperformed extensive experiments on two real datasets Google Earth and ISPRS and\ncompare the proposed RDUNet against several variations of Dense Networks. The\nexperimental results show that RDUNet outperforms the other state-of-the-art\napproaches on the sea-land segmentation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:00:59 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Shamsolmoali", "Pourya", ""], ["Zareapoor", "Masoumeh", ""], ["Wang", "Ruili", ""], ["Zhou", "Huiyu", ""], ["Yang", "Jie", ""]]}, {"id": "2003.07790", "submitter": "Jean Ollion", "authors": "Jean Ollion and Charles Ollion", "title": "DistNet: Deep Tracking by displacement regression: application to\n  bacteria growing in the Mother Machine", "comments": "10 pages, 3 figures, 3 tables. Accepted in MICCAI 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mother machine is a popular microfluidic device that allows long-term\ntime-lapse imaging of thousands of cells in parallel by microscopy. It has\nbecome a valuable tool for single-cell level quantitative analysis and\ncharacterization of many cellular processes such as gene expression and\nregulation, mutagenesis or response to antibiotics. The automated and\nquantitative analysis of the massive amount of data generated by such\nexperiments is now the limiting step. In particular the segmentation and\ntracking of bacteria cells imaged in phase-contrast microscopy---with error\nrates compatible with high-throughput data---is a challenging problem.\n  In this work, we describe a novel formulation of the multi-object tracking\nproblem, in which tracking is performed by a regression of the bacteria's\ndisplacement, allowing simultaneous tracking of multiple bacteria, despite\ntheir growth and division over time. Our method performs jointly segmentation\nand tracking, leveraging sequential information to increase segmentation\naccuracy.\n  We introduce a Deep Neural Network architecture taking advantage of a\nself-attention mechanism which yields extremely low tracking error rate and\nsegmentation error rate. We demonstrate superior performance and speed compared\nto state-of-the-art methods. Our method is named DiSTNet which stands for\nDISTance+DISplacement Segmentation and Tracking Network.\n  While this method is particularly well suited for mother machine microscopy\ndata, its general joint tracking and segmentation formulation could be applied\nto many other problems with different geometries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:13:44 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 10:12:44 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ollion", "Jean", ""], ["Ollion", "Charles", ""]]}, {"id": "2003.07797", "submitter": "Matteo Gamba", "authors": "Matteo Gamba, Stefan Carlsson, Hossein Azizpour, M{\\aa}rten Bj\\\"orkman", "title": "Hyperplane Arrangements of Trained ConvNets Are Biased", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We investigate the geometric properties of the functions learned by trained\nConvNets in the preactivation space of their convolutional layers, by\nperforming an empirical study of hyperplane arrangements induced by a\nconvolutional layer. We introduce statistics over the weights of a trained\nnetwork to study local arrangements and relate them to the training dynamics.\nWe observe that trained ConvNets show a significant statistical bias towards\nregular hyperplane configurations. Furthermore, we find that layers showing\nbiased configurations are critical to validation performance for the\narchitectures considered, trained on CIFAR10, CIFAR100 and ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:22:17 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Gamba", "Matteo", ""], ["Carlsson", "Stefan", ""], ["Azizpour", "Hossein", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""]]}, {"id": "2003.07801", "submitter": "Caner Mercan", "authors": "Caner Mercan, Germonda Reijnen-Mooij, David Tellez Martin, Johannes\n  Lotz, Nick Weiss, Marcel van Gerven, Francesco Ciompi", "title": "Virtual staining for mitosis detection in Breast Histopathology", "comments": "5 pages, 4 figures. Accepted for publication at the IEEE\n  International Symposium on Biomedical Imaging (ISBI), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a virtual staining methodology based on Generative Adversarial\nNetworks to map histopathology images of breast cancer tissue from H&E stain to\nPHH3 and vice versa. We use the resulting synthetic images to build\nConvolutional Neural Networks (CNN) for automatic detection of mitotic figures,\na strong prognostic biomarker used in routine breast cancer diagnosis and\ngrading. We propose several scenarios, in which CNN trained with synthetically\ngenerated histopathology images perform on par with or even better than the\nsame baseline model trained with real images. We discuss the potential of this\napplication to scale the number of training samples without the need for manual\nannotations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:33:34 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Mercan", "Caner", ""], ["Reijnen-Mooij", "Germonda", ""], ["Martin", "David Tellez", ""], ["Lotz", "Johannes", ""], ["Weiss", "Nick", ""], ["van Gerven", "Marcel", ""], ["Ciompi", "Francesco", ""]]}, {"id": "2003.07803", "submitter": "Yilei Shi", "authors": "Yilei Shi, Richard Bamler, Yuanyuan Wang, Xiao Xiang Zhu", "title": "SAR Tomography at the Limit: Building Height Reconstruction Using Only\n  3-5 TanDEM-X Bistatic Interferograms", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2020.2986052", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-baseline interferometric synthetic aperture radar (InSAR) techniques\nare effective approaches for retrieving the 3-D information of urban areas. In\norder to obtain a plausible reconstruction, it is necessary to use more than\ntwenty interferograms. Hence, these methods are commonly not appropriate for\nlarge-scale 3-D urban mapping using TanDEM-X data where only a few acquisitions\nare available in average for each city. This work proposes a new SAR\ntomographic processing framework to work with those extremely small stacks,\nwhich integrates the non-local filtering into SAR tomography inversion. The\napplicability of the algorithm is demonstrated using a TanDEM-X multi-baseline\nstack with 5 bistatic interferograms over the whole city of Munich, Germany.\nSystematic comparison of our result with TanDEM-X raw digital elevation models\n(DEM) and airborne LiDAR data shows that the relative height accuracy of two\nthird buildings is within two meters, which outperforms the TanDEM-X raw DEM.\nThe promising performance of the proposed algorithm paved the first step\ntowards high quality large-scale 3-D urban mapping.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 16:37:33 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Shi", "Yilei", ""], ["Bamler", "Richard", ""], ["Wang", "Yuanyuan", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2003.07823", "submitter": "Aykut Erdem", "authors": "Ahmet Serdar Karadeniz, Erkut Erdem, Aykut Erdem", "title": "Burst Denoising of Dark Images", "comments": "This paper has been withdrawn by the authors to be replaced by a new\n  version available at arXiv:2006.09845", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing images under extremely low-light conditions poses significant\nchallenges for the standard camera pipeline. Images become too dark and too\nnoisy, which makes traditional image enhancement techniques almost impossible\nto apply. Very recently, researchers have shown promising results using\nlearning based approaches. Motivated by these ideas, in this paper, we propose\na deep learning framework for obtaining clean and colorful RGB images from\nextremely dark raw images. The backbone of our framework is a novel\ncoarse-to-fine network architecture that generates high-quality outputs in a\nprogressive manner. The coarse network predicts a low-resolution, denoised raw\nimage, which is then fed to the fine network to recover fine-scale details and\nrealistic textures. To further reduce noise and improve color accuracy, we\nextend this network to a permutation invariant structure so that it takes a\nburst of low-light images as input and merges information from multiple images\nat the feature-level. Our experiments demonstrate that the proposed approach\nleads to perceptually more pleasing results than state-of-the-art methods by\nproducing much sharper and higher quality images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:17:36 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 05:28:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Karadeniz", "Ahmet Serdar", ""], ["Erdem", "Erkut", ""], ["Erdem", "Aykut", ""]]}, {"id": "2003.07833", "submitter": "Akshita Gupta", "authors": "Sanath Narayan, Akshita Gupta, Fahad Shahbaz Khan, Cees G. M. Snoek,\n  Ling Shao", "title": "Latent Embedding Feedback and Discriminative Features for Zero-Shot\n  Classification", "comments": "Accepted for publication at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning strives to classify unseen categories for which no data is\navailable during training. In the generalized variant, the test samples can\nfurther belong to seen or unseen categories. The state-of-the-art relies on\nGenerative Adversarial Networks that synthesize unseen class features by\nleveraging class-specific semantic embeddings. During training, they generate\nsemantically consistent features, but discard this constraint during feature\nsynthesis and classification. We propose to enforce semantic consistency at all\nstages of (generalized) zero-shot learning: training, feature synthesis and\nclassification. We first introduce a feedback loop, from a semantic embedding\ndecoder, that iteratively refines the generated features during both the\ntraining and feature synthesis stages. The synthesized features together with\ntheir corresponding latent embeddings from the decoder are then transformed\ninto discriminative features and utilized during classification to reduce\nambiguities among categories. Experiments on (generalized) zero-shot object and\naction classification reveal the benefit of semantic consistency and iterative\nfeedback, outperforming existing methods on six zero-shot learning benchmarks.\nSource code at https://github.com/akshitac8/tfvaegan.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:34:16 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 12:27:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Narayan", "Sanath", ""], ["Gupta", "Akshita", ""], ["Khan", "Fahad Shahbaz", ""], ["Snoek", "Cees G. M.", ""], ["Shao", "Ling", ""]]}, {"id": "2003.07847", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Ye Yuan and Kris Kitani", "title": "PTP: Parallelized Tracking and Prediction with Graph Neural Networks and\n  Diversity Sampling", "comments": "Published in Robotics and Automation Letters (RA-L) 2021, with the\n  ICRA 2021 option. The first two authors contributed equally. Project website:\n  https://www.xinshuoweng.com/projects/PTP/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-object tracking (MOT) and trajectory prediction are two critical\ncomponents in modern 3D perception systems that require accurate modeling of\nmulti-agent interaction. We hypothesize that it is beneficial to unify both\ntasks under one framework in order to learn a shared feature representation of\nagent interaction. Furthermore, instead of performing tracking and prediction\nsequentially which can propagate errors from tracking to prediction, we propose\na parallelized framework to mitigate the issue. Also, our parallel\ntrack-forecast framework incorporates two additional novel computational units.\nFirst, we use a feature interaction technique by introducing Graph Neural\nNetworks (GNNs) to capture the way in which agents interact with one another.\nThe GNN is able to improve discriminative feature learning for MOT association\nand provide socially-aware contexts for trajectory prediction. Second, we use a\ndiversity sampling function to improve the quality and diversity of our\nforecasted trajectories. The learned sampling function is trained to\nefficiently extract a variety of outcomes from a generative trajectory\ndistribution and helps avoid the problem of generating duplicate trajectory\nsamples. We evaluate on KITTI and nuScenes datasets showing that our method\nwith socially-aware feature learning and diversity sampling achieves new\nstate-of-the-art performance on 3D MOT and trajectory prediction. Project\nwebsite is: https://www.xinshuoweng.com/projects/PTP\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:53:33 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:56:15 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Weng", "Xinshuo", ""], ["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2003.07848", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng, Stephen Gould", "title": "Learning to Structure an Image with Few Colors", "comments": null, "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color and structure are the two pillars that construct an image. Usually, the\nstructure is well expressed through a rich spectrum of colors, allowing objects\nin an image to be recognized by neural networks. However, under extreme\nlimitations of color space, the structure tends to vanish, and thus a neural\nnetwork might fail to understand the image. Interested in exploring this\ninterplay between color and structure, we study the scientific problem of\nidentifying and preserving the most informative image structures while\nconstraining the color space to just a few bits, such that the resulting image\ncan be recognized with possibly high accuracy. To this end, we propose a color\nquantization network, ColorCNN, which learns to structure the images from the\nclassification loss in an end-to-end manner. Given a color space size, ColorCNN\nquantizes colors in the original image by generating a color index map and an\nRGB color palette. Then, this color-quantized image is fed to a pre-trained\ntask network to evaluate its performance. In our experiment, with only a 1-bit\ncolor space (i.e., two colors), the proposed network achieves 82.1% top-1\naccuracy on the CIFAR10 dataset, outperforming traditional color quantization\nmethods by a large margin. For applications, when encoded with PNG, the\nproposed color quantization shows superiority over other image compression\nmethods in the extremely low bit-rate regime. The code is available at:\nhttps://github.com/hou-yz/color_distillation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:56:15 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:41:13 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""], ["Gould", "Stephen", ""]]}, {"id": "2003.07849", "submitter": "Takuhiro Kaneko", "authors": "Takuhiro Kaneko, Tatsuya Harada", "title": "Blur, Noise, and Compression Robust Generative Adversarial Networks", "comments": "Accepted to CVPR 2021. Project page:\n  https://takuhirok.github.io/BNCR-GAN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have gained considerable attention\nowing to their ability to reproduce images. However, they can recreate training\nimages faithfully despite image degradation in the form of blur, noise, and\ncompression, generating similarly degraded images. To solve this problem, the\nrecently proposed noise robust GAN (NR-GAN) provides a partial solution by\ndemonstrating the ability to learn a clean image generator directly from noisy\nimages using a two-generator model comprising image and noise generators.\nHowever, its application is limited to noise, which is relatively easy to\ndecompose owing to its additive and reversible characteristics, and its\napplication to irreversible image degradation, in the form of blur,\ncompression, and combination of all, remains a challenge. To address these\nproblems, we propose blur, noise, and compression robust GAN (BNCR-GAN) that\ncan learn a clean image generator directly from degraded images without\nknowledge of degradation parameters (e.g., blur kernel types, noise amounts, or\nquality factor values). Inspired by NR-GAN, BNCR-GAN uses a multiple-generator\nmodel composed of image, blur-kernel, noise, and quality-factor generators.\nHowever, in contrast to NR-GAN, to address irreversible characteristics, we\nintroduce masking architectures adjusting degradation strength values in a\ndata-driven manner using bypasses before and after degradation. Furthermore, to\nsuppress uncertainty caused by the combination of blur, noise, and compression,\nwe introduce adaptive consistency losses imposing consistency between\nirreversible degradation processes according to the degradation strengths. We\ndemonstrate the effectiveness of BNCR-GAN through large-scale comparative\nstudies on CIFAR-10 and a generality analysis on FFHQ. In addition, we\ndemonstrate the applicability of BNCR-GAN in image restoration.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:56:22 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 14:49:46 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kaneko", "Takuhiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2003.07853", "submitter": "Huiyu Wang", "authors": "Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille,\n  Liang-Chieh Chen", "title": "Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation", "comments": "ECCV 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution exploits locality for efficiency at a cost of missing long range\ncontext. Self-attention has been adopted to augment CNNs with non-local\ninteractions. Recent works prove it possible to stack self-attention layers to\nobtain a fully attentional network by restricting the attention to a local\nregion. In this paper, we attempt to remove this constraint by factorizing 2D\nself-attention into two 1D self-attentions. This reduces computation complexity\nand allows performing attention within a larger or even global region. In\ncompanion, we also propose a position-sensitive self-attention design.\nCombining both yields our position-sensitive axial-attention layer, a novel\nbuilding block that one could stack to form axial-attention models for image\nclassification and dense prediction. We demonstrate the effectiveness of our\nmodel on four large-scale datasets. In particular, our model outperforms all\nexisting stand-alone self-attention models on ImageNet. Our Axial-DeepLab\nimproves 2.8% PQ over bottom-up state-of-the-art on COCO test-dev. This\nprevious state-of-the-art is attained by our small variant that is 3.8x\nparameter-efficient and 27x computation-efficient. Axial-DeepLab also achieves\nstate-of-the-art results on Mapillary Vistas and Cityscapes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 17:59:56 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 18:09:32 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Wang", "Huiyu", ""], ["Zhu", "Yukun", ""], ["Green", "Bradley", ""], ["Adam", "Hartwig", ""], ["Yuille", "Alan", ""], ["Chen", "Liang-Chieh", ""]]}, {"id": "2003.07908", "submitter": "Bas Peters", "authors": "Bas Peters", "title": "Deep connections between learning from limited labels & physical\n  parameter estimation -- inspiration for regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently established equivalences between differential equations and the\nstructure of neural networks enabled some interpretation of training of a\nneural network as partial-differential-equation (PDE) constrained optimization.\nWe add to the previously established connections, explicit regularization that\nis particularly beneficial in the case of single large-scale examples with\npartial annotation. We show that explicit regularization of model parameters in\nPDE constrained optimization translates to regularization of the network\noutput. Examination of the structure of the corresponding Lagrangian and\nbackpropagation algorithm do not reveal additional computational challenges. A\nhyperspectral imaging example shows that minimum prior information together\nwith cross-validation for optimal regularization parameters boosts the\nsegmentation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:33:50 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Peters", "Bas", ""]]}, {"id": "2003.07911", "submitter": "Simon Hadush Nrea", "authors": "Simon Hadush, Yaecob Girmay, Abiot Sinamo, Gebrekirstos Hagos", "title": "Breast Cancer Detection Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is prevalent in Ethiopia that accounts 34% among women cancer\npatients. The diagnosis technique in Ethiopia is manual which was proven to be\ntedious, subjective, and challenging. Deep learning techniques are\nrevolutionizing the field of medical image analysis and hence in this study, we\nproposed Convolutional Neural Networks (CNNs) for breast mass detection so as\nto minimize the overheads of manual analysis. CNN architecture is designed for\nthe feature extraction stage and adapted both the Region Proposal Network (RPN)\nand Region of Interest (ROI) portion of the faster R-CNN for the automated\nbreast mass abnormality detection. Our model detects mass region and classifies\nthem into benign or malignant abnormality in mammogram(MG) images at once. For\nthe proposed model, MG images were collected from different hospitals,\nlocally.The images were passed through different preprocessing stages such as\ngaussian filter, median filter, bilateral filters and extracted the region of\nthe breast from the background of the MG image. The performance of the model on\ntest dataset is found to be: detection accuracy 91.86%, sensitivity of 94.67%\nand AUC-ROC of 92.2%.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:41:00 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 09:22:17 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 06:11:54 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hadush", "Simon", ""], ["Girmay", "Yaecob", ""], ["Sinamo", "Abiot", ""], ["Hagos", "Gebrekirstos", ""]]}, {"id": "2003.07923", "submitter": "Cheryl Sital", "authors": "Cheryl Sital, Tom Brosch, Dominique Tio, Alexander Raaijmakers,\n  J\\\"urgen Weese", "title": "3D medical image segmentation with labeled and unlabeled data using\n  autoencoders at the example of liver segmentation in CT images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of anatomical structures with convolutional neural\nnetworks (CNNs) constitutes a large portion of research in medical image\nanalysis. The majority of CNN-based methods rely on an abundance of labeled\ndata for proper training. Labeled medical data is often scarce, but unlabeled\ndata is more widely available. This necessitates approaches that go beyond\ntraditional supervised learning and leverage unlabeled data for segmentation\ntasks. This work investigates the potential of autoencoder-extracted features\nto improve segmentation with a CNN. Two strategies were considered. First,\ntransfer learning where pretrained autoencoder features were used as\ninitialization for the convolutional layers in the segmentation network.\nSecond, multi-task learning where the tasks of segmentation and feature\nextraction, by means of input reconstruction, were learned and optimized\nsimultaneously. A convolutional autoencoder was used to extract features from\nunlabeled data and a multi-scale, fully convolutional CNN was used to perform\nthe target task of 3D liver segmentation in CT images. For both strategies,\nexperiments were conducted with varying amounts of labeled and unlabeled\ntraining data. The proposed learning strategies improved results in $75\\%$ of\nthe experiments compared to training from scratch and increased the dice score\nby up to $0.040$ and $0.024$ for a ratio of unlabeled to labeled training data\nof about $32 : 1$ and $12.5 : 1$, respectively. The results indicate that both\ntraining strategies are more effective with a large ratio of unlabeled to\nlabeled training data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:20:43 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sital", "Cheryl", ""], ["Brosch", "Tom", ""], ["Tio", "Dominique", ""], ["Raaijmakers", "Alexander", ""], ["Weese", "J\u00fcrgen", ""]]}, {"id": "2003.07932", "submitter": "Marco Forte", "authors": "Marco Forte, Brian Price, Scott Cohen, Ning Xu, Fran\\c{c}ois Piti\\'e", "title": "Getting to 99% Accuracy in Interactive Segmentation", "comments": "Submitted for review to Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive object cutout tools are the cornerstone of the image editing\nworkflow. Recent deep-learning based interactive segmentation algorithms have\nmade significant progress in handling complex images and rough binary\nselections can typically be obtained with just a few clicks. Yet, deep learning\ntechniques tend to plateau once this rough selection has been reached. In this\nwork, we interpret this plateau as the inability of current algorithms to\nsufficiently leverage each user interaction and also as the limitations of\ncurrent training/testing datasets.\n  We propose a novel interactive architecture and a novel training scheme that\nare both tailored to better exploit the user workflow. We also show that\nsignificant improvements can be further gained by introducing a synthetic\ntraining dataset that is specifically designed for complex object boundaries.\nComprehensive experiments support our approach, and our network achieves state\nof the art performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:50:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Forte", "Marco", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Xu", "Ning", ""], ["Piti\u00e9", "Fran\u00e7ois", ""]]}, {"id": "2003.07934", "submitter": "Jose Mejia", "authors": "Miriam Zulema Jacobo, Jose Mejia", "title": "Segmentation of brain tumor on magnetic resonance imaging using a\n  convolutional architecture", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The brain is a complex organ controlling cognitive process and physical\nfunctions. Tumors in the brain are accelerated cell growths affecting the\nnormal function and processes in the brain. MRI scans provides detailed images\nof the body being one of the most common tests to diagnose brain tumors. The\nprocess of segmentation of brain tumors from magnetic resonance imaging can\nprovide a valuable guide for diagnosis, treatment planning and prediction of\nresults. Here we consider the problem brain tumor segmentation using a Deep\nlearning architecture for use in tumor segmentation. Although the proposed\narchitecture is simple and computationally easy to train, it is capable of\nreaching $IoU$ levels of 0.95.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:55:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Jacobo", "Miriam Zulema", ""], ["Mejia", "Jose", ""]]}, {"id": "2003.07936", "submitter": "Yichun Shi", "authors": "Yichun Shi, Anil K. Jain", "title": "Boosting Unconstrained Face Recognition with Auxiliary Unlabeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, significant progress has been made in face recognition,\nwhich can be partially attributed to the availability of large-scale labeled\nface datasets. However, since the faces in these datasets usually contain\nlimited degree and types of variation, the resulting trained models generalize\npoorly to more realistic unconstrained face datasets. While collecting labeled\nfaces with larger variations could be helpful, it is practically infeasible due\nto privacy and labor cost. In comparison, it is easier to acquire a large\nnumber of unlabeled faces from different domains, which could be used to\nregularize the learning of face representations. We present an approach to use\nsuch unlabeled faces to learn generalizable face representations, where we\nassume neither the access to identity labels nor domain labels for unlabeled\nimages. Experimental results on unconstrained datasets show that a small amount\nof unlabeled data with sufficient diversity can (i) lead to an appreciable gain\nin recognition performance and (ii) outperform the supervised baseline when\ncombined with less than half of the labeled data. Compared with the\nstate-of-the-art face recognition methods, our method further improves their\nperformance on challenging benchmarks, such as IJB-B, IJB-C and IJB-S.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 20:58:56 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 09:11:41 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shi", "Yichun", ""], ["Jain", "Anil K.", ""]]}, {"id": "2003.07948", "submitter": "Edemir Ferreira De Andrade Junior", "authors": "Edemir Ferreira, Matheus Brito, Remis Balaniuk, M\\'ario S. Alvim, and\n  Jefersson A. dos Santos", "title": "BrazilDAM: A Benchmark dataset for Tailings Dam Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present BrazilDAM, a novel public dataset based on Sentinel-2\nand Landsat-8 satellite images covering all tailings dams cataloged by the\nBrazilian National Mining Agency (ANM). The dataset was built using\ngeoreferenced images from 769 dams, recorded between 2016 and 2019. The time\nseries were processed in order to produce cloud free images. The dams contain\nmining waste from different ore categories and have highly varying shapes,\nareas and volumes, making BrazilDAM particularly interesting and challenging to\nbe used in machine learning benchmarks. The original catalog contains, besides\nthe dam coordinates, information about: the main ore, constructive method, risk\ncategory, and associated potential damage. To evaluate BrazilDAM's predictive\npotential we performed classification essays using state-of-the-art deep\nConvolutional Neural Network (CNNs). In the experiments, we achieved an average\nclassification accuracy of 94.11% in tailing dam binary classification task. In\naddition, others four setups of experiments were made using the complementary\ninformation from the original catalog, exhaustively exploiting the capacity of\nthe proposed dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:20:13 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 14:47:06 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Ferreira", "Edemir", ""], ["Brito", "Matheus", ""], ["Balaniuk", "Remis", ""], ["Alvim", "M\u00e1rio S.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "2003.07955", "submitter": "Matheus Pereira", "authors": "Matheus Barros Pereira and Jefersson Alex dos Santos", "title": "An End-to-end Framework For Low-Resolution Remote Sensing Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution images for remote sensing applications are often not\naffordable or accessible, especially when in need of a wide temporal span of\nrecordings. Given the easy access to low-resolution (LR) images from\nsatellites, many remote sensing works rely on this type of data. The problem is\nthat LR images are not appropriate for semantic segmentation, due to the need\nfor high-quality data for accurate pixel prediction for this task. In this\npaper, we propose an end-to-end framework that unites a super-resolution and a\nsemantic segmentation module in order to produce accurate thematic maps from LR\ninputs. It allows the semantic segmentation network to conduct the\nreconstruction process, modifying the input image with helpful textures. We\nevaluate the framework with three remote sensing datasets. The results show\nthat the framework is capable of achieving a semantic segmentation performance\nclose to native high-resolution data, while also surpassing the performance of\na network trained with LR inputs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:41:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Pereira", "Matheus Barros", ""], ["Santos", "Jefersson Alex dos", ""]]}, {"id": "2003.07969", "submitter": "Ankit Vora", "authors": "Siddharth Agarwal, Ankit Vora, Gaurav Pandey, Wayne Williams, Helen\n  Kourous and James McBride", "title": "Ford Multi-AV Seasonal Dataset", "comments": "7 pages, 7 figures, Submitted to International Journal of Robotics\n  Research (IJRR), Visit website at https://avdata.ford.com", "journal-ref": "IJRR, Volume: 39 issue: 12 (2020), page(s): 1367-1376", "doi": "10.1177/0278364920961451", "report-no": null, "categories": "cs.RO cs.CV cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a challenging multi-agent seasonal dataset collected by a\nfleet of Ford autonomous vehicles at different days and times during 2017-18.\nThe vehicles traversed an average route of 66 km in Michigan that included a\nmix of driving scenarios such as the Detroit Airport, freeways, city-centers,\nuniversity campus and suburban neighbourhoods, etc. Each vehicle used in this\ndata collection is a Ford Fusion outfitted with an Applanix POS-LV GNSS system,\nfour HDL-32E Velodyne 3D-lidar scanners, 6 Point Grey 1.3 MP Cameras arranged\non the rooftop for 360-degree coverage and 1 Pointgrey 5 MP camera mounted\nbehind the windshield for the forward field of view. We present the seasonal\nvariation in weather, lighting, construction and traffic conditions experienced\nin dynamic urban environments. This dataset can help design robust algorithms\nfor autonomous vehicles and multi-agent systems. Each log in the dataset is\ntime-stamped and contains raw data from all the sensors, calibration values,\npose trajectory, ground truth pose, and 3D maps. All data is available in\nRosbag format that can be visualized, modified and applied using the\nopen-source Robot Operating System (ROS). We also provide the output of\nstate-of-the-art reflectivity-based localization for bench-marking purposes.\nThe dataset can be freely downloaded at our website.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 22:33:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Agarwal", "Siddharth", ""], ["Vora", "Ankit", ""], ["Pandey", "Gaurav", ""], ["Williams", "Wayne", ""], ["Kourous", "Helen", ""], ["McBride", "James", ""]]}, {"id": "2003.07990", "submitter": "Daniel Gordon", "authors": "Daniel Gordon, Kiana Ehsani, Dieter Fox, Ali Farhadi", "title": "Watching the World Go By: Representation Learning from Unlabeled Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent single image unsupervised representation learning techniques show\nremarkable success on a variety of tasks. The basic principle in these works is\ninstance discrimination: learning to differentiate between two augmented\nversions of the same image and a large batch of unrelated images. Networks\nlearn to ignore the augmentation noise and extract semantically meaningful\nrepresentations. Prior work uses artificial data augmentation techniques such\nas cropping, and color jitter which can only affect the image in superficial\nways and are not aligned with how objects actually change e.g. occlusion,\ndeformation, viewpoint change. In this paper, we argue that videos offer this\nnatural augmentation for free. Videos can provide entirely new views of\nobjects, show deformation, and even connect semantically similar but visually\ndistinct concepts. We propose Video Noise Contrastive Estimation, a method for\nusing unlabeled video to learn strong, transferable single image\nrepresentations. We demonstrate improvements over recent unsupervised single\nimage techniques, as well as over fully supervised ImageNet pretraining, across\na variety of temporal and non-temporal tasks. Code and the Random Related Video\nViews dataset are available at https://www.github.com/danielgordon10/vince\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 00:07:21 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 17:23:14 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Gordon", "Daniel", ""], ["Ehsani", "Kiana", ""], ["Fox", "Dieter", ""], ["Farhadi", "Ali", ""]]}, {"id": "2003.07999", "submitter": "Donghao Zhang", "authors": "Donghao Zhang, Siqi Liu, Shikha Chaganti, Eli Gibson, Zhoubing Xu,\n  Sasa Grbic, Weidong Cai, and Dorin Comaniciu", "title": "Graph Attention Network based Pruning for Reconstructing 3D Liver Vessel\n  Morphology from Contrasted CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the injection of contrast material into blood vessels, multi-phase\ncontrasted CT images can enhance the visibility of vessel networks in the human\nbody. Reconstructing the 3D geometric morphology of liver vessels from the\ncontrasted CT images can enable multiple liver preoperative surgical planning\napplications. Automatic reconstruction of liver vessel morphology remains a\nchallenging problem due to the morphological complexity of liver vessels and\nthe inconsistent vessel intensities among different multi-phase contrasted CT\nimages. On the other side, high integrity is required for the 3D reconstruction\nto avoid decision making biases. In this paper, we propose a framework for\nliver vessel morphology reconstruction using both a fully convolutional neural\nnetwork and a graph attention network. A fully convolutional neural network is\nfirst trained to produce the liver vessel centerline heatmap. An\nover-reconstructed liver vessel graph model is then traced based on the heatmap\nusing an image processing based algorithm. We use a graph attention network to\nprune the false-positive branches by predicting the presence probability of\neach segmented branch in the initial reconstruction using the aggregated CNN\nfeatures. We evaluated the proposed framework on an in-house dataset consisting\nof 418 multi-phase abdomen CT images with contrast. The proposed graph network\npruning improves the overall reconstruction F1 score by 6.4% over the baseline.\nIt also outperformed the other state-of-the-art curvilinear structure\nreconstruction algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:03:33 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhang", "Donghao", ""], ["Liu", "Siqi", ""], ["Chaganti", "Shikha", ""], ["Gibson", "Eli", ""], ["Xu", "Zhoubing", ""], ["Grbic", "Sasa", ""], ["Cai", "Weidong", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2003.08002", "submitter": "Pourya Shamsolmoali", "authors": "Pourya Shamsolmoali, Masoumeh Zareapoor, Huiyu Zhou, Jie Yang", "title": "AMIL: Adversarial Multi Instance Learning for Human Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Human pose estimation has an important impact on a wide range of applications\nfrom human-computer interface to surveillance and content-based video\nretrieval. For human pose estimation, joint obstructions and overlapping upon\nhuman bodies result in departed pose estimation. To address these problems, by\nintegrating priors of the structure of human bodies, we present a novel\nstructure-aware network to discreetly consider such priors during the training\nof the network. Typically, learning such constraints is a challenging task.\nInstead, we propose generative adversarial networks as our learning model in\nwhich we design two residual multiple instance learning (MIL) models with the\nidentical architecture, one is used as the generator and the other one is used\nas the discriminator. The discriminator task is to distinguish the actual poses\nfrom the fake ones. If the pose generator generates the results that the\ndiscriminator is not able to distinguish from the real ones, the model has\nsuccessfully learnt the priors. In the proposed model, the discriminator\ndifferentiates the ground-truth heatmaps from the generated ones, and later the\nadversarial loss back-propagates to the generator. Such procedure assists the\ngenerator to learn reasonable body configurations and is proved to be\nadvantageous to improve the pose estimation accuracy. Meanwhile, we propose a\nnovel function for MIL. It is an adjustable structure for both instance\nselection and modeling to appropriately pass the information between instances\nin a single bag. In the proposed residual MIL neural network, the pooling\naction adequately updates the instance contribution to its bag. The proposed\nadversarial residual multi-instance neural network that is based on pooling has\nbeen validated on two datasets for the human pose estimation task and\nsuccessfully outperforms the other state-of-arts models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:22:16 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Shamsolmoali", "Pourya", ""], ["Zareapoor", "Masoumeh", ""], ["Zhou", "Huiyu", ""], ["Yang", "Jie", ""]]}, {"id": "2003.08005", "submitter": "Parag Mali", "authors": "Parag Mali, Puneeth Kukkadapu, Mahshad Mahdavi, Richard Zanibbi", "title": "ScanSSD: Scanning Single Shot Detector for Mathematical Formulas in PDF\n  Document Images", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Scanning Single Shot Detector (ScanSSD) for locating math\nformulas offset from text and embedded in textlines. ScanSSD uses only visual\nfeatures for detection: no formatting or typesetting information such as\nlayout, font, or character labels are employed. Given a 600 dpi document page\nimage, a Single Shot Detector (SSD) locates formulas at multiple scales using\nsliding windows, after which candidate detections are pooled to obtain\npage-level results. For our experiments we use the TFD-ICDAR2019v2 dataset, a\nmodification of the GTDB scanned math article collection. ScanSSD detects\ncharacters in formulas with high accuracy, obtaining a 0.926 f-score, and\ndetects formulas with high recall overall. Detection errors are largely minor,\nsuch as splitting formulas at large whitespace gaps (e.g., for variable\nconstraints) and merging formulas on adjacent textlines. Formula detection\nf-scores of 0.796 (IOU $\\geq0.5$) and 0.733 (IOU $\\ge 0.75$) are obtained. Our\ndata, evaluation tools, and code are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:32:44 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Mali", "Parag", ""], ["Kukkadapu", "Puneeth", ""], ["Mahdavi", "Mahshad", ""], ["Zanibbi", "Richard", ""]]}, {"id": "2003.08013", "submitter": "Lindsey Gray", "authors": "Lindsey Gray (1), Thomas Klijnsma (1), Shamik Ghosh (2) ((1) Fermi\n  National Accelerator Laboratory, (2) Saha Institute of Nuclear Physics)", "title": "A Dynamic Reduction Network for Point Clouds", "comments": "4 pages, 2 figures, to be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying whole images is a classic problem in machine learning, and graph\nneural networks are a powerful methodology to learn highly irregular\ngeometries. It is often the case that certain parts of a point cloud are more\nimportant than others when determining overall classification. On graph\nstructures this started by pooling information at the end of convolutional\nfilters, and has evolved to a variety of staged pooling techniques on static\ngraphs. In this paper, a dynamic graph formulation of pooling is introduced\nthat removes the need for predetermined graph structure. It achieves this by\ndynamically learning the most important relationships between data via an\nintermediate clustering. The network architecture yields interesting results\nconsidering representation size and efficiency. It also adapts easily to a\nlarge number of tasks from image classification to energy regression in high\nenergy particle physics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 02:10:12 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Gray", "Lindsey", ""], ["Klijnsma", "Thomas", ""], ["Ghosh", "Shamik", ""]]}, {"id": "2003.08021", "submitter": "Niloufar Salehi", "authors": "Niloufar Salehi Dastjerdi and M. Omair Ahmad", "title": "Applying r-spatiogram in object tracking for occlusion handling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Object tracking is one of the most important problems in computer vision. The\naim of video tracking is to extract the trajectories of a target or object of\ninterest, i.e. accurately locate a moving target in a video sequence and\ndiscriminate target from non-targets in the feature space of the sequence. So,\nfeature descriptors can have significant effects on such discrimination. In\nthis paper, we use the basic idea of many trackers which consists of three main\ncomponents of the reference model, i.e., object modeling, object detection and\nlocalization, and model updating. However, there are major improvements in our\nsystem. Our forth component, occlusion handling, utilizes the r-spatiogram to\ndetect the best target candidate. While spatiogram contains some moments upon\nthe coordinates of the pixels, r-spatiogram computes region-based compactness\non the distribution of the given feature in the image that captures richer\nfeatures to represent the objects. The proposed research develops an efficient\nand robust way to keep tracking the object throughout video sequences in the\npresence of significant appearance variations and severe occlusions. The\nproposed method is evaluated on the Princeton RGBD tracking dataset considering\nsequences with different challenges and the obtained results demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 02:42:51 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Dastjerdi", "Niloufar Salehi", ""], ["Ahmad", "M. Omair", ""]]}, {"id": "2003.08024", "submitter": "Yu Tian", "authors": "Yu Tian, Kunbo Zhang, Leyuan Wang, Zhenan Sun", "title": "Face Anti-Spoofing by Learning Polarization Cues in a Real-World\n  Scenario", "comments": "14pages,8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is the key to preventing security breaches in biometric\nrecognition applications. Existing software-based and hardware-based face\nliveness detection methods are effective in constrained environments or\ndesignated datasets only. Deep learning method using RGB and infrared images\ndemands a large amount of training data for new attacks. In this paper, we\npresent a face anti-spoofing method in a real-world scenario by automatic\nlearning the physical characteristics in polarization images of a real face\ncompared to a deceptive attack. A computational framework is developed to\nextract and classify the unique face features using convolutional neural\nnetworks and SVM together. Our real-time polarized face anti-spoofing (PAAS)\ndetection method uses a on-chip integrated polarization imaging sensor with\noptimized processing algorithms. Extensive experiments demonstrate the\nadvantages of the PAAS technique to counter diverse face spoofing attacks\n(print, replay, mask) in uncontrolled indoor and outdoor conditions by learning\npolarized face images of 33 people. A four-directional polarized face image\ndataset is released to inspire future applications within biometric\nanti-spoofing field.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 03:04:03 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:48:58 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Tian", "Yu", ""], ["Zhang", "Kunbo", ""], ["Wang", "Leyuan", ""], ["Sun", "Zhenan", ""]]}, {"id": "2003.08027", "submitter": "Shuai Wang", "authors": "Shuai Wang, Fan Lyu, Wei Feng, and Song Wang", "title": "MUTATT: Visual-Textual Mutual Guidance for Referring Expression\n  Comprehension", "comments": "6 pages, Accepted by ICME-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension (REC) aims to localize a text-related\nregion in a given image by a referring expression in natural language. Existing\nmethods focus on how to build convincing visual and language representations\nindependently, which may significantly isolate visual and language information.\nIn this paper, we argue that for REC the referring expression and the target\nregion are semantically correlated and subject, location and relationship\nconsistency exist between vision and language.On top of this, we propose a\nnovel approach called MutAtt to construct mutual guidance between vision and\nlanguage, which treat vision and language equally thus yield compact\ninformation matching. Specifically, for each module of subject, location and\nrelationship, MutAtt builds two kinds of attention-based mutual guidance\nstrategies. One strategy is to generate vision-guided language embedding for\nthe sake of matching relevant visual feature. The other reversely generates\nlanguage-guided visual feature to match relevant language embedding. This\nmutual guidance strategy can effectively guarantees the vision-language\nconsistency in three modules. Experiments on three popular REC datasets\ndemonstrate that the proposed approach outperforms the current state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 03:14:58 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 05:01:15 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Shuai", ""], ["Lyu", "Fan", ""], ["Feng", "Wei", ""], ["Wang", "Song", ""]]}, {"id": "2003.08033", "submitter": "Qi Xia", "authors": "Qi Xia, Haojie Liu and Zhan Ma", "title": "Object-Based Image Coding: A Learning-Driven Revisit", "comments": "ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Object-Based Image Coding (OBIC) that was extensively studied about two\ndecades ago, promised a vast application perspective for both ultra-low bitrate\ncommunication and high-level semantical content understanding, but it had\nrarely been used due to the inefficient compact representation of object with\narbitrary shape. A fundamental issue behind is how to efficiently process the\narbitrary-shaped objects at a fine granularity (e.g., feature element or pixel\nwise). To attack this, we have proposed to apply the element-wise masking and\ncompression by devising an object segmentation network for image layer\ndecomposition, and parallel convolution-based neural image compression networks\nto process masked foreground objects and background scene separately. All\ncomponents are optimized in an end-to-end learning framework to intelligently\nweigh their (e.g., object and background) contributions for visually pleasant\nreconstruction. We have conducted comprehensive experiments to evaluate the\nperformance on PASCAL VOC dataset at a very low bitrate scenario (e.g.,\n$\\lesssim$0.1 bits per pixel - bpp) which have demonstrated noticeable\nsubjective quality improvement compared with JPEG2K, HEVC-based BPG and another\nlearned image compression method. All relevant materials are made publicly\naccessible at https://njuvision.github.io/Neural-Object-Coding/.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 04:00:17 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Xia", "Qi", ""], ["Liu", "Haojie", ""], ["Ma", "Zhan", ""]]}, {"id": "2003.08040", "submitter": "Zhonghao Wang", "authors": "Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong,\n  Wen-mei Hwu, Thomas S. Huang, Humphrey Shi", "title": "Differential Treatment for Stuff and Things: A Simple Unsupervised\n  Domain Adaptation Method for Semantic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of unsupervised domain adaptation for semantic\nsegmentation by easing the domain shift between the source domain (synthetic\ndata) and the target domain (real data) in this work. State-of-the-art\napproaches prove that performing semantic-level alignment is helpful in\ntackling the domain shift issue. Based on the observation that stuff categories\nusually share similar appearances across images of different domains while\nthings (i.e. object instances) have much larger differences, we propose to\nimprove the semantic-level alignment with different strategies for stuff\nregions and for things: 1) for the stuff categories, we generate feature\nrepresentation for each class and conduct the alignment operation from the\ntarget domain to the source domain; 2) for the thing categories, we generate\nfeature representation for each individual instance and encourage the instance\nin the target domain to align with the most similar one in the source domain.\nIn this way, the individual differences within thing categories will also be\nconsidered to alleviate over-alignment. In addition to our proposed method, we\nfurther reveal the reason why the current adversarial loss is often unstable in\nminimizing the distribution discrepancy and show that our method can help ease\nthis issue by minimizing the most similar stuff and instance features between\nthe source and the target domains. We conduct extensive experiments in two\nunsupervised domain adaptation tasks, i.e. GTA5 to Cityscapes and SYNTHIA to\nCityscapes, and achieve the new state-of-the-art segmentation accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 04:43:25 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 04:25:09 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 17:56:27 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Wang", "Zhonghao", ""], ["Yu", "Mo", ""], ["Wei", "Yunchao", ""], ["Feris", "Rogerio", ""], ["Xiong", "Jinjun", ""], ["Hwu", "Wen-mei", ""], ["Huang", "Thomas S.", ""], ["Shi", "Humphrey", ""]]}, {"id": "2003.08042", "submitter": "Xu Li", "authors": "Xu Li, Jingwen Wang, Lin Ma, Kaihao Zhang, Fengzong Lian, Zhanhui Kang\n  and Jinjun Wang", "title": "STH: Spatio-Temporal Hybrid Convolution for Efficient Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective and Efficient spatio-temporal modeling is essential for action\nrecognition. Existing methods suffer from the trade-off between model\nperformance and model complexity. In this paper, we present a novel\nSpatio-Temporal Hybrid Convolution Network (denoted as \"STH\") which\nsimultaneously encodes spatial and temporal video information with a small\nparameter cost. Different from existing works that sequentially or parallelly\nextract spatial and temporal information with different convolutional layers,\nwe divide the input channels into multiple groups and interleave the spatial\nand temporal operations in one convolutional layer, which deeply incorporates\nspatial and temporal clues. Such a design enables efficient spatio-temporal\nmodeling and maintains a small model scale. STH-Conv is a general building\nblock, which can be plugged into existing 2D CNN architectures such as ResNet\nand MobileNet by replacing the conventional 2D-Conv blocks (2D convolutions).\nSTH network achieves competitive or even better performance than its\ncompetitors on benchmark datasets such as Something-Something (V1 & V2),\nJester, and HMDB-51. Moreover, STH enjoys performance superiority over 3D CNNs\nwhile maintaining an even smaller parameter cost than 2D CNNs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 04:46:30 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Li", "Xu", ""], ["Wang", "Jingwen", ""], ["Ma", "Lin", ""], ["Zhang", "Kaihao", ""], ["Lian", "Fengzong", ""], ["Kang", "Zhanhui", ""], ["Wang", "Jinjun", ""]]}, {"id": "2003.08047", "submitter": "Kanako Marusaki", "authors": "Kanako Marusaki and Hiroshi Watanabe", "title": "Capsule GAN Using Capsule Network for Generator Architecture", "comments": "7 pages and 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Capsule GAN, a Generative adversarial network using\nCapsule Network not only in the discriminator but also in the generator.\nRecently, Generative adversarial networks (GANs) has been intensively studied.\nHowever, generating images by GANs is difficult. Therefore, GANs sometimes\ngenerate poor quality images. These GANs use convolutional neural networks\n(CNNs). However, CNNs have the defect that the relational information between\nfeatures of the image may be lost. Capsule Network, proposed by Hinton in 2017,\novercomes the defect of CNNs. Capsule GAN reported previously uses Capsule\nNetwork in the discriminator. However, instead of using Capsule Network,\nCapsule GAN reported in previous studies uses CNNs in generator architecture\nlike DCGAN. This paper introduces two approaches to use Capsule Network in the\ngenerator. One is to use DigitCaps layer from the discriminator as the input to\nthe generator. DigitCaps layer is the output layer of Capsule Network. It has\nthe features of the input images of the discriminator. The other is to use the\nreverse operation of recognition process in Capsule Network in the generator.\nWe compare Capsule GAN proposed in this paper with conventional GAN using CNN\nand Capsule GAN which uses Capsule Network in the discriminator only. The\ndatasets are MNIST, Fashion-MNIST and color images. We show that Capsule GAN\noutperforms the GAN using CNN and the GAN using Capsule Network in the\ndiscriminator only. The architecture of Capsule GAN proposed in this paper is a\nbasic architecture using Capsule Network. Therefore, we can apply the existing\nimprovement techniques for GANs to Capsule GAN.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:14:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Marusaki", "Kanako", ""], ["Watanabe", "Hiroshi", ""]]}, {"id": "2003.08048", "submitter": "Diego L. Guarin", "authors": "Diego L. Guarin, Aidan Dempster, Andrea Bandini, Yana Yunusova and\n  Babak Taati", "title": "Estimation of Orofacial Kinematics in Parkinson's Disease: Comparison of\n  2D and 3D Markerless Systems for Motion Tracking", "comments": "4 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orofacial deficits are common in people with Parkinson's disease (PD) and\ntheir evolution might represent an important biomarker of disease progression.\nWe are developing an automated system for assessment of orofacial function in\nPD that can be used in-home or in-clinic and can provide useful and objective\nclinical information that informs disease management. Our current approach\nrelies on color and depth cameras for the estimation of 3D facial movements.\nHowever, depth cameras are not commonly available, might be expensive, and\nrequire specialized software for control and data processing. The objective of\nthis paper was to evaluate if depth cameras are needed to differentiate between\nhealthy controls and PD patients based on features extracted from orofacial\nkinematics. Results indicate that 2D features, extracted from color cameras\nonly, are as informative as 3D features, extracted from color and depth\ncameras, differentiating healthy controls from PD patients. These results pave\nthe way for the development of a universal system for automatic and objective\nassessment of orofacial function in PD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:18:27 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Guarin", "Diego L.", ""], ["Dempster", "Aidan", ""], ["Bandini", "Andrea", ""], ["Yunusova", "Yana", ""], ["Taati", "Babak", ""]]}, {"id": "2003.08052", "submitter": "Yuan Shen", "authors": "Yuan Shen, Shanduojiao Jiang, Muhammad Rizky Wellyanto, and Ranjitha\n  Kumar", "title": "Can AI decrypt fashion jargon for you?", "comments": "5 pages, 6 figures, Accepted at workshop paper for AI4HCI at CHI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When people talk about fashion, they care about the underlying meaning of\nfashion concepts,e.g., style.For example, people ask questions like what\nfeatures make this dress smart.However, the product descriptions in today\nfashion websites are full of domain specific and low level words. It is not\nclear to people how exactly those low level descriptions can contribute to a\nstyle or any high level fashion concept. In this paper, we proposed a data\ndriven solution to address this concept understanding issues by leveraging a\nlarge number of existing product data on fashion sites. We first collected and\ncategorized 1546 fashion keywords into 5 different fashion categories. Then, we\ncollected a new fashion product dataset with 853,056 products in total.\nFinally, we trained a deep learning model that can explicitly predict and\nexplain high level fashion concepts in a product image with its low level and\ndomain specific fashion features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:32:04 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Shen", "Yuan", ""], ["Jiang", "Shanduojiao", ""], ["Wellyanto", "Muhammad Rizky", ""], ["Kumar", "Ranjitha", ""]]}, {"id": "2003.08056", "submitter": "Changhee Won", "authors": "Changhee Won, Hochang Seok, Zhaopeng Cui, Marc Pollefeys, Jongwoo Lim", "title": "OmniSLAM: Omnidirectional Localization and Dense Mapping for\n  Wide-baseline Multi-camera Systems", "comments": "accepted by ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an omnidirectional localization and dense mapping\nsystem for a wide-baseline multiview stereo setup with ultra-wide field-of-view\n(FOV) fisheye cameras, which has a 360 degrees coverage of stereo observations\nof the environment. For more practical and accurate reconstruction, we first\nintroduce improved and light-weighted deep neural networks for the\nomnidirectional depth estimation, which are faster and more accurate than the\nexisting networks. Second, we integrate our omnidirectional depth estimates\ninto the visual odometry (VO) and add a loop closing module for global\nconsistency. Using the estimated depth map, we reproject keypoints onto each\nother view, which leads to a better and more efficient feature matching\nprocess. Finally, we fuse the omnidirectional depth maps and the estimated rig\nposes into the truncated signed distance function (TSDF) volume to acquire a 3D\nmap. We evaluate our method on synthetic datasets with ground-truth and\nreal-world sequences of challenging environments, and the extensive experiments\nshow that the proposed system generates excellent reconstruction results in\nboth synthetic and real-world environments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 05:52:10 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Won", "Changhee", ""], ["Seok", "Hochang", ""], ["Cui", "Zhaopeng", ""], ["Pollefeys", "Marc", ""], ["Lim", "Jongwoo", ""]]}, {"id": "2003.08061", "submitter": "Zezheng Wang", "authors": "Zezheng Wang, Zitong Yu, Chenxu Zhao, Xiangyu Zhu, Yunxiao Qin,\n  Qiusheng Zhou, Feng Zhou, Zhen Lei", "title": "Deep Spatial Gradient and Temporal Depth Learning for Face Anti-spoofing", "comments": "Accepted by CVPR2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face anti-spoofing is critical to the security of face recognition systems.\nDepth supervised learning has been proven as one of the most effective methods\nfor face anti-spoofing. Despite the great success, most previous works still\nformulate the problem as a single-frame multi-task one by simply augmenting the\nloss with depth, while neglecting the detailed fine-grained information and the\ninterplay between facial depths and moving patterns. In contrast, we design a\nnew approach to detect presentation attacks from multiple frames based on two\ninsights: 1) detailed discriminative clues (e.g., spatial gradient magnitude)\nbetween living and spoofing face may be discarded through stacked vanilla\nconvolutions, and 2) the dynamics of 3D moving faces provide important clues in\ndetecting the spoofing faces. The proposed method is able to capture\ndiscriminative details via Residual Spatial Gradient Block (RSGB) and encode\nspatio-temporal information from Spatio-Temporal Propagation Module (STPM)\nefficiently. Moreover, a novel Contrastive Depth Loss is presented for more\naccurate depth supervision. To assess the efficacy of our method, we also\ncollect a Double-modal Anti-spoofing Dataset (DMAD) which provides actual depth\nfor each sample. The experiments demonstrate that the proposed approach\nachieves state-of-the-art results on five benchmark datasets including\nOULU-NPU, SiW, CASIA-MFSD, Replay-Attack, and the new DMAD. Codes will be\navailable at https://github.com/clks-wzz/FAS-SGTD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 06:11:20 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Wang", "Zezheng", ""], ["Yu", "Zitong", ""], ["Zhao", "Chenxu", ""], ["Zhu", "Xiangyu", ""], ["Qin", "Yunxiao", ""], ["Zhou", "Qiusheng", ""], ["Zhou", "Feng", ""], ["Lei", "Zhen", ""]]}, {"id": "2003.08069", "submitter": "Changxing Ding", "authors": "Changxing Ding, Kan Wang, Pengfei Wang, and Dacheng Tao", "title": "Multi-task Learning with Coarse Priors for Robust Part-aware Person\n  Re-identification", "comments": "Accepted Version to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2020.3024900", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-level representations are important for robust person re-identification\n(ReID), but in practice feature quality suffers due to the body part\nmisalignment problem. In this paper, we present a robust, compact, and\neasy-to-use method called the Multi-task Part-aware Network (MPN), which is\ndesigned to extract semantically aligned part-level features from pedestrian\nimages. MPN solves the body part misalignment problem via multi-task learning\n(MTL) in the training stage. More specifically, it builds one main task (MT)\nand one auxiliary task (AT) for each body part on the top of the same backbone\nmodel. The ATs are equipped with a coarse prior of the body part locations for\ntraining images. ATs then transfer the concept of the body parts to the MTs via\noptimizing the MT parameters to identify part-relevant channels from the\nbackbone model. Concept transfer is accomplished by means of two novel\nalignment strategies: namely, parameter space alignment via hard parameter\nsharing and feature space alignment in a class-wise manner. With the aid of the\nlearned high-quality parameters, MTs can independently extract semantically\naligned part-level features from relevant channels in the testing stage. MPN\nhas three key advantages: 1) it does not need to conduct body part detection in\nthe inference stage; 2) its model is very compact and efficient for both\ntraining and testing; 3) in the training stage, it requires only coarse priors\nof body part locations, which are easy to obtain. Systematic experiments on\nfour large-scale ReID databases demonstrate that MPN consistently outperforms\nstate-of-the-art approaches by significant margins. Code is available at\nhttps://github.com/WangKan0128/MPN.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:10:44 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 09:37:20 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 07:39:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ding", "Changxing", ""], ["Wang", "Kan", ""], ["Wang", "Pengfei", ""], ["Tao", "Dacheng", ""]]}, {"id": "2003.08073", "submitter": "Moab Arar", "authors": "Moab Arar, Yiftach Ginger, Dov Danon, Ilya Leizerson, Amit Bermano,\n  Daniel Cohen-Or", "title": "Unsupervised Multi-Modal Image Registration via Geometry Preserving\n  Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications, such as autonomous driving, heavily rely on multi-modal\ndata where spatial alignment between the modalities is required. Most\nmulti-modal registration methods struggle computing the spatial correspondence\nbetween the images using prevalent cross-modality similarity measures. In this\nwork, we bypass the difficulties of developing cross-modality similarity\nmeasures, by training an image-to-image translation network on the two input\nmodalities. This learned translation allows training the registration network\nusing simple and reliable mono-modality metrics. We perform multi-modal\nregistration using two networks - a spatial transformation network and a\ntranslation network. We show that by encouraging our translation network to be\ngeometry preserving, we manage to train an accurate spatial transformation\nnetwork. Compared to state-of-the-art multi-modal methods our presented method\nis unsupervised, requiring no pairs of aligned modalities for training, and can\nbe adapted to any pair of modalities. We evaluate our method quantitatively and\nqualitatively on commercial datasets, showing that it performs well on several\nmodalities and achieves accurate alignment.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:21:09 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Arar", "Moab", ""], ["Ginger", "Yiftach", ""], ["Danon", "Dov", ""], ["Leizerson", "Ilya", ""], ["Bermano", "Amit", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2003.08074", "submitter": "Benjamin Meyer", "authors": "Luke Ditria, Benjamin J. Meyer, Tom Drummond", "title": "OpenGAN: Open Set Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing conditional Generative Adversarial Networks (cGANs) are limited\nto conditioning on pre-defined and fixed class-level semantic labels or\nattributes. We propose an open set GAN architecture (OpenGAN) that is\nconditioned per-input sample with a feature embedding drawn from a metric\nspace. Using a state-of-the-art metric learning model that encodes both\nclass-level and fine-grained semantic information, we are able to generate\nsamples that are semantically similar to a given source image. The semantic\ninformation extracted by the metric learning model transfers to\nout-of-distribution novel classes, allowing the generative model to produce\nsamples that are outside of the training distribution. We show that our\nproposed method is able to generate 256$\\times$256 resolution images from novel\nclasses that are of similar visual quality to those from the training classes.\nIn lieu of a source image, we demonstrate that random sampling of the metric\nspace also results in high-quality samples. We show that interpolation in the\nfeature space and latent space results in semantically and visually plausible\ntransformations in the image space. Finally, the usefulness of the generated\nsamples to the downstream task of data augmentation is demonstrated. We show\nthat classifier performance can be significantly improved by augmenting the\ntraining data with OpenGAN samples on classes that are outside of the GAN\ntraining distribution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:24:37 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Ditria", "Luke", ""], ["Meyer", "Benjamin J.", ""], ["Drummond", "Tom", ""]]}, {"id": "2003.08077", "submitter": "Xinjie Feng", "authors": "Xinjie Feng, Hongxun Yao, Yuankai Qi, Jun Zhang, and Shengping Zhang", "title": "Scene Text Recognition via Transformer", "comments": "We found that there are some errors in the experiment code, and we\n  are correcting the result temporarily, so we temporarily withdraw this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text recognition with arbitrary shape is very challenging due to large\nvariations in text shapes, fonts, colors, backgrounds, etc. Most\nstate-of-the-art algorithms rectify the input image into the normalized image,\nthen treat the recognition as a sequence prediction task. The bottleneck of\nsuch methods is the rectification, which will cause errors due to distortion\nperspective. In this paper, we find that the rectification is completely\nunnecessary. What all we need is the spatial attention. We therefore propose a\nsimple but extremely effective scene text recognition method based on\ntransformer [50]. Different from previous transformer based models [56,34],\nwhich just use the decoder of the transformer to decode the convolutional\nattention, the proposed method use a convolutional feature maps as word\nembedding input into transformer. In such a way, our method is able to make\nfull use of the powerful attention mechanism of the transformer. Extensive\nexperimental results show that the proposed method significantly outperforms\nstate-of-the-art methods by a very large margin on both regular and irregular\ntext datasets. On one of the most challenging CUTE dataset whose\nstate-of-the-art prediction accuracy is 89.6%, our method achieves 99.3%, which\nis a pretty surprising result. We will release our source code and believe that\nour method will be a new benchmark of scene text recognition with arbitrary\nshapes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:38:02 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 07:26:54 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 14:17:08 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2020 02:56:28 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Feng", "Xinjie", ""], ["Yao", "Hongxun", ""], ["Qi", "Yuankai", ""], ["Zhang", "Jun", ""], ["Zhang", "Shengping", ""]]}, {"id": "2003.08082", "submitter": "Tzu-Ming Harry Hsu", "authors": "Tzu-Ming Harry Hsu, Hang Qi, and Matthew Brown", "title": "Federated Visual Classification with Real-World Data Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning enables visual models to be trained on-device, bringing\nadvantages for user privacy (data need never leave the device), but challenges\nin terms of data diversity and quality. Whilst typical models in the datacenter\nare trained using data that are independent and identically distributed (IID),\ndata at source are typically far from IID. Furthermore, differing quantities of\ndata are typically available at each device (imbalance). In this work, we\ncharacterize the effect these real-world data distributions have on distributed\nlearning, using as a benchmark the standard Federated Averaging (FedAvg)\nalgorithm. To do so, we introduce two new large-scale datasets for species and\nlandmark classification, with realistic per-user data splits that simulate\nreal-world edge learning scenarios. We also develop two new algorithms (FedVC,\nFedIR) that intelligently resample and reweight over the client pool, bringing\nlarge improvements in accuracy and stability in training. The datasets are made\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 07:55:49 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 01:13:08 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 14:25:27 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Hsu", "Tzu-Ming Harry", ""], ["Qi", "Hang", ""], ["Brown", "Matthew", ""]]}, {"id": "2003.08111", "submitter": "Irtiza Hasan", "authors": "Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso", "title": "Transformer Networks for Trajectory Forecasting", "comments": "To appear in International Conference on Pattern Recognition (ICPR)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent successes on forecasting the people motion are based on LSTM\nmodels and all most recent progress has been achieved by modelling the social\ninteraction among people and the people interaction with the scene. We question\nthe use of the LSTM models and propose the novel use of Transformer Networks\nfor trajectory forecasting. This is a fundamental switch from the sequential\nstep-by-step processing of LSTMs to the only-attention-based memory mechanisms\nof Transformers. In particular, we consider both the original Transformer\nNetwork (TF) and the larger Bidirectional Transformer (BERT), state-of-the-art\non all natural language processing tasks. Our proposed Transformers predict the\ntrajectories of the individual people in the scene. These are \"simple\" model\nbecause each person is modelled separately without any complex human-human nor\nscene interaction terms. In particular, the TF model without bells and whistles\nyields the best score on the largest and most challenging trajectory\nforecasting benchmark of TrajNet. Additionally, its extension which predicts\nmultiple plausible future trajectories performs on par with more engineered\ntechniques on the 5 datasets of ETH + UCY. Finally, we show that Transformers\nmay deal with missing observations, as it may be the case with real sensor\ndata. Code is available at https://github.com/FGiuliari/Trajectory-Transformer.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 09:17:49 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 11:19:49 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 15:26:14 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Giuliari", "Francesco", ""], ["Hasan", "Irtiza", ""], ["Cristani", "Marco", ""], ["Galasso", "Fabio", ""]]}, {"id": "2003.08124", "submitter": "Ziwei Liu", "authors": "Hang Zhou, Jihao Liu, Ziwei Liu, Yu Liu, Xiaogang Wang", "title": "Rotate-and-Render: Unsupervised Photorealistic Face Rotation from\n  Single-View Images", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020. Code and models are available at:\n  https://github.com/Hangz-nju-cuhk/Rotate-and-Render", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though face rotation has achieved rapid progress in recent years, the lack of\nhigh-quality paired training data remains a great hurdle for existing methods.\nThe current generative models heavily rely on datasets with multi-view images\nof the same person. Thus, their generated results are restricted by the scale\nand domain of the data source. To overcome these challenges, we propose a novel\nunsupervised framework that can synthesize photo-realistic rotated faces using\nonly single-view image collections in the wild. Our key insight is that\nrotating faces in the 3D space back and forth, and re-rendering them to the 2D\nplane can serve as a strong self-supervision. We leverage the recent advances\nin 3D face modeling and high-resolution GAN to constitute our building blocks.\nSince the 3D rotation-and-render on faces can be applied to arbitrary angles\nwithout losing details, our approach is extremely suitable for in-the-wild\nscenarios (i.e. no paired data are available), where existing methods fall\nshort. Extensive experiments demonstrate that our approach has superior\nsynthesis quality as well as identity preservation over the state-of-the-art\nmethods, across a wide range of poses and domains. Furthermore, we validate\nthat our rotate-and-render framework naturally can act as an effective data\naugmentation engine for boosting modern face recognition systems even on strong\nbaseline models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 09:54:46 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Zhou", "Hang", ""], ["Liu", "Jihao", ""], ["Liu", "Ziwei", ""], ["Liu", "Yu", ""], ["Wang", "Xiaogang", ""]]}, {"id": "2003.08134", "submitter": "Chen Zhang", "authors": "Chen Zhang, Xiaobo Lu, Zhiliang Huang", "title": "A Driver Fatigue Recognition Algorithm Based on Spatio-Temporal Feature\n  Sequence", "comments": null, "journal-ref": null, "doi": "10.1109/CISP-BMEI48845.2019.8965990", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researches show that fatigue driving is one of the important causes of road\ntraffic accidents, so it is of great significance to study the driver fatigue\nrecognition algorithm to improve road traffic safety. In recent years, with the\ndevelopment of deep learning, the field of pattern recognition has made great\ndevelopment. This paper designs a real-time fatigue state recognition algorithm\nbased on spatio-temporal feature sequence, which can be mainly applied to the\nscene of fatigue driving recognition. The algorithm is divided into three task\nnetworks: face detection network, facial landmark detection and head pose\nestimation network, fatigue recognition network. Experiments show that the\nalgorithm has the advantages of small volume, high speed and high accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 10:25:27 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Zhang", "Chen", ""], ["Lu", "Xiaobo", ""], ["Huang", "Zhiliang", ""]]}, {"id": "2003.08151", "submitter": "Hamidreza Kasaei", "authors": "S. Hamidreza Kasaei, Jorik Melsen, Floris van Beers, Christiaan\n  Steenkist, and Klemen Voncina", "title": "The State of Lifelong Learning in Service Robots: Current Bottlenecks in\n  Object Perception and Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service robots are appearing more and more in our daily life. The development\nof service robots combines multiple fields of research, from object perception\nto object manipulation. The state-of-the-art continues to improve to make a\nproper coupling between object perception and manipulation. This coupling is\nnecessary for service robots not only to perform various tasks in a reasonable\namount of time but also to continually adapt to new environments and safely\ninteract with non-expert human users. Nowadays, robots are able to recognize\nvarious objects, and quickly plan a collision-free trajectory to grasp a target\nobject in predefined settings. Besides, in most of the cases, there is a\nreliance on large amounts of training data. Therefore, the knowledge of such\nrobots is fixed after the training phase, and any changes in the environment\nrequire complicated, time-consuming, and expensive robot re-programming by\nhuman experts. Therefore, these approaches are still too rigid for real-life\napplications in unstructured environments, where a significant portion of the\nenvironment is unknown and cannot be directly sensed or controlled. In such\nenvironments, no matter how extensive the training data used for batch\nlearning, a robot will always face new objects. Therefore, apart from batch\nlearning, the robot should be able to continually learn about new object\ncategories and grasp affordances from very few training examples on-site.\nMoreover, apart from robot self-learning, non-expert users could interactively\nguide the process of experience acquisition by teaching new concepts, or by\ncorrecting insufficient or erroneous concepts. In this way, the robot will\nconstantly learn how to help humans in everyday tasks by gaining more and more\nexperiences without the need for re-programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 11:00:55 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 10:40:18 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 18:58:38 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kasaei", "S. Hamidreza", ""], ["Melsen", "Jorik", ""], ["van Beers", "Floris", ""], ["Steenkist", "Christiaan", ""], ["Voncina", "Klemen", ""]]}, {"id": "2003.08152", "submitter": "Qiangpeng Yang", "authors": "Qiangpeng Yang, Hongsheng Jin, Jun Huang, Wei Lin", "title": "SwapText: Image Based Texts Transfer in Scenes", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swapping text in scene images while preserving original fonts, colors, sizes\nand background textures is a challenging task due to the complex interplay\nbetween different factors. In this work, we present SwapText, a three-stage\nframework to transfer texts across scene images. First, a novel text swapping\nnetwork is proposed to replace text labels only in the foreground image.\nSecond, a background completion network is learned to reconstruct background\nimages. Finally, the generated foreground image and background image are used\nto generate the word image by the fusion network. Using the proposing\nframework, we can manipulate the texts of the input images even with severe\ngeometric distortion. Qualitative and quantitative results are presented on\nseveral scene text datasets, including regular and irregular text datasets. We\nconducted extensive experiments to prove the usefulness of our method such as\nimage based text translation, text image synthesis, etc.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 11:02:17 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Yang", "Qiangpeng", ""], ["Jin", "Hongsheng", ""], ["Huang", "Jun", ""], ["Lin", "Wei", ""]]}, {"id": "2003.08162", "submitter": "Qi Zhang", "authors": "Qi Zhang and Antoni B. Chan", "title": "3D Crowd Counting via Multi-View Fusion with 3D Gaussian Kernels", "comments": "8 pages, 5 figures, AAAI Conference on Artificial Intelligence, AAAI,\n  New York, Feb 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting has been studied for decades and a lot of works have achieved\ngood performance, especially the DNNs-based density map estimation methods.\nMost existing crowd counting works focus on single-view counting, while few\nworks have studied multi-view counting for large and wide scenes, where\nmultiple cameras are used. Recently, an end-to-end multi-view crowd counting\nmethod called multi-view multi-scale (MVMS) has been proposed, which fuses\nmultiple camera views using a CNN to predict a 2D scene-level density map on\nthe ground-plane. Unlike MVMS, we propose to solve the multi-view crowd\ncounting task through 3D feature fusion with 3D scene-level density maps,\ninstead of the 2D ground-plane ones. Compared to 2D fusion, the 3D fusion\nextracts more information of the people along z-dimension (height), which helps\nto solve the scale variations across multiple views. The 3D density maps still\npreserve the 2D density maps property that the sum is the count, while also\nproviding 3D information about the crowd density. We also explore the\nprojection consistency among the 3D prediction and the ground-truth in the 2D\nviews to further enhance the counting performance. The proposed method is\ntested on 3 multi-view counting datasets and achieves better or comparable\ncounting performance to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 11:35:11 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Zhang", "Qi", ""], ["Chan", "Antoni B.", ""]]}, {"id": "2003.08165", "submitter": "David Ha", "authors": "Yujin Tang, Duong Nguyen, David Ha", "title": "Neuroevolution of Self-Interpretable Agents", "comments": "To appear at the Genetic and Evolutionary Computation Conference\n  (GECCO 2020) as a full paper", "journal-ref": null, "doi": "10.1145/3377930.3389847", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inattentional blindness is the psychological phenomenon that causes one to\nmiss things in plain sight. It is a consequence of the selective attention in\nperception that lets us remain focused on important parts of our world without\ndistraction from irrelevant details. Motivated by selective attention, we study\nthe properties of artificial agents that perceive the world through the lens of\na self-attention bottleneck. By constraining access to only a small fraction of\nthe visual input, we show that their policies are directly interpretable in\npixel space. We find neuroevolution ideal for training self-attention\narchitectures for vision-based reinforcement learning (RL) tasks, allowing us\nto incorporate modules that can include discrete, non-differentiable operations\nwhich are useful for our agent. We argue that self-attention has similar\nproperties as indirect encoding, in the sense that large implicit weight\nmatrices are generated from a small number of key-query parameters, thus\nenabling our agent to solve challenging vision based tasks with at least 1000x\nfewer parameters than existing methods. Since our agent attends to only task\ncritical visual hints, they are able to generalize to environments where task\nirrelevant elements are modified while conventional methods fail. Videos of our\nresults and source code available at https://attentionagent.github.io/\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 11:40:35 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 09:00:39 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Tang", "Yujin", ""], ["Nguyen", "Duong", ""], ["Ha", "David", ""]]}, {"id": "2003.08177", "submitter": "Guan-An Wang", "authors": "Guan'an Wang, Shuo Yang, Huanyu Liu, Zhicheng Wang, Yang Yang,\n  Shuliang Wang, Gang Yu, Erjin Zhou and Jian Sun", "title": "High-Order Information Matters: Learning Relation and Topology for\n  Occluded Person Re-Identification", "comments": "accepted by CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occluded person re-identification (ReID) aims to match occluded person images\nto holistic ones across dis-joint cameras. In this paper, we propose a novel\nframework by learning high-order relation and topology information for\ndiscriminative features and robust alignment. At first, we use a CNN backbone\nand a key-points estimation model to extract semantic local features. Even so,\noccluded images still suffer from occlusion and outliers. Then, we view the\nlocal features of an image as nodes of a graph and propose an adaptive\ndirection graph convolutional (ADGC)layer to pass relation information between\nnodes. The proposed ADGC layer can automatically suppress the message-passing\nof meaningless features by dynamically learning di-rection and degree of\nlinkage. When aligning two groups of local features from two images, we view it\nas a graph matching problem and propose a cross-graph embedded-alignment (CGEA)\nlayer to jointly learn and embed topology information to local features, and\nstraightly predict similarity score. The proposed CGEA layer not only take full\nuse of alignment learned by graph matching but also re-place sensitive\none-to-one matching with a robust soft one. Finally, extensive experiments on\noccluded, partial, and holistic ReID tasks show the effectiveness of our\nproposed method. Specifically, our framework significantly outperforms\nstate-of-the-art by6.5%mAP scores on Occluded-Duke dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 12:18:35 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:12:46 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 09:10:23 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2020 03:40:39 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wang", "Guan'an", ""], ["Yang", "Shuo", ""], ["Liu", "Huanyu", ""], ["Wang", "Zhicheng", ""], ["Yang", "Yang", ""], ["Wang", "Shuliang", ""], ["Yu", "Gang", ""], ["Zhou", "Erjin", ""], ["Sun", "Jian", ""]]}, {"id": "2003.08209", "submitter": "Mostafa Kiani", "authors": "M. Kiani", "title": "Identification and Classification of Phenomena in Multispectral\n  Satellite Imagery Using a New Image Smoother Method and its Applications in\n  Environmental Remote Sensing", "comments": "Second International Congress on Engineering, Technology and\n  Innovation", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper a new method of image smoothing for satellite imagery and its\napplications in environmental remote sensing are presented. This method is\nbased on the global gradient minimization over the whole image. With respect to\nthe image discrete identity, the continuous minimization problem is\ndiscretized. Using the finite difference numerical method of differentiation, a\nsimple yet efficient 5*5-pixel template is derived. Convolution of the derived\ntemplate with the image in different bands results in the discrimination of\nvarious image elements. This method is extremely fast, besides being highly\nprecise. A case study is presented for the northern Iran, covering parts of the\nCaspian Sea. Comparison of the method with the usual Laplacian template reveals\nthat it is more capable of distinguishing phenomena in the image.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:34:43 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Kiani", "M.", ""]]}, {"id": "2003.08210", "submitter": "Mostafa Kiani", "authors": "M. Kiani", "title": "Optimal Image Smoothing and Its Applications in Anomaly Detection in\n  Remote Sensing", "comments": "Second International Conference On Biology and Earth Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper is focused on deriving an optimal image smoother. The optimization\nis done through the minimization of the norm of the Laplace operator in the\nimage coordinate system. Discretizing the Laplace operator and using the method\nof Euler-Lagrange result in a weighted average scheme for the optimal smoother.\nSatellite imagery can be smoothed by this optimal smoother. It is also very\nfast and can be used for detecting the anomalies in the image. A real anomaly\ndetecting problem is considered for the Qom region in Iran. Satellite image in\ndifferent bands are smoothed. Comparing the smoothed and original images in\ndifferent bands, the maps of anomalies are presented. Comparison between the\nderived method and the existing methods reveals that it is more efficient in\ndetecting anomalies in the region.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:26:14 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Kiani", "M.", ""]]}, {"id": "2003.08221", "submitter": "Jun Seo", "authors": "Jun Seo, Sung Whan Yoon, Jaekyun Moon", "title": "Task-Adaptive Clustering for Semi-Supervised Few-Shot Classification", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning aims to handle previously unseen tasks using only a small\namount of new training data. In preparing (or meta-training) a few-shot\nlearner, however, massive labeled data are necessary. In the real world,\nunfortunately, labeled data are expensive and/or scarce. In this work, we\npropose a few-shot learner that can work well under the semi-supervised setting\nwhere a large portion of training data is unlabeled. Our method employs\nexplicit task-conditioning in which unlabeled sample clustering for the current\ntask takes place in a new projection space different from the embedding feature\nspace. The conditioned clustering space is linearly constructed so as to\nquickly close the gap between the class centroids for the current task and the\nindependent per-class reference vectors meta-trained across tasks. In a more\ngeneral setting, our method introduces a concept of controlling the degree of\ntask-conditioning for meta-learning: the amount of task-conditioning varies\nwith the number of repetitive updates for the clustering space. Extensive\nsimulation results based on the miniImageNet and tieredImageNet datasets show\nstate-of-the-art semi-supervised few-shot classification performance of the\nproposed method. Simulation results also indicate that the proposed\ntask-adaptive clustering shows graceful degradation with a growing number of\ndistractor samples, i.e., unlabeled sample images coming from outside the\ncandidate classes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 13:50:19 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Seo", "Jun", ""], ["Yoon", "Sung Whan", ""], ["Moon", "Jaekyun", ""]]}, {"id": "2003.08229", "submitter": "Constantino Reyes-Aldasoro PhD", "authors": "Elena D'Amato, Constantino Carlos Reyes-Aldasoro, Maria Felicia\n  Faienza, Marcella Zollino", "title": "Detection of Pitt-Hopkins Syndrome based on morphological facial\n  features", "comments": "Submitted to MIUA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work describes an automatic methodology to discriminate between\nindividuals with the genetic disorder Pitt-Hopkins syndrome (PTHS), and healthy\nindividuals. As input data, the methodology accepts unconstrained frontal\nfacial photographs, from which faces are located with Histograms of Oriented\nGradients features descriptors. Pre-processing steps of the methodology consist\nof colour normalisation, scaling down, rotation, and cropping in order to\nproduce a series of images of faces with consistent dimensions. Sixty eight\nfacial landmarks are automatically located on each face through a cascade of\nregression functions learnt via gradient boosting to estimate the shape from an\ninitial approximation. The intensities of a sparse set of pixels indexed\nrelative to this initial estimate are used to determine the landmarks. A set of\ncarefully selected geometric features, for example, relative width of the\nmouth, or angle of the nose, are extracted from the landmarks. The features are\nused to investigate the statistical differences between the two populations of\nPTHS and healthy controls. The methodology was tested on 71 individuals with\nPTHS and 55 healthy controls. Two geometric features related to the nose and\nmouth showed statistical difference between the two populations.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:01:16 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 08:43:41 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["D'Amato", "Elena", ""], ["Reyes-Aldasoro", "Constantino Carlos", ""], ["Faienza", "Maria Felicia", ""], ["Zollino", "Marcella", ""]]}, {"id": "2003.08230", "submitter": "Yuanqiang Cai", "authors": "Yuanqiang Cai, Longyin Wen, Libo Zhang, Dawei Du, Weiqiang Wang", "title": "Rethinking Object Detection in Retail Stores", "comments": "Information Error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convention standard for object detection uses a bounding box to represent\neach individual object instance. However, it is not practical in the\nindustry-relevant applications in the context of warehouses due to severe\nocclusions among groups of instances of the same categories. In this paper, we\npropose a new task, ie, simultaneously object localization and counting,\nabbreviated as Locount, which requires algorithms to localize groups of objects\nof interest with the number of instances. However, there does not exist a\ndataset or benchmark designed for such a task. To this end, we collect a\nlarge-scale object localization and counting dataset with rich annotations in\nretail stores, which consists of 50,394 images with more than 1.9 million\nobject instances in 140 categories. Together with this dataset, we provide a\nnew evaluation protocol and divide the training and testing subsets to fairly\nevaluate the performance of algorithms for Locount, developing a new benchmark\nfor the Locount task. Moreover, we present a cascaded localization and counting\nnetwork as a strong baseline, which gradually classifies and regresses the\nbounding boxes of objects with the predicted numbers of instances enclosed in\nthe bounding boxes, trained in an end-to-end manner. Extensive experiments are\nconducted on the proposed dataset to demonstrate its significance and the\nanalysis discussions on failure cases are provided to indicate future\ndirections. Dataset is available at\nhttps://isrc.iscas.ac.cn/gitlab/research/locount-dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:01:54 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 04:58:48 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 05:14:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Cai", "Yuanqiang", ""], ["Wen", "Longyin", ""], ["Zhang", "Libo", ""], ["Du", "Dawei", ""], ["Wang", "Weiqiang", ""]]}, {"id": "2003.08235", "submitter": "Jun Seo", "authors": "Young-Hyun Park, Jun Seo, Jaekyun Moon", "title": "CAFENet: Class-Agnostic Few-Shot Edge Detection Network", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle a novel few-shot learning challenge, which we call few-shot\nsemantic edge detection, aiming to localize crisp boundaries of novel\ncategories using only a few labeled samples. We also present a Class-Agnostic\nFew-shot Edge detection Network (CAFENet) based on meta-learning strategy.\nCAFENet employs a semantic segmentation module in small-scale to compensate for\nlack of semantic information in edge labels. The predicted segmentation mask is\nused to generate an attention map to highlight the target object region, and\nmake the decoder module concentrate on that region. We also propose a new\nregularization method based on multi-split matching. In meta-training, the\nmetric-learning problem with high-dimensional vectors are divided into small\nsubproblems with low-dimensional sub-vectors. Since there is no existing\ndataset for few-shot semantic edge detection, we construct two new datasets,\nFSE-1000 and SBD-$5^i$, and evaluate the performance of the proposed CAFENet on\nthem. Extensive simulation results confirm the performance merits of the\ntechniques adopted in CAFENet.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:18:59 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Park", "Young-Hyun", ""], ["Seo", "Jun", ""], ["Moon", "Jaekyun", ""]]}, {"id": "2003.08237", "submitter": "Hugo Touvron", "authors": "Hugo Touvron, Andrea Vedaldi, Matthijs Douze, Herv\\'e J\\'egou", "title": "Fixing the train-test resolution discrepancy: FixEfficientNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides an extensive analysis of the performance of the\nEfficientNet image classifiers with several recent training procedures, in\nparticular one that corrects the discrepancy between train and test images. The\nresulting network, called FixEfficientNet, significantly outperforms the\ninitial architecture with the same number of parameters.\n  For instance, our FixEfficientNet-B0 trained without additional training data\nachieves 79.3% top-1 accuracy on ImageNet with 5.3M parameters. This is a +0.5%\nabsolute improvement over the Noisy student EfficientNet-B0 trained with 300M\nunlabeled images. An EfficientNet-L2 pre-trained with weak supervision on 300M\nunlabeled images and further optimized with FixRes achieves 88.5% top-1\naccuracy (top-5: 98.7%), which establishes the new state of the art for\nImageNet with a single crop.\n  These improvements are thoroughly evaluated with cleaner protocols than the\none usually employed for Imagenet, and particular we show that our improvement\nremains in the experimental setting of ImageNet-v2, that is less prone to\noverfitting, and with ImageNet Real Labels. In both cases we also establish the\nnew state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:22:58 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 06:10:54 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 08:29:00 GMT"}, {"version": "v4", "created": "Mon, 20 Apr 2020 08:03:36 GMT"}, {"version": "v5", "created": "Wed, 18 Nov 2020 09:56:31 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Touvron", "Hugo", ""], ["Vedaldi", "Andrea", ""], ["Douze", "Matthijs", ""], ["J\u00e9gou", "Herv\u00e9", ""]]}, {"id": "2003.08240", "submitter": "Xinhai Liu", "authors": "Xinhai Liu, Zhizhong Han, Fangzhou Hong, Yu-Shen Liu, Matthias Zwicker", "title": "LRC-Net: Learning Discriminative Features on Point Clouds by Encoding\n  Local Region Contexts", "comments": "To be published at GMP2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative feature directly on point clouds is still challenging\nin the understanding of 3D shapes. Recent methods usually partition point\nclouds into local region sets, and then extract the local region features with\nfixed-size CNN or MLP, and finally aggregate all individual local features into\na global feature using simple max pooling. However, due to the irregularity and\nsparsity in sampled point clouds, it is hard to encode the fine-grained\ngeometry of local regions and their spatial relationships when only using the\nfixed-size filters and individual local feature integration, which limit the\nability to learn discriminative features. To address this issue, we present a\nnovel Local-Region-Context Network (LRC-Net), to learn discriminative features\non point clouds by encoding the fine-grained contexts inside and among local\nregions simultaneously. LRC-Net consists of two main modules. The first module,\nnamed intra-region context encoding, is designed for capturing the geometric\ncorrelation inside each local region by novel variable-size convolution filter.\nThe second module, named inter-region context encoding, is proposed for\nintegrating the spatial relationships among local regions based on spatial\nsimilarity measures. Experimental results show that LRC-Net is competitive with\nstate-of-the-art methods in shape classification and shape segmentation\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 14:34:08 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 05:48:44 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Xinhai", ""], ["Han", "Zhizhong", ""], ["Hong", "Fangzhou", ""], ["Liu", "Yu-Shen", ""], ["Zwicker", "Matthias", ""]]}, {"id": "2003.08264", "submitter": "Donghyun Kim", "authors": "Donghyun Kim, Kuniaki Saito, Tae-Hyun Oh, Bryan A. Plummer, Stan\n  Sclaroff, and Kate Saenko", "title": "Cross-domain Self-supervised Learning for Domain Adaptation with Few\n  Source Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing unsupervised domain adaptation methods aim to transfer knowledge\nfrom a label-rich source domain to an unlabeled target domain. However,\nobtaining labels for some source domains may be very expensive, making complete\nlabeling as used in prior work impractical. In this work, we investigate a new\ndomain adaptation scenario with sparsely labeled source data, where only a few\nexamples in the source domain have been labeled, while the target domain is\nunlabeled. We show that when labeled source examples are limited, existing\nmethods often fail to learn discriminative features applicable for both source\nand target domains. We propose a novel Cross-Domain Self-supervised (CDS)\nlearning approach for domain adaptation, which learns features that are not\nonly domain-invariant but also class-discriminative. Our self-supervised\nlearning method captures apparent visual similarity with in-domain\nself-supervision in a domain adaptive manner and performs cross-domain feature\nmatching with across-domain self-supervision. In extensive experiments with\nthree standard benchmark datasets, our method significantly boosts performance\nof target accuracy in the new target domain with few source labels and is even\nhelpful on classical domain adaptation scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:11:07 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Kim", "Donghyun", ""], ["Saito", "Kuniaki", ""], ["Oh", "Tae-Hyun", ""], ["Plummer", "Bryan A.", ""], ["Sclaroff", "Stan", ""], ["Saenko", "Kate", ""]]}, {"id": "2003.08273", "submitter": "Ya Lu", "authors": "Ya Lu, Thomai Stathopoulou, Maria F. Vasiloglou, Stergios\n  Christodoulidis, Zeno Stanga, Stavroula Mougiakakou", "title": "An Artificial Intelligence-Based System to Assess Nutrient Intake for\n  Hospitalised Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular monitoring of nutrient intake in hospitalised patients plays a\ncritical role in reducing the risk of disease-related malnutrition. Although\nseveral methods to estimate nutrient intake have been developed, there is still\na clear demand for a more reliable and fully automated technique, as this could\nimprove data accuracy and reduce both the burden on participants and health\ncosts. In this paper, we propose a novel system based on artificial\nintelligence (AI) to accurately estimate nutrient intake, by simply processing\nRGB Depth (RGB-D) image pairs captured before and after meal consumption. The\nsystem includes a novel multi-task contextual network for food segmentation, a\nfew-shot learning-based classifier built by limited training samples for food\nrecognition, and an algorithm for 3D surface construction. This allows\nsequential food segmentation, recognition, and estimation of the consumed food\nvolume, permitting fully automatic estimation of the nutrient intake for each\nmeal. For the development and evaluation of the system, a dedicated new\ndatabase containing images and nutrient recipes of 322 meals is assembled,\ncoupled to data annotation using innovative strategies. Experimental results\ndemonstrate that the estimated nutrient intake is highly correlated (> 0.91) to\nthe ground truth and shows very small mean relative errors (< 20%),\noutperforming existing techniques proposed for nutrient intake assessment.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:28:51 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Lu", "Ya", ""], ["Stathopoulou", "Thomai", ""], ["Vasiloglou", "Maria F.", ""], ["Christodoulidis", "Stergios", ""], ["Stanga", "Zeno", ""], ["Mougiakakou", "Stavroula", ""]]}, {"id": "2003.08275", "submitter": "Noureldien Hussein", "authors": "Noureldien Hussein, Efstratios Gavves, Arnold W.M. Smeulders", "title": "PIC: Permutation Invariant Convolution for Recognizing Long-range\n  Activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural operations as convolutions, self-attention, and vector aggregation are\nthe go-to choices for recognizing short-range actions. However, they have three\nlimitations in modeling long-range activities. This paper presents PIC,\nPermutation Invariant Convolution, a novel neural layer to model the temporal\nstructure of long-range activities. It has three desirable properties. i.\nUnlike standard convolution, PIC is invariant to the temporal permutations of\nfeatures within its receptive field, qualifying it to model the weak temporal\nstructures. ii. Different from vector aggregation, PIC respects local\nconnectivity, enabling it to learn long-range temporal abstractions using\ncascaded layers. iii. In contrast to self-attention, PIC uses shared weights,\nmaking it more capable of detecting the most discriminant visual evidence\nacross long and noisy videos. We study the three properties of PIC and\ndemonstrate its effectiveness in recognizing the long-range activities of\nCharades, Breakfast, and MultiThumos.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:30:03 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Hussein", "Noureldien", ""], ["Gavves", "Efstratios", ""], ["Smeulders", "Arnold W. M.", ""]]}, {"id": "2003.08282", "submitter": "Raymond Baldwin", "authors": "R. Wes Baldwin, Mohammed Almatrafi, Vijayan Asari, Keigo Hirakawa", "title": "Event Probability Mask (EPM) and Event Denoising Convolutional Neural\n  Network (EDnCNN) for Neuromorphic Cameras", "comments": "submitted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel method for labeling real-world neuromorphic\ncamera sensor data by calculating the likelihood of generating an event at each\npixel within a short time window, which we refer to as \"event probability mask\"\nor EPM. Its applications include (i) objective benchmarking of event denoising\nperformance, (ii) training convolutional neural networks for noise removal\ncalled \"event denoising convolutional neural network\" (EDnCNN), and (iii)\nestimating internal neuromorphic camera parameters. We provide the first\ndataset (DVSNOISE20) of real-world labeled neuromorphic camera events for noise\nremoval.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:44:05 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 13:56:36 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Baldwin", "R. Wes", ""], ["Almatrafi", "Mohammed", ""], ["Asari", "Vijayan", ""], ["Hirakawa", "Keigo", ""]]}, {"id": "2003.08284", "submitter": "Weikai Tan", "authors": "Weikai Tan, Nannan Qin, Lingfei Ma, Ying Li, Jing Du, Guorong Cai, Ke\n  Yang, Jonathan Li", "title": "Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation\n  of Urban Roadways", "comments": "Toronto-3D dataset can be downloaded at\n  https://github.com/WeikaiTan/Toronto-3D", "journal-ref": null, "doi": "10.1109/CVPRW50498.2020.00109", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of large-scale outdoor point clouds is essential for\nurban scene understanding in various applications, especially autonomous\ndriving and urban high-definition (HD) mapping. With rapid developments of\nmobile laser scanning (MLS) systems, massive point clouds are available for\nscene understanding, but publicly accessible large-scale labeled datasets,\nwhich are essential for developing learning-based methods, are still limited.\nThis paper introduces Toronto-3D, a large-scale urban outdoor point cloud\ndataset acquired by a MLS system in Toronto, Canada for semantic segmentation.\nThis dataset covers approximately 1 km of point clouds and consists of about\n78.3 million points with 8 labeled object classes. Baseline experiments for\nsemantic segmentation were conducted and the results confirmed the capability\nof this dataset to train deep learning models effectively. Toronto-3D is\nreleased to encourage new research, and the labels will be improved and updated\nwith feedback from the research community.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:45:06 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 15:20:16 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 14:48:42 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Tan", "Weikai", ""], ["Qin", "Nannan", ""], ["Ma", "Lingfei", ""], ["Li", "Ying", ""], ["Du", "Jing", ""], ["Cai", "Guorong", ""], ["Yang", "Ke", ""], ["Li", "Jonathan", ""]]}, {"id": "2003.08303", "submitter": "Mar\\'ia J. G\\'omez-Silva", "authors": "M. J. G\\'omez-Silva, J.M. Armingol, A. de la Escalera", "title": "Triplet Permutation Method for Deep Learning of Single-Shot Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving Single-Shot Person Re-Identification (Re-Id) by training Deep\nConvolutional Neural Networks is a daunting challenge, due to the lack of\ntraining data, since only two images per person are available. This causes the\noverfitting of the models, leading to degenerated performance. This paper\nformulates the Triplet Permutation method to generate multiple training sets,\nfrom a certain re-id dataset. This is a novel strategy for feeding triplet\nnetworks, which reduces the overfitting of the Single-Shot Re-Id model. The\nimproved performance has been demonstrated over one of the most challenging\nRe-Id datasets, PRID2011, proving the effectiveness of the method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 15:57:13 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["G\u00f3mez-Silva", "M. J.", ""], ["Armingol", "J. M.", ""], ["de la Escalera", "A.", ""]]}, {"id": "2003.08310", "submitter": "Kyle Wilson", "authors": "Kyle Wilson and David Bindel", "title": "On the Distribution of Minima in Intrinsic-Metric Rotation Averaging", "comments": "To be published in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation Averaging is a non-convex optimization problem that determines\norientations of a collection of cameras from their images of a 3D scene. The\nproblem has been studied using a variety of distances and robustifiers. The\nintrinsic (or geodesic) distance on SO(3) is geometrically meaningful; but\nwhile some extrinsic distance-based solvers admit (conditional) guarantees of\ncorrectness, no comparable results have been found under the intrinsic metric.\n  In this paper, we study the spatial distribution of local minima. First, we\ndo a novel empirical study to demonstrate sharp transitions in qualitative\nbehavior: as problems become noisier, they transition from a single\n(easy-to-find) dominant minimum to a cost surface filled with minima. In the\nsecond part of this paper we derive a theoretical bound for when this\ntransition occurs. This is an extension of the results of [24], which used\nlocal convexity as a proxy to study the difficulty of problem. By recognizing\nthe underlying quotient manifold geometry of the problem we achieve an n-fold\nimprovement over prior work. Incidentally, our analysis also extends the prior\n$l_2$ work to general $l_p$ costs. Our results suggest using algebraic\nconnectivity as an indicator of problem difficulty.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 16:03:22 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Wilson", "Kyle", ""], ["Bindel", "David", ""]]}, {"id": "2003.08325", "submitter": "Marc Habermann", "authors": "Marc Habermann, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll,\n  Christian Theobalt", "title": "DeepCap: Monocular Human Performance Capture Using Weak Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human performance capture is a highly important computer vision problem with\nmany applications in movie production and virtual/augmented reality. Many\nprevious performance capture approaches either required expensive multi-view\nsetups or did not recover dense space-time coherent geometry with\nframe-to-frame correspondences. We propose a novel deep learning approach for\nmonocular dense human performance capture. Our method is trained in a weakly\nsupervised manner based on multi-view supervision completely removing the need\nfor training data with 3D ground truth annotations. The network architecture is\nbased on two separate networks that disentangle the task into a pose estimation\nand a non-rigid surface deformation step. Extensive qualitative and\nquantitative evaluations show that our approach outperforms the state of the\nart in terms of quality and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 16:39:56 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Habermann", "Marc", ""], ["Xu", "Weipeng", ""], ["Zollhoefer", "Michael", ""], ["Pons-Moll", "Gerard", ""], ["Theobalt", "Christian", ""]]}, {"id": "2003.08332", "submitter": "Amine Bohi", "authors": "Karim Makki, Amine Bohi, Augustin C. Ogier, Marc-Emmanuel Bellemare", "title": "A new geodesic-based feature for characterization of 3D shapes:\n  application to soft tissue organ temporal deformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for characterizing 3D shapes from point\nclouds and we show a direct application on a study of organ temporal\ndeformations. As an example, we characterize the behavior of a bladder during a\nforced respiratory motion with a reduced number of 3D surface points: first, a\nset of equidistant points representing the vertices of quadrilateral mesh for\nthe surface in the first time frame are tracked throughout a long dynamic MRI\nsequence using a Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. Second, a novel geometric feature which is invariant to scaling and\nrotation is proposed for characterizing the temporal organ deformations by\nemploying an Eulerian Partial Differential Equations (PDEs) methodology. We\ndemonstrate the robustness of our feature on both synthetic 3D shapes and\nrealistic dynamic MRI data portraying the bladder deformation during forced\nrespiratory motions. Promising results are obtained, showing that the proposed\nfeature may be useful for several computer vision applications such as medical\nimaging, aerodynamics and robotics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 16:56:41 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Makki", "Karim", ""], ["Bohi", "Amine", ""], ["Ogier", "Augustin C.", ""], ["Bellemare", "Marc-Emmanuel", ""]]}, {"id": "2003.08333", "submitter": "Zongxin Yang", "authors": "Zongxin Yang, Yunchao Wei, Yi Yang", "title": "Collaborative Video Object Segmentation by Foreground-Background\n  Integration", "comments": "ECCV 2020, Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the principles of embedding learning to tackle the\nchallenging semi-supervised video object segmentation. Different from previous\npractices that only explore the embedding learning using pixels from foreground\nobject (s), we consider background should be equally treated and thus propose\nCollaborative video object segmentation by Foreground-Background Integration\n(CFBI) approach. Our CFBI implicitly imposes the feature embedding from the\ntarget foreground object and its corresponding background to be contrastive,\npromoting the segmentation results accordingly. With the feature embedding from\nboth foreground and background, our CFBI performs the matching process between\nthe reference and the predicted sequence from both pixel and instance levels,\nmaking the CFBI be robust to various object scales. We conduct extensive\nexperiments on three popular benchmarks, i.e., DAVIS 2016, DAVIS 2017, and\nYouTube-VOS. Our CFBI achieves the performance (J$F) of 89.4%, 81.9%, and\n81.4%, respectively, outperforming all the other state-of-the-art methods.\nCode: https://github.com/z-x-yang/CFBI.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 16:59:46 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 11:31:22 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Yang", "Zongxin", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2003.08337", "submitter": "Amine Amyar", "authors": "Amine Amyar, Romain Modzelewski, Pierre Vera, Vincent Morard, and Su\n  Ruan", "title": "Weakly Supervised PET Tumor Detection Using Class Response", "comments": "Submitted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenges in medical imaging is the lack of data and\nannotated data. It is proven that classical segmentation methods such as U-NET\nare useful but still limited due to the lack of annotated data. Using a weakly\nsupervised learning is a promising way to address this problem, however, it is\nchallenging to train one model to detect and locate efficiently different type\nof lesions due to the huge variation in images. In this paper, we present a\nnovel approach to locate different type of lesions in positron emission\ntomography (PET) images using only a class label at the image-level. First, a\nsimple convolutional neural network classifier is trained to predict the type\nof cancer on two 2D MIP images. Then, a pseudo-localization of the tumor is\ngenerated using class activation maps, back-propagated and corrected in a\nmultitask learning approach with prior knowledge, resulting in a tumor\ndetection mask. Finally, we use the mask generated from the two 2D images to\ndetect the tumor in the 3D image. The advantage of our proposed method consists\nof detecting the whole tumor volume in 3D images, using only two 2D images of\nPET image, and showing a very promising results. It can be used as a tool to\nlocate very efficiently tumors in a PET scan, which is a time-consuming task\nfor physicians. In addition, we show that our proposed method can be used to\nconduct a radiomics study with state of the art results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:06:08 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 08:01:06 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Amyar", "Amine", ""], ["Modzelewski", "Romain", ""], ["Vera", "Pierre", ""], ["Morard", "Vincent", ""], ["Ruan", "Su", ""]]}, {"id": "2003.08348", "submitter": "Mihai Dusmanu", "authors": "Mihai Dusmanu, Johannes L. Sch\\\"onberger, Marc Pollefeys", "title": "Multi-View Optimization of Local Feature Geometry", "comments": "Accepted at ECCV 2020. 28 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of refining the geometry of local image\nfeatures from multiple views without known scene or camera geometry. Current\napproaches to local feature detection are inherently limited in their keypoint\nlocalization accuracy because they only operate on a single view. This\nlimitation has a negative impact on downstream tasks such as\nStructure-from-Motion, where inaccurate keypoints lead to large errors in\ntriangulation and camera localization. Our proposed method naturally\ncomplements the traditional feature extraction and matching paradigm. We first\nestimate local geometric transformations between tentative matches and then\noptimize the keypoint locations over multiple views jointly according to a\nnon-linear least squares formulation. Throughout a variety of experiments, we\nshow that our method consistently improves the triangulation and camera\nlocalization performance for both hand-crafted and learned local features.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:22:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:23:43 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Dusmanu", "Mihai", ""], ["Sch\u00f6nberger", "Johannes L.", ""], ["Pollefeys", "Marc", ""]]}, {"id": "2003.08354", "submitter": "Jerrin Thomas Panachakel", "authors": "Jerrin Thomas Panachakel and Jeena R.S", "title": "Two Tier Prediction of Stroke Using Artificial Neural Networks and\n  Support Vector Machines", "comments": "arXiv admin note: text overlap with arXiv:1706.08227", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebrovascular accident (CVA) or stroke is the rapid loss of brain function\ndue to disturbance in the blood supply to the brain. Statistically, stroke is\nthe second leading cause of death. This has motivated us to suggest a two-tier\nsystem for predicting stroke; the first tier makes use of Artificial Neural\nNetwork (ANN) to predict the chances of a person suffering from stroke. The ANN\nis trained the using the values of various risk factors of stroke of several\npatients who had stroke. Once a person is classified as having a high risk of\nstroke, s/he undergoes another the tier-2 classification test where his/her\nneuro MRI (Magnetic resonance imaging) is analysed to predict the chances of\nstroke. The tier-2 uses Non-negative Matrix Factorization and Haralick Textural\nfeatures for feature extraction and SVM classifier for classification. We have\nobtained an accuracy of 96.67% in tier-1 and an accuracy of 70% in tier-2.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 07:20:59 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 00:29:19 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Panachakel", "Jerrin Thomas", ""], ["S", "Jeena R.", ""]]}, {"id": "2003.08355", "submitter": "Zehua Wang", "authors": "Wei Hu, Qianjiang Hu, Zehua Wang, Xiang Gao", "title": "Dynamic Point Cloud Denoising via Manifold-to-Manifold Distance", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2021.3092826", "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D dynamic point clouds provide a natural discrete representation of\nreal-world objects or scenes in motion, with a wide range of applications in\nimmersive telepresence, autonomous driving, surveillance, \\etc. Nevertheless,\ndynamic point clouds are often perturbed by noise due to hardware, software or\nother causes. While a plethora of methods have been proposed for static point\ncloud denoising, few efforts are made for the denoising of dynamic point\nclouds, which is quite challenging due to the irregular sampling patterns both\nspatially and temporally. In this paper, we represent dynamic point clouds\nnaturally on spatial-temporal graphs, and exploit the temporal consistency with\nrespect to the underlying surface (manifold). In particular, we define a\nmanifold-to-manifold distance and its discrete counterpart on graphs to measure\nthe variation-based intrinsic distance between surface patches in the temporal\ndomain, provided that graph operators are discrete counterparts of functionals\non Riemannian manifolds. Then, we construct the spatial-temporal graph\nconnectivity between corresponding surface patches based on the temporal\ndistance and between points in adjacent patches in the spatial domain.\nLeveraging the initial graph representation, we formulate dynamic point cloud\ndenoising as the joint optimization of the desired point cloud and underlying\ngraph representation, regularized by both spatial smoothness and temporal\nconsistency. We reformulate the optimization and present an efficient\nalgorithm. Experimental results show that the proposed method significantly\noutperforms independent denoising of each frame from state-of-the-art static\npoint cloud denoising approaches, on both Gaussian noise and simulated LiDAR\nnoise.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 04:54:20 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 07:21:36 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 12:53:04 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hu", "Wei", ""], ["Hu", "Qianjiang", ""], ["Wang", "Zehua", ""], ["Gao", "Xiang", ""]]}, {"id": "2003.08362", "submitter": "Boaz Fish", "authors": "Boaz Fish and Ben Zion Bobrovsky", "title": "Neural Network Tracking of Moving Objects with Unknown Equations of\n  Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a Neural Network design that can be used to track\nthe location of a moving object within a given range based on the object's\nnoisy coordinates measurement. A function commonly performed by the KLMn\nfilter, our goal is to show that our method outperforms the Kalman filter in\ncertain scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 13:16:14 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Fish", "Boaz", ""], ["Bobrovsky", "Ben Zion", ""]]}, {"id": "2003.08365", "submitter": "Quanshi Zhang", "authors": "Hao Zhang, Yiting Chen, Liyao Xiang, Haotian Ma, Jie Shi, Quanshi\n  Zhang", "title": "Deep Quaternion Features for Privacy Protection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to revise the neural network to construct the\nquaternion-valued neural network (QNN), in order to prevent intermediate-layer\nfeatures from leaking input information. The QNN uses quaternion-valued\nfeatures, where each element is a quaternion. The QNN hides input information\ninto a random phase of quaternion-valued features. Even if attackers have\nobtained network parameters and intermediate-layer features, they cannot\nextract input information without knowing the target phase. In this way, the\nQNN can effectively protect the input privacy. Besides, the output accuracy of\nQNNs only degrades mildly compared to traditional neural networks, and the\ncomputational cost is much less than other privacy-preserving methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:38:24 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 09:37:52 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Yiting", ""], ["Xiang", "Liyao", ""], ["Ma", "Haotian", ""], ["Shi", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2003.08367", "submitter": "Pratul Srinivasan", "authors": "Pratul P. Srinivasan, Ben Mildenhall, Matthew Tancik, Jonathan T.\n  Barron, Richard Tucker, Noah Snavely", "title": "Lighthouse: Predicting Lighting Volumes for Spatially-Coherent\n  Illumination", "comments": "CVPR 2020. Project page:\n  https://people.eecs.berkeley.edu/~pratul/lighthouse/ [Updates: typos\n  corrected]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning solution for estimating the incident illumination\nat any 3D location within a scene from an input narrow-baseline stereo image\npair. Previous approaches for predicting global illumination from images either\npredict just a single illumination for the entire scene, or separately estimate\nthe illumination at each 3D location without enforcing that the predictions are\nconsistent with the same 3D scene. Instead, we propose a deep learning model\nthat estimates a 3D volumetric RGBA model of a scene, including content outside\nthe observed field of view, and then uses standard volume rendering to estimate\nthe incident illumination at any 3D location within that volume. Our model is\ntrained without any ground truth 3D data and only requires a held-out\nperspective view near the input stereo pair and a spherical panorama taken\nwithin each scene as supervision, as opposed to prior methods for\nspatially-varying lighting estimation, which require ground truth scene\ngeometry for training. We demonstrate that our method can predict consistent\nspatially-varying lighting that is convincing enough to plausibly relight and\ninsert highly specular virtual objects into real images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:46:30 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 17:04:29 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Srinivasan", "Pratul P.", ""], ["Mildenhall", "Ben", ""], ["Tancik", "Matthew", ""], ["Barron", "Jonathan T.", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""]]}, {"id": "2003.08375", "submitter": "Amir Rahimi", "authors": "Amir Rahimi, Amirreza Shaban, Thalaiyasingam Ajanthan, Richard\n  Hartley, Byron Boots", "title": "Pairwise Similarity Knowledge Transfer for Weakly Supervised Object\n  Localization", "comments": "ECCV 2020. formerly \"In Defense of Graph Inference Algorithms for\n  Weakly Supervised Object Localization\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Localization (WSOL) methods only require image level\nlabels as opposed to expensive bounding box annotations required by fully\nsupervised algorithms. We study the problem of learning localization model on\ntarget classes with weakly supervised image labels, helped by a fully annotated\nsource dataset. Typically, a WSOL model is first trained to predict class\ngeneric objectness scores on an off-the-shelf fully supervised source dataset\nand then it is progressively adapted to learn the objects in the weakly\nsupervised target dataset. In this work, we argue that learning only an\nobjectness function is a weak form of knowledge transfer and propose to learn a\nclasswise pairwise similarity function that directly compares two input\nproposals as well. The combined localization model and the estimated object\nannotations are jointly learned in an alternating optimization paradigm as is\ntypically done in standard WSOL methods. In contrast to the existing work that\nlearns pairwise similarities, our approach optimizes a unified objective with\nconvergence guarantee and it is computationally efficient for large-scale\napplications. Experiments on the COCO and ILSVRC 2013 detection datasets show\nthat the performance of the localization model improves significantly with the\ninclusion of pairwise similarity function. For instance, in the ILSVRC dataset,\nthe Correct Localization (CorLoc) performance improves from 72.8% to 78.2%\nwhich is a new state-of-the-art for WSOL task in the context of knowledge\ntransfer.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:53:33 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 14:09:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Rahimi", "Amir", ""], ["Shaban", "Amirreza", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Hartley", "Richard", ""], ["Boots", "Byron", ""]]}, {"id": "2003.08376", "submitter": "Xinshuo Weng", "authors": "Xinshuo Weng and Jianren Wang and Sergey Levine and Kris Kitani and\n  Nicholas Rhinehart", "title": "Inverting the Pose Forecasting Pipeline with SPF2: Sequential Pointcloud\n  Forecasting for Sequential Pose Forecasting", "comments": "Published in Conference on Robot Learning (CoRL), 2020. Project\n  webpage: http://www.xinshuoweng.com/projects/SPF2/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many autonomous systems forecast aspects of the future in order to aid\ndecision-making. For example, self-driving vehicles and robotic manipulation\nsystems often forecast future object poses by first detecting and tracking\nobjects. However, this detect-then-forecast pipeline is expensive to scale, as\npose forecasting algorithms typically require labeled sequences of object\nposes, which are costly to obtain in 3D space. Can we scale performance without\nrequiring additional labels? We hypothesize yes, and propose inverting the\ndetect-then-forecast pipeline. Instead of detecting, tracking and then\nforecasting the objects, we propose to first forecast 3D sensor data (e.g.,\npoint clouds with $100$k points) and then detect/track objects on the predicted\npoint cloud sequences to obtain future poses, i.e., a forecast-then-detect\npipeline. This inversion makes it less expensive to scale pose forecasting, as\nthe sensor data forecasting task requires no labels. Part of this work's focus\nis on the challenging first step -- Sequential Pointcloud Forecasting (SPF),\nfor which we also propose an effective approach, SPFNet. To compare our\nforecast-then-detect pipeline relative to the detect-then-forecast pipeline, we\npropose an evaluation procedure and two metrics. Through experiments on a\nrobotic manipulation dataset and two driving datasets, we show that SPFNet is\neffective for the SPF task, our forecast-then-detect pipeline outperforms the\ndetect-then-forecast approaches to which we compared, and that pose forecasting\nperformance improves with the addition of unlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:54:28 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 16:08:10 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 02:48:22 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Weng", "Xinshuo", ""], ["Wang", "Jianren", ""], ["Levine", "Sergey", ""], ["Kitani", "Kris", ""], ["Rhinehart", "Nicholas", ""]]}, {"id": "2003.08384", "submitter": "Chowdhury Rahman", "authors": "Abu Saleh Md. Abir, Sanjana Rahman, Samia Ellin, Maisha Farzana, Md\n  Hridoy Manik, Chowdhury Rafeed Rahman", "title": "Confronting the Constraints for Optical Character Segmentation from\n  Printed Bangla Text Image", "comments": null, "journal-ref": null, "doi": "10.1145/3428363.3428367", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world of digitization, optical character recognition holds the\nautomation to written history. Optical character recognition system basically\nconverts printed images into editable texts for better storage and usability.\nTo be completely functional, the system needs to go through some crucial\nmethods such as pre-processing and segmentation. Pre-processing helps printed\ndata to be noise free and gets rid of skewness efficiently whereas segmentation\nhelps the image fragment into line, word and character precisely for better\nconversion. These steps hold the door to better accuracy and consistent results\nfor a printed image to be ready for conversion. Our proposed algorithm is able\nto segment characters both from ideal and non-ideal cases of scanned or\ncaptured images giving a sustainable outcome. The implementation of our work is\nprovided here: https://cutt.ly/rgdfBIa\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:58:05 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 09:44:23 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 16:26:04 GMT"}, {"version": "v4", "created": "Tue, 11 Aug 2020 10:50:27 GMT"}, {"version": "v5", "created": "Tue, 5 Jan 2021 18:11:50 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Abir", "Abu Saleh Md.", ""], ["Rahman", "Sanjana", ""], ["Ellin", "Samia", ""], ["Farzana", "Maisha", ""], ["Manik", "Md Hridoy", ""], ["Rahman", "Chowdhury Rafeed", ""]]}, {"id": "2003.08386", "submitter": "Ye Yuan", "authors": "Ye Yuan, Kris Kitani", "title": "DLow: Diversifying Latent Flows for Diverse Human Motion Prediction", "comments": "ECCV 2020. Project Page: https://www.ye-yuan.com/dlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are often used for human motion prediction as they are\nable to model multi-modal data distributions and characterize diverse human\nbehavior. While much care has been taken into designing and learning deep\ngenerative models, how to efficiently produce diverse samples from a deep\ngenerative model after it has been trained is still an under-explored problem.\nTo obtain samples from a pretrained generative model, most existing generative\nhuman motion prediction methods draw a set of independent Gaussian latent codes\nand convert them to motion samples. Clearly, this random sampling strategy is\nnot guaranteed to produce diverse samples for two reasons: (1) The independent\nsampling cannot force the samples to be diverse; (2) The sampling is based\nsolely on likelihood which may only produce samples that correspond to the\nmajor modes of the data distribution. To address these problems, we propose a\nnovel sampling method, Diversifying Latent Flows (DLow), to produce a diverse\nset of samples from a pretrained deep generative model. Unlike random\n(independent) sampling, the proposed DLow sampling method samples a single\nrandom variable and then maps it with a set of learnable mapping functions to a\nset of correlated latent codes. The correlated latent codes are then decoded\ninto a set of correlated samples. During training, DLow uses a\ndiversity-promoting prior over samples as an objective to optimize the latent\nmappings to improve sample diversity. The design of the prior is highly\nflexible and can be customized to generate diverse motions with common features\n(e.g., similar leg motion but diverse upper-body motion). Our experiments\ndemonstrate that DLow outperforms state-of-the-art baseline methods in terms of\nsample diversity and accuracy. Our code is released on the project page:\nhttps://www.ye-yuan.com/dlow.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 17:58:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:53:24 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Yuan", "Ye", ""], ["Kitani", "Kris", ""]]}, {"id": "2003.08400", "submitter": "Jingwei Huang", "authors": "Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu, Chiyu Max\n  Jiang, Leonidas Guibas, Matthias Nie{\\ss}ner, Thomas Funkhouser", "title": "Adversarial Texture Optimization from RGB-D Scans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic color texture generation is an important step in RGB-D surface\nreconstruction, but remains challenging in practice due to inaccuracies in\nreconstructed geometry, misaligned camera poses, and view-dependent imaging\nartifacts.\n  In this work, we present a novel approach for color texture generation using\na conditional adversarial loss obtained from weakly-supervised views.\n  Specifically, we propose an approach to produce photorealistic textures for\napproximate surfaces, even from misaligned images, by learning an objective\nfunction that is robust to these errors.\n  The key idea of our approach is to learn a patch-based conditional\ndiscriminator which guides the texture optimization to be tolerant to\nmisalignments.\n  Our discriminator takes a synthesized view and a real image, and evaluates\nwhether the synthesized one is realistic, under a broadened definition of\nrealism.\n  We train the discriminator by providing as `real' examples pairs of input\nviews and their misaligned versions -- so that the learned adversarial loss\nwill tolerate errors from the scans.\n  Experiments on synthetic and real data under quantitative or qualitative\nevaluation demonstrate the advantage of our approach in comparison to state of\nthe art. Our code is publicly available with video demonstration.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:00:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Huang", "Jingwei", ""], ["Thies", "Justus", ""], ["Dai", "Angela", ""], ["Kundu", "Abhijit", ""], ["Jiang", "Chiyu Max", ""], ["Guibas", "Leonidas", ""], ["Nie\u00dfner", "Matthias", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2003.08407", "submitter": "Dmytro Kotovenko", "authors": "Dmytro Kotovenko, Artsiom Sanakoyeu, Pingchuan Ma, Sabine Lang,\n  Bj\\\"orn Ommer", "title": "A Content Transformation Block For Image Style Transfer", "comments": "Accepted to CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Style transfer has recently received a lot of attention, since it allows to\nstudy fundamental challenges in image understanding and synthesis. Recent work\nhas significantly improved the representation of color and texture and\ncomputational speed and image resolution. The explicit transformation of image\ncontent has, however, been mostly neglected: while artistic style affects\nformal characteristics of an image, such as color, shape or texture, it also\ndeforms, adds or removes content details. This paper explicitly focuses on a\ncontent-and style-aware stylization of a content image. Therefore, we introduce\na content transformation module between the encoder and decoder. Moreover, we\nutilize similar content appearing in photographs and style samples to learn how\nstyle alters content details and we generalize this to other class details.\nAdditionally, this work presents a novel normalization layer critical for high\nresolution image synthesis. The robustness and speed of our model enables a\nvideo stylization in real-time and high definition. We perform extensive\nqualitative and quantitative evaluations to demonstrate the validity of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:00:23 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kotovenko", "Dmytro", ""], ["Sanakoyeu", "Artsiom", ""], ["Ma", "Pingchuan", ""], ["Lang", "Sabine", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2003.08413", "submitter": "Weinan Song", "authors": "Weinan Song, Yuan Liang, Jiawei Yang, Kun Wang, and Lei He", "title": "Oral-3D: Reconstructing the 3D Bone Structure of Oral Cavity from 2D\n  Panoramic X-ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Panoramic X-ray (PX) provides a 2D picture of the patient's mouth in a\npanoramic view to help dentists observe the invisible disease inside the gum.\nHowever, it provides limited 2D information compared with cone-beam computed\ntomography (CBCT), another dental imaging method that generates a 3D picture of\nthe oral cavity but with more radiation dose and a higher price. Consequently,\nit is of great interest to reconstruct the 3D structure from a 2D X-ray image,\nwhich can greatly explore the application of X-ray imaging in dental surgeries.\nIn this paper, we propose a framework, named Oral-3D, to reconstruct the 3D\noral cavity from a single PX image and prior information of the dental arch.\nSpecifically, we first train a generative model to learn the cross-dimension\ntransformation from 2D to 3D. Then we restore the shape of the oral cavity with\na deformation module with the dental arch curve, which can be obtained simply\nby taking a photo of the patient's mouth. To be noted, Oral-3D can restore both\nthe density of bony tissues and the curved mandible surface. Experimental\nresults show that Oral-3D can efficiently and effectively reconstruct the 3D\noral structure and show critical information in clinical applications, e.g.,\ntooth pulling and dental implants. To the best of our knowledge, we are the\nfirst to explore this domain transformation problem between these two imaging\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:02:57 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:37:18 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 19:18:33 GMT"}, {"version": "v4", "created": "Sat, 9 Jan 2021 00:22:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Song", "Weinan", ""], ["Liang", "Yuan", ""], ["Yang", "Jiawei", ""], ["Wang", "Kun", ""], ["He", "Lei", ""]]}, {"id": "2003.08423", "submitter": "Gabriel Bernardino", "authors": "Gabriel Bernardino, Amir Hodzic, Helene Langet, Damien LeGallois,\n  Mathieu De Craene, Miguel Angel Gonz\\'alez Ballester, Eric Saloux, Bart\n  Bijnens", "title": "Volumetric parcellation of the right ventricle for regional geometric\n  and functional assessment", "comments": null, "journal-ref": "Medical Image Analysis, Available online 6 April 2021, 102044", "doi": "10.1016/j.media.2021.102044", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  3D echocardiography is an increasingly popular tool for assessing cardiac\nremodelling in the right ventricle (RV). It allows quantification of the\ncardiac chambers without any geometric assumptions, which is the main weakness\nof 2D echocardiography. However, regional quantification of geometry and\nfunction is limited by the lower spatial and temporal resolution and the\nscarcity of identifiable anatomical landmarks. We developed a technique for\nregionally assessing the 3 relevant RV regions: apical, inlet and outflow. The\nmethod's inputs are end-diastolic (ED) and end-systolic (ES) segmented 3D\nsurface models. The method first defines a partition of the ED endocardium\nusing the geodesic distances from each surface point to apex, tricuspid valve\nand pulmonary valve: the landmarks that define the 3 regions. The ED surface\nmesh is then tetrahedralised, and the endocardial-defined partition is\ninterpolated in the blood cavity via the Laplace equation. For obtaining an ES\npartition, the endocardial partition is transported from ED to ES using a\ncommercial image-based tracking, and then interpolated towards the endocardium,\nsimilarly to ED, for computing volumes and ejection fraction (EF). We present a\nfull assessment of the method's validity and reproducibility. First, we assess\nreproducibility under segmentation variability, obtaining intra- and inter-\nobserver errors (4-10% and 10-23% resp.). Finally, we use a synthetic\nremodelling dataset to identify the situations in which our method is able to\ncorrectly determine the region that has remodelled. This dataset is generated\nby a novel mesh reconstruction method that deforms a reference mesh, locally\nimposing a given strain, expressed in anatomical coordinates. We show that the\nparcellation method is adequate for capturing local circumferential and global\ncircumferential and longitudinal RV remodelling.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:31:26 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 18:37:48 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 17:35:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bernardino", "Gabriel", ""], ["Hodzic", "Amir", ""], ["Langet", "Helene", ""], ["LeGallois", "Damien", ""], ["De Craene", "Mathieu", ""], ["Ballester", "Miguel Angel Gonz\u00e1lez", ""], ["Saloux", "Eric", ""], ["Bijnens", "Bart", ""]]}, {"id": "2003.08429", "submitter": "Ali Athar", "authors": "Ali Athar, Sabarinath Mahadevan, Aljo\\v{s}a O\\v{s}ep, Laura\n  Leal-Taix\\'e, Bastian Leibe", "title": "STEm-Seg: Spatio-temporal Embeddings for Instance Segmentation in Videos", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for instance segmentation in videos typi-cally involve\nmulti-stage pipelines that follow the tracking-by-detectionparadigm and model a\nvideo clip as a sequence of images. Multiple net-works are used to detect\nobjects in individual frames, and then associatethese detections over time.\nHence, these methods are often non-end-to-end trainable and highly tailored to\nspecific tasks. In this paper, we pro-pose a different approach that is\nwell-suited to a variety of tasks involvinginstance segmentation in videos. In\nparticular, we model a video clip asa single 3D spatio-temporal volume, and\npropose a novel approach thatsegments and tracks instances across space and\ntime in a single stage. Ourproblem formulation is centered around the idea of\nspatio-temporal em-beddings which are trained to cluster pixels belonging to a\nspecific objectinstance over an entire video clip. To this end, we introduce\n(i) novel mix-ing functions that enhance the feature representation of\nspatio-temporalembeddings, and (ii) a single-stage, proposal-free network that\ncan rea-son about temporal context. Our network is trained end-to-end to\nlearnspatio-temporal embeddings as well as parameters required to clusterthese\nembeddings, thus simplifying inference. Our method achieves state-of-the-art\nresults across multiple datasets and tasks. Code and modelsare available at\nhttps://github.com/sabarim/STEm-Seg.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:40:52 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 12:43:18 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 17:05:06 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Athar", "Ali", ""], ["Mahadevan", "Sabarinath", ""], ["O\u0161ep", "Aljo\u0161a", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Leibe", "Bastian", ""]]}, {"id": "2003.08436", "submitter": "Huan Wang", "authors": "Huan Wang, Yijun Li, Yuehai Wang, Haoji Hu, Ming-Hsuan Yang", "title": "Collaborative Distillation for Ultra-Resolution Universal Style Transfer", "comments": "Accepted by CVPR 2020, higher-resolution images than the camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal style transfer methods typically leverage rich representations from\ndeep Convolutional Neural Network (CNN) models (e.g., VGG-19) pre-trained on\nlarge collections of images. Despite the effectiveness, its application is\nheavily constrained by the large model size to handle ultra-resolution images\ngiven limited memory. In this work, we present a new knowledge distillation\nmethod (named Collaborative Distillation) for encoder-decoder based neural\nstyle transfer to reduce the convolutional filters. The main idea is\nunderpinned by a finding that the encoder-decoder pairs construct an exclusive\ncollaborative relationship, which is regarded as a new kind of knowledge for\nstyle transfer models. Moreover, to overcome the feature size mismatch when\napplying collaborative distillation, a linear embedding loss is introduced to\ndrive the student network to learn a linear embedding of the teacher's\nfeatures. Extensive experiments show the effectiveness of our method when\napplied to different universal style transfer approaches (WCT and AdaIN), even\nif the model size is reduced by 15.5 times. Especially, on WCT with the\ncompressed models, we achieve ultra-resolution (over 40 megapixels) universal\nstyle transfer on a 12GB GPU for the first time. Further experiments on\noptimization-based stylization scheme show the generality of our algorithm on\ndifferent stylization paradigms. Our code and trained models are available at\nhttps://github.com/mingsun-tse/collaborative-distillation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:59:31 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 15:09:17 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Wang", "Huan", ""], ["Li", "Yijun", ""], ["Wang", "Yuehai", ""], ["Hu", "Haoji", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "2003.08440", "submitter": "Yingda Xia", "authors": "Yingda Xia, Yi Zhang, Fengze Liu, Wei Shen, Alan Yuille", "title": "Synthesize then Compare: Detecting Failures and Anomalies for Semantic\n  Segmentation", "comments": "ECCV 2020 Oral. The first two authors contributed equally to this\n  work. Code available at https://github.com/YingdaXia/SynthCP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect failures and anomalies are fundamental requirements for\nbuilding reliable systems for computer vision applications, especially\nsafety-critical applications of semantic segmentation, such as autonomous\ndriving and medical image analysis. In this paper, we systematically study\nfailure and anomaly detection for semantic segmentation and propose a unified\nframework, consisting of two modules, to address these two related problems.\nThe first module is an image synthesis module, which generates a synthesized\nimage from a segmentation layout map, and the second is a comparison module,\nwhich computes the difference between the synthesized image and the input\nimage. We validate our framework on three challenging datasets and improve the\nstate-of-the-arts by large margins, \\emph{i.e.}, 6% AUPR-Error on Cityscapes,\n7% Pearson correlation on pancreatic tumor segmentation in MSD and 20% AUPR on\nStreetHazards anomaly segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 19:02:57 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 02:42:54 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Xia", "Yingda", ""], ["Zhang", "Yi", ""], ["Liu", "Fengze", ""], ["Shen", "Wei", ""], ["Yuille", "Alan", ""]]}, {"id": "2003.08441", "submitter": "Yingda Xia", "authors": "Yingda Xia, Qihang Yu, Wei Shen, Yuyin Zhou, Elliot K. Fishman, Alan\n  L. Yuille", "title": "Detecting Pancreatic Ductal Adenocarcinoma in Multi-phase CT Scans via\n  Alignment Ensemble", "comments": "The first two authors contributed equally to this work. Accepted to\n  MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic ductal adenocarcinoma (PDAC) is one of the most lethal cancers\namong the population. Screening for PDACs in dynamic contrast-enhanced CT is\nbeneficial for early diagnosis. In this paper, we investigate the problem of\nautomated detecting PDACs in multi-phase (arterial and venous) CT scans.\nMultiple phases provide more information than single phase, but they are\nunaligned and inhomogeneous in texture, making it difficult to combine\ncross-phase information seamlessly. We study multiple phase alignment\nstrategies, i.e., early alignment (image registration), late alignment\n(high-level feature registration), and slow alignment (multi-level feature\nregistration), and suggest an ensemble of all these alignments as a promising\nway to boost the performance of PDAC detection. We provide an extensive\nempirical evaluation on two PDAC datasets and show that the proposed alignment\nensemble significantly outperforms previous state-of-the-art approaches,\nillustrating the strong potential for clinical use.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 19:06:27 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 16:27:11 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 19:34:38 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Xia", "Yingda", ""], ["Yu", "Qihang", ""], ["Shen", "Wei", ""], ["Zhou", "Yuyin", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2003.08462", "submitter": "Jose Dolz", "authors": "Abdur R Feyjie, Reza Azad, Marco Pedersoli, Claude Kauffman, Ismail\n  Ben Ayed, Jose Dolz", "title": "Semi-supervised few-shot learning for medical image segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the great progress of deep neural networks on\nsemantic segmentation, particularly in medical imaging. Nevertheless, training\nhigh-performing models require large amounts of pixel-level ground truth masks,\nwhich can be prohibitive to obtain in the medical domain. Furthermore, training\nsuch models in a low-data regime highly increases the risk of overfitting.\nRecent attempts to alleviate the need for large annotated datasets have\ndeveloped training strategies under the few-shot learning paradigm, which\naddresses this shortcoming by learning a novel class from only a few labeled\nexamples. In this context, a segmentation model is trained on episodes, which\nrepresent different segmentation problems, each of them trained with a very\nsmall labeled dataset. In this work, we propose a novel few-shot learning\nframework for semantic segmentation, where unlabeled images are also made\navailable at each episode. To handle this new learning paradigm, we propose to\ninclude surrogate tasks that can leverage very powerful supervisory signals\n--derived from the data itself-- for semantic feature learning. We show that\nincluding unlabeled surrogate tasks in the episodic training leads to more\npowerful feature representations, which ultimately results in better\ngenerability to unseen tasks. We demonstrate the efficiency of our method in\nthe task of skin lesion segmentation in two publicly available datasets.\nFurthermore, our approach is general and model-agnostic, which can be combined\nwith different deep architectures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 20:37:18 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 01:40:33 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Feyjie", "Abdur R", ""], ["Azad", "Reza", ""], ["Pedersoli", "Marco", ""], ["Kauffman", "Claude", ""], ["Ayed", "Ismail Ben", ""], ["Dolz", "Jose", ""]]}, {"id": "2003.08469", "submitter": "Julia Moosbauer", "authors": "Abhijeet Parida, Aadhithya Sankar, Rami Eisawy, Tom Finck, Benedikt\n  Wiestler, Franz Pfister, Julia Moosbauer", "title": "Train, Learn, Expand, Repeat", "comments": "Published as a workshop paper at AI4AH, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality labeled data is essential to successfully train supervised\nmachine learning models. Although a large amount of unlabeled data is present\nin the medical domain, labeling poses a major challenge: medical professionals\nwho can expertly label the data are a scarce and expensive resource. Making\nmatters worse, voxel-wise delineation of data (e.g. for segmentation tasks) is\ntedious and suffers from high inter-rater variance, thus dramatically limiting\navailable training data. We propose a recursive training strategy to perform\nthe task of semantic segmentation given only very few training samples with\npixel-level annotations. We expand on this small training set having cheaper\nimage-level annotations using a recursive training strategy. We apply this\ntechnique on the segmentation of intracranial hemorrhage (ICH) in CT (computed\ntomography) scans of the brain, where typically few annotated data is\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 20:55:38 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 12:25:11 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Parida", "Abhijeet", ""], ["Sankar", "Aadhithya", ""], ["Eisawy", "Rami", ""], ["Finck", "Tom", ""], ["Wiestler", "Benedikt", ""], ["Pfister", "Franz", ""], ["Moosbauer", "Julia", ""]]}, {"id": "2003.08472", "submitter": "Madan Ravi Ganesh", "authors": "Madan Ravi Ganesh, Jason J. Corso, Salimeh Yasaei Sekeh", "title": "MINT: Deep Network Compression via Mutual Information-based Neuron\n  Trimming", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches to deep neural network compression via pruning either\nevaluate a filter's importance using its weights or optimize an alternative\nobjective function with sparsity constraints. While these methods offer a\nuseful way to approximate contributions from similar filters, they often either\nignore the dependency between layers or solve a more difficult optimization\nobjective than standard cross-entropy. Our method, Mutual Information-based\nNeuron Trimming (MINT), approaches deep compression via pruning by enforcing\nsparsity based on the strength of the relationship between filters of adjacent\nlayers, across every pair of layers. The relationship is calculated using\nconditional geometric mutual information which evaluates the amount of similar\ninformation exchanged between the filters using a graph-based criterion. When\npruning a network, we ensure that retained filters contribute the majority of\nthe information towards succeeding layers which ensures high performance. Our\nnovel approach outperforms existing state-of-the-art compression-via-pruning\nmethods on the standard benchmarks for this task: MNIST, CIFAR-10, and\nILSVRC2012, across a variety of network architectures. In addition, we discuss\nour observations of a common denominator between our pruning methodology's\nresponse to adversarial attacks and calibration statistics when compared to the\noriginal network.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:05:02 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ganesh", "Madan Ravi", ""], ["Corso", "Jason J.", ""], ["Sekeh", "Salimeh Yasaei", ""]]}, {"id": "2003.08476", "submitter": "Gennaro Vessio Dr.", "authors": "Giovanna Castellano and Eufemia Lella and Gennaro Vessio", "title": "Visual link retrieval and knowledge discovery in painting datasets", "comments": "Published on Multimedia Tools and Applications. Modified references.\n  Corrected typos. Added observations according to reviewers", "journal-ref": null, "doi": "10.1007/s11042-020-09995-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual arts are of inestimable importance for the cultural, historic and\neconomic growth of our society. One of the building blocks of most analysis in\nvisual arts is to find similarity relationships among paintings of different\nartists and painting schools. To help art historians better understand visual\narts, this paper presents a framework for visual link retrieval and knowledge\ndiscovery in digital painting datasets. Visual link retrieval is accomplished\nby using a deep convolutional neural network to perform feature extraction and\na fully unsupervised nearest neighbor mechanism to retrieve links among\ndigitized paintings. Historical knowledge discovery is achieved by performing a\ngraph analysis that makes it possible to study influences among artists. An\nexperimental evaluation on a database collecting paintings by very popular\nartists shows the effectiveness of the method. The unsupervised strategy makes\nthe method interesting especially in cases where metadata are scarce,\nunavailable or difficult to collect.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 21:16:33 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:31:07 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Castellano", "Giovanna", ""], ["Lella", "Eufemia", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2003.08485", "submitter": "Aniket Anand Deshmukh", "authors": "Aniket Anand Deshmukh, Abhimanu Kumar, Levi Boyles, Denis Charles,\n  Eren Manavoglu, Urun Dogan", "title": "Self-Supervised Contextual Bandits in Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual bandits are a common problem faced by machine learning\npractitioners in domains as diverse as hypothesis testing to product\nrecommendations. There have been a lot of approaches in exploiting rich data\nrepresentations for contextual bandit problems with varying degree of success.\nSelf-supervised learning is a promising approach to find rich data\nrepresentations without explicit labels. In a typical self-supervised learning\nscheme, the primary task is defined by the problem objective (e.g. clustering,\nclassification, embedding generation etc.) and the secondary task is defined by\nthe self-supervision objective (e.g. rotation prediction, words in\nneighborhood, colorization, etc.). In the usual self-supervision, we learn\nimplicit labels from the training data for a secondary task. However, in the\ncontextual bandit setting, we don't have the advantage of getting implicit\nlabels due to lack of data in the initial phase of learning. We provide a novel\napproach to tackle this issue by combining a contextual bandit objective with a\nself supervision objective. By augmenting contextual bandit learning with\nself-supervision we get a better cumulative reward. Our results on eight\npopular computer vision datasets show substantial gains in cumulative reward.\nWe provide cases where the proposed scheme doesn't perform optimally and give\nalternative methods for better learning in these cases.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 22:06:34 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Deshmukh", "Aniket Anand", ""], ["Kumar", "Abhimanu", ""], ["Boyles", "Levi", ""], ["Charles", "Denis", ""], ["Manavoglu", "Eren", ""], ["Dogan", "Urun", ""]]}, {"id": "2003.08499", "submitter": "Kaan Ak\\c{s}it", "authors": "Kaan Ak\\c{s}it, Jan Kautz, David Luebke", "title": "Gaze-Sensing LEDs for Head Mounted Displays", "comments": "14 pages, 7 figures. THIS WORK WAS CONDUCTED IN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify\ntwo off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our\nkey contribution is to exploit the sensing capability of LEDs to create\nlow-power gaze tracker for virtual reality (VR) applications. This yields a\nsimple approach using minimal hardware to achieve good accuracy and low latency\nusing light-weight supervised Gaussian Process Regression (GPR) running on a\nmobile device. With our hardware, we show that Minkowski distance measure based\nGPR implementation outperforms the commonly used radial basis function-based\nsupport vector regression (SVR) without the need to precisely determine free\nparameters. We show that our gaze estimation method does not require complex\ndimension reduction techniques, feature extraction, or distortion corrections\ndue to off-axis optical paths. We demonstrate two complete HMD prototypes with\na sample eye-tracked application, and report on a series of subjective tests\nusing our prototypes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 23:03:06 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ak\u015fit", "Kaan", ""], ["Kautz", "Jan", ""], ["Luebke", "David", ""]]}, {"id": "2003.08502", "submitter": "Xingtong Liu", "authors": "Xingtong Liu, Maia Stiber, Jindan Huang, Masaru Ishii, Gregory D.\n  Hager, Russell H. Taylor, Mathias Unberath", "title": "Reconstructing Sinus Anatomy from Endoscopic Video -- Towards a\n  Radiation-free Approach for Quantitative Longitudinal Assessment", "comments": "Accepted to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reconstructing accurate 3D surface models of sinus anatomy directly from an\nendoscopic video is a promising avenue for cross-sectional and longitudinal\nanalysis to better understand the relationship between sinus anatomy and\nsurgical outcomes. We present a patient-specific, learning-based method for 3D\nreconstruction of sinus surface anatomy directly and only from endoscopic\nvideos. We demonstrate the effectiveness and accuracy of our method on in and\nex vivo data where we compare to sparse reconstructions from Structure from\nMotion, dense reconstruction from COLMAP, and ground truth anatomy from CT. Our\ntextured reconstructions are watertight and enable measurement of clinically\nrelevant parameters in good agreement with CT. The source code is available at\nhttps://github.com/lppllppl920/DenseReconstruction-Pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 23:11:10 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 03:34:07 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Liu", "Xingtong", ""], ["Stiber", "Maia", ""], ["Huang", "Jindan", ""], ["Ishii", "Masaru", ""], ["Hager", "Gregory D.", ""], ["Taylor", "Russell H.", ""], ["Unberath", "Mathias", ""]]}, {"id": "2003.08505", "submitter": "Kevin Musgrave", "authors": "Kevin Musgrave, Serge Belongie, Ser-Nam Lim", "title": "A Metric Learning Reality Check", "comments": "Visit https://www.github.com/KevinMusgrave/powerful-benchmarker for\n  supplementary material, including the source code, configuration files, log\n  files, and interactive bayesian optimization plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep metric learning papers from the past four years have consistently\nclaimed great advances in accuracy, often more than doubling the performance of\ndecade-old methods. In this paper, we take a closer look at the field to see if\nthis is actually true. We find flaws in the experimental methodology of\nnumerous metric learning papers, and show that the actual improvements over\ntime have been marginal at best.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 23:28:04 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 01:19:10 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 01:18:33 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Musgrave", "Kevin", ""], ["Belongie", "Serge", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2003.08514", "submitter": "Debashis Sen", "authors": "G\\\"okhan Yildirim, Debashis Sen, Mohan Kankanhalli, and Sabine\n  S\\\"usstrunk", "title": "Evaluating Salient Object Detection in Natural Images with Multiple\n  Objects having Multi-level Saliency", "comments": "Accepted Article", "journal-ref": "IET Image Processing, 2019", "doi": "10.1049/iet-ipr.2019.0787", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Salient object detection is evaluated using binary ground truth with the\nlabels being salient object class and background. In this paper, we corroborate\nbased on three subjective experiments on a novel image dataset that objects in\nnatural images are inherently perceived to have varying levels of importance.\nOur dataset, named SalMoN (saliency in multi-object natural images), has 588\nimages containing multiple objects. The subjective experiments performed record\nspontaneous attention and perception through eye fixation duration, point\nclicking and rectangle drawing. As object saliency in a multi-object image is\ninherently multi-level, we propose that salient object detection must be\nevaluated for the capability to detect all multi-level salient objects apart\nfrom the salient object class detection capability. For this purpose, we\ngenerate multi-level maps as ground truth corresponding to all the dataset\nimages using the results of the subjective experiments, with the labels being\nmulti-level salient objects and background. We then propose the use of mean\nabsolute error, Kendall's rank correlation and average area under\nprecision-recall curve to evaluate existing salient object detection methods on\nour multi-level saliency ground truth dataset. Approaches that represent\nsaliency detection on images as local-global hierarchical processing of a graph\nperform well in our dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:06:40 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Yildirim", "G\u00f6khan", ""], ["Sen", "Debashis", ""], ["Kankanhalli", "Mohan", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "2003.08515", "submitter": "Fanbo Xiang", "authors": "Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu,\n  Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, Li Yi, Angel X. Chang,\n  Leonidas J. Guibas, Hao Su", "title": "SAPIEN: A SimulAted Part-based Interactive ENvironment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building home assistant robots has long been a pursuit for vision and\nrobotics researchers. To achieve this task, a simulated environment with\nphysically realistic simulation, sufficient articulated objects, and\ntransferability to the real robot is indispensable. Existing environments\nachieve these requirements for robotics simulation with different levels of\nsimplification and focus. We take one step further in constructing an\nenvironment that supports household tasks for training robot learning\nalgorithm. Our work, SAPIEN, is a realistic and physics-rich simulated\nenvironment that hosts a large-scale set for articulated objects. Our SAPIEN\nenables various robotic vision and interaction tasks that require detailed\npart-level understanding.We evaluate state-of-the-art vision algorithms for\npart detection and motion attribute recognition as well as demonstrate robotic\ninteraction tasks using heuristic approaches and reinforcement learning\nalgorithms. We hope that our SAPIEN can open a lot of research directions yet\nto be explored, including learning cognition through interaction, part motion\ndiscovery, and construction of robotics-ready simulated game environment.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:11:34 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Xiang", "Fanbo", ""], ["Qin", "Yuzhe", ""], ["Mo", "Kaichun", ""], ["Xia", "Yikuan", ""], ["Zhu", "Hao", ""], ["Liu", "Fangchen", ""], ["Liu", "Minghua", ""], ["Jiang", "Hanxiao", ""], ["Yuan", "Yifu", ""], ["Wang", "He", ""], ["Yi", "Li", ""], ["Chang", "Angel X.", ""], ["Guibas", "Leonidas J.", ""], ["Su", "Hao", ""]]}, {"id": "2003.08520", "submitter": "Minho Hwang", "authors": "Minho Hwang, Brijen Thananjeyan, Samuel Paradis, Daniel Seita, Jeffrey\n  Ichnowski, Danyal Fer, Thomas Low, and Ken Goldberg", "title": "Efficiently Calibrating Cable-Driven Surgical Robots with RGBD Fiducial\n  Sensing and Recurrent Neural Networks", "comments": "8 pages, 11 figures, 3 tables", "journal-ref": "IEEE Robotics and Automation Letters, 5 (2020) 5937-5944", "doi": "10.1109/LRA.2020.3010746", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of surgical subtasks using cable-driven robotic surgical\nassistants (RSAs) such as Intuitive Surgical's da Vinci Research Kit (dVRK) is\nchallenging due to imprecision in control from cable-related effects such as\ncable stretching and hysteresis. We propose a novel approach to efficiently\ncalibrate such robots by placing a 3D printed fiducial coordinate frames on the\narm and end-effector that is tracked using RGBD sensing. To measure the\ncoupling and history-dependent effects between joints, we analyze data from\nsampled trajectories and consider 13 approaches to modeling. These models\ninclude linear regression and LSTM recurrent neural networks, each with varying\ntemporal window length to provide compensatory feedback. With the proposed\nmethod, data collection of 1800 samples takes 31 minutes and model training\ntakes under 1 minute. Results on a test set of reference trajectories suggest\nthat the trained model can reduce the mean tracking error of the physical robot\nfrom 2.96 mm to 0.65 mm. Results on the execution of open-loop trajectories of\nthe FLS peg transfer surgeon training task suggest that the best model\nincreases success rate from 39.4 % to 96.7 %, producing performance comparable\nto that of an expert surgical resident. Supplementary materials, including code\nand 3D-printable models, are available at\nhttps://sites.google.com/berkeley.edu/surgical-calibration\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:24:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 03:26:16 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 00:40:50 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 23:45:22 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Hwang", "Minho", ""], ["Thananjeyan", "Brijen", ""], ["Paradis", "Samuel", ""], ["Seita", "Daniel", ""], ["Ichnowski", "Jeffrey", ""], ["Fer", "Danyal", ""], ["Low", "Thomas", ""], ["Goldberg", "Ken", ""]]}, {"id": "2003.08525", "submitter": "Shengjun Zhang", "authors": "Zelin Deng, Xiaolong Yan, Shengjun Zhang, Colleen P. Bailey", "title": "Extremal Region Analysis based Deep Learning Framework for Detecting\n  Defects", "comments": "Unsatisfied with results", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximally stable extreme region (MSER) analysis based convolutional neural\nnetwork (CNN) for unified defect detection framework is proposed in this paper.\nOur proposed framework utilizes the generality and stability of MSER to\ngenerate the desired defect candidates. Then a specific trained binary CNN\nclassifier is adopted over the defect candidates to produce the final defect\nset. Defect datasets over different categories \\blue{are used} in the\nexperiments. More generally, the parameter settings in MSER can be adjusted to\nsatisfy different requirements in various industries (high precision, high\nrecall, etc). Extensive experimental results have shown the efficacy of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:35:10 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 01:32:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Deng", "Zelin", ""], ["Yan", "Xiaolong", ""], ["Zhang", "Shengjun", ""], ["Bailey", "Colleen P.", ""]]}, {"id": "2003.08526", "submitter": "Yunhao Ge", "authors": "Yunhao Ge, Jiaping Zhao and Laurent Itti", "title": "Pose Augmentation: Class-agnostic Object Pose Transformation for Object\n  Recognition", "comments": "ECCV 2020, with supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object pose increases intraclass object variance which makes object\nrecognition from 2D images harder. To render a classifier robust to pose\nvariations, most deep neural networks try to eliminate the influence of pose by\nusing large datasets with many poses for each class. Here, we propose a\ndifferent approach: a class-agnostic object pose transformation network\n(OPT-Net) can transform an image along 3D yaw and pitch axes to synthesize\nadditional poses continuously. Synthesized images lead to better training of an\nobject classifier. We design a novel eliminate-add structure to explicitly\ndisentangle pose from object identity: first eliminate pose information of the\ninput image and then add target pose information (regularized as continuous\nvariables) to synthesize any target pose. We trained OPT-Net on images of toy\nvehicles shot on a turntable from the iLab-20M dataset. After training on\nunbalanced discrete poses (5 classes with 6 poses per object instance, plus 5\nclasses with only 2 poses), we show that OPT-Net can synthesize balanced\ncontinuous new poses along yaw and pitch axes with high quality. Training a\nResNet-18 classifier with original plus synthesized poses improves mAP accuracy\nby 9% overtraining on original poses only. Further, the pre-trained OPT-Net can\ngeneralize to new object classes, which we demonstrate on both iLab-20M and\nRGB-D. We also show that the learned features can generalize to ImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 00:39:37 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 05:19:29 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 03:29:28 GMT"}, {"version": "v4", "created": "Thu, 14 Jan 2021 02:03:09 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ge", "Yunhao", ""], ["Zhao", "Jiaping", ""], ["Itti", "Laurent", ""]]}, {"id": "2003.08539", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang, Yun Gu, Xiaolin Huang, Enmei Tu and Jie Yang", "title": "Stereo Endoscopic Image Super-Resolution Using Disparity-Constrained\n  Parallel Attention", "comments": "6 pages, 4 figures, accepted as a workshop paper at AI4AH, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of stereo cameras in computer assisted surgery\ntechniques, a second viewpoint would provide additional information in surgery.\nHowever, how to effectively access and use stereo information for the\nsuper-resolution (SR) purpose is often a challenge. In this paper, we propose a\ndisparity-constrained stereo super-resolution network (DCSSRnet) to\nsimultaneously compute a super-resolved image in a stereo image pair. In\nparticular, we incorporate a disparity-based constraint mechanism into the\ngeneration of SR images in a deep neural network framework with an additional\natrous parallax-attention modules. Experiment results on laparoscopic images\ndemonstrate that the proposed framework outperforms current SR methods on both\nquantitative and qualitative evaluations. Our DCSSRnet provides a promising\nsolution on enhancing spatial resolution of stereo image pairs, which will be\nextremely beneficial for the endoscopic surgery.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 02:12:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhang", "Tianyi", ""], ["Gu", "Yun", ""], ["Huang", "Xiaolin", ""], ["Tu", "Enmei", ""], ["Yang", "Jie", ""]]}, {"id": "2003.08550", "submitter": "Xiaozhou Ren", "authors": "Zhuoping Yu, Xiaozhou Ren, Yuyao Huang, Wei Tian, Junqiao Zhao", "title": "Detecting Lane and Road Markings at A Distance with Perspective\n  Transformer Layers", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of lane and road markings is a task of great importance\nfor intelligent vehicles. In existing approaches, the detection accuracy often\ndegrades with the increasing distance. This is due to the fact that distant\nlane and road markings occupy a small number of pixels in the image, and scales\nof lane and road markings are inconsistent at various distances and\nperspectives. The Inverse Perspective Mapping (IPM) can be used to eliminate\nthe perspective distortion, but the inherent interpolation can lead to\nartifacts especially around distant lane and road markings and thus has a\nnegative impact on the accuracy of lane marking detection and segmentation. To\nsolve this problem, we adopt the Encoder-Decoder architecture in Fully\nConvolutional Networks and leverage the idea of Spatial Transformer Networks to\nintroduce a novel semantic segmentation neural network. This approach\ndecomposes the IPM process into multiple consecutive differentiable homographic\ntransform layers, which are called \"Perspective Transformer Layers\".\nFurthermore, the interpolated feature map is refined by subsequent\nconvolutional layers thus reducing the artifacts and improving the accuracy.\nThe effectiveness of the proposed method in lane marking detection is validated\non two public datasets: TuSimple and ApolloScape\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 03:22:52 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 06:38:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yu", "Zhuoping", ""], ["Ren", "Xiaozhou", ""], ["Huang", "Yuyao", ""], ["Tian", "Wei", ""], ["Zhao", "Junqiao", ""]]}, {"id": "2003.08556", "submitter": "Donghuan Lu", "authors": "Donghuan Lu, Sujun Zhao, Peng Xie, Kai Ma, Lijuan Liu, Yefeng Zheng", "title": "Quality Control of Neuron Reconstruction Based on Deep Learning", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuron reconstruction is essential to generate exquisite neuron connectivity\nmap for understanding brain function. Despite the significant amount of effect\nthat has been made on automatic reconstruction methods, manual tracing by\nwell-trained human annotators is still necessary. To ensure the quality of\nreconstructed neurons and provide guidance for annotators to improve their\nefficiency, we propose a deep learning based quality control method for neuron\nreconstruction in this paper. By formulating the quality control problem into a\nbinary classification task regarding each single point, the proposed approach\novercomes the technical difficulties resulting from the large image size and\ncomplex neuron morphology. Not only it provides the evaluation of\nreconstruction quality, but also can locate exactly where the wrong tracing\nbegins. This work presents one of the first comprehensive studies for\nwhole-brain scale quality control of neuron reconstructions. Experiments on\nfive-fold cross validation with a large dataset demonstrate that the proposed\napproach can detect 74.7% errors with only 1.4% false alerts.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 03:44:29 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Lu", "Donghuan", ""], ["Zhao", "Sujun", ""], ["Xie", "Peng", ""], ["Ma", "Kai", ""], ["Liu", "Lijuan", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2003.08559", "submitter": "Wenjin Wang", "authors": "Wenjin Wang, Yunqing Hu, Yin Zhang", "title": "Lifelong Learning with Searchable Extension Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifelong learning remains an open problem. One of its main difficulties is\ncatastrophic forgetting. Many dynamic expansion approaches have been proposed\nto address this problem, but they all use homogeneous models of predefined\nstructure for all tasks. The common original model and expansion structures\nignore the requirement of different model structures on different tasks, which\nleads to a less compact model for multiple tasks and causes the model size to\nincrease rapidly as the number of tasks increases. Moreover, they can not\nperform best on all tasks. To solve those problems, in this paper, we propose a\nnew lifelong learning framework named Searchable Extension Units (SEU) by\nintroducing Neural Architecture Search into lifelong learning, which breaks\ndown the need for a predefined original model and searches for specific\nextension units for different tasks, without compromising the performance of\nthe model on different tasks. Our approach can obtain a much more compact model\nwithout catastrophic forgetting. The experimental results on the PMNIST, the\nsplit CIFAR10 dataset, the split CIFAR100 dataset, and the Mixture dataset\nempirically prove that our method can achieve higher accuracy with much smaller\nmodel, whose size is about 25-33 percentage of that of the state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 03:45:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wang", "Wenjin", ""], ["Hu", "Yunqing", ""], ["Zhang", "Yin", ""]]}, {"id": "2003.08560", "submitter": "Xingjian Zhen", "authors": "Han Yang, Xingjian Zhen, Ying Chi, Lei Zhang, and Xian-Sheng Hua", "title": "CPR-GCN: Conditional Partial-Residual Graph Convolutional Network in\n  Automated Anatomical Labeling of Coronary Arteries", "comments": "This work is done by Xingjian Zhen during internship in Alibaba Damo\n  Academy", "journal-ref": "CVPR 2020 oral", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated anatomical labeling plays a vital role in coronary artery disease\ndiagnosing procedure. The main challenge in this problem is the large\nindividual variability inherited in human anatomy. Existing methods usually\nrely on the position information and the prior knowledge of the topology of the\ncoronary artery tree, which may lead to unsatisfactory performance when the\nmain branches are confusing. Motivated by the wide application of the graph\nneural network in structured data, in this paper, we propose a conditional\npartial-residual graph convolutional network (CPR-GCN), which takes both\nposition and CT image into consideration, since CT image contains abundant\ninformation such as branch size and spanning direction. Two majority parts, a\nPartial-Residual GCN and a conditions extractor, are included in CPR-GCN. The\nconditions extractor is a hybrid model containing the 3D CNN and the LSTM,\nwhich can extract 3D spatial image features along the branches. On the\ntechnical side, the Partial-Residual GCN takes the position features of the\nbranches, with the 3D spatial image features as conditions, to predict the\nlabel for each branches. While on the mathematical side, our approach twists\nthe partial differential equation (PDE) into the graph modeling. A dataset with\n511 subjects is collected from the clinic and annotated by two experts with a\ntwo-phase annotation process. According to the five-fold cross-validation, our\nCPR-GCN yields 95.8% meanRecall, 95.4% meanPrecision and 0.955 meanF1, which\noutperforms state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 04:02:12 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 03:25:41 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 02:36:04 GMT"}, {"version": "v4", "created": "Sat, 18 Apr 2020 03:07:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Yang", "Han", ""], ["Zhen", "Xingjian", ""], ["Chi", "Ying", ""], ["Zhang", "Lei", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2003.08562", "submitter": "Daiki Hirata", "authors": "Daiki Hirata, Norikazu Takahashi", "title": "Ensemble learning in CNN augmented with fully connected subnetworks", "comments": "6 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have shown remarkable performance in\ngeneral object recognition tasks. In this paper, we propose a new model called\nEnsNet which is composed of one base CNN and multiple Fully Connected\nSubNetworks (FCSNs). In this model, the set of feature-maps generated by the\nlast convolutional layer in the base CNN is divided along channels into\ndisjoint subsets, and these subsets are assigned to the FCSNs. Each of the\nFCSNs is trained independent of others so that it can predict the class label\nfrom the subset of the feature-maps assigned to it. The output of the overall\nmodel is determined by majority vote of the base CNN and the FCSNs.\nExperimental results using the MNIST, Fashion-MNIST and CIFAR-10 datasets show\nthat the proposed approach further improves the performance of CNNs. In\nparticular, an EnsNet achieves a state-of-the-art error rate of 0.16% on MNIST.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 04:02:49 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 00:01:00 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 07:03:45 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Hirata", "Daiki", ""], ["Takahashi", "Norikazu", ""]]}, {"id": "2003.08583", "submitter": "Shubham Agrawal", "authors": "Shubham Agrawal, Anuj Pahuja, Simon Lucey", "title": "High Accuracy Face Geometry Capture using a Smartphone Video", "comments": "Presented at The IEEE Winter Conference on Applications of Computer\n  Vision (WACV), 2020, pp. 81-90", "journal-ref": "The IEEE Winter Conference on Applications of Computer Vision\n  (WACV), 2020, pp. 81-90", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What's the most accurate 3D model of your face you can obtain while sitting\nat your desk? We attempt to answer this question in our work. High fidelity\nface reconstructions have so far been limited to either studio settings or\nthrough expensive 3D scanners. On the other hand, unconstrained reconstruction\nmethods are typically limited by low-capacity models. Our method reconstructs\naccurate face geometry of a subject using a video shot from a smartphone in an\nunconstrained environment. Our approach takes advantage of recent advances in\nvisual SLAM, keypoint detection, and object detection to improve accuracy and\nrobustness. By not being constrained to a model subspace, our reconstructed\nmeshes capture important details while being robust to noise and being\ntopologically consistent. Our evaluations show that our method outperforms\ncurrent single and multi-view baselines by a significant margin, both in terms\nof geometric accuracy and in capturing person-specific details important for\nmaking realistic looking models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 05:46:58 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Agrawal", "Shubham", ""], ["Pahuja", "Anuj", ""], ["Lucey", "Simon", ""]]}, {"id": "2003.08593", "submitter": "Yueqi Duan", "authors": "Yueqi Duan, Haidong Zhu, He Wang, Li Yi, Ram Nevatia, Leonidas J.\n  Guibas", "title": "Curriculum DeepSDF", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When learning to sketch, beginners start with simple and flexible shapes, and\nthen gradually strive for more complex and accurate ones in the subsequent\ntraining sessions. In this paper, we design a \"shape curriculum\" for learning\ncontinuous Signed Distance Function (SDF) on shapes, namely Curriculum DeepSDF.\nInspired by how humans learn, Curriculum DeepSDF organizes the learning task in\nascending order of difficulty according to the following two criteria: surface\naccuracy and sample difficulty. The former considers stringency in supervising\nwith ground truth, while the latter regards the weights of hard training\nsamples near complex geometry and fine structure. More specifically, Curriculum\nDeepSDF learns to reconstruct coarse shapes at first, and then gradually\nincreases the accuracy and focuses more on complex local details. Experimental\nresults show that a carefully-designed curriculum leads to significantly better\nshape reconstructions with the same training data, training epochs and network\narchitecture as DeepSDF. We believe that the application of shape curricula can\nbenefit the training process of a wide variety of 3D shape representation\nlearning methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 06:40:59 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 19:23:49 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 21:18:48 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Duan", "Yueqi", ""], ["Zhu", "Haidong", ""], ["Wang", "He", ""], ["Yi", "Li", ""], ["Nevatia", "Ram", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2003.08597", "submitter": "Gennaro Vessio Dr.", "authors": "Giovanna Castellano and Gennaro Vessio", "title": "Deep convolutional embedding for digitized painting clustering", "comments": "Accepted at ICPR2020. Added references. Corrected typos. Added new\n  results and observations according to reviewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering artworks is difficult for several reasons. On the one hand,\nrecognizing meaningful patterns in accordance with domain knowledge and visual\nperception is extremely difficult. On the other hand, applying traditional\nclustering and feature reduction techniques to the highly dimensional pixel\nspace can be ineffective. To address these issues, we propose to use a deep\nconvolutional embedding model for digitized painting clustering, in which the\ntask of mapping the raw input data to an abstract, latent space is jointly\noptimized with the task of finding a set of cluster centroids in this latent\nfeature space. Quantitative and qualitative experimental results show the\neffectiveness of the proposed method. The model is also capable of\noutperforming other state-of-the-art deep clustering approaches to the same\nproblem. The proposed method can be useful for several art-related tasks, in\nparticular visual link retrieval and historical knowledge discovery in painting\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 06:49:38 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 15:21:49 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Castellano", "Giovanna", ""], ["Vessio", "Gennaro", ""]]}, {"id": "2003.08603", "submitter": "Deepak Singla", "authors": "Deepak Singla, Soham Chatterjee, Lavanya Ramapantulu, Andres Ussa,\n  Bharath Ramesh and Arindam Basu", "title": "HyNNA: Improved Performance for Neuromorphic Vision Sensor based\n  Surveillance using Hybrid Neural Network Architecture", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in the Internet of Video Things (IoVT) domain have very tight\nconstraints with respect to power and area. While neuromorphic vision sensors\n(NVS) may offer advantages over traditional imagers in this domain, the\nexisting NVS systems either do not meet the power constraints or have not\ndemonstrated end-to-end system performance. To address this, we improve on a\nrecently proposed hybrid event-frame approach by using morphological image\nprocessing algorithms for region proposal and address the low-power requirement\nfor object detection and classification by exploring various convolutional\nneural network (CNN) architectures. Specifically, we compare the results\nobtained from our object detection framework against the state-of-the-art\nlow-power NVS surveillance system and show an improved accuracy of 82.16% from\n63.1%. Moreover, we show that using multiple bits does not improve accuracy,\nand thus, system designers can save power and area by using only single bit\nevent polarity information. In addition, we explore the CNN architecture space\nfor object classification and show useful insights to trade-off accuracy for\nlower power using lesser memory and arithmetic operations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:18:33 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Singla", "Deepak", ""], ["Chatterjee", "Soham", ""], ["Ramapantulu", "Lavanya", ""], ["Ussa", "Andres", ""], ["Ramesh", "Bharath", ""], ["Basu", "Arindam", ""]]}, {"id": "2003.08605", "submitter": "Kudaibergen Urinbayev", "authors": "Kudaibergen Urinbayev, Yerassyl Orazbek, Yernur Nurambek, Almas\n  Mirzakhmetov, and Huseyin Atakan Varol", "title": "End-to-End Deep Diagnosis of X-ray Images", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an end-to-end deep learning framework for X-ray\nimage diagnosis. As the first step, our system determines whether a submitted\nimage is an X-ray or not. After it classifies the type of the X-ray, it runs\nthe dedicated abnormality classification network. In this work, we only focus\non the chest X-rays for abnormality classification. However, the system can be\nextended to other X-ray types easily. Our deep learning classifiers are based\non DenseNet-121 architecture. The test set accuracy obtained for 'X-ray or\nNot', 'X-ray Type Classification', and 'Chest Abnormality Classification' tasks\nare 0.987, 0.976, and 0.947, respectively, resulting into an end-to-end\naccuracy of 0.91. For achieving better results than the state-of-the-art in the\n'Chest Abnormality Classification', we utilize the new RAdam optimizer. We also\nuse Gradient-weighted Class Activation Mapping for visual explanation of the\nresults. Our results show the feasibility of a generalized online projectional\nradiography diagnosis system.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:20:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Urinbayev", "Kudaibergen", ""], ["Orazbek", "Yerassyl", ""], ["Nurambek", "Yernur", ""], ["Mirzakhmetov", "Almas", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2003.08607", "submitter": "Hui Tang", "authors": "Hui Tang, Ke Chen, and Kui Jia", "title": "Unsupervised Domain Adaptation via Structurally Regularized Deep\n  Clustering", "comments": "14 pages, 5 figures, 8 tables, accepted by CVPR2020 - Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) is to make predictions for unlabeled\ndata on a target domain, given labeled data on a source domain whose\ndistribution shifts from the target one. Mainstream UDA methods learn aligned\nfeatures between the two domains, such that a classifier trained on the source\nfeatures can be readily applied to the target ones. However, such a\ntransferring strategy has a potential risk of damaging the intrinsic\ndiscrimination of target data. To alleviate this risk, we are motivated by the\nassumption of structural domain similarity, and propose to directly uncover the\nintrinsic target discrimination via discriminative clustering of target data.\nWe constrain the clustering solutions using structural source regularization\nthat hinges on our assumed structural domain similarity. Technically, we use a\nflexible framework of deep network based discriminative clustering that\nminimizes the KL divergence between predictive label distribution of the\nnetwork and an introduced auxiliary one; replacing the auxiliary distribution\nwith that formed by ground-truth labels of source data implements the\nstructural source regularization via a simple strategy of joint network\ntraining. We term our proposed method as Structurally Regularized Deep\nClustering (SRDC), where we also enhance target discrimination with clustering\nof intermediate network features, and enhance structural regularization with\nsoft selection of less divergent source examples. Careful ablation studies show\nthe efficacy of our proposed SRDC. Notably, with no explicit domain alignment,\nSRDC outperforms all existing methods on three UDA benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:26:41 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Tang", "Hui", ""], ["Chen", "Ke", ""], ["Jia", "Kui", ""]]}, {"id": "2003.08608", "submitter": "Zuyao Chen", "authors": "Zuyao Chen, Runmin Cong, Qianqian Xu, and Qingming Huang", "title": "DPANet: Depth Potentiality-Aware Gated Attention Network for RGB-D\n  Salient Object Detection", "comments": "Accepted by IEEE Transactions on Image Processing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main issues in RGB-D salient object detection: (1) how to\neffectively integrate the complementarity from the cross-modal RGB-D data; (2)\nhow to prevent the contamination effect from the unreliable depth map. In fact,\nthese two problems are linked and intertwined, but the previous methods tend to\nfocus only on the first problem and ignore the consideration of depth map\nquality, which may yield the model fall into the sub-optimal state. In this\npaper, we address these two issues in a holistic model synergistically, and\npropose a novel network named DPANet to explicitly model the potentiality of\nthe depth map and effectively integrate the cross-modal complementarity. By\nintroducing the depth potentiality perception, the network can perceive the\npotentiality of depth information in a learning-based manner, and guide the\nfusion process of two modal data to prevent the contamination occurred. The\ngated multi-modality attention module in the fusion process exploits the\nattention mechanism with a gate controller to capture long-range dependencies\nfrom a cross-modal perspective. Experimental results compared with 15\nstate-of-the-art methods on 8 datasets demonstrate the validity of the proposed\napproach both quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:27:54 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 15:08:35 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 02:58:33 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 02:34:17 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Chen", "Zuyao", ""], ["Cong", "Runmin", ""], ["Xu", "Qianqian", ""], ["Huang", "Qingming", ""]]}, {"id": "2003.08609", "submitter": "Yonghui Zhang", "authors": "Yonghui Zhang (1-4), Ke Gu (1-4) ((1) Engineering Research Center of\n  Intelligent Perception and Autonomous Control, AMinistry of Education, (2)\n  Beijing Key Laboratory of Computational Intelligence and Intelligent System,\n  (3) Beijing Key Laboratory of Computational Intelligence and Intelligent\n  System, (4) Faculty of Information Technology, Beijing University of\n  Technology, China)", "title": "AQPDBJUT Dataset: Picture-Based PM Monitoring in the Campus of BJUT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring the students in good physical levels is imperative for their future\nhealth. In recent years, the continually growing concentration of Particulate\nMatter (PM) has done increasingly serious harm to student health. Hence, it is\nhighly required to prevent and control PM concentrations in the campus. As the\nsource of PM prevention and control, developing a good model for PM monitoring\nis extremely urgent and has posed a big challenge. It has been found in prior\nworks that photobased methods are available for PM monitoring. To verify the\neffectiveness of existing PM monitoring methods in the campus, we establish a\nnew dataset which includes 1,500 photos collected in the Beijing University of\nTechnology. Experiments show that stated-of-the-art methods are far from ideal\nfor PM monitoring in the campus.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 07:28:13 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 15:28:56 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhang", "Yonghui", "", "1-4"], ["Gu", "Ke", "", "1-4"]]}, {"id": "2003.08624", "submitter": "Kaichun Mo", "authors": "Kaichun Mo, He Wang, Xinchen Yan, Leonidas J. Guibas", "title": "PT2PC: Learning to Generate 3D Point Cloud Shapes from Part Tree\n  Conditions", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D generative shape modeling is a fundamental research area in computer\nvision and interactive computer graphics, with many real-world applications.\nThis paper investigates the novel problem of generating 3D shape point cloud\ngeometry from a symbolic part tree representation. In order to learn such a\nconditional shape generation procedure in an end-to-end fashion, we propose a\nconditional GAN \"part tree\"-to-\"point cloud\" model (PT2PC) that disentangles\nthe structural and geometric factors. The proposed model incorporates the part\ntree condition into the architecture design by passing messages top-down and\nbottom-up along the part tree hierarchy. Experimental results and user study\ndemonstrate the strengths of our method in generating perceptually plausible\nand diverse 3D point clouds, given the part tree condition. We also propose a\nnovel structural measure for evaluating if the generated shape point clouds\nsatisfy the part tree conditions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:27:25 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 01:28:18 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Mo", "Kaichun", ""], ["Wang", "He", ""], ["Yan", "Xinchen", ""], ["Guibas", "Leonidas J.", ""]]}, {"id": "2003.08626", "submitter": "Mingyu Ding", "authors": "An Zhao, Mingyu Ding, Zhiwu Lu, Tao Xiang, Yulei Niu, Jiechao Guan,\n  Ji-Rong Wen, Ping Luo", "title": "Domain-Adaptive Few-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing few-shot learning (FSL) methods make the implicit assumption that\nthe few target class samples are from the same domain as the source class\nsamples. However, in practice this assumption is often invalid -- the target\nclasses could come from a different domain. This poses an additional challenge\nof domain adaptation (DA) with few training samples. In this paper, the problem\nof domain-adaptive few-shot learning (DA-FSL) is tackled, which requires\nsolving FSL and DA in a unified framework. To this end, we propose a novel\ndomain-adversarial prototypical network (DAPN) model. It is designed to address\na specific challenge in DA-FSL: the DA objective means that the source and\ntarget data distributions need to be aligned, typically through a shared\ndomain-adaptive feature embedding space; but the FSL objective dictates that\nthe target domain per class distribution must be different from that of any\nsource domain class, meaning aligning the distributions across domains may harm\nthe FSL performance. How to achieve global domain distribution alignment whilst\nmaintaining source/target per-class discriminativeness thus becomes the key.\nOur solution is to explicitly enhance the source/target per-class separation\nbefore domain-adaptive feature embedding learning in the DAPN, in order to\nalleviate the negative effect of domain alignment on FSL. Extensive experiments\nshow that our DAPN outperforms the state-of-the-art FSL and DA models, as well\nas their na\\\"ive combinations. The code is available at\nhttps://github.com/dingmyu/DAPN.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:31:14 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Zhao", "An", ""], ["Ding", "Mingyu", ""], ["Lu", "Zhiwu", ""], ["Xiang", "Tao", ""], ["Niu", "Yulei", ""], ["Guan", "Jiechao", ""], ["Wen", "Ji-Rong", ""], ["Luo", "Ping", ""]]}, {"id": "2003.08628", "submitter": "Xialin Li", "authors": "Xialin Li, Chen Li and Wenwei Zhao", "title": "Foldover Features for Dynamic Object Behavior Description in Microscopic\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior description is conducive to the analysis of tiny objects, similar\nobjects, objects with weak visual information and objects with similar visual\ninformation, playing a fundamental role in the identification and\nclassification of dynamic objects in microscopic videos. To this end, we\npropose foldover features to describe the behavior of dynamic objects. First,\nwe generate foldover for each object in microscopic videos in X, Y and Z\ndirections, respectively. Then, we extract foldover features from the X, Y and\nZ directions with statistical methods, respectively. Finally, we use four\ndifferent classifiers to test the effectiveness of the proposed foldover\nfeatures. In the experiment, we use a sperm microscopic video dataset to\nevaluate the proposed foldover features, including three types of 1374 sperms,\nand obtain the highest classification accuracy of 96.5%.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:39:39 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 02:49:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Xialin", ""], ["Li", "Chen", ""], ["Zhao", "Wenwei", ""]]}, {"id": "2003.08632", "submitter": "Berat Kurar Barakat", "authors": "Berat Kurar Barakat, Ahmad Droby, Rym Alasam, Boraq Madi, Irina\n  Rabaev, Raed Shammes and Jihad El-Sana", "title": "Unsupervised deep learning for text line segmentation", "comments": "An unsupervised CNN for text line segmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised deep learning method for text line segmentation\nthat is inspired by the relative variance between text lines and spaces among\ntext lines. Handwritten text line segmentation is important for the efficiency\nof further processing. A common method is to train a deep learning network for\nembedding the document image into an image of blob lines that are tracing the\ntext lines. Previous methods learned such embedding in a supervised manner,\nrequiring the annotation of many document images. This paper presents an\nunsupervised embedding of document image patches without a need for\nannotations. The number of foreground pixels over the text lines is relatively\ndifferent from the number of foreground pixels over the spaces among text\nlines. Generating similar and different pairs relying on this principle\ndefinitely leads to outliers. However, as the results show, the outliers do not\nharm the convergence and the network learns to discriminate the text lines from\nthe spaces between text lines. Remarkably, with a challenging Arabic\nhandwritten text line segmentation dataset, VML-AHTE, we achieved superior\nperformance over the supervised methods. Additionally, the proposed method was\nevaluated on the ICDAR 2017 and ICFHR 2010 handwritten text line segmentation\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:57:53 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:11:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Barakat", "Berat Kurar", ""], ["Droby", "Ahmad", ""], ["Alasam", "Rym", ""], ["Madi", "Boraq", ""], ["Rabaev", "Irina", ""], ["Shammes", "Raed", ""], ["El-Sana", "Jihad", ""]]}, {"id": "2003.08633", "submitter": "Erwin Quiring", "authors": "Erwin Quiring and Konrad Rieck", "title": "Backdooring and Poisoning Neural Networks with Image-Scaling Attacks", "comments": "IEEE Deep Learning and Security Workshop (DLS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoors and poisoning attacks are a major threat to the security of\nmachine-learning and vision systems. Often, however, these attacks leave\nvisible artifacts in the images that can be visually detected and weaken the\nefficacy of the attacks. In this paper, we propose a novel strategy for hiding\nbackdoor and poisoning attacks. Our approach builds on a recent class of\nattacks against image scaling. These attacks enable manipulating images such\nthat they change their content when scaled to a specific resolution. By\ncombining poisoning and image-scaling attacks, we can conceal the trigger of\nbackdoors as well as hide the overlays of clean-label poisoning. Furthermore,\nwe consider the detection of image-scaling attacks and derive an adaptive\nattack. In an empirical evaluation, we demonstrate the effectiveness of our\nstrategy. First, we show that backdoors and poisoning work equally well when\ncombined with image-scaling attacks. Second, we demonstrate that current\ndetection defenses against image-scaling attacks are insufficient to uncover\nour manipulations. Overall, our work provides a novel means for hiding traces\nof manipulations, being applicable to different poisoning approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:59:50 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Quiring", "Erwin", ""], ["Rieck", "Konrad", ""]]}, {"id": "2003.08635", "submitter": "Osamu Shouno", "authors": "Osamu Shouno", "title": "Photo-Realistic Video Prediction on Natural Videos of Largely Changing\n  Frames", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have significantly improved performance of\nvideo prediction. However, state-of-the-art methods still suffer from\nblurriness and distortions in their future predictions, especially when there\nare large motions between frames. To address these issues, we propose a deep\nresidual network with the hierarchical architecture where each layer makes a\nprediction of future state at different spatial resolution, and these\npredictions of different layers are merged via top-down connections to generate\nfuture frames. We trained our model with adversarial and perceptual loss\nfunctions, and evaluated it on a natural video dataset captured by car-mounted\ncameras. Our model quantitatively outperforms state-of-the-art baselines in\nfuture frame prediction on video sequences of both largely and slightly\nchanging frames. Furthermore, our model generates future frames with finer\ndetails and textures that are perceptually more realistic than the baselines,\nespecially under fast camera motions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 09:06:06 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Shouno", "Osamu", ""]]}, {"id": "2003.08645", "submitter": "Akash Kumar", "authors": "Akash Kumar and Arnav Bhavsar", "title": "Detecting Deepfakes with Metric Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arrival of several face-swapping applications such as FaceApp,\nSnapChat, MixBooth, FaceBlender and many more, the authenticity of digital\nmedia content is hanging on a very loose thread. On social media platforms,\nvideos are widely circulated often at a high compression factor. In this work,\nwe analyze several deep learning approaches in the context of deepfakes\nclassification in high compression scenario and demonstrate that a proposed\napproach based on metric learning can be very effective in performing such a\nclassification. Using less number of frames per video to assess its realism,\nthe metric learning approach using a triplet network architecture proves to be\nfruitful. It learns to enhance the feature space distance between the cluster\nof real and fake videos embedding vectors. We validated our approaches on two\ndatasets to analyze the behavior in different environments. We achieved a\nstate-of-the-art AUC score of 99.2% on the Celeb-DF dataset and accuracy of\n90.71% on a highly compressed Neural Texture dataset. Our approach is\nespecially helpful on social media platforms where data compression is\ninevitable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 09:44:23 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kumar", "Akash", ""], ["Bhavsar", "Arnav", ""]]}, {"id": "2003.08646", "submitter": "Guangli Li", "authors": "Guangli Li, Lei Liu, Xueying Wang, Xiu Ma, Xiaobing Feng", "title": "LANCE: Efficient Low-Precision Quantized Winograd Convolution for Neural\n  Networks Based on Graphics Processing Units", "comments": "Accepted by ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerating deep convolutional neural networks has become an active topic\nand sparked an interest in academia and industry. In this paper, we propose an\nefficient low-precision quantized Winograd convolution algorithm, called LANCE,\nwhich combines the advantages of fast convolution and quantization techniques.\nBy embedding linear quantization operations into the Winograd-domain, the fast\nconvolution can be performed efficiently under low-precision computation on\ngraphics processing units. We test neural network models with LANCE on\nrepresentative image classification datasets, including SVHN, CIFAR, and\nImageNet. The experimental results show that our 8-bit quantized Winograd\nconvolution improves the performance by up to 2.40x over the full-precision\nconvolution with trivial accuracy loss.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 09:46:50 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 03:03:26 GMT"}, {"version": "v3", "created": "Tue, 28 Jul 2020 13:15:20 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Li", "Guangli", ""], ["Liu", "Lei", ""], ["Wang", "Xueying", ""], ["Ma", "Xiu", ""], ["Feng", "Xiaobing", ""]]}, {"id": "2003.08663", "submitter": "Amine Amyar", "authors": "Amine Amyar, Su Ruan, Pierre Vera, Pierre Decazes, and Romain\n  Modzelewski", "title": "RADIOGAN: Deep Convolutional Conditional Generative adversarial Network\n  To Generate PET Images", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenges in medical imaging is the lack of data. It is\nproven that classical data augmentation methods are useful but still limited\ndue to the huge variation in images. Using generative adversarial networks\n(GAN) is a promising way to address this problem, however, it is challenging to\ntrain one model to generate different classes of lesions. In this paper, we\npropose a deep convolutional conditional generative adversarial network to\ngenerate MIP positron emission tomography image (PET) which is a 2D image that\nrepresents a 3D volume for fast interpretation, according to different lesions\nor non lesion (normal). The advantage of our proposed method consists of one\nmodel that is capable of generating different classes of lesions trained on a\nsmall sample size for each class of lesion, and showing a very promising\nresults. In addition, we show that a walk through a latent space can be used as\na tool to evaluate the images generated.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 10:14:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Amyar", "Amine", ""], ["Ruan", "Su", ""], ["Vera", "Pierre", ""], ["Decazes", "Pierre", ""], ["Modzelewski", "Romain", ""]]}, {"id": "2003.08680", "submitter": "Rui Xiang", "authors": "Rui Xiang, Rongjie Lai, Hongkai Zhao", "title": "Efficient and Robust Shape Correspondence via Sparsity-Enforced\n  Quadratic Assignment", "comments": "8 pages, 6 figures. Compared to the version to be published in the\n  2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\n  Figure 1 has been changed to a more illustrative example and run time table 1\n  has been updated by our recently optimized code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel local pairwise descriptor and then develop\na simple, effective iterative method to solve the resulting quadratic\nassignment through sparsity control for shape correspondence between two\napproximate isometric surfaces. Our pairwise descriptor is based on the\nstiffness and mass matrix of finite element approximation of the\nLaplace-Beltrami differential operator, which is local in space, sparse to\nrepresent, and extremely easy to compute while containing global information.\nIt allows us to deal with open surfaces, partial matching, and topological\nperturbations robustly. To solve the resulting quadratic assignment problem\nefficiently, the two key ideas of our iterative algorithm are: 1) select pairs\nwith good (approximate) correspondence as anchor points, 2) solve a regularized\nquadratic assignment problem only in the neighborhood of selected anchor points\nthrough sparsity control. These two ingredients can improve and increase the\nnumber of anchor points quickly while reducing the computation cost in each\nquadratic assignment iteration significantly. With enough high-quality anchor\npoints, one may use various pointwise global features with reference to these\nanchor points to further improve the dense shape correspondence. We use various\nexperiments to show the efficiency, quality, and versatility of our method on\nlarge data sets, patches, and point clouds (without global meshes).\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 10:56:16 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 21:23:38 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xiang", "Rui", ""], ["Lai", "Rongjie", ""], ["Zhao", "Hongkai", ""]]}, {"id": "2003.08685", "submitter": "Joel Frank", "authors": "Joel Frank, Thorsten Eisenhofer, Lea Sch\\\"onherr, Asja Fischer,\n  Dorothea Kolossa, Thorsten Holz", "title": "Leveraging Frequency Analysis for Deep Fake Image Recognition", "comments": "Accepted to ICML 2020. New experiments, updated several sections,\n  code: https://github.com/RUB-SysSec/GANDCTAnalysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks can generate images that are astonishingly realistic, so\nmuch so that it is often hard for humans to distinguish them from actual\nphotos. These achievements have been largely made possible by Generative\nAdversarial Networks (GANs). While deep fake images have been thoroughly\ninvestigated in the image domain - a classical approach from the area of image\nforensics - an analysis in the frequency domain has been missing so far. In\nthis paper, we address this shortcoming and our results reveal that in\nfrequency space, GAN-generated images exhibit severe artifacts that can be\neasily identified. We perform a comprehensive analysis, showing that these\nartifacts are consistent across different neural network architectures, data\nsets, and resolutions. In a further investigation, we demonstrate that these\nartifacts are caused by upsampling operations found in all current GAN\narchitectures, indicating a structural and fundamental problem in the way\nimages are generated via GANs. Based on this analysis, we demonstrate how the\nfrequency representation can be used to identify deep fake images in an\nautomated way, surpassing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 11:06:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 18:59:03 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 11:21:45 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Frank", "Joel", ""], ["Eisenhofer", "Thorsten", ""], ["Sch\u00f6nherr", "Lea", ""], ["Fischer", "Asja", ""], ["Kolossa", "Dorothea", ""], ["Holz", "Thorsten", ""]]}, {"id": "2003.08717", "submitter": "Thierry Deruyttere", "authors": "Thierry Deruyttere, Guillem Collell, Marie-Francine Moens", "title": "Giving Commands to a Self-driving Car: A Multimodal Reasoner for Visual\n  Grounding", "comments": "Updated acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new spatial memory module and a spatial reasoner for the Visual\nGrounding (VG) task. The goal of this task is to find a certain object in an\nimage based on a given textual query. Our work focuses on integrating the\nregions of a Region Proposal Network (RPN) into a new multi-step reasoning\nmodel which we have named a Multimodal Spatial Region Reasoner (MSRR). The\nintroduced model uses the object regions from an RPN as initialization of a 2D\nspatial memory and then implements a multi-step reasoning process scoring each\nregion according to the query, hence why we call it a multimodal reasoner. We\nevaluate this new model on challenging datasets and our experiments show that\nour model that jointly reasons over the object regions of the image and words\nof the query largely improves accuracy compared to current state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 12:40:41 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:44:40 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 12:08:13 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Deruyttere", "Thierry", ""], ["Collell", "Guillem", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "2003.08729", "submitter": "Chengcheng Jia", "authors": "Chengcheng Jia, Bo Wu, Xiao-Ping Zhang", "title": "Dynamic Spatiotemporal Graph Neural Network with Tensor Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic spatial graph construction is a challenge in graph neural network\n(GNN) for time series data problems. Although some adaptive graphs are\nconceivable, only a 2D graph is embedded in the network to reflect the current\nspatial relation, regardless of all the previous situations. In this work, we\ngenerate a spatial tensor graph (STG) to collect all the dynamic spatial\nrelations, as well as a temporal tensor graph (TTG) to find the latent pattern\nalong time at each node. These two tensor graphs share the same nodes and\nedges, which leading us to explore their entangled correlations by Projected\nEntangled Pair States (PEPS) to optimize the two graphs. We experimentally\ncompare the accuracy and time costing with the state-of-the-art GNN based\nmethods on the public traffic datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 20:47:22 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Jia", "Chengcheng", ""], ["Wu", "Bo", ""], ["Zhang", "Xiao-Ping", ""]]}, {"id": "2003.08730", "submitter": "Markus Fiedler", "authors": "Selim Ickin and Markus Fiedler and Konstantinos Vandikas", "title": "Customized Video QoE Estimation with Algorithm-Agnostic Transfer\n  Learning", "comments": "6 pages, 4 figures, 6 tables, 18 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of QoE models by means of Machine Learning (ML) is\nchallenging, amongst others due to small-size datasets, lack of diversity in\nuser profiles in the source domain, and too much diversity in the target\ndomains of QoE models. Furthermore, datasets can be hard to share between\nresearch entities, as the machine learning models and the collected user data\nfrom the user studies may be IPR- or GDPR-sensitive. This makes a decentralized\nlearning-based framework appealing for sharing and aggregating learned\nknowledge in-between the local models that map the obtained metrics to the user\nQoE, such as Mean Opinion Scores (MOS). In this paper, we present a transfer\nlearning-based ML model training approach, which allows decentralized local\nmodels to share generic indicators on MOS to learn a generic base model, and\nthen customize the generic base model further using additional features that\nare unique to those specific localized (and potentially sensitive) QoE nodes.\nWe show that the proposed approach is agnostic to specific ML algorithms,\nstacked upon each other, as it does not necessitate the collaborating localized\nnodes to run the same ML algorithm. Our reproducible results reveal the\nadvantages of stacking various generic and specific models with corresponding\nweight factors. Moreover, we identify the optimal combination of algorithms and\nweight factors for the corresponding localized QoE nodes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:28:10 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ickin", "Selim", ""], ["Fiedler", "Markus", ""], ["Vandikas", "Konstantinos", ""]]}, {"id": "2003.08731", "submitter": "Natasa Sarafijanovic-Djukic", "authors": "Natasa Sarafijanovic-Djukic and Jesse Davis", "title": "Fast Distance-based Anomaly Detection in Images Using an Inception-like\n  Autoencoder", "comments": "22nd International Conference on Discovery Science, DS 2019", "journal-ref": "InInternational Conference on Discovery Science 2019 Oct 28 (pp.\n  493-508). Springer, Cham", "doi": "10.1007/978-3-030-33778-0_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of anomaly detection is to identify examples that deviate from\nnormal or expected behavior. We tackle this problem for images. We consider a\ntwo-phase approach. First, using normal examples, a convolutional autoencoder\n(CAE) is trained to extract a low-dimensional representation of the images.\nHere, we propose a novel architectural choice when designing the CAE, an\nInception-like CAE. It combines convolutional filters of different kernel sizes\nand it uses a Global Average Pooling (GAP) operation to extract the\nrepresentations from the CAE's bottleneck layer. Second, we employ a\ndistanced-based anomaly detector in the low-dimensional space of the learned\nrepresentation for the images. However, instead of computing the exact\ndistance, we compute an approximate distance using product quantization. This\nalleviates the high memory and prediction time costs of distance-based anomaly\ndetectors. We compare our proposed approach to a number of baselines and\nstate-of-the-art methods on four image datasets, and we find that our approach\nresulted in improved predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 16:10:53 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sarafijanovic-Djukic", "Natasa", ""], ["Davis", "Jesse", ""]]}, {"id": "2003.08732", "submitter": "David Ojika PhD", "authors": "David Ojika, Bhavesh Patel, G. Anthony Reina, Trent Boyer, Chad\n  Martin, Prashant Shah", "title": "Addressing the Memory Bottleneck in AI Model Training", "comments": "Presented at Workshop on MLOps Systems at MLSys 2020 Conference,\n  Austin TX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using medical imaging as case-study, we demonstrate how Intel-optimized\nTensorFlow on an x86-based server equipped with 2nd Generation Intel Xeon\nScalable Processors with large system memory allows for the training of\nmemory-intensive AI/deep-learning models in a scale-up server configuration. We\nbelieve our work represents the first training of a deep neural network having\nlarge memory footprint (~ 1 TB) on a single-node server. We recommend this\nconfiguration to scientists and researchers who wish to develop large,\nstate-of-the-art AI models but are currently limited by memory.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 18:16:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Ojika", "David", ""], ["Patel", "Bhavesh", ""], ["Reina", "G. Anthony", ""], ["Boyer", "Trent", ""], ["Martin", "Chad", ""], ["Shah", "Prashant", ""]]}, {"id": "2003.08736", "submitter": "Genshun Dong", "authors": "Genshun Dong, Yan Yan, Chunhua Shen and Hanzi Wang", "title": "Real-Time High-Performance Semantic Image Segmentation of Urban Street\n  Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) have recently shown outstanding\nperformance in semantic image segmentation. However, state-of-the-art\nDCNN-based semantic segmentation methods usually suffer from high computational\ncomplexity due to the use of complex network architectures. This greatly limits\ntheir applications in the real-world scenarios that require real-time\nprocessing. In this paper, we propose a real-time high-performance DCNN-based\nmethod for robust semantic segmentation of urban street scenes, which achieves\na good trade-off between accuracy and speed. Specifically, a Lightweight\nBaseline Network with Atrous convolution and Attention (LBN-AA) is firstly used\nas our baseline network to efficiently obtain dense feature maps. Then, the\nDistinctive Atrous Spatial Pyramid Pooling (DASPP), which exploits the\ndifferent sizes of pooling operations to encode the rich and distinctive\nsemantic information, is developed to detect objects at multiple scales.\nMeanwhile, a Spatial detail-Preserving Network (SPN) with shallow convolutional\nlayers is designed to generate high-resolution feature maps preserving the\ndetailed spatial information. Finally, a simple but practical Feature Fusion\nNetwork (FFN) is used to effectively combine both shallow and deep features\nfrom the semantic branch (DASPP) and the spatial branch (SPN), respectively.\nExtensive experimental results show that the proposed method respectively\nachieves the accuracy of 73.6% and 68.0% mean Intersection over Union (mIoU)\nwith the inference speed of 51.0 fps and 39.3 fps on the challenging Cityscapes\nand CamVid test datasets (by only using a single NVIDIA TITAN X card). This\ndemonstrates that the proposed method offers excellent performance at the\nreal-time speed for semantic segmentation of urban street scenes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 08:45:53 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 12:27:53 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Dong", "Genshun", ""], ["Yan", "Yan", ""], ["Shen", "Chunhua", ""], ["Wang", "Hanzi", ""]]}, {"id": "2003.08737", "submitter": "Erlei Zhang", "authors": "Shaode Yu, Zhicheng Zhang, Xiaokun Liang, Junjie Wu, Erlei Zhang,\n  Wenjian Qin, and Yaoqin Xie", "title": "A Matlab Toolbox for Feature Importance Ranking", "comments": null, "journal-ref": null, "doi": "10.1109/ICMIPE47306.2019.9098233", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More attention is being paid for feature importance ranking (FIR), in\nparticular when thousands of features can be extracted for intelligent\ndiagnosis and personalized medicine. A large number of FIR approaches have been\nproposed, while few are integrated for comparison and real-life applications.\nIn this study, a matlab toolbox is presented and a total of 30 algorithms are\ncollected. Moreover, the toolbox is evaluated on a database of 163 ultrasound\nimages. To each breast mass lesion, 15 features are extracted. To figure out\nthe optimal subset of features for classification, all combinations of features\nare tested and linear support vector machine is used for the malignancy\nprediction of lesions annotated in ultrasound images. At last, the\neffectiveness of FIR is analyzed according to performance comparison. The\ntoolbox is online (https://github.com/NicoYuCN/matFIR). In our future work,\nmore FIR methods, feature selection methods and machine learning classifiers\nwill be integrated.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 20:35:10 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Yu", "Shaode", ""], ["Zhang", "Zhicheng", ""], ["Liang", "Xiaokun", ""], ["Wu", "Junjie", ""], ["Zhang", "Erlei", ""], ["Qin", "Wenjian", ""], ["Xie", "Yaoqin", ""]]}, {"id": "2003.08740", "submitter": "Abhishek Dubey", "authors": "Vijaya Kumar Sundar, Shreyas Ramakrishna, Zahra Rahiminasab, Arvind\n  Easwaran, Abhishek Dubey", "title": "Out-of-Distribution Detection in Multi-Label Datasets using Latent Space\n  of $\\beta$-VAE", "comments": "Workshop on Assured Autonomy (WAAS) -2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Enabled Components (LECs) are widely being used in a variety of\nperception based autonomy tasks like image segmentation, object detection,\nend-to-end driving, etc. These components are trained with large image datasets\nwith multimodal factors like weather conditions, time-of-day, traffic-density,\netc. The LECs learn from these factors during training, and while testing if\nthere is variation in any of these factors, the components get confused\nresulting in low confidence predictions. The images with factors not seen\nduring training is commonly referred to as Out-of-Distribution (OOD). For safe\nautonomy it is important to identify the OOD images, so that a suitable\nmitigation strategy can be performed. Classical one-class classifiers like SVM\nand SVDD are used to perform OOD detection. However, the multiple labels\nattached to the images in these datasets, restricts the direct application of\nthese techniques. We address this problem using the latent space of the\n$\\beta$-Variational Autoencoder ($\\beta$-VAE). We use the fact that compact\nlatent space generated by an appropriately selected $\\beta$-VAE will encode the\ninformation about these factors in a few latent variables, and that can be used\nfor computationally inexpensive detection. We evaluate our approach on the\nnuScenes dataset, and our results shows the latent space of $\\beta$-VAE is\nsensitive to encode changes in the values of the generative factor.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:12:02 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sundar", "Vijaya Kumar", ""], ["Ramakrishna", "Shreyas", ""], ["Rahiminasab", "Zahra", ""], ["Easwaran", "Arvind", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2003.08741", "submitter": "Shuo Jiang", "authors": "Shuo Jiang, Jianxi Luo, Guillermo Ruiz Pava, Jie Hu, Christopher L.\n  Magee", "title": "A Convolutional Neural Network-based Patent Image Retrieval Method for\n  Design Ideation", "comments": "11 pages, 11 figures", "journal-ref": "ASME 2020 International Design Engineering Technical Conferences\n  and Computers and Information in Engineering Conference", "doi": "10.1115/DETC2020-22048", "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The patent database is often used in searches of inspirational stimuli for\ninnovative design opportunities because of its large size, extensive variety\nand rich design information in patent documents. However, most patent mining\nresearch only focuses on textual information and ignores visual information.\nHerein, we propose a convolutional neural network (CNN)-based patent image\nretrieval method. The core of this approach is a novel neural network\narchitecture named Dual-VGG that is aimed to accomplish two tasks: visual\nmaterial type prediction and international patent classification (IPC) class\nlabel prediction. In turn, the trained neural network provides the deep\nfeatures in the image embedding vectors that can be utilized for patent image\nretrieval and visual mapping. The accuracy of both training tasks and patent\nimage embedding space are evaluated to show the performance of our model. This\napproach is also illustrated in a case study of robot arm design retrieval.\nCompared to traditional keyword-based searching and Google image searching, the\nproposed method discovers more useful visual information for engineering\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 13:32:08 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 20:31:24 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 22:58:48 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Jiang", "Shuo", ""], ["Luo", "Jianxi", ""], ["Pava", "Guillermo Ruiz", ""], ["Hu", "Jie", ""], ["Magee", "Christopher L.", ""]]}, {"id": "2003.08743", "submitter": "Dom Huh", "authors": "Dom Huh, Sai Gurrapu, Frederick Olson, Huzefa Rangwala, Parth Pathak,\n  Jana Kosecka", "title": "Generative Multi-Stream Architecture For American Sign Language\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advancements in deep model architectures, tasks in computer vision can\nreach optimal convergence provided proper data preprocessing and model\nparameter initialization. However, training on datasets with low\nfeature-richness for complex applications limit and detriment optimal\nconvergence below human performance. In past works, researchers have provided\nexternal sources of complementary data at the cost of supplementary hardware,\nwhich are fed in streams to counteract this limitation and boost performance.\nWe propose a generative multi-stream architecture, eliminating the need for\nadditional hardware with the intent to improve feature richness without risking\nimpracticability. We also introduce the compact spatio-temporal residual block\nto the standard 3-dimensional convolutional model, C3D. Our rC3D model performs\ncomparatively to the top C3D residual variant architecture, the pseudo-3D\nmodel, on the FASL-RGB dataset. Our methods have achieved 95.62% validation\naccuracy with a variance of 1.42% from training, outperforming past models by\n0.45% in validation accuracy and 5.53% in variance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 21:04:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Huh", "Dom", ""], ["Gurrapu", "Sai", ""], ["Olson", "Frederick", ""], ["Rangwala", "Huzefa", ""], ["Pathak", "Parth", ""], ["Kosecka", "Jana", ""]]}, {"id": "2003.08744", "submitter": "Emilie Wirbel", "authors": "Thibault Buhet, and Emilie Wirbel and Andrei Bursuc and Xavier\n  Perrotton", "title": "PLOP: Probabilistic poLynomial Objects trajectory Planning for\n  autonomous driving", "comments": "Accepted at CorRL 2020 (matching camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To navigate safely in urban environments, an autonomous vehicle (ego vehicle)\nmust understand and anticipate its surroundings, in particular the behavior and\nintents of other road users (neighbors). Most of the times, multiple decision\nchoices are acceptable for all road users (e.g., turn right or left, or\ndifferent ways of avoiding an obstacle), leading to a highly uncertain and\nmulti-modal decision space. We focus here on predicting multiple feasible\nfuture trajectories for both ego vehicle and neighbors through a probabilistic\nframework. We rely on a conditional imitation learning algorithm, conditioned\nby a navigation command for the ego vehicle (e.g., \"turn right\"). Our model\nprocesses ego vehicle front-facing camera images and bird-eye view grid,\ncomputed from Lidar point clouds, with detections of past and present objects,\nin order to generate multiple trajectories for both ego vehicle and its\nneighbors. Our approach is computationally efficient and relies only on\non-board sensors. We evaluate our method offline on the publicly available\ndataset nuScenes, achieving state-of-the-art performance, investigate the\nimpact of our architecture choices on online simulated experiments and show\npreliminary insights for real vehicle control\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:55:07 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 15:39:51 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 08:29:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Buhet", "Thibault", ""], ["Wirbel", "Emilie", ""], ["Bursuc", "Andrei", ""], ["Perrotton", "Xavier", ""]]}, {"id": "2003.08745", "submitter": "Alice Plebe", "authors": "Alice Plebe and Mauro Da Lio", "title": "On the Road with 16 Neurons: Mental Imagery with Bio-inspired Deep\n  Neural Networks", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy for visual prediction in the context of\nautonomous driving. Humans, when not distracted or drunk, are still the best\ndrivers you can currently find. For this reason we take inspiration from two\ntheoretical ideas about the human mind and its neural organization. The first\nidea concerns how the brain uses a hierarchical structure of neuron ensembles\nto extract abstract concepts from visual experience and code them into compact\nrepresentations. The second idea suggests that these neural perceptual\nrepresentations are not neutral but functional to the prediction of the future\nstate of affairs in the environment. Similarly, the prediction mechanism is not\nneutral but oriented to the current planning of a future action. We identify\nwithin the deep learning framework two artificial counterparts of the\naforementioned neurocognitive theories. We find a correspondence between the\nfirst theoretical idea and the architecture of convolutional autoencoders,\nwhile we translate the second theory into a training procedure that learns\ncompact representations which are not neutral but oriented to driving tasks,\nfrom two distinct perspectives. From a static perspective, we force groups of\nneural units in the compact representations to distinctly represent specific\nconcepts crucial to the driving task. From a dynamic perspective, we encourage\nthe compact representations to be predictive of how the current road scenario\nwill change in the future. We successfully learn compact representations that\nuse as few as 16 neural units for each of the two basic driving concepts we\nconsider: car and lane. We prove the efficiency of our proposed perceptual\nrepresentations on the SYNTHIA dataset. Our source code is available at\nhttps://github.com/3lis/rnn_vae\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 16:46:29 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Plebe", "Alice", ""], ["Da Lio", "Mauro", ""]]}, {"id": "2003.08747", "submitter": "Laura Rieger", "authors": "Laura Rieger, Lars Kai Hansen", "title": "IROF: a low resource evaluation metric for explanation methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of machine learning in health care hinges on the transparency of\nthe used algorithms, necessitating the need for explanation methods. However,\ndespite a growing literature on explaining neural networks, no consensus has\nbeen reached on how to evaluate those explanation methods. We propose IROF, a\nnew approach to evaluating explanation methods that circumvents the need for\nmanual evaluation. Compared to other recent work, our approach requires several\norders of magnitude less computational resources and no human input, making it\naccessible to lower resource groups and robust to human bias.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 13:01:30 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Rieger", "Laura", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "2003.08748", "submitter": "Marco Grinet", "authors": "Marco A. V. M. Grinet, Nuno M. Garcia, Ana I. R. Gouveia, Jose A. F.\n  Moutinho, Abel J. P. Gomes", "title": "Reduction of Surgical Risk Through the Evaluation of Medical Imaging\n  Diagnostics", "comments": "25 pages, 7 figures, Scientific grant report", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer aided diagnosis (CAD) of Breast Cancer (BRCA) images has been an\nactive area of research in recent years. The main goals of this research is to\ndevelop reliable automatic methods for detecting and diagnosing different types\nof BRCA from diagnostic images. In this paper, we present a review of the state\nof the art CAD methods applied to magnetic resonance (MRI) and mammography\nimages of BRCA patients. The review aims to provide an extensive introduction\nto different features extracted from BRCA images through texture and\nstatistical analysis and to categorize deep learning frameworks and data\nstructures capable of using metadata to aggregate relevant information to\nassist oncologists and radiologists. We divide the existing literature\naccording to the imaging modality and into radiomics, machine learning, or\ncombination of both. We also emphasize the difference between each modality and\nmethods strengths and weaknesses and analyze their performance in detecting\nBRCA through a quantitative comparison. We compare the results of various\napproaches for implementing CAD systems for the detection of BRCA. Each\napproachs standard workflow components are reviewed and summary tables\nprovided. We present an extensive literature review of radiomics feature\nextraction techniques and machine learning methods applied in BRCA diagnosis\nand detection, focusing on data preparation, data structures, pre processing\nand post processing strategies available in the literature. There is a growing\ninterest on radiomic feature extraction and machine learning methods for BRCA\ndetection through histopathological images, MRI and mammography images.\nHowever, there isnt a CAD method able to combine distinct data types to provide\nthe best diagnostic results. Employing data fusion techniques to medical images\nand patient data could lead to improved detection and classification results.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 17:06:57 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Grinet", "Marco A. V. M.", ""], ["Garcia", "Nuno M.", ""], ["Gouveia", "Ana I. R.", ""], ["Moutinho", "Jose A. F.", ""], ["Gomes", "Abel J. P.", ""]]}, {"id": "2003.08749", "submitter": "Yaser Banadaki", "authors": "Yaser Banadaki, Nariman Razaviarab, Hadi Fekrmandi, and Safura Sharifi", "title": "Toward Enabling a Reliable Quality Monitoring System for Additive\n  Manufacturing Process using Deep Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive Manufacturing (AM) is a crucial component of the smart industry. In\nthis paper, we propose an automated quality grading system for the AM process\nusing a deep convolutional neural network (CNN) model. The CNN model is trained\noffline using the images of the internal and surface defects in the\nlayer-by-layer deposition of materials and tested online by studying the\nperformance of detecting and classifying the failure in AM process at different\nextruder speeds and temperatures. The model demonstrates the accuracy of 94%\nand specificity of 96%, as well as above 75% in three classifier measures of\nthe Fscore, the sensitivity, and precision for classifying the quality of the\nprinting process in five grades in real-time. The proposed online model adds an\nautomated, consistent, and non-contact quality control signal to the AM process\nthat eliminates the manual inspection of parts after they are entirely built.\nThe quality monitoring signal can also be used by the machine to suggest\nremedial actions by adjusting the parameters in real-time. The proposed quality\npredictive model serves as a proof-of-concept for any type of AM machines to\nproduce reliable parts with fewer quality hiccups while limiting the waste of\nboth time and materials.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 20:49:20 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Banadaki", "Yaser", ""], ["Razaviarab", "Nariman", ""], ["Fekrmandi", "Hadi", ""], ["Sharifi", "Safura", ""]]}, {"id": "2003.08750", "submitter": "Joshua Levy", "authors": "Joshua J. Levy, Rebecca M. Lebeaux, Anne G. Hoen, Brock C.\n  Christensen, Louis J. Vaickus, Todd A. MacKenzie", "title": "Longevity Associated Geometry Identified in Satellite Images: Sidewalks,\n  Driveways and Hiking Trails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance: Following a century of increase, life expectancy in the United\nStates has stagnated and begun to decline in recent decades. Using satellite\nimages and street view images prior work has demonstrated associations of the\nbuilt environment with income, education, access to care and health factors\nsuch as obesity. However, assessment of learned image feature relationships\nwith variation in crude mortality rate across the United States has been\nlacking.\n  Objective: Investigate prediction of county-level mortality rates in the U.S.\nusing satellite images.\n  Design: Satellite images were extracted with the Google Static Maps\napplication programming interface for 430 counties representing approximately\n68.9% of the US population. A convolutional neural network was trained using\ncrude mortality rates for each county in 2015 to predict mortality. Learned\nimage features were interpreted using Shapley Additive Feature Explanations,\nclustered, and compared to mortality and its associated covariate predictors.\n  Main Outcomes and Measures: County mortality was predicted using satellite\nimages.\n  Results: Predicted mortality from satellite images in a held-out test set of\ncounties was strongly correlated to the true crude mortality rate (Pearson\nr=0.72). Learned image features were clustered, and we identified 10 clusters\nthat were associated with education, income, geographical region, race and age.\n  Conclusion and Relevance: The application of deep learning techniques to\nremotely-sensed features of the built environment can serve as a useful\npredictor of mortality in the United States. Tools that are able to identify\nimage features associated with health-related outcomes can inform targeted\npublic health interventions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 20:23:11 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Levy", "Joshua J.", ""], ["Lebeaux", "Rebecca M.", ""], ["Hoen", "Anne G.", ""], ["Christensen", "Brock C.", ""], ["Vaickus", "Louis J.", ""], ["MacKenzie", "Todd A.", ""]]}, {"id": "2003.08751", "submitter": "Ines Rieger", "authors": "Ines Rieger, Jaspar Pahl and Dominik Seuss", "title": "Unique Class Group Based Multi-Label Balancing Optimizer for Action Unit\n  Detection", "comments": "Accepted at the 15th IEEE International Conference on Automatic Face\n  and Gesture Recognition 2020, Workshop \"Affect Recognition in-the-wild:\n  Uni/Multi-Modal Analysis & VA-AU-Expression Challenges\". arXiv admin note:\n  substantial text overlap with arXiv:2002.03238", "journal-ref": "Proceedings of the 15th IEEE International Conference on Automatic\n  Face and Gesture Recognition (FG 2020) 619-623", "doi": "10.1109/FG47880.2020.00101", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing methods for single-label data cannot be applied to multi-label\nproblems as they would also resample the samples with high occurrences. We\npropose to reformulate this problem as an optimization problem in order to\nbalance multi-label data. We apply this balancing algorithm to training\ndatasets for detecting isolated facial movements, so-called Action Units.\nSeveral Action Units can describe combined emotions or physical states such as\npain. As datasets in this area are limited and mostly imbalanced, we show how\noptimized balancing and then augmentation can improve Action Unit detection. At\nthe IEEE Conference on Face and Gesture Recognition 2020, we ranked third in\nthe Affective Behavior Analysis in-the-wild (ABAW) challenge for the Action\nUnit detection task.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 15:34:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Rieger", "Ines", ""], ["Pahl", "Jaspar", ""], ["Seuss", "Dominik", ""]]}, {"id": "2003.08752", "submitter": "Mengxiao Hu", "authors": "Mengxiao Hu, Jinlong Li, Maolin Hu, Tao Hu", "title": "Hierarchical Modes Exploring in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conditional Generative Adversarial Networks (cGANs), when two different\ninitial noises are concatenated with the same conditional information, the\ndistance between their outputs is relatively smaller, which makes minor modes\nlikely to collapse into large modes. To prevent this happen, we proposed a\nhierarchical mode exploring method to alleviate mode collapse in cGANs by\nintroducing a diversity measurement into the objective function as the\nregularization term. We also introduced the Expected Ratios of Expansion (ERE)\ninto the regularization term, by minimizing the sum of differences between the\nreal change of distance and ERE, we can control the diversity of generated\nimages w.r.t specific-level features. We validated the proposed algorithm on\nfour conditional image synthesis tasks including categorical generation, paired\nand un-paired image translation and text-to-image generation. Both qualitative\nand quantitative results show that the proposed method is effective in\nalleviating the mode collapse problem in cGANs, and can control the diversity\nof output images w.r.t specific-level features.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 10:43:50 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Hu", "Mengxiao", ""], ["Li", "Jinlong", ""], ["Hu", "Maolin", ""], ["Hu", "Tao", ""]]}, {"id": "2003.08753", "submitter": "Al Amin Hosain", "authors": "Al Amin Hosain, Panneer Selvam Santhalingam, Parth Pathak, Huzefa\n  Rangwala and Jana Kosecka", "title": "FineHand: Learning Hand Shapes for American Sign Language Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American Sign Language recognition is a difficult gesture recognition\nproblem, characterized by fast, highly articulate gestures. These are comprised\nof arm movements with different hand shapes, facial expression and head\nmovements. Among these components, hand shape is the vital, often the most\ndiscriminative part of a gesture. In this work, we present an approach for\neffective learning of hand shape embeddings, which are discriminative for ASL\ngestures. For hand shape recognition our method uses a mix of manually labelled\nhand shapes and high confidence predictions to train deep convolutional neural\nnetwork (CNN). The sequential gesture component is captured by recursive neural\nnetwork (RNN) trained on the embeddings learned in the first stage. We will\ndemonstrate that higher quality hand shape models can significantly improve the\naccuracy of final video gesture classification in challenging conditions with\nvariety of speakers, different illumination and significant motion blurr. We\ncompare our model to alternative approaches exploiting different modalities and\nrepresentations of the data and show improved video gesture recognition\naccuracy on GMU-ASL51 benchmark dataset\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 23:32:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Hosain", "Al Amin", ""], ["Santhalingam", "Panneer Selvam", ""], ["Pathak", "Parth", ""], ["Rangwala", "Huzefa", ""], ["Kosecka", "Jana", ""]]}, {"id": "2003.08754", "submitter": "Anh Nguyen", "authors": "Naman Bansal, Chirag Agarwal, Anh Nguyen", "title": "SAM: The Sensitivity of Attribution Methods to Hyperparameters", "comments": "Oral paper at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribution methods can provide powerful insights into the reasons for a\nclassifier's decision. We argue that a key desideratum of an explanation method\nis its robustness to input hyperparameters which are often randomly set or\nempirically tuned. High sensitivity to arbitrary hyperparameter choices does\nnot only impede reproducibility but also questions the correctness of an\nexplanation and impairs the trust of end-users. In this paper, we provide a\nthorough empirical study on the sensitivity of existing attribution methods. We\nfound an alarming trend that many methods are highly sensitive to changes in\ntheir common hyperparameters e.g. even changing a random seed can yield a\ndifferent explanation! Interestingly, such sensitivity is not reflected in the\naverage explanation accuracy scores over the dataset as commonly reported in\nthe literature. In addition, explanations generated for robust classifiers\n(i.e. which are trained to be invariant to pixel-wise perturbations) are\nsurprisingly more robust than those generated for regular classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 22:09:22 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 03:32:08 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Bansal", "Naman", ""], ["Agarwal", "Chirag", ""], ["Nguyen", "Anh", ""]]}, {"id": "2003.08755", "submitter": "Francesco Bardozzo", "authors": "Francesco Bardozzo, Borja De La Osa, Lubomira Horanska, Javier\n  Fumanal-Idocin, Mattia delli Priscoli, Luigi Troiano, Roberto Tagliaferri,\n  Javier Fernandez, Humberto Bustince", "title": "Adaptive binarization based on fuzzy integrals", "comments": "11 pages, 3 figures, 3 algorithms, Journal paper under a revision of\n  IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1016/j.inffus.2020.10.020", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive binarization methodologies threshold the intensity of the pixels\nwith respect to adjacent pixels exploiting the integral images. In turn, the\nintegral images are generally computed optimally using the summed-area-table\nalgorithm (SAT). This document presents a new adaptive binarization technique\nbased on fuzzy integral images through an efficient design of a modified SAT\nfor fuzzy integrals. We define this new methodology as FLAT (Fuzzy Local\nAdaptive Thresholding). The experimental results show that the proposed\nmethodology have produced an image quality thresholding often better than\ntraditional algorithms and saliency neural networks. We propose a new\ngeneralization of the Sugeno and CF 1,2 integrals to improve existing results\nwith an efficient integral image computation. Therefore, these new generalized\nfuzzy integrals can be used as a tool for grayscale processing in real-time and\ndeep-learning applications. Index Terms: Image Thresholding, Image Processing,\nFuzzy Integrals, Aggregation Functions\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 18:30:57 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Bardozzo", "Francesco", ""], ["De La Osa", "Borja", ""], ["Horanska", "Lubomira", ""], ["Fumanal-Idocin", "Javier", ""], ["Priscoli", "Mattia delli", ""], ["Troiano", "Luigi", ""], ["Tagliaferri", "Roberto", ""], ["Fernandez", "Javier", ""], ["Bustince", "Humberto", ""]]}, {"id": "2003.08756", "submitter": "Mohammad Javad Shafiee", "authors": "Mohammad Javad Shafiee, Ahmadreza Jeddi, Amir Nazemi, Paul Fieguth,\n  and Alexander Wong", "title": "Deep Neural Network Perception Models and Robust Autonomous Driving\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes the robustness of deep learning models in autonomous\ndriving applications and discusses the practical solutions to address that.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 15:02:05 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Jeddi", "Ahmadreza", ""], ["Nazemi", "Amir", ""], ["Fieguth", "Paul", ""], ["Wong", "Alexander", ""]]}, {"id": "2003.08757", "submitter": "Ranjie Duan", "authors": "Ranjie Duan, Xingjun Ma, Yisen Wang, James Bailey, A. K. Qin, Yun Yang", "title": "Adversarial Camouflage: Hiding Physical-World Attacks with Natural\n  Styles", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are known to be vulnerable to adversarial\nexamples. Existing works have mostly focused on either digital adversarial\nexamples created via small and imperceptible perturbations, or physical-world\nadversarial examples created with large and less realistic distortions that are\neasily identified by human observers. In this paper, we propose a novel\napproach, called Adversarial Camouflage (\\emph{AdvCam}), to craft and\ncamouflage physical-world adversarial examples into natural styles that appear\nlegitimate to human observers. Specifically, \\emph{AdvCam} transfers large\nadversarial perturbations into customized styles, which are then \"hidden\"\non-target object or off-target background. Experimental evaluation shows that,\nin both digital and physical-world scenarios, adversarial examples crafted by\n\\emph{AdvCam} are well camouflaged and highly stealthy, while remaining\neffective in fooling state-of-the-art DNN image classifiers. Hence,\n\\emph{AdvCam} is a flexible approach that can help craft stealthy attacks to\nevaluate the robustness of DNNs. \\emph{AdvCam} can also be used to protect\nprivate information from being detected by deep learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 07:22:41 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 05:15:12 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Duan", "Ranjie", ""], ["Ma", "Xingjun", ""], ["Wang", "Yisen", ""], ["Bailey", "James", ""], ["Qin", "A. K.", ""], ["Yang", "Yun", ""]]}, {"id": "2003.08758", "submitter": "Patricia Wollstadt", "authors": "Skylar Sible, Rodrigo Iza-Teran, Jochen Garcke, Nikola Aulig, Patricia\n  Wollstadt", "title": "A Compact Spectral Descriptor for Shape Deformations", "comments": "To be published in Proc. of the European Conference on Artificial\n  Intelligence 2020, 12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern product design in the engineering domain is increasingly driven by\ncomputational analysis including finite-element based simulation, computational\noptimization, and modern data analysis techniques such as machine learning. To\napply these methods, suitable data representations for components under\ndevelopment as well as for related design criteria have to be found. While a\ncomponent's geometry is typically represented by a polygon surface mesh, it is\noften not clear how to parametrize critical design properties in order to\nenable efficient computational analysis. In the present work, we propose a\nnovel methodology to obtain a parameterization of a component's plastic\ndeformation behavior under stress, which is an important design criterion in\nmany application domains, for example, when optimizing the crash behavior in\nthe automotive context. Existing parameterizations limit computational analysis\nto relatively simple deformations and typically require extensive input by an\nexpert, making the design process time intensive and costly. Hence, we propose\na way to derive a compact descriptor of deformation behavior that is based on\nspectral mesh processing and enables a low-dimensional representation of also\ncomplex deformations.We demonstrate the descriptor's ability to represent\nrelevant deformation behavior by applying it in a nearest-neighbor search to\nidentify similar simulation results in a filtering task. The proposed\ndescriptor provides a novel approach to the parametrization of geometric\ndeformation behavior and enables the use of state-of-the-art data analysis\ntechniques such as machine learning to engineering tasks concerned with plastic\ndeformation behavior.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:34:30 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sible", "Skylar", ""], ["Iza-Teran", "Rodrigo", ""], ["Garcke", "Jochen", ""], ["Aulig", "Nikola", ""], ["Wollstadt", "Patricia", ""]]}, {"id": "2003.08759", "submitter": "Marie Alaghband", "authors": "Marie Alaghband, Niloofar Yousefi, Ivan Garibay", "title": "Facial Expression Phoenix (FePh): An Annotated Sequenced Dataset for\n  Facial and Emotion-Specified Expressions in Sign Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Facial expressions are important parts of both gesture and sign language\nrecognition systems. Despite the recent advances in both fields, annotated\nfacial expression datasets in the context of sign language are still scarce\nresources. In this manuscript, we introduce an annotated sequenced facial\nexpression dataset in the context of sign language, comprising over $3000$\nfacial images extracted from the daily news and weather forecast of the public\ntv-station PHOENIX. Unlike the majority of currently existing facial expression\ndatasets, FePh provides sequenced semi-blurry facial images with different head\nposes, orientations, and movements. In addition, in the majority of images,\nidentities are mouthing the words, which makes the data more challenging. To\nannotate this dataset we consider primary, secondary, and tertiary dyads of\nseven basic emotions of \"sad\", \"surprise\", \"fear\", \"angry\", \"neutral\",\n\"disgust\", and \"happy\". We also considered the \"None\" class if the image's\nfacial expression could not be described by any of the aforementioned emotions.\nAlthough we provide FePh as a facial expression dataset of signers in sign\nlanguage, it has a wider application in gesture recognition and Human Computer\nInteraction (HCI) systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 03:42:36 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 18:36:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Alaghband", "Marie", ""], ["Yousefi", "Niloofar", ""], ["Garibay", "Ivan", ""]]}, {"id": "2003.08760", "submitter": "Minh Vu", "authors": "Minh H. Vu, Tommy L\\\"ofstedt, Tufve Nyholm, Raphael Sznitman", "title": "A Question-Centric Model for Visual Question Answering in Medical\n  Imaging", "comments": "Accepted at IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2020.2978284", "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning methods have proven extremely effective at performing a variety\nof medical image analysis tasks. With their potential use in clinical routine,\ntheir lack of transparency has however been one of their few weak points,\nraising concerns regarding their behavior and failure modes. While most\nresearch to infer model behavior has focused on indirect strategies that\nestimate prediction uncertainties and visualize model support in the input\nimage space, the ability to explicitly query a prediction model regarding its\nimage content offers a more direct way to determine the behavior of trained\nmodels. To this end, we present a novel Visual Question Answering approach that\nallows an image to be queried by means of a written question. Experiments on a\nvariety of medical and natural image datasets show that by fusing image and\nquestion features in a novel way, the proposed approach achieves an equal or\nhigher accuracy compared to current methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 10:16:16 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Vu", "Minh H.", ""], ["L\u00f6fstedt", "Tommy", ""], ["Nyholm", "Tufve", ""], ["Sznitman", "Raphael", ""]]}, {"id": "2003.08761", "submitter": "Ruimao Zhang", "authors": "Ruimao Zhang, Zhanglin Peng, Lingyun Wu, Zhen Li, Ping Luo", "title": "Exemplar Normalization for Learning Deep Representation", "comments": "Accepted by CVPR2020, normalization methods, image classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalization techniques are important in different advanced neural networks\nand different tasks. This work investigates a novel dynamic\nlearning-to-normalize (L2N) problem by proposing Exemplar Normalization (EN),\nwhich is able to learn different normalization methods for different\nconvolutional layers and image samples of a deep network. EN significantly\nimproves flexibility of the recently proposed switchable normalization (SN),\nwhich solves a static L2N problem by linearly combining several normalizers in\neach normalization layer (the combination is the same for all samples). Instead\nof directly employing a multi-layer perceptron (MLP) to learn data-dependent\nparameters as conditional batch normalization (cBN) did, the internal\narchitecture of EN is carefully designed to stabilize its optimization, leading\nto many appealing benefits. (1) EN enables different convolutional layers,\nimage samples, categories, benchmarks, and tasks to use different normalization\nmethods, shedding light on analyzing them in a holistic view. (2) EN is\neffective for various network architectures and tasks. (3) It could replace any\nnormalization layers in a deep network and still produce stable model training.\nExtensive experiments demonstrate the effectiveness of EN in a wide spectrum of\ntasks including image recognition, noisy label learning, and semantic\nsegmentation. For example, by replacing BN in the ordinary ResNet50,\nimprovement produced by EN is 300% more than that of SN on both ImageNet and\nthe noisy WebVision dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 13:23:40 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 14:58:40 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Zhang", "Ruimao", ""], ["Peng", "Zhanglin", ""], ["Wu", "Lingyun", ""], ["Li", "Zhen", ""], ["Luo", "Ping", ""]]}, {"id": "2003.08763", "submitter": "A. Ben Hamza", "authors": "David Pickup, Xianfang Sun, Paul L Rosin, Ralph R Martin, Z Cheng,\n  Zhouhui Lian, Masaki Aono, A Ben Hamza, A Bronstein, M Bronstein, S Bu,\n  Umberto Castellani, S Cheng, Valeria Garro, Andrea Giachetti, Afzal Godil,\n  Luca Isaia, J Han, Henry Johan, L Lai, Bo Li, C Li, Haisheng Li, Roee Litman,\n  X Liu, Z Liu, Yijuan Lu, L Sun, G Tam, Atsushi Tatsuma, J Ye", "title": "Shape retrieval of non-rigid 3d human models", "comments": "International Journal of Computer Vision, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D models of humans are commonly used within computer graphics and vision,\nand so the ability to distinguish between body shapes is an important shape\nretrieval problem. We extend our recent paper which provided a benchmark for\ntesting non-rigid 3D shape retrieval algorithms on 3D human models. This\nbenchmark provided a far stricter challenge than previous shape benchmarks. We\nhave added 145 new models for use as a separate training set, in order to\nstandardise the training data used and provide a fairer comparison. We have\nalso included experiments with the FAUST dataset of human scans. All\nparticipants of the previous benchmark study have taken part in the new tests\nreported here, many providing updated results using the new data. In addition,\nfurther participants have also taken part, and we provide extra analysis of the\nretrieval results. A total of 25 different shape retrieval methods.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 20:03:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Pickup", "David", ""], ["Sun", "Xianfang", ""], ["Rosin", "Paul L", ""], ["Martin", "Ralph R", ""], ["Cheng", "Z", ""], ["Lian", "Zhouhui", ""], ["Aono", "Masaki", ""], ["Hamza", "A Ben", ""], ["Bronstein", "A", ""], ["Bronstein", "M", ""], ["Bu", "S", ""], ["Castellani", "Umberto", ""], ["Cheng", "S", ""], ["Garro", "Valeria", ""], ["Giachetti", "Andrea", ""], ["Godil", "Afzal", ""], ["Isaia", "Luca", ""], ["Han", "J", ""], ["Johan", "Henry", ""], ["Lai", "L", ""], ["Li", "Bo", ""], ["Li", "C", ""], ["Li", "Haisheng", ""], ["Litman", "Roee", ""], ["Liu", "X", ""], ["Liu", "Z", ""], ["Lu", "Yijuan", ""], ["Sun", "L", ""], ["Tam", "G", ""], ["Tatsuma", "Atsushi", ""], ["Ye", "J", ""]]}, {"id": "2003.08765", "submitter": "Wei Zhen Teoh", "authors": "Shanmeng Sun, Wei Zhen Teoh, Michael Guerzhoy", "title": "Salient Facial Features from Humans and Deep Neural Networks", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we explore the features that are used by humans and by\nconvolutional neural networks (ConvNets) to classify faces. We use Guided\nBackpropagation (GB) to visualize the facial features that influence the output\nof a ConvNet the most when identifying specific individuals; we explore how to\nbest use GB for that purpose. We use a human intelligence task to find out\nwhich facial features humans find to be the most important for identifying\nspecific individuals. We explore the differences between the saliency\ninformation gathered from humans and from ConvNets.\n  Humans develop biases in employing available information on facial features\nto discriminate across faces. Studies show these biases are influenced both by\nneurological development and by each individual's social experience. In recent\nyears the computer vision community has achieved human-level performance in\nmany face processing tasks with deep neural network-based models. These face\nprocessing systems are also subject to systematic biases due to model\narchitectural choices and training data distribution.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 22:41:04 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sun", "Shanmeng", ""], ["Teoh", "Wei Zhen", ""], ["Guerzhoy", "Michael", ""]]}, {"id": "2003.08766", "submitter": "Javier Gonzalez-Trejo", "authors": "Javier Gonzalez-Trejo, Diego Mercado-Ravell", "title": "Dense Crowds Detection and Surveillance with Drones using Density Maps", "comments": "2020 International Conference on Unmanned Aircraft Systems (ICUAS),\n  Athens, Greece, 2020", "journal-ref": null, "doi": "10.1109/ICUAS48674.2020.9213886", "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and Counting people in a human crowd from a moving drone present\nchallenging problems that arisefrom the constant changing in the image\nperspective andcamera angle. In this paper, we test two different\nstate-of-the-art approaches, density map generation with VGG19 trainedwith the\nBayes loss function and detect-then-count with FasterRCNN with ResNet50-FPN as\nbackbone, in order to comparetheir precision for counting and detecting people\nin differentreal scenarios taken from a drone flight. We show empiricallythat\nboth proposed methodologies perform especially well fordetecting and counting\npeople in sparse crowds when thedrone is near the ground. Nevertheless, VGG19\nprovides betterprecision on both tasks while also being lighter than\nFasterRCNN. Furthermore, VGG19 outperforms Faster RCNN whendealing with dense\ncrowds, proving to be more robust toscale variations and strong occlusions,\nbeing more suitable forsurveillance applications using drones\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 02:05:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gonzalez-Trejo", "Javier", ""], ["Mercado-Ravell", "Diego", ""]]}, {"id": "2003.08767", "submitter": "Aleksandar Vakanski", "authors": "Yalin Liao, Aleksandar Vakanski, Min Xian, David Paul, Russell Baker", "title": "A Review of Computational Approaches for Evaluation of Rehabilitation\n  Exercises", "comments": "29 pages, 1 figure", "journal-ref": "Computers in Biology and Medicine, vol. 119, 2020", "doi": "10.1016/j.compbiomed.2020.103687", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in data analytics and computer-aided diagnostics stimulate\nthe vision of patient-centric precision healthcare, where treatment plans are\ncustomized based on the health records and needs of every patient. In physical\nrehabilitation, the progress in machine learning and the advent of affordable\nand reliable motion capture sensors have been conducive to the development of\napproaches for automated assessment of patient performance and progress toward\nfunctional recovery. The presented study reviews computational approaches for\nevaluating patient performance in rehabilitation programs using motion capture\nsystems. Such approaches will play an important role in supplementing\ntraditional rehabilitation assessment performed by trained clinicians, and in\nassisting patients participating in home-based rehabilitation. The reviewed\ncomputational methods for exercise evaluation are grouped into three main\ncategories: discrete movement score, rule-based, and template-based approaches.\nThe review places an emphasis on the application of machine learning methods\nfor movement evaluation in rehabilitation. Related work in the literature on\ndata representation, feature engineering, movement segmentation, and scoring\nfunctions is presented. The study also reviews existing sensors for capturing\nrehabilitation movements and provides an informative listing of pertinent\nbenchmark datasets. The significance of this paper is in being the first to\nprovide a comprehensive review of computational methods for evaluation of\npatient performance in rehabilitation programs.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 22:18:56 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 02:55:34 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Liao", "Yalin", ""], ["Vakanski", "Aleksandar", ""], ["Xian", "Min", ""], ["Paul", "David", ""], ["Baker", "Russell", ""]]}, {"id": "2003.08769", "submitter": "Nitish Nag", "authors": "Nitish Nag, Bindu Rajanna, Ramesh Jain", "title": "Personalized Taste and Cuisine Preference Modeling via Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the exponential growth in the usage of social media to share live\nupdates about life, taking pictures has become an unavoidable phenomenon.\nIndividuals unknowingly create a unique knowledge base with these images. The\nfood images, in particular, are of interest as they contain a plethora of\ninformation. From the image metadata and using computer vision tools, we can\nextract distinct insights for each user to build a personal profile. Using the\nunderlying connection between cuisines and their inherent tastes, we attempt to\ndevelop such a profile for an individual based solely on the images of his\nfood. Our study provides insights about an individual's inclination towards\nparticular cuisines. Interpreting these insights can lead to the development of\na more precise recommendation system. Such a system would avoid the generic\napproach in favor of a personalized recommendation system.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:07:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Nag", "Nitish", ""], ["Rajanna", "Bindu", ""], ["Jain", "Ramesh", ""]]}, {"id": "2003.08770", "submitter": "Chenhan Jiang", "authors": "Chenhan Jiang, Shaoju Wang, Hang Xu, Xiaodan Liang, Nong Xiao", "title": "ElixirNet: Relation-aware Network Architecture Adaptation for Medical\n  Lesion Detection", "comments": "7 pages, 5 figure, AAAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most advances in medical lesion detection network are limited to subtle\nmodification on the conventional detection network designed for natural images.\nHowever, there exists a vast domain gap between medical images and natural\nimages where the medical image detection often suffers from several\ndomain-specific challenges, such as high lesion/background similarity, dominant\ntiny lesions, and severe class imbalance. Is a hand-crafted detection network\ntailored for natural image undoubtedly good enough over a discrepant medical\nlesion domain? Is there more powerful operations, filters, and sub-networks\nthat better fit the medical lesion detection problem to be discovered? In this\npaper, we introduce a novel ElixirNet that includes three components: 1)\nTruncatedRPN balances positive and negative data for false positive reduction;\n2) Auto-lesion Block is automatically customized for medical images to\nincorporate relation-aware operations among region proposals, and leads to more\nsuitable and efficient classification and localization. 3) Relation transfer\nmodule incorporates the semantic relationship and transfers the relevant\ncontextual information with an interpretable the graph thus alleviates the\nproblem of lack of annotations for all types of lesions. Experiments on\nDeepLesion and Kits19 prove the effectiveness of ElixirNet, achieving\nimprovement of both sensitivity and precision over FPN with fewer parameters.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 05:29:49 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Jiang", "Chenhan", ""], ["Wang", "Shaoju", ""], ["Xu", "Hang", ""], ["Liang", "Xiaodan", ""], ["Xiao", "Nong", ""]]}, {"id": "2003.08771", "submitter": "Tim Oosterhuis", "authors": "Tim Oosterhuis and Lambert Schomaker", "title": "\"Who is Driving around Me?\" Unique Vehicle Instance Classification using\n  Deep Neural Features", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being aware of other traffic is a prerequisite for self-driving cars to\noperate in the real world. In this paper, we show how the intrinsic feature\nmaps of an object detection CNN can be used to uniquely identify vehicles from\na dash-cam feed. Feature maps of a pretrained `YOLO' network are used to create\n700 deep integrated feature signatures (DIFS) from 20 different images of 35\nvehicles from a high resolution dataset and 340 signatures from 20 different\nimages of 17 vehicles of a lower resolution tracking benchmark dataset. The\nYOLO network was trained to classify general object categories, e.g. classify a\ndetected object as a `car' or `truck'. 5-Fold nearest neighbor (1NN)\nclassification was used on DIFS created from feature maps in the middle layers\nof the network to correctly identify unique vehicles at a rate of 96.7\\% for\nthe high resolution data and with a rate of 86.8\\% for the lower resolution\ndata. We conclude that a deep neural detection network trained to distinguish\nbetween different classes can be successfully used to identify different\ninstances belonging to the same class, through the creation of deep integrated\nfeature signatures (DIFS).\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 13:57:46 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Oosterhuis", "Tim", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2003.08773", "submitter": "Eddie Yan", "authors": "Eddie Yan and Yanping Huang", "title": "Do CNNs Encode Data Augmentations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentations are important ingredients in the recipe for training\nrobust neural networks, especially in computer vision. A fundamental question\nis whether neural network features encode data augmentation transformations. To\nanswer this question, we introduce a systematic approach to investigate which\nlayers of neural networks are the most predictive of augmentation\ntransformations. Our approach uses features in pre-trained vision models with\nminimal additional processing to predict common properties transformed by\naugmentation (scale, aspect ratio, hue, saturation, contrast, and brightness).\nSurprisingly, neural network features not only predict data augmentation\ntransformations, but they predict many transformations with high accuracy.\nAfter validating that neural networks encode features corresponding to\naugmentation transformations, we show that these features are encoded in the\nearly layers of modern CNNs, though the augmentation signal fades in deeper\nlayers.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 00:42:23 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 18:02:18 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Yan", "Eddie", ""], ["Huang", "Yanping", ""]]}, {"id": "2003.08774", "submitter": "Agnieszka Grabska-Barwinska", "authors": "Agnieszka Grabska-Barwi\\'nska", "title": "Measuring and improving the quality of visual explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of to explain neural network decisions goes hand in hand with\ntheir safe deployment. Several methods have been proposed to highlight features\nimportant for a given network decision. However, there is no consensus on how\nto measure effectiveness of these methods. We propose a new procedure for\nevaluating explanations. We use it to investigate visual explanations extracted\nfrom a range of possible sources in a neural network. We quantify the benefit\nof combining these sources and challenge a recent appeal for taking bias\nparameters into account. We support our conclusions with a general assessment\nof the impact of bias parameters in ImageNet classifiers\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 00:52:00 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 10:00:58 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Grabska-Barwi\u0144ska", "Agnieszka", ""]]}, {"id": "2003.08777", "submitter": "Zongxian Li", "authors": "Zongxian Li, Qixiang Ye, Chong Zhang, Jingjing Liu, Shijian Lu and\n  Yonghong Tian", "title": "Self-Guided Adaptation: Progressive Representation Alignment for Domain\n  Adaptive Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation (UDA) has achieved unprecedented success in\nimproving the cross-domain robustness of object detection models. However,\nexisting UDA methods largely ignore the instantaneous data distribution during\nmodel learning, which could deteriorate the feature representation given large\ndomain shift. In this work, we propose a Self-Guided Adaptation (SGA) model,\ntarget at aligning feature representation and transferring object detection\nmodels across domains while considering the instantaneous alignment difficulty.\nThe core of SGA is to calculate \"hardness\" factors for sample pairs indicating\ndomain distance in a kernel space. With the hardness factor, the proposed SGA\nadaptively indicates the importance of samples and assigns them different\nconstrains. Indicated by hardness factors, Self-Guided Progressive Sampling\n(SPS) is implemented in an \"easy-to-hard\" way during model adaptation. Using\nmulti-stage convolutional features, SGA is further aggregated to fully align\nhierarchical representations of detection models. Extensive experiments on\ncommonly used benchmarks show that SGA improves the state-of-the-art methods\nwith significant margins, while demonstrating the effectiveness on large domain\nshift.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 13:30:45 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 09:18:10 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Zongxian", ""], ["Ye", "Qixiang", ""], ["Zhang", "Chong", ""], ["Liu", "Jingjing", ""], ["Lu", "Shijian", ""], ["Tian", "Yonghong", ""]]}, {"id": "2003.08788", "submitter": "Debayan Deb", "authors": "Debayan Deb, Divyansh Aggarwal, Anil K. Jain", "title": "Child Face Age-Progression via Deep Feature Aging", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.07538", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a gallery of face images of missing children, state-of-the-art face\nrecognition systems fall short in identifying a child (probe) recovered at a\nlater age. We propose a feature aging module that can age-progress deep face\nfeatures output by a face matcher. In addition, the feature aging module guides\nage-progression in the image space such that synthesized aged faces can be\nutilized to enhance longitudinal face recognition performance of any face\nmatcher without requiring any explicit training. For time lapses larger than 10\nyears (the missing child is found after 10 or more years), the proposed\nage-progression module improves the closed-set identification accuracy of\nFaceNet from 16.53% to 21.44% and CosFace from 60.72% to 66.12% on a child\ncelebrity dataset, namely ITWCC. The proposed method also outperforms\nstate-of-the-art approaches with a rank-1 identification rate of 95.91%,\ncompared to 94.91%, on a public aging dataset, FG-NET, and 99.58%, compared to\n99.50%, on CACD-VS. These results suggest that aging face features enhances the\nability to identify young children who are possible victims of child\ntrafficking or abduction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 23:03:46 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Deb", "Debayan", ""], ["Aggarwal", "Divyansh", ""], ["Jain", "Anil K.", ""]]}, {"id": "2003.08791", "submitter": "Denis Korzhenkov", "authors": "Ivan Anokhin, Pavel Solovev, Denis Korzhenkov, Alexey Kharlamov, Taras\n  Khakhulin, Alexey Silvestrov, Sergey Nikolenko, Victor Lempitsky, Gleb\n  Sterkin", "title": "High-Resolution Daytime Translation Without Domain Labels", "comments": "accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling daytime changes in high resolution photographs, e.g., re-rendering\nthe same scene under different illuminations typical for day, night, or dawn,\nis a challenging image manipulation task. We present the high-resolution\ndaytime translation (HiDT) model for this task. HiDT combines a generative\nimage-to-image model and a new upsampling scheme that allows to apply image\ntranslation at high resolution. The model demonstrates competitive results in\nterms of both commonly used GAN metrics and human evaluation. Importantly, this\ngood performance comes as a result of training on a dataset of still landscape\nimages with no daytime labels available. Our results are available at\nhttps://saic-mdal.github.io/HiDT/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 13:59:31 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 11:59:50 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Anokhin", "Ivan", ""], ["Solovev", "Pavel", ""], ["Korzhenkov", "Denis", ""], ["Kharlamov", "Alexey", ""], ["Khakhulin", "Taras", ""], ["Silvestrov", "Alexey", ""], ["Nikolenko", "Sergey", ""], ["Lempitsky", "Victor", ""], ["Sterkin", "Gleb", ""]]}, {"id": "2003.08793", "submitter": "Jingda Du", "authors": "Zhenshen Qu, Jingda Du, Yong Cao, Qiuyu Guan and Pengbo Zhao", "title": "Deep Active Learning for Remote Sensing Object Detection", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, CNN object detectors have achieved high accuracy on remote sensing\nimages but require huge labor and time costs on annotation. In this paper, we\npropose a new uncertainty-based active learning which can select images with\nmore information for annotation and detector can still reach high performance\nwith a fraction of the training images. Our method not only analyzes objects'\nclassification uncertainty to find least confident objects but also considers\ntheir regression uncertainty to declare outliers. Besides, we bring out two\nextra weights to overcome two difficulties in remote sensing datasets,\nclass-imbalance and difference in images' objects amount. We experiment our\nactive learning algorithm on DOTA dataset with CenterNet as object detector. We\nachieve same-level performance as full supervision with only half images. We\neven override full supervision with 55% images and augmented weights on least\nconfident images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 15:57:36 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Qu", "Zhenshen", ""], ["Du", "Jingda", ""], ["Cao", "Yong", ""], ["Guan", "Qiuyu", ""], ["Zhao", "Pengbo", ""]]}, {"id": "2003.08797", "submitter": "Maciej Pajak", "authors": "Shayne Shaw, Maciej Pajak, Aneta Lisowska, Sotirios A Tsaftaris,\n  Alison Q O'Neil", "title": "Teacher-Student chain for efficient semi-supervised histology image\n  classification", "comments": "AI for Affordable Healthcare (AI4AH) workshop at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning shows great potential for the domain of digital pathology. An\nautomated digital pathology system could serve as a second reader, perform\ninitial triage in large screening studies, or assist in reporting. However, it\nis expensive to exhaustively annotate large histology image databases, since\nmedical specialists are a scarce resource. In this paper, we apply the\nsemi-supervised teacher-student knowledge distillation technique proposed by\nYalniz et al. (2019) to the task of quantifying prognostic features in\ncolorectal cancer. We obtain accuracy improvements through extending this\napproach to a chain of students, where each student's predictions are used to\ntrain the next student i.e. the student becomes the teacher. Using the chain\napproach, and only 0.5% labelled data (the remaining 99.5% in the unlabelled\npool), we match the accuracy of training on 100% labelled data. At lower\npercentages of labelled data, similar gains in accuracy are seen, allowing some\nrecovery of accuracy even from a poor initial choice of labelled training set.\nIn conclusion, this approach shows promise for reducing the annotation burden,\nthus increasing the affordability of automated digital pathology systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:01:43 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 09:53:31 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Shaw", "Shayne", ""], ["Pajak", "Maciej", ""], ["Lisowska", "Aneta", ""], ["Tsaftaris", "Sotirios A", ""], ["O'Neil", "Alison Q", ""]]}, {"id": "2003.08798", "submitter": "Joseph K J", "authors": "K J Joseph, Jathushan Rajasegaran, Salman Khan, Fahad Shahbaz Khan,\n  Vineeth Balasubramanian", "title": "Incremental Object Detection via Meta-Learning", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a real-world setting, object instances from new classes can be\ncontinuously encountered by object detectors. When existing object detectors\nare applied to such scenarios, their performance on old classes deteriorates\nsignificantly. A few efforts have been reported to address this limitation, all\nof which apply variants of knowledge distillation to avoid catastrophic\nforgetting. We note that although distillation helps to retain previous\nlearning, it obstructs fast adaptability to new tasks, which is a critical\nrequirement for incremental learning. In this pursuit, we propose a\nmeta-learning approach that learns to reshape model gradients, such that\ninformation across incremental tasks is optimally shared. This ensures a\nseamless information transfer via a meta-learned gradient preconditioning that\nminimizes forgetting and maximizes knowledge transfer. In comparison to\nexisting meta-learning methods, our approach is task-agnostic, allows\nincremental addition of new-classes and scales to high-capacity models for\nobject detection. We evaluate our approach on a variety of incremental learning\nsettings defined on PASCAL-VOC and MS COCO datasets, where our approach\nperforms favourably well against state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 13:40:00 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 06:23:43 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Joseph", "K J", ""], ["Rajasegaran", "Jathushan", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Balasubramanian", "Vineeth", ""]]}, {"id": "2003.08799", "submitter": "Irtiza Hasan", "authors": "Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, and Ling\n  Shao", "title": "Generalizable Pedestrian Detection: The Elephant In The Room", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is used in many vision based applications ranging from\nvideo surveillance to autonomous driving. Despite achieving high performance,\nit is still largely unknown how well existing detectors generalize to unseen\ndata. This is important because a practical detector should be ready to use in\nvarious scenarios in applications. To this end, we conduct a comprehensive\nstudy in this paper, using a general principle of direct cross-dataset\nevaluation. Through this study, we find that existing state-of-the-art\npedestrian detectors, though perform quite well when trained and tested on the\nsame dataset, generalize poorly in cross dataset evaluation. We demonstrate\nthat there are two reasons for this trend. Firstly, their designs (e.g. anchor\nsettings) may be biased towards popular benchmarks in the traditional\nsingle-dataset training and test pipeline, but as a result largely limit their\ngeneralization capability. Secondly, the training source is generally not dense\nin pedestrians and diverse in scenarios. Under direct cross-dataset evaluation,\nsurprisingly, we find that a general purpose object detector, without\npedestrian-tailored adaptation in design, generalizes much better compared to\nexisting state-of-the-art pedestrian detectors. Furthermore, we illustrate that\ndiverse and dense datasets, collected by crawling the web, serve to be an\nefficient source of pre-training for pedestrian detection. Accordingly, we\npropose a progressive training pipeline and find that it works well for\nautonomous-driving oriented pedestrian detection. Consequently, the study\nconducted in this paper suggests that more emphasis should be put on\ncross-dataset evaluation for the future design of generalizable pedestrian\ndetectors. Code and models can be accessed at\nhttps://github.com/hasanirtiza/Pedestron.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 14:14:52 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 03:40:14 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 20:51:57 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 09:49:46 GMT"}, {"version": "v5", "created": "Thu, 9 Jul 2020 06:52:34 GMT"}, {"version": "v6", "created": "Thu, 30 Jul 2020 13:41:03 GMT"}, {"version": "v7", "created": "Wed, 9 Dec 2020 08:56:09 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Hasan", "Irtiza", ""], ["Liao", "Shengcai", ""], ["Li", "Jinpeng", ""], ["Akram", "Saad Ullah", ""], ["Shao", "Ling", ""]]}, {"id": "2003.08802", "submitter": "Maosen Li", "authors": "Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, Qi Tian", "title": "Dynamic Multiscale Graph Neural Networks for 3D Skeleton-Based Human\n  Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel dynamic multiscale graph neural networks (DMGNN) to predict\n3D skeleton-based human motions. The core idea of DMGNN is to use a multiscale\ngraph to comprehensively model the internal relations of a human body for\nmotion feature learning. This multiscale graph is adaptive during training and\ndynamic across network layers. Based on this graph, we propose a multiscale\ngraph computational unit (MGCU) to extract features at individual scales and\nfuse features across scales. The entire model is action-category-agnostic and\nfollows an encoder-decoder framework. The encoder consists of a sequence of\nMGCUs to learn motion features. The decoder uses a proposed graph-based gate\nrecurrent unit to generate future poses. Extensive experiments show that the\nproposed DMGNN outperforms state-of-the-art methods in both short and long-term\npredictions on the datasets of Human 3.6M and CMU Mocap. We further investigate\nthe learned multiscale graphs for the interpretability. The codes could be\ndownloaded from https://github.com/limaosen0/DMGNN.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 02:49:51 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Li", "Maosen", ""], ["Chen", "Siheng", ""], ["Zhao", "Yangheng", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""], ["Tian", "Qi", ""]]}, {"id": "2003.08803", "submitter": "Asifullah Khan", "authors": "Anabia Sohail, Muhammad Ahsan Mukhtar, Asifullah Khan, Muhammad Mohsin\n  Zafar, Aneela Zameer, Saranjam Khan", "title": "Deep Object Detection based Mitosis Analysis in Breast Cancer\n  Histopathological Images", "comments": "Tables: 4, Figures 11, Pages: 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical evaluation of breast tissue biopsies for mitotic nuclei detection\nis considered an important prognostic biomarker in tumor grading and cancer\nprogression. However, automated mitotic nuclei detection poses several\nchallenges because of the unavailability of pixel-level annotations, different\nmorphological configurations of mitotic nuclei, their sparse representation,\nand close resemblance with non-mitotic nuclei. These challenges undermine the\nprecision of the automated detection model and thus make detection difficult in\na single phase. This work proposes an end-to-end detection system for mitotic\nnuclei identification in breast cancer histopathological images. Deep object\ndetection-based Mask R-CNN is adapted for mitotic nuclei detection that\ninitially selects the candidate mitotic region with maximum recall. However, in\nthe second phase, these candidate regions are refined by multi-object loss\nfunction to improve the precision. The performance of the proposed detection\nmodel shows improved discrimination ability (F-score of 0.86) for mitotic\nnuclei with significant precision (0.86) as compared to the two-stage detection\nmodels (F-score of 0.701) on TUPAC16 dataset. Promising results suggest that\nthe deep object detection-based model has the potential to learn the\ncharacteristic features of mitotic nuclei from weakly annotated data and\nsuggests that it can be adapted for the identification of other nuclear bodies\nin histopathological images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 00:51:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Sohail", "Anabia", ""], ["Mukhtar", "Muhammad Ahsan", ""], ["Khan", "Asifullah", ""], ["Zafar", "Muhammad Mohsin", ""], ["Zameer", "Aneela", ""], ["Khan", "Saranjam", ""]]}, {"id": "2003.08806", "submitter": "Zhengyang Wu", "authors": "Zhengyang Wu, Srivignesh Rajendran, Tarrence van As, Joelle\n  Zimmermann, Vijay Badrinarayanan, Andrew Rabinovich", "title": "MagicEyes: A Large Scale Eye Gaze Estimation Dataset for Mixed Reality", "comments": "arXiv admin note: substantial text overlap with arXiv:1908.09060", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of Virtual and Mixed Reality (XR) devices, eye tracking\nhas received significant attention in the computer vision community. Eye gaze\nestimation is a crucial component in XR -- enabling energy efficient rendering,\nmulti-focal displays, and effective interaction with content. In head-mounted\nXR devices, the eyes are imaged off-axis to avoid blocking the field of view.\nThis leads to increased challenges in inferring eye related quantities and\nsimultaneously provides an opportunity to develop accurate and robust learning\nbased approaches. To this end, we present MagicEyes, the first large scale eye\ndataset collected using real MR devices with comprehensive ground truth\nlabeling. MagicEyes includes $587$ subjects with $80,000$ images of\nhuman-labeled ground truth and over $800,000$ images with gaze target labels.\nWe evaluate several state-of-the-art methods on MagicEyes and also propose a\nnew multi-task EyeNet model designed for detecting the cornea, glints and pupil\nalong with eye segmentation in a single forward pass.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 08:23:57 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Wu", "Zhengyang", ""], ["Rajendran", "Srivignesh", ""], ["van As", "Tarrence", ""], ["Zimmermann", "Joelle", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "2003.08808", "submitter": "Mohammad Hamed Mozaffari", "authors": "M. Hamed Mozaffari, Won-Sook Lee", "title": "Deep Learning for Automatic Tracking of Tongue Surface in Real-time\n  Ultrasound Videos, Landmarks instead of Contours", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One usage of medical ultrasound imaging is to visualize and characterize\nhuman tongue shape and motion during a real-time speech to study healthy or\nimpaired speech production. Due to the low-contrast characteristic and noisy\nnature of ultrasound images, it might require expertise for non-expert users to\nrecognize tongue gestures in applications such as visual training of a second\nlanguage. Moreover, quantitative analysis of tongue motion needs the tongue\ndorsum contour to be extracted, tracked, and visualized. Manual tongue contour\nextraction is a cumbersome, subjective, and error-prone task. Furthermore, it\nis not a feasible solution for real-time applications. The growth of deep\nlearning has been vigorously exploited in various computer vision tasks,\nincluding ultrasound tongue contour tracking. In the current methods, the\nprocess of tongue contour extraction comprises two steps of image segmentation\nand post-processing. This paper presents a new novel approach of automatic and\nreal-time tongue contour tracking using deep neural networks. In the proposed\nmethod, instead of the two-step procedure, landmarks of the tongue surface are\ntracked. This novel idea enables researchers in this filed to benefits from\navailable previously annotated databases to achieve high accuracy results. Our\nexperiment disclosed the outstanding performances of the proposed technique in\nterms of generalization, performance, and accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 00:38:13 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Mozaffari", "M. Hamed", ""], ["Lee", "Won-Sook", ""]]}, {"id": "2003.08813", "submitter": "Gen Luo", "authors": "Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng\n  Deng and Rongrong Ji", "title": "Multi-task Collaborative Network for Joint Referring Expression\n  Comprehension and Segmentation", "comments": "accpected by CVPR2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Referring expression comprehension (REC) and segmentation (RES) are two\nhighly-related tasks, which both aim at identifying the referent according to a\nnatural language expression. In this paper, we propose a novel Multi-task\nCollaborative Network (MCN) to achieve a joint learning of REC and RES for the\nfirst time. In MCN, RES can help REC to achieve better language-vision\nalignment, while REC can help RES to better locate the referent. In addition,\nwe address a key challenge in this multi-task setup, i.e., the prediction\nconflict, with two innovative designs namely, Consistency Energy Maximization\n(CEM) and Adaptive Soft Non-Located Suppression (ASNLS). Specifically, CEM\nenables REC and RES to focus on similar visual regions by maximizing the\nconsistency energy between two tasks. ASNLS supresses the response of unrelated\nregions in RES based on the prediction of REC. To validate our model, we\nconduct extensive experiments on three benchmark datasets of REC and RES, i.e.,\nRefCOCO, RefCOCO+ and RefCOCOg. The experimental results report the significant\nperformance gains of MCN over all existing methods, i.e., up to +7.13% for REC\nand +11.50% for RES over SOTA, which well confirm the validity of our model for\njoint REC and RES learning.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 14:25:18 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Luo", "Gen", ""], ["Zhou", "Yiyi", ""], ["Sun", "Xiaoshuai", ""], ["Cao", "Liujuan", ""], ["Wu", "Chenglin", ""], ["Deng", "Cheng", ""], ["Ji", "Rongrong", ""]]}, {"id": "2003.08817", "submitter": "Adrian Bowman Prof.", "authors": "Stanislav Katina and Liberty Vittert and Adrian W. Bowman", "title": "Functional Data Analysis and Visualisation of Three-dimensional Surface\n  Shape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of high resolution imaging has made data on surface shape\nwidespread. Methods for the analysis of shape based on landmarks are well\nestablished but high resolution data require a functional approach. The\nstarting point is a systematic and consistent description of each surface\nshape. Three innovative forms of analysis are then introduced. The first uses\nsurface integration to address issues of registration, principal component\nanalysis and the measurement of asymmetry, all in functional form.\nComputational issues are handled through discrete approximations to integrals,\nbased in this case on appropriate surface area weighted sums. The second\ninnovation is to focus on sub-spaces where interesting behaviour such as group\ndifferences are exhibited, rather than on individual principal components. The\nthird innovation concerns the comparison of individual shapes with a relevant\ncontrol set, where the concept of a normal range is extended to the highly\nmultivariate setting of surface shape. This has particularly strong\napplications to medical contexts where the assessment of individual patients is\nvery important. All of these ideas are developed and illustrated in the\nimportant context of human facial shape, with a strong emphasis on the\neffective visual communication of effects of interest.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 13:54:24 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Katina", "Stanislav", ""], ["Vittert", "Liberty", ""], ["Bowman", "Adrian W.", ""]]}, {"id": "2003.08818", "submitter": "Mengjiao Hu", "authors": "Mengjiao Hu, Kang Sim, Juan Helen Zhou, Xudong Jiang, Cuntai Guan", "title": "Brain MRI-based 3D Convolutional Neural Networks for Classification of\n  Schizophrenia and Controls", "comments": "4 PAGES", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Network (CNN) has been successfully applied on\nclassification of both natural images and medical images but not yet been\napplied to differentiating patients with schizophrenia from healthy controls.\nGiven the subtle, mixed, and sparsely distributed brain atrophy patterns of\nschizophrenia, the capability of automatic feature learning makes CNN a\npowerful tool for classifying schizophrenia from controls as it removes the\nsubjectivity in selecting relevant spatial features. To examine the feasibility\nof applying CNN to classification of schizophrenia and controls based on\nstructural Magnetic Resonance Imaging (MRI), we built 3D CNN models with\ndifferent architectures and compared their performance with a handcrafted\nfeature-based machine learning approach. Support vector machine (SVM) was used\nas classifier and Voxel-based Morphometry (VBM) was used as feature for\nhandcrafted feature-based machine learning. 3D CNN models with sequential\narchitecture, inception module and residual module were trained from scratch.\nCNN models achieved higher cross-validation accuracy than handcrafted\nfeature-based machine learning. Moreover, testing on an independent dataset, 3D\nCNN models greatly outperformed handcrafted feature-based machine learning.\nThis study underscored the potential of CNN for identifying patients with\nschizophrenia using 3D brain MR images and paved the way for imaging-based\nindividual-level diagnosis and prognosis in psychiatric disorders.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 10:05:21 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Hu", "Mengjiao", ""], ["Sim", "Kang", ""], ["Zhou", "Juan Helen", ""], ["Jiang", "Xudong", ""], ["Guan", "Cuntai", ""]]}, {"id": "2003.08821", "submitter": "Luke Darlow", "authors": "Luke Nicholas Darlow, Amos Storkey", "title": "DHOG: Deep Hierarchical Object Grouping", "comments": "15 pages, submitted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, a number of competitive methods have tackled unsupervised\nrepresentation learning by maximising the mutual information between the\nrepresentations produced from augmentations. The resulting representations are\nthen invariant to stochastic augmentation strategies, and can be used for\ndownstream tasks such as clustering or classification. Yet data augmentations\npreserve many properties of an image and so there is potential for a suboptimal\nchoice of representation that relies on matching easy-to-find features in the\ndata. We demonstrate that greedy or local methods of maximising mutual\ninformation (such as stochastic gradient optimisation) discover local optima of\nthe mutual information criterion; the resulting representations are also\nless-ideally suited to complex downstream tasks. Earlier work has not\nspecifically identified or addressed this issue. We introduce deep hierarchical\nobject grouping (DHOG) that computes a number of distinct discrete\nrepresentations of images in a hierarchical order, eventually generating\nrepresentations that better optimise the mutual information objective. We also\nfind that these representations align better with the downstream task of\ngrouping into underlying object classes. We tested DHOG on unsupervised\nclustering, which is a natural downstream test as the target representation is\na discrete labelling of the data. We achieved new state-of-the-art results on\nthe three main benchmarks without any prefiltering or Sobel-edge detection that\nproved necessary for many previous methods to work. We obtain accuracy\nimprovements of: 4.3% on CIFAR-10, 1.5% on CIFAR-100-20, and 7.2% on SVHN.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 14:11:48 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Darlow", "Luke Nicholas", ""], ["Storkey", "Amos", ""]]}, {"id": "2003.08834", "submitter": "Zhiwen Shao", "authors": "Zhiwen Shao, Zhilei Liu, Jianfei Cai, Lizhuang Ma", "title": "J$\\hat{\\text{A}}$A-Net: Joint Facial Action Unit Detection and Face\n  Alignment via Adaptive Attention", "comments": "This paper is the extended version of arXiv:1803.05588, and is\n  accepted by International Journal of Computer Vision", "journal-ref": null, "doi": "10.1007/s11263-020-01378-z", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit (AU) detection and face alignment are two highly\ncorrelated tasks, since facial landmarks can provide precise AU locations to\nfacilitate the extraction of meaningful local features for AU detection.\nHowever, most existing AU detection works handle the two tasks independently by\ntreating face alignment as a preprocessing, and often use landmarks to\npredefine a fixed region or attention for each AU. In this paper, we propose a\nnovel end-to-end deep learning framework for joint AU detection and face\nalignment, which has not been explored before. In particular, multi-scale\nshared feature is learned firstly, and high-level feature of face alignment is\nfed into AU detection. Moreover, to extract precise local features, we propose\nan adaptive attention learning module to refine the attention map of each AU\nadaptively. Finally, the assembled local features are integrated with face\nalignment feature and global feature for AU detection. Extensive experiments\ndemonstrate that our framework (i) significantly outperforms the\nstate-of-the-art AU detection methods on the challenging BP4D, DISFA, GFT and\nBP4D+ benchmarks, (ii) can adaptively capture the irregular region of each AU,\n(iii) achieves competitive performance for face alignment, and (iv) also works\nwell under partial occlusions and non-frontal poses. The code for our method is\navailable at https://github.com/ZhiwenShao/PyTorch-JAANet.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 12:50:19 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 04:03:52 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 03:08:58 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Shao", "Zhiwen", ""], ["Liu", "Zhilei", ""], ["Cai", "Jianfei", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2003.08854", "submitter": "Oliver Groth", "authors": "Oliver Groth, Chia-Man Hung, Andrea Vedaldi, Ingmar Posner", "title": "Goal-Conditioned End-to-End Visuomotor Control for Versatile Skill\n  Primitives", "comments": "revised manuscript with additional baselines and generalisation\n  experiments; 11 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Visuomotor control (VMC) is an effective means of achieving basic\nmanipulation tasks such as pushing or pick-and-place from raw images.\nConditioning VMC on desired goal states is a promising way of achieving\nversatile skill primitives. However, common conditioning schemes either rely on\ntask-specific fine tuning - e.g. using one-shot imitation learning (IL) - or on\nsampling approaches using a forward model of scene dynamics i.e.\nmodel-predictive control (MPC), leaving deployability and planning horizon\nseverely limited. In this paper we propose a conditioning scheme which avoids\nthese pitfalls by learning the controller and its conditioning in an end-to-end\nmanner. Our model predicts complex action sequences based directly on a dynamic\nimage representation of the robot motion and the distance to a given target\nobservation. In contrast to related works, this enables our approach to\nefficiently perform complex manipulation tasks from raw image observations\nwithout predefined control primitives or test time demonstrations. We report\nsignificant improvements in task success over representative MPC and IL\nbaselines. We also demonstrate our model's generalisation capabilities in\nchallenging, unseen tasks featuring visual noise, cluttered scenes and unseen\nobject geometries.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:04:37 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 17:58:17 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Groth", "Oliver", ""], ["Hung", "Chia-Man", ""], ["Vedaldi", "Andrea", ""], ["Posner", "Ingmar", ""]]}, {"id": "2003.08865", "submitter": "Yuan Gao", "authors": "Yuan Gao, Robert Bregovic, Reinhard Koch and Atanas Gotchev", "title": "DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one\nof the most effective methods for Densely-Sampled Light Field (DSLF)\nreconstruction. The ST-based DSLF reconstruction typically relies on an\niterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse\nregularization in shearlet domain, involving dozens of transformations between\nimage domain and shearlet domain, which are in general time-consuming. To\novercome this limitation, a novel learning-based ST approach, referred to as\nDeep Residual Shearlet Transform (DRST), is proposed in this paper.\nSpecifically, for an input sparsely-sampled EPI, DRST employs a deep fully\nConvolutional Neural Network (CNN) to predict the residuals of the shearlet\ncoefficients in shearlet domain in order to reconstruct a densely-sampled EPI\nin image domain. The DRST network is trained on synthetic Sparsely-Sampled\nLight Field (SSLF) data only by leveraging elaborately-designed masks.\nExperimental results on three challenging real-world light field evaluation\ndatasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the\nsuperiority of the proposed learning-based DRST approach over the\nnon-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a\n2.4x speedup over ST, at least.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:28:08 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Gao", "Yuan", ""], ["Bregovic", "Robert", ""], ["Koch", "Reinhard", ""], ["Gotchev", "Atanas", ""]]}, {"id": "2003.08866", "submitter": "Zheng Zhang", "authors": "Zhenda Xie, Zheng Zhang, Xizhou Zhu, Gao Huang, Stephen Lin", "title": "Spatially Adaptive Inference with Stochastic Feature Sampling and\n  Interpolation", "comments": "ECCV2020(Oral), Code:\n  https://github.com/zdaxie/SpatiallyAdaptiveInference-Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the feature maps of CNNs, there commonly exists considerable spatial\nredundancy that leads to much repetitive processing. Towards reducing this\nsuperfluous computation, we propose to compute features only at sparsely\nsampled locations, which are probabilistically chosen according to activation\nresponses, and then densely reconstruct the feature map with an efficient\ninterpolation procedure. With this sampling-interpolation scheme, our network\navoids expending computation on spatial locations that can be effectively\ninterpolated, while being robust to activation prediction errors through\nbroadly distributed sampling. A technical challenge of this sampling-based\napproach is that the binary decision variables for representing discrete\nsampling locations are non-differentiable, making them incompatible with\nbackpropagation. To circumvent this issue, we make use of a reparameterization\ntrick based on the Gumbel-Softmax distribution, with which backpropagation can\niterate these variables towards binary values. The presented network is\nexperimentally shown to save substantial computation while maintaining accuracy\nover a variety of computer vision tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:36:31 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 17:43:16 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 05:36:19 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2020 07:40:10 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Xie", "Zhenda", ""], ["Zhang", "Zheng", ""], ["Zhu", "Xizhou", ""], ["Huang", "Gao", ""], ["Lin", "Stephen", ""]]}, {"id": "2003.08870", "submitter": "Tongxue Zhou", "authors": "Tongxue Zhou, St\\'ephane Canu, Pierre Vera, Su Ruan", "title": "Brain tumor segmentation with missing modalities via latent multi-source\n  correlation representation", "comments": "10 pages, 6 figures, accepted by MICCAI 2020. arXiv admin note: text\n  overlap with arXiv:2102.03111", "journal-ref": "MICCAI 2020 pp. 533-541", "doi": "10.1007/978-3-030-59719-1_52", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal MR images can provide complementary information for accurate brain\ntumor segmentation. However, it's common to have missing imaging modalities in\nclinical practice. Since there exists a strong correlation between multi\nmodalities, a novel correlation representation block is proposed to specially\ndiscover the latent multi-source correlation. Thanks to the obtained\ncorrelation representation, the segmentation becomes more robust in the case of\nmissing modalities. The model parameter estimation module first maps the\nindividual representation produced by each encoder to obtain independent\nparameters, then, under these parameters, the correlation expression module\ntransforms all the individual representations to form a latent multi-source\ncorrelation representation. Finally, the correlation representations across\nmodalities are fused via the attention mechanism into a shared representation\nto emphasize the most important features for segmentation. We evaluate our\nmodel on BraTS 2018 datasets, it outperforms the current state-of-the-art\nmethod and produces robust results when one or more modalities are missing.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:47:36 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 10:26:29 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 09:39:20 GMT"}, {"version": "v4", "created": "Sat, 3 Oct 2020 09:13:02 GMT"}, {"version": "v5", "created": "Tue, 20 Apr 2021 12:27:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Tongxue", ""], ["Canu", "St\u00e9phane", ""], ["Vera", "Pierre", ""], ["Ruan", "Su", ""]]}, {"id": "2003.08872", "submitter": "Shai Bagon", "authors": "Liad Pollak Zuckerman, Eyal Naor, George Pisha, Shai Bagon, Michal\n  Irani", "title": "Across Scales & Across Dimensions: Temporal Super-Resolution using Deep\n  Internal Learning", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  When a very fast dynamic event is recorded with a low-framerate camera, the\nresulting video suffers from severe motion blur (due to exposure time) and\nmotion aliasing (due to low sampling rate in time). True Temporal\nSuper-Resolution (TSR) is more than just Temporal-Interpolation (increasing\nframerate). It can also recover new high temporal frequencies beyond the\ntemporal Nyquist limit of the input video, thus resolving both motion-blur and\nmotion-aliasing effects that temporal frame interpolation (as sophisticated as\nit maybe) cannot undo. In this paper we propose a \"Deep Internal Learning\"\napproach for true TSR. We train a video-specific CNN on examples extracted\ndirectly from the low-framerate input video. Our method exploits the strong\nrecurrence of small space-time patches inside a single video sequence, both\nwithin and across different spatio-temporal scales of the video. We further\nobserve (for the first time) that small space-time patches recur also\nacross-dimensions of the video sequence - i.e., by swapping the spatial and\ntemporal dimensions. In particular, the higher spatial resolution of video\nframes provides strong examples as to how to increase the temporal resolution\nof that video. Such internal video-specific examples give rise to strong\nself-supervision, requiring no data but the input video itself. This results in\nZero-Shot Temporal-SR of complex videos, which removes both motion blur and\nmotion aliasing, outperforming previous supervised methods trained on external\nvideo datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 15:53:01 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 06:22:43 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 10:09:29 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zuckerman", "Liad Pollak", ""], ["Naor", "Eyal", ""], ["Pisha", "George", ""], ["Bagon", "Shai", ""], ["Irani", "Michal", ""]]}, {"id": "2003.08885", "submitter": "Dor Verbin", "authors": "Dor Verbin, Steven J. Gortler, Todd Zickler", "title": "Unique Geometry and Texture from Corresponding Image Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sufficient condition for recovering unique texture and\nviewpoints from unknown orthographic projections of a flat texture process. We\nshow that four observations are sufficient in general, and we characterize the\nambiguous cases. The results are applicable to shape from texture and\ntexture-based structure from motion.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 16:14:13 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 22:46:52 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Verbin", "Dor", ""], ["Gortler", "Steven J.", ""], ["Zickler", "Todd", ""]]}, {"id": "2003.08890", "submitter": "Vincent Andrearczyk", "authors": "Vincent Andrearczyk, Julien Fageot, Valentin Oreiller, Xavier Montet,\n  Adrien Depeursinge", "title": "Local Rotation Invariance in 3D CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Rotation Invariant (LRI) image analysis was shown to be fundamental\nin many applications and in particular in medical imaging where local\nstructures of tissues occur at arbitrary rotations. LRI constituted the\ncornerstone of several breakthroughs in texture analysis, including Local\nBinary Patterns (LBP), Maximum Response 8 (MR8) and steerable filterbanks.\nWhereas globally rotation invariant Convolutional Neural Networks (CNN) were\nrecently proposed, LRI was very little investigated in the context of deep\nlearning. LRI designs allow learning filters accounting for all orientations,\nwhich enables a drastic reduction of trainable parameters and training data\nwhen compared to standard 3D CNNs. In this paper, we propose and compare\nseveral methods to obtain LRI CNNs with directional sensitivity. Two methods\nuse orientation channels (responses to rotated kernels), either by explicitly\nrotating the kernels or using steerable filters. These orientation channels\nconstitute a locally rotation equivariant representation of the data. Local\npooling across orientations yields LRI image analysis. Steerable filters are\nused to achieve a fine and efficient sampling of 3D rotations as well as a\nreduction of trainable parameters and operations, thanks to a parametric\nrepresentations involving solid Spherical Harmonics (SH), which are products of\nSH with associated learned radial profiles.Finally, we investigate a third\nstrategy to obtain LRI based on rotational invariants calculated from responses\nto a learned set of solid SHs. The proposed methods are evaluated and compared\nto standard CNNs on 3D datasets including synthetic textured volumes composed\nof rotated patterns, and pulmonary nodule classification in CT. The results\nshow the importance of LRI image analysis while resulting in a drastic\nreduction of trainable parameters, outperforming standard 3D CNNs trained with\ndata augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 16:24:49 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Andrearczyk", "Vincent", ""], ["Fageot", "Julien", ""], ["Oreiller", "Valentin", ""], ["Montet", "Xavier", ""], ["Depeursinge", "Adrien", ""]]}, {"id": "2003.08897", "submitter": "Longteng Guo", "authors": "Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, and Hanqing\n  Lu", "title": "Normalized and Geometry-Aware Self-Attention Network for Image\n  Captioning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-attention (SA) network has shown profound value in image captioning. In\nthis paper, we improve SA from two aspects to promote the performance of image\ncaptioning. First, we propose Normalized Self-Attention (NSA), a\nreparameterization of SA that brings the benefits of normalization inside SA.\nWhile normalization is previously only applied outside SA, we introduce a novel\nnormalization method and demonstrate that it is both possible and beneficial to\nperform it on the hidden activations inside SA. Second, to compensate for the\nmajor limit of Transformer that it fails to model the geometry structure of the\ninput objects, we propose a class of Geometry-aware Self-Attention (GSA) that\nextends SA to explicitly and efficiently consider the relative geometry\nrelations between the objects in the image. To construct our image captioning\nmodel, we combine the two modules and apply it to the vanilla self-attention\nnetwork. We extensively evaluate our proposals on MS-COCO image captioning\ndataset and superior results are achieved when comparing to state-of-the-art\napproaches. Further experiments on three challenging tasks, i.e. video\ncaptioning, machine translation, and visual question answering, show the\ngenerality of our methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 16:54:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Guo", "Longteng", ""], ["Liu", "Jing", ""], ["Zhu", "Xinxin", ""], ["Yao", "Peng", ""], ["Lu", "Shichen", ""], ["Lu", "Hanqing", ""]]}, {"id": "2003.08907", "submitter": "Brandon Carter", "authors": "Brandon Carter, Siddhartha Jain, Jonas Mueller, David Gifford", "title": "Overinterpretation reveals image classification model pathologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classifiers are typically scored on their test set accuracy, but high\naccuracy can mask a subtle type of model failure. We find that high scoring\nconvolutional neural networks (CNN) exhibit troubling pathologies that allow\nthem to display high accuracy even in the absence of semantically salient\nfeatures. When a model provides a high-confidence decision without salient\nsupporting input features we say that the classifier has overinterpreted its\ninput, finding too much class-evidence in patterns that appear nonsensical to\nhumans. Here, we demonstrate that state of the art neural networks for CIFAR-10\nand ImageNet suffer from overinterpretation, and find CIFAR-10 trained models\nmake confident predictions even when 95% of an input image has been masked and\nhumans are unable to discern salient features in the remaining pixel subset.\nAlthough these patterns portend potential model fragility in real-world\ndeployment, they are in fact valid statistical patterns of the image\nclassification benchmark that alone suffice to attain high test accuracy. We\nfind that ensembling strategies can help mitigate model overinterpretation, and\nclassifiers which rely on more semantically meaningful features can improve\naccuracy over both the test set and out-of-distribution images from a different\nsource than the training data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:12:23 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Carter", "Brandon", ""], ["Jain", "Siddhartha", ""], ["Mueller", "Jonas", ""], ["Gifford", "David", ""]]}, {"id": "2003.08932", "submitter": "Shuyang Gu", "authors": "Shuyang Gu, Jianmin Bao, Dong Chen, Fang Wen", "title": "GIQA: Generated Image Quality Assessment", "comments": "ECCV2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) have achieved impressive results\ntoday, but not all generated images are perfect. A number of quantitative\ncriteria have recently emerged for generative model, but none of them are\ndesigned for a single generated image. In this paper, we propose a new research\ntopic, Generated Image Quality Assessment (GIQA), which quantitatively\nevaluates the quality of each generated image. We introduce three GIQA\nalgorithms from two perspectives: learning-based and data-based. We evaluate a\nnumber of images generated by various recent GAN models on different datasets\nand demonstrate that they are consistent with human assessments. Furthermore,\nGIQA is available to many applications, like separately evaluating the realism\nand diversity of generative models, and enabling online hard negative mining\n(OHEM) in the training of GANs to improve the results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:56:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 05:52:49 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 09:57:48 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Gu", "Shuyang", ""], ["Bao", "Jianmin", ""], ["Chen", "Dong", ""], ["Wen", "Fang", ""]]}, {"id": "2003.08933", "submitter": "Ayan Sinha", "authors": "Ayan Sinha, Zak Murez, James Bartolozzi, Vijay Badrinarayanan and\n  Andrew Rabinovich", "title": "DELTAS: Depth Estimation by Learning Triangulation And densification of\n  Sparse points", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view stereo (MVS) is the golden mean between the accuracy of active\ndepth sensing and the practicality of monocular depth estimation. Cost volume\nbased approaches employing 3D convolutional neural networks (CNNs) have\nconsiderably improved the accuracy of MVS systems. However, this accuracy comes\nat a high computational cost which impedes practical adoption. Distinct from\ncost volume approaches, we propose an efficient depth estimation approach by\nfirst (a) detecting and evaluating descriptors for interest points, then (b)\nlearning to match and triangulate a small set of interest points, and finally\n(c) densifying this sparse set of 3D points using CNNs. An end-to-end network\nefficiently performs all three steps within a deep learning framework and\ntrained with intermediate 2D image and 3D geometric supervision, along with\ndepth supervision. Crucially, our first step complements pose estimation using\ninterest point detection and descriptor learning. We demonstrate\nstate-of-the-art results on depth estimation with lower compute for different\nscene lengths. Furthermore, our method generalizes to newer environments and\nthe descriptors output by our network compare favorably to strong baselines.\nCode is available at https://github.com/magicleap/DELTAS\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:56:41 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 17:59:12 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Sinha", "Ayan", ""], ["Murez", "Zak", ""], ["Bartolozzi", "James", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "2003.08934", "submitter": "Ben Mildenhall", "authors": "Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T.\n  Barron, Ravi Ramamoorthi, Ren Ng", "title": "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis", "comments": "ECCV 2020 (oral). Project page with videos and code:\n  http://tancik.com/nerf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that achieves state-of-the-art results for synthesizing\nnovel views of complex scenes by optimizing an underlying continuous volumetric\nscene function using a sparse set of input views. Our algorithm represents a\nscene using a fully-connected (non-convolutional) deep network, whose input is\na single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing\ndirection $(\\theta, \\phi)$) and whose output is the volume density and\nview-dependent emitted radiance at that spatial location. We synthesize views\nby querying 5D coordinates along camera rays and use classic volume rendering\ntechniques to project the output colors and densities into an image. Because\nvolume rendering is naturally differentiable, the only input required to\noptimize our representation is a set of images with known camera poses. We\ndescribe how to effectively optimize neural radiance fields to render\nphotorealistic novel views of scenes with complicated geometry and appearance,\nand demonstrate results that outperform prior work on neural rendering and view\nsynthesis. View synthesis results are best viewed as videos, so we urge readers\nto view our supplementary video for convincing comparisons.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:57:23 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 22:17:31 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Mildenhall", "Ben", ""], ["Srinivasan", "Pratul P.", ""], ["Tancik", "Matthew", ""], ["Barron", "Jonathan T.", ""], ["Ramamoorthi", "Ravi", ""], ["Ng", "Ren", ""]]}, {"id": "2003.08935", "submitter": "Yawei Li", "authors": "Yawei Li, Shuhang Gu, Christoph Mayer, Luc Van Gool, and Radu Timofte", "title": "Group Sparsity: The Hinge Between Filter Pruning and Decomposition for\n  Network Compression", "comments": "Accepted by CVPR2020. Code is available at\n  https://github.com/ofsoundof/group_sparsity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze two popular network compression techniques, i.e.\nfilter pruning and low-rank decomposition, in a unified sense. By simply\nchanging the way the sparsity regularization is enforced, filter pruning and\nlow-rank decomposition can be derived accordingly. This provides another\nflexible choice for network compression because the techniques complement each\nother. For example, in popular network architectures with shortcut connections\n(e.g. ResNet), filter pruning cannot deal with the last convolutional layer in\na ResBlock while the low-rank decomposition methods can. In addition, we\npropose to compress the whole network jointly instead of in a layer-wise\nmanner. Our approach proves its potential as it compares favorably to the\nstate-of-the-art on several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:57:26 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Li", "Yawei", ""], ["Gu", "Shuhang", ""], ["Mayer", "Christoph", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.08936", "submitter": "Muyang Li", "authors": "Muyang Li, Ji Lin, Yaoyao Ding, Zhijian Liu, Jun-Yan Zhu, Song Han", "title": "GAN Compression: Efficient Architectures for Interactive Conditional\n  GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Generative Adversarial Networks (cGANs) have enabled controllable\nimage synthesis for many computer vision and graphics applications. However,\nrecent cGANs are 1-2 orders of magnitude more computationally-intensive than\nmodern recognition CNNs. For example, Gau-GAN consumes 281G MACs per image,\ncompared to 0.44GMACs for MobileNet-v3, making it difficult for interactive\ndeployment. In this work, we propose a general-purpose compression framework\nfor reducing the inference time and model size of the generator in cGANs.\nDirectly applying existing CNNs compression methods yields poor performance due\nto the difficulty of GAN training and the differences in generator\narchitectures. We address these challenges in two ways. First, to stabilize the\nGAN training, we transfer knowledge of multiple intermediate representations of\nthe original model to its compressed model, and unify unpaired and paired\nlearning. Second, instead of reusing existing CNN designs, our method\nautomatically finds efficient architectures via neural architecture search\n(NAS). To accelerate the search process, we decouple the model training and\narchitecture search via weight sharing. Experiments demonstrate the\neffectiveness of our method across different supervision settings (paired and\nunpaired), model architectures, and learning methods (e.g., pix2pix, GauGAN,\nCycleGAN). Without losing image quality, we reduce the computation of CycleGAN\nby more than 20x and GauGAN by 9x, paving the way for interactive image\nsynthesis. The code and demo are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 17:59:05 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 03:44:39 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 05:30:02 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Muyang", ""], ["Lin", "Ji", ""], ["Ding", "Yaoyao", ""], ["Liu", "Zhijian", ""], ["Zhu", "Jun-Yan", ""], ["Han", "Song", ""]]}, {"id": "2003.08951", "submitter": "Yuya Obinata", "authors": "Yuya Obinata and Takuma Yamamoto", "title": "Temporal Extension Module for Skeleton-Based Action Recognition", "comments": "Accepted on ICPR2020, 7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a module that extends the temporal graph of a graph convolutional\nnetwork (GCN) for action recognition with a sequence of skeletons. Existing\nmethods attempt to represent a more appropriate spatial graph on an\nintra-frame, but disregard optimization of the temporal graph on the\ninterframe. Concretely, these methods connect between vertices corresponding\nonly to the same joint on the inter-frame. In this work, we focus on adding\nconnections to neighboring multiple vertices on the inter-frame and extracting\nadditional features based on the extended temporal graph. Our module is a\nsimple yet effective method to extract correlated features of multiple joints\nin human movement. Moreover, our module aids in further performance\nimprovements, along with other GCN methods that optimize only the spatial\ngraph. We conduct extensive experiments on two large datasets, NTU RGB+D and\nKinetics-Skeleton, and demonstrate that our module is effective for several\nexisting models and our final model achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:00:04 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 02:39:04 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Obinata", "Yuya", ""], ["Yamamoto", "Takuma", ""]]}, {"id": "2003.08981", "submitter": "Chiyu Jiang", "authors": "Chiyu Max Jiang, Avneesh Sud, Ameesh Makadia, Jingwei Huang, Matthias\n  Nie{\\ss}ner, Thomas Funkhouser", "title": "Local Implicit Grid Representations for 3D Scenes", "comments": "CVPR 2020. Supplementary Video: https://youtu.be/XCyl1-vxfII", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape priors learned from data are commonly used to reconstruct 3D objects\nfrom partial or noisy data. Yet no such shape priors are available for indoor\nscenes, since typical 3D autoencoders cannot handle their scale, complexity, or\ndiversity. In this paper, we introduce Local Implicit Grid Representations, a\nnew 3D shape representation designed for scalability and generality. The\nmotivating idea is that most 3D surfaces share geometric details at some scale\n-- i.e., at a scale smaller than an entire object and larger than a small\npatch. We train an autoencoder to learn an embedding of local crops of 3D\nshapes at that size. Then, we use the decoder as a component in a shape\noptimization that solves for a set of latent codes on a regular grid of\noverlapping crops such that an interpolation of the decoded local shapes\nmatches a partial or noisy observation. We demonstrate the value of this\nproposed approach for 3D surface reconstruction from sparse point observations,\nshowing significantly better results than alternative approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:58:13 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Jiang", "Chiyu Max", ""], ["Sud", "Avneesh", ""], ["Makadia", "Ameesh", ""], ["Huang", "Jingwei", ""], ["Nie\u00dfner", "Matthias", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2003.08983", "submitter": "J\\'er\\^ome Rony", "authors": "Malik Boudiaf, J\\'er\\^ome Rony, Imtiaz Masud Ziko, Eric Granger, Marco\n  Pedersoli, Pablo Piantanida, Ismail Ben Ayed", "title": "A unifying mutual information view of metric learning: cross-entropy vs.\n  pairwise losses", "comments": "ECCV 2020 (Spotlight) - Code available at:\n  https://github.com/jeromerony/dml_cross_entropy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, substantial research efforts in Deep Metric Learning (DML) focused\non designing complex pairwise-distance losses, which require convoluted schemes\nto ease optimization, such as sample mining or pair weighting. The standard\ncross-entropy loss for classification has been largely overlooked in DML. On\nthe surface, the cross-entropy may seem unrelated and irrelevant to metric\nlearning as it does not explicitly involve pairwise distances. However, we\nprovide a theoretical analysis that links the cross-entropy to several\nwell-known and recent pairwise losses. Our connections are drawn from two\ndifferent perspectives: one based on an explicit optimization insight; the\nother on discriminative and generative views of the mutual information between\nthe labels and the learned features. First, we explicitly demonstrate that the\ncross-entropy is an upper bound on a new pairwise loss, which has a structure\nsimilar to various pairwise losses: it minimizes intra-class distances while\nmaximizing inter-class distances. As a result, minimizing the cross-entropy can\nbe seen as an approximate bound-optimization (or Majorize-Minimize) algorithm\nfor minimizing this pairwise loss. Second, we show that, more generally,\nminimizing the cross-entropy is actually equivalent to maximizing the mutual\ninformation, to which we connect several well-known pairwise losses.\nFurthermore, we show that various standard pairwise losses can be explicitly\nrelated to one another via bound relationships. Our findings indicate that the\ncross-entropy represents a proxy for maximizing the mutual information -- as\npairwise losses do -- without the need for convoluted sample-mining heuristics.\nOur experiments over four standard DML benchmarks strongly support our\nfindings. We obtain state-of-the-art results, outperforming recent and complex\nDML methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:59:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 22:15:43 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Boudiaf", "Malik", ""], ["Rony", "J\u00e9r\u00f4me", ""], ["Ziko", "Imtiaz Masud", ""], ["Granger", "Eric", ""], ["Pedersoli", "Marco", ""], ["Piantanida", "Pablo", ""], ["Ayed", "Ismail Ben", ""]]}, {"id": "2003.09003", "submitter": "Patrick Dendorfer", "authors": "Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel\n  Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\\'e", "title": "MOT20: A benchmark for multi object tracking in crowded scenes", "comments": "The sequences of the new MOT20 benchmark were previously presented in\n  the CVPR 2019 tracking challenge ( arXiv:1906.04567 ). The differences\n  between the two challenges are: - New and corrected annotations - New\n  sequences, as we had to crop and transform some old sequences to achieve\n  higher quality in the annotations. - New baselines evaluations and different\n  sets of public detections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for research. The benchmark for\nMultiple Object Tracking, MOTChallenge, was launched with the goal to establish\na standardized evaluation of multiple object tracking methods. The challenge\nfocuses on multiple people tracking, since pedestrians are well studied in the\ntracking community, and precise tracking and detection has high practical\nrelevance. Since the first release, MOT15, MOT16, and MOT17 have tremendously\ncontributed to the community by introducing a clean dataset and precise\nframework to benchmark multi-object trackers. In this paper, we present our\nMOT20benchmark, consisting of 8 new sequences depicting very crowded\nchallenging scenes. The benchmark was presented first at the 4thBMTT MOT\nChallenge Workshop at the Computer Vision and Pattern Recognition Conference\n(CVPR) 2019, and gives to chance to evaluate state-of-the-art methods for\nmultiple object tracking when handling extremely crowded scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 20:08:24 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Dendorfer", "Patrick", ""], ["Rezatofighi", "Hamid", ""], ["Milan", "Anton", ""], ["Shi", "Javen", ""], ["Cremers", "Daniel", ""], ["Reid", "Ian", ""], ["Roth", "Stefan", ""], ["Schindler", "Konrad", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "2003.09005", "submitter": "Yassine Ouali", "authors": "Yassine Ouali, C\\'eline Hudelot, Myriam Tami", "title": "Semi-Supervised Semantic Segmentation with Cross-Consistency Training", "comments": "Published at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel cross-consistency based semi-supervised\napproach for semantic segmentation. Consistency training has proven to be a\npowerful semi-supervised learning framework for leveraging unlabeled data under\nthe cluster assumption, in which the decision boundary should lie in\nlow-density regions. In this work, we first observe that for semantic\nsegmentation, the low-density regions are more apparent within the hidden\nrepresentations than within the inputs. We thus propose cross-consistency\ntraining, where an invariance of the predictions is enforced over different\nperturbations applied to the outputs of the encoder. Concretely, a shared\nencoder and a main decoder are trained in a supervised manner using the\navailable labeled examples. To leverage the unlabeled examples, we enforce a\nconsistency between the main decoder predictions and those of the auxiliary\ndecoders, taking as inputs different perturbed versions of the encoder's\noutput, and consequently, improving the encoder's representations. The proposed\nmethod is simple and can easily be extended to use additional training signal,\nsuch as image-level labels or pixel-level labels across different domains. We\nperform an ablation study to tease apart the effectiveness of each component,\nand conduct extensive experiments to demonstrate that our method achieves\nstate-of-the-art results in several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 20:10:37 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 20:48:55 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2020 14:11:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ouali", "Yassine", ""], ["Hudelot", "C\u00e9line", ""], ["Tami", "Myriam", ""]]}, {"id": "2003.09015", "submitter": "Toufiq Parag", "authors": "Toufiq Parag and Hongcheng Wang", "title": "Multilayer Dense Connections for Hierarchical Concept Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a pivotal function for many computer vision tasks such as\nobject classification, detection, scene segmentation. Multinomial logistic\nregression with a single final layer of dense connections has become the\nubiquitous technique for CNN-based classification. While these classifiers\nproject a mapping between the input and a set of output category classes, they\ndo not typically yield a comprehensive description of the category. In\nparticular, when a CNN based image classifier correctly identifies the image of\na Chimpanzee, its output does not clarify that Chimpanzee is a member of\nPrimate, Mammal, Chordate families and a living thing. We propose a multilayer\ndense connectivity for concurrent prediction of category and its conceptual\nsuperclasses in hierarchical order by the same CNN. We experimentally\ndemonstrate that our proposed network can simultaneously predict both the\ncoarse superclasses and finer categories better than several existing\nalgorithms in multiple datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 20:56:09 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:20:39 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Parag", "Toufiq", ""], ["Wang", "Hongcheng", ""]]}, {"id": "2003.09018", "submitter": "M Tanjid Hasan Tonmoy", "authors": "Saif Mahmud, M Tanjid Hasan Tonmoy, Kishor Kumar Bhaumik, A K M\n  Mahbubur Rahman, M Ashraful Amin, Mohammad Shoyaib, Muhammad Asif Hossain\n  Khan, Amin Ahsan Ali", "title": "Human Activity Recognition from Wearable Sensor Data Using\n  Self-Attention", "comments": "Accepted for publication at the 24th European Conference on\n  Artificial Intelligence (ECAI-2020); 8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human Activity Recognition from body-worn sensor data poses an inherent\nchallenge in capturing spatial and temporal dependencies of time-series\nsignals. In this regard, the existing recurrent or convolutional or their\nhybrid models for activity recognition struggle to capture spatio-temporal\ncontext from the feature space of sensor reading sequence. To address this\ncomplex problem, we propose a self-attention based neural network model that\nforegoes recurrent architectures and utilizes different types of attention\nmechanisms to generate higher dimensional feature representation used for\nclassification. We performed extensive experiments on four popular publicly\navailable HAR datasets: PAMAP2, Opportunity, Skoda and USC-HAD. Our model\nachieve significant performance improvement over recent state-of-the-art models\nin both benchmark test subjects and Leave-one-subject-out evaluation. We also\nobserve that the sensor attention maps produced by our model is able capture\nthe importance of the modality and placement of the sensors in predicting the\ndifferent activity classes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 14:16:57 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Mahmud", "Saif", ""], ["Tonmoy", "M Tanjid Hasan", ""], ["Bhaumik", "Kishor Kumar", ""], ["Rahman", "A K M Mahbubur", ""], ["Amin", "M Ashraful", ""], ["Shoyaib", "Mohammad", ""], ["Khan", "Muhammad Asif Hossain", ""], ["Ali", "Amin Ahsan", ""]]}, {"id": "2003.09033", "submitter": "Julian Lo", "authors": "Julian Lo (1), Morgan Heisler (1), Vinicius Vanzan (2), Sonja Karst (2\n  and 3), Ivana Zadro Matovinovic (4), Sven Loncaric (4), Eduardo V. Navajas\n  (2), Mirza Faisal Beg (1), Marinko V. Sarunic (1) ((1) School of Engineering\n  Science, Simon Fraser University, Canada, (2) Department of Ophthalmology and\n  Visual Sciences, University of British Columbia, Canada, (3) Department of\n  Ophthalmology and Optometry, Medical University of Vienna, Austria, (4)\n  Faculty of Electrical Engineering and Computing, University of Zagreb,\n  Croatia)", "title": "Microvasculature Segmentation and Inter-capillary Area Quantification of\n  the Deep Vascular Complex using Transfer Learning", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": "10.1167/tvst.9.2.38", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Purpose: Optical Coherence Tomography Angiography (OCT-A) permits\nvisualization of the changes to the retinal circulation due to diabetic\nretinopathy (DR), a microvascular complication of diabetes. We demonstrate\naccurate segmentation of the vascular morphology for the superficial capillary\nplexus and deep vascular complex (SCP and DVC) using a convolutional neural\nnetwork (CNN) for quantitative analysis.\n  Methods: Retinal OCT-A with a 6x6mm field of view (FOV) were acquired using a\nZeiss PlexElite. Multiple-volume acquisition and averaging enhanced the vessel\nnetwork contrast used for training the CNN. We used transfer learning from a\nCNN trained on 76 images from smaller FOVs of the SCP acquired using different\nOCT systems. Quantitative analysis of perfusion was performed on the automated\nvessel segmentations in representative patients with DR.\n  Results: The automated segmentations of the OCT-A images maintained the\nhierarchical branching and lobular morphologies of the SCP and DVC,\nrespectively. The network segmented the SCP with an accuracy of 0.8599, and a\nDice index of 0.8618. For the DVC, the accuracy was 0.7986, and the Dice index\nwas 0.8139. The inter-rater comparisons for the SCP had an accuracy and Dice\nindex of 0.8300 and 0.6700, respectively, and 0.6874 and 0.7416 for the DVC.\n  Conclusions: Transfer learning reduces the amount of manually-annotated\nimages required, while producing high quality automatic segmentations of the\nSCP and DVC. Using high quality training data preserves the characteristic\nappearance of the capillary networks in each layer.\n  Translational Relevance: Accurate retinal microvasculature segmentation with\nthe CNN results in improved perfusion analysis in diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 22:27:02 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Lo", "Julian", "", "2\n  and 3"], ["Heisler", "Morgan", "", "2\n  and 3"], ["Vanzan", "Vinicius", "", "2\n  and 3"], ["Karst", "Sonja", "", "2\n  and 3"], ["Matovinovic", "Ivana Zadro", ""], ["Loncaric", "Sven", ""], ["Navajas", "Eduardo V.", ""], ["Beg", "Mirza Faisal", ""], ["Sarunic", "Marinko V.", ""]]}, {"id": "2003.09044", "submitter": "Ryan Hoque", "authors": "Ryan Hoque, Daniel Seita, Ashwin Balakrishna, Aditya Ganapathi, Ajay\n  Kumar Tanwani, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg", "title": "VisuoSpatial Foresight for Multi-Step, Multi-Task Fabric Manipulation", "comments": "Robotics: Science and Systems (RSS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic fabric manipulation has applications in home robotics, textiles,\nsenior care and surgery. Existing fabric manipulation techniques, however, are\ndesigned for specific tasks, making it difficult to generalize across different\nbut related tasks. We extend the Visual Foresight framework to learn fabric\ndynamics that can be efficiently reused to accomplish different fabric\nmanipulation tasks with a single goal-conditioned policy. We introduce\nVisuoSpatial Foresight (VSF), which builds on prior work by learning visual\ndynamics on domain randomized RGB images and depth maps simultaneously and\ncompletely in simulation. We experimentally evaluate VSF on multi-step fabric\nsmoothing and folding tasks against 5 baseline methods in simulation and on the\nda Vinci Research Kit (dVRK) surgical robot without any demonstrations at train\nor test time. Furthermore, we find that leveraging depth significantly improves\nperformance. RGBD data yields an 80% improvement in fabric folding success rate\nover pure RGB data. Code, data, videos, and supplementary material are\navailable at https://sites.google.com/view/fabric-vsf/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 23:12:10 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 21:02:52 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 07:10:49 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Hoque", "Ryan", ""], ["Seita", "Daniel", ""], ["Balakrishna", "Ashwin", ""], ["Ganapathi", "Aditya", ""], ["Tanwani", "Ajay Kumar", ""], ["Jamali", "Nawid", ""], ["Yamane", "Katsu", ""], ["Iba", "Soshi", ""], ["Goldberg", "Ken", ""]]}, {"id": "2003.09049", "submitter": "Chu Wang", "authors": "Chu Wang, Babak Samari, Vladimir G. Kim, Siddhartha Chaudhuri, Kaleem\n  Siddiqi", "title": "Affinity Graph Supervision for Visual Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affinity graphs are widely used in deep architectures, including graph\nconvolutional neural networks and attention networks. Thus far, the literature\nhas focused on abstracting features from such graphs, while the learning of the\naffinities themselves has been overlooked. Here we propose a principled method\nto directly supervise the learning of weights in affinity graphs, to exploit\nmeaningful connections between entities in the data source. Applied to a visual\nattention network, our affinity supervision improves relationship recovery\nbetween objects, even without the use of manually annotated relationship\nlabels. We further show that affinity learning between objects boosts scene\ncategorization performance and that the supervision of affinity can also be\napplied to graphs built from mini-batches, for neural network training. In an\nimage classification task we demonstrate consistent improvement over the\nbaseline, with diverse network architectures and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 23:52:51 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Chu", ""], ["Samari", "Babak", ""], ["Kim", "Vladimir G.", ""], ["Chaudhuri", "Siddhartha", ""], ["Siddiqi", "Kaleem", ""]]}, {"id": "2003.09053", "submitter": "Dmitry Petrov", "authors": "Dmitry Petrov, Evangelos Kalogerakis", "title": "Cross-Shape Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that processes 3D point clouds by performing graph\nconvolution operations across shapes. In this manner, point descriptors are\nlearned by allowing interaction and propagation of feature representations\nwithin a shape collection. To enable this form of non-local, cross-shape graph\nconvolution, our method learns a pairwise point attention mechanism indicating\nthe degree of interaction between points on different shapes. Our method also\nlearns to create a graph over shapes of an input collection whose edges connect\nshapes deemed as useful for performing cross-shape convolution. The edges are\nalso equipped with learned weights indicating the compatibility of each shape\npair for cross-shape convolution. Our experiments demonstrate that this\ninteraction and propagation of point representations across shapes make them\nmore discriminative. In particular, our results show significantly improved\nperformance for 3D point cloud semantic segmentation compared to conventional\napproaches, especially in cases with the limited number of training examples.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 00:23:10 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 15:53:11 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 18:09:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Petrov", "Dmitry", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "2003.09070", "submitter": "Szu-Yeu Hu", "authors": "Szu-Yeu Hu, Shuhang Wang, Wei-Hung Weng, JingChao Wang, XiaoHong Wang,\n  Arinc Ozturk, Qian Li, Viksit Kumar, Anthony E. Samir", "title": "Weakly Supervised Context Encoder using DICOM metadata in Ultrasound\n  Imaging", "comments": "Accept as a workshop paper at AI4AH, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning algorithms geared towards clinical adaption rely on a\nsignificant amount of high fidelity labeled data. Low-resource settings pose\nchallenges like acquiring high fidelity data and becomes the bottleneck for\ndeveloping artificial intelligence applications. Ultrasound images, stored in\nDigital Imaging and Communication in Medicine (DICOM) format, have additional\nmetadata data corresponding to ultrasound image parameters and medical exams.\nIn this work, we leverage DICOM metadata from ultrasound images to help learn\nrepresentations of the ultrasound image. We demonstrate that the proposed\nmethod outperforms the non-metadata based approaches across different\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:17:03 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Hu", "Szu-Yeu", ""], ["Wang", "Shuhang", ""], ["Weng", "Wei-Hung", ""], ["Wang", "JingChao", ""], ["Wang", "XiaoHong", ""], ["Ozturk", "Arinc", ""], ["Li", "Qian", ""], ["Kumar", "Viksit", ""], ["Samir", "Anthony E.", ""]]}, {"id": "2003.09075", "submitter": "Omid Bazgir", "authors": "Omid Bazgir, Kai Barck, Richard A.D. Carano, Robby M. Weimer, Luke Xie", "title": "Kidney segmentation using 3D U-Net localized with Expectation\n  Maximization", "comments": null, "journal-ref": null, "doi": "10.1109/SSIAI49293.2020.9094601", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney volume is greatly affected in several renal diseases. Precise and\nautomatic segmentation of the kidney can help determine kidney size and\nevaluate renal function. Fully convolutional neural networks have been used to\nsegment organs from large biomedical 3D images. While these networks\ndemonstrate state-of-the-art segmentation performances, they do not immediately\ntranslate to small foreground objects, small sample sizes, and anisotropic\nresolution in MRI datasets. In this paper we propose a new framework to address\nsome of the challenges for segmenting 3D MRI. These methods were implemented on\npreclinical MRI for segmenting kidneys in an animal model of lupus nephritis.\nOur implementation strategy is twofold: 1) to utilize additional MRI diffusion\nimages to detect the general kidney area, and 2) to reduce the 3D U-Net kernels\nto handle small sample sizes. Using this approach, a Dice similarity\ncoefficient of 0.88 was achieved with a limited dataset of n=196. This\nsegmentation strategy with careful optimization can be applied to various renal\ninjuries or other organ systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:38:32 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Bazgir", "Omid", ""], ["Barck", "Kai", ""], ["Carano", "Richard A. D.", ""], ["Weimer", "Robby M.", ""], ["Xie", "Luke", ""]]}, {"id": "2003.09078", "submitter": "Timo Stoffregen", "authors": "Timo Stoffregen, Cedric Scheerlinck, Davide Scaramuzza, Tom Drummond,\n  Nick Barnes, Lindsay Kleeman, Robert Mahony", "title": "Reducing the Sim-to-Real Gap for Event Cameras", "comments": "Figure 5 fixed (had a glitch)", "journal-ref": "European Conference on Computer Vision, 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are paradigm-shifting novel sensors that report asynchronous,\nper-pixel brightness changes called 'events' with unparalleled low latency.\nThis makes them ideal for high speed, high dynamic range scenes where\nconventional cameras would fail. Recent work has demonstrated impressive\nresults using Convolutional Neural Networks (CNNs) for video reconstruction and\noptic flow with events. We present strategies for improving training data for\nevent based CNNs that result in 20-40% boost in performance of existing\nstate-of-the-art (SOTA) video reconstruction networks retrained with our\nmethod, and up to 15% for optic flow networks. A challenge in evaluating event\nbased video reconstruction is lack of quality ground truth images in existing\ndatasets. To address this, we present a new High Quality Frames (HQF) dataset,\ncontaining events and ground truth frames from a DAVIS240C that are\nwell-exposed and minimally motion-blurred. We evaluate our method on HQF +\nseveral existing major event camera datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:44:29 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 11:03:43 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 09:12:22 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 05:26:45 GMT"}, {"version": "v5", "created": "Sat, 22 Aug 2020 05:49:32 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Stoffregen", "Timo", ""], ["Scheerlinck", "Cedric", ""], ["Scaramuzza", "Davide", ""], ["Drummond", "Tom", ""], ["Barnes", "Nick", ""], ["Kleeman", "Lindsay", ""], ["Mahony", "Robert", ""]]}, {"id": "2003.09080", "submitter": "Huu Le", "authors": "Huu Le and Christopher Zach", "title": "A Graduated Filter Method for Large Scale Robust Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the highly non-convex nature of large-scale robust parameter\nestimation, avoiding poor local minima is challenging in real-world\napplications where input data is contaminated by a large or unknown fraction of\noutliers. In this paper, we introduce a novel solver for robust estimation that\npossesses a strong ability to escape poor local minima. Our algorithm is built\nupon the class of traditional graduated optimization techniques, which are\nconsidered state-of-the-art local methods to solve problems having many poor\nminima. The novelty of our work lies in the introduction of an adaptive kernel\n(or residual) scaling scheme, which allows us to achieve faster convergence\nrates. Like other existing methods that aim to return good local minima for\nrobust estimation tasks, our method relaxes the original robust problem but\nadapts a filter framework from non-linear constrained optimization to\nautomatically choose the level of relaxation. Experimental results on real\nlarge-scale datasets such as bundle adjustment instances demonstrate that our\nproposed method achieves competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 02:51:31 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Le", "Huu", ""], ["Zach", "Christopher", ""]]}, {"id": "2003.09085", "submitter": "Jakaria Rabbi", "authors": "Jakaria Rabbi, Nilanjan Ray, Matthias Schubert, Subir Chowdhury and\n  Dennis Chao", "title": "Small-Object Detection in Remote Sensing Images with End-to-End\n  Edge-Enhanced GAN and Object Detector Network", "comments": "This paper contains 27 pages and accepted for publication in MDPI\n  remote sensing journal. GitHub Repository:\n  https://github.com/Jakaria08/EESRGAN (Implementation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection performance of small objects in remote sensing images is not\nsatisfactory compared to large objects, especially in low-resolution and noisy\nimages. A generative adversarial network (GAN)-based model called enhanced\nsuper-resolution GAN (ESRGAN) shows remarkable image enhancement performance,\nbut reconstructed images miss high-frequency edge information. Therefore,\nobject detection performance degrades for small objects on recovered noisy and\nlow-resolution remote sensing images. Inspired by the success of edge enhanced\nGAN (EEGAN) and ESRGAN, we apply a new edge-enhanced super-resolution GAN\n(EESRGAN) to improve the image quality of remote sensing images and use\ndifferent detector networks in an end-to-end manner where detector loss is\nbackpropagated into the EESRGAN to improve the detection performance. We\npropose an architecture with three components: ESRGAN, Edge Enhancement Network\n(EEN), and Detection network. We use residual-in-residual dense blocks (RRDB)\nfor both the ESRGAN and EEN, and for the detector network, we use the faster\nregion-based convolutional network (FRCNN) (two-stage detector) and single-shot\nmulti-box detector (SSD) (one stage detector). Extensive experiments on a\npublic (car overhead with context) and a self-assembled (oil and gas storage\ntank) satellite dataset show superior performance of our method compared to the\nstandalone state-of-the-art object detectors.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:07:30 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 01:23:00 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 07:18:12 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 04:31:30 GMT"}, {"version": "v5", "created": "Tue, 28 Apr 2020 20:11:11 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Rabbi", "Jakaria", ""], ["Ray", "Nilanjan", ""], ["Schubert", "Matthias", ""], ["Chowdhury", "Subir", ""], ["Chao", "Dennis", ""]]}, {"id": "2003.09087", "submitter": "Minjee Kim", "authors": "Minjee Kim, Joonmyeong Choi, Namkug Kim", "title": "Fully Automated Hand Hygiene Monitoring\\\\in Operating Room using 3D\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand hygiene is one of the most significant factors in preventing hospital\nacquired infections (HAI) which often be transmitted by medical staffs in\ncontact with patients in the operating room (OR). Hand hygiene monitoring could\nbe important to investigate and reduce the outbreak of infections within the\nOR. However, an effective monitoring tool for hand hygiene compliance is\ndifficult to develop due to the visual complexity of the OR scene. Recent\nprogress in video understanding with convolutional neural net (CNN) has\nincreased the application of recognition and detection of human actions.\nLeveraging this progress, we proposed a fully automated hand hygiene monitoring\ntool of the alcohol-based hand rubbing action of anesthesiologists on OR video\nusing spatio-temporal features with 3D CNN. First, the region of interest (ROI)\nof anesthesiologists' upper body were detected and cropped. A temporal\nsmoothing filter was applied to the ROIs. Then, the ROIs were given to a 3D CNN\nand classified into two classes: rubbing hands or other actions. We observed\nthat a transfer learning from Kinetics-400 is beneficial and the optical flow\nstream was not helpful in our dataset. The final accuracy, precision, recall\nand F1 score in testing is 0.76, 0.85, 0.65 and 0.74, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:18:15 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Kim", "Minjee", ""], ["Choi", "Joonmyeong", ""], ["Kim", "Namkug", ""]]}, {"id": "2003.09088", "submitter": "Jingwen Ye", "authors": "Jingwen Ye, Yixin Ji, Xinchao Wang, Xin Gao, Mingli Song", "title": "Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have provided procedures for learning one\nnetwork to amalgamate multiple streams of knowledge from the pre-trained\nConvolutional Neural Network (CNN) models, thus reduce the annotation cost.\nHowever, almost all existing methods demand massive training data, which may be\nunavailable due to privacy or transmission issues. In this paper, we propose a\ndata-free knowledge amalgamate strategy to craft a well-behaved multi-task\nstudent network from multiple single/multi-task teachers. The main idea is to\nconstruct the group-stack generative adversarial networks (GANs) which have two\ndual generators. First one generator is trained to collect the knowledge by\nreconstructing the images approximating the original dataset utilized for\npre-training the teachers. Then a dual generator is trained by taking the\noutput from the former generator as input. Finally we treat the dual part\ngenerator as the target network and regroup it. As demonstrated on several\nbenchmarks of multi-label classification, the proposed method without any\ntraining data achieves the surprisingly competitive results, even compared with\nsome full-supervised methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:20:52 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Ye", "Jingwen", ""], ["Ji", "Yixin", ""], ["Wang", "Xinchao", ""], ["Gao", "Xin", ""], ["Song", "Mingli", ""]]}, {"id": "2003.09089", "submitter": "Nikan Namiri", "authors": "Nikan K. Namiri, Io Flament, Bruno Astuto, Rutwik Shah, Radhika\n  Tibrewala, Francesco Caliva, Thomas M. Link, Valentina Pedoia, Sharmila\n  Majumdar", "title": "Hierarchical Severity Staging of Anterior Cruciate Ligament Injuries\n  using Deep Learning with MRI Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To evaluate the diagnostic utility of two convolutional neural\nnetworks (CNNs) for severity staging of anterior cruciate ligament (ACL)\ninjuries.\n  Materials and Methods: This retrospective analysis was conducted on 1243 knee\nMR images (1008 intact, 18 partially torn, 77 fully torn, and 140 reconstructed\nACLs) from 224 patients (age 47 +/- 14 years, 54% women) acquired between 2011\nand 2014. The radiologists used a modified scoring metric. To classify ACL\ninjuries with deep learning, two types of CNNs were used, one with\nthree-dimensional (3D) and the other with two-dimensional (2D) convolutional\nkernels. Performance metrics included sensitivity, specificity, weighted\nCohen's kappa, and overall accuracy, followed by McNemar's test to compare the\nCNNs performance.\n  Results: The overall accuracy and weighted Cohen's kappa reported for ACL\ninjury classification were higher using the 2D CNN (accuracy: 92% (233/254) and\nkappa: 0.83) than the 3D CNN (accuracy: 89% (225/254) and kappa: 0.83) (P =\n.27). The 2D CNN and 3D CNN performed similarly in classifying intact ACLs (2D\nCNN: 93% (188/203) sensitivity and 90% (46/51) specificity; 3D CNN: 89%\n(180/203) sensitivity and 88% (45/51) specificity). Classification of full\ntears by both networks were also comparable (2D CNN: 82% (14/17) sensitivity\nand 94% (222/237) specificity; 3D CNN: 76% (13/17) sensitivity and 100%\n(236/237) specificity). The 2D CNN classified all reconstructed ACLs correctly.\n  Conclusion: 2D and 3D CNNs applied to ACL lesion classification had high\nsensitivity and specificity, suggesting that these networks could be used to\nhelp grade ACL injuries by non-experts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 03:21:40 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 19:40:52 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Namiri", "Nikan K.", ""], ["Flament", "Io", ""], ["Astuto", "Bruno", ""], ["Shah", "Rutwik", ""], ["Tibrewala", "Radhika", ""], ["Caliva", "Francesco", ""], ["Link", "Thomas M.", ""], ["Pedoia", "Valentina", ""], ["Majumdar", "Sharmila", ""]]}, {"id": "2003.09093", "submitter": "Guangcheng Wang", "authors": "Zhongyuan Wang, Guangcheng Wang, Baojin Huang, Zhangyang Xiong, Qi\n  Hong, Hao Wu, Peng Yi, Kui Jiang, Nanxi Wang, Yingjiao Pei, Heling Chen, Yu\n  Miao, Zhibing Huang, Jinbi Liang", "title": "Masked Face Recognition Dataset and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to effectively prevent the spread of COVID-19 virus, almost everyone\nwears a mask during coronavirus epidemic. This almost makes conventional facial\nrecognition technology ineffective in many cases, such as community access\ncontrol, face access control, facial attendance, facial security checks at\ntrain stations, etc. Therefore, it is very urgent to improve the recognition\nperformance of the existing face recognition technology on the masked faces.\nMost current advanced face recognition approaches are designed based on deep\nlearning, which depend on a large number of face samples. However, at present,\nthere are no publicly available masked face recognition datasets. To this end,\nthis work proposes three types of masked face datasets, including Masked Face\nDetection Dataset (MFDD), Real-world Masked Face Recognition Dataset (RMFRD)\nand Simulated Masked Face Recognition Dataset (SMFRD). Among them, to the best\nof our knowledge, RMFRD is currently theworld's largest real-world masked face\ndataset. These datasets are freely available to industry and academia, based on\nwhich various applications on masked faces can be developed. The\nmulti-granularity masked face recognition model we developed achieves 95%\naccuracy, exceeding the results reported by the industry. Our datasets are\navailable at: https://github.com/X-zhangyang/Real-World-Masked-Face-Dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 04:15:19 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 07:58:57 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wang", "Zhongyuan", ""], ["Wang", "Guangcheng", ""], ["Huang", "Baojin", ""], ["Xiong", "Zhangyang", ""], ["Hong", "Qi", ""], ["Wu", "Hao", ""], ["Yi", "Peng", ""], ["Jiang", "Kui", ""], ["Wang", "Nanxi", ""], ["Pei", "Yingjiao", ""], ["Chen", "Heling", ""], ["Miao", "Yu", ""], ["Huang", "Zhibing", ""], ["Liang", "Jinbi", ""]]}, {"id": "2003.09108", "submitter": "Dong Wang", "authors": "Dong Wang, Yuan Zhang, Kexin Zhang, Liwei Wang", "title": "FocalMix: Semi-Supervised Learning for 3D Medical Image Detection", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying artificial intelligence techniques in medical imaging is one of the\nmost promising areas in medicine. However, most of the recent success in this\narea highly relies on large amounts of carefully annotated data, whereas\nannotating medical images is a costly process. In this paper, we propose a\nnovel method, called FocalMix, which, to the best of our knowledge, is the\nfirst to leverage recent advances in semi-supervised learning (SSL) for 3D\nmedical image detection. We conducted extensive experiments on two widely used\ndatasets for lung nodule detection, LUNA16 and NLST. Results show that our\nproposed SSL methods can achieve a substantial improvement of up to 17.3% over\nstate-of-the-art supervised learning approaches with 400 unlabeled CT scans.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 05:12:31 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wang", "Dong", ""], ["Zhang", "Yuan", ""], ["Zhang", "Kexin", ""], ["Wang", "Liwei", ""]]}, {"id": "2003.09114", "submitter": "German I. Parisi", "authors": "German I. Parisi and Vincenzo Lomonaco", "title": "Online Continual Learning on Sequences", "comments": "L. Oneto et al. (eds.), Recent Trends in Learning From Data, Studies\n  in Computational Intelligence 896", "journal-ref": null, "doi": "10.1007/978-3-030-43883-8_8", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online continual learning (OCL) refers to the ability of a system to learn\nover time from a continuous stream of data without having to revisit previously\nencountered training samples. Learning continually in a single data pass is\ncrucial for agents and robots operating in changing environments and required\nto acquire, fine-tune, and transfer increasingly complex representations from\nnon-i.i.d. input distributions. Machine learning models that address OCL must\nalleviate \\textit{catastrophic forgetting} in which hidden representations are\ndisrupted or completely overwritten when learning from streams of novel input.\nIn this chapter, we summarize and discuss recent deep learning models that\naddress OCL on sequential input through the use (and combination) of synaptic\nregularization, structural plasticity, and experience replay. Different\nimplementations of replay have been proposed that alleviate catastrophic\nforgetting in connectionists architectures via the re-occurrence of (latent\nrepresentations of) input sequences and that functionally resemble mechanisms\nof hippocampal replay in the mammalian brain. Empirical evidence shows that\narchitectures endowed with experience replay typically outperform architectures\nwithout in (online) incremental learning tasks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 05:49:31 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Parisi", "German I.", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "2003.09116", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang, Yang Zhao, Hao Zhang", "title": "Dual-discriminator GAN: A GAN way of profile face recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wealth of angle problems occur when facial recognition is performed: At\npresent, the feature extraction network presents eigenvectors with large\ndifferences between the frontal face and profile face recognition of the same\nperson in many cases. For this reason, the state-of-the-art facial recognition\nnetwork will use multiple samples for the same target to ensure that\neigenvector differences caused by angles are ignored during training. However,\nthere is another solution available, which is to generate frontal face images\nwith profile face images before recognition. In this paper, we proposed a\nmethod of generating frontal faces with image-to-image profile faces based on\nGenerative Adversarial Network (GAN).\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 06:01:58 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Zhang", "Xinyu", ""], ["Zhao", "Yang", ""], ["Zhang", "Hao", ""]]}, {"id": "2003.09119", "submitter": "Guoxuan Li", "authors": "Zhiwei Dong, Guoxuan Li, Yue Liao, Fei Wang, Pengju Ren, Chen Qian", "title": "CentripetalNet: Pursuing High-quality Keypoint Pairs for Object\n  Detection", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keypoint-based detectors have achieved pretty-well performance. However,\nincorrect keypoint matching is still widespread and greatly affects the\nperformance of the detector. In this paper, we propose CentripetalNet which\nuses centripetal shift to pair corner keypoints from the same instance.\nCentripetalNet predicts the position and the centripetal shift of the corner\npoints and matches corners whose shifted results are aligned. Combining\nposition information, our approach matches corner points more accurately than\nthe conventional embedding approaches do. Corner pooling extracts information\ninside the bounding boxes onto the border. To make this information more aware\nat the corners, we design a cross-star deformable convolution network to\nconduct feature adaption. Furthermore, we explore instance segmentation on\nanchor-free detectors by equipping our CentripetalNet with a mask prediction\nmodule. On MS-COCO test-dev, our CentripetalNet not only outperforms all\nexisting anchor-free detectors with an AP of 48.0% but also achieves comparable\nperformance to the state-of-the-art instance segmentation approaches with a\n40.2% MaskAP. Code will be available at\nhttps://github.com/KiveeDong/CentripetalNet.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 06:23:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Dong", "Zhiwei", ""], ["Li", "Guoxuan", ""], ["Liao", "Yue", ""], ["Wang", "Fei", ""], ["Ren", "Pengju", ""], ["Qian", "Chen", ""]]}, {"id": "2003.09124", "submitter": "Younghyun Jo", "authors": "Younghyun Jo, Jaeyeon Kang, Seoung Wug Oh, Seonghyeon Nam, Peter\n  Vajda, and Seon Joo Kim", "title": "Learning the Loss Functions in a Discriminative Space for Video\n  Restoration", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With more advanced deep network architectures and learning schemes such as\nGANs, the performance of video restoration algorithms has greatly improved\nrecently. Meanwhile, the loss functions for optimizing deep neural networks\nremain relatively unchanged. To this end, we propose a new framework for\nbuilding effective loss functions by learning a discriminative space specific\nto a video restoration task. Our framework is similar to GANs in that we\niteratively train two networks - a generator and a loss network. The generator\nlearns to restore videos in a supervised fashion, by following ground truth\nfeatures through the feature matching in the discriminative space learned by\nthe loss network. In addition, we also introduce a new relation loss in order\nto maintain the temporal consistency in output videos. Experiments on video\nsuperresolution and deblurring show that our method generates visually more\npleasing videos with better quantitative perceptual metric values than the\nother state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 06:58:27 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Jo", "Younghyun", ""], ["Kang", "Jaeyeon", ""], ["Oh", "Seoung Wug", ""], ["Nam", "Seonghyeon", ""], ["Vajda", "Peter", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2003.09133", "submitter": "Martin Eberhart", "authors": "Martin Eberhart", "title": "Efficient computation of backprojection arrays for 3D light field\n  deconvolution", "comments": "15 pages, 11 figures, 1 table. This is a thoroughly reworked version\n  of the manuscript, with a clearer structure. It avoids any ambigiuties\n  envoked by using the term 'transpose' in the context of multi-dimensional\n  arrays", "journal-ref": "Optics Express 29(15) 24129-24143 (2021)", "doi": "10.1364/OE.431174", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field deconvolution allows three-dimensional investigations from a\nsingle snapshot recording of a plenoptic camera. It is based on a linear image\nformation model, and iterative volume reconstruction requires to define the\nbackprojection of individual image pixels into object space. This is\neffectively a reversal of the point spread function (PSF), and backprojection\narrays H' can be derived from the shift-variant PSFs H of the optical system,\nwhich is a very time consuming step for high resolution cameras. This paper\nillustrates the common structure of backprojection arrays and the significance\nof their efficient computation. A new algorithm is presented to determine H'\nfrom H, which is based on the distinct relation of the elements' positions\nwithin the two multi-dimensional arrays. It permits a pure array\nre-arrangement, and while results are identical to those from published codes,\ncomputation times are drastically reduced. This is shown by benchmarking the\nnew method using various sample PSF arrays against existing algorithms. The\npaper is complemented by practical hints for the experimental acquisition of\nlight field PSFs in a photographic setup.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 07:55:45 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:15:32 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Eberhart", "Martin", ""]]}, {"id": "2003.09148", "submitter": "Nico Messikommer", "authors": "Nico Messikommer, Daniel Gehrig, Antonio Loquercio, Davide Scaramuzza", "title": "Event-based Asynchronous Sparse Convolutional Networks", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV), 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors that respond to per-pixel brightness\nchanges in the form of asynchronous and sparse \"events\". Recently, pattern\nrecognition algorithms, such as learning-based methods, have made significant\nprogress with event cameras by converting events into synchronous dense,\nimage-like representations and applying traditional machine learning methods\ndeveloped for standard cameras. However, these approaches discard the spatial\nand temporal sparsity inherent in event data at the cost of higher\ncomputational complexity and latency. In this work, we present a general\nframework for converting models trained on synchronous image-like event\nrepresentations into asynchronous models with identical output, thus directly\nleveraging the intrinsic asynchronous and sparse nature of the event data. We\nshow both theoretically and experimentally that this drastically reduces the\ncomputational complexity and latency of high-capacity, synchronous neural\nnetworks without sacrificing accuracy. In addition, our framework has several\ndesirable characteristics: (i) it exploits spatio-temporal sparsity of events\nexplicitly, (ii) it is agnostic to the event representation, network\narchitecture, and task, and (iii) it does not require any train-time change,\nsince it is compatible with the standard neural networks' training process. We\nthoroughly validate the proposed framework on two computer vision tasks: object\ndetection and object recognition. In these tasks, we reduce the computational\ncomplexity up to 20 times with respect to high-latency neural networks. At the\nsame time, we outperform state-of-the-art asynchronous approaches up to 24% in\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:39:49 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:52:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Messikommer", "Nico", ""], ["Gehrig", "Daniel", ""], ["Loquercio", "Antonio", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.09149", "submitter": "Magda Friedjungov\\'a", "authors": "Magda Friedjungov\\'a, Daniel Va\\v{s}ata, Tom\\'a\\v{s} Chobola, Marcel\n  Ji\\v{r}ina", "title": "Unsupervised Latent Space Translation Network", "comments": "To be published in conference proceedings of ESANN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One task that is often discussed in a computer vision is the mapping of an\nimage from one domain to a corresponding image in another domain known as\nimage-to-image translation. Currently there are several approaches solving this\ntask. In this paper, we present an enhancement of the UNIT framework that aids\nin removing its main drawbacks. More specifically, we introduce an additional\nadversarial discriminator on the latent representation used instead of VAE,\nwhich enforces the latent space distributions of both domains to be similar. On\nMNIST and USPS domain adaptation tasks, this approach greatly outperforms\ncompeting approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:41:37 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Friedjungov\u00e1", "Magda", ""], ["Va\u0161ata", "Daniel", ""], ["Chobola", "Tom\u00e1\u0161", ""], ["Ji\u0159ina", "Marcel", ""]]}, {"id": "2003.09150", "submitter": "Fan Zhang", "authors": "Fan Zhang, Meng Li, Guisheng Zhai, Yizhao Liu", "title": "Multi-branch and Multi-scale Attention Learning for Fine-Grained Visual\n  Categorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is one of the most\nauthoritative academic competitions in the field of Computer Vision (CV) in\nrecent years. But applying ILSVRC's annual champion directly to fine-grained\nvisual categorization (FGVC) tasks does not achieve good performance. To FGVC\ntasks, the small inter-class variations and the large intra-class variations\nmake it a challenging problem. Our attention object location module (AOLM) can\npredict the position of the object and attention part proposal module (APPM)\ncan propose informative part regions without the need of bounding-box or part\nannotations. The obtained object images not only contain almost the entire\nstructure of the object, but also contains more details, part images have many\ndifferent scales and more fine-grained features, and the raw images contain the\ncomplete object. The three kinds of training images are supervised by our\nmulti-branch network. Therefore, our multi-branch and multi-scale learning\nnetwork(MMAL-Net) has good classification ability and robustness for images of\ndifferent scales. Our approach can be trained end-to-end, while provides short\ninference time. Through the comprehensive experiments demonstrate that our\napproach can achieves state-of-the-art results on CUB-200-2011, FGVC-Aircraft\nand Stanford Cars datasets. Our code will be available at\nhttps://github.com/ZF1044404254/MMAL-Net\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:43:28 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 11:42:15 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 14:15:33 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Zhang", "Fan", ""], ["Li", "Meng", ""], ["Zhai", "Guisheng", ""], ["Liu", "Yizhao", ""]]}, {"id": "2003.09151", "submitter": "Hong-Gyu Jung", "authors": "Hong-Gyu Jung and Seong-Whan Lee", "title": "Few-Shot Learning with Geometric Constraints", "comments": "Accepted for publication in IEEE Transactions on Neural Networks and\n  Learning Systems (T-NNLS)", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2957187", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of few-shot learning for\nclassification. We assume a network trained for base categories with a large\nnumber of training examples, and we aim to add novel categories to it that have\nonly a few, e.g., one or five, training examples. This is a challenging\nscenario because: 1) high performance is required in both the base and novel\ncategories; and 2) training the network for the new categories with a few\ntraining examples can contaminate the feature space trained well for the base\ncategories. To address these challenges, we propose two geometric constraints\nto fine-tune the network with a few training examples. The first constraint\nenables features of the novel categories to cluster near the category weights,\nand the second maintains the weights of the novel categories far from the\nweights of the base categories. By applying the proposed constraints, we\nextract discriminative features for the novel categories while preserving the\nfeature space learned for the base categories. Using public data sets for\nfew-shot learning that are subsets of ImageNet, we demonstrate that the\nproposed method outperforms prevalent methods by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:50:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Jung", "Hong-Gyu", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2003.09152", "submitter": "Xiu-Shen Wei", "authors": "Chang-Dong Xu and Xing-Ran Zhao and Xin Jin and Xiu-Shen Wei", "title": "Exploring Categorical Regularization for Domain Adaptive Object\n  Detection", "comments": "To appear in CVPR 2020. X.-S. Wei is the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the domain adaptive object detection problem, where\nthe main challenge lies in significant domain gaps between source and target\ndomains. Previous work seeks to plainly align image-level and instance-level\nshifts to eventually minimize the domain discrepancy. However, they still\noverlook to match crucial image regions and important instances across domains,\nwhich will strongly affect domain shift mitigation. In this work, we propose a\nsimple but effective categorical regularization framework for alleviating this\nissue. It can be applied as a plug-and-play component on a series of Domain\nAdaptive Faster R-CNN methods which are prominent for dealing with domain\nadaptive detection. Specifically, by integrating an image-level multi-label\nclassifier upon the detection backbone, we can obtain the sparse but crucial\nimage regions corresponding to categorical information, thanks to the weakly\nlocalization ability of the classification manner. Meanwhile, at the instance\nlevel, we leverage the categorical consistency between image-level predictions\n(by the classifier) and instance-level predictions (by the detection head) as a\nregularization factor to automatically hunt for the hard aligned instances of\ntarget domains. Extensive experiments of various domain shift scenarios show\nthat our method obtains a significant performance gain over original Domain\nAdaptive Faster R-CNN detectors. Furthermore, qualitative visualization and\nanalyses can demonstrate the ability of our method for attending on the key\nregions/instances targeting on domain adaptation. Our code is open-source and\navailable at \\url{https://github.com/Megvii-Nanjing/CR-DA-DET}.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:53:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xu", "Chang-Dong", ""], ["Zhao", "Xing-Ran", ""], ["Jin", "Xin", ""], ["Wei", "Xiu-Shen", ""]]}, {"id": "2003.09163", "submitter": "Xuangeng Chu", "authors": "Xuangeng Chu, Anlin Zheng, Xiangyu Zhang, Jian Sun", "title": "Detection in Crowded Scenes: One Proposal, Multiple Predictions", "comments": "CVPR 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective proposal-based object detector, aiming at\ndetecting highly-overlapped instances in crowded scenes. The key of our\napproach is to let each proposal predict a set of correlated instances rather\nthan a single one in previous proposal-based frameworks. Equipped with new\ntechniques such as EMD Loss and Set NMS, our detector can effectively handle\nthe difficulty of detecting highly overlapped objects. On a FPN-Res50 baseline,\nour detector can obtain 4.9\\% AP gains on challenging CrowdHuman dataset and\n1.0\\% $\\text{MR}^{-2}$ improvements on CityPersons dataset, without bells and\nwhistles. Moreover, on less crowed datasets like COCO, our approach can still\nachieve moderate improvement, suggesting the proposed method is robust to\ncrowdedness. Code and pre-trained models will be released at\nhttps://github.com/megvii-model/CrowdDetection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 09:48:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 14:59:34 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Chu", "Xuangeng", ""], ["Zheng", "Anlin", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2003.09168", "submitter": "Andres C Rodriguez", "authors": "Andres C. Rodriguez, Stefano D'Aronco, Konrad Schindler and Jan Dirk\n  Wegner", "title": "Privileged Pooling: Better Sample Efficiency Through Supervised\n  Attention", "comments": "privileged pooling, supervised attention, training set bias,\n  fine-grained species recognition, camera trap images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scheme for supervised image classification that uses privileged\ninformation, in the form of keypoint annotations for the training data, to\nlearn strong models from small and/or biased training sets. Our main motivation\nis the recognition of animal species for ecological applications such as\nbiodiversity modelling, which is challenging because of long-tailed species\ndistributions due to rare species, and strong dataset biases such as repetitive\nscene background in camera traps. To counteract these challenges, we propose a\nvisual attention mechanism that is supervised via keypoint annotations that\nhighlight important object parts. This privileged information, implemented as a\nnovel privileged pooling operation, is only required during training and helps\nthe model to focus on regions that are discriminative. In experiments with\nthree different animal species datasets, we show that deep networks with\nprivileged pooling can use small training sets more efficiently and generalize\nbetter.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:03:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 11:45:26 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 09:55:21 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Rodriguez", "Andres C.", ""], ["D'Aronco", "Stefano", ""], ["Schindler", "Konrad", ""], ["Wegner", "Jan Dirk", ""]]}, {"id": "2003.09171", "submitter": "Gunhee Nam", "authors": "Gunhee Nam, Seoung Wug Oh, Joon-Young Lee, Seon Joo Kim", "title": "DMV: Visual Object Tracking via Part-level Dense Memory and Voting-based\n  Retrieval", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel memory-based tracker via part-level dense memory and\nvoting-based retrieval, called DMV. Since deep learning techniques have been\nintroduced to the tracking field, Siamese trackers have attracted many\nresearchers due to the balance between speed and accuracy. However, most of\nthem are based on a single template matching, which limits the performance as\nit restricts the accessible in-formation to the initial target features. In\nthis paper, we relieve this limitation by maintaining an external memory that\nsaves the tracking record. Part-level retrieval from the memory also liberates\nthe information from the template and allows our tracker to better handle the\nchallenges such as appearance changes and occlusions. By updating the memory\nduring tracking, the representative power for the target object can be enhanced\nwithout online learning. We also propose a novel voting mechanism for the\nmemory reading to filter out unreliable information in the memory. We\ncomprehensively evaluate our tracker on OTB-100,TrackingNet, GOT-10k, LaSOT,\nand UAV123, which show that our method yields comparable results to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:05:30 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Nam", "Gunhee", ""], ["Oh", "Seoung Wug", ""], ["Lee", "Joon-Young", ""], ["Kim", "Seon Joo", ""]]}, {"id": "2003.09175", "submitter": "Rui Xiang", "authors": "Rui Xiang, Feng Zheng, Huapeng Su, Zhe Zhang", "title": "3dDepthNet: Point Cloud Guided Depth Completion Network for Sparse Depth\n  and Single Color Image", "comments": "8 pages, 10 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end deep learning network named\n3dDepthNet, which produces an accurate dense depth image from a single pair of\nsparse LiDAR depth and color image for robotics and autonomous driving tasks.\nBased on the dimensional nature of depth images, our network offers a novel\n3D-to-2D coarse-to-fine dual densification design that is both accurate and\nlightweight. Depth densification is first performed in 3D space via point cloud\ncompletion, followed by a specially designed encoder-decoder structure that\nutilizes the projected dense depth from 3D completion and the original RGB-D\nimages to perform 2D image completion. Experiments on the KITTI dataset show\nour network achieves state-of-art accuracy while being more efficient. Ablation\nand generalization tests prove that each module in our network has positive\ninfluences on the final results, and furthermore, our network is resilient to\neven sparser depth.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:19:32 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xiang", "Rui", ""], ["Zheng", "Feng", ""], ["Su", "Huapeng", ""], ["Zhang", "Zhe", ""]]}, {"id": "2003.09177", "submitter": "Morten Hannemose", "authors": "Morten Hannemose and Jakob Wilm and Jeppe Revall Frisvad", "title": "Superaccurate Camera Calibration via Inverse Rendering", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": "10.1117/12.2531769", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prevalent routine for camera calibration is based on the detection\nof well-defined feature points on a purpose-made calibration artifact. These\ncould be checkerboard saddle points, circles, rings or triangles, often printed\non a planar structure. The feature points are first detected and then used in a\nnonlinear optimization to estimate the internal camera parameters.We propose a\nnew method for camera calibration using the principle of inverse rendering.\nInstead of relying solely on detected feature points, we use an estimate of the\ninternal parameters and the pose of the calibration object to implicitly render\na non-photorealistic equivalent of the optical features. This enables us to\ncompute pixel-wise differences in the image domain without interpolation\nartifacts. We can then improve our estimate of the internal parameters by\nminimizing pixel-wise least-squares differences. In this way, our model\noptimizes a meaningful metric in the image space assuming normally distributed\nnoise characteristic for camera sensors.We demonstrate using synthetic and real\ncamera images that our method improves the accuracy of estimated camera\nparameters as compared with current state-of-the-art calibration routines. Our\nmethod also estimates these parameters more robustly in the presence of noise\nand in situations where the number of calibration images is limited.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:26:16 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Hannemose", "Morten", ""], ["Wilm", "Jakob", ""], ["Frisvad", "Jeppe Revall", ""]]}, {"id": "2003.09182", "submitter": "Debashis Sen", "authors": "Sobhan Kanti Dhara and Debashis Sen", "title": "Across-scale Process Similarity based Interpolation for Image\n  Super-Resolution", "comments": null, "journal-ref": "Applied Soft Computing, Volume 81, August 2019, 105508", "doi": "10.1016/j.asoc.2019.105508", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A pivotal step in image super-resolution techniques is interpolation, which\naims at generating high resolution images without introducing artifacts such as\nblurring and ringing. In this paper, we propose a technique that performs\ninterpolation through an infusion of high frequency signal components computed\nby exploiting `process similarity'. By `process similarity', we refer to the\nresemblance between a decomposition of the image at a resolution to the\ndecomposition of the image at another resolution. In our approach, the\ndecompositions generating image details and approximations are obtained through\nthe discrete wavelet (DWT) and stationary wavelet (SWT) transforms. The\ncomplementary nature of DWT and SWT is leveraged to get the structural relation\nbetween the input image and its low resolution approximation. The structural\nrelation is represented by optimal model parameters obtained through particle\nswarm optimization (PSO). Owing to process similarity, these parameters are\nused to generate the high resolution output image from the input image. The\nproposed approach is compared with six existing techniques qualitatively and in\nterms of PSNR, SSIM, and FSIM measures, along with computation time (CPU time).\nIt is found that our approach is the fastest in terms of CPU time and produces\ncomparable results.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 10:39:46 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Dhara", "Sobhan Kanti", ""], ["Sen", "Debashis", ""]]}, {"id": "2003.09208", "submitter": "Misgina Tsighe Hagos", "authors": "Misgina Tsighe Hagos", "title": "Diagnosis of Diabetic Retinopathy in Ethiopia: Before the Deep Learning\n  based Automation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Introducing automated Diabetic Retinopathy (DR) diagnosis into Ethiopia is\nstill a challenging task, despite recent reports that present trained Deep\nLearning (DL) based DR classifiers surpassing manual graders. This is mainly\nbecause of the expensive cost of conventional retinal imaging devices used in\nDL based classifiers. Current approaches that provide mobile based binary\nclassification of DR, and the way towards a cheaper and offline multi-class\nclassification of DR will be discussed in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 11:41:28 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 21:13:17 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Hagos", "Misgina Tsighe", ""]]}, {"id": "2003.09210", "submitter": "Zixiang Zhao", "authors": "Zixiang Zhao, Shuang Xu, Chunxia Zhang, Junmin Liu, Pengfei Li,\n  Jiangshe Zhang", "title": "DIDFuse: Deep Image Decomposition for Infrared and Visible Image Fusion", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2020/135", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared and visible image fusion, a hot topic in the field of image\nprocessing, aims at obtaining fused images keeping the advantages of source\nimages. This paper proposes a novel auto-encoder (AE) based fusion network. The\ncore idea is that the encoder decomposes an image into background and detail\nfeature maps with low- and high-frequency information, respectively, and that\nthe decoder recovers the original image. To this end, the loss function makes\nthe background/detail feature maps of source images similar/dissimilar. In the\ntest phase, background and detail feature maps are respectively merged via a\nfusion module, and the fused image is recovered by the decoder. Qualitative and\nquantitative results illustrate that our method can generate fusion images\ncontaining highlighted targets and abundant detail texture information with\nstrong robustness and meanwhile surpass state-of-the-art (SOTA) approaches.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 11:45:20 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 15:10:24 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 07:41:43 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhao", "Zixiang", ""], ["Xu", "Shuang", ""], ["Zhang", "Chunxia", ""], ["Liu", "Junmin", ""], ["Li", "Pengfei", ""], ["Zhang", "Jiangshe", ""]]}, {"id": "2003.09234", "submitter": "Siwei Lyu", "authors": "Siwei Lyu", "title": "DeepFake Detection: Current Challenges and Next Steps", "comments": "arXiv admin note: text overlap with arXiv:1909.12962", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality fake videos and audios generated by AI-algorithms (the deep\nfakes) have started to challenge the status of videos and audios as definitive\nevidence of events. In this paper, we highlight a few of these challenges and\ndiscuss the research opportunities in this direction.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 13:20:42 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Lyu", "Siwei", ""]]}, {"id": "2003.09260", "submitter": "Olivier Colliot", "authors": "Alexandre Morin (ARAMIS), Jorge Samper-Gonz\\'alez (ARAMIS), Anne\n  Bertrand (ARAMIS), Sebastian Stroer, Didier Dormont (ICM, ARAMIS), Aline\n  Mendes, Pierrick Coup\\'e, Jamila Ahdidan, Marcel L\\'evy (IM2A), Dalila Samri,\n  Harald Hampel, Bruno Dubois (APM), Marc Teichmann (FRONTlab), St\\'ephane\n  Epelbaum (ARAMIS), Olivier Colliot (ARAMIS)", "title": "Accuracy of MRI Classification Algorithms in a Tertiary Memory Center\n  Clinical Routine Cohort", "comments": null, "journal-ref": "Journal of Alzheimer's Disease, IOS Press, 2020, pp.1-10", "doi": "10.3233/JAD-190594", "report-no": null, "categories": "q-bio.QM cs.CV eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND:Automated volumetry software (AVS) has recently become widely\navailable to neuroradiologists. MRI volumetry with AVS may support the\ndiagnosis of dementias by identifying regional atrophy. Moreover, automatic\nclassifiers using machine learning techniques have recently emerged as\npromising approaches to assist diagnosis. However, the performance of both AVS\nand automatic classifiers has been evaluated mostly in the artificial setting\nof research datasets.OBJECTIVE:Our aim was to evaluate the performance of two\nAVS and an automatic classifier in the clinical routine condition of a memory\nclinic.METHODS:We studied 239 patients with cognitive troubles from a single\nmemory center cohort. Using clinical routine T1-weighted MRI, we evaluated the\nclassification performance of: 1) univariate volumetry using two AVS (volBrain\nand Neuroreader$^{TM}$); 2) Support Vector Machine (SVM) automatic classifier,\nusing either the AVS volumes (SVM-AVS), or whole gray matter (SVM-WGM); 3)\nreading by two neuroradiologists. The performance measure was the balanced\ndiagnostic accuracy. The reference standard was consensus diagnosis by three\nneurologists using clinical, biological (cerebrospinal fluid) and imaging data\nand following international criteria.RESULTS:Univariate AVS volumetry provided\nonly moderate accuracies (46% to 71% with hippocampal volume). The accuracy\nimproved when using SVM-AVS classifier (52% to 85%), becoming close to that of\nSVM-WGM (52 to 90%). Visual classification by neuroradiologists ranged between\nSVM-AVS and SVM-WGM.CONCLUSION:In the routine practice of a memory clinic, the\nuse of volumetric measures provided by AVS yields only moderate accuracy.\nAutomatic classifiers can improve accuracy and could be a useful tool to assist\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:44:46 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Morin", "Alexandre", "", "ARAMIS"], ["Samper-Gonz\u00e1lez", "Jorge", "", "ARAMIS"], ["Bertrand", "Anne", "", "ARAMIS"], ["Stroer", "Sebastian", "", "ICM, ARAMIS"], ["Dormont", "Didier", "", "ICM, ARAMIS"], ["Mendes", "Aline", "", "IM2A"], ["Coup\u00e9", "Pierrick", "", "IM2A"], ["Ahdidan", "Jamila", "", "IM2A"], ["L\u00e9vy", "Marcel", "", "IM2A"], ["Samri", "Dalila", "", "APM"], ["Hampel", "Harald", "", "APM"], ["Dubois", "Bruno", "", "APM"], ["Teichmann", "Marc", "", "FRONTlab"], ["Epelbaum", "St\u00e9phane", "", "ARAMIS"], ["Colliot", "Olivier", "", "ARAMIS"]]}, {"id": "2003.09262", "submitter": "Ruben Tolosana", "authors": "Oscar Delgado-Mohatar, Julian Fierrez, Ruben Tolosana and Ruben\n  Vera-Rodriguez", "title": "Blockchain meets Biometrics: Concepts, Application to Template\n  Protection, and Trends", "comments": "arXiv admin note: text overlap with arXiv:1904.13128", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technologies provide excellent architectures and practical tools\nfor securing and managing the sensitive and private data stored in biometric\ntemplates, but at a cost. We discuss opportunities and challenges in the\nintegration of blockchain and biometrics, with emphasis in biometric template\nstorage and protection, a key problem in biometrics still largely unsolved. Key\ntradeoffs involved in that integration, namely, latency, processing time,\neconomic cost, and biometric performance are experimentally studied through the\nimplementation of a smart contract on the Ethereum blockchain platform, which\nis publicly available in github for research purposes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 08:11:13 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Delgado-Mohatar", "Oscar", ""], ["Fierrez", "Julian", ""], ["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2003.09265", "submitter": "Andrew Pryhuber", "authors": "Sameer Agarwal, Andrew Pryhuber, Rainer Sinn and Rekha R. Thomas", "title": "The Chiral Domain of a Camera Arrangement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the chiral domain of an arrangement of projective cameras\n$\\mathcal{A} = \\{A_1,\\dots, A_m\\}$ which is the subset of $\\mathbb{P}^3$\nvisible in $\\mathcal{A}$. It generalizes the classical definition of chirality\nto include all of $\\mathbb{P}^3$. We give an algebraic description of the\nchiral domain which allows us to define and describe a chiral version of\nTriggs' joint image. The chiral domain also leads to a polyhedral derivation of\nHartley's chiral inequalities for deciding when a projective reconstruction can\nbe made chiral.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 04:48:40 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 20:42:48 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 23:30:27 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Agarwal", "Sameer", ""], ["Pryhuber", "Andrew", ""], ["Sinn", "Rainer", ""], ["Thomas", "Rekha R.", ""]]}, {"id": "2003.09282", "submitter": "Adrian Spurr", "authors": "Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Otmar Hilliges, Jan Kautz", "title": "Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating 3D hand pose from 2D images is a difficult, inverse problem due to\nthe inherent scale and depth ambiguities. Current state-of-the-art methods\ntrain fully supervised deep neural networks with 3D ground-truth data. However,\nacquiring 3D annotations is expensive, typically requiring calibrated\nmulti-view setups or labor intensive manual annotations. While annotations of\n2D keypoints are much easier to obtain, how to efficiently leverage such\nweakly-supervised data to improve the task of 3D hand pose prediction remains\nan important open question. The key difficulty stems from the fact that direct\napplication of additional 2D supervision mostly benefits the 2D proxy objective\nbut does little to alleviate the depth and scale ambiguities. Embracing this\nchallenge we propose a set of novel losses. We show by extensive experiments\nthat our proposed constraints significantly reduce the depth ambiguity and\nallow the network to more effectively leverage additional 2D annotated images.\nFor example, on the challenging freiHAND dataset using additional 2D annotation\nwithout our proposed biomechanical constraints reduces the depth error by only\n$15\\%$, whereas the error is reduced significantly by $50\\%$ when the proposed\nbiomechanical constraints are used.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 14:03:13 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 15:55:41 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Spurr", "Adrian", ""], ["Iqbal", "Umar", ""], ["Molchanov", "Pavlo", ""], ["Hilliges", "Otmar", ""], ["Kautz", "Jan", ""]]}, {"id": "2003.09293", "submitter": "Nikhil Varma Keetha", "authors": "Nikhil Varma Keetha, Samson Anosh Babu P, Chandra Sekhara Rao\n  Annavarapu", "title": "U-Det: A Modified U-Net architecture with bidirectional feature network\n  for lung nodule segmentation", "comments": "14 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis and analysis of lung cancer involve a precise and efficient\nlung nodule segmentation in computed tomography (CT) images. However, the\nanonymous shapes, visual features, and surroundings of the nodule in the CT\nimage pose a challenging problem to the robust segmentation of the lung\nnodules. This article proposes U-Det, a resource-efficient model architecture,\nwhich is an end to end deep learning approach to solve the task at hand. It\nincorporates a Bi-FPN (bidirectional feature network) between the encoder and\ndecoder. Furthermore, it uses Mish activation function and class weights of\nmasks to enhance segmentation efficiency. The proposed model is extensively\ntrained and evaluated on the publicly available LUNA-16 dataset consisting of\n1186 lung nodules. The U-Det architecture outperforms the existing U-Net model\nwith the Dice similarity coefficient (DSC) of 82.82% and achieves results\ncomparable to human experts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 14:25:22 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Keetha", "Nikhil Varma", ""], ["P", "Samson Anosh Babu", ""], ["Annavarapu", "Chandra Sekhara Rao", ""]]}, {"id": "2003.09316", "submitter": "Ning Xie", "authors": "Ning Xie, Ji Hu, Junjie Chen, Qiqi Zhang, and Changsheng Chen", "title": "Detection of Information Hiding at Anti-Copying 2D Barcodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the problem of detecting the use of information hiding at\nanti-copying 2D barcodes. Prior hidden information detection schemes are either\nheuristicbased or Machine Learning (ML) based. The key limitation of prior\nheuristics-based schemes is that they do not answer the fundamental question of\nwhy the information hidden at a 2D barcode can be detected. The key limitation\nof prior MLbased information schemes is that they lack robustness because a\nprinted 2D barcode is very much environmentally dependent, and thus an\ninformation hiding detection scheme trained in one environment often does not\nwork well in another environment. In this paper, we propose two hidden\ninformation detection schemes at the existing anti-copying 2D barcodes. The\nfirst scheme is to directly use the pixel distance to detect the use of an\ninformation hiding scheme in a 2D barcode, referred as to the Pixel Distance\nBased Detection (PDBD) scheme. The second scheme is first to calculate the\nvariance of the raw signal and the covariance between the recovered signal and\nthe raw signal, and then based on the variance results, detects the use of\ninformation hiding scheme in a 2D barcode, referred as to the Pixel Variance\nBased Detection (PVBD) scheme. Moreover, we design advanced IC attacks to\nevaluate the security of two existing anti-copying 2D barcodes. We implemented\nour schemes and conducted extensive performance comparison between our schemes\nand prior schemes under different capturing devices, such as a scanner and a\ncamera phone. Our experimental results show that the PVBD scheme can correctly\ndetect the existence of the hidden information at both the 2LQR code and the\nLCAC 2D barcode. Moreover, the probability of successfully attacking of our IC\nattacks achieves 0.6538 for the 2LQR code and 1 for the LCAC 2D barcode.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 15:06:50 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xie", "Ning", ""], ["Hu", "Ji", ""], ["Chen", "Junjie", ""], ["Zhang", "Qiqi", ""], ["Chen", "Changsheng", ""]]}, {"id": "2003.09338", "submitter": "Nikita Dvornik", "authors": "Nikita Dvornik, Cordelia Schmid, Julien Mairal", "title": "Selecting Relevant Features from a Multi-domain Representation for\n  Few-shot Classification", "comments": "ECCV'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular approaches for few-shot classification consist of first learning a\ngeneric data representation based on a large annotated dataset, before adapting\nthe representation to new classes given only a few labeled samples. In this\nwork, we propose a new strategy based on feature selection, which is both\nsimpler and more effective than previous feature adaptation approaches. First,\nwe obtain a multi-domain representation by training a set of semantically\ndifferent feature extractors. Then, given a few-shot learning task, we use our\nmulti-domain feature bank to automatically select the most relevant\nrepresentations. We show that a simple non-parametric classifier built on top\nof such features produces high accuracy and generalizes to domains never seen\nduring training, which leads to state-of-the-art results on MetaDataset and\nimproved accuracy on mini-ImageNet.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 15:44:17 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 12:51:34 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Dvornik", "Nikita", ""], ["Schmid", "Cordelia", ""], ["Mairal", "Julien", ""]]}, {"id": "2003.09354", "submitter": "Varun Tolani", "authors": "Varun Tolani, Somil Bansal, Aleksandra Faust, Claire Tomlin", "title": "Visual Navigation Among Humans with Optimal Control as a Supervisor", "comments": "Project Website: https://smlbansal.github.io/LB-WayPtNav-DH/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real world visual navigation requires robots to operate in unfamiliar,\nhuman-occupied dynamic environments. Navigation around humans is especially\ndifficult because it requires anticipating their future motion, which can be\nquite challenging. We propose an approach that combines learning-based\nperception with model-based optimal control to navigate among humans based only\non monocular, first-person RGB images. Our approach is enabled by our novel\ndata-generation tool, HumANav that allows for photorealistic renderings of\nindoor environment scenes with humans in them, which are then used to train the\nperception module entirely in simulation. Through simulations and experiments\non a mobile robot, we demonstrate that the learned navigation policies can\nanticipate and react to humans without explicitly predicting future human\nmotion, generalize to previously unseen environments and human behaviors, and\ntransfer directly from simulation to reality. Videos describing our approach\nand experiments, as well as a demo of HumANav are available on the project\nwebsite.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:13:47 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:09:24 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Tolani", "Varun", ""], ["Bansal", "Somil", ""], ["Faust", "Aleksandra", ""], ["Tomlin", "Claire", ""]]}, {"id": "2003.09365", "submitter": "Xuan Li", "authors": "Xuan Li, Yuchen Lu, Christian Desrosiers, Xue Liu", "title": "Out-of-Distribution Detection for Skin Lesion Images with Deep Isolation\n  Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of out-of-distribution detection in skin\ndisease images. Publicly available medical datasets normally have a limited\nnumber of lesion classes (e.g. HAM10000 has 8 lesion classes). However, there\nexists a few thousands of clinically identified diseases. Hence, it is\nimportant if lesions not in the training data can be differentiated. Toward\nthis goal, we propose DeepIF, a non-parametric Isolation Forest based approach\ncombined with deep convolutional networks. We conduct comprehensive experiments\nto compare our DeepIF with three baseline models. Results demonstrate\nstate-of-the-art performance of our proposed approach on the task of detecting\nabnormal skin lesions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:39:50 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Li", "Xuan", ""], ["Lu", "Yuchen", ""], ["Desrosiers", "Christian", ""], ["Liu", "Xue", ""]]}, {"id": "2003.09373", "submitter": "Philipp Terh\\\"orst", "authors": "Philipp Terh\\\"orst, Jan Niklas Kolf, Naser Damer, Florian\n  Kirchbuchner, Arjan Kuijper", "title": "SER-FIQ: Unsupervised Estimation of Face Image Quality Based on\n  Stochastic Embedding Robustness", "comments": "Accepted at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face image quality is an important factor to enable high performance face\nrecognition systems. Face quality assessment aims at estimating the suitability\nof a face image for recognition. Previous work proposed supervised solutions\nthat require artificially or human labelled quality values. However, both\nlabelling mechanisms are error-prone as they do not rely on a clear definition\nof quality and may not know the best characteristics for the utilized face\nrecognition system. Avoiding the use of inaccurate quality labels, we proposed\na novel concept to measure face quality based on an arbitrary face recognition\nmodel. By determining the embedding variations generated from random\nsubnetworks of a face model, the robustness of a sample representation and\nthus, its quality is estimated. The experiments are conducted in a\ncross-database evaluation setting on three publicly available databases. We\ncompare our proposed solution on two face embeddings against six\nstate-of-the-art approaches from academia and industry. The results show that\nour unsupervised solution outperforms all other approaches in the majority of\nthe investigated scenarios. In contrast to previous works, the proposed\nsolution shows a stable performance over all scenarios. Utilizing the deployed\nface recognition model for our face quality assessment methodology avoids the\ntraining phase completely and further outperforms all baseline approaches by a\nlarge margin. Our solution can be easily integrated into current face\nrecognition systems and can be modified to other tasks beyond face recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:50:30 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Terh\u00f6rst", "Philipp", ""], ["Kolf", "Jan Niklas", ""], ["Damer", "Naser", ""], ["Kirchbuchner", "Florian", ""], ["Kuijper", "Arjan", ""]]}, {"id": "2003.09391", "submitter": "Yongqiang Tang", "authors": "Lei Tian, Yongqiang Tang, Liangchen Hu, Zhida Ren, and Wensheng Zhang", "title": "Domain Adaptation by Class Centroid Matching and Local Manifold\n  Self-Learning", "comments": "Accepted by IEEE Transactions on Image Processing, Code Available:\n  https://github.com/LeiTian-qj/CMMS/tree/master", "journal-ref": null, "doi": "10.1109/TIP.2020.3031220", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation has been a fundamental technology for transferring\nknowledge from a source domain to a target domain. The key issue of domain\nadaptation is how to reduce the distribution discrepancy between two domains in\na proper way such that they can be treated indifferently for learning. In this\npaper, we propose a novel domain adaptation approach, which can thoroughly\nexplore the data distribution structure of target domain.Specifically, we\nregard the samples within the same cluster in target domain as a whole rather\nthan individuals and assigns pseudo-labels to the target cluster by class\ncentroid matching. Besides, to exploit the manifold structure information of\ntarget data more thoroughly, we further introduce a local manifold\nself-learning strategy into our proposal to adaptively capture the inherent\nlocal connectivity of target samples. An efficient iterative optimization\nalgorithm is designed to solve the objective function of our proposal with\ntheoretical convergence guarantee. In addition to unsupervised domain\nadaptation, we further extend our method to the semi-supervised scenario\nincluding both homogeneous and heterogeneous settings in a direct but elegant\nway. Extensive experiments on seven benchmark datasets validate the significant\nsuperiority of our proposal in both unsupervised and semi-supervised manners.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:59:27 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 02:30:49 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 05:48:44 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 06:17:08 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Tian", "Lei", ""], ["Tang", "Yongqiang", ""], ["Hu", "Liangchen", ""], ["Ren", "Zhida", ""], ["Zhang", "Wensheng", ""]]}, {"id": "2003.09392", "submitter": "Yansong Tang", "authors": "Yansong Tang and Jiwen Lu and Jie Zhou", "title": "Comprehensive Instructional Video Analysis: The COIN Dataset and\n  Performance Evaluation", "comments": "Accepted by T-PAMI, journal version of COIN dataset arXiv:1903.02874", "journal-ref": null, "doi": "10.1109/TPAMI.2020.2980824", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the substantial and explosively inscreased instructional videos on\nthe Internet, novices are able to acquire knowledge for completing various\ntasks. Over the past decade, growing efforts have been devoted to investigating\nthe problem on instructional video analysis. However, the most existing\ndatasets in this area have limitations in diversity and scale, which makes them\nfar from many real-world applications where more diverse activities occur. To\naddress this, we present a large-scale dataset named as \"COIN\" for\nCOmprehensive INstructional video analysis. Organized with a hierarchical\nstructure, the COIN dataset contains 11,827 videos of 180 tasks in 12 domains\n(e.g., vehicles, gadgets, etc.) related to our daily life. With a new developed\ntoolbox, all the videos are annotated efficiently with a series of step labels\nand the corresponding temporal boundaries. In order to provide a benchmark for\ninstructional video analysis, we evaluate plenty of approaches on the COIN\ndataset under five different settings. Furthermore, we exploit two important\ncharacteristics (i.e., task-consistency and ordering-dependency) for localizing\nimportant steps in instructional videos. Accordingly, we propose two simple yet\neffective methods, which can be easily plugged into conventional proposal-based\naction detection models. We believe the introduction of the COIN dataset will\npromote the future in-depth research on instructional video analysis for the\ncommunity. Our dataset, annotation toolbox and source code are available at\nhttp://coin-dataset.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:59:44 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Tang", "Yansong", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2003.09404", "submitter": "Insaf Setitra", "authors": "Insaf Setitra, Noureddine Aouaa, Abdelkrim Meziane, Afef Benrabia,\n  Houria Kaced, Hanene Belabassi, Sara Ait Ziane, Nadia Henda Zenati, and\n  Oualid Djekkoune", "title": "RGB-Topography and X-rays Image Registration for Idiopathic Scoliosis\n  Children Patient Follow-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Children diagnosed with a scoliosis pathology are exposed during their follow\nup to ionic radiations in each X-rays diagnosis. This exposure can have\nnegative effects on the patient's health and cause diseases in the adult age.\nIn order to reduce X-rays scanning, recent systems provide diagnosis of\nscoliosis patients using solely RGB images. The output of such systems is a set\nof augmented images and scoliosis related angles. These angles, however,\nconfuse the physicians due to their large number. Moreover, the lack of X-rays\nscans makes it impossible for the physician to compare RGB and X-rays images,\nand decide whether to reduce X-rays exposure or not. In this work, we exploit\nboth RGB images of scoliosis captured during clinical diagnosis, and X-rays\nhard copies provided by patients in order to register both images and give a\nrich comparison of diagnoses. The work consists in, first, establishing the\nmonomodal (RGB topography of the back) and multimodal (RGB and Xrays) image\ndatabase, then registering images based on patient landmarks, and finally\nblending registered images for a visual analysis and follow up by the\nphysician. The proposed registration is based on a rigid transformation that\npreserves the topology of the patient's back. Parameters of the rigid\ntransformation are estimated using a proposed angle minimization of Cervical\nvertebra 7, and Posterior Superior Iliac Spine landmarks of a source and target\ndiagnoses. Experiments conducted on the constructed database show a better\nmonomodal and multimodal registration using our proposed method compared to\nregistration using an Equation System Solving based registration.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 17:33:23 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Setitra", "Insaf", ""], ["Aouaa", "Noureddine", ""], ["Meziane", "Abdelkrim", ""], ["Benrabia", "Afef", ""], ["Kaced", "Houria", ""], ["Belabassi", "Hanene", ""], ["Ziane", "Sara Ait", ""], ["Zenati", "Nadia Henda", ""], ["Djekkoune", "Oualid", ""]]}, {"id": "2003.09405", "submitter": "Yiran Xu", "authors": "Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu,\n  Yunsheng Li, Nuno Vasconcelos", "title": "Explainable Object-induced Action Decision for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new paradigm is proposed for autonomous driving. The new paradigm lies\nbetween the end-to-end and pipelined approaches, and is inspired by how humans\nsolve the problem. While it relies on scene understanding, the latter only\nconsiders objects that could originate hazard. These are denoted as\naction-inducing, since changes in their state should trigger vehicle actions.\nThey also define a set of explanations for these actions, which should be\nproduced jointly with the latter. An extension of the BDD100K dataset,\nannotated for a set of 4 actions and 21 explanations, is proposed. A new\nmulti-task formulation of the problem, which optimizes the accuracy of both\naction commands and explanations, is then introduced. A CNN architecture is\nfinally proposed to solve this problem, by combining reasoning about action\ninducing objects and global scene context. Experimental results show that the\nrequirement of explanations improves the recognition of action-inducing\nobjects, which in turn leads to better action predictions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 17:33:44 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Xu", "Yiran", ""], ["Yang", "Xiaoyin", ""], ["Gong", "Lihang", ""], ["Lin", "Hsuan-Chu", ""], ["Wu", "Tz-Ying", ""], ["Li", "Yunsheng", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2003.09424", "submitter": "Mucahid Barstugan Asst. Prof. Dr", "authors": "Mucahid Barstugan, Umut Ozkaya, Saban Ozturk", "title": "Coronavirus (COVID-19) Classification using CT Images by Machine\n  Learning Methods", "comments": "The paper has 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents early phase detection of Coronavirus (COVID-19), which is\nnamed by World Health Organization (WHO), by machine learning methods. The\ndetection process was implemented on abdominal Computed Tomography (CT) images.\nThe expert radiologists detected from CT images that COVID-19 shows different\nbehaviours from other viral pneumonia. Therefore, the clinical experts specify\nthat COV\\.ID-19 virus needs to be diagnosed in early phase. For detection of\nthe COVID-19, four different datasets were formed by taking patches sized as\n16x16, 32x32, 48x48, 64x64 from 150 CT images. The feature extraction process\nwas applied to patches to increase the classification performance. Grey Level\nCo-occurrence Matrix (GLCM), Local Directional Pattern (LDP), Grey Level Run\nLength Matrix (GLRLM), Grey-Level Size Zone Matrix (GLSZM), and Discrete\nWavelet Transform (DWT) algorithms were used as feature extraction methods.\nSupport Vector Machines (SVM) classified the extracted features. 2-fold, 5-fold\nand 10-fold cross-validations were implemented during the classification\nprocess. Sensitivity, specificity, accuracy, precision, and F-score metrics\nwere used to evaluate the classification performance. The best classification\naccuracy was obtained as 99.68% with 10-fold cross-validation and GLSZM feature\nextraction method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 12:48:39 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Barstugan", "Mucahid", ""], ["Ozkaya", "Umut", ""], ["Ozturk", "Saban", ""]]}, {"id": "2003.09439", "submitter": "Tariq Bdair", "authors": "Tariq Bdair, Benedikt Wiestler, Nassir Navab, and Shadi Albarqouni", "title": "ROAM: Random Layer Mixup for Semi-Supervised Learning in Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is one of the major challenges addressed by\nmachine learning methods. Yet, deep learning methods profoundly depend on a\nlarge amount of annotated data, which is time-consuming and costly. Though,\nsemi-supervised learning methods approach this problem by leveraging an\nabundant amount of unlabeled data along with a small amount of labeled data in\nthe training process. Recently, MixUp regularizer has been successfully\nintroduced to semi-supervised learning methods showing superior performance.\nMixUp augments the model with new data points through linear interpolation of\nthe data at the input space. We argue that this option is limited. Instead, we\npropose ROAM, a RandOm lAyer Mixup, which encourages the network to be less\nconfident for interpolated data points at randomly selected space. ROAM\ngenerates more data points that have never seen before, and hence it avoids\nover-fitting and enhances the generalization ability. We conduct extensive\nexperiments to validate our method on three publicly available datasets on\nwhole-brain image segmentation. ROAM achieves state-of-the-art (SOTA) results\nin fully supervised (89.5%) and semi-supervised (87.0%) settings with a\nrelative improvement of up to 2.40% and 16.50%, respectively for the\nwhole-brain segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 18:07:12 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:13:32 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 13:46:16 GMT"}, {"version": "v4", "created": "Mon, 16 Nov 2020 22:50:12 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Bdair", "Tariq", ""], ["Wiestler", "Benedikt", ""], ["Navab", "Nassir", ""], ["Albarqouni", "Shadi", ""]]}, {"id": "2003.09461", "submitter": "Alexander Meinke", "authors": "Maximilian Augustin, Alexander Meinke, Matthias Hein", "title": "Adversarial Robustness on In- and Out-Distribution Improves\n  Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have led to major improvements in image classification but\nsuffer from being non-robust to adversarial changes, unreliable uncertainty\nestimates on out-distribution samples and their inscrutable black-box\ndecisions. In this work we propose RATIO, a training procedure for Robustness\nvia Adversarial Training on In- and Out-distribution, which leads to robust\nmodels with reliable and robust confidence estimates on the out-distribution.\nRATIO has similar generative properties to adversarial training so that visual\ncounterfactuals produce class specific features. While adversarial training\ncomes at the price of lower clean accuracy, RATIO achieves state-of-the-art\n$l_2$-adversarial robustness on CIFAR10 and maintains better clean accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 18:57:52 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 17:36:00 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Augustin", "Maximilian", ""], ["Meinke", "Alexander", ""], ["Hein", "Matthias", ""]]}, {"id": "2003.09483", "submitter": "Jie Luo", "authors": "Jie Luo, Guangshen Ma, Sarah Frisken, Parikshit Juvekar, Nazim\n  Haouchine, Zhe Xu, Yiming Xiao, Alexandra Golby, Patrick Codd, Masashi\n  Sugiyama, and William Wells III", "title": "Do Public Datasets Assure Unbiased Comparisons for Registration\n  Evaluation?", "comments": "Draft 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing availability of new image registration approaches, an\nunbiased evaluation is becoming more needed so that clinicians can choose the\nmost suitable approaches for their applications. Current evaluations typically\nuse landmarks in manually annotated datasets. As a result, the quality of\nannotations is crucial for unbiased comparisons. Even though most data\nproviders claim to have quality control over their datasets, an objective\nthird-party screening can be reassuring for intended users. In this study, we\nuse the variogram to screen the manually annotated landmarks in two datasets\nused to benchmark registration in image-guided neurosurgeries. The variogram\nprovides an intuitive 2D representation of the spatial characteristics of\nannotated landmarks. Using variograms, we identified potentially problematic\ncases and had them examined by experienced radiologists. We found that (1) a\nsmall number of annotations may have fiducial localization errors; (2) the\nlandmark distribution for some cases is not ideal to offer fair comparisons. If\nunresolved, both findings could incur bias in registration evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:04:47 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Luo", "Jie", ""], ["Ma", "Guangshen", ""], ["Frisken", "Sarah", ""], ["Juvekar", "Parikshit", ""], ["Haouchine", "Nazim", ""], ["Xu", "Zhe", ""], ["Xiao", "Yiming", ""], ["Golby", "Alexandra", ""], ["Codd", "Patrick", ""], ["Sugiyama", "Masashi", ""], ["Wells", "William", "III"]]}, {"id": "2003.09487", "submitter": "Zhaoshuo Li", "authors": "Zhaoshuo Li, Amirreza Shaban, Jean-Gabriel Simard, Dinesh Rabindran,\n  Simon DiMaio, Omid Mohareri", "title": "A Robotic 3D Perception System for Operating Room Environment Awareness", "comments": "Accepted in IPCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: We describe a 3D multi-view perception system for the da Vinci\nsurgical system to enable Operating room (OR) scene understanding and context\nawareness.\n  Methods: Our proposed system is comprised of four Time-of-Flight (ToF)\ncameras rigidly attached to strategic locations on the daVinci Xi patient side\ncart (PSC). The cameras are registered to the robot's kinematic chain by\nperforming a one-time calibration routine and therefore, information from all\ncameras can be fused and represented in one common coordinate frame. Based on\nthis architecture, a multi-view 3D scene semantic segmentation algorithm is\ncreated to enable recognition of common and salient objects/equipment and\nsurgical activities in a da Vinci OR. Our proposed 3D semantic segmentation\nmethod has been trained and validated on a novel densely annotated dataset that\nhas been captured from clinical scenarios.\n  Results: The results show that our proposed architecture has acceptable\nregistration error ($3.3\\%\\pm1.4\\%$ of object-camera distance) and can robustly\nimprove scene segmentation performance (mean Intersection Over Union - mIOU)\nfor less frequently appearing classes ($\\ge 0.013$) compared to a single-view\nmethod.\n  Conclusion: We present the first dynamic multi-view perception system with a\nnovel segmentation architecture, which can be used as a building block\ntechnology for applications such as surgical workflow analysis, automation of\nsurgical sub-tasks and advanced guidance systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:27:06 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 17:20:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Zhaoshuo", ""], ["Shaban", "Amirreza", ""], ["Simard", "Jean-Gabriel", ""], ["Rabindran", "Dinesh", ""], ["DiMaio", "Simon", ""], ["Mohareri", "Omid", ""]]}, {"id": "2003.09514", "submitter": "Tony C. W. Mok", "authors": "Tony C.W. Mok, Albert C.S. Chung", "title": "Fast Symmetric Diffeomorphic Image Registration with Convolutional\n  Neural Networks", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffeomorphic deformable image registration is crucial in many medical image\nstudies, as it offers unique, special properties including topology\npreservation and invertibility of the transformation. Recent deep\nlearning-based deformable image registration methods achieve fast image\nregistration by leveraging a convolutional neural network (CNN) to learn the\nspatial transformation from the synthetic ground truth or the similarity\nmetric. However, these approaches often ignore the topology preservation of the\ntransformation and the smoothness of the transformation which is enforced by a\nglobal smoothing energy function alone. Moreover, deep learning-based\napproaches often estimate the displacement field directly, which cannot\nguarantee the existence of the inverse transformation. In this paper, we\npresent a novel, efficient unsupervised symmetric image registration method\nwhich maximizes the similarity between images within the space of diffeomorphic\nmaps and estimates both forward and inverse transformations simultaneously. We\nevaluate our method on 3D image registration with a large scale brain image\ndataset. Our method achieves state-of-the-art registration accuracy and running\ntime while maintaining desirable diffeomorphic properties.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 22:07:24 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 16:29:48 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 08:43:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mok", "Tony C. W.", ""], ["Chung", "Albert C. S.", ""]]}, {"id": "2003.09545", "submitter": "Francesco Pittaluga", "authors": "Francesco Pittaluga, Zaid Tasneem, Justin Folden, Brevin Tilmon, Ayan\n  Chakrabarti, Sanjeev J. Koppal", "title": "Towards a MEMS-based Adaptive LIDAR", "comments": "14 pages, 5 figures, project site:\n  https://www.fpittaluga.com/adaptivelidar, to be published in International\n  Conference on 3D Vision 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a proof-of-concept LIDAR design that allows adaptive real-time\nmeasurements according to dynamically specified measurement patterns. We\ndescribe our optical setup and calibration, which enables fast sparse depth\nmeasurements using a scanning MEMS (micro-electro-mechanical) mirror. We\nvalidate the efficacy of our prototype LIDAR design by testing on over 75\nstatic and dynamic scenes spanning a range of environments. We show CNN-based\ndepth-map completion experiments which demonstrate that our sensor can realize\nadaptive depth sensing for dynamic scenes.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 01:11:50 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 05:12:19 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Pittaluga", "Francesco", ""], ["Tasneem", "Zaid", ""], ["Folden", "Justin", ""], ["Tilmon", "Brevin", ""], ["Chakrabarti", "Ayan", ""], ["Koppal", "Sanjeev J.", ""]]}, {"id": "2003.09553", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, Franziska Meier, Roberto Calandra, Trevor Darrell,\n  Marcus Rohrbach", "title": "Adversarial Continual Learning", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning aims to learn new tasks without forgetting previously\nlearned ones. We hypothesize that representations learned to solve each task in\na sequence have a shared structure while containing some task-specific\nproperties. We show that shared features are significantly less prone to\nforgetting and propose a novel hybrid continual learning framework that learns\na disjoint representation for task-invariant and task-specific features\nrequired to solve a sequence of tasks. Our model combines architecture growth\nto prevent forgetting of task-specific skills and an experience replay approach\nto preserve shared skills. We demonstrate our hybrid approach is effective in\navoiding forgetting and show it is superior to both architecture-based and\nmemory-based approaches on class incrementally learning of a single dataset as\nwell as a sequence of multiple datasets in image classification. Our code is\navailable at\n\\url{https://github.com/facebookresearch/Adversarial-Continual-Learning}.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:08:17 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 15:42:20 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Meier", "Franziska", ""], ["Calandra", "Roberto", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Marcus", ""]]}, {"id": "2003.09556", "submitter": "Koteswar Rao Jerripothula", "authors": "Koteswar Rao Jerripothula", "title": "Appearance Fusion of Multiple Cues for Video Co-localization", "comments": "17 Pages and 8 figures. Submitted to ACCV20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the joint object discovery problem in videos while\nutilizing multiple object-related cues. In contrast to the usual spatial fusion\napproach, a novel appearance fusion approach is presented here. Specifically,\nthis paper proposes an effective fusion process of different GMMs derived from\nmultiple cues into one GMM. Much the same as any fusion strategy, this approach\nalso needs some guidance. The proposed method relies on reliability and\nconsensus phenomenon for guidance. As a case study, we pursue the \"video\nco-localization\" object discovery problem to propose our methodology. Our\nexperiments on YouTube Objects and YouTube Co-localization datasets demonstrate\nthat the proposed method of appearance fusion undoubtedly has an advantage over\nboth the spatial fusion strategy and the current state-of-the-art video\nco-localization methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:26:36 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 04:53:03 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Jerripothula", "Koteswar Rao", ""]]}, {"id": "2003.09565", "submitter": "Abhishek Aich", "authors": "Abhishek Aich, Akash Gupta, Rameswar Panda, Rakib Hyder, M. Salman\n  Asif, Amit K. Roy-Chowdhury", "title": "Non-Adversarial Video Synthesis with Learned Priors", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing works in video synthesis focus on generating videos\nusing adversarial learning. Despite their success, these methods often require\ninput reference frame or fail to generate diverse videos from the given data\ndistribution, with little to no uniformity in the quality of videos that can be\ngenerated. Different from these methods, we focus on the problem of generating\nvideos from latent noise vectors, without any reference input frames. To this\nend, we develop a novel approach that jointly optimizes the input latent space,\nthe weights of a recurrent neural network and a generator through\nnon-adversarial learning. Optimizing for the input latent space along with the\nnetwork weights allows us to generate videos in a controlled environment, i.e.,\nwe can faithfully generate all videos the model has seen during the learning\nprocess as well as new unseen videos. Extensive experiments on three\nchallenging and diverse datasets well demonstrate that our approach generates\nsuperior quality videos compared to the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 02:57:33 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 22:11:29 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 20:54:58 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Aich", "Abhishek", ""], ["Gupta", "Akash", ""], ["Panda", "Rameswar", ""], ["Hyder", "Rakib", ""], ["Asif", "M. Salman", ""], ["Roy-Chowdhury", "Amit K.", ""]]}, {"id": "2003.09572", "submitter": "Yuxiao Zhou", "authors": "Yuxiao Zhou and Marc Habermann and Weipeng Xu and Ikhsanul Habibie and\n  Christian Theobalt and Feng Xu", "title": "Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for monocular hand shape and pose estimation at\nunprecedented runtime performance of 100fps and at state-of-the-art accuracy.\nThis is enabled by a new learning based architecture designed such that it can\nmake use of all the sources of available hand training data: image data with\neither 2D or 3D annotations, as well as stand-alone 3D animations without\ncorresponding image data. It features a 3D hand joint detection module and an\ninverse kinematics module which regresses not only 3D joint positions but also\nmaps them to joint rotations in a single feed-forward pass. This output makes\nthe method more directly usable for applications in computer vision and\ngraphics compared to only regressing 3D joint positions. We demonstrate that\nour architectural design leads to a significant quantitative and qualitative\nimprovement over the state of the art on several challenging benchmarks. Our\nmodel is publicly available for future research.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 03:51:54 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 14:32:08 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zhou", "Yuxiao", ""], ["Habermann", "Marc", ""], ["Xu", "Weipeng", ""], ["Habibie", "Ikhsanul", ""], ["Theobalt", "Christian", ""], ["Xu", "Feng", ""]]}, {"id": "2003.09575", "submitter": "Yen-Cheng Liu", "authors": "Yen-Cheng Liu, Junjiao Tian, Chih-Yao Ma, Nathan Glaser, Chia-Wen Kuo\n  and Zsolt Kira", "title": "Who2com: Collaborative Perception via Learnable Handshake Communication", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the problem of collaborative perception, where\nrobots can combine their local observations with those of neighboring agents in\na learnable way to improve accuracy on a perception task. Unlike existing work\nin robotics and multi-agent reinforcement learning, we formulate the problem as\none where learned information must be shared across a set of agents in a\nbandwidth-sensitive manner to optimize for scene understanding tasks such as\nsemantic segmentation. Inspired by networking communication protocols, we\npropose a multi-stage handshake communication mechanism where the neural\nnetwork can learn to compress relevant information needed for each stage.\nSpecifically, a target agent with degraded sensor data sends a compressed\nrequest, the other agents respond with matching scores, and the target agent\ndetermines who to connect with (i.e., receive information from). We\nadditionally develop the AirSim-CP dataset and metrics based on the AirSim\nsimulator where a group of aerial robots perceive diverse landscapes, such as\nroads, grasslands, buildings, etc. We show that for the semantic segmentation\ntask, our handshake communication method significantly improves accuracy by\napproximately 20% over decentralized baselines, and is comparable to\ncentralized ones using a quarter of the bandwidth.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 04:16:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Liu", "Yen-Cheng", ""], ["Tian", "Junjiao", ""], ["Ma", "Chih-Yao", ""], ["Glaser", "Nathan", ""], ["Kuo", "Chia-Wen", ""], ["Kira", "Zsolt", ""]]}, {"id": "2003.09583", "submitter": "Daqi Liu", "authors": "Daqi Liu, Bo Chen, Tat-Jun Chin and Mark Rutten", "title": "Topological Sweep for Multi-Target Detection of Geostationary Space\n  Objects", "comments": "12 pages, 12 figures, accepted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2020.3021232", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conducting surveillance of the Earth's orbit is a key task towards achieving\nspace situational awareness (SSA). Our work focuses on the optical detection of\nman-made objects (e.g., satellites, space debris) in Geostationary orbit (GEO),\nwhich is home to major space assets such as telecommunications and navigational\nsatellites. GEO object detection is challenging due to the distance of the\ntargets, which appear as small dim points among a clutter of bright stars. In\nthis paper, we propose a novel multi-target detection technique based on\ntopological sweep, to find GEO objects from a short sequence of optical images.\nOur topological sweep technique exploits the geometric duality that underpins\nthe approximately linear trajectory of target objects across the input\nsequence, to extract the targets from significant clutter and noise. Unlike\nstandard multi-target methods, our algorithm deterministically solves a\ncombinatorial problem to ensure high-recall rates without requiring accurate\ninitializations. The usage of geometric duality also yields an algorithm that\nis computationally efficient and suitable for online processing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 06:00:41 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 15:29:20 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 06:06:10 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Liu", "Daqi", ""], ["Chen", "Bo", ""], ["Chin", "Tat-Jun", ""], ["Rutten", "Mark", ""]]}, {"id": "2003.09585", "submitter": "Aydogan Ozcan", "authors": "Yilin Luo, Luzhe Huang, Yair Rivenson, Aydogan Ozcan", "title": "Single-shot autofocusing of microscopy images using deep learning", "comments": "27 pages, 8 figures, 9 supplementary figures, 2 supplementary tables", "journal-ref": "ACS Photonics (2021)", "doi": "10.1021/acsphotonics.0c01774", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a deep learning-based offline autofocusing method, termed\nDeep-R, that is trained to rapidly and blindly autofocus a single-shot\nmicroscopy image of a specimen that is acquired at an arbitrary out-of-focus\nplane. We illustrate the efficacy of Deep-R using various tissue sections that\nwere imaged using fluorescence and brightfield microscopy modalities and\ndemonstrate snapshot autofocusing under different scenarios, such as a uniform\naxial defocus as well as a sample tilt within the field-of-view. Our results\nreveal that Deep-R is significantly faster when compared with standard online\nalgorithmic autofocusing methods. This deep learning-based blind autofocusing\nframework opens up new opportunities for rapid microscopic imaging of large\nsample areas, also reducing the photon dose on the sample.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 06:07:27 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 06:20:14 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Luo", "Yilin", ""], ["Huang", "Luzhe", ""], ["Rivenson", "Yair", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "2003.09595", "submitter": "Bin Yan", "authors": "Bin Yan and Dong Wang and Huchuan Lu and Xiaoyun Yang", "title": "Cooling-Shrinking Attack: Blinding the Tracker with Imperceptible Noises", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attack of CNN aims at deceiving models to misbehave by adding\nimperceptible perturbations to images. This feature facilitates to understand\nneural networks deeply and to improve the robustness of deep learning models.\nAlthough several works have focused on attacking image classifiers and object\ndetectors, an effective and efficient method for attacking single object\ntrackers of any target in a model-free way remains lacking. In this paper, a\ncooling-shrinking attack method is proposed to deceive state-of-the-art\nSiameseRPN-based trackers. An effective and efficient perturbation generator is\ntrained with a carefully designed adversarial loss, which can simultaneously\ncool hot regions where the target exists on the heatmaps and force the\npredicted bounding box to shrink, making the tracked target invisible to\ntrackers. Numerous experiments on OTB100, VOT2018, and LaSOT datasets show that\nour method can effectively fool the state-of-the-art SiameseRPN++ tracker by\nadding small perturbations to the template or the search regions. Besides, our\nmethod has good transferability and is able to deceive other top-performance\ntrackers such as DaSiamRPN, DaSiamRPN-UpdateNet, and DiMP. The source codes are\navailable at https://github.com/MasterBin-IIAU/CSA.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 07:13:40 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Yan", "Bin", ""], ["Wang", "Dong", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2003.09600", "submitter": "Lingfeng Li", "authors": "Lingfeng li and Shousheng Luo and Xue-Cheng Tai and Jiang Yang", "title": "A level set representation method for N-dimensional convex shape and\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new efficient method for convex shape\nrepresentation, which is regardless of the dimension of the concerned objects,\nusing level-set approaches. Convexity prior is very useful for object\ncompletion in computer vision. It is a very challenging task to design an\nefficient method for high dimensional convex objects representation. In this\npaper, we prove that the convexity of the considered object is equivalent to\nthe convexity of the associated signed distance function. Then, the second\norder condition of convex functions is used to characterize the shape convexity\nequivalently. We apply this new method to two applications: object segmentation\nwith convexity prior and convex hull problem (especially with outliers). For\nboth applications, the involved problems can be written as a general\noptimization problem with three constraints. Efficient algorithm based on\nalternating direction method of multipliers is presented for the optimization\nproblem. Numerical experiments are conducted to verify the effectiveness and\nefficiency of the proposed representation method and algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 07:37:44 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["li", "Lingfeng", ""], ["Luo", "Shousheng", ""], ["Tai", "Xue-Cheng", ""], ["Yang", "Jiang", ""]]}, {"id": "2003.09669", "submitter": "Dechun Cong", "authors": "Quan Zhou, Dechun Cong, Bin Kang, Xiaofu Wu, Baoyu Zheng, Huimin Lu\n  and Longin Jan Latecki", "title": "BiCANet: Bi-directional Contextual Aggregating Network for Image\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring contextual information in convolution neural networks (CNNs) has\ngained substantial attention in recent years for semantic segmentation. This\npaper introduces a Bi-directional Contextual Aggregating Network, called\nBiCANet, for semantic segmentation. Unlike previous approaches that encode\ncontext in feature space, BiCANet aggregates contextual cues from a categorical\nperspective, which is mainly consist of three parts: contextual condensed\nprojection block (CCPB), bi-directional context interaction block (BCIB), and\nmuti-scale contextual fusion block (MCFB). More specifically, CCPB learns a\ncategory-based mapping through a split-transform-merge architecture, which\ncondenses contextual cues with different receptive fields from intermediate\nlayer. BCIB, on the other hand, employs dense skipped-connections to enhance\nthe class-level context exchanging. Finally, MCFB integrates multi-scale\ncontextual cues by investigating short- and long-ranged spatial dependencies.\nTo evaluate BiCANet, we have conducted extensive experiments on three semantic\nsegmentation datasets: PASCAL VOC 2012, Cityscapes, and ADE20K. The\nexperimental results demonstrate that BiCANet outperforms recent\nstate-of-the-art networks without any postprocess techniques. Particularly,\nBiCANet achieves the mIoU score of 86.7%, 82.4% and 38.66% on PASCAL VOC 2012,\nCityscapes and ADE20K testset, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 14:23:04 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhou", "Quan", ""], ["Cong", "Dechun", ""], ["Kang", "Bin", ""], ["Wu", "Xiaofu", ""], ["Zheng", "Baoyu", ""], ["Lu", "Huimin", ""], ["Latecki", "Longin Jan", ""]]}, {"id": "2003.09671", "submitter": "Bernhard C. Geiger", "authors": "Bernhard C. Geiger", "title": "On Information Plane Analyses of Neural Network Classifiers -- A Review", "comments": "12 pages, 3 figures; accepted for publication in IEEE Transactions on\n  Neural Networks and Learning Systems. (c) 2021 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the current literature concerned with information plane analyses of\nneural network classifiers. While the underlying information bottleneck theory\nand the claim that information-theoretic compression is causally linked to\ngeneralization are plausible, empirical evidence was found to be both\nsupporting and conflicting. We review this evidence together with a detailed\nanalysis of how the respective information quantities were estimated. Our\nsurvey suggests that compression visualized in information planes is not\nnecessarily information-theoretic, but is rather often compatible with\ngeometric compression of the latent representations. This insight gives the\ninformation plane a renewed justification.\n  Aside from this, we shed light on the problem of estimating mutual\ninformation in deterministic neural networks and its consequences.\nSpecifically, we argue that even in feed-forward neural networks the data\nprocessing inequality need not hold for estimates of mutual information.\nSimilarly, while a fitting phase, in which the mutual information between the\nlatent representation and the target increases, is necessary (but not\nsufficient) for good classification performance, depending on the specifics of\nmutual information estimation such a fitting phase need not be visible in the\ninformation plane.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 14:43:45 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 15:19:33 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 15:06:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Geiger", "Bernhard C.", ""]]}, {"id": "2003.09682", "submitter": "Janine Thoma", "authors": "Janine Thoma, Danda Pani Paudel, Ajad Chhatkuli, Luc Van Gool", "title": "Geometrically Mappable Image Features", "comments": "Implementation available at\n  https://github.com/janinethoma/geometrically_mappable", "journal-ref": "IEEE Robotics and Automation Letters 5, no. 2 (2020): 2062-2069", "doi": "10.1109/LRA.2020.2970616", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based localization of an agent in a map is an important problem in\nrobotics and computer vision. In that context, localization by learning\nmatchable image features is gaining popularity due to recent advances in\nmachine learning. Features that uniquely describe the visual contents of images\nhave a wide range of applications, including image retrieval and understanding.\nIn this work, we propose a method that learns image features targeted for\nimage-retrieval-based localization. Retrieval-based localization has several\nbenefits, such as easy maintenance and quick computation. However, the\nstate-of-the-art features only provide visual similarity scores which do not\nexplicitly reveal the geometric distance between query and retrieved images.\nKnowing this distance is highly desirable for accurate localization, especially\nwhen the reference images are sparsely distributed in the scene. Therefore, we\npropose a novel loss function for learning image features which are both\nvisually representative and geometrically relatable. This is achieved by\nguiding the learning process such that the feature and geometric distances\nbetween images are directly proportional. In our experiments we show that our\nfeatures not only offer significantly better localization accuracy, but also\nallow to estimate the trajectory of a query sequence in absence of the\nreference images.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 15:36:38 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Thoma", "Janine", ""], ["Paudel", "Danda Pani", ""], ["Chhatkuli", "Ajad", ""], ["Van Gool", "Luc", ""]]}, {"id": "2003.09689", "submitter": "Yulong Fan", "authors": "Yulong Fan, Rong Chen, Bo Li", "title": "Multi-Task Learning Enhanced Single Image De-Raining", "comments": "15 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rain removal in images is an important task in computer vision filed and\nattracting attentions of more and more people. In this paper, we address a\nnon-trivial issue of removing visual effect of rain streak from a single image.\nDiffering from existing work, our method combines various semantic constraint\ntask in a proposed multi-task regression model for rain removal. These tasks\nreinforce the model's capabilities from the content, edge-aware, and local\ntexture similarity respectively. To further improve the performance of\nmulti-task learning, we also present two simple but powerful dynamic weighting\nalgorithms. The proposed multi-task enhanced network (MENET) is a powerful\nconvolutional neural network based on U-Net for rain removal research, with a\nspecific focus on utilize multiple tasks constraints and exploit the synergy\namong them to facilitate the model's rain removal capacity. It is noteworthy\nthat the adaptive weighting scheme has further resulted in improved network\ncapability. We conduct several experiments on synthetic and real rain images,\nand achieve superior rain removal performance over several selected\nstate-of-the-art (SOTA) approaches. The overall effect of our method is\nimpressive, even in the decomposition of heavy rain and rain streak\naccumulation.The source code and some results can be found\nat:https://github.com/SumiHui/MENET.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 16:19:56 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 13:23:06 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Fan", "Yulong", ""], ["Chen", "Rong", ""], ["Li", "Bo", ""]]}, {"id": "2003.09691", "submitter": "Victoria Fernandez Abrevaya", "authors": "Victoria Fernandez Abrevaya, Adnane Boukhayma, Philip H. S. Torr,\n  Edmond Boyer", "title": "Cross-modal Deep Face Normals with Deactivable Skip Connections", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for estimating surface normals from in-the-wild color\nimages of faces. While data-driven strategies have been proposed for single\nface images, limited available ground truth data makes this problem difficult.\nTo alleviate this issue, we propose a method that can leverage all available\nimage and normal data, whether paired or not, thanks to a novel cross-modal\nlearning architecture. In particular, we enable additional training with single\nmodality data, either color or normal, by using two encoder-decoder networks\nwith a shared latent space. The proposed architecture also enables face details\nto be transferred between the image and normal domains, given paired data,\nthrough skip connections between the image encoder and normal decoder. Core to\nour approach is a novel module that we call deactivable skip connections, which\nallows integrating both the auto-encoded and image-to-normal branches within\nthe same architecture that can be trained end-to-end. This allows learning of a\nrich latent space that can accurately capture the normal information. We\ncompare against state-of-the-art methods and show that our approach can achieve\nsignificant improvements, both quantitative and qualitative, with natural face\nimages.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 16:26:59 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 13:54:14 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Abrevaya", "Victoria Fernandez", ""], ["Boukhayma", "Adnane", ""], ["Torr", "Philip H. S.", ""], ["Boyer", "Edmond", ""]]}, {"id": "2003.09717", "submitter": "Yang Feng", "authors": "Yang Feng, Yu Wang, Jiebo Luo", "title": "Video-based Person Re-Identification using Gated Convolutional Recurrent\n  Neural Networks", "comments": "This work was done in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been successfully applied to solving the\nvideo-based person re-identification problem with impressive results reported.\nThe existing networks for person re-id are designed to extract discriminative\nfeatures that preserve the identity information. Usually, whole video frames\nare fed into the neural networks and all the regions in a frame are equally\ntreated. This may be a suboptimal choice because many regions, e.g., background\nregions in the video, are not related to the person. Furthermore, the person of\ninterest may be occluded by another person or something else. These unrelated\nregions may hinder person re-identification. In this paper, we introduce a\nnovel gating mechanism to deep neural networks. Our gating mechanism will learn\nwhich regions are helpful for person re-identification and let these regions\npass the gate. The unrelated background regions or occluding regions are\nfiltered out by the gate. In each frame, the color channels and optical flow\nchannels provide quite different information. To better leverage such\ninformation, we generate one gate using the color channels and another gate\nusing the optical flow channels. These two gates are combined to provide a more\nreliable gate with a novel fusion method. Experimental results on two major\ndatasets demonstrate the performance improvements due to the proposed gating\nmechanism.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 18:15:37 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Feng", "Yang", ""], ["Wang", "Yu", ""], ["Luo", "Jiebo", ""]]}, {"id": "2003.09754", "submitter": "Yichen Li", "authors": "Yichen Li and Kaichun Mo and Lin Shao and Minhyuk Sung and Leonidas\n  Guibas", "title": "Learning 3D Part Assembly from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous assembly is a crucial capability for robots in many applications.\nFor this task, several problems such as obstacle avoidance, motion planning,\nand actuator control have been extensively studied in robotics. However, when\nit comes to task specification, the space of possibilities remains\nunderexplored. Towards this end, we introduce a novel problem,\nsingle-image-guided 3D part assembly, along with a learningbased solution. We\nstudy this problem in the setting of furniture assembly from a given complete\nset of parts and a single image depicting the entire assembled object. Multiple\nchallenges exist in this setting, including handling ambiguity among parts\n(e.g., slats in a chair back and leg stretchers) and 3D pose prediction for\nparts and part subassemblies, whether visible or occluded. We address these\nissues by proposing a two-module pipeline that leverages strong 2D-3D\ncorrespondences and assembly-oriented graph message-passing to infer part\nrelationships. In experiments with a PartNet-based synthetic benchmark, we\ndemonstrate the effectiveness of our framework as compared with three baseline\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 21:19:28 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 17:44:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Yichen", ""], ["Mo", "Kaichun", ""], ["Shao", "Lin", ""], ["Sung", "Minhyuk", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2003.09763", "submitter": "Minghan Zhu", "authors": "Minghan Zhu, Maani Ghaffari, Yuanxin Zhong, Pingping Lu, Zhong Cao,\n  Ryan M. Eustice and Huei Peng", "title": "Monocular Depth Prediction through Continuous 3D Loss", "comments": "8 pages, 4 figures. Accepted by IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports a new continuous 3D loss function for learning depth from\nmonocular images. The dense depth prediction from a monocular image is\nsupervised using sparse LIDAR points, which enables us to leverage available\nopen source datasets with camera-LIDAR sensor suites during training.\nCurrently, accurate and affordable range sensor is not readily available.\nStereo cameras and LIDARs measure depth either inaccurately or sparsely/costly.\nIn contrast to the current point-to-point loss evaluation approach, the\nproposed 3D loss treats point clouds as continuous objects; therefore, it\ncompensates for the lack of dense ground truth depth due to LIDAR's sparsity\nmeasurements. We applied the proposed loss in three state-of-the-art monocular\ndepth prediction approaches DORN, BTS, and Monodepth2. Experimental evaluation\nshows that the proposed loss improves the depth prediction accuracy and\nproduces point-clouds with more consistent 3D geometric structures compared\nwith all tested baselines, implying the benefit of the proposed loss on general\ndepth prediction networks. A video demo of this work is available at\nhttps://youtu.be/5HL8BjSAY4Y.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 22:47:12 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 20:36:41 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Zhu", "Minghan", ""], ["Ghaffari", "Maani", ""], ["Zhong", "Yuanxin", ""], ["Lu", "Pingping", ""], ["Cao", "Zhong", ""], ["Eustice", "Ryan M.", ""], ["Peng", "Huei", ""]]}, {"id": "2003.09764", "submitter": "Roy Or-El", "authors": "Roy Or-El, Soumyadip Sengupta, Ohad Fried, Eli Shechtman, Ira\n  Kemelmacher-Shlizerman", "title": "Lifespan Age Transformation Synthesis", "comments": "ECCV 2020 Camera-Ready version. Main Changes: 1. Added Ethics & Bias\n  statement in the supplementary material 2. Comparison figures to PyGAN [46]\n  and S2GAN [13] were removed due to copyright issues. These figures can be\n  found in the project's webpage (link is provided in the paper). 3. Added\n  links to the code and dataset (Github)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We address the problem of single photo age progression and regression-the\nprediction of how a person might look in the future, or how they looked in the\npast. Most existing aging methods are limited to changing the texture,\noverlooking transformations in head shape that occur during the human aging and\ngrowth process. This limits the applicability of previous methods to aging of\nadults to slightly older adults, and application of those methods to photos of\nchildren does not produce quality results. We propose a novel multi-domain\nimage-to-image generative adversarial network architecture, whose learned\nlatent space models a continuous bi-directional aging process. The network is\ntrained on the FFHQ dataset, which we labeled for ages, gender, and semantic\nsegmentation. Fixed age classes are used as anchors to approximate continuous\nage transformation. Our framework can predict a full head portrait for ages\n0-70 from a single photo, modifying both texture and shape of the head. We\ndemonstrate results on a wide variety of photos and datasets, and show\nsignificant improvement over the state of the art.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 22:48:06 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 12:08:55 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Or-El", "Roy", ""], ["Sengupta", "Soumyadip", ""], ["Fried", "Ohad", ""], ["Shechtman", "Eli", ""], ["Kemelmacher-Shlizerman", "Ira", ""]]}, {"id": "2003.09773", "submitter": "Chiranjibi Sitaula", "authors": "Chiranjibi Sitaula and Yong Xiang and Anish Basnet and Sunil Aryal and\n  Xuequan Lu", "title": "HDF: Hybrid Deep Features for Scene Image Representation", "comments": "8 pages, Accepted in IEEE WCCI 2020 Conference", "journal-ref": "Proceedings of IJCNN2020", "doi": "10.1109/IJCNN48605.2020.9207106", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays it is prevalent to take features extracted from pre-trained deep\nlearning models as image representations which have achieved promising\nclassification performance. Existing methods usually consider either\nobject-based features or scene-based features only. However, both types of\nfeatures are important for complex images like scene images, as they can\ncomplement each other. In this paper, we propose a novel type of features --\nhybrid deep features, for scene images. Specifically, we exploit both\nobject-based and scene-based features at two levels: part image level (i.e.,\nparts of an image) and whole image level (i.e., a whole image), which produces\na total number of four types of deep features. Regarding the part image level,\nwe also propose two new slicing techniques to extract part based features.\nFinally, we aggregate these four types of deep features via the concatenation\noperator. We demonstrate the effectiveness of our hybrid deep features on three\ncommonly used scene datasets (MIT-67, Scene-15, and Event-8), in terms of the\nscene image classification task. Extensive comparisons show that our introduced\nfeatures can produce state-of-the-art classification accuracies which are more\nconsistent and stable than the results of existing features across all\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 01:05:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sitaula", "Chiranjibi", ""], ["Xiang", "Yong", ""], ["Basnet", "Anish", ""], ["Aryal", "Sunil", ""], ["Lu", "Xuequan", ""]]}, {"id": "2003.09784", "submitter": "Yonghui Zhang", "authors": "Yonghui Zhang (1-4), Ke Gu (1-4) ((1) Engineering Research Center of\n  Intelligent Perception and Autonomous Control, AMinistry of Education, (2)\n  Beijing Key Laboratory of Computational Intelligence and Intelligent System,\n  (3) Beijing Key Laboratory of Computational Intelligence and Intelligent\n  System, (4) Faculty of Information Technology, Beijing University of\n  Technology, China)", "title": "AQPDCITY Dataset: Picture-Based PM Monitoring in the Urban Area of Big\n  Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Particulate Matters (PMs) are closely related to people's living and\nhealth, it has become one of the most important indicator of air quality\nmonitoring around the world. But the existing sensor-based methods for PM\nmonitoring have remarkable disadvantages, such as low-density monitoring\nstations and high-requirement monitoring conditions. It is highly desired to\ndevise a method that can obtain the PM concentration at any location for the\nfollowing air quality control in time. The prior works indicate that the PM\nconcentration can be monitored by using ubiquitous photos. To further\ninvestigate such issue, we gathered 1,500 photos in big cities to establish a\nnew AQPDCITY dataset. Experiments conducted to check nine state-of-the-art\nmethods on this dataset show that the performance of those above methods\nperform poorly in the AQPDCITY dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 02:37:11 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:32:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhang", "Yonghui", "", "1-4"], ["Gu", "Ke", "", "1-4"]]}, {"id": "2003.09785", "submitter": "Ziyi Zhao", "authors": "Ziyi Zhao, Zhao Jin, Wentian Bai, Wentan Bai, Carlos Caicedo, M. Cenk\n  Gursoy, Qinru Qiu", "title": "Mission-Aware Spatio-Temporal Deep Learning Model for UAS Instantaneous\n  Density Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of daily sUAS operations in uncontrolled low altitude airspace is\nexpected to reach into the millions in a few years. Therefore, UAS density\nprediction has become an emerging and challenging problem. In this paper, a\ndeep learning-based UAS instantaneous density prediction model is presented.\nThe model takes two types of data as input: 1) the historical density generated\nfrom the historical data, and 2) the future sUAS mission information. The\narchitecture of our model contains four components: Historical Density\nFormulation module, UAS Mission Translation module, Mission Feature Extraction\nmodule, and Density Map Projection module. The training and testing data are\ngenerated by a python based simulator which is inspired by the multi-agent air\ntraffic resource usage simulator (MATRUS) framework. The quality of prediction\nis measured by the correlation score and the Area Under the Receiver Operating\nCharacteristics (AUROC) between the predicted value and simulated value. The\nexperimental results demonstrate outstanding performance of the deep\nlearning-based UAS density predictor. Compared to the baseline models, for\nsimplified traffic scenario where no-fly zones and safe distance among sUASs\nare not considered, our model improves the prediction accuracy by more than\n15.2% and its correlation score reaches 0.947. In a more realistic scenario,\nwhere the no-fly zone avoidance and the safe distance among sUASs are\nmaintained using A* routing algorithm, our model can still achieve 0.823\ncorrelation score. Meanwhile, the AUROC can reach 0.951 for the hot spot\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 02:40:28 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhao", "Ziyi", ""], ["Jin", "Zhao", ""], ["Bai", "Wentian", ""], ["Bai", "Wentan", ""], ["Caicedo", "Carlos", ""], ["Gursoy", "M. Cenk", ""], ["Qiu", "Qinru", ""]]}, {"id": "2003.09790", "submitter": "Zhonghua Wu", "authors": "Zhonghua Wu and Qingyi Tao and Guosheng Lin and Jianfei Cai", "title": "Exploring Bottom-up and Top-down Cues with Attentive Learning for Webly\n  Supervised Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully supervised object detection has achieved great success in recent years.\nHowever, abundant bounding boxes annotations are needed for training a detector\nfor novel classes. To reduce the human labeling effort, we propose a novel\nwebly supervised object detection (WebSOD) method for novel classes which only\nrequires the web images without further annotations. Our proposed method\ncombines bottom-up and top-down cues for novel class detection. Within our\napproach, we introduce a bottom-up mechanism based on the well-trained fully\nsupervised object detector (i.e. Faster RCNN) as an object region estimator for\nweb images by recognizing the common objectiveness shared by base and novel\nclasses. With the estimated regions on the web images, we then utilize the\ntop-down attention cues as the guidance for region classification. Furthermore,\nwe propose a residual feature refinement (RFR) block to tackle the domain\nmismatch between web domain and the target domain. We demonstrate our proposed\nmethod on PASCAL VOC dataset with three different novel/base splits. Without\nany target-domain novel-class images and annotations, our proposed webly\nsupervised object detection model is able to achieve promising performance for\nnovel classes. Moreover, we also conduct transfer learning experiments on large\nscale ILSVRC 2013 detection dataset and achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 03:11:24 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wu", "Zhonghua", ""], ["Tao", "Qingyi", ""], ["Lin", "Guosheng", ""], ["Cai", "Jianfei", ""]]}, {"id": "2003.09799", "submitter": "Jiamiao Xu", "authors": "Jiamiao Xu, Fangzhao Wang, Qinmu Peng, Xinge You, Shuo Wang, Xiao-Yuan\n  Jing, C. L. Philip Chen", "title": "Modal Regression based Structured Low-rank Matrix Recovery for\n  Multi-view Learning", "comments": "This article has been accepted by IEEE Transactions on Neural\n  Networks and Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank Multi-view Subspace Learning (LMvSL) has shown great potential in\ncross-view classification in recent years. Despite their empirical success,\nexisting LMvSL based methods are incapable of well handling view discrepancy\nand discriminancy simultaneously, which thus leads to the performance\ndegradation when there is a large discrepancy among multi-view data. To\ncircumvent this drawback, motivated by the block-diagonal representation\nlearning, we propose Structured Low-rank Matrix Recovery (SLMR), a unique\nmethod of effectively removing view discrepancy and improving discriminancy\nthrough the recovery of structured low-rank matrix. Furthermore, recent\nlow-rank modeling provides a satisfactory solution to address data contaminated\nby predefined assumptions of noise distribution, such as Gaussian or Laplacian\ndistribution. However, these models are not practical since complicated noise\nin practice may violate those assumptions and the distribution is generally\nunknown in advance. To alleviate such limitation, modal regression is elegantly\nincorporated into the framework of SLMR (term it MR-SLMR). Different from\nprevious LMvSL based methods, our MR-SLMR can handle any zero-mode noise\nvariable that contains a wide range of noise, such as Gaussian noise, random\nnoise and outliers. The alternating direction method of multipliers (ADMM)\nframework and half-quadratic theory are used to efficiently optimize MR-SLMR.\nExperimental results on four public databases demonstrate the superiority of\nMR-SLMR and its robustness to complicated noise.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 03:57:38 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xu", "Jiamiao", ""], ["Wang", "Fangzhao", ""], ["Peng", "Qinmu", ""], ["You", "Xinge", ""], ["Wang", "Shuo", ""], ["Jing", "Xiao-Yuan", ""], ["Chen", "C. L. Philip", ""]]}, {"id": "2003.09802", "submitter": "Xinyu Liu", "authors": "Xinyu Liu, Xiren Miao, Hao Jiang, Jing Chen", "title": "Review of data analysis in vision inspection of power lines with an\n  in-depth discussion of deep learning technology", "comments": null, "journal-ref": null, "doi": "10.1016/j.arcontrol.2020.09.002", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread popularity of unmanned aerial vehicles enables an immense\namount of power lines inspection data to be collected. How to employ massive\ninspection data especially the visible images to maintain the reliability,\nsafety, and sustainability of power transmission is a pressing issue. To date,\nsubstantial works have been conducted on the analysis of power lines inspection\ndata. With the aim of providing a comprehensive overview for researchers who\nare interested in developing a deep-learning-based analysis system for power\nlines inspection data, this paper conducts a thorough review of the current\nliterature and identifies the challenges for future research. Following the\ntypical procedure of inspection data analysis, we categorize current works in\nthis area into component detection and fault diagnosis. For each aspect, the\ntechniques and methodologies adopted in the literature are summarized. Some\nvaluable information is also included such as data description and method\nperformance. Further, an in-depth discussion of existing deep-learning-related\nanalysis methods in power lines inspection is proposed. Finally, we conclude\nthe paper with several research trends for the future of this area, such as\ndata quality problems, small object detection, embedded application, and\nevaluation baseline.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 04:09:59 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Liu", "Xinyu", ""], ["Miao", "Xiren", ""], ["Jiang", "Hao", ""], ["Chen", "Jing", ""]]}, {"id": "2003.09852", "submitter": "Lior Yariv", "authors": "Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Ronen\n  Basri, Yaron Lipman", "title": "Multiview Neural Surface Reconstruction by Disentangling Geometry and\n  Appearance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the challenging problem of multiview 3D surface\nreconstruction. We introduce a neural network architecture that simultaneously\nlearns the unknown geometry, camera parameters, and a neural renderer that\napproximates the light reflected from the surface towards the camera. The\ngeometry is represented as a zero level-set of a neural network, while the\nneural renderer, derived from the rendering equation, is capable of\n(implicitly) modeling a wide set of lighting conditions and materials. We\ntrained our network on real world 2D images of objects with different material\nproperties, lighting conditions, and noisy camera initializations from the DTU\nMVS dataset. We found our model to produce state of the art 3D surface\nreconstructions with high fidelity, resolution and detail.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 10:20:13 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 14:43:32 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 10:30:06 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Yariv", "Lior", ""], ["Kasten", "Yoni", ""], ["Moran", "Dror", ""], ["Galun", "Meirav", ""], ["Atzmon", "Matan", ""], ["Basri", "Ronen", ""], ["Lipman", "Yaron", ""]]}, {"id": "2003.09853", "submitter": "Federico Becattini", "authors": "Pietro Bongini, Federico Becattini, Andrew D. Bagdanov, Alberto Del\n  Bimbo", "title": "Visual Question Answering for Cultural Heritage", "comments": "accepted at FlorenceHeritech 2020", "journal-ref": null, "doi": "10.1088/1757-899X/949/1/012074", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology and the fruition of cultural heritage are becoming increasingly\nmore entwined, especially with the advent of smart audio guides, virtual and\naugmented reality, and interactive installations. Machine learning and computer\nvision are important components of this ongoing integration, enabling new\ninteraction modalities between user and museum. Nonetheless, the most frequent\nway of interacting with paintings and statues still remains taking pictures.\nYet images alone can only convey the aesthetics of the artwork, lacking is\ninformation which is often required to fully understand and appreciate it.\nUsually this additional knowledge comes both from the artwork itself (and\ntherefore the image depicting it) and from an external source of knowledge,\nsuch as an information sheet. While the former can be inferred by computer\nvision algorithms, the latter needs more structured data to pair visual content\nwith relevant information. Regardless of its source, this information still\nmust be be effectively transmitted to the user. A popular emerging trend in\ncomputer vision is Visual Question Answering (VQA), in which users can interact\nwith a neural network by posing questions in natural language and receiving\nanswers about the visual content. We believe that this will be the evolution of\nsmart audio guides for museum visits and simple image browsing on personal\nsmartphones. This will turn the classic audio guide into a smart personal\ninstructor with which the visitor can interact by asking for explanations\nfocused on specific interests. The advantages are twofold: on the one hand the\ncognitive burden of the visitor will decrease, limiting the flow of information\nto what the user actually wants to hear; and on the other hand it proposes the\nmost natural way of interacting with a guide, favoring engagement.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 10:26:08 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Bongini", "Pietro", ""], ["Becattini", "Federico", ""], ["Bagdanov", "Andrew D.", ""], ["Del Bimbo", "Alberto", ""]]}, {"id": "2003.09855", "submitter": "Xinyu Liu", "authors": "Xinyu Liu, Xiaoguang Di", "title": "TanhExp: A Smooth Activation Function with High Convergence Speed for\n  Lightweight Neural Networks", "comments": "This paper is a preprint of a paper accepted by IET Computer Vision\n  and is subject to Institution of Engineering and Technology Copyright. When\n  the final version is published, the copy of record will be available at the\n  IET Digital Library", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight or mobile neural networks used for real-time computer vision\ntasks contain fewer parameters than normal networks, which lead to a\nconstrained performance. In this work, we proposed a novel activation function\nnamed Tanh Exponential Activation Function (TanhExp) which can improve the\nperformance for these networks on image classification task significantly. The\ndefinition of TanhExp is f(x) = xtanh(e^x). We demonstrate the simplicity,\nefficiency, and robustness of TanhExp on various datasets and network models\nand TanhExp outperforms its counterparts in both convergence speed and\naccuracy. Its behaviour also remains stable even with noise added and dataset\naltered. We show that without increasing the size of the network, the capacity\nof lightweight neural networks can be enhanced by TanhExp with only a few\ntraining epochs and no extra parameters added.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 10:40:31 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 13:43:34 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Liu", "Xinyu", ""], ["Di", "Xiaoguang", ""]]}, {"id": "2003.09860", "submitter": "Feng Shi", "authors": "Feng Shi, Liming Xia, Fei Shan, Dijia Wu, Ying Wei, Huan Yuan, Huiting\n  Jiang, Yaozong Gao, He Sui, Dinggang Shen", "title": "Large-Scale Screening of COVID-19 from Community Acquired Pneumonia\n  using Infection Size-Aware Classification", "comments": null, "journal-ref": "Physics in Medicine & Biology (2021)", "doi": "10.1088/1361-6560/abe838", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worldwide spread of coronavirus disease (COVID-19) has become a\nthreatening risk for global public health. It is of great importance to rapidly\nand accurately screen patients with COVID-19 from community acquired pneumonia\n(CAP). In this study, a total of 1658 patients with COVID-19 and 1027 patients\nof CAP underwent thin-section CT. All images were preprocessed to obtain the\nsegmentations of both infections and lung fields, which were used to extract\nlocation-specific features. An infection Size Aware Random Forest method\n(iSARF) was proposed, in which subjects were automated categorized into groups\nwith different ranges of infected lesion sizes, followed by random forests in\neach group for classification. Experimental results show that the proposed\nmethod yielded sensitivity of 0.907, specificity of 0.833, and accuracy of\n0.879 under five-fold cross-validation. Large performance margins against\ncomparison methods were achieved especially for the cases with infection size\nin the medium range, from 0.01% to 10%. The further inclusion of Radiomics\nfeatures show slightly improvement. It is anticipated that our proposed\nframework could assist clinical decision making.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 11:12:06 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Shi", "Feng", ""], ["Xia", "Liming", ""], ["Shan", "Fei", ""], ["Wu", "Dijia", ""], ["Wei", "Ying", ""], ["Yuan", "Huan", ""], ["Jiang", "Huiting", ""], ["Gao", "Yaozong", ""], ["Sui", "He", ""], ["Shen", "Dinggang", ""]]}, {"id": "2003.09869", "submitter": "Xinxun Xu", "authors": "Xinxun Xu, Cheng Deng, Muli Yang and Hao Wang", "title": "Progressive Domain-Independent Feature Decomposition Network for\n  Zero-Shot Sketch-Based Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot sketch-based image retrieval (ZS-SBIR) is a specific cross-modal\nretrieval task for searching natural images given free-hand sketches under the\nzero-shot scenario. Most existing methods solve this problem by simultaneously\nprojecting visual features and semantic supervision into a low-dimensional\ncommon space for efficient retrieval. However, such low-dimensional projection\ndestroys the completeness of semantic knowledge in original semantic space, so\nthat it is unable to transfer useful knowledge well when learning semantic from\ndifferent modalities. Moreover, the domain information and semantic information\nare entangled in visual features, which is not conducive for cross-modal\nmatching since it will hinder the reduction of domain gap between sketch and\nimage. In this paper, we propose a Progressive Domain-independent Feature\nDecomposition (PDFD) network for ZS-SBIR. Specifically, with the supervision of\noriginal semantic knowledge, PDFD decomposes visual features into domain\nfeatures and semantic ones, and then the semantic features are projected into\ncommon space as retrieval features for ZS-SBIR. The progressive projection\nstrategy maintains strong semantic supervision. Besides, to guarantee the\nretrieval features to capture clean and complete semantic information, the\ncross-reconstruction loss is introduced to encourage that any combinations of\nretrieval features and domain features can reconstruct the visual features.\nExtensive experiments demonstrate the superiority of our PDFD over\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 12:07:23 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xu", "Xinxun", ""], ["Deng", "Cheng", ""], ["Yang", "Muli", ""], ["Wang", "Hao", ""]]}, {"id": "2003.09871", "submitter": "Alexander Wong", "authors": "Linda Wang and Alexander Wong", "title": "COVID-Net: A Tailored Deep Convolutional Neural Network Design for\n  Detection of COVID-19 Cases from Chest X-Ray Images", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic continues to have a devastating effect on the health\nand well-being of the global population. A critical step in the fight against\nCOVID-19 is effective screening of infected patients, with one of the key\nscreening approaches being radiology examination using chest radiography.\nMotivated by this and inspired by the open source efforts of the research\ncommunity, in this study we introduce COVID-Net, a deep convolutional neural\nnetwork design tailored for the detection of COVID-19 cases from chest X-ray\n(CXR) images that is open source and available to the general public. To the\nbest of the authors' knowledge, COVID-Net is one of the first open source\nnetwork designs for COVID-19 detection from CXR images at the time of initial\nrelease. We also introduce COVIDx, an open access benchmark dataset that we\ngenerated comprising of 13,975 CXR images across 13,870 patient patient cases,\nwith the largest number of publicly available COVID-19 positive cases to the\nbest of the authors' knowledge. Furthermore, we investigate how COVID-Net makes\npredictions using an explainability method in an attempt to not only gain\ndeeper insights into critical factors associated with COVID cases, which can\naid clinicians in improved screening, but also audit COVID-Net in a responsible\nand transparent manner to validate that it is making decisions based on\nrelevant information from the CXR images. By no means a production-ready\nsolution, the hope is that the open access COVID-Net, along with the\ndescription on constructing the open source COVIDx dataset, will be leveraged\nand build upon by both researchers and citizen data scientists alike to\naccelerate the development of highly accurate yet practical deep learning\nsolutions for detecting COVID-19 cases and accelerate treatment of those who\nneed it the most.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 12:26:36 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 18:05:03 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 04:32:20 GMT"}, {"version": "v4", "created": "Mon, 11 May 2020 17:48:55 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wang", "Linda", ""], ["Wong", "Alexander", ""]]}, {"id": "2003.09893", "submitter": "Sina Mohammadi", "authors": "Sina Mohammadi, Sina Ghofrani Majelan, Shahriar B. Shokouhi", "title": "Ensembles of Deep Neural Networks for Action Recognition in Still Images", "comments": "5 pages, 2 figures, 3 tables, Accepted by ICCKE 2019", "journal-ref": "2019 9th International Conference on Computer and Knowledge\n  Engineering (ICCKE), Mashhad, Iran, 2019, pp. 315-318", "doi": "10.1109/ICCKE48569.2019.8965014", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that notable improvements have been made recently in the\nfield of feature extraction and classification, human action recognition is\nstill challenging, especially in images, in which, unlike videos, there is no\nmotion. Thus, the methods proposed for recognizing human actions in videos\ncannot be applied to still images. A big challenge in action recognition in\nstill images is the lack of large enough datasets, which is problematic for\ntraining deep Convolutional Neural Networks (CNNs) due to the overfitting\nissue. In this paper, by taking advantage of pre-trained CNNs, we employ the\ntransfer learning technique to tackle the lack of massive labeled action\nrecognition datasets. Furthermore, since the last layer of the CNN has\nclass-specific information, we apply an attention mechanism on the output\nfeature maps of the CNN to extract more discriminative and powerful features\nfor classification of human actions. Moreover, we use eight different\npre-trained CNNs in our framework and investigate their performance on Stanford\n40 dataset. Finally, we propose using the Ensemble Learning technique to\nenhance the overall accuracy of action classification by combining the\npredictions of multiple models. The best setting of our method is able to\nachieve 93.17$\\%$ accuracy on the Stanford 40 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 13:44:09 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Mohammadi", "Sina", ""], ["Majelan", "Sina Ghofrani", ""], ["Shokouhi", "Shahriar B.", ""]]}, {"id": "2003.09934", "submitter": "Jingwei Song", "authors": "Jingwei Song, Shaobo Xia, Jun Wang, Dong Chen", "title": "Curved Buildings Reconstruction from Airborne LiDAR Data by Matching and\n  Deforming Geometric Primitives", "comments": "12 pages. 14 figures", "journal-ref": null, "doi": "10.1109/TGRS.2020.2995732", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne LiDAR (Light Detection and Ranging) data is widely applied in\nbuilding reconstruction, with studies reporting success in typical buildings.\nHowever, the reconstruction of curved buildings remains an open research\nproblem. To this end, we propose a new framework for curved building\nreconstruction via assembling and deforming geometric primitives. The input\nLiDAR point cloud are first converted into contours where individual buildings\nare identified. After recognizing geometric units (primitives) from building\ncontours, we get initial models by matching basic geometric primitives to these\nprimitives. To polish assembly models, we employ a warping field for model\nrefinements. Specifically, an embedded deformation (ED) graph is constructed\nvia downsampling the initial model. Then, the point-to-model displacements are\nminimized by adjusting node parameters in the ED graph based on our objective\nfunction. The presented framework is validated on several highly curved\nbuildings collected by various LiDAR in different cities. The experimental\nresults, as well as accuracy comparison, demonstrate the advantage and\neffectiveness of our method. {The new insight attributes to an efficient\nreconstruction manner.} Moreover, we prove that the primitive-based framework\nsignificantly reduces the data storage to 10-20 percent of classical mesh\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 16:05:10 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Song", "Jingwei", ""], ["Xia", "Shaobo", ""], ["Wang", "Jun", ""], ["Chen", "Dong", ""]]}, {"id": "2003.09970", "submitter": "Marcos Baptista Rios", "authors": "Marcos Baptista Rios, Roberto J. L\\'opez-Sastre, Fabian Caba Heilbron,\n  Jan van Gemert, Francisco Javier Acevedo-Rodr\\'iguez, and Saturnino\n  Maldonado-Basc\\'on", "title": "The Instantaneous Accuracy: a Novel Metric for the Problem of Online\n  Human Behaviour Recognition in Untrimmed Videos", "comments": "Published at ICCV 2019 workshop: Human Behaviour Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Online Human Behaviour Recognition in untrimmed videos, aka\nOnline Action Detection (OAD), needs to be revisited. Unlike traditional\noffline action detection approaches, where the evaluation metrics are clear and\nwell established, in the OAD setting we find few works and no consensus on the\nevaluation protocols to be used. In this paper we introduce a novel online\nmetric, the Instantaneous Accuracy ($IA$), that exhibits an \\emph{online}\nnature, solving most of the limitations of the previous (offline) metrics. We\nconduct a thorough experimental evaluation on TVSeries dataset, comparing the\nperformance of various baseline methods to the state of the art. Our results\nconfirm the problems of previous evaluation protocols, and suggest that an\nIA-based protocol is more adequate to the online scenario for human behaviour\nunderstanding. Code of the metric available https://github.com/gramuah/ia\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 19:04:05 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 10:06:37 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Rios", "Marcos Baptista", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""], ["Heilbron", "Fabian Caba", ""], ["van Gemert", "Jan", ""], ["Acevedo-Rodr\u00edguez", "Francisco Javier", ""], ["Maldonado-Basc\u00f3n", "Saturnino", ""]]}, {"id": "2003.09971", "submitter": "Ruotian Luo", "authors": "Ruotian Luo", "title": "A Better Variant of Self-Critical Sequence Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a simple yet better variant of Self-Critical\nSequence Training. We make a simple change in the choice of baseline function\nin REINFORCE algorithm. The new baseline can bring better performance with no\nextra cost, compared to the greedy decoding baseline.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 19:04:25 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 21:59:36 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Luo", "Ruotian", ""]]}, {"id": "2003.10010", "submitter": "Florian Shkurti", "authors": "Karim Koreitem, Florian Shkurti, Travis Manderson, Wei-Di Chang, Juan\n  Camilo Gamboa Higuera, Gregory Dudek", "title": "One-Shot Informed Robotic Visual Search in the Wild", "comments": "Accepted at IROS 2020. Code\n  https://github.com/rvl-lab-utoronto/visual_search_in_the_wild and videos\n  https://www.youtube.com/watch?v=0i_el5XGCus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of underwater robot navigation for the purpose of\ncollecting scientifically relevant video data for environmental monitoring. The\nmajority of field robots that currently perform monitoring tasks in\nunstructured natural environments navigate via path-tracking a pre-specified\nsequence of waypoints. Although this navigation method is often necessary, it\nis limiting because the robot does not have a model of what the scientist deems\nto be relevant visual observations. Thus, the robot can neither visually search\nfor particular types of objects, nor focus its attention on parts of the scene\nthat might be more relevant than the pre-specified waypoints and viewpoints. In\nthis paper we propose a method that enables informed visual navigation via a\nlearned visual similarity operator that guides the robot's visual search\ntowards parts of the scene that look like an exemplar image, which is given by\nthe user as a high-level specification for data collection. We propose and\nevaluate a weakly supervised video representation learning method that\noutperforms ImageNet embeddings for similarity tasks in the underwater domain.\nWe also demonstrate the deployment of this similarity operator during informed\nvisual navigation in collaborative environmental monitoring scenarios, in\nlarge-scale field trials, where the robot and a human scientist collaboratively\nsearch for relevant visual content.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 22:14:42 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 07:14:23 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Koreitem", "Karim", ""], ["Shkurti", "Florian", ""], ["Manderson", "Travis", ""], ["Chang", "Wei-Di", ""], ["Higuera", "Juan Camilo Gamboa", ""], ["Dudek", "Gregory", ""]]}, {"id": "2003.10016", "submitter": "Berk Kaya", "authors": "Berk Kaya, Radu Timofte", "title": "Self-Supervised 2D Image to 3D Shape Translation with Disentangled\n  Representations", "comments": "Published in 2020 International Conference on 3D Vision (3DV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to translate between 2D image views and 3D object\nshapes. Recent progress in deep learning enabled us to learn structure-aware\nrepresentations from a scene. However, the existing literature assumes that\npairs of images and 3D shapes are available for training in full supervision.\nIn this paper, we propose SIST, a Self-supervised Image to Shape Translation\nframework that fulfills three tasks: (i) reconstructing the 3D shape from a\nsingle image; (ii) learning disentangled representations for shape, appearance\nand viewpoint; and (iii) generating a realistic RGB image from these\nindependent factors. In contrast to the existing approaches, our method does\nnot require image-shape pairs for training. Instead, it uses unpaired image and\nshape datasets from the same object class and jointly trains image generator\nand shape reconstruction networks. Our translation method achieves promising\nresults, comparable in quantitative and qualitative terms to the\nstate-of-the-art achieved by fully-supervised methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 22:44:02 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 22:55:47 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Kaya", "Berk", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.10027", "submitter": "Yinpeng Chen", "authors": "Yinpeng Chen, Xiyang Dai, Mengchen Liu, Dongdong Chen, Lu Yuan, and\n  Zicheng Liu", "title": "Dynamic ReLU", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified linear units (ReLU) are commonly used in deep neural networks. So\nfar ReLU and its generalizations (non-parametric or parametric) are static,\nperforming identically for all input samples. In this paper, we propose dynamic\nReLU (DY-ReLU), a dynamic rectifier of which parameters are generated by a\nhyper function over all in-put elements. The key insight is that DY-ReLU\nencodes the global context into the hyper function, and adapts the piecewise\nlinear activation function accordingly. Compared to its static counterpart,\nDY-ReLU has negligible extra computational cost, but significantly more\nrepresentation capability, especially for light-weight neural networks. By\nsimply using DY-ReLU for MobileNetV2, the top-1 accuracy on ImageNet\nclassification is boosted from 72.0% to 76.2% with only 5% additional FLOPs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 23:45:35 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 17:46:33 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Chen", "Yinpeng", ""], ["Dai", "Xiyang", ""], ["Liu", "Mengchen", ""], ["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Liu", "Zicheng", ""]]}, {"id": "2003.10033", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Binod Bhattarai, Tae-Kyun Kim, and Jens Rittscher", "title": "Additive Angular Margin for Few Shot Learning to Classify Clinical\n  Endoscopy Images", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Endoscopy is a widely used imaging modality to diagnose and treat diseases in\nhollow organs as for example the gastrointestinal tract, the kidney and the\nliver. However, due to varied modalities and use of different imaging protocols\nat various clinical centers impose significant challenges when generalising\ndeep learning models. Moreover, the assembly of large datasets from different\nclinical centers can introduce a huge label bias that renders any learnt model\nunusable. Also, when using new modality or presence of images with rare\npatterns, a bulk amount of similar image data and their corresponding labels\nare required for training these models. In this work, we propose to use a\nfew-shot learning approach that requires less training data and can be used to\npredict label classes of test samples from an unseen dataset. We propose a\nnovel additive angular margin metric in the framework of prototypical network\nin few-shot learning setting. We compare our approach to the several\nestablished methods on a large cohort of multi-center, multi-organ, and\nmulti-modal endoscopy data. The proposed algorithm outperforms existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 00:20:52 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 20:28:04 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ali", "Sharib", ""], ["Bhattarai", "Binod", ""], ["Kim", "Tae-Kyun", ""], ["Rittscher", "Jens", ""]]}, {"id": "2003.10041", "submitter": "Taro Makino", "authors": "Witold Oleszkiewicz, Taro Makino, Stanis{\\l}aw Jastrz\\k{e}bski, Tomasz\n  Trzci\\'nski, Linda Moy, Kyunghyun Cho, Laura Heacock, Krzysztof J. Geras", "title": "Understanding the robustness of deep neural network classifiers for\n  breast cancer screening", "comments": "Accepted as a workshop paper at AI4AH, ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) show promise in breast cancer screening, but\ntheir robustness to input perturbations must be better understood before they\ncan be clinically implemented. There exists extensive literature on this\nsubject in the context of natural images that can potentially be built upon.\nHowever, it cannot be assumed that conclusions about robustness will transfer\nfrom natural images to mammogram images, due to significant differences between\nthe two image modalities. In order to determine whether conclusions will\ntransfer, we measure the sensitivity of a radiologist-level screening mammogram\nimage classifier to four commonly studied input perturbations that natural\nimage classifiers are sensitive to. We find that mammogram image classifiers\nare also sensitive to these perturbations, which suggests that we can build on\nthe existing literature. We also perform a detailed analysis on the effects of\nlow-pass filtering, and find that it degrades the visibility of clinically\nmeaningful features called microcalcifications. Since low-pass filtering\nremoves semantically meaningful information that is predictive of breast\ncancer, we argue that it is undesirable for mammogram image classifiers to be\ninvariant to it. This is in contrast to natural images, where we do not want\nDNNs to be sensitive to low-pass filtering due to its tendency to remove\ninformation that is human-incomprehensible.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 01:26:36 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Oleszkiewicz", "Witold", ""], ["Makino", "Taro", ""], ["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Trzci\u0144ski", "Tomasz", ""], ["Moy", "Linda", ""], ["Cho", "Kyunghyun", ""], ["Heacock", "Laura", ""], ["Geras", "Krzysztof J.", ""]]}, {"id": "2003.10042", "submitter": "Lie Ju", "authors": "Lie Ju, Xin Wang, Quan Zhou, Hu Zhu, Mehrtash Harandi, Paul\n  Bonnington, Tom Drummond, and Zongyuan Ge", "title": "Bridge the Domain Gap Between Ultra-wide-field and Traditional Fundus\n  Images via Adversarial Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, advances in retinal imaging technology have enabled effective\ndiagnosis and management of retinal disease using fundus cameras. Recently,\nultra-wide-field (UWF) fundus imaging by Optos camera is gradually put into use\nbecause of its broader insights on fundus for some lesions that are not\ntypically seen in traditional fundus images. Research on traditional fundus\nimages is an active topic but studies on UWF fundus images are few. One of the\nmost important reasons is that UWF fundus images are hard to obtain. In this\npaper, for the first time, we explore domain adaptation from the traditional\nfundus to UWF fundus images. We propose a flexible framework to bridge the\ndomain gap between two domains and co-train a UWF fundus diagnosis model by\npseudo-labelling and adversarial learning. We design a regularisation technique\nto regulate the domain adaptation. Also, we apply MixUp to overcome the\nover-fitting issue from incorrect generated pseudo-labels. Our experimental\nresults on either single or both domains demonstrate that the proposed method\ncan well adapt and transfer the knowledge from traditional fundus images to UWF\nfundus images and improve the performance of retinal disease recognition.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 01:31:39 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 01:42:22 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ju", "Lie", ""], ["Wang", "Xin", ""], ["Zhou", "Quan", ""], ["Zhu", "Hu", ""], ["Harandi", "Mehrtash", ""], ["Bonnington", "Paul", ""], ["Drummond", "Tom", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2003.10045", "submitter": "Carl Cheng", "authors": "Carl Cheng, Evan Hu", "title": "Architectural Resilience to Foreground-and-Background Adversarial Noise", "comments": "9 pages, 8 figures; updated email addresses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks in the form of imperceptible perturbations of normal\nimages have been extensively studied, and for every new defense methodology\ncreated, multiple adversarial attacks are found to counteract it. In\nparticular, a popular style of attack, exemplified in recent years by DeepFool\nand Carlini-Wagner, relies solely on white-box scenarios in which full access\nto the predictive model and its weights are required. In this work, we instead\npropose distinct model-agnostic benchmark perturbations of images in order to\ninvestigate the resilience and robustness of different network architectures.\nResults empirically determine that increasing depth within most types of\nConvolutional Neural Networks typically improves model resilience towards\ngeneral attacks, with improvement steadily decreasing as the model becomes\ndeeper. Additionally, we find that a notable difference in adversarial\nrobustness exists between residual architectures with skip connections and\nnon-residual architectures of similar complexity. Our findings provide\ndirection for future understanding of residual connections and depth on network\nrobustness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 01:38:20 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 05:28:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cheng", "Carl", ""], ["Hu", "Evan", ""]]}, {"id": "2003.10063", "submitter": "Thiago Paix\\~ao", "authors": "Thiago M. Paix\\~ao, Rodrigo F. Berriel, Maria C. S. Boeres, Alessando\n  L. Koerich, Claudine Badue, Alberto F. De Souza and Thiago Oliveira-Santos", "title": "Fast(er) Reconstruction of Shredded Text Documents via Self-Supervised\n  Deep Asymmetric Metric Learning", "comments": "Accepted to CVPR 2020. Main Paper (9 pages, 10 figures) and\n  Supplementary Material (5 pages, 9 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of shredded documents consists in arranging the pieces of\npaper (shreds) in order to reassemble the original aspect of such documents.\nThis task is particularly relevant for supporting forensic investigation as\ndocuments may contain criminal evidence. As an alternative to the laborious and\ntime-consuming manual process, several researchers have been investigating ways\nto perform automatic digital reconstruction. A central problem in automatic\nreconstruction of shredded documents is the pairwise compatibility evaluation\nof the shreds, notably for binary text documents. In this context, deep\nlearning has enabled great progress for accurate reconstructions in the domain\nof mechanically-shredded documents. A sensitive issue, however, is that current\ndeep model solutions require an inference whenever a pair of shreds has to be\nevaluated. This work proposes a scalable deep learning approach for measuring\npairwise compatibility in which the number of inferences scales linearly\n(rather than quadratically) with the number of shreds. Instead of predicting\ncompatibility directly, deep models are leveraged to asymmetrically project the\nraw shred content onto a common metric space in which distance is proportional\nto the compatibility. Experimental results show that our method has accuracy\ncomparable to the state-of-the-art with a speed-up of about 22 times for a test\ninstance with 505 shreds (20 mixed shredded-pages from different documents).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:22:06 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 19:37:50 GMT"}, {"version": "v3", "created": "Sun, 29 Mar 2020 23:01:00 GMT"}, {"version": "v4", "created": "Wed, 29 Apr 2020 00:21:31 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Paix\u00e3o", "Thiago M.", ""], ["Berriel", "Rodrigo F.", ""], ["Boeres", "Maria C. S.", ""], ["Koerich", "Alessando L.", ""], ["Badue", "Claudine", ""], ["De Souza", "Alberto F.", ""], ["Oliveira-Santos", "Thiago", ""]]}, {"id": "2003.10065", "submitter": "Liang Lin", "authors": "Qingxing Cao and Xiaodan Liang and Keze Wang and Liang Lin", "title": "Linguistically Driven Graph Capsule Network for Visual Question\n  Reasoning", "comments": "Submitted to TPAMI 2020. We have achieved an end-to-end interpretable\n  structural reasoning for general images without the requirement of layout\n  annotations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, studies of visual question answering have explored various\narchitectures of end-to-end networks and achieved promising results on both\nnatural and synthetic datasets, which require explicitly compositional\nreasoning. However, it has been argued that these black-box approaches lack\ninterpretability of results, and thus cannot perform well on generalization\ntasks due to overfitting the dataset bias. In this work, we aim to combine the\nbenefits of both sides and overcome their limitations to achieve an end-to-end\ninterpretable structural reasoning for general images without the requirement\nof layout annotations. Inspired by the property of a capsule network that can\ncarve a tree structure inside a regular convolutional neural network (CNN), we\npropose a hierarchical compositional reasoning model called the \"Linguistically\ndriven Graph Capsule Network\", where the compositional process is guided by the\nlinguistic parse tree. Specifically, we bind each capsule in the lowest layer\nto bridge the linguistic embedding of a single word in the original question\nwith visual evidence and then route them to the same capsule if they are\nsiblings in the parse tree. This compositional process is achieved by\nperforming inference on a linguistically driven conditional random field (CRF)\nand is performed across multiple graph capsule layers, which results in a\ncompositional reasoning process inside a CNN. Experiments on the CLEVR dataset,\nCLEVR compositional generation test, and FigureQA dataset demonstrate the\neffectiveness and composition generalization ability of our end-to-end model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:34:25 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Cao", "Qingxing", ""], ["Liang", "Xiaodan", ""], ["Wang", "Keze", ""], ["Lin", "Liang", ""]]}, {"id": "2003.10071", "submitter": "Zixin Luo", "authors": "Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao,\n  Shiwei Li, Tian Fang, Long Quan", "title": "ASLFeat: Learning Local Features of Accurate Shape and Localization", "comments": "Accepted to CVPR 2020, supplementary materials included, code\n  available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on mitigating two limitations in the joint learning of\nlocal feature detectors and descriptors. First, the ability to estimate the\nlocal shape (scale, orientation, etc.) of feature points is often neglected\nduring dense feature extraction, while the shape-awareness is crucial to\nacquire stronger geometric invariance. Second, the localization accuracy of\ndetected keypoints is not sufficient to reliably recover camera geometry, which\nhas become the bottleneck in tasks such as 3D reconstruction. In this paper, we\npresent ASLFeat, with three light-weight yet effective modifications to\nmitigate above issues. First, we resort to deformable convolutional networks to\ndensely estimate and apply local transformation. Second, we take advantage of\nthe inherent feature hierarchy to restore spatial resolution and low-level\ndetails for accurate keypoint localization. Finally, we use a peakiness\nmeasurement to relate feature responses and derive more indicative detection\nscores. The effect of each modification is thoroughly studied, and the\nevaluation is extensively conducted across a variety of practical scenarios.\nState-of-the-art results are reported that demonstrate the superiority of our\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 04:03:03 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 12:47:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Luo", "Zixin", ""], ["Zhou", "Lei", ""], ["Bai", "Xuyang", ""], ["Chen", "Hongkai", ""], ["Zhang", "Jiahui", ""], ["Yao", "Yao", ""], ["Li", "Shiwei", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2003.10111", "submitter": "Kumar Abhishek", "authors": "Kumar Abhishek, Ghassan Hamarneh, and Mark S. Drew", "title": "Illumination-based Transformations Improve Skin Lesion Segmentation in\n  Dermoscopic Images", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantic segmentation of skin lesions is an important and common initial\ntask in the computer aided diagnosis of dermoscopic images. Although deep\nlearning-based approaches have considerably improved the segmentation accuracy,\nthere is still room for improvement by addressing the major challenges, such as\nvariations in lesion shape, size, color and varying levels of contrast. In this\nwork, we propose the first deep semantic segmentation framework for dermoscopic\nimages which incorporates, along with the original RGB images, information\nextracted using the physics of skin illumination and imaging. In particular, we\nincorporate information from specific color bands, illumination invariant\ngrayscale images, and shading-attenuated images. We evaluate our method on\nthree datasets: the ISBI ISIC 2017 Skin Lesion Segmentation Challenge dataset,\nthe DermoFit Image Library, and the PH2 dataset and observe improvements of\n12.02%, 4.30%, and 8.86% respectively in the mean Jaccard index over a baseline\nmodel trained only with RGB images.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 07:43:35 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Abhishek", "Kumar", ""], ["Hamarneh", "Ghassan", ""], ["Drew", "Mark S.", ""]]}, {"id": "2003.10120", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Jiaqi Chen, Hefeng Wu, Tianshui Chen, Guanbin Li, Liang\n  Lin", "title": "Efficient Crowd Counting via Structured Knowledge Transfer", "comments": "This paper has been accepted by ACM MM 2020. Our code and models are\n  available at {\\url{https://github.com/HCPLab-SYSU/SKT}}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is an application-oriented task and its inference efficiency\nis crucial for real-world applications. However, most previous works relied on\nheavy backbone networks and required prohibitive run-time consumption, which\nwould seriously restrict their deployment scopes and cause poor scalability. To\nliberate these crowd counting models, we propose a novel Structured Knowledge\nTransfer (SKT) framework, which fully exploits the structured knowledge of a\nwell-trained teacher network to generate a lightweight but still highly\neffective student network. Specifically, it is integrated with two\ncomplementary transfer modules, including an Intra-Layer Pattern Transfer which\nsequentially distills the knowledge embedded in layer-wise features of the\nteacher network to guide feature learning of the student network and an\nInter-Layer Relation Transfer which densely distills the cross-layer\ncorrelation knowledge of the teacher to regularize the student's feature\nevolutio Consequently, our student network can derive the layer-wise and\ncross-layer knowledge from the teacher network to learn compact yet effective\nfeatures. Extensive evaluations on three benchmarks well demonstrate the\neffectiveness of our SKT for extensive crowd counting models. In particular,\nonly using around $6\\%$ of the parameters and computation cost of original\nmodels, our distilled VGG-based models obtain at least 6.5$\\times$ speed-up on\nan Nvidia 1080 GPU and even achieve state-of-the-art performance. Our code and\nmodels are available at {\\url{https://github.com/HCPLab-SYSU/SKT}}.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:05:41 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 04:43:26 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 15:31:57 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Liu", "Lingbo", ""], ["Chen", "Jiaqi", ""], ["Wu", "Hefeng", ""], ["Chen", "Tianshui", ""], ["Li", "Guanbin", ""], ["Lin", "Liang", ""]]}, {"id": "2003.10129", "submitter": "Udbhav Bamba", "authors": "Suyog Jadhav, Udbhav Bamba, Arnav Chavan, Rishabh Tiwari, Aryan Raj", "title": "Multi-Plateau Ensemble for Endoscopic Artefact Segmentation and\n  Detection", "comments": "EndoCV2020 workshop ISBI 2020 camera ready", "journal-ref": "http://ceur-ws.org/Vol-2595/endoCV2020_paper_id_20.pdf", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endoscopic artefact detection challenge consists of 1) Artefact detection, 2)\nSemantic segmentation, and 3) Out-of-sample generalisation. For Semantic\nsegmentation task, we propose a multi-plateau ensemble of FPN (Feature Pyramid\nNetwork) with EfficientNet as feature extractor/encoder. For Object detection\ntask, we used a three model ensemble of RetinaNet with Resnet50 Backbone and\nFasterRCNN (FPN + DC5) with Resnext101 Backbone}. A PyTorch implementation to\nour approach to the problem is available at\nhttps://github.com/ubamba98/EAD2020.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:28:36 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jadhav", "Suyog", ""], ["Bamba", "Udbhav", ""], ["Chavan", "Arnav", ""], ["Tiwari", "Rishabh", ""], ["Raj", "Aryan", ""]]}, {"id": "2003.10130", "submitter": "Bo Jiang", "authors": "Bo Jiang and Ziyan Zhang", "title": "Incomplete Graph Representation and Learning via Partial Graph Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Neural Networks (GNNs) are gaining increasing attention on graph data\nlearning tasks in recent years. However, in many applications, graph may be\ncoming in an incomplete form where attributes of graph nodes are partially\nunknown/missing. Existing GNNs are generally designed on complete graphs which\ncan not deal with attribute-incomplete graph data directly. To address this\nproblem, we develop a novel partial aggregation based GNNs, named Partial Graph\nNeural Networks (PaGNNs), for attribute-incomplete graph representation and\nlearning. Our work is motivated by the observation that the neighborhood\naggregation function in standard GNNs can be equivalently viewed as the\nneighborhood reconstruction formulation. Based on it, we define two novel\npartial aggregation (reconstruction) functions on incomplete graph and derive\nPaGNNs for incomplete graph data learning. Extensive experiments on several\ndatasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:29:59 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:23:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Jiang", "Bo", ""], ["Zhang", "Ziyan", ""]]}, {"id": "2003.10138", "submitter": "Ji Liu", "authors": "Yi Guo, Ji Liu", "title": "Depth Edge Guided CNNs for Sparse Depth Upsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided sparse depth upsampling aims to upsample an irregularly sampled sparse\ndepth map when an aligned high-resolution color image is given as guidance.\nMany neural networks have been designed for this task. However, they often\nignore the structural difference between the depth and the color image,\nresulting in obvious artifacts such as texture copy and depth blur at the\nupsampling depth. Inspired by the normalized convolution operation, we propose\na guided convolutional layer to recover dense depth from sparse and irregular\ndepth image with an depth edge image as guidance. Our novel guided network can\nprevent the depth value from crossing the depth edge to facilitate upsampling.\nWe further design a convolution network based on proposed convolutional layer\nto combine the advantages of different algorithms and achieve better\nperformance. We conduct comprehensive experiments to verify our method on\nreal-world indoor and synthetic outdoor datasets. Our method produces strong\nresults. It outperforms state-of-the-art methods on the Virtual KITTI dataset\nand the Middlebury dataset. It also presents strong generalization capability\nunder different 3D point densities, various lighting and weather conditions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 08:56:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Guo", "Yi", ""], ["Liu", "Ji", ""]]}, {"id": "2003.10142", "submitter": "Chia-Yuan Chang", "authors": "Chia-Yuan Chang, Shuo-En Chang, Pei-Yung Hsiao, and Li-Chen Fu", "title": "EPSNet: Efficient Panoptic Segmentation Network with Cross-layer\n  Attention Fusion", "comments": "ACCV 2020. Code is available at: https://github.com/neo85824/epsnet", "journal-ref": null, "doi": "10.1007/978-3-030-69525-5_41", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation is a scene parsing task which unifies semantic\nsegmentation and instance segmentation into one single task. However, the\ncurrent state-of-the-art studies did not take too much concern on inference\ntime. In this work, we propose an Efficient Panoptic Segmentation Network\n(EPSNet) to tackle the panoptic segmentation tasks with fast inference speed.\nBasically, EPSNet generates masks based on simple linear combination of\nprototype masks and mask coefficients. The light-weight network branches for\ninstance segmentation and semantic segmentation only need to predict mask\ncoefficients and produce masks with the shared prototypes predicted by\nprototype network branch. Furthermore, to enhance the quality of shared\nprototypes, we adopt a module called \"cross-layer attention fusion module\",\nwhich aggregates the multi-scale features with attention mechanism helping them\ncapture the long-range dependencies between each other. To validate the\nproposed work, we have conducted various experiments on the challenging COCO\npanoptic dataset, which achieve highly promising performance with significantly\nfaster inference speed (53ms on GPU).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:11:44 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 08:29:13 GMT"}, {"version": "v3", "created": "Thu, 24 Dec 2020 05:15:41 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chang", "Chia-Yuan", ""], ["Chang", "Shuo-En", ""], ["Hsiao", "Pei-Yung", ""], ["Fu", "Li-Chen", ""]]}, {"id": "2003.10144", "submitter": "Ning Zhenyuan", "authors": "Zhenyuan Ning, Ke Wang, Shengzhou Zhong, Qianjin Feng, Yu Zhang", "title": "CF2-Net: Coarse-to-Fine Fusion Convolutional Network for Breast\n  Ultrasound Image Segmentation", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast ultrasound (BUS) image segmentation plays a crucial role in a\ncomputer-aided diagnosis system, which is regarded as a useful tool to help\nincrease the accuracy of breast cancer diagnosis. Recently, many deep learning\nmethods have been developed for segmentation of BUS image and show some\nadvantages compared with conventional region-, model-, and traditional\nlearning-based methods. However, previous deep learning methods typically use\nskip-connection to concatenate the encoder and decoder, which might not make\nfull fusion of coarse-to-fine features from encoder and decoder. Since the\nstructure and edge of lesion in BUS image are common blurred, these would make\nit difficult to learn the discriminant information of structure and edge, and\nreduce the performance. To this end, we propose and evaluate a coarse-to-fine\nfusion convolutional network (CF2-Net) based on a novel feature integration\nstrategy (forming an 'E'-like type) for BUS image segmentation. To enhance\ncontour and provide structural information, we concatenate a super-pixel image\nand the original image as the input of CF2-Net. Meanwhile, to highlight the\ndifferences in the lesion regions with variable sizes and relieve the imbalance\nissue, we further design a weighted-balanced loss function to train the CF2-Net\neffectively. The proposed CF2-Net was evaluated on an open dataset by using\nfour-fold cross validation. The results of the experiment demonstrate that the\nCF2-Net obtains state-of-the-art performance when compared with other deep\nlearning-based methods\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:27:26 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ning", "Zhenyuan", ""], ["Wang", "Ke", ""], ["Zhong", "Shengzhou", ""], ["Feng", "Qianjin", ""], ["Zhang", "Yu", ""]]}, {"id": "2003.10151", "submitter": "Ahmed Nassar", "authors": "Ahmed Samy Nassar, Stefano D'Aronco, S\\'ebastien Lef\\`evre, and Jan D.\n  Wegner", "title": "GeoGraph: Learning graph-based multi-view object detection with\n  geometric cues end-to-end", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an end-to-end learnable approach that detects static\nurban objects from multiple views, re-identifies instances, and finally assigns\na geographic position per object. Our method relies on a Graph Neural Network\n(GNN) to, detect all objects and output their geographic positions given images\nand approximate camera poses as input. Our GNN simultaneously models relative\npose and image evidence, and is further able to deal with an arbitrary number\nof input views. Our method is robust to occlusion, with similar appearance of\nneighboring objects, and severe changes in viewpoints by jointly reasoning\nabout visual image appearance and relative pose. Experimental evaluation on two\nchallenging, large-scale datasets and comparison with state-of-the-art methods\nshow significant and systematic improvements both in accuracy and efficiency,\nwith 2-6% gain in detection and re-ID average precision as well as 8x reduction\nof training time.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:40:35 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 14:38:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Nassar", "Ahmed Samy", ""], ["D'Aronco", "Stefano", ""], ["Lef\u00e8vre", "S\u00e9bastien", ""], ["Wegner", "Jan D.", ""]]}, {"id": "2003.10152", "submitter": "Chunhua Shen", "authors": "Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, Chunhua Shen", "title": "SOLOv2: Dynamic and Fast Instance Segmentation", "comments": "Accepted to Proc. Advances in Neural Information Processing Systems\n  (NeurIPS'20). Code is available at: https://git.io/AdelaiDet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we aim at building a simple, direct, and fast instance\nsegmentation framework with strong performance. We follow the principle of the\nSOLO method of Wang et al. \"SOLO: segmenting objects by locations\".\nImportantly, we take one step further by dynamically learning the mask head of\nthe object segmenter such that the mask head is conditioned on the location.\nSpecifically, the mask branch is decoupled into a mask kernel branch and mask\nfeature branch, which are responsible for learning the convolution kernel and\nthe convolved features respectively. Moreover, we propose Matrix NMS (non\nmaximum suppression) to significantly reduce the inference time overhead due to\nNMS of masks. Our Matrix NMS performs NMS with parallel matrix operations in\none shot, and yields better results. We demonstrate a simple direct instance\nsegmentation system, outperforming a few state-of-the-art methods in both speed\nand accuracy. A light-weight version of SOLOv2 executes at 31.3 FPS and yields\n37.1% AP. Moreover, our state-of-the-art results in object detection (from our\nmask byproduct) and panoptic segmentation show the potential to serve as a new\nstrong baseline for many instance-level recognition tasks besides instance\nsegmentation. Code is available at: https://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:44:21 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 01:21:33 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 23:49:17 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Wang", "Xinlong", ""], ["Zhang", "Rufeng", ""], ["Kong", "Tao", ""], ["Li", "Lei", ""], ["Shen", "Chunhua", ""]]}, {"id": "2003.10168", "submitter": "Peng Lu", "authors": "Huawei Wei, Peng Lu, Yichen Wei", "title": "Balanced Alignment for Face Recognition: A Joint Learning Approach", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face alignment is crucial for face recognition and has been widely adopted.\nHowever, current practice is too simple and under-explored. There lacks an\nunderstanding of how important face alignment is and how it should be\nperformed, for recognition. This work studies these problems and makes two\ncontributions. First, it provides an in-depth and quantitative study of how\nalignment strength affects recognition accuracy. Our results show that\nexcessive alignment is harmful and an optimal balanced point of alignment is in\nneed. To strike the balance, our second contribution is a novel joint learning\napproach where alignment learning is controllable with respect to its strength\nand driven by recognition. Our proposed method is validated by comprehensive\nexperiments on several benchmarks, especially the challenging ones with large\npose.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 10:35:22 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Wei", "Huawei", ""], ["Lu", "Peng", ""], ["Wei", "Yichen", ""]]}, {"id": "2003.10176", "submitter": "Nikolaos Zioulis Mr.", "authors": "Vladimiros Sterzentsenko and Alexandros Doumanoglou and Spyridon\n  Thermos and Nikolaos Zioulis and Dimitrios Zarpalas and Petros Daras", "title": "Deep Soft Procrustes for Markerless Volumetric Sensor Alignment", "comments": "10 pages, 7 figures, to appear in IEEE VR 2020. Code and models at\n  https://vcl3d.github.io/StructureNet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of consumer grade depth sensors, low-cost volumetric capture\nsystems are easier to deploy. Their wider adoption though depends on their\nusability and by extension on the practicality of spatially aligning multiple\nsensors. Most existing alignment approaches employ visual patterns, e.g.\ncheckerboards, or markers and require high user involvement and technical\nknowledge. More user-friendly and easier-to-use approaches rely on markerless\nmethods that exploit geometric patterns of a physical structure. However,\ncurrent SoA approaches are bounded by restrictions in the placement and the\nnumber of sensors. In this work, we improve markerless data-driven\ncorrespondence estimation to achieve more robust and flexible multi-sensor\nspatial alignment. In particular, we incorporate geometric constraints in an\nend-to-end manner into a typical segmentation based model and bridge the\nintermediate dense classification task with the targeted pose estimation one.\nThis is accomplished by a soft, differentiable procrustes analysis that\nregularizes the segmentation and achieves higher extrinsic calibration\nperformance in expanded sensor placement configurations, while being\nunrestricted by the number of sensors of the volumetric capture system. Our\nmodel is experimentally shown to achieve similar results with marker-based\nmethods and outperform the markerless ones, while also being robust to the pose\nvariations of the calibration structure. Code and pretrained models are\navailable at https://vcl3d.github.io/StructureNet/.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 10:51:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Sterzentsenko", "Vladimiros", ""], ["Doumanoglou", "Alexandros", ""], ["Thermos", "Spyridon", ""], ["Zioulis", "Nikolaos", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "2003.10184", "submitter": "Fabian Mentzer", "authors": "Fabian Mentzer, Luc Van Gool, Michael Tschannen", "title": "Learning Better Lossless Compression Using Lossy Compression", "comments": "CVPR'20 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We leverage the powerful lossy image compression algorithm BPG to build a\nlossless image compression system. Specifically, the original image is first\ndecomposed into the lossy reconstruction obtained after compressing it with BPG\nand the corresponding residual. We then model the distribution of the residual\nwith a convolutional neural network-based probabilistic model that is\nconditioned on the BPG reconstruction, and combine it with entropy coding to\nlosslessly encode the residual. Finally, the image is stored using the\nconcatenation of the bitstreams produced by BPG and the learned residual coder.\nThe resulting compression system achieves state-of-the-art performance in\nlearned lossless full-resolution image compression, outperforming previous\nlearned approaches as well as PNG, WebP, and JPEG2000.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 11:21:52 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Mentzer", "Fabian", ""], ["Van Gool", "Luc", ""], ["Tschannen", "Michael", ""]]}, {"id": "2003.10211", "submitter": "Yibo Yang", "authors": "Xia Li, Yibo Yang, Qijie Zhao, Tiancheng Shen, Zhouchen Lin, Hong Liu", "title": "Spatial Pyramid Based Graph Reasoning for Semantic Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolution operation suffers from a limited receptive filed, while\nglobal modeling is fundamental to dense prediction tasks, such as semantic\nsegmentation. In this paper, we apply graph convolution into the semantic\nsegmentation task and propose an improved Laplacian. The graph reasoning is\ndirectly performed in the original feature space organized as a spatial\npyramid. Different from existing methods, our Laplacian is data-dependent and\nwe introduce an attention diagonal matrix to learn a better distance metric. It\ngets rid of projecting and re-projecting processes, which makes our proposed\nmethod a light-weight module that can be easily plugged into current computer\nvision architectures. More importantly, performing graph reasoning directly in\nthe feature space retains spatial relationships and makes spatial pyramid\npossible to explore multiple long-range contextual patterns from different\nscales. Experiments on Cityscapes, COCO Stuff, PASCAL Context and PASCAL VOC\ndemonstrate the effectiveness of our proposed methods on semantic segmentation.\nWe achieve comparable performance with advantages in computational and memory\noverhead.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 12:28:07 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Xia", ""], ["Yang", "Yibo", ""], ["Zhao", "Qijie", ""], ["Shen", "Tiancheng", ""], ["Lin", "Zhouchen", ""], ["Liu", "Hong", ""]]}, {"id": "2003.10238", "submitter": "Xixia Xu", "authors": "Xixia Xu, Qi Zou, Xue Lin", "title": "Multi-Person Pose Estimation with Enhanced Feature Aggregation and\n  Selection", "comments": "arXiv admin note: text overlap with arXiv:1905.03466 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Enhanced Feature Aggregation and Selection network\n(EFASNet) for multi-person 2D human pose estimation. Due to enhanced feature\nrepresentation, our method can well handle crowded, cluttered and occluded\nscenes. More specifically, a Feature Aggregation and Selection Module (FASM),\nwhich constructs hierarchical multi-scale feature aggregation and makes the\naggregated features discriminative, is proposed to get more accurate\nfine-grained representation, leading to more precise joint locations. Then, we\nperform a simple Feature Fusion (FF) strategy which effectively fuses\nhigh-resolution spatial features and low-resolution semantic features to obtain\nmore reliable context information for well-estimated joints. Finally, we build\na Dense Upsampling Convolution (DUC) module to generate more precise\nprediction, which can recover missing joint details that are usually\nunavailable in common upsampling process. As a result, the predicted keypoint\nheatmaps are more accurate. Comprehensive experiments demonstrate that the\nproposed approach outperforms the state-of-the-art methods and achieves the\nsuperior performance over three benchmark datasets: the recent big dataset\nCrowdPose, the COCO keypoint detection dataset and the MPII Human Pose dataset.\nOur code will be released upon acceptance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 08:33:25 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xu", "Xixia", ""], ["Zou", "Qi", ""], ["Lin", "Xue", ""]]}, {"id": "2003.10258", "submitter": "Mathis Brosowsky", "authors": "Mathis Brosowsky (1 and 2), Olaf D\\\"unkel (1), Daniel Slieter (1),\n  Marius Z\\\"ollner (2) ((1) Dr. Ing. h.c. F. Porsche AG, (2) FZI Research\n  Center for Information Technology)", "title": "Sample-Specific Output Constraints for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks reach state-of-the-art performance in a variety of learning\ntasks. However, a lack of understanding the decision making process yields to\nan appearance as black box. We address this and propose ConstraintNet, a neural\nnetwork with the capability to constrain the output space in each forward pass\nvia an additional input. The prediction of ConstraintNet is proven within the\nspecified domain. This enables ConstraintNet to exclude unintended or even\nhazardous outputs explicitly whereas the final prediction is still learned from\ndata. We focus on constraints in form of convex polytopes and show the\ngeneralization to further classes of constraints. ConstraintNet can be\nconstructed easily by modifying existing neural network architectures. We\nhighlight that ConstraintNet is end-to-end trainable with no overhead in the\nforward and backward pass. For illustration purposes, we model ConstraintNet by\nmodifying a CNN and construct constraints for facial landmark prediction tasks.\nFurthermore, we demonstrate the application to a follow object controller for\nvehicles as a safety-critical application. We submitted an approach and system\nfor the generation of safety-critical outputs of an entity based on\nConstraintNet at the German Patent and Trademark Office with the official\nregistration mark DE10 2019 119 739.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 13:13:11 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Brosowsky", "Mathis", "", "1 and 2"], ["D\u00fcnkel", "Olaf", ""], ["Slieter", "Daniel", ""], ["Z\u00f6llner", "Marius", ""]]}, {"id": "2003.10275", "submitter": "Yangtao Zheng", "authors": "Yangtao Zheng, Di Huang, Songtao Liu and Yunhong Wang", "title": "Cross-domain Object Detection through Coarse-to-Fine Feature Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed great progress in deep learning based object\ndetection. However, due to the domain shift problem, applying off-the-shelf\ndetectors to an unseen domain leads to significant performance drop. To address\nsuch an issue, this paper proposes a novel coarse-to-fine feature adaptation\napproach to cross-domain object detection. At the coarse-grained stage,\ndifferent from the rough image-level or instance-level feature alignment used\nin the literature, foreground regions are extracted by adopting the attention\nmechanism, and aligned according to their marginal distributions via\nmulti-layer adversarial learning in the common feature space. At the\nfine-grained stage, we conduct conditional distribution alignment of\nforegrounds by minimizing the distance of global prototypes with the same\ncategory but from different domains. Thanks to this coarse-to-fine feature\nadaptation, domain knowledge in foreground regions can be effectively\ntransferred. Extensive experiments are carried out in various cross-domain\ndetection scenarios. The results are state-of-the-art, which demonstrate the\nbroad applicability and effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 13:40:06 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zheng", "Yangtao", ""], ["Huang", "Di", ""], ["Liu", "Songtao", ""], ["Wang", "Yunhong", ""]]}, {"id": "2003.10281", "submitter": "Jose Pedro Iglesias", "authors": "Jos\\'e Pedro Iglesias, Carl Olsson, Marcus Valtonen \\\"Ornhag", "title": "Accurate Optimization of Weighted Nuclear Norm for Non-Rigid Structure\n  from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting a matrix of a given rank to data in a least squares sense can be done\nvery effectively using 2nd order methods such as Levenberg-Marquardt by\nexplicitly optimizing over a bilinear parameterization of the matrix. In\ncontrast, when applying more general singular value penalties, such as weighted\nnuclear norm priors, direct optimization over the elements of the matrix is\ntypically used. Due to non-differentiability of the resulting objective\nfunction, first order sub-gradient or splitting methods are predominantly used.\nWhile these offer rapid iterations it is well known that they become inefficent\nnear the minimum due to zig-zagging and in practice one is therefore often\nforced to settle for an approximate solution.\n  In this paper we show that more accurate results can in many cases be\nachieved with 2nd order methods. Our main result shows how to construct\nbilinear formulations, for a general class of regularizers including weighted\nnuclear norm penalties, that are provably equivalent to the original problems.\nWith these formulations the regularizing function becomes twice differentiable\nand 2nd order methods can be applied. We show experimentally, on a number of\nstructure from motion problems, that our approach outperforms state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 13:52:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 20:22:14 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Iglesias", "Jos\u00e9 Pedro", ""], ["Olsson", "Carl", ""], ["\u00d6rnhag", "Marcus Valtonen", ""]]}, {"id": "2003.10299", "submitter": "Tobias Ross", "authors": "Tobias Ross, Annika Reinke, Peter M. Full, Martin Wagner, Hannes\n  Kenngott, Martin Apitz, Hellena Hempe, Diana Mindroc Filimon, Patrick Scholz,\n  Thuy Nuong Tran, Pierangela Bruno, Pablo Arbel\\'aez, Gui-Bin Bian, Sebastian\n  Bodenstedt, Jon Lindstr\\\"om Bolmgren, Laura Bravo-S\\'anchez, Hua-Bin Chen,\n  Cristina Gonz\\'alez, Dong Guo, P{\\aa}l Halvorsen, Pheng-Ann Heng, Enes\n  Hosgor, Zeng-Guang Hou, Fabian Isensee, Debesh Jha, Tingting Jiang, Yueming\n  Jin, Kadir Kirtac, Sabrina Kletz, Stefan Leger, Zhixuan Li, Klaus H.\n  Maier-Hein, Zhen-Liang Ni, Michael A. Riegler, Klaus Schoeffmann, Ruohua Shi,\n  Stefanie Speidel, Michael Stenzel, Isabell Twick, Gutai Wang, Jiacheng Wang,\n  Liansheng Wang, Lu Wang, Yujie Zhang, Yan-Jie Zhou, Lei Zhu, Manuel\n  Wiesenfarth, Annette Kopp-Schneider, Beat P. M\\\"uller-Stich, Lena Maier-Hein", "title": "Robust Medical Instrument Segmentation Challenge 2019", "comments": "A pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intraoperative tracking of laparoscopic instruments is often a prerequisite\nfor computer and robotic-assisted interventions. While numerous methods for\ndetecting, segmenting and tracking of medical instruments based on endoscopic\nvideo images have been proposed in the literature, key limitations remain to be\naddressed: Firstly, robustness, that is, the reliable performance of\nstate-of-the-art methods when run on challenging images (e.g. in the presence\nof blood, smoke or motion artifacts). Secondly, generalization; algorithms\ntrained for a specific intervention in a specific hospital should generalize to\nother interventions or institutions.\n  In an effort to promote solutions for these limitations, we organized the\nRobust Medical Instrument Segmentation (ROBUST-MIS) challenge as an\ninternational benchmarking competition with a specific focus on the robustness\nand generalization capabilities of algorithms. For the first time in the field\nof endoscopic image processing, our challenge included a task on binary\nsegmentation and also addressed multi-instance detection and segmentation. The\nchallenge was based on a surgical data set comprising 10,040 annotated images\nacquired from a total of 30 surgical procedures from three different types of\nsurgery. The validation of the competing methods for the three tasks (binary\nsegmentation, multi-instance detection and multi-instance segmentation) was\nperformed in three different stages with an increasing domain gap between the\ntraining and the test data. The results confirm the initial hypothesis, namely\nthat algorithm performance degrades with an increasing domain gap. While the\naverage detection and segmentation quality of the best-performing algorithms is\nhigh, future research should concentrate on detection and segmentation of\nsmall, crossing, moving and transparent instrument(s) (parts).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:35:08 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 12:27:18 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Ross", "Tobias", ""], ["Reinke", "Annika", ""], ["Full", "Peter M.", ""], ["Wagner", "Martin", ""], ["Kenngott", "Hannes", ""], ["Apitz", "Martin", ""], ["Hempe", "Hellena", ""], ["Filimon", "Diana Mindroc", ""], ["Scholz", "Patrick", ""], ["Tran", "Thuy Nuong", ""], ["Bruno", "Pierangela", ""], ["Arbel\u00e1ez", "Pablo", ""], ["Bian", "Gui-Bin", ""], ["Bodenstedt", "Sebastian", ""], ["Bolmgren", "Jon Lindstr\u00f6m", ""], ["Bravo-S\u00e1nchez", "Laura", ""], ["Chen", "Hua-Bin", ""], ["Gonz\u00e1lez", "Cristina", ""], ["Guo", "Dong", ""], ["Halvorsen", "P\u00e5l", ""], ["Heng", "Pheng-Ann", ""], ["Hosgor", "Enes", ""], ["Hou", "Zeng-Guang", ""], ["Isensee", "Fabian", ""], ["Jha", "Debesh", ""], ["Jiang", "Tingting", ""], ["Jin", "Yueming", ""], ["Kirtac", "Kadir", ""], ["Kletz", "Sabrina", ""], ["Leger", "Stefan", ""], ["Li", "Zhixuan", ""], ["Maier-Hein", "Klaus H.", ""], ["Ni", "Zhen-Liang", ""], ["Riegler", "Michael A.", ""], ["Schoeffmann", "Klaus", ""], ["Shi", "Ruohua", ""], ["Speidel", "Stefanie", ""], ["Stenzel", "Michael", ""], ["Twick", "Isabell", ""], ["Wang", "Gutai", ""], ["Wang", "Jiacheng", ""], ["Wang", "Liansheng", ""], ["Wang", "Lu", ""], ["Zhang", "Yujie", ""], ["Zhou", "Yan-Jie", ""], ["Zhu", "Lei", ""], ["Wiesenfarth", "Manuel", ""], ["Kopp-Schneider", "Annette", ""], ["M\u00fcller-Stich", "Beat P.", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "2003.10304", "submitter": "Andr\\'as Luk\\'acs", "authors": "Guszt\\'av Ga\\'al, Bal\\'azs Maga, Andr\\'as Luk\\'acs", "title": "Attention U-Net Based Adversarial Architectures for Chest X-ray Lung\n  Segmentation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray is the most common test among medical imaging modalities. It is\napplied for detection and differentiation of, among others, lung cancer,\ntuberculosis, and pneumonia, the last with importance due to the COVID-19\ndisease. Integrating computer-aided detection methods into the radiologist\ndiagnostic pipeline, greatly reduces the doctors' workload, increasing\nreliability and quantitative analysis. Here we present a novel deep learning\napproach for lung segmentation, a basic, but arduous task in the diagnostic\npipeline. Our method uses state-of-the-art fully convolutional neural networks\nin conjunction with an adversarial critic model. It generalized well to CXR\nimages of unseen datasets with different patient profiles, achieving a final\nDSC of 97.5% on the JSRT dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:45:48 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ga\u00e1l", "Guszt\u00e1v", ""], ["Maga", "Bal\u00e1zs", ""], ["Luk\u00e1cs", "Andr\u00e1s", ""]]}, {"id": "2003.10308", "submitter": "Alessandro Di Nuovo", "authors": "Alessandro Di Nuovo", "title": "A Developmental Neuro-Robotics Approach for Boosting the Recognition of\n  Handwritten Digits", "comments": "Accepted for presentation at IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developmental psychology and neuroimaging research identified a close link\nbetween numbers and fingers, which can boost the initial number knowledge in\nchildren. Recent evidence shows that a simulation of the children's embodied\nstrategies can improve the machine intelligence too. This article explores the\napplication of embodied strategies to convolutional neural network models in\nthe context of developmental neuro-robotics, where the training information is\nlikely to be gradually acquired while operating rather than being abundant and\nfully available as the classical machine learning scenarios. The experimental\nanalyses show that the proprioceptive information from the robot fingers can\nimprove network accuracy in the recognition of handwritten Arabic digits when\ntraining examples and epochs are few. This result is comparable to brain\nimaging and longitudinal studies with young children. In conclusion, these\nfindings also support the relevance of the embodiment in the case of artificial\nagents' training and show a possible way for the humanization of the learning\nprocess, where the robotic body can express the internal processes of\nartificial intelligence making it more understandable for humans.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 14:55:00 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Di Nuovo", "Alessandro", ""]]}, {"id": "2003.10315", "submitter": "Ziqi Zhang", "authors": "Ziqi Zhang, Xinge Zhu, Yingwei Li, Xiangqun Chen, Yao Guo", "title": "Adversarial Attacks on Monocular Depth Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of deep learning have brought exceptional performance on many\ncomputer vision tasks such as semantic segmentation and depth estimation.\nHowever, the vulnerability of deep neural networks towards adversarial examples\nhave caused grave concerns for real-world deployment. In this paper, we present\nto the best of our knowledge the first systematic study of adversarial attacks\non monocular depth estimation, an important task of 3D scene understanding in\nscenarios such as autonomous driving and robot navigation. In order to\nunderstand the impact of adversarial attacks on depth estimation, we first\ndefine a taxonomy of different attack scenarios for depth estimation, including\nnon-targeted attacks, targeted attacks and universal attacks. We then adapt\nseveral state-of-the-art attack methods for classification on the field of\ndepth estimation. Besides, multi-task attacks are introduced to further improve\nthe attack performance for universal attacks. Experimental results show that it\nis possible to generate significant errors on depth estimation. In particular,\nwe demonstrate that our methods can conduct targeted attacks on given objects\n(such as a car), resulting in depth estimation 3-4x away from the ground truth\n(e.g., from 20m to 80m).\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:04:30 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhang", "Ziqi", ""], ["Zhu", "Xinge", ""], ["Li", "Yingwei", ""], ["Chen", "Xiangqun", ""], ["Guo", "Yao", ""]]}, {"id": "2003.10333", "submitter": "Difan Liu", "authors": "Difan Liu, Mohamed Nabail, Aaron Hertzmann, Evangelos Kalogerakis", "title": "Neural Contours: Learning to Draw Lines from 3D Shapes", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method for learning to generate line drawings from 3D\nmodels. Our architecture incorporates a differentiable module operating on\ngeometric features of the 3D model, and an image-based module operating on\nview-based shape representations. At test time, geometric and view-based\nreasoning are combined with the help of a neural module to create a line\ndrawing. The model is trained on a large number of crowdsourced comparisons of\nline drawings. Experiments demonstrate that our method achieves significant\nimprovements in line drawing over the state-of-the-art when evaluated on\nstandard benchmarks, resulting in drawings that are comparable to those\nproduced by experienced human artists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 15:37:49 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 17:20:05 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 03:22:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Liu", "Difan", ""], ["Nabail", "Mohamed", ""], ["Hertzmann", "Aaron", ""], ["Kalogerakis", "Evangelos", ""]]}, {"id": "2003.10350", "submitter": "Andrei Zanfir", "authors": "Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, Bill Freeman, Rahul\n  Sukthankar and Cristian Sminchisescu", "title": "Weakly Supervised 3D Human Pose and Shape Reconstruction with\n  Normalizing Flows", "comments": null, "journal-ref": "ECCV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular 3D human pose and shape estimation is challenging due to the many\ndegrees of freedom of the human body and thedifficulty to acquire training data\nfor large-scale supervised learning in complex visual scenes. In this paper we\npresent practical semi-supervised and self-supervised models that support\ntraining and good generalization in real-world images and video. Our\nformulation is based on kinematic latent normalizing flow representations and\ndynamics, as well as differentiable, semantic body part alignment loss\nfunctions that support self-supervised learning. In extensive experiments using\n3D motion capture datasets like CMU, Human3.6M, 3DPW, or AMASS, as well as\nimage repositories like COCO, we show that the proposed methods outperform the\nstate of the art, supporting the practical construction of an accurate family\nof models based on large-scale training with diverse and incompletely labeled\nimage and video data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:11:51 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 14:46:19 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zanfir", "Andrei", ""], ["Bazavan", "Eduard Gabriel", ""], ["Xu", "Hongyi", ""], ["Freeman", "Bill", ""], ["Sukthankar", "Rahul", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "2003.10381", "submitter": "Oliver Scheel", "authors": "Alessandro Berlati, Oliver Scheel, Luigi Di Stefano, Federico Tombari", "title": "Ambiguity in Sequential Data: Predicting Uncertain Futures with\n  Recurrent Models", "comments": null, "journal-ref": "Robotics and Automation Letters 2020 (RA-L)", "doi": "10.1109/LRA.2020.2974716", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity is inherently present in many machine learning tasks, but\nespecially for sequential models seldom accounted for, as most only output a\nsingle prediction. In this work we propose an extension of the Multiple\nHypothesis Prediction (MHP) model to handle ambiguous predictions with\nsequential data, which is of special importance, as often multiple futures are\nequally likely. Our approach can be applied to the most common recurrent\narchitectures and can be used with any loss function. Additionally, we\nintroduce a novel metric for ambiguous problems, which is better suited to\naccount for uncertainties and coincides with our intuitive understanding of\ncorrectness in the presence of multiple labels. We test our method on several\nexperiments and across diverse tasks dealing with time series data, such as\ntrajectory forecasting and maneuver prediction, achieving promising results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 09:15:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Berlati", "Alessandro", ""], ["Scheel", "Oliver", ""], ["Di Stefano", "Luigi", ""], ["Tombari", "Federico", ""]]}, {"id": "2003.10399", "submitter": "Saima Sharmin", "authors": "Saima Sharmin, Nitin Rathi, Priyadarshini Panda and Kaushik Roy", "title": "Inherent Adversarial Robustness of Deep Spiking Neural Networks: Effects\n  of Discrete Input Encoding and Non-Linear Activations", "comments": "Accepted in 16th European Conference on Computer Vision (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent quest for trustworthy neural networks, we present Spiking\nNeural Network (SNN) as a potential candidate for inherent robustness against\nadversarial attacks. In this work, we demonstrate that adversarial accuracy of\nSNNs under gradient-based attacks is higher than their non-spiking counterparts\nfor CIFAR datasets on deep VGG and ResNet architectures, particularly in\nblackbox attack scenario. We attribute this robustness to two fundamental\ncharacteristics of SNNs and analyze their effects. First, we exhibit that input\ndiscretization introduced by the Poisson encoder improves adversarial\nrobustness with reduced number of timesteps. Second, we quantify the amount of\nadversarial accuracy with increased leak rate in Leaky-Integrate-Fire (LIF)\nneurons. Our results suggest that SNNs trained with LIF neurons and smaller\nnumber of timesteps are more robust than the ones with IF (Integrate-Fire)\nneurons and larger number of timesteps. Also we overcome the bottleneck of\ncreating gradient-based adversarial inputs in temporal domain by proposing a\ntechnique for crafting attacks from SNN\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:20:24 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 21:05:40 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Sharmin", "Saima", ""], ["Rathi", "Nitin", ""], ["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "2003.10401", "submitter": "Yanwei Li", "authors": "Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang\n  Wang, Jian Sun", "title": "Learning Dynamic Routing for Semantic Segmentation", "comments": "Accepted by CVPR 2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, numerous handcrafted and searched networks have been applied for\nsemantic segmentation. However, previous works intend to handle inputs with\nvarious scales in pre-defined static architectures, such as FCN, U-Net, and\nDeepLab series. This paper studies a conceptually new method to alleviate the\nscale variance in semantic representation, named dynamic routing. The proposed\nframework generates data-dependent routes, adapting to the scale distribution\nof each image. To this end, a differentiable gating function, called soft\nconditional gate, is proposed to select scale transform paths on the fly. In\naddition, the computational cost can be further reduced in an end-to-end manner\nby giving budget constraints to the gating function. We further relax the\nnetwork level routing space to support multi-path propagations and\nskip-connections in each forward, bringing substantial network capacity. To\ndemonstrate the superiority of the dynamic property, we compare with several\nstatic architectures, which can be modeled as special cases in the routing\nspace. Extensive experiments are conducted on Cityscapes and PASCAL VOC 2012 to\nillustrate the effectiveness of the dynamic framework. Code is available at\nhttps://github.com/yanwei-li/DynamicRouting.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:22:14 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Li", "Yanwei", ""], ["Song", "Lin", ""], ["Chen", "Yukang", ""], ["Li", "Zeming", ""], ["Zhang", "Xiangyu", ""], ["Wang", "Xingang", ""], ["Sun", "Jian", ""]]}, {"id": "2003.10424", "submitter": "He Sun", "authors": "He Sun, Adrian V. Dalca and Katherine L. Bouman", "title": "Learning a Probabilistic Strategy for Computational Imaging Sensor\n  Selection", "comments": "This paper has been accepted to the IEEE International Conference on\n  Computational Photography (ICCP) 2020. Keywords: Computational Imaging,\n  Optimized Sensing, Ising Model, Deep Learning, VLBI, Interferometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV astro-ph.IM cs.CV cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimized sensing is important for computational imaging in low-resource\nenvironments, when images must be recovered from severely limited measurements.\nIn this paper, we propose a physics-constrained, fully differentiable,\nautoencoder that learns a probabilistic sensor-sampling strategy for optimized\nsensor design. The proposed method learns a system's preferred sampling\ndistribution that characterizes the correlations between different sensor\nselections as a binary, fully-connected Ising model. The learned probabilistic\nmodel is achieved by using a Gibbs sampling inspired network architecture, and\nis trained end-to-end with a reconstruction network for efficient co-design.\nThe proposed framework is applicable to sensor selection problems in a variety\nof computational imaging applications. In this paper, we demonstrate the\napproach in the context of a very-long-baseline-interferometry (VLBI) array\ndesign task, where sensor correlations and atmospheric noise present unique\nchallenges. We demonstrate results broadly consistent with expectation, and\ndraw attention to particular structures preferred in the telescope array\ngeometry that can be leveraged to plan future observations and design array\nexpansions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:52:17 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Sun", "He", ""], ["Dalca", "Adrian V.", ""], ["Bouman", "Katherine L.", ""]]}, {"id": "2003.10428", "submitter": "Kai Zhang", "authors": "Kai Zhang, Luc Van Gool, Radu Timofte", "title": "Deep Unfolding Network for Image Super-Resolution", "comments": "Accepted by CVPR 2020. Project page: https://github.com/cszn/USRNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based single image super-resolution (SISR) methods are continuously\nshowing superior effectiveness and efficiency over traditional model-based\nmethods, largely due to the end-to-end training. However, different from\nmodel-based methods that can handle the SISR problem with different scale\nfactors, blur kernels and noise levels under a unified MAP (maximum a\nposteriori) framework, learning-based methods generally lack such flexibility.\nTo address this issue, this paper proposes an end-to-end trainable unfolding\nnetwork which leverages both learning-based methods and model-based methods.\nSpecifically, by unfolding the MAP inference via a half-quadratic splitting\nalgorithm, a fixed number of iterations consisting of alternately solving a\ndata subproblem and a prior subproblem can be obtained. The two subproblems\nthen can be solved with neural modules, resulting in an end-to-end trainable,\niterative network. As a result, the proposed network inherits the flexibility\nof model-based methods to super-resolve blurry, noisy images for different\nscale factors via a single model, while maintaining the advantages of\nlearning-based methods. Extensive experiments demonstrate the superiority of\nthe proposed deep unfolding network in terms of flexibility, effectiveness and\nalso generalizability.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:55:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zhang", "Kai", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.10432", "submitter": "Zachary Murez", "authors": "Zak Murez, Tarrence van As, James Bartolozzi, Ayan Sinha, Vijay\n  Badrinarayanan, and Andrew Rabinovich", "title": "Atlas: End-to-End 3D Scene Reconstruction from Posed Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end 3D reconstruction method for a scene by directly\nregressing a truncated signed distance function (TSDF) from a set of posed RGB\nimages. Traditional approaches to 3D reconstruction rely on an intermediate\nrepresentation of depth maps prior to estimating a full 3D model of a scene. We\nhypothesize that a direct regression to 3D is more effective. A 2D CNN extracts\nfeatures from each image independently which are then back-projected and\naccumulated into a voxel volume using the camera intrinsics and extrinsics.\nAfter accumulation, a 3D CNN refines the accumulated features and predicts the\nTSDF values. Additionally, semantic segmentation of the 3D model is obtained\nwithout significant computation. This approach is evaluated on the Scannet\ndataset where we significantly outperform state-of-the-art baselines (deep\nmultiview stereo followed by traditional TSDF fusion) both quantitatively and\nqualitatively. We compare our 3D semantic segmentation to prior methods that\nuse a depth sensor since no previous work attempts the problem with only RGB\ninput.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 17:59:15 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 19:07:47 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 13:46:36 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Murez", "Zak", ""], ["van As", "Tarrence", ""], ["Bartolozzi", "James", ""], ["Sinha", "Ayan", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "2003.10469", "submitter": "Aviv Shamsian", "authors": "Aviv Shamsian, Ofri Kleinfeld, Amir Globerson, Gal Chechik", "title": "Learning Object Permanence from Video", "comments": "16th European Conference on Computer Vision (ECCV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object Permanence allows people to reason about the location of non-visible\nobjects, by understanding that they continue to exist even when not perceived\ndirectly. Object Permanence is critical for building a model of the world,\nsince objects in natural visual scenes dynamically occlude and contain\neach-other. Intensive studies in developmental psychology suggest that object\npermanence is a challenging task that is learned through extensive experience.\nHere we introduce the setup of learning Object Permanence from data. We explain\nwhy this learning problem should be dissected into four components, where\nobjects are (1) visible, (2) occluded, (3) contained by another object and (4)\ncarried by a containing object. The fourth subtask, where a target object is\ncarried by a containing object, is particularly challenging because it requires\na system to reason about a moving location of an invisible object. We then\npresent a unified deep architecture that learns to predict object location\nunder these four scenarios. We evaluate the architecture and system on a new\ndataset based on CATER, and find that it outperforms previous localization\nmethods and various baselines.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:03:01 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 10:25:57 GMT"}, {"version": "v3", "created": "Tue, 14 Jul 2020 13:30:57 GMT"}, {"version": "v4", "created": "Thu, 16 Jul 2020 09:16:04 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Shamsian", "Aviv", ""], ["Kleinfeld", "Ofri", ""], ["Globerson", "Amir", ""], ["Chechik", "Gal", ""]]}, {"id": "2003.10471", "submitter": "G\\\"orkem Algan", "authors": "G\\\"orkem Algan, \\.Ilkay Ulusoy", "title": "Label Noise Types and Their Effects on Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of deep learning is mostly due to the availability of big\ndatasets with clean annotations. However, gathering a cleanly annotated dataset\nis not always feasible due to practical challenges. As a result, label noise is\na common problem in datasets, and numerous methods to train deep neural\nnetworks in the presence of noisy labels are proposed in the literature. These\nmethods commonly use benchmark datasets with synthetic label noise on the\ntraining set. However, there are multiple types of label noise, and each of\nthem has its own characteristic impact on learning. Since each work generates a\ndifferent kind of label noise, it is problematic to test and compare those\nalgorithms in the literature fairly. In this work, we provide a detailed\nanalysis of the effects of different kinds of label noise on learning.\nMoreover, we propose a generic framework to generate feature-dependent label\nnoise, which we show to be the most challenging case for learning. Our proposed\nmethod aims to emphasize similarities among data instances by sparsely\ndistributing them in the feature domain. By this approach, samples that are\nmore likely to be mislabeled are detected from their softmax probabilities, and\ntheir labels are flipped to the corresponding class. The proposed method can be\napplied to any clean dataset to synthesize feature-dependent noisy labels. For\nthe ease of other researchers to test their algorithms with noisy labels, we\nshare corrupted labels for the most commonly used benchmark datasets. Our code\nand generated noisy synthetic labels are available online.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:03:39 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Algan", "G\u00f6rkem", ""], ["Ulusoy", "\u0130lkay", ""]]}, {"id": "2003.10477", "submitter": "Yiding Yang", "authors": "Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, Xinchao Wang", "title": "Distilling Knowledge from Graph Convolutional Networks", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing knowledge distillation methods focus on convolutional neural\nnetworks (CNNs), where the input samples like images lie in a grid domain, and\nhave largely overlooked graph convolutional networks (GCN) that handle non-grid\ndata. In this paper, we propose to our best knowledge the first dedicated\napproach to distilling knowledge from a pre-trained GCN model. To enable the\nknowledge transfer from the teacher GCN to the student, we propose a local\nstructure preserving module that explicitly accounts for the topological\nsemantics of the teacher. In this module, the local structure information from\nboth the teacher and the student are extracted as distributions, and hence\nminimizing the distance between these distributions enables topology-aware\nknowledge transfer from the teacher, yielding a compact yet high-performance\nstudent model. Moreover, the proposed approach is readily extendable to dynamic\ngraph models, where the input graphs for the teacher and the student may\ndiffer. We evaluate the proposed method on two different datasets using GCN\nmodels of different architectures, and demonstrate that our method achieves the\nstate-of-the-art knowledge distillation performance for GCN models. Code is\npublicly available at https://github.com/ihollywhy/DistillGCN.PyTorch.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 18:23:11 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 03:09:30 GMT"}, {"version": "v3", "created": "Sat, 28 Mar 2020 19:30:02 GMT"}, {"version": "v4", "created": "Sun, 10 Jan 2021 03:55:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Yang", "Yiding", ""], ["Qiu", "Jiayan", ""], ["Song", "Mingli", ""], ["Tao", "Dacheng", ""], ["Wang", "Xinchao", ""]]}, {"id": "2003.10506", "submitter": "Yanran Li", "authors": "Lingteng Qiu, Xuanye Zhang, Yanran Li, Guanbin Li, Xiaojun Wu, Zixiang\n  Xiong, Xiaoguang Han and Shuguang Cui", "title": "Peeking into occluded joints: A novel framework for crowd pose\n  estimation", "comments": "The code of OPEC-Net is available at:\n  https://lingtengqiu.github.io/2020/03/22/OPEC-Net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although occlusion widely exists in nature and remains a fundamental\nchallenge for pose estimation, existing heatmap-based approaches suffer serious\ndegradation on occlusions. Their intrinsic problem is that they directly\nlocalize the joints based on visual information; however, the invisible joints\nare lack of that. In contrast to localization, our framework estimates the\ninvisible joints from an inference perspective by proposing an Image-Guided\nProgressive GCN module which provides a comprehensive understanding of both\nimage context and pose structure. Moreover, existing benchmarks contain limited\nocclusions for evaluation. Therefore, we thoroughly pursue this problem and\npropose a novel OPEC-Net framework together with a new Occluded Pose (OCPose)\ndataset with 9k annotated images. Extensive quantitative and qualitative\nevaluations on benchmarks demonstrate that OPEC-Net achieves significant\nimprovements over recent leading works. Notably, our OCPose is the most complex\nocclusion dataset with respect to average IoU between adjacent instances.\nSource code and OCPose will be publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 19:32:40 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 13:48:17 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 02:01:35 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Qiu", "Lingteng", ""], ["Zhang", "Xuanye", ""], ["Li", "Yanran", ""], ["Li", "Guanbin", ""], ["Wu", "Xiaojun", ""], ["Xiong", "Zixiang", ""], ["Han", "Xiaoguang", ""], ["Cui", "Shuguang", ""]]}, {"id": "2003.10543", "submitter": "Pierre Ambrosini", "authors": "Pierre Ambrosini, Eva Hollemans, Charlotte F. Kweldam, Geert J. L. H.\n  van Leenders, Sjoerd Stallinga, Frans Vos", "title": "Automated Detection of Cribriform Growth Patterns in Prostate Histology\n  Images", "comments": "15 pages, 6 figures", "journal-ref": "Sci Rep 10, 14904 (2020)", "doi": "10.1038/s41598-020-71942-7", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cribriform growth patterns in prostate carcinoma are associated with poor\nprognosis. We aimed to introduce a deep learning method to detect such patterns\nautomatically. To do so, convolutional neural network was trained to detect\ncribriform growth patterns on 128 prostate needle biopsies. Ensemble learning\ntaking into account other tumor growth patterns during training was used to\ncope with heterogeneous and limited tumor tissue occurrences. ROC and FROC\nanalyses were applied to assess network performance regarding detection of\nbiopsies harboring cribriform growth pattern. The ROC analysis yielded a mean\narea under the curve up to 0.81. FROC analysis demonstrated a sensitivity of\n0.9 for regions larger than 0.0150 mm2 with on average 7.5 false positives. To\nbenchmark method performance for intra-observer annotation variability, false\npositive and negative detections were re-evaluated by the pathologists.\nPathologists considered 9% of the false positive regions as cribriform, and 11%\nas possibly cribriform; 44% of the false negative regions were not annotated as\ncribriform. As a final experiment, the network was also applied on a dataset of\n60 biopsy regions annotated by 23 pathologists. With the cut-off reaching\nhighest sensitivity, all images annotated as cribriform by at least 7/23 of the\npathologists, were all detected as cribriform by the network and 9/60 of the\nimages were detected as cribriform whereas no pathologist labelled them as\nsuch. In conclusion, the proposed deep learning method has high sensitivity for\ndetecting cribriform growth patterns at the expense of a limited number of\nfalse positives. It can detect cribriform regions that are labelled as such by\nat least a minority of pathologists. Therefore, it could assist clinical\ndecision making by suggesting suspicious regions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:56:24 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 13:15:52 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Ambrosini", "Pierre", ""], ["Hollemans", "Eva", ""], ["Kweldam", "Charlotte F.", ""], ["van Leenders", "Geert J. L. H.", ""], ["Stallinga", "Sjoerd", ""], ["Vos", "Frans", ""]]}, {"id": "2003.10557", "submitter": "Roee Litman", "authors": "Sharon Fogel (1), Hadar Averbuch-Elor (2), Sarel Cohen, Shai Mazor (1)\n  and Roee Litman (1) ((1) Amazon Rekognition Israel, (2) Cornell University)", "title": "ScrabbleGAN: Semi-Supervised Varying Length Handwritten Text Generation", "comments": "in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical character recognition (OCR) systems performance have improved\nsignificantly in the deep learning era. This is especially true for handwritten\ntext recognition (HTR), where each author has a unique style, unlike printed\ntext, where the variation is smaller by design. That said, deep learning based\nHTR is limited, as in every other task, by the number of training examples.\nGathering data is a challenging and costly task, and even more so, the labeling\ntask that follows, of which we focus here. One possible approach to reduce the\nburden of data annotation is semi-supervised learning. Semi supervised methods\nuse, in addition to labeled data, some unlabeled samples to improve\nperformance, compared to fully supervised ones. Consequently, such methods may\nadapt to unseen images during test time.\n  We present ScrabbleGAN, a semi-supervised approach to synthesize handwritten\ntext images that are versatile both in style and lexicon. ScrabbleGAN relies on\na novel generative model which can generate images of words with an arbitrary\nlength. We show how to operate our approach in a semi-supervised manner,\nenjoying the aforementioned benefits such as performance boost over state of\nthe art supervised HTR. Furthermore, our generator can manipulate the resulting\ntext style. This allows us to change, for instance, whether the text is\ncursive, or how thin is the pen stroke.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:41:19 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Fogel", "Sharon", "", "Amazon Rekognition Israel"], ["Averbuch-Elor", "Hadar", "", "Cornell University"], ["Cohen", "Sarel", "", "Amazon Rekognition Israel"], ["Mazor", "Shai", "", "Amazon Rekognition Israel"], ["Litman", "Roee", "", "Amazon Rekognition Israel"]]}, {"id": "2003.10566", "submitter": "Al Cannaday", "authors": "Alan B. Cannaday II, Curt H. Davis, Grant J. Scott, Blake Ruprecht,\n  Derek T. Anderson", "title": "Broad Area Search and Detection of Surface-to-Air Missile Sites Using\n  Spatial Fusion of Component Object Detections from Deep Neural Networks", "comments": "9 pages, 9 figures, 9 tables, pre-published expansion of IGARSS2019\n  conference paper \"Improved Search and Detection of Surface-to-Air Missile\n  Sites Using Spatial Fusion of Component Object Detections from Deep Neural\n  Networks\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we demonstrate how Deep Neural Network (DNN) detections of multiple\nconstitutive or component objects that are part of a larger, more complex, and\nencompassing feature can be spatially fused to improve the search, detection,\nand retrieval (ranking) of the larger complex feature. First, scores computed\nfrom a spatial clustering algorithm are normalized to a reference space so that\nthey are independent of image resolution and DNN input chip size. Then,\nmulti-scale DNN detections from various component objects are fused to improve\nthe detection and retrieval of DNN detections of a larger complex feature. We\ndemonstrate the utility of this approach for broad area search and detection of\nSurface-to-Air Missile (SAM) sites that have a very low occurrence rate (only\n16 sites) over a ~90,000 km^2 study area in SE China. The results demonstrate\nthat spatial fusion of multi-scale component-object DNN detections can reduce\nthe detection error rate of SAM Sites by $>$85% while still maintaining a 100%\nrecall. The novel spatial fusion approach demonstrated here can be easily\nextended to a wide variety of other challenging object search and detection\nproblems in large-scale remote sensing image datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 22:10:19 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 18:35:31 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 17:44:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cannaday", "Alan B.", "II"], ["Davis", "Curt H.", ""], ["Scott", "Grant J.", ""], ["Ruprecht", "Blake", ""], ["Anderson", "Derek T.", ""]]}, {"id": "2003.10589", "submitter": "Liliang Ren", "authors": "Liliang Ren, Zhuonan Hao", "title": "A Simple Fix for Convolutional Neural Network via Coordinate Embedding", "comments": "6 pages, 8 figures, Course Project for ECE271B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) has been widely applied in the realm of\ncomputer vision. However, given the fact that CNN models are translation\ninvariant, they are not aware of the coordinate information of each pixel. Thus\nthe generalization ability of CNN will be limited since the coordinate\ninformation is crucial for a model to learn affine transformations which\ndirectly operate on the coordinate of each pixel. In this project, we proposed\na simple approach to incorporate the coordinate information to the CNN model\nthrough coordinate embedding. Our approach does not change the downstream model\narchitecture and can be easily applied to the pre-trained models for the task\nlike object detection. Our experiments on the German Traffic Sign Detection\nBenchmark show that our approach not only significantly improve the model\nperformance but also have better robustness with respect to the affine\ntransformation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:31:27 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ren", "Liliang", ""], ["Hao", "Zhuonan", ""]]}, {"id": "2003.10593", "submitter": "Vincent Christlein", "authors": "Martin Mayr, Martin Stumpf, Anguelos Nicolaou, Mathias Seuret, Andreas\n  Maier, Vincent Christlein", "title": "Spatio-Temporal Handwriting Imitation", "comments": "Main paper: 16 pages, supplemental material: 7 pages", "journal-ref": null, "doi": "10.1007/978-3-030-68238-5_38", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most people think that their handwriting is unique and cannot be imitated by\nmachines, especially not using completely new content. Current cursive\nhandwriting synthesis is visually limited or needs user interaction. We show\nthat subdividing the process into smaller subtasks makes it possible to imitate\nsomeone's handwriting with a high chance to be visually indistinguishable for\nhumans. Therefore, a given handwritten sample will be used as the target style.\nThis sample is transferred to an online sequence. Then, a method for online\nhandwriting synthesis is used to produce a new realistic-looking text primed\nwith the online input sequence. This new text is then rendered and\nstyle-adapted to the input pen. We show the effectiveness of the pipeline by\ngenerating in- and out-of-vocabulary handwritten samples that are validated in\na comprehensive user study. Additionally, we show that also a typical writer\nidentification system can partially be fooled by the created fake handwritings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:46:40 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:09:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mayr", "Martin", ""], ["Stumpf", "Martin", ""], ["Nicolaou", "Anguelos", ""], ["Seuret", "Mathias", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2003.10596", "submitter": "Apurva Gandhi", "authors": "Apurva Gandhi and Shomik Jain", "title": "Adversarial Perturbations Fool Deepfake Detectors", "comments": "To appear in the proceedings of the International Joint Conference on\n  Neural Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work uses adversarial perturbations to enhance deepfake images and fool\ncommon deepfake detectors. We created adversarial perturbations using the Fast\nGradient Sign Method and the Carlini and Wagner L2 norm attack in both blackbox\nand whitebox settings. Detectors achieved over 95% accuracy on unperturbed\ndeepfakes, but less than 27% accuracy on perturbed deepfakes. We also explore\ntwo improvements to deepfake detectors: (i) Lipschitz regularization, and (ii)\nDeep Image Prior (DIP). Lipschitz regularization constrains the gradient of the\ndetector with respect to the input in order to increase robustness to input\nperturbations. The DIP defense removes perturbations using generative\nconvolutional neural networks in an unsupervised manner. Regularization\nimproved the detection of perturbed deepfakes on average, including a 10%\naccuracy boost in the blackbox case. The DIP defense achieved 95% accuracy on\nperturbed deepfakes that fooled the original detector, while retaining 98%\naccuracy in other cases on a 100 image subsample.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 00:54:02 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:41:32 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Gandhi", "Apurva", ""], ["Jain", "Shomik", ""]]}, {"id": "2003.10601", "submitter": "Md Sirajus Salekin", "authors": "Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi,\n  Thao Ho and Yu Sun", "title": "First Investigation Into the Use of Deep Learning for Continuous\n  Assessment of Neonatal Postoperative Pain", "comments": "Accepted in the 15th IEEE International Conference on Automatic Face\n  and Gesture Recognition (FG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first investigation into the use of fully automated\ndeep learning framework for assessing neonatal postoperative pain. It\nspecifically investigates the use of Bilinear Convolutional Neural Network\n(B-CNN) to extract facial features during different levels of postoperative\npain followed by modeling the temporal pattern using Recurrent Neural Network\n(RNN). Although acute and postoperative pain have some common characteristics\n(e.g., visual action units), postoperative pain has a different dynamic, and it\nevolves in a unique pattern over time. Our experimental results indicate a\nclear difference between the pattern of acute and postoperative pain. They also\nsuggest the efficiency of using a combination of bilinear CNN with RNN model\nfor the continuous assessment of postoperative pain intensity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:13:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Salekin", "Md Sirajus", ""], ["Zamzmi", "Ghada", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Ho", "Thao", ""], ["Sun", "Yu", ""]]}, {"id": "2003.10606", "submitter": "Arka Sadhu", "authors": "Arka Sadhu, Kan Chen, Ram Nevatia", "title": "Video Object Grounding using Semantic Roles in Language Description", "comments": "CVPR20 camera-ready including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the task of Video Object Grounding (VOG), which grounds objects in\nvideos referred to in natural language descriptions. Previous methods apply\nimage grounding based algorithms to address VOG, fail to explore the object\nrelation information and suffer from limited generalization. Here, we\ninvestigate the role of object relations in VOG and propose a novel framework\nVOGNet to encode multi-modal object relations via self-attention with relative\nposition encoding. To evaluate VOGNet, we propose novel contrasting sampling\nmethods to generate more challenging grounding input samples, and construct a\nnew dataset called ActivityNet-SRL (ASRL) based on existing caption and\ngrounding datasets. Experiments on ASRL validate the need of encoding object\nrelations in VOG, and our VOGNet outperforms competitive baselines by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:31:14 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Sadhu", "Arka", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "2003.10607", "submitter": "Lie Ju", "authors": "Lie Ju, Xin Wang, Xin Zhao, Huimin Lu, Dwarikanath Mahapatra, Paul\n  Bonnington, Zongyuan Ge", "title": "Synergic Adversarial Label Learning for Grading Retinal Diseases via\n  Knowledge Distillation and Multi-task Learning", "comments": null, "journal-ref": null, "doi": "10.1109/jbhi.2021.3052916", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for comprehensive and automated screening methods for retinal image\nclassification has long been recognized. Well-qualified doctors annotated\nimages are very expensive and only a limited amount of data is available for\nvarious retinal diseases such as age-related macular degeneration (AMD) and\ndiabetic retinopathy (DR). Some studies show that AMD and DR share some common\nfeatures like hemorrhagic points and exudation but most classification\nalgorithms only train those disease models independently. Inspired by knowledge\ndistillation where additional monitoring signals from various sources is\nbeneficial to train a robust model with much fewer data. We propose a method\ncalled synergic adversarial label learning (SALL) which leverages relevant\nretinal disease labels in both semantic and feature space as additional signals\nand train the model in a collaborative manner. Our experiments on DR and AMD\nfundus image classification task demonstrate that the proposed method can\nsignificantly improve the accuracy of the model for grading diseases. In\naddition, we conduct additional experiments to show the effectiveness of SALL\nfrom the aspects of reliability and interpretability in the context of medical\nimaging application.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:32:04 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:25:39 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 04:47:16 GMT"}, {"version": "v4", "created": "Sat, 30 Jan 2021 13:55:21 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ju", "Lie", ""], ["Wang", "Xin", ""], ["Zhao", "Xin", ""], ["Lu", "Huimin", ""], ["Mahapatra", "Dwarikanath", ""], ["Bonnington", "Paul", ""], ["Ge", "Zongyuan", ""]]}, {"id": "2003.10608", "submitter": "Shangbang Long", "authors": "Shangbang Long, Cong Yao", "title": "UnrealText: Synthesizing Realistic Scene Text Images from the Unreal\n  World", "comments": "adding experiments with Mask-RCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data has been a critical tool for training scene text detection and\nrecognition models. On the one hand, synthetic word images have proven to be a\nsuccessful substitute for real images in training scene text recognizers. On\nthe other hand, however, scene text detectors still heavily rely on a large\namount of manually annotated real-world images, which are expensive. In this\npaper, we introduce UnrealText, an efficient image synthesis method that\nrenders realistic images via a 3D graphics engine. 3D synthetic engine provides\nrealistic appearance by rendering scene and text as a whole, and allows for\nbetter text region proposals with access to precise scene information, e.g.\nnormal and even object meshes. The comprehensive experiments verify its\neffectiveness on both scene text detection and recognition. We also generate a\nmultilingual version for future research into multilingual scene text detection\nand recognition. Additionally, we re-annotate scene text recognition datasets\nin a case-sensitive way and include punctuation marks for more comprehensive\nevaluations. The code and the generated datasets are released at:\nhttps://github.com/Jyouhou/UnrealText/ .\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:37:42 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 13:18:45 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 17:27:50 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 15:34:53 GMT"}, {"version": "v5", "created": "Sun, 16 Aug 2020 19:09:37 GMT"}, {"version": "v6", "created": "Tue, 18 Aug 2020 01:06:39 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Long", "Shangbang", ""], ["Yao", "Cong", ""]]}, {"id": "2003.10629", "submitter": "Lei Zhou", "authors": "Lei Zhou, Zixin Luo, Tianwei Shen, Jiahui Zhang, Mingmin Zhen, Yao\n  Yao, Tian Fang and Long Quan", "title": "KFNet: Learning Temporal Camera Relocalization using Kalman Filtering", "comments": "An oral paper of CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal camera relocalization estimates the pose with respect to each video\nframe in sequence, as opposed to one-shot relocalization which focuses on a\nstill image. Even though the time dependency has been taken into account,\ncurrent temporal relocalization methods still generally underperform the\nstate-of-the-art one-shot approaches in terms of accuracy. In this work, we\nimprove the temporal relocalization method by using a network architecture that\nincorporates Kalman filtering (KFNet) for online camera relocalization. In\nparticular, KFNet extends the scene coordinate regression problem to the time\ndomain in order to recursively establish 2D and 3D correspondences for the pose\ndetermination. The network architecture design and the loss formulation are\nbased on Kalman filtering in the context of Bayesian learning. Extensive\nexperiments on multiple relocalization benchmarks demonstrate the high accuracy\nof KFNet at the top of both one-shot and temporal relocalization approaches.\nOur codes are released at https://github.com/zlthinker/KFNet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 02:52:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Zhou", "Lei", ""], ["Luo", "Zixin", ""], ["Shen", "Tianwei", ""], ["Zhang", "Jiahui", ""], ["Zhen", "Mingmin", ""], ["Yao", "Yao", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "2003.10647", "submitter": "Jiaming Song", "authors": "Jiaming Song, Lunjia Hu, Michael Auli, Yann Dauphin, Tengyu Ma", "title": "Robust and On-the-fly Dataset Denoising for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memorization in over-parameterized neural networks could severely hurt\ngeneralization in the presence of mislabeled examples. However, mislabeled\nexamples are hard to avoid in extremely large datasets collected with weak\nsupervision. We address this problem by reasoning counterfactually about the\nloss distribution of examples with uniform random labels had they were trained\nwith the real examples, and use this information to remove noisy examples from\nthe training set. First, we observe that examples with uniform random labels\nhave higher losses when trained with stochastic gradient descent under large\nlearning rates. Then, we propose to model the loss distribution of the\ncounterfactual examples using only the network parameters, which is able to\nmodel such examples with remarkable success. Finally, we propose to remove\nexamples whose loss exceeds a certain quantile of the modeled loss\ndistribution. This leads to On-the-fly Data Denoising (ODD), a simple yet\neffective algorithm that is robust to mislabeled examples, while introducing\nalmost zero computational overhead compared to standard training. ODD is able\nto achieve state-of-the-art results on a wide range of datasets including\nreal-world ones such as WebVision and Clothing1M.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 03:59:26 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 04:59:33 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Song", "Jiaming", ""], ["Hu", "Lunjia", ""], ["Auli", "Michael", ""], ["Dauphin", "Yann", ""], ["Ma", "Tengyu", ""]]}, {"id": "2003.10656", "submitter": "Yuliang Guo", "authors": "Yuliang Guo, Guang Chen, Peitao Zhao, Weide Zhang, Jinghao Miao,\n  Jingao Wang and Tae Eun Choe", "title": "Gen-LaneNet: A Generalized and Scalable Approach for 3D Lane Detection", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-58589-1_40", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalized and scalable method, called Gen-LaneNet, to detect\n3D lanes from a single image. The method, inspired by the latest\nstate-of-the-art 3D-LaneNet, is a unified framework solving image encoding,\nspatial transform of features and 3D lane prediction in a single network.\nHowever, we propose unique designs for Gen-LaneNet in two folds. First, we\nintroduce a new geometry-guided lane anchor representation in a new coordinate\nframe and apply a specific geometric transformation to directly calculate real\n3D lane points from the network output. We demonstrate that aligning the lane\npoints with the underlying top-view features in the new coordinate frame is\ncritical towards a generalized method in handling unfamiliar scenes. Second, we\npresent a scalable two-stage framework that decouples the learning of image\nsegmentation subnetwork and geometry encoding subnetwork. Compared to\n3D-LaneNet, the proposed Gen-LaneNet drastically reduces the amount of 3D lane\nlabels required to achieve a robust solution in real-world application.\nMoreover, we release a new synthetic dataset and its construction strategy to\nencourage the development and evaluation of 3D lane detection methods. In\nexperiments, we conduct extensive ablation study to substantiate the proposed\nGen-LaneNet significantly outperforms 3D-LaneNet in average precision(AP) and\nF-score.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 04:52:06 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Guo", "Yuliang", ""], ["Chen", "Guang", ""], ["Zhao", "Peitao", ""], ["Zhang", "Weide", ""], ["Miao", "Jinghao", ""], ["Wang", "Jingao", ""], ["Choe", "Tae Eun", ""]]}, {"id": "2003.10658", "submitter": "Weide Liu", "authors": "Weide Liu, Chi Zhang, Guosheng Lin, Fayao Liu", "title": "CRNet: Cross-Reference Networks for Few-Shot Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, state-of-the-art image segmentation algorithms are\nbased on deep convolutional neural networks. To render a deep network with the\nability to understand a concept, humans need to collect a large amount of\npixel-level annotated data to train the models, which is time-consuming and\ntedious. Recently, few-shot segmentation is proposed to solve this problem.\nFew-shot segmentation aims to learn a segmentation model that can be\ngeneralized to novel classes with only a few training images. In this paper, we\npropose a cross-reference network (CRNet) for few-shot segmentation. Unlike\nprevious works which only predict the mask in the query image, our proposed\nmodel concurrently make predictions for both the support image and the query\nimage. With a cross-reference mechanism, our network can better find the\nco-occurrent objects in the two images, thus helping the few-shot segmentation\ntask. We also develop a mask refinement module to recurrently refine the\nprediction of the foreground regions. For the $k$-shot learning, we propose to\nfinetune parts of networks to take advantage of multiple labeled support\nimages. Experiments on the PASCAL VOC 2012 dataset show that our network\nachieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 04:55:43 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Liu", "Weide", ""], ["Zhang", "Chi", ""], ["Lin", "Guosheng", ""], ["Liu", "Fayao", ""]]}, {"id": "2003.10663", "submitter": "Zhongguo Li", "authors": "Zhongguo Li, Fan Lyu, Wei Feng, Song Wang", "title": "Modeling Cross-view Interaction Consistency for Paired Egocentric\n  Interaction Recognition", "comments": "ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Augmented Reality (AR), egocentric action recognition\n(EAR) plays important role in accurately understanding demands from the user.\nHowever, EAR is designed to help recognize human-machine interaction in single\negocentric view, thus difficult to capture interactions between two\nface-to-face AR users. Paired egocentric interaction recognition (PEIR) is the\ntask to collaboratively recognize the interactions between two persons with the\nvideos in their corresponding views. Unfortunately, existing PEIR methods\nalways directly use linear decision function to fuse the features extracted\nfrom two corresponding egocentric videos, which ignore consistency of\ninteraction in paired egocentric videos. The consistency of interactions in\npaired videos, and features extracted from them are correlated to each other.\nOn top of that, we propose to build the relevance between two views using\nbiliear pooling, which capture the consistency of two views in feature-level.\nSpecifically, each neuron in the feature maps from one view connects to the\nneurons from another view, which guarantee the compact consistency between two\nviews. Then all possible paired neurons are used for PEIR for the inside\nconsistent information of them. To be efficient, we use compact bilinear\npooling with Count Sketch to avoid directly computing outer product in\nbilinear. Experimental results on dataset PEV shows the superiority of the\nproposed methods on the task PEIR.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 05:05:34 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Zhongguo", ""], ["Lyu", "Fan", ""], ["Feng", "Wei", ""], ["Wang", "Song", ""]]}, {"id": "2003.10664", "submitter": "Pradipta Ghosh", "authors": "Pradipta Ghosh, Xiaochen Liu, Hang Qiu, Marcos A. M. Vieira, Gaurav S.\n  Sukhatme, and Ramesh Govindan", "title": "On Localizing a Camera from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public cameras often have limited metadata describing their attributes. A key\nmissing attribute is the precise location of the camera, using which it is\npossible to precisely pinpoint the location of events seen in the camera. In\nthis paper, we explore the following question: under what conditions is it\npossible to estimate the location of a camera from a single image taken by the\ncamera? We show that, using a judicious combination of projective geometry,\nneural networks, and crowd-sourced annotations from human workers, it is\npossible to position 95% of the images in our test data set to within 12 m.\nThis performance is two orders of magnitude better than PoseNet, a\nstate-of-the-art neural network that, when trained on a large corpus of images\nin an area, can estimate the pose of a single image. Finally, we show that the\ncamera's inferred position and intrinsic parameters can help design a number of\nvirtual sensors, all of which are reasonably accurate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 05:09:01 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ghosh", "Pradipta", ""], ["Liu", "Xiaochen", ""], ["Qiu", "Hang", ""], ["Vieira", "Marcos A. M.", ""], ["Sukhatme", "Gaurav S.", ""], ["Govindan", "Ramesh", ""]]}, {"id": "2003.10670", "submitter": "Xuesong Li", "authors": "Xuesong Li, Jose Guivant, Subhan Khan", "title": "Real-time 3D object proposal generation and classification under limited\n  processing resources", "comments": null, "journal-ref": "Robotics and Autonomous Systems, 130 (2020) 103557", "doi": "10.1016/j.robot.2020.103557", "report-no": "2-s2.0-85084829367", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of detecting 3D objects is important to various robotic\napplications. The existing deep learning-based detection techniques have\nachieved impressive performance. However, these techniques are limited to run\nwith a graphics processing unit (GPU) in a real-time environment. To achieve\nreal-time 3D object detection with limited computational resources for robots,\nwe propose an efficient detection method consisting of 3D proposal generation\nand classification. The proposal generation is mainly based on point\nsegmentation, while the proposal classification is performed by a lightweight\nconvolution neural network (CNN) model. To validate our method, KITTI datasets\nare utilized. The experimental results demonstrate the capability of proposed\nreal-time 3D object detection method from the point cloud with a competitive\nperformance of object recall and classification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 05:36:53 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Li", "Xuesong", ""], ["Guivant", "Jose", ""], ["Khan", "Subhan", ""]]}, {"id": "2003.10685", "submitter": "Jia-Qi Zhang", "authors": "Min Shi, Jia-Qi Zhang, Shu-Yu Chen, Lin Gao, Yu-Kun Lai, Fang-Lue\n  Zhang", "title": "Deep Line Art Video Colorization with a Few References", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coloring line art images based on the colors of reference images is an\nimportant stage in animation production, which is time-consuming and tedious.\nIn this paper, we propose a deep architecture to automatically color line art\nvideos with the same color style as the given reference images. Our framework\nconsists of a color transform network and a temporal constraint network. The\ncolor transform network takes the target line art images as well as the line\nart and color images of one or more reference images as input, and generates\ncorresponding target color images. To cope with larger differences between the\ntarget line art image and reference color images, our architecture utilizes\nnon-local similarity matching to determine the region correspondences between\nthe target image and the reference images, which are used to transform the\nlocal color information from the references to the target. To ensure global\ncolor style consistency, we further incorporate Adaptive Instance Normalization\n(AdaIN) with the transformation parameters obtained from a style embedding\nvector that describes the global color style of the references, extracted by an\nembedder. The temporal constraint network takes the reference images and the\ntarget image together in chronological order, and learns the spatiotemporal\nfeatures through 3D convolution to ensure the temporal consistency of the\ntarget image and the reference image. Our model can achieve even better\ncoloring results by fine-tuning the parameters with only a small amount of\nsamples when dealing with an animation of a new style. To evaluate our method,\nwe build a line art coloring dataset. Experiments show that our method achieves\nthe best performance on line art video coloring compared to the\nstate-of-the-art methods and other baselines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 06:57:40 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 07:34:55 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Shi", "Min", ""], ["Zhang", "Jia-Qi", ""], ["Chen", "Shu-Yu", ""], ["Gao", "Lin", ""], ["Lai", "Yu-Kun", ""], ["Zhang", "Fang-Lue", ""]]}, {"id": "2003.10689", "submitter": "Shengrong Zhao", "authors": "Hu Liang, Shengrong Zhao", "title": "Learning regularization and intensity-gradient-based fidelity for single\n  image super resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to extract more and useful information for single image super resolution\nis an imperative and difficult problem. Learning-based method is a\nrepresentative method for such task. However, the results are not so stable as\nthere may exist big difference between the training data and the test data. The\nregularization-based method can effectively utilize the self-information of\nobservation. However, the degradation model used in regularization-based method\njust considers the degradation in intensity space. It may not reconstruct\nimages well as the degradation reflections in various feature space are not\nconsidered. In this paper, we first study the image degradation progress, and\nestablish degradation model both in intensity and gradient space. Thus, a\ncomprehensive data consistency constraint is established for the\nreconstruction. Consequently, more useful information can be extracted from the\nobserved data. Second, the regularization term is learned by a designed\nsymmetric residual deep neural-network. It can search similar external\ninformation from a predefined dataset avoiding the artificial tendency.\nFinally, the proposed fidelity term and designed regularization term are\nembedded into the regularization framework. Further, an optimization method is\ndeveloped based on the half-quadratic splitting method and the pseudo conjugate\nmethod. Experimental results indicated that the subjective and the objective\nmetric corresponding to the proposed method were better than those obtained by\nthe comparison methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:03:18 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Liang", "Hu", ""], ["Zhao", "Shengrong", ""]]}, {"id": "2003.10690", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Masahiro Oda, Kensaku Mori", "title": "Organ Segmentation From Full-size CT Images Using Memory-Efficient FCN", "comments": null, "journal-ref": "Proc. SPIE 11314, Medical Imaging 2020: Computer-Aided Diagnosis,\n  113140I", "doi": "10.1117/12.2551024", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a memory-efficient fully convolutional network (FCN)\nincorporated with several memory-optimized techniques to reduce the run-time\nGPU memory demand during training phase. In medical image segmentation tasks,\nsubvolume cropping has become a common preprocessing. Subvolumes (or small\npatch volumes) were cropped to reduce GPU memory demand. However, small patch\nvolumes capture less spatial context that leads to lower accuracy. As a pilot\nstudy, the purpose of this work is to propose a memory-efficient FCN which\nenables us to train the model on full size CT image directly without subvolume\ncropping, while maintaining the segmentation accuracy. We optimize our network\nfrom both architecture and implementation. With the development of computing\nhardware, such as graphics processing unit (GPU) and tensor processing unit\n(TPU), now deep learning applications is able to train networks with large\ndatasets within acceptable time. Among these applications, semantic\nsegmentation using fully convolutional network (FCN) also has gained a\nsignificant improvement against traditional image processing approaches in both\ncomputer vision and medical image processing fields. However, unlike general\ncolor images used in computer vision tasks, medical images have larger scales\nthan color images such as 3D computed tomography (CT) images, micro CT images,\nand histopathological images. For training these medical images, the large\ndemand of computing resource become a severe problem. In this paper, we present\na memory-efficient FCN to tackle the high GPU memory demand challenge in organ\nsegmentation problem from clinical CT images. The experimental results\ndemonstrated that our GPU memory demand is about 40% of baseline architecture,\nparameter amount is about 30% of the baseline.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:12:45 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Wang", "Chenglong", ""], ["Oda", "Masahiro", ""], ["Mori", "Kensaku", ""]]}, {"id": "2003.10739", "submitter": "Duo Li", "authors": "Duo Li and Qifeng Chen", "title": "Dynamic Hierarchical Mimicking Towards Consistent Optimization\n  Objectives", "comments": "Accepted by CVPR 2020. Code and pretrained models are available at\n  https://github.com/d-li14/DHM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the depth of modern Convolutional Neural Networks (CNNs) surpasses that\nof the pioneering networks with a significant margin, the traditional way of\nappending supervision only over the final classifier and progressively\npropagating gradient flow upstream remains the training mainstay. Seminal\nDeeply-Supervised Networks (DSN) were proposed to alleviate the difficulty of\noptimization arising from gradient flow through a long chain. However, it is\nstill vulnerable to issues including interference to the hierarchical\nrepresentation generation process and inconsistent optimization objectives, as\nillustrated theoretically and empirically in this paper. Complementary to\nprevious training strategies, we propose Dynamic Hierarchical Mimicking, a\ngeneric feature learning mechanism, to advance CNN training with enhanced\ngeneralization ability. Partially inspired by DSN, we fork delicately designed\nside branches from the intermediate layers of a given neural network. Each\nbranch can emerge from certain locations of the main branch dynamically, which\nnot only retains representation rooted in the backbone network but also\ngenerates more diverse representations along its own pathway. We go one step\nfurther to promote multi-level interactions among different branches through an\noptimization formula with probabilistic prediction matching losses, thus\nguaranteeing a more robust optimization process and better representation\nability. Experiments on both category and instance recognition tasks\ndemonstrate the substantial improvements of our proposed method over its\ncorresponding counterparts using diverse state-of-the-art CNN architectures.\nCode and models are publicly available at https://github.com/d-li14/DHM\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:56:13 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Duo", ""], ["Chen", "Qifeng", ""]]}, {"id": "2003.10751", "submitter": "Tobias Czempiel", "authors": "Tobias Czempiel, Magdalini Paschali, Matthias Keicher, Walter Simson,\n  Hubertus Feussner, Seong Tae Kim, Nassir Navab", "title": "TeCNO: Surgical Phase Recognition with Multi-Stage Temporal\n  Convolutional Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical phase recognition is a challenging and crucial task with\nthe potential to improve patient safety and become an integral part of\nintra-operative decision-support systems. In this paper, we propose, for the\nfirst time in workflow analysis, a Multi-Stage Temporal Convolutional Network\n(MS-TCN) that performs hierarchical prediction refinement for surgical phase\nrecognition. Causal, dilated convolutions allow for a large receptive field and\nonline inference with smooth predictions even during ambiguous transitions. Our\nmethod is thoroughly evaluated on two datasets of laparoscopic cholecystectomy\nvideos with and without the use of additional surgical tool information.\nOutperforming various state-of-the-art LSTM approaches, we verify the\nsuitability of the proposed causal MS-TCN for surgical phase recognition.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:12:30 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Czempiel", "Tobias", ""], ["Paschali", "Magdalini", ""], ["Keicher", "Matthias", ""], ["Simson", "Walter", ""], ["Feussner", "Hubertus", ""], ["Kim", "Seong Tae", ""], ["Navab", "Nassir", ""]]}, {"id": "2003.10757", "submitter": "Daniel Ward", "authors": "Daniel Ward, Peyman Moghadam", "title": "Scalable learning for bridging the species gap in image-based plant\n  phenotyping", "comments": "Under review. Abstract modified to meed arXiv requirements. Dataset\n  available at: https://csiro-robotics.github.io/UPGen_Webpage/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional paradigm of applying deep learning -- collect, annotate and\ntrain on data -- is not applicable to image-based plant phenotyping as almost\n400,000 different plant species exists. Data costs include growing physical\nsamples, imaging and labelling them. Model performance is impacted by the\nspecies gap between the domain of each plant species, it is not generalisable\nand may not transfer to unseen plant species. In this paper, we investigate the\nuse of synthetic data for leaf instance segmentation. We study multiple\nsynthetic data training regimes using Mask-RCNN when few or no annotated real\ndata is available. We also present UPGen: a Universal Plant Generator for\nbridging the species gap. UPGen leverages domain randomisation to produce\nwidely distributed data samples and models stochastic biological variation. Our\nmethods outperform standard practices, such as transfer learning from publicly\navailable plant data, by 26.6% and 51.46% on two unseen plant species\nrespectively. We benchmark UPGen by competing in the CVPPP Leaf Segmentation\nChallenge and set a new state-of-the-art, a mean of 88% across A1-4 test\ndatasets. This study is applicable to use of synthetic data for automating the\nmeasurement of phenotypic traits. Our synthetic dataset and pretrained model\nare available at https://csiro-robotics.github.io/UPGen_Webpage/.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:26:40 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 02:50:45 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ward", "Daniel", ""], ["Moghadam", "Peyman", ""]]}, {"id": "2003.10758", "submitter": "Qiang Wang", "authors": "Qiang Wang, Shaohuai Shi, Shizhen Zheng, Kaiyong Zhao, Xiaowen Chu", "title": "FADNet: A Fast and Accurate Network for Disparity Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved great success in the area of\ncomputer vision. The disparity estimation problem tends to be addressed by DNNs\nwhich achieve much better prediction accuracy in stereo matching than\ntraditional hand-crafted feature based methods. On one hand, however, the\ndesigned DNNs require significant memory and computation resources to\naccurately predict the disparity, especially for those 3D convolution based\nnetworks, which makes it difficult for deployment in real-time applications. On\nthe other hand, existing computation-efficient networks lack expression\ncapability in large-scale datasets so that they cannot make an accurate\nprediction in many scenarios. To this end, we propose an efficient and accurate\ndeep network for disparity estimation named FADNet with three main features: 1)\nIt exploits efficient 2D based correlation layers with stacked blocks to\npreserve fast computation; 2) It combines the residual structures to make the\ndeeper model easier to learn; 3) It contains multi-scale predictions so as to\nexploit a multi-scale weight scheduling training technique to improve the\naccuracy. We conduct experiments to demonstrate the effectiveness of FADNet on\ntwo popular datasets, Scene Flow and KITTI 2015. Experimental results show that\nFADNet achieves state-of-the-art prediction accuracy, and runs at a significant\norder of magnitude faster speed than existing 3D models. The codes of FADNet\nare available at https://github.com/HKBU-HPML/FADNet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:27:11 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Wang", "Qiang", ""], ["Shi", "Shaohuai", ""], ["Zheng", "Shizhen", ""], ["Zhao", "Kaiyong", ""], ["Chu", "Xiaowen", ""]]}, {"id": "2003.10760", "submitter": "Alice Yang", "authors": "Alice Yi Yang and Ling Cheng", "title": "Surface Damage Detection Scheme using Convolutional Neural Network and\n  Artificial Neural Network", "comments": null, "journal-ref": null, "doi": "10.23919/FUSION45008.2020.9190400", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surface damage on concrete is important as the damage can affect the\nstructural integrity of the structure. This paper proposes a two-step surface\ndamage detection scheme using Convolutional Neural Network (CNN) and Artificial\nNeural Network (ANN). The CNN classifies given input images into two\ncategories: positive and negative. The positive category is where the surface\ndamage is present within the image, otherwise the image is classified as\nnegative. This is an image-based classification. The ANN accepts image inputs\nthat have been classified as positive by the ANN. This reduces the number of\nimages that are further processed by the ANN. The ANN performs feature-based\nclassification, in which the features are extracted from the detected edges\nwithin the image. The edges are detected using Canny edge detection. A total of\n19 features are extracted from the detected edges. These features are inputs\ninto the ANN. The purpose of the ANN is to highlight only the positive damaged\nedges within the image. The CNN achieves an accuracy of 80.7% for image\nclassification and the ANN achieves an accuracy of 98.1% for surface detection.\nThe decreased accuracy in the CNN is due to the false positive detection,\nhowever false positives are tolerated whereas false negatives are not. The\nfalse negative detection for both CNN and ANN in the two-step scheme are 0%.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 10:29:02 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 14:59:38 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Alice Yi", ""], ["Cheng", "Ling", ""]]}, {"id": "2003.10769", "submitter": "Biraja Ghoshal", "authors": "Biraja Ghoshal, Allan Tucker", "title": "Estimating Uncertainty and Interpretability in Deep Learning for\n  Coronavirus (COVID-19) Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has achieved state of the art performance in medical imaging.\nHowever, these methods for disease detection focus exclusively on improving the\naccuracy of classification or predictions without quantifying uncertainty in a\ndecision. Knowing how much confidence there is in a computer-based medical\ndiagnosis is essential for gaining clinicians trust in the technology and\ntherefore improve treatment. Today, the 2019 Coronavirus (SARS-CoV-2)\ninfections are a major healthcare challenge around the world. Detecting\nCOVID-19 in X-ray images is crucial for diagnosis, assessment and treatment.\nHowever, diagnostic uncertainty in the report is a challenging and yet\ninevitable task for radiologist. In this paper, we investigate how drop-weights\nbased Bayesian Convolutional Neural Networks (BCNN) can estimate uncertainty in\nDeep Learning solution to improve the diagnostic performance of the\nhuman-machine team using publicly available COVID-19 chest X-ray dataset and\nshow that the uncertainty in prediction is highly correlates with accuracy of\nprediction. We believe that the availability of uncertainty-aware deep learning\nsolution will enable a wider adoption of Artificial Intelligence (AI) in a\nclinical setting.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 21:58:13 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:48:13 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Ghoshal", "Biraja", ""], ["Tucker", "Allan", ""]]}, {"id": "2003.10773", "submitter": "Yusen Liu", "authors": "Yusen Liu, Dayiheng Liu, Jiancheng Lv, Yongsheng Sang", "title": "Generating Chinese Poetry from Images via Concrete and Abstract\n  Information", "comments": "Accepted by the 2020 International Joint Conference on Neural\n  Networks (IJCNN 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the automatic generation of classical Chinese poetry has\nmade great progress. Besides focusing on improving the quality of the generated\npoetry, there is a new topic about generating poetry from an image. However,\nthe existing methods for this topic still have the problem of topic drift and\nsemantic inconsistency, and the image-poem pairs dataset is hard to be built\nwhen training these models. In this paper, we extract and integrate the\nConcrete and Abstract information from images to address those issues. We\nproposed an infilling-based Chinese poetry generation model which can infill\nthe Concrete keywords into each line of poems in an explicit way, and an\nabstract information embedding to integrate the Abstract information into\ngenerated poems. In addition, we use non-parallel data during training and\nconstruct separate image datasets and poem datasets to train the different\ncomponents in our framework. Both automatic and human evaluation results show\nthat our approach can generate poems which have better consistency with images\nwithout losing the quality.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:17:20 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Liu", "Yusen", ""], ["Liu", "Dayiheng", ""], ["Lv", "Jiancheng", ""], ["Sang", "Yongsheng", ""]]}, {"id": "2003.10778", "submitter": "Jevgenij Gamper", "authors": "Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benes, Simon Graham,\n  Mostafa Jahanifar, Syed Ali Khurram, Ayesha Azam, Katherine Hewitt, Nasir\n  Rajpoot", "title": "PanNuke Dataset Extension, Insights and Baselines", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging area of computational pathology (CPath) is ripe ground for the\napplication of deep learning (DL) methods to healthcare due to the sheer volume\nof raw pixel data in whole-slide images (WSIs) of cancerous tissue slides.\nHowever, it is imperative for the DL algorithms relying on nuclei-level details\nto be able to cope with data from `the clinical wild', which tends to be quite\nchallenging.\n  We study, and extend recently released PanNuke dataset consisting of ~200,000\nnuclei categorized into 5 clinically important classes for the challenging\ntasks of segmenting and classifying nuclei in WSIs. Previous pan-cancer\ndatasets consisted of only up to 9 different tissues and up to 21,000 unlabeled\nnuclei and just over 24,000 labeled nuclei with segmentation masks. PanNuke\nconsists of 19 different tissue types that have been semi-automatically\nannotated and quality controlled by clinical pathologists, leading to a dataset\nwith statistics similar to the clinical wild and with minimal selection bias.\nWe study the performance of segmentation and classification models when applied\nto the proposed dataset and demonstrate the application of models trained on\nPanNuke to whole-slide images. We provide comprehensive statistics about the\ndataset and outline recommendations and research directions to address the\nlimitations of existing DL tools when applied to real-world CPath applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:25:12 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 20:03:30 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 10:57:58 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 11:00:15 GMT"}, {"version": "v5", "created": "Tue, 7 Apr 2020 18:44:37 GMT"}, {"version": "v6", "created": "Mon, 20 Apr 2020 14:10:43 GMT"}, {"version": "v7", "created": "Wed, 22 Apr 2020 08:52:04 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Gamper", "Jevgenij", ""], ["Koohbanani", "Navid Alemi", ""], ["Benes", "Ksenija", ""], ["Graham", "Simon", ""], ["Jahanifar", "Mostafa", ""], ["Khurram", "Syed Ali", ""], ["Azam", "Ayesha", ""], ["Hewitt", "Katherine", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2003.10780", "submitter": "Muhammad Abdullah Jamal", "authors": "Muhammad Abdullah Jamal and Matthew Brown and Ming-Hsuan Yang and\n  Liqiang Wang and Boqing Gong", "title": "Rethinking Class-Balanced Methods for Long-Tailed Visual Recognition\n  from a Domain Adaptation Perspective", "comments": "Accepted for publication at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object frequency in the real world often follows a power law, leading to a\nmismatch between datasets with long-tailed class distributions seen by a\nmachine learning model and our expectation of the model to perform well on all\nclasses. We analyze this mismatch from a domain adaptation point of view. First\nof all, we connect existing class-balanced methods for long-tailed\nclassification to target shift, a well-studied scenario in domain adaptation.\nThe connection reveals that these methods implicitly assume that the training\ndata and test data share the same class-conditioned distribution, which does\nnot hold in general and especially for the tail classes. While a head class\ncould contain abundant and diverse training examples that well represent the\nexpected data at inference time, the tail classes are often short of\nrepresentative training data. To this end, we propose to augment the classic\nclass-balanced learning by explicitly estimating the differences between the\nclass-conditioned distributions with a meta-learning approach. We validate our\napproach with six benchmark datasets and three loss functions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 11:28:42 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Jamal", "Muhammad Abdullah", ""], ["Brown", "Matthew", ""], ["Yang", "Ming-Hsuan", ""], ["Wang", "Liqiang", ""], ["Gong", "Boqing", ""]]}, {"id": "2003.10815", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Peter Corcoran", "title": "Dataset Cleaning -- A Cross Validation Methodology for Large Facial\n  Datasets using Face Recognition", "comments": "2020 Twelfth International Conference on Quality of Multimedia\n  Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large \"in the wild\" face datasets have been released in an\nattempt to facilitate progress in tasks such as face detection, face\nrecognition, and other tasks. Most of these datasets are acquired from webpages\nwith automatic procedures. As a consequence, noisy data are often found.\nFurthermore, in these large face datasets, the annotation of identities is\nimportant as they are used for training face recognition algorithms. But due to\nthe automatic way of gathering these datasets and due to their large size, many\nidentities folder contain mislabeled samples which deteriorates the quality of\nthe datasets. In this work, it is presented a semi-automatic method for\ncleaning the noisy large face datasets with the use of face recognition. This\nmethodology is applied to clean the CelebA dataset and show its effectiveness.\nFurthermore, the list with the mislabelled samples in the CelebA dataset is\nmade available.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:01:13 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Corcoran", "Peter", ""]]}, {"id": "2003.10817", "submitter": "Kedan Li", "authors": "Kedan Li, Min Jin Chong, Jingen Liu, David Forsyth", "title": "Toward Accurate and Realistic Virtual Try-on Through Shape Matching and\n  Multiple Warps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A virtual try-on method takes a product image and an image of a model and\nproduces an image of the model wearing the product. Most methods essentially\ncompute warps from the product image to the model image and combine using image\ngeneration methods. However, obtaining a realistic image is challenging because\nthe kinematics of garments is complex and because outline, texture, and shading\ncues in the image reveal errors to human viewers. The garment must have\nappropriate drapes; texture must be warped to be consistent with the shape of a\ndraped garment; small details (buttons, collars, lapels, pockets, etc.) must be\nplaced appropriately on the garment, and so on. Evaluation is particularly\ndifficult and is usually qualitative.\n  This paper uses quantitative evaluation on a challenging, novel dataset to\ndemonstrate that (a) for any warping method, one can choose target models\nautomatically to improve results, and (b) learning multiple coordinated\nspecialized warpers offers further improvements on results. Target models are\nchosen by a learned embedding procedure that predicts a representation of the\nproducts the model is wearing. This prediction is used to match products to\nmodels. Specialized warpers are trained by a method that encourages a second\nwarper to perform well in locations where the first works poorly. The warps are\nthen combined using a U-Net. Qualitative evaluation confirms that these\nimprovements are wholesale over outline, texture shading, and garment details.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 03:59:06 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 01:15:54 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Li", "Kedan", ""], ["Chong", "Min Jin", ""], ["Liu", "Jingen", ""], ["Forsyth", "David", ""]]}, {"id": "2003.10819", "submitter": "Niklas Gunnarsson", "authors": "Niklas Gunnarsson, Jens Sj\\\"olund and Thomas B. Sch\\\"on", "title": "Registration by tracking for sequential 2D MRI", "comments": "Currently under review for a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our anatomy is in constant motion. With modern MR imaging it is possible to\nrecord this motion in real-time during an ongoing radiation therapy session. In\nthis paper we present an image registration method that exploits the sequential\nnature of 2D MR images to estimate the corresponding displacement field. The\nmethod employs several discriminative correlation filters that independently\ntrack specific points. Together with a sparse-to-dense interpolation scheme we\ncan then estimate of the displacement field. The discriminative correlation\nfilters are trained online, and our method is modality agnostic. For the\ninterpolation scheme we use a neural network with normalized convolutions that\nis trained using synthetic diffeomorphic displacement fields. The method is\nevaluated on a segmented cardiac dataset and when compared to two conventional\nmethods we observe an improved performance. This improvement is especially\npronounced when it comes to the detection of larger motions of small objects.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:12:42 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gunnarsson", "Niklas", ""], ["Sj\u00f6lund", "Jens", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "2003.10822", "submitter": "Mira Park Dr.", "authors": "Thi Phuoc Hanh Nguyen, Zinan Cai, Khanh Nguyen, Sokuntheariddh Keth,\n  Ningyuan Shen, Mira Park", "title": "Pre-processing Image using Brightening, CLAHE and RETINEX", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on finding the most optimal pre-processing methods\nconsidering three common algorithms for image enhancement: Brightening, CLAHE\nand Retinex. For the purpose of image training in general, these methods will\nbe combined to find out the most optimal method for image enhancement. We have\ncarried out the research on the different permutation of three methods:\nBrightening, CLAHE and Retinex. The evaluation is based on Canny Edge detection\napplied to all processed images. Then the sharpness of objects will be\njustified by true positive pixels number in comparison between images. After\nusing different number combinations pre-processing functions on images, CLAHE\nproves to be the most effective in edges improvement, Brightening does not show\nmuch effect on the edges enhancement, and the Retinex even reduces the\nsharpness of images and shows little contribution on images enhancement.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 10:35:24 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Nguyen", "Thi Phuoc Hanh", ""], ["Cai", "Zinan", ""], ["Nguyen", "Khanh", ""], ["Keth", "Sokuntheariddh", ""], ["Shen", "Ningyuan", ""], ["Park", "Mira", ""]]}, {"id": "2003.10823", "submitter": "Natalia Efremova", "authors": "Conrad James Foley, Sagar Vaze, Mohamed El Amine Seddiq, Alexey\n  Unagaev, Natalia Efremova", "title": "SMArtCast: Predicting soil moisture interpolations into the future using\n  Earth observation data in a deep learning framework", "comments": "Climate change AI workshop", "journal-ref": "ICLR 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil moisture is critical component of crop health and monitoring it can\nenable further actions for increasing yield or preventing catastrophic die off.\nAs climate change increases the likelihood of extreme weather events and\nreduces the predictability of weather, and non-optimal soil moistures for crops\nmay become more likely. In this work, we a series of LSTM architectures to\nanalyze measurements of soil moisture and vegetation indiced derived from\nsatellite imagery. The system learns to predict the future values of these\nmeasurements. These spatially sparse values and indices are used as input\nfeatures to an interpolation method that infer spatially dense moisture map for\na future time point. This has the potential to provide advance warning for soil\nmoistures that may be inhospitable to crops across an area with limited\nmonitoring capacity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 23:06:14 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 20:46:38 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Foley", "Conrad James", ""], ["Vaze", "Sagar", ""], ["Seddiq", "Mohamed El Amine", ""], ["Unagaev", "Alexey", ""], ["Efremova", "Natalia", ""]]}, {"id": "2003.10826", "submitter": "Yizhak Ben-Shabat", "authors": "Yizhak Ben-Shabat and Stephen Gould", "title": "DeepFit: 3D Surface Fitting via Neural Network Weighted Least Squares", "comments": "arXiv admin note: text overlap with arXiv:1812.00709", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a surface fitting method for unstructured 3D point clouds. This\nmethod, called DeepFit, incorporates a neural network to learn point-wise\nweights for weighted least squares polynomial surface fitting. The learned\nweights act as a soft selection for the neighborhood of surface points thus\navoiding the scale selection required of previous methods. To train the network\nwe propose a novel surface consistency loss that improves point weight\nestimation. The method enables extracting normal vectors and other geometrical\nproperties, such as principal curvatures, the latter were not presented as\nground truth during training. We achieve state-of-the-art results on a\nbenchmark normal and curvature estimation dataset, demonstrate robustness to\nnoise, outliers and density variations, and show its application on noise\nremoval.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 09:18:54 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Ben-Shabat", "Yizhak", ""], ["Gould", "Stephen", ""]]}, {"id": "2003.10834", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Mehdi Minaei, Amirali Abdolrashidi", "title": "Palm-GAN: Generating Realistic Palmprint Images Using Total-Variation\n  Regularized GAN", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.10482,\n  arXiv:1812.04822", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic palmprint (more generally biometric) images has always\nbeen an interesting and, at the same time, challenging problem. Classical\nstatistical models fail to generate realistic-looking palmprint images, as they\nare not powerful enough to capture the complicated texture representation of\npalmprint images. In this work, we present a deep learning framework based on\ngenerative adversarial networks (GAN), which is able to generate realistic\npalmprint images. To help the model learn more realistic images, we proposed to\nadd a suitable regularization to the loss function, which imposes the line\nconnectivity of generated palmprint images. This is very desirable for\npalmprints, as the principal lines in palm are usually connected. We apply this\nframework to a popular palmprint databases, and generate images which look very\nrealistic, and similar to the samples in this database. Through experimental\nresults, we show that the generated palmprint images look very realistic, have\na good diversity, and are able to capture different parts of the prior\ndistribution. We also report the Frechet Inception distance (FID) of the\nproposed model, and show that our model is able to achieve really good\nquantitative performance in terms of FID score.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 03:24:36 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Minaee", "Shervin", ""], ["Minaei", "Mehdi", ""], ["Abdolrashidi", "Amirali", ""]]}, {"id": "2003.10839", "submitter": "Ophir Gozes", "authors": "Ophir Gozes and Hayit Greenspan", "title": "Bone Structures Extraction and Enhancement in Chest Radiographs via CNN\n  Trained on Synthetic Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.05989", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a deep learning-based image processing technique\nfor extraction of bone structures in chest radiographs using a U-Net FCNN. The\nU-Net was trained to accomplish the task in a fully supervised setting. To\ncreate the training image pairs, we employed simulated X-Ray or Digitally\nReconstructed Radiographs (DRR), derived from 664 CT scans belonging to the\nLIDC-IDRI dataset. Using HU based segmentation of bone structures in the CT\ndomain, a synthetic 2D \"Bone x-ray\" DRR is produced and used for training the\nnetwork. For the reconstruction loss, we utilize two loss functions- L1 Loss\nand perceptual loss. Once the bone structures are extracted, the original image\ncan be enhanced by fusing the original input x-ray and the synthesized \"Bone\nX-ray\". We show that our enhancement technique is applicable to real x-ray\ndata, and display our results on the NIH Chest X-Ray-14 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 20:27:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gozes", "Ophir", ""], ["Greenspan", "Hayit", ""]]}, {"id": "2003.10847", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Shabab Bazrafkan, Peter Corcoran", "title": "Re-Training StyleGAN -- A First Step Towards Building Large, Scalable\n  Synthetic Facial Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  StyleGAN is a state-of-art generative adversarial network architecture that\ngenerates random 2D high-quality synthetic facial data samples. In this paper,\nwe recap the StyleGAN architecture and training methodology and present our\nexperiences of retraining it on a number of alternative public datasets.\nPractical issues and challenges arising from the retraining process are\ndiscussed. Tests and validation results are presented and a comparative\nanalysis of several different re-trained StyleGAN weightings is provided 1. The\nrole of this tool in building large, scalable datasets of synthetic facial data\nis also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:47:07 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Bazrafkan", "Shabab", ""], ["Corcoran", "Peter", ""]]}, {"id": "2003.10849", "submitter": "Ceren Kaya", "authors": "Ali Narin, Ceren Kaya, Ziynet Pamuk", "title": "Automatic Detection of Coronavirus Disease (COVID-19) Using X-ray Images\n  and Deep Convolutional Neural Networks", "comments": "The manuscript has 31 pages, 12 figures and 5 tables", "journal-ref": "Pattern Analysis and Applications (2021)", "doi": "10.1007/s10044-021-00984-y", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2019 novel coronavirus disease (COVID-19), with a starting point in\nChina, has spread rapidly among people living in other countries, and is\napproaching approximately 34,986,502 cases worldwide according to the\nstatistics of European Centre for Disease Prevention and Control. There are a\nlimited number of COVID-19 test kits available in hospitals due to the\nincreasing cases daily. Therefore, it is necessary to implement an automatic\ndetection system as a quick alternative diagnosis option to prevent COVID-19\nspreading among people. In this study, five pre-trained convolutional neural\nnetwork based models (ResNet50, ResNet101, ResNet152, InceptionV3 and\nInception-ResNetV2) have been proposed for the detection of coronavirus\npneumonia infected patient using chest X-ray radiographs. We have implemented\nthree different binary classifications with four classes (COVID-19, normal\n(healthy), viral pneumonia and bacterial pneumonia) by using 5-fold cross\nvalidation. Considering the performance results obtained, it has seen that the\npre-trained ResNet50 model provides the highest classification performance\n(96.1% accuracy for Dataset-1, 99.5% accuracy for Dataset-2 and 99.7% accuracy\nfor Dataset-3) among other four used models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 13:50:23 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 11:35:35 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 08:29:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Narin", "Ali", ""], ["Kaya", "Ceren", ""], ["Pamuk", "Ziynet", ""]]}, {"id": "2003.10867", "submitter": "Jingwei Song", "authors": "Jingwei Song, Jun Wang, Liang Zhao, Shoudong Huang and Gamini\n  Dissanayake", "title": "Dynamic Reconstruction of Deformable Soft-tissue with Stereo Scope in\n  Minimal Invasive Surgery", "comments": "Published in IROS2017 ()2017 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems. arXiv admin note: text overlap with\n  arXiv:1803.02009", "journal-ref": null, "doi": "10.1109/LRA.2017.2735487", "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In minimal invasive surgery, it is important to rebuild and visualize the\nlatest deformed shape of soft-tissue surfaces to mitigate tissue damages. This\npaper proposes an innovative Simultaneous Localization and Mapping (SLAM)\nalgorithm for deformable dense reconstruction of surfaces using a sequence of\nimages from a stereoscope. We introduce a warping field based on the Embedded\nDeformation (ED) nodes with 3D shapes recovered from consecutive pairs of\nstereo images. The warping field is estimated by deforming the last updated\nmodel to the current live model. Our SLAM system can: (1) Incrementally build a\nlive model by progressively fusing new observations with vivid accurate\ntexture. (2) Estimate the deformed shape of unobserved region with the\nprinciple As-Rigid-As-Possible. (3) Show the consecutive shape of models. (4)\nEstimate the current relative pose between the soft-tissue and the scope.\nIn-vivo experiments with publicly available datasets demonstrate that the 3D\nmodels can be incrementally built for different soft-tissues with different\ndeformations from sequences of stereo images obtained by laparoscopes. Results\nshow the potential clinical application of our SLAM system for providing\nsurgeon useful shape and texture information in minimal invasive surgery.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 16:50:38 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Song", "Jingwei", ""], ["Wang", "Jun", ""], ["Zhao", "Liang", ""], ["Huang", "Shoudong", ""], ["Dissanayake", "Gamini", ""]]}, {"id": "2003.10873", "submitter": "Min Wang", "authors": "Min Wang, Feng Qiu, Wentao Liu, Chen Qian, Xiaowei Zhou, Lizhuang Ma", "title": "Monocular Human Pose and Shape Reconstruction using Part Differentiable\n  Rendering", "comments": "Accepted by Pacific Graphcis 2020", "journal-ref": "In Computer Graphics Forum, vol. 39, no. 7, pp. 351-362. 2020", "doi": "10.1111/cgf.14150", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superior human pose and shape reconstruction from monocular images depends on\nremoving the ambiguities caused by occlusions and shape variance. Recent works\nsucceed in regression-based methods which estimate parametric models directly\nthrough a deep neural network supervised by 3D ground truth. However, 3D ground\ntruth is neither in abundance nor can efficiently be obtained. In this paper,\nwe introduce body part segmentation as critical supervision. Part segmentation\nnot only indicates the shape of each body part but helps to infer the\nocclusions among parts as well. To improve the reconstruction with part\nsegmentation, we propose a part-level differentiable renderer that enables\npart-based models to be supervised by part segmentation in neural networks or\noptimization loops. We also introduce a general parametric model engaged in the\nrendering pipeline as an intermediate representation between skeletons and\ndetailed shapes, which consists of primitive geometries for better\ninterpretability. The proposed approach combines parameter regression, body\nmodel optimization, and detailed model registration altogether. Experimental\nresults demonstrate that the proposed method achieves balanced evaluation on\npose and shape, and outperforms the state-of-the-art approaches on Human3.6M,\nUP-3D and LSP datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:25:46 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 08:57:26 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Wang", "Min", ""], ["Qiu", "Feng", ""], ["Liu", "Wentao", ""], ["Qian", "Chen", ""], ["Zhou", "Xiaowei", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2003.10895", "submitter": "Amir Livne", "authors": "Amir Livne, Alex Bronstein, Ron Kimmel, Ziv Aviv, Shahaf Grofit", "title": "Do We Need Depth in State-Of-The-Art Face Authentication?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some face recognition methods are designed to utilize geometric information\nextracted from depth sensors to overcome the weaknesses of single-image based\nrecognition technologies. However, the accurate acquisition of the depth\nprofile is an expensive and challenging process. Here, we introduce a novel\nmethod that learns to recognize faces from stereo camera systems without the\nneed to explicitly compute the facial surface or depth map. The raw face stereo\nimages along with the location in the image from which the face is extracted\nallow the proposed CNN to improve the recognition task while avoiding the need\nto explicitly handle the geometric structure of the face. This way, we keep the\nsimplicity and cost efficiency of identity authentication from a single image,\nwhile enjoying the benefits of geometric data without explicitly reconstructing\nit. We demonstrate that the suggested method outperforms both existing\nsingle-image and explicit depth based methods on large-scale benchmarks, and\neven capable of recognize spoofing attacks. We also provide an ablation study\nthat shows that the suggested method uses the face locations in the left and\nright images to encode informative features that improve the overall\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:51:25 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 11:52:04 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Livne", "Amir", ""], ["Bronstein", "Alex", ""], ["Kimmel", "Ron", ""], ["Aviv", "Ziv", ""], ["Grofit", "Shahaf", ""]]}, {"id": "2003.10898", "submitter": "Hughes Perreault", "authors": "Hughes Perreault, Maguelonne H\\'eritier, Pierre Gravel,\n  Guillaume-Alexandre Bilodeau and Nicolas Saunier", "title": "RN-VID: A Feature Fusion Architecture for Video Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consecutive frames in a video are highly redundant. Therefore, to perform the\ntask of video object detection, executing single frame detectors on every frame\nwithout reusing any information is quite wasteful. It is with this idea in mind\nthat we propose RN-VID (standing for RetinaNet-VIDeo), a novel approach to\nvideo object detection. Our contributions are twofold. First, we propose a new\narchitecture that allows the usage of information from nearby frames to enhance\nfeature maps. Second, we propose a novel module to merge feature maps of same\ndimensions using re-ordering of channels and 1 x 1 convolutions. We then\ndemonstrate that RN-VID achieves better mean average precision (mAP) than\ncorresponding single frame detectors with little additional cost during\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 14:54:46 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 15:53:28 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Perreault", "Hughes", ""], ["H\u00e9ritier", "Maguelonne", ""], ["Gravel", "Pierre", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2003.10925", "submitter": "Zhenzhong Chen", "authors": "Nannan Li, Zhenzhong Chen", "title": "Learning Compact Reward for Image Captioning", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning has shown its advances in generating natural and diverse\ndescriptions in image captioning. However, the learned reward of existing\nadversarial methods is vague and ill-defined due to the reward ambiguity\nproblem. In this paper, we propose a refined Adversarial Inverse Reinforcement\nLearning (rAIRL) method to handle the reward ambiguity problem by disentangling\nreward for each word in a sentence, as well as achieve stable adversarial\ntraining by refining the loss function to shift the generator towards Nash\nequilibrium. In addition, we introduce a conditional term in the loss function\nto mitigate mode collapse and to increase the diversity of the generated\ndescriptions. Our experiments on MS COCO and Flickr30K show that our method can\nlearn compact reward for image captioning.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 15:31:05 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Li", "Nannan", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2003.10955", "submitter": "Shengyu Zhao", "authors": "Shengyu Zhao, Yilun Sheng, Yue Dong, Eric I-Chao Chang, Yan Xu", "title": "MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask", "comments": "CVPR 2020 (Oral)", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), 2020, pp. 6278-6287", "doi": "10.1109/CVPR42600.2020.00631", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature warping is a core technique in optical flow estimation; however, the\nambiguity caused by occluded areas during warping is a major problem that\nremains unsolved. In this paper, we propose an asymmetric occlusion-aware\nfeature matching module, which can learn a rough occlusion mask that filters\nuseless (occluded) areas immediately after feature warping without any explicit\nsupervision. The proposed module can be easily integrated into end-to-end\nnetwork architectures and enjoys performance gains while introducing negligible\ncomputational cost. The learned occlusion mask can be further fed into a\nsubsequent network cascade with dual feature pyramids with which we achieve\nstate-of-the-art performance. At the time of submission, our method, called\nMaskFlownet, surpasses all published optical flow methods on the MPI Sintel,\nKITTI 2012 and 2015 benchmarks. Code is available at\nhttps://github.com/microsoft/MaskFlownet.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 16:29:21 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 16:00:20 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhao", "Shengyu", ""], ["Sheng", "Yilun", ""], ["Dong", "Yue", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""]]}, {"id": "2003.10959", "submitter": "Yuhuang Hu", "authors": "Yuhuang Hu and Tobi Delbruck and Shih-Chii Liu", "title": "Learning to Exploit Multiple Vision Modalities by Using Grafted Networks", "comments": "Accepted at ECCV 2020, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel vision sensors such as thermal, hyperspectral, polarization, and event\ncameras provide information that is not available from conventional intensity\ncameras. An obstacle to using these sensors with current powerful deep neural\nnetworks is the lack of large labeled training datasets. This paper proposes a\nNetwork Grafting Algorithm (NGA), where a new front end network driven by\nunconventional visual inputs replaces the front end network of a pretrained\ndeep network that processes intensity frames. The self-supervised training uses\nonly synchronously-recorded intensity frames and novel sensor data to maximize\nfeature similarity between the pretrained network and the grafted network. We\nshow that the enhanced grafted network reaches competitive average precision\n(AP50) scores to the pretrained network on an object detection task using\nthermal and event camera datasets, with no increase in inference costs.\nParticularly, the grafted network driven by thermal frames showed a relative\nimprovement of 49.11% over the use of intensity frames. The grafted front end\nhas only 5--8% of the total parameters and can be trained in a few hours on a\nsingle GPU equivalent to 5% of the time that would be needed to train the\nentire object detector from labeled data. NGA allows new vision sensors to\ncapitalize on previously pretrained powerful deep models, saving on training\ncost and widening a range of applications for novel sensors.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 16:37:52 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 11:06:05 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 11:35:18 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Hu", "Yuhuang", ""], ["Delbruck", "Tobi", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "2003.10983", "submitter": "Rohan Chabra", "authors": "Rohan Chabra, Jan Eric Lenssen, Eddy Ilg, Tanner Schmidt, Julian\n  Straub, Steven Lovegrove, Richard Newcombe", "title": "Deep Local Shapes: Learning Local SDF Priors for Detailed 3D\n  Reconstruction", "comments": "Accepted at ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently reconstructing complex and intricate surfaces at scale is a\nlong-standing goal in machine perception. To address this problem we introduce\nDeep Local Shapes (DeepLS), a deep shape representation that enables encoding\nand reconstruction of high-quality 3D shapes without prohibitive memory\nrequirements. DeepLS replaces the dense volumetric signed distance function\n(SDF) representation used in traditional surface reconstruction systems with a\nset of locally learned continuous SDFs defined by a neural network, inspired by\nrecent work such as DeepSDF. Unlike DeepSDF, which represents an object-level\nSDF with a neural network and a single latent code, we store a grid of\nindependent latent codes, each responsible for storing information about\nsurfaces in a small local neighborhood. This decomposition of scenes into local\nshapes simplifies the prior distribution that the network must learn, and also\nenables efficient inference. We demonstrate the effectiveness and\ngeneralization power of DeepLS by showing object shape encoding and\nreconstructions of full scenes, where DeepLS delivers high compression,\naccuracy, and local shape completion.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:21:50 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 09:27:30 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 21:52:09 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chabra", "Rohan", ""], ["Lenssen", "Jan Eric", ""], ["Ilg", "Eddy", ""], ["Schmidt", "Tanner", ""], ["Straub", "Julian", ""], ["Lovegrove", "Steven", ""], ["Newcombe", "Richard", ""]]}, {"id": "2003.10985", "submitter": "Chen Chen", "authors": "Kui Jiang and Zhongyuan Wang and Peng Yi and Chen Chen and Baojin\n  Huang and Yimin Luo and Jiayi Ma and Junjun Jiang", "title": "Multi-Scale Progressive Fusion Network for Single Image Deraining", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks in the air appear in various blurring degrees and resolutions\ndue to different distances from their positions to the camera. Similar rain\npatterns are visible in a rain image as well as its multi-scale (or\nmulti-resolution) versions, which makes it possible to exploit such\ncomplementary information for rain streak representation. In this work, we\nexplore the multi-scale collaborative representation for rain streaks from the\nperspective of input image scales and hierarchical deep features in a unified\nframework, termed multi-scale progressive fusion network (MSPFN) for single\nimage rain streak removal. For similar rain streaks at different positions, we\nemploy recurrent calculation to capture the global texture, thus allowing to\nexplore the complementary and redundant information at the spatial dimension to\ncharacterize target rain streaks. Besides, we construct multi-scale pyramid\nstructure, and further introduce the attention mechanism to guide the fine\nfusion of this correlated information from different scales. This multi-scale\nprogressive fusion strategy not only promotes the cooperative representation,\nbut also boosts the end-to-end training. Our proposed method is extensively\nevaluated on several benchmark datasets and achieves state-of-the-art results.\nMoreover, we conduct experiments on joint deraining, detection, and\nsegmentation tasks, and inspire a new research direction of vision task-driven\nimage deraining. The source code is available at\n\\url{https://github.com/kuihua/MSPFN}.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:22:37 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 17:05:34 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Jiang", "Kui", ""], ["Wang", "Zhongyuan", ""], ["Yi", "Peng", ""], ["Chen", "Chen", ""], ["Huang", "Baojin", ""], ["Luo", "Yimin", ""], ["Ma", "Jiayi", ""], ["Jiang", "Junjun", ""]]}, {"id": "2003.10987", "submitter": "Cong Gao", "authors": "Cong Gao, Xingtong Liu, Wenhao Gu, Benjamin Killeen, Mehran Armand,\n  Russell Taylor and Mathias Unberath", "title": "Generalizing Spatial Transformers to Projective Geometry with\n  Applications to 2D/3D Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable rendering is a technique to connect 3D scenes with\ncorresponding 2D images. Since it is differentiable, processes during image\nformation can be learned. Previous approaches to differentiable rendering focus\non mesh-based representations of 3D scenes, which is inappropriate for medical\napplications where volumetric, voxelized models are used to represent anatomy.\nWe propose a novel Projective Spatial Transformer module that generalizes\nspatial transformers to projective geometry, thus enabling differentiable\nvolume rendering. We demonstrate the usefulness of this architecture on the\nexample of 2D/3D registration between radiographs and CT scans. Specifically,\nwe show that our transformer enables end-to-end learning of an image processing\nand projection model that approximates an image similarity function that is\nconvex with respect to the pose parameters, and can thus be optimized\neffectively using conventional gradient descent. To the best of our knowledge,\nthis is the first time that spatial transformers have been described for\nprojective geometry. The source code will be made public upon publication of\nthis manuscript and we hope that our developments will benefit related 3D\nresearch applications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:26:50 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gao", "Cong", ""], ["Liu", "Xingtong", ""], ["Gu", "Wenhao", ""], ["Killeen", "Benjamin", ""], ["Armand", "Mehran", ""], ["Taylor", "Russell", ""], ["Unberath", "Mathias", ""]]}, {"id": "2003.11001", "submitter": "Maxime Mulamba Ke Tchomba", "authors": "Maxime Mulamba, Jayanta Mandi, Rocsildes Canoy, Tias Guns", "title": "Hybrid Classification and Reasoning for Image-based Constraint Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased interest in solving complex constrained problems where\npart of the input is not given as facts but received as raw sensor data such as\nimages or speech. We will use \"visual sudoku\" as a prototype problem, where the\ngiven cell digits are handwritten and provided as an image thereof. In this\ncase, one first has to train and use a classifier to label the images, so that\nthe labels can be used for solving the problem. In this paper, we explore the\nhybridization of classifying the images with the reasoning of a constraint\nsolver. We show that pure constraint reasoning on predictions does not give\nsatisfactory results. Instead, we explore the possibilities of a tighter\nintegration, by exposing the probabilistic estimates of the classifier to the\nconstraint solver. This allows joint inference on these probabilistic\nestimates, where we use the solver to find the maximum likelihood solution. We\nexplore the trade-off between the power of the classifier and the power of the\nconstraint reasoning, as well as further integration through the additional use\nof structural knowledge. Furthermore, we investigate the effect of calibration\nof the probabilistic estimates on the reasoning. Our results show that such\nhybrid approaches vastly outperform a separate approach, which encourages a\nfurther integration of prediction (probabilities) and constraint solving.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:39:49 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Mulamba", "Maxime", ""], ["Mandi", "Jayanta", ""], ["Canoy", "Rocsildes", ""], ["Guns", "Tias", ""]]}, {"id": "2003.11004", "submitter": "Josue Page", "authors": "Josue Page, Federico Saltarin, Yury Belyaev, Ruth Lyck, Paolo Favaro", "title": "Learning to Reconstruct Confocal Microscopy Stacks from Single Light\n  Field Images", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning approach to reconstruct confocal microscopy\nstacks from single light field images. To perform the reconstruction, we\nintroduce the LFMNet, a novel neural network architecture inspired by the U-Net\ndesign. It is able to reconstruct with high-accuracy a 112x112x57.6$\\mu m^3$\nvolume (1287x1287x64 voxels) in 50ms given a single light field image of\n1287x1287 pixels, thus dramatically reducing 720-fold the time for confocal\nscanning of assays at the same volumetric resolution and 64-fold the required\nstorage. To prove the applicability in life sciences, our approach is evaluated\nboth quantitatively and qualitatively on mouse brain slices with fluorescently\nlabelled blood vessels. Because of the drastic reduction in scan time and\nstorage space, our setup and method are directly applicable to real-time in\nvivo 3D microscopy. We provide analysis of the optical design, of the network\narchitecture and of our training procedure to optimally reconstruct volumes for\na given target depth range. To train our network, we built a data set of 362\nlight field images of mouse brain blood vessels and the corresponding aligned\nset of 3D confocal scans, which we use as ground truth. The data set will be\nmade available for research purposes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:46:03 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Page", "Josue", ""], ["Saltarin", "Federico", ""], ["Belyaev", "Yury", ""], ["Lyck", "Ruth", ""], ["Favaro", "Paolo", ""]]}, {"id": "2003.11013", "submitter": "Pietro Astolfi", "authors": "Pietro Astolfi, Ruben Verhagen, Laurent Petit, Emanuele Olivetti,\n  Jonathan Masci, Davide Boscaini, Paolo Avesani", "title": "Tractogram filtering of anatomically non-plausible fibers with geometric\n  deep learning", "comments": "Accepted at MICCAI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tractograms are virtual representations of the white matter fibers of the\nbrain. They are of primary interest for tasks like presurgical planning, and\ninvestigation of neuroplasticity or brain disorders. Each tractogram is\ncomposed of millions of fibers encoded as 3D polylines. Unfortunately, a large\nportion of those fibers are not anatomically plausible and can be considered\nartifacts of the tracking algorithms. Common methods for tractogram filtering\nare based on signal reconstruction, a principled approach, but unable to\nconsider the knowledge of brain anatomy. In this work, we address the problem\nof tractogram filtering as a supervised learning problem by exploiting the\nground truth annotations obtained with a recent heuristic method, which labels\nfibers as either anatomically plausible or non-plausible according to\nwell-established anatomical properties. The intuitive idea is to model a fiber\nas a point cloud and the goal is to investigate whether and how a geometric\ndeep learning model might capture its anatomical properties. Our contribution\nis an extension of the Dynamic Edge Convolution model that exploits the\nsequential relations of points in a fiber and discriminates with high accuracy\nplausible/non-plausible fibers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:56:44 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 13:57:11 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Astolfi", "Pietro", ""], ["Verhagen", "Ruben", ""], ["Petit", "Laurent", ""], ["Olivetti", "Emanuele", ""], ["Masci", "Jonathan", ""], ["Boscaini", "Davide", ""], ["Avesani", "Paolo", ""]]}, {"id": "2003.11014", "submitter": "Goutam Bhat", "authors": "Goutam Bhat, Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "Know Your Surroundings: Exploiting Scene Information for Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art trackers only rely on a target appearance model in\norder to localize the object in each frame. Such approaches are however prone\nto fail in case of e.g. fast appearance changes or presence of distractor\nobjects, where a target appearance model alone is insufficient for robust\ntracking. Having the knowledge about the presence and locations of other\nobjects in the surrounding scene can be highly beneficial in such cases. This\nscene information can be propagated through the sequence and used to, for\ninstance, explicitly avoid distractor objects and eliminate target candidate\nregions.\n  In this work, we propose a novel tracking architecture which can utilize\nscene information for tracking. Our tracker represents such information as\ndense localized state vectors, which can encode, for example, if the local\nregion is target, background, or distractor. These state vectors are propagated\nthrough the sequence and combined with the appearance model output to localize\nthe target. Our network is learned to effectively utilize the scene information\nby directly maximizing tracking performance on video segments. The proposed\napproach sets a new state-of-the-art on 3 tracking benchmarks, achieving an AO\nscore of 63.6% on the recent GOT-10k dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 17:59:04 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:15:51 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bhat", "Goutam", ""], ["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.11038", "submitter": "Sunnie S. Y. Kim", "authors": "Sunnie S. Y. Kim, Nicholas Kolkin, Jason Salavon, Gregory\n  Shakhnarovich", "title": "Deformable Style Transfer", "comments": "ECCV 2020 (21 pages, 11 figures including the supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both geometry and texture are fundamental aspects of visual style. Existing\nstyle transfer methods, however, primarily focus on texture, almost entirely\nignoring geometry. We propose deformable style transfer (DST), an\noptimization-based approach that jointly stylizes the texture and geometry of a\ncontent image to better match a style image. Unlike previous geometry-aware\nstylization methods, our approach is neither restricted to a particular domain\n(such as human faces), nor does it require training sets of matching\nstyle/content pairs. We demonstrate our method on a diverse set of content and\nstyle images including portraits, animals, objects, scenes, and paintings. Code\nhas been made publicly available at https://github.com/sunniesuhyoung/DST.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 18:00:18 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 02:50:28 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Kim", "Sunnie S. Y.", ""], ["Kolkin", "Nicholas", ""], ["Salavon", "Jason", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "2003.11055", "submitter": "Mohamed Karar", "authors": "Ezz El-Din Hemdan, Marwa A. Shouman and Mohamed Esmail Karar", "title": "COVIDX-Net: A Framework of Deep Learning Classifiers to Diagnose\n  COVID-19 in X-Ray Images", "comments": "14 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background and Purpose: Coronaviruses (CoV) are perilous viruses that may\ncause Severe Acute Respiratory Syndrome (SARS-CoV), Middle East Respiratory\nSyndrome (MERS-CoV). The novel 2019 Coronavirus disease (COVID-19) was\ndiscovered as a novel disease pneumonia in the city of Wuhan, China at the end\nof 2019. Now, it becomes a Coronavirus outbreak around the world, the number of\ninfected people and deaths are increasing rapidly every day according to the\nupdated reports of the World Health Organization (WHO). Therefore, the aim of\nthis article is to introduce a new deep learning framework; namely COVIDX-Net\nto assist radiologists to automatically diagnose COVID-19 in X-ray images.\nMaterials and Methods: Due to the lack of public COVID-19 datasets, the study\nis validated on 50 Chest X-ray images with 25 confirmed positive COVID-19\ncases. The COVIDX-Net includes seven different architectures of deep\nconvolutional neural network models, such as modified Visual Geometry Group\nNetwork (VGG19) and the second version of Google MobileNet. Each deep neural\nnetwork model is able to analyze the normalized intensities of the X-ray image\nto classify the patient status either negative or positive COVID-19 case.\nResults: Experiments and evaluation of the COVIDX-Net have been successfully\ndone based on 80-20% of X-ray images for the model training and testing phases,\nrespectively. The VGG19 and Dense Convolutional Network (DenseNet) models\nshowed a good and similar performance of automated COVID-19 classification with\nf1-scores of 0.89 and 0.91 for normal and COVID-19, respectively. Conclusions:\nThis study demonstrated the useful application of deep learning models to\nclassify COVID-19 in X-ray images based on the proposed COVIDX-Net framework.\nClinical studies are the next milestone of this research work.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 18:21:10 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Hemdan", "Ezz El-Din", ""], ["Shouman", "Marwa A.", ""], ["Karar", "Mohamed Esmail", ""]]}, {"id": "2003.11064", "submitter": "Charles Nicklas Christensen M.Sc.", "authors": "Charles N. Christensen, Edward N. Ward, Pietro Lio, Clemens F.\n  Kaminski", "title": "ML-SIM: A deep neural network for reconstruction of structured\n  illumination microscopy images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured illumination microscopy (SIM) has become an important technique\nfor optical super-resolution imaging because it allows a doubling of image\nresolution at speeds compatible for live-cell imaging. However, the\nreconstruction of SIM images is often slow and prone to artefacts. Here we\npropose a versatile reconstruction method, ML-SIM, which makes use of machine\nlearning. The model is an end-to-end deep residual neural network that is\ntrained on a simulated data set to be free of common SIM artefacts. ML-SIM is\nthus robust to noise and irregularities in the illumination patterns of the raw\nSIM input frames. The reconstruction method is widely applicable and does not\nrequire the acquisition of experimental training data. Since the training data\nare generated from simulations of the SIM process on images from generic\nlibraries the method can be efficiently adapted to specific experimental SIM\nimplementations. The reconstruction quality enabled by our method is compared\nwith traditional SIM reconstruction methods, and we demonstrate advantages in\nterms of noise, reconstruction fidelity and contrast for both simulated and\nexperimental inputs. In addition, reconstruction of one SIM frame typically\nonly takes ~100ms to perform on PCs with modern Nvidia graphics cards, making\nthe technique compatible with real-time imaging. The full implementation and\nthe trained networks are available at http://ML-SIM.com.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 18:42:23 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Christensen", "Charles N.", ""], ["Ward", "Edward N.", ""], ["Lio", "Pietro", ""], ["Kaminski", "Clemens F.", ""]]}, {"id": "2003.11066", "submitter": "Abhinav Goel", "authors": "Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, and George K. Thiruvathukal", "title": "A Survey of Methods for Low-Power Deep Learning and Computer Vision", "comments": "Accepted for publication at 2020 IEEE 6th World Forum on Internet of\n  Things (WF-IoT), New Orleans, LA, USA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are successful in many computer vision tasks.\nHowever, the most accurate DNNs require millions of parameters and operations,\nmaking them energy, computation and memory intensive. This impedes the\ndeployment of large DNNs in low-power devices with limited compute resources.\nRecent research improves DNN models by reducing the memory requirement, energy\nconsumption, and number of operations without significantly decreasing the\naccuracy. This paper surveys the progress of low-power deep learning and\ncomputer vision, specifically in regards to inference, and discusses the\nmethods for compacting and accelerating DNN models. The techniques can be\ndivided into four major categories: (1) parameter quantization and pruning, (2)\ncompressed convolutional filters and matrix factorization, (3) network\narchitecture search, and (4) knowledge distillation. We analyze the accuracy,\nadvantages, disadvantages, and potential solutions to the problems with the\ntechniques in each category. We also discuss new evaluation metrics as a\nguideline for future research.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 18:47:24 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Goel", "Abhinav", ""], ["Tung", "Caleb", ""], ["Lu", "Yung-Hsiang", ""], ["Thiruvathukal", "George K.", ""]]}, {"id": "2003.11076", "submitter": "Pushyami Kaveti", "authors": "Pushyami Kaveti, Sammie Katt, Hanumant Singh", "title": "Removing Dynamic Objects for Static Scene Reconstruction using Light\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a general expectation that robots should operate in environments\nthat consist of static and dynamic entities including people, furniture and\nautomobiles. These dynamic environments pose challenges to visual simultaneous\nlocalization and mapping (SLAM) algorithms by introducing errors into the\nfront-end. Light fields provide one possible method for addressing such\nproblems by capturing a more complete visual information of a scene. In\ncontrast to a single ray from a perspective camera, Light Fields capture a\nbundle of light rays emerging from a single point in space, allowing us to see\nthrough dynamic objects by refocusing past them.\n  In this paper we present a method to synthesize a refocused image of the\nstatic background in the presence of dynamic objects that uses a light-field\nacquired with a linear camera array. We simultaneously estimate both the depth\nand the refocused image of the static scene using semantic segmentation for\ndetecting dynamic objects in a single time step. This eliminates the need for\ninitializing a static map . The algorithm is parallelizable and is implemented\non GPU allowing us execute it at close to real time speeds. We demonstrate the\neffectiveness of our method on real-world data acquired using a small robot\nwith a five camera array.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:05:17 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Kaveti", "Pushyami", ""], ["Katt", "Sammie", ""], ["Singh", "Hanumant", ""]]}, {"id": "2003.11087", "submitter": "Tomas Wilkinson", "authors": "Tomas Wilkinson and Carl Nettelblad", "title": "Bootstrapping Weakly Supervised Segmentation-free Word Spotting through\n  HMM-based Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in word spotting in handwritten documents has yielded impressive\nresults. This progress has largely been made by supervised learning systems,\nwhich are dependent on manually annotated data, making deployment to new\ncollections a significant effort. In this paper, we propose an approach that\nutilises transcripts without bounding box annotations to train\nsegmentation-free query-by-string word spotting models, given a partially\ntrained model. This is done through a training-free alignment procedure based\non hidden Markov models. This procedure creates a tentative mapping between\nword region proposals and the transcriptions to automatically create additional\nweakly annotated training data, without choosing any single alignment\npossibility as the correct one. When only using between 1% and 7% of the fully\nannotated training sets for partial convergence, we automatically annotate the\nremaining training data and successfully train using it. On all our datasets,\nour final trained model then comes within a few mAP% of the performance from a\nmodel trained with the full training set used as ground truth. We believe that\nthis will be a significant advance towards a more general use of word spotting,\nsince digital transcription data will already exist for parts of many\ncollections of interest.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:41:18 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wilkinson", "Tomas", ""], ["Nettelblad", "Carl", ""]]}, {"id": "2003.11089", "submitter": "Wei Chen", "authors": "Wei Chen, Xi Jia, Hyung Jin Chang, Jinming Duan, Ales Leonardis", "title": "G2L-Net: Global to Local Network for Real-time 6D Pose Estimation with\n  Embedding Vector Features", "comments": "10 pages, 11 figures, accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel real-time 6D object pose estimation\nframework, named G2L-Net. Our network operates on point clouds from RGB-D\ndetection in a divide-and-conquer fashion. Specifically, our network consists\nof three steps. First, we extract the coarse object point cloud from the RGB-D\nimage by 2D detection. Second, we feed the coarse object point cloud to a\ntranslation localization network to perform 3D segmentation and object\ntranslation prediction. Third, via the predicted segmentation and translation,\nwe transfer the fine object point cloud into a local canonical coordinate, in\nwhich we train a rotation localization network to estimate initial object\nrotation. In the third step, we define point-wise embedding vector features to\ncapture viewpoint-aware information. To calculate more accurate rotation, we\nadopt a rotation residual estimator to estimate the residual between initial\nrotation and ground truth, which can boost initial pose estimation performance.\nOur proposed G2L-Net is real-time despite the fact multiple steps are stacked\nvia the proposed coarse-to-fine framework. Extensive experiments on two\nbenchmark datasets show that G2L-Net achieves state-of-the-art performance in\nterms of both accuracy and speed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 19:42:24 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 08:36:23 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chen", "Wei", ""], ["Jia", "Xi", ""], ["Chang", "Hyung Jin", ""], ["Duan", "Jinming", ""], ["Leonardis", "Ales", ""]]}, {"id": "2003.11100", "submitter": "Helard Becerra Martinez Dr", "authors": "Helard Martinez and Andrew Hines and Mylene C. Q. Farias", "title": "How deep is your encoder: an analysis of features descriptors for an\n  autoencoder-based audio-visual quality metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of audio-visual quality assessment models poses a number of\nchallenges in order to obtain accurate predictions. One of these challenges is\nthe modelling of the complex interaction that audio and visual stimuli have and\nhow this interaction is interpreted by human users. The No-Reference\nAudio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with\nthis problem from a machine learning perspective. The metric receives two sets\nof audio and video features descriptors and produces a low-dimensional set of\nfeatures used to predict the audio-visual quality. A basic implementation of\nNAViDAd was able to produce accurate predictions tested with a range of\ndifferent audio-visual databases. The current work performs an ablation study\non the base architecture of the metric. Several modules are removed or\nre-trained using different configurations to have a better understanding of the\nmetric functionality. The results presented in this study provided important\nfeedback that allows us to understand the real capacity of the metric's\narchitecture and eventually develop a much better audio-visual quality metric.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 20:15:12 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Martinez", "Helard", ""], ["Hines", "Andrew", ""], ["Farias", "Mylene C. Q.", ""]]}, {"id": "2003.11110", "submitter": "Junfeng Guo", "authors": "Junfeng Guo, Ting Wang, Cong Liu", "title": "PoisHygiene: Detecting and Mitigating Poisoning Attacks in Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The black-box nature of deep neural networks (DNNs) facilitates attackers to\nmanipulate the behavior of DNN through data poisoning. Being able to detect and\nmitigate poisoning attacks, typically categorized into backdoor and adversarial\npoisoning (AP), is critical in enabling safe adoption of DNNs in many\napplication domains. Although recent works demonstrate encouraging results on\ndetection of certain backdoor attacks, they exhibit inherent limitations which\nmay significantly constrain the applicability. Indeed, no technique exists for\ndetecting AP attacks, which represents a harder challenge given that such\nattacks exhibit no common and explicit rules while backdoor attacks do (i.e.,\nembedding backdoor triggers into poisoned data). We believe the key to detect\nand mitigate AP attacks is the capability of observing and leveraging essential\npoisoning-induced properties within an infected DNN model. In this paper, we\npresent PoisHygiene, the first effective and robust detection and mitigation\nframework against AP attacks. PoisHygiene is fundamentally motivated by Dr.\nErnest Rutherford's story (i.e., the 1908 Nobel Prize winner), on observing the\nstructure of atom through random electron sampling.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 20:55:08 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 20:03:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Guo", "Junfeng", ""], ["Wang", "Ting", ""], ["Liu", "Cong", ""]]}, {"id": "2003.11113", "submitter": "Karsten Roth", "authors": "Karsten Roth, Timo Milbich, Bj\\\"orn Ommer", "title": "PADS: Policy-Adapted Sampling for Visual Similarity Learning", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning visual similarity requires to learn relations, typically between\ntriplets of images. Albeit triplet approaches being powerful, their\ncomputational complexity mostly limits training to only a subset of all\npossible training triplets. Thus, sampling strategies that decide when to use\nwhich training sample during learning are crucial. Currently, the prominent\nparadigm are fixed or curriculum sampling strategies that are predefined before\ntraining starts. However, the problem truly calls for a sampling process that\nadjusts based on the actual state of the similarity representation during\ntraining. We, therefore, employ reinforcement learning and have a teacher\nnetwork adjust the sampling distribution based on the current state of the\nlearner network, which represents visual similarity. Experiments on benchmark\ndatasets using standard triplet-based losses show that our adaptive sampling\nstrategy significantly outperforms fixed sampling strategies. Moreover,\nalthough our adaptive sampling is only applied on top of basic triplet-learning\nframeworks, we reach competitive results to state-of-the-art approaches that\nemploy diverse additional learning signals or strong ensemble architectures.\nCode can be found under https://github.com/Confusezius/CVPR2020_PADS.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 21:01:07 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 12:56:16 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Roth", "Karsten", ""], ["Milbich", "Timo", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2003.11136", "submitter": "Dung Nguyen", "authors": "Dung Nguyen, Sridha Sridharan, Duc Thanh Nguyen, Simon Denman, Son N.\n  Tran, Rui Zeng, and Clinton Fookes", "title": "Joint Deep Cross-Domain Transfer Learning for Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been applied to achieve significant progress in emotion\nrecognition. Despite such substantial progress, existing approaches are still\nhindered by insufficient training data, and the resulting models do not\ngeneralize well under mismatched conditions. To address this challenge, we\npropose a learning strategy which jointly transfers the knowledge learned from\nrich datasets to source-poor datasets. Our method is also able to learn\ncross-domain features which lead to improved recognition performance. To\ndemonstrate the robustness of our proposed framework, we conducted experiments\non three benchmark emotion datasets including eNTERFACE, SAVEE, and EMODB.\nExperimental results show that the proposed method surpassed state-of-the-art\ntransfer learning schemes by a significant margin.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 22:30:42 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Nguyen", "Dung", ""], ["Sridharan", "Sridha", ""], ["Nguyen", "Duc Thanh", ""], ["Denman", "Simon", ""], ["Tran", "Son N.", ""], ["Zeng", "Rui", ""], ["Fookes", "Clinton", ""]]}, {"id": "2003.11142", "submitter": "Jiahui Yu", "authors": "Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan\n  Kindermans, Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, Quoc Le", "title": "BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage\n  Models", "comments": "Accepted in ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has shown promising results discovering\nmodels that are both accurate and fast. For NAS, training a one-shot model has\nbecome a popular strategy to rank the relative quality of different\narchitectures (child models) using a single set of shared weights. However,\nwhile one-shot model weights can effectively rank different network\narchitectures, the absolute accuracies from these shared weights are typically\nfar below those obtained from stand-alone training. To compensate, existing\nmethods assume that the weights must be retrained, finetuned, or otherwise\npost-processed after the search is completed. These steps significantly\nincrease the compute requirements and complexity of the architecture search and\nmodel deployment. In this work, we propose BigNAS, an approach that challenges\nthe conventional wisdom that post-processing of the weights is necessary to get\ngood prediction accuracies. Without extra retraining or post-processing steps,\nwe are able to train a single set of shared weights on ImageNet and use these\nweights to obtain child models whose sizes range from 200 to 1000 MFLOPs. Our\ndiscovered model family, BigNASModels, achieve top-1 accuracies ranging from\n76.5% to 80.9%, surpassing state-of-the-art models in this range including\nEfficientNets and Once-for-All networks without extra retraining or\npost-processing. We present ablative study and analysis to further understand\nthe proposed BigNASModels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 23:00:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 03:48:01 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 02:00:22 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yu", "Jiahui", ""], ["Jin", "Pengchong", ""], ["Liu", "Hanxiao", ""], ["Bender", "Gabriel", ""], ["Kindermans", "Pieter-Jan", ""], ["Tan", "Mingxing", ""], ["Huang", "Thomas", ""], ["Song", "Xiaodan", ""], ["Pang", "Ruoming", ""], ["Le", "Quoc", ""]]}, {"id": "2003.11145", "submitter": "Sunpreet Singh Arora", "authors": "Dinh-Luan Nguyen and Sunpreet S. Arora and Yuhang Wu and Hao Yang", "title": "Adversarial Light Projection Attacks on Face Recognition Systems: A\n  Feasibility Study", "comments": "To appear in the proceedings of the IEEE Computer Vision and Pattern\n  Recognition (CVPR) Biometrics Workshop 2020 - 9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based systems have been shown to be vulnerable to adversarial\nattacks in both digital and physical domains. While feasible, digital attacks\nhave limited applicability in attacking deployed systems, including face\nrecognition systems, where an adversary typically has access to the input and\nnot the transmission channel. In such setting, physical attacks that directly\nprovide a malicious input through the input channel pose a bigger threat. We\ninvestigate the feasibility of conducting real-time physical attacks on face\nrecognition systems using adversarial light projections. A setup comprising a\ncommercially available web camera and a projector is used to conduct the\nattack. The adversary uses a transformation-invariant adversarial pattern\ngeneration method to generate a digital adversarial pattern using one or more\nimages of the target available to the adversary. The digital adversarial\npattern is then projected onto the adversary's face in the physical domain to\neither impersonate a target (impersonation) or evade recognition (obfuscation).\nWe conduct preliminary experiments using two open-source and one commercial\nface recognition system on a pool of 50 subjects. Our experimental results\ndemonstrate the vulnerability of face recognition systems to light projection\nattacks in both white-box and black-box attack settings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 23:06:25 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 00:44:07 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Nguyen", "Dinh-Luan", ""], ["Arora", "Sunpreet S.", ""], ["Wu", "Yuhang", ""], ["Yang", "Hao", ""]]}, {"id": "2003.11154", "submitter": "Saeed Anwar", "authors": "Saeed Anwar, Nick Barnes and Lars Petersson", "title": "A Systematic Evaluation: Fine-Grained CNN vs. Traditional CNN\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To make the best use of the underlying minute and subtle differences,\nfine-grained classifiers collect information about inter-class variations. The\ntask is very challenging due to the small differences between the colors,\nviewpoint, and structure in the same class entities. The classification becomes\nmore difficult due to the similarities between the differences in viewpoint\nwith other classes and differences with its own. In this work, we investigate\nthe performance of the landmark general CNN classifiers, which presented\ntop-notch results on large scale classification datasets, on the fine-grained\ndatasets, and compare it against state-of-the-art fine-grained classifiers. In\nthis paper, we pose two specific questions: (i) Do the general CNN classifiers\nachieve comparable results to fine-grained classifiers? (ii) Do general CNN\nclassifiers require any specific information to improve upon the fine-grained\nones? Throughout this work, we train the general CNN classifiers without\nintroducing any aspect that is specific to fine-grained datasets. We show an\nextensive evaluation on six datasets to determine whether the fine-grained\nclassifier is able to elevate the baseline in their experiments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 23:49:14 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 05:07:40 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Anwar", "Saeed", ""], ["Barnes", "Nick", ""], ["Petersson", "Lars", ""]]}, {"id": "2003.11163", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Chunyu Wang, Wenhu Qin, Wenjun Zeng", "title": "Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A\n  Geometric Approach", "comments": "Accepted by CVPR 2020. Code is released at\n  https://github.com/CHUNYUWANG/imu-human-pose-pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to estimate 3D human pose from multi-view images and a few IMUs\nattached at person's limbs. It operates by firstly detecting 2D poses from the\ntwo signals, and then lifting them to the 3D space. We present a geometric\napproach to reinforce the visual features of each pair of joints based on the\nIMUs. This notably improves 2D pose estimation accuracy especially when one\njoint is occluded. We call this approach Orientation Regularized Network (ORN).\nThen we lift the multi-view 2D poses to the 3D space by an Orientation\nRegularized Pictorial Structure Model (ORPSM) which jointly minimizes the\nprojection error between the 3D and 2D poses, along with the discrepancy\nbetween the 3D pose and IMU orientations. The simple two-step approach reduces\nthe error of the state-of-the-art by a large margin on a public dataset. Our\ncode will be released at https://github.com/CHUNYUWANG/imu-human-pose-pytorch.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 00:26:54 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 05:48:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Zhang", "Zhe", ""], ["Wang", "Chunyu", ""], ["Qin", "Wenhu", ""], ["Zeng", "Wenjun", ""]]}, {"id": "2003.11172", "submitter": "Puneet Kohli", "authors": "Yiwen Hua, Puneet Kohli, Pritish Uplavikar, Anand Ravi, Saravana\n  Gunaseelan, Jason Orozco, and Edward Li", "title": "Holopix50k: A Large-Scale In-the-wild Stereo Image Dataset", "comments": "Main paper: 17 pages, 7 figures, 3 tables. Supplementary: 11 pages, 7\n  figures, 4 tables. See http://github.com/leiainc/holopix50k for downloading\n  the dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the mass-market adoption of dual-camera mobile phones, leveraging stereo\ninformation in computer vision has become increasingly important. Current\nstate-of-the-art methods utilize learning-based algorithms, where the amount\nand quality of training samples heavily influence results. Existing stereo\nimage datasets are limited either in size or subject variety. Hence, algorithms\ntrained on such datasets do not generalize well to scenarios encountered in\nmobile photography. We present Holopix50k, a novel in-the-wild stereo image\ndataset, comprising 49,368 image pairs contributed by users of the Holopix\nmobile social platform. In this work, we describe our data collection process\nand statistically compare our dataset to other popular stereo datasets. We\nexperimentally show that using our dataset significantly improves results for\ntasks such as stereo super-resolution and self-supervised monocular depth\nestimation. Finally, we showcase practical applications of our dataset to\nmotivate novel works and use cases. The Holopix50k dataset is available at\nhttp://github.com/leiainc/holopix50k\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 01:13:04 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Hua", "Yiwen", ""], ["Kohli", "Puneet", ""], ["Uplavikar", "Pritish", ""], ["Ravi", "Anand", ""], ["Gunaseelan", "Saravana", ""], ["Orozco", "Jason", ""], ["Li", "Edward", ""]]}, {"id": "2003.11177", "submitter": "Saeed Izadi", "authors": "Saeed Izadi, Ghassan Hamarneh", "title": "Patch-based Non-Local Bayesian Networks for Blind Confocal Microscopy\n  Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confocal microscopy is essential for histopathologic cell visualization and\nquantification. Despite its significant role in biology, fluorescence confocal\nmicroscopy suffers from the presence of inherent noise during image\nacquisition. Non-local patch-wise Bayesian mean filtering (NLB) was until\nrecently the state-of-the-art denoising approach. However, classic denoising\nmethods have been outperformed by neural networks in recent years. In this\nwork, we propose to exploit the strengths of NLB in the framework of Bayesian\ndeep learning. We do so by designing a convolutional neural network and\ntraining it to learn parameters of a Gaussian model approximating the prior on\nnoise-free patches given their nearest, similar yet non-local, neighbors. We\nthen apply Bayesian reasoning to leverage the prior and information from the\nnoisy patch in the process of approximating the noise-free patch. Specifically,\nwe use the closed-form analytic \\textit{maximum a posteriori} (MAP) estimate in\nthe NLB algorithm to obtain the noise-free patch that maximizes the posterior\ndistribution. The performance of our proposed method is evaluated on confocal\nmicroscopy images with real noise Poisson-Gaussian noise. Our experiments\nreveal the superiority of our approach against state-of-the-art unsupervised\ndenoising techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 01:49:58 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 23:36:22 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Izadi", "Saeed", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "2003.11192", "submitter": "Ankit Vora", "authors": "Ankit Vora, Siddharth Agarwal, Gaurav Pandey and James McBride", "title": "Aerial Imagery based LIDAR Localization for Autonomous Vehicles", "comments": "6 pages, 7 figures, Submitted to International Conference on\n  Intelligent Robots and Systems (IROS-2020), For the video, see\n  https://www.youtube.com/watch?v=vcY74Z9bOLk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a localization technique using aerial imagery maps and\nLIDAR based ground reflectivity for autonomous vehicles in urban environments.\nTraditional localization techniques using LIDAR reflectivity rely on high\ndefinition reflectivity maps generated from a mapping vehicle. The cost and\neffort required to maintain such prior maps are generally very high because it\nrequires a fleet of expensive mapping vehicles. In this work we propose a\nlocalization technique where the vehicle localizes using aerial/satellite\nimagery, eradicating the need to develop and maintain complex high-definition\nmaps. The proposed technique has been tested on a real world dataset collected\nfrom a test track in Ann Arbor, Michigan. This research concludes that aerial\nimagery based maps provides real-time localization performance similar to\nstate-of-the-art LIDAR based maps for autonomous vehicles in urban environments\nat reduced costs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 02:52:05 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Vora", "Ankit", ""], ["Agarwal", "Siddharth", ""], ["Pandey", "Gaurav", ""], ["McBride", "James", ""]]}, {"id": "2003.11209", "submitter": "Ya Zhou", "authors": "Ya Zhou, Jianfeng Xu, Kazuyuki Tasaka, Zhibo Chen, Weiping Li", "title": "Prior-enlightened and Motion-robust Video Deblurring", "comments": "26 pages, 13 figures, and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various blur distortions in video will cause negative impact on both human\nviewing and video-based applications, which makes motion-robust deblurring\nmethods urgently needed. Most existing works have strong dataset dependency and\nlimited generalization ability in handling challenging scenarios, like blur in\nlow contrast or severe motion areas, and non-uniform blur. Therefore, we\npropose a PRiOr-enlightened and MOTION-robust video deblurring model\n(PROMOTION) suitable for challenging blurs. On the one hand, we use 3D group\nconvolution to efficiently encode heterogeneous prior information, explicitly\nenhancing the scenes' perception while mitigating the output's artifacts. On\nthe other hand, we design the priors representing blur distribution, to better\nhandle non-uniform blur in spatio-temporal domain. Besides the classical camera\nshake caused global blurry, we also prove the generalization for the downstream\ntask suffering from local blur. Extensive experiments demonstrate we can\nachieve the state-of-the-art performance on well-known REDS and GoPro datasets,\nand bring machine task gain.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:16:56 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 02:30:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhou", "Ya", ""], ["Xu", "Jianfeng", ""], ["Tasaka", "Kazuyuki", ""], ["Chen", "Zhibo", ""], ["Li", "Weiping", ""]]}, {"id": "2003.11211", "submitter": "Shuhei Yokoo", "authors": "Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi Iizuka", "title": "Two-stage Discriminative Re-ranking for Large-scale Landmark Retrieval", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient pipeline for large-scale landmark image retrieval\nthat addresses the diversity of the dataset through two-stage discriminative\nre-ranking. Our approach is based on embedding the images in a feature-space\nusing a convolutional neural network trained with a cosine softmax loss. Due to\nthe variance of the images, which include extreme viewpoint changes such as\nhaving to retrieve images of the exterior of a landmark from images of the\ninterior, this is very challenging for approaches based exclusively on visual\nsimilarity. Our proposed re-ranking approach improves the results in two steps:\nin the sort-step, $k$-nearest neighbor search with soft-voting to sort the\nretrieved results based on their label similarity to the query images, and in\nthe insert-step, we add additional samples from the dataset that were not\nretrieved by image-similarity. This approach allows overcoming the low visual\ndiversity in retrieved images. In-depth experimental results show that the\nproposed approach significantly outperforms existing approaches on the\nchallenging Google Landmarks Datasets. Using our methods, we achieved 1st place\nin the Google Landmark Retrieval 2019 challenge and 3rd place in the Google\nLandmark Recognition 2019 challenge on Kaggle. Our code is publicly available\nhere: \\url{https://github.com/lyakaap/Landmark2019-1st-and-3rd-Place-Solution}\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:23:18 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Yokoo", "Shuhei", ""], ["Ozaki", "Kohei", ""], ["Simo-Serra", "Edgar", ""], ["Iizuka", "Satoshi", ""]]}, {"id": "2003.11213", "submitter": "Hongfeng You", "authors": "Hongfeng You, Shengwei Tian, Long Yu, Xiang Ma, Yan Xing and Ning Xin", "title": "A New Multiple Max-pooling Integration Module and Cross Multiscale\n  Deconvolution Network Based on Image Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better retain the deep features of an image and solve the sparsity problem\nof the end-to-end segmentation model, we propose a new deep convolutional\nnetwork model for medical image pixel segmentation, called MC-Net. The core of\nthis network model consists of four parts, namely, an encoder network, a\nmultiple max-pooling integration module, a cross multiscale deconvolution\ndecoder network and a pixel-level classification layer. In the network\nstructure of the encoder, we use multiscale convolution instead of the\ntraditional single-channel convolution. The multiple max-pooling integration\nmodule first integrates the output features of each submodule of the encoder\nnetwork and reduces the number of parameters by convolution using a kernel size\nof 1. At the same time, each max-pooling layer (the pooling size of each layer\nis different) is spliced after each convolution to achieve the translation\ninvariance of the feature maps of each submodule. We use the output feature\nmaps from the multiple max-pooling integration module as the input of the\ndecoder network; the multiscale convolution of each submodule in the decoder\nnetwork is cross-fused with the feature maps generated by the corresponding\nmultiscale convolution in the encoder network. Using the above feature map\nprocessing methods solves the sparsity problem after the max-pooling\nlayer-generating matrix and enhances the robustness of the classification. We\ncompare our proposed model with the well-known Fully Convolutional Networks for\nSemantic Segmentation (FCNs), DecovNet, PSPNet, U-net, SgeNet and other\nstate-of-the-art segmentation networks such as HyperDenseNet, MS-Dual,\nEspnetv2, Denseaspp using one binary Kaggle 2018 data science bowl dataset and\ntwo multiclass dataset and obtain encouraging experimental results.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:27:01 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["You", "Hongfeng", ""], ["Tian", "Shengwei", ""], ["Yu", "Long", ""], ["Ma", "Xiang", ""], ["Xing", "Yan", ""], ["Xin", "Ning", ""]]}, {"id": "2003.11228", "submitter": "Bin Zhang", "authors": "Bin Zhang, Jian Li, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li,\n  Feiyue Huang, Yili Xia, Wenjiang Pei, Rongrong Ji", "title": "ASFD: Automatic and Scalable Face Detector", "comments": "Ranked No.1 on WIDER Face\n  (http://shuoyang1213.me/WIDERFACE/WiderFace_Results.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Automatic and Scalable Face Detector\n(ASFD), which is based on a combination of neural architecture search\ntechniques as well as a new loss design. First, we propose an automatic feature\nenhance module named Auto-FEM by improved differential architecture search,\nwhich allows efficient multi-scale feature fusion and context enhancement.\nSecond, we use Distance-based Regression and Margin-based Classification (DRMC)\nmulti-task loss to predict accurate bounding boxes and learn highly\ndiscriminative deep features. Third, we adopt compound scaling methods and\nuniformly scale the backbone, feature modules, and head networks to develop a\nfamily of ASFD, which are consistently more efficient than the state-of-the-art\nface detectors. Extensive experiments conducted on popular benchmarks, e.g.\nWIDER FACE and FDDB, demonstrate that our ASFD-D6 outperforms the prior strong\ncompetitors, and our lightweight ASFD-D0 runs at more than 120 FPS with\nMobilenet for VGA-resolution images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 06:00:47 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 03:02:49 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 16:09:40 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhang", "Bin", ""], ["Li", "Jian", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Xia", "Yili", ""], ["Pei", "Wenjiang", ""], ["Ji", "Rongrong", ""]]}, {"id": "2003.11236", "submitter": "Shan You", "authors": "Shan You, Tao Huang, Mingmin Yang, Fei Wang, Chen Qian, Changshui\n  Zhang", "title": "GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a supernet matters for one-shot neural architecture search (NAS)\nmethods since it serves as a basic performance estimator for different\narchitectures (paths). Current methods mainly hold the assumption that a\nsupernet should give a reasonable ranking over all paths. They thus treat all\npaths equally, and spare much effort to train paths. However, it is harsh for a\nsingle supernet to evaluate accurately on such a huge-scale search space (e.g.,\n$7^{21}$). In this paper, instead of covering all paths, we ease the burden of\nsupernet by encouraging it to focus more on evaluation of those\npotentially-good ones, which are identified using a surrogate portion of\nvalidation data. Concretely, during training, we propose a multi-path sampling\nstrategy with rejection, and greedily filter the weak paths. The training\nefficiency is thus boosted since the training space has been greedily shrunk\nfrom all paths to those potentially-good ones. Moreover, we further adopt an\nexploration and exploitation policy by introducing an empirical candidate path\npool. Our proposed method GreedyNAS is easy-to-follow, and experimental results\non ImageNet dataset indicate that it can achieve better Top-1 accuracy under\nsame search space and FLOPs or latency level, but with only $\\sim$60\\% of\nsupernet training cost. By searching on a larger space, our GreedyNAS can also\nobtain new state-of-the-art architectures.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 06:54:10 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["You", "Shan", ""], ["Huang", "Tao", ""], ["Yang", "Mingmin", ""], ["Wang", "Fei", ""], ["Qian", "Chen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2003.11241", "submitter": "Qilong Wang", "authors": "Qilong Wang, Li Zhang, Banggu Wu, Dongwei Ren, Peihua Li, Wangmeng\n  Zuo, Qinghua Hu", "title": "What Deep CNNs Benefit from Global Covariance Pooling: An Optimization\n  Perspective", "comments": "Accepted to CVPR 2020; Project Page:\n  https://github.com/ZhangLi-CS/GCP_Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated that global covariance pooling (GCP) has the\nability to improve performance of deep convolutional neural networks (CNNs) on\nvisual classification task. Despite considerable advance, the reasons on\neffectiveness of GCP on deep CNNs have not been well studied. In this paper, we\nmake an attempt to understand what deep CNNs benefit from GCP in a viewpoint of\noptimization. Specifically, we explore the effect of GCP on deep CNNs in terms\nof the Lipschitzness of optimization loss and the predictiveness of gradients,\nand show that GCP can make the optimization landscape more smooth and the\ngradients more predictive. Furthermore, we discuss the connection between GCP\nand second-order optimization for deep CNNs. More importantly, above findings\ncan account for several merits of covariance pooling for training deep CNNs\nthat have not been recognized previously or fully explored, including\nsignificant acceleration of network convergence (i.e., the networks trained\nwith GCP can support rapid decay of learning rates, achieving favorable\nperformance while significantly reducing number of training epochs), stronger\nrobustness to distorted examples generated by image corruptions and\nperturbations, and good generalization ability to different vision tasks, e.g.,\nobject detection and instance segmentation. We conduct extensive experiments\nusing various deep CNN models on diversified tasks, and the results provide\nstrong support to our findings.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 07:00:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wang", "Qilong", ""], ["Zhang", "Li", ""], ["Wu", "Banggu", ""], ["Ren", "Dongwei", ""], ["Li", "Peihua", ""], ["Zuo", "Wangmeng", ""], ["Hu", "Qinghua", ""]]}, {"id": "2003.11242", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng", "title": "Safety-Aware Hardening of 3D Object Detection Neural Network Systems", "comments": "This version is similar to v1 with an added statement: \"The\n  evaluation using KITTI dataset in this paper is for knowledge dissemination\n  and scientific publication and is not for commercial use\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how state-of-the-art neural networks for 3D object detection using a\nsingle-stage pipeline can be made safety aware. We start with the safety\nspecification (reflecting the capability of other components) that partitions\nthe 3D input space by criticality, where the critical area employs a separate\ncriterion on robustness under perturbation, quality of bounding boxes, and the\ntolerance over false negatives demonstrated on the training set. In the\narchitecture design, we consider symbolic error propagation to allow\nfeature-level perturbation. Subsequently, we introduce a specialized loss\nfunction reflecting (1) the safety specification, (2) the use of single-stage\ndetection architecture, and finally, (3) the characterization of robustness\nunder perturbation. We also replace the commonly seen non-max-suppression\npost-processing algorithm by a safety-aware non-max-inclusion algorithm, in\norder to maintain the safety claim created by the neural network. The concept\nis detailed by extending the state-of-the-art PIXOR detector which creates\nobject bounding boxes in bird's eye view with inputs from point clouds.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 07:06:11 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 00:57:49 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 09:46:22 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Cheng", "Chih-Hong", ""]]}, {"id": "2003.11249", "submitter": "Jongwon Choi", "authors": "Jongwon Choi, Kwang Moo Yi, Jihoon Kim, Jinho Choo, Byoungjip Kim,\n  Jin-Yeop Chang, Youngjune Gwon, Hyung Jin Chang", "title": "VaB-AL: Incorporating Class Imbalance and Difficulty with Variational\n  Bayes for Active Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning for discriminative models has largely been studied with the\nfocus on individual samples, with less emphasis on how classes are distributed\nor which classes are hard to deal with. In this work, we show that this is\nharmful. We propose a method based on the Bayes' rule, that can naturally\nincorporate class imbalance into the Active Learning framework. We derive that\nthree terms should be considered together when estimating the probability of a\nclassifier making a mistake for a given sample; i) probability of mislabelling\na class, ii) likelihood of the data given a predicted class, and iii) the prior\nprobability on the abundance of a predicted class. Implementing these terms\nrequires a generative model and an intractable likelihood estimation.\nTherefore, we train a Variational Auto Encoder (VAE) for this purpose. To\nfurther tie the VAE with the classifier and facilitate VAE training, we use the\nclassifiers' deep feature representations as input to the VAE. By considering\nall three probabilities, among them especially the data imbalance, we can\nsubstantially improve the potential of existing methods under limited data\nbudget. We show that our method can be applied to classification tasks on\nmultiple different datasets -- including one that is a real-world dataset with\nheavy data imbalance -- significantly outperforming the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 07:34:06 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 12:18:11 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Choi", "Jongwon", ""], ["Yi", "Kwang Moo", ""], ["Kim", "Jihoon", ""], ["Choo", "Jinho", ""], ["Kim", "Byoungjip", ""], ["Chang", "Jin-Yeop", ""], ["Gwon", "Youngjune", ""], ["Chang", "Hyung Jin", ""]]}, {"id": "2003.11265", "submitter": "Ashkan Abbasi", "authors": "Ashkan Abbasi, Amirhassan Monadjemi, Leyuan Fang, Hossein Rabbani,\n  Neda Noormohammadi, Yi Zhang", "title": "Multiscale Sparsifying Transform Learning for Image Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-driven sparse methods such as synthesis dictionary learning (e.g.,\nK-SVD) and sparsifying transform learning have been proven effective in image\ndenoising. However, they are intrinsically single-scale which can lead to\nsuboptimal results. We propose two methods developed based on wavelet subbands\nmixing to efficiently combine the merits of both single and multiscale methods.\nWe show that an efficient multiscale method can be devised without the need for\ndenoising detail subbands which substantially reduces the runtime. The proposed\nmethods are initially derived within the framework of sparsifying transform\nlearning denoising, and then, they are generalized to propose our multiscale\nextensions for the well-known K-SVD and SAIST image denoising methods. We\nanalyze and assess the studied methods thoroughly and compare them with the\nwell-known and state-of-the-art methods. The experiments show that our methods\nare able to offer good trade-offs between performance and complexity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 08:13:16 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 10:24:21 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 09:25:40 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 07:25:02 GMT"}, {"version": "v5", "created": "Sun, 25 Jul 2021 18:16:20 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Abbasi", "Ashkan", ""], ["Monadjemi", "Amirhassan", ""], ["Fang", "Leyuan", ""], ["Rabbani", "Hossein", ""], ["Noormohammadi", "Neda", ""], ["Zhang", "Yi", ""]]}, {"id": "2003.11282", "submitter": "Guo Lu", "authors": "Guo Lu, Chunlei Cai, Xiaoyun Zhang, Li Chen, Wanli Ouyang, Dong Xu,\n  Zhiyong Gao", "title": "Content Adaptive and Error Propagation Aware Deep Video Compression", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, learning based video compression methods attract increasing\nattention. However, the previous works suffer from error propagation due to the\naccumulation of reconstructed error in inter predictive coding. Meanwhile, the\nprevious learning based video codecs are also not adaptive to different video\ncontents. To address these two problems, we propose a content adaptive and\nerror propagation aware video compression system. Specifically, our method\nemploys a joint training strategy by considering the compression performance of\nmultiple consecutive frames instead of a single frame. Based on the learned\nlong-term temporal information, our approach effectively alleviates error\npropagation in reconstructed frames. More importantly, instead of using the\nhand-crafted coding modes in the traditional compression systems, we design an\nonline encoder updating scheme in our system. The proposed approach updates the\nparameters for encoder according to the rate-distortion criterion but keeps the\ndecoder unchanged in the inference stage. Therefore, the encoder is adaptive to\ndifferent video contents and achieves better compression performance by\nreducing the domain gap between the training and testing datasets. Our method\nis simple yet effective and outperforms the state-of-the-art learning based\nvideo codecs on benchmark datasets without increasing the model size or\ndecreasing the decoding speed.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 09:04:24 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Lu", "Guo", ""], ["Cai", "Chunlei", ""], ["Zhang", "Xiaoyun", ""], ["Chen", "Li", ""], ["Ouyang", "Wanli", ""], ["Xu", "Dong", ""], ["Gao", "Zhiyong", ""]]}, {"id": "2003.11288", "submitter": "Roee Litman", "authors": "Ron Litman, Oron Anschel, Shahar Tsiper, Roee Litman, Shai Mazor and\n  R. Manmatha", "title": "SCATTER: Selective Context Attentional Scene Text Recognizer", "comments": "In CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene Text Recognition (STR), the task of recognizing text against complex\nimage backgrounds, is an active area of research. Current state-of-the-art\n(SOTA) methods still struggle to recognize text written in arbitrary shapes. In\nthis paper, we introduce a novel architecture for STR, named Selective Context\nATtentional Text Recognizer (SCATTER). SCATTER utilizes a stacked block\narchitecture with intermediate supervision during training, that paves the way\nto successfully train a deep BiLSTM encoder, thus improving the encoding of\ncontextual dependencies. Decoding is done using a two-step 1D attention\nmechanism. The first attention step re-weights visual features from a CNN\nbackbone together with contextual features computed by a BiLSTM layer. The\nsecond attention step, similar to previous papers, treats the features as a\nsequence and attends to the intra-sequence relationships. Experiments show that\nthe proposed approach surpasses SOTA performance on irregular text recognition\nbenchmarks by 3.7\\% on average.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 09:20:28 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Litman", "Ron", ""], ["Anschel", "Oron", ""], ["Tsiper", "Shahar", ""], ["Litman", "Roee", ""], ["Mazor", "Shai", ""], ["Manmatha", "R.", ""]]}, {"id": "2003.11291", "submitter": "Junbo Yin", "authors": "Junbo Yin, Wenguan Wang, Qinghao Meng, Ruigang Yang, Jianbing Shen", "title": "A Unified Object Motion and Affinity Model for Online Multi-Object\n  Tracking", "comments": "Accepted to CVPR 2020. Code: https://github.com/yinjunbo/UMA-MOT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current popular online multi-object tracking (MOT) solutions apply single\nobject trackers (SOTs) to capture object motions, while often requiring an\nextra affinity network to associate objects, especially for the occluded ones.\nThis brings extra computational overhead due to repetitive feature extraction\nfor SOT and affinity computation. Meanwhile, the model size of the\nsophisticated affinity network is usually non-trivial. In this paper, we\npropose a novel MOT framework that unifies object motion and affinity model\ninto a single network, named UMA, in order to learn a compact feature that is\ndiscriminative for both object motion and affinity measure. In particular, UMA\nintegrates single object tracking and metric learning into a unified triplet\nnetwork by means of multi-task learning. Such design brings advantages of\nimproved computation efficiency, low memory requirement and simplified training\nprocedure. In addition, we equip our model with a task-specific attention\nmodule, which is used to boost task-aware feature learning. The proposed UMA\ncan be easily trained end-to-end, and is elegant - requiring only one training\nstage. Experimental results show that it achieves promising performance on\nseveral MOT Challenge benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 09:36:43 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 03:08:41 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Yin", "Junbo", ""], ["Wang", "Wenguan", ""], ["Meng", "Qinghao", ""], ["Yang", "Ruigang", ""], ["Shen", "Jianbing", ""]]}, {"id": "2003.11303", "submitter": "Sunghun Joung", "authors": "Sunghun Joung, Seungryong Kim, Hanjae Kim, Minsu Kim, Ig-Jae Kim,\n  Junghyun Cho, Kwanghoon Sohn", "title": "Cylindrical Convolutional Networks for Joint Object Detection and\n  Viewpoint Estimation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing techniques to encode spatial invariance within deep convolutional\nneural networks only model 2D transformation fields. This does not account for\nthe fact that objects in a 2D space are a projection of 3D ones, and thus they\nhave limited ability to severe object viewpoint changes. To overcome this\nlimitation, we introduce a learnable module, cylindrical convolutional networks\n(CCNs), that exploit cylindrical representation of a convolutional kernel\ndefined in the 3D space. CCNs extract a view-specific feature through a\nview-specific convolutional kernel to predict object category scores at each\nviewpoint. With the view-specific feature, we simultaneously determine\nobjective category and viewpoints using the proposed sinusoidal soft-argmax\nmodule. Our experiments demonstrate the effectiveness of the cylindrical\nconvolutional networks on joint object detection and viewpoint estimation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:24:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Joung", "Sunghun", ""], ["Kim", "Seungryong", ""], ["Kim", "Hanjae", ""], ["Kim", "Minsu", ""], ["Kim", "Ig-Jae", ""], ["Cho", "Junghyun", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "2003.11315", "submitter": "Rixing Zhu", "authors": "Rixing Zhu, Jianwu Fang, Hongke Xu, Hongkai Yu, Jianru Xue", "title": "DCDLearn: Multi-order Deep Cross-distance Learning for Vehicle\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle re-identification (Re-ID) has become a popular research topic owing\nto its practicability in intelligent transportation systems. Vehicle Re-ID\nsuffers the numerous challenges caused by drastic variation in illumination,\nocclusions, background, resolutions, viewing angles, and so on. To address it,\nthis paper formulates a multi-order deep cross-distance learning\n(\\textbf{DCDLearn}) model for vehicle re-identification, where an efficient\none-view CycleGAN model is developed to alleviate exhaustive and enumerative\ncross-camera matching problem in previous works and smooth the domain\ndiscrepancy of cross cameras. Specially, we treat the transferred images and\nthe reconstructed images generated by one-view CycleGAN as multi-order\naugmented data for deep cross-distance learning, where the cross distances of\nmulti-order image set with distinct identities are learned by optimizing an\nobjective function with multi-order augmented triplet loss and center loss to\nachieve the camera-invariance and identity-consistency. Extensive experiments\non three vehicle Re-ID datasets demonstrate that the proposed method achieves\nsignificant improvement over the state-of-the-arts, especially for the small\nscale dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 10:46:54 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 04:34:22 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhu", "Rixing", ""], ["Fang", "Jianwu", ""], ["Xu", "Hongke", ""], ["Yu", "Hongkai", ""], ["Xue", "Jianru", ""]]}, {"id": "2003.11337", "submitter": "Zhong Gao", "authors": "Zhuoping Yu, Zhong Gao, Hansheng Chen, and Yuyao Huang", "title": "SPFCN: Select and Prune the Fully Convolutional Networks for Real-time\n  Parking Slot Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For vehicles equipped with the automatic parking system, the accuracy and\nspeed of the parking slot detection are crucial. But the high accuracy is\nobtained at the price of low speed or expensive computation equipment, which\nare sensitive for many car manufacturers. In this paper, we proposed a detector\nusing CNN(convolutional neural networks) for faster speed and smaller model\nsize while keeps accuracy. To achieve the optimal balance, we developed a\nstrategy to select the best receptive fields and prune the redundant channels\nautomatically after each training epoch. The proposed model is capable of\njointly detecting corners and line features of parking slots while running\nefficiently in real time on average processors. The model has a frame rate of\nabout 30 FPS on a 2.3 GHz CPU core, yielding parking slot corner localization\nerror of 1.51$\\pm$2.14 cm (std. err.) and slot detection accuracy of 98\\%,\ngenerally satisfying the requirements in both speed and accuracy on on-board\nmobile terminals.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 11:35:16 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 03:40:08 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Yu", "Zhuoping", ""], ["Gao", "Zhong", ""], ["Chen", "Hansheng", ""], ["Huang", "Yuyao", ""]]}, {"id": "2003.11339", "submitter": "Jie Chang", "authors": "Jie Chang, Zhonghao Lan, Changmao Cheng, Yichen Wei", "title": "Data Uncertainty Learning in Face Recognition", "comments": "Accepted as poster by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling data uncertainty is important for noisy images, but seldom explored\nfor face recognition. The pioneer work, PFE, considers uncertainty by modeling\neach face image embedding as a Gaussian distribution. It is quite effective.\nHowever, it uses fixed feature (mean of the Gaussian) from an existing model.\nIt only estimates the variance and relies on an ad-hoc and costly metric. Thus,\nit is not easy to use. It is unclear how uncertainty affects feature learning.\n  This work applies data uncertainty learning to face recognition, such that\nthe feature (mean) and uncertainty (variance) are learnt simultaneously, for\nthe first time. Two learning methods are proposed. They are easy to use and\noutperform existing deterministic methods as well as PFE on challenging\nunconstrained scenarios. We also provide insightful analysis on how\nincorporating uncertainty estimation helps reducing the adverse effects of\nnoisy samples and affects the feature learning.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 11:40:38 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Chang", "Jie", ""], ["Lan", "Zhonghao", ""], ["Cheng", "Changmao", ""], ["Wei", "Yichen", ""]]}, {"id": "2003.11342", "submitter": "Lingxi Xie", "authors": "Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, Qi Tian", "title": "Circumventing Outliers of AutoAugment with Knowledge Distillation", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AutoAugment has been a powerful algorithm that improves the accuracy of many\nvision tasks, yet it is sensitive to the operator space as well as\nhyper-parameters, and an improper setting may degenerate network optimization.\nThis paper delves deep into the working mechanism, and reveals that AutoAugment\nmay remove part of discriminative information from the training image and so\ninsisting on the ground-truth label is no longer the best option. To relieve\nthe inaccuracy of supervision, we make use of knowledge distillation that\nrefers to the output of a teacher model to guide network training. Experiments\nare performed in standard image classification benchmarks, and demonstrate the\neffectiveness of our approach in suppressing noise of data augmentation and\nstabilizing training. Upon the cooperation of knowledge distillation and\nAutoAugment, we claim the new state-of-the-art on ImageNet classification with\na top-1 accuracy of 85.8%.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 11:51:41 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wei", "Longhui", ""], ["Xiao", "An", ""], ["Xie", "Lingxi", ""], ["Chen", "Xin", ""], ["Zhang", "Xiaopeng", ""], ["Tian", "Qi", ""]]}, {"id": "2003.11386", "submitter": "Gui-Song Xia", "authors": "Zhu-Cun Xue, Nan Xue, Gui-Song Xia", "title": "Fisheye Distortion Rectification from Deep Straight Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel line-aware rectification network (LaRecNet) to\naddress the problem of fisheye distortion rectification based on the classical\nobservation that straight lines in 3D space should be still straight in image\nplanes. Specifically, the proposed LaRecNet contains three sequential modules\nto (1) learn the distorted straight lines from fisheye images; (2) estimate the\ndistortion parameters from the learned heatmaps and the image appearance; and\n(3) rectify the input images via a proposed differentiable rectification layer.\nTo better train and evaluate the proposed model, we create a synthetic\nline-rich fisheye (SLF) dataset that contains the distortion parameters and\nwell-annotated distorted straight lines of fisheye images. The proposed method\nenables us to simultaneously calibrate the geometric distortion parameters and\nrectify fisheye images. Extensive experiments demonstrate that our model\nachieves state-of-the-art performance in terms of both geometric accuracy and\nimage quality on several evaluation metrics. In particular, the images\nrectified by LaRecNet achieve an average reprojection error of 0.33 pixels on\nthe SLF dataset and produce the highest peak signal-to-noise ratio (PSNR) and\nstructure similarity index (SSIM) compared with the groundtruth.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 13:20:00 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Xue", "Zhu-Cun", ""], ["Xue", "Nan", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2003.11458", "submitter": "Denis Kleyko", "authors": "Denis Kleyko and Ross W. Gayler and Evgeny Osipov", "title": "Commentaries on \"Learning Sensorimotor Control with Neuromorphic\n  Sensors: Toward Hyperdimensional Active Perception\" [Science Robotics Vol. 4\n  Issue 30 (2019) 1-10", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This correspondence comments on the findings reported in a recent Science\nRobotics article by Mitrokhin et al. [1]. The main goal of this commentary is\nto expand on some of the issues touched on in that article. Our experience is\nthat hyperdimensional computing is very different from other approaches to\ncomputation and that it can take considerable exposure to its concepts before\nattaining practically useful understanding. Therefore, in order to provide an\noverview of the area to the first time reader of [1], the commentary includes a\nbrief historic overview as well as connects the findings of the article to a\nlarger body of literature existing in the area.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 15:53:58 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Kleyko", "Denis", ""], ["Gayler", "Ross W.", ""], ["Osipov", "Evgeny", ""]]}, {"id": "2003.11476", "submitter": "Haoran Song", "authors": "Haoran Song, Wenchao Ding, Yuxuan Chen, Shaojie Shen, Michael Yu Wang,\n  Qifeng Chen", "title": "PiP: Planning-informed Trajectory Prediction for Autonomous Driving", "comments": "European Conference on Computer Vision (ECCV) 2020; Project page at\n  http://haoran-song.github.io/planning-informed-prediction", "journal-ref": null, "doi": "10.1007/978-3-030-58589-1_36", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is critical to predict the motion of surrounding vehicles for self-driving\nplanning, especially in a socially compliant and flexible way. However, future\nprediction is challenging due to the interaction and uncertainty in driving\nbehaviors. We propose planning-informed trajectory prediction (PiP) to tackle\nthe prediction problem in the multi-agent setting. Our approach is\ndifferentiated from the traditional manner of prediction, which is only based\non historical information and decoupled with planning. By informing the\nprediction process with the planning of ego vehicle, our method achieves the\nstate-of-the-art performance of multi-agent forecasting on highway datasets.\nMoreover, our approach enables a novel pipeline which couples the prediction\nand planning, by conditioning PiP on multiple candidate trajectories of the ego\nvehicle, which is highly beneficial for autonomous driving in interactive\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 16:09:54 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 06:14:56 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Song", "Haoran", ""], ["Ding", "Wenchao", ""], ["Chen", "Yuxuan", ""], ["Shen", "Shaojie", ""], ["Wang", "Michael Yu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2003.11504", "submitter": "Ali Senhaji", "authors": "Ali Senhaji, Jenni Raitoharju, Moncef Gabbouj and Alexandros Iosifidis", "title": "Not all domains are equally complex: Adaptive Multi-Domain Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches are highly specialized and require training separate\nmodels for different tasks. Multi-domain learning looks at ways to learn a\nmultitude of different tasks, each coming from a different domain, at once. The\nmost common approach in multi-domain learning is to form a domain agnostic\nmodel, the parameters of which are shared among all domains, and learn a small\nnumber of extra domain-specific parameters for each individual new domain.\nHowever, different domains come with different levels of difficulty;\nparameterizing the models of all domains using an augmented version of the\ndomain agnostic model leads to unnecessarily inefficient solutions, especially\nfor easy to solve tasks. We propose an adaptive parameterization approach to\ndeep neural networks for multi-domain learning. The proposed approach performs\non par with the original approach while reducing by far the number of\nparameters, leading to efficient multi-domain learning solutions.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:16:00 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Senhaji", "Ali", ""], ["Raitoharju", "Jenni", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2003.11509", "submitter": "Jongseok Lee", "authors": "Jongseok Lee, Ribin Balachandran, Yuri S. Sarkisov, Marco De Stefano,\n  Andre Coelho, Kashmira Shinde, Min Jun Kim, Rudolph Triebel and Konstantin\n  Kondak", "title": "Visual-Inertial Telepresence for Aerial Manipulation", "comments": "Accepted to International Conference on Robotics and Automation\n  (ICRA) 2020, IEEE copyright, 8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel telepresence system for enhancing aerial\nmanipulation capabilities. It involves not only a haptic device, but also a\nvirtual reality that provides a 3D visual feedback to a remotely-located\nteleoperator in real-time. We achieve this by utilizing onboard visual and\ninertial sensors, an object tracking algorithm and a pre-generated object\ndatabase. As the virtual reality has to closely match the real remote scene, we\npropose an extension of a marker tracking algorithm with visual-inertial\nodometry. Both indoor and outdoor experiments show benefits of our proposed\nsystem in achieving advanced aerial manipulation tasks, namely grasping,\nplacing, force exertion and peg-in-hole insertion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:26:03 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 17:41:40 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Jongseok", ""], ["Balachandran", "Ribin", ""], ["Sarkisov", "Yuri S.", ""], ["De Stefano", "Marco", ""], ["Coelho", "Andre", ""], ["Shinde", "Kashmira", ""], ["Kim", "Min Jun", ""], ["Triebel", "Rudolph", ""], ["Kondak", "Konstantin", ""]]}, {"id": "2003.11512", "submitter": "Tobias Hinz", "authors": "Tobias Hinz, Matthew Fisher, Oliver Wang, Stefan Wermter", "title": "Improved Techniques for Training Single-Image GANs", "comments": "WACV 2021. Code and supplementary material available at\n  https://github.com/tohinz/ConSinGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been an interest in the potential of learning generative\nmodels from a single image, as opposed to from a large dataset. This task is of\npractical significance, as it means that generative models can be used in\ndomains where collecting a large dataset is not feasible. However, training a\nmodel capable of generating realistic images from only a single sample is a\ndifficult problem. In this work, we conduct a number of experiments to\nunderstand the challenges of training these methods and propose some best\npractices that we found allowed us to generate improved results over previous\nwork in this space. One key piece is that unlike prior single image generation\nmethods, we concurrently train several stages in a sequential multi-stage\nmanner, allowing us to learn models with fewer stages of increasing image\nresolution. Compared to a recent state of the art baseline, our model is up to\nsix times faster to train, has fewer parameters, and can better capture the\nglobal structure of images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:33:25 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 10:55:13 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Hinz", "Tobias", ""], ["Fisher", "Matthew", ""], ["Wang", "Oliver", ""], ["Wermter", "Stefan", ""]]}, {"id": "2003.11535", "submitter": "Brais Martinez", "authors": "Brais Martinez and Jing Yang and Adrian Bulat and Georgios\n  Tzimiropoulos", "title": "Training Binary Neural Networks with Real-to-Binary Convolutions", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to train binary networks to within a few percent points\n($\\sim 3-5 \\%$) of the full precision counterpart. We first show how to build a\nstrong baseline, which already achieves state-of-the-art accuracy, by combining\nrecently proposed advances and carefully adjusting the optimization procedure.\nSecondly, we show that by attempting to minimize the discrepancy between the\noutput of the binary and the corresponding real-valued convolution, additional\nsignificant accuracy gains can be obtained. We materialize this idea in two\ncomplementary ways: (1) with a loss function, during training, by matching the\nspatial attention maps computed at the output of the binary and real-valued\nconvolutions, and (2) in a data-driven manner, by using the real-valued\nactivations, available during inference prior to the binarization process, for\nre-scaling the activations right after the binary convolution. Finally, we show\nthat, when putting all of our improvements together, the proposed model beats\nthe current state of the art by more than 5% top-1 accuracy on ImageNet and\nreduces the gap to its real-valued counterpart to less than 3% and 5% top-1\naccuracy on CIFAR-100 and ImageNet respectively when using a ResNet-18\narchitecture. Code available at https://github.com/brais-martinez/real2binary.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:54:38 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Martinez", "Brais", ""], ["Yang", "Jing", ""], ["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2003.11536", "submitter": "Chiara Pero", "authors": "Carmen Bisogni, Michele Nappi, Chiara Pero and Stefano Ricciardi", "title": "HP2IFS: Head Pose estimation exploiting Partitioned Iterated Function\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the actual head orientation from 2D images, with regard to its\nthree degrees of freedom, is a well known problem that is highly significant\nfor a large number of applications involving head pose knowledge. Consequently,\nthis topic has been tackled by a plethora of methods and algorithms the most\npart of which exploits neural networks. Machine learning methods, indeed,\nachieve accurate head rotation values yet require an adequate training stage\nand, to that aim, a relevant number of positive and negative examples. In this\npaper we take a different approach to this topic by using fractal coding theory\nand particularly Partitioned Iterated Function Systems to extract the fractal\ncode from the input head image and to compare this representation to the\nfractal code of a reference model through Hamming distance. According to\nexperiments conducted on both the BIWI and the AFLW2000 databases, the proposed\nPIFS based head pose estimation method provides accurate yaw/pitch/roll angular\nvalues, with a performance approaching that of state of the art of\nmachine-learning based algorithms and exceeding most of non-training based\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:56:45 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Bisogni", "Carmen", ""], ["Nappi", "Michele", ""], ["Pero", "Chiara", ""], ["Ricciardi", "Stefano", ""]]}, {"id": "2003.11539", "submitter": "Yonglong Tian", "authors": "Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua B. Tenenbaum, and\n  Phillip Isola", "title": "Rethinking Few-Shot Image Classification: a Good Embedding Is All You\n  Need?", "comments": "First two authors contributed equally. Project Page:\n  https://people.csail.mit.edu/yuewang/projects/rfs/ Code:\n  http://github.com/WangYueFt/rfs/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The focus of recent meta-learning research has been on the development of\nlearning algorithms that can quickly adapt to test time tasks with limited data\nand low computational cost. Few-shot learning is widely used as one of the\nstandard benchmarks in meta-learning. In this work, we show that a simple\nbaseline: learning a supervised or self-supervised representation on the\nmeta-training set, followed by training a linear classifier on top of this\nrepresentation, outperforms state-of-the-art few-shot learning methods. An\nadditional boost can be achieved through the use of self-distillation. This\ndemonstrates that using a good learned embedding model can be more effective\nthan sophisticated meta-learning algorithms. We believe that our findings\nmotivate a rethinking of few-shot image classification benchmarks and the\nassociated role of meta-learning algorithms. Code is available at:\nhttp://github.com/WangYueFt/rfs/.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:58:42 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 08:11:10 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Tian", "Yonglong", ""], ["Wang", "Yue", ""], ["Krishnan", "Dilip", ""], ["Tenenbaum", "Joshua B.", ""], ["Isola", "Phillip", ""]]}, {"id": "2003.11540", "submitter": "Goutam Bhat", "authors": "Goutam Bhat, Felix J\\\"aremo Lawin, Martin Danelljan, Andreas Robinson,\n  Michael Felsberg, Luc Van Gool, Radu Timofte", "title": "Learning What to Learn for Video Object Segmentation", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object segmentation (VOS) is a highly challenging problem, since the\ntarget object is only defined during inference with a given first-frame\nreference mask. The problem of how to capture and utilize this limited target\ninformation remains a fundamental research question. We address this by\nintroducing an end-to-end trainable VOS architecture that integrates a\ndifferentiable few-shot learning module. This internal learner is designed to\npredict a powerful parametric model of the target by minimizing a segmentation\nerror in the first frame. We further go beyond standard few-shot learning\ntechniques by learning what the few-shot learner should learn. This allows us\nto achieve a rich internal representation of the target in the current frame,\nsignificantly increasing the segmentation accuracy of our approach. We perform\nextensive experiments on multiple benchmarks. Our approach sets a new\nstate-of-the-art on the large-scale YouTube-VOS 2018 dataset by achieving an\noverall score of 81.5, corresponding to a 2.6% relative improvement over the\nprevious best result.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 17:58:43 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 16:10:19 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bhat", "Goutam", ""], ["Lawin", "Felix J\u00e4remo", ""], ["Danelljan", "Martin", ""], ["Robinson", "Andreas", ""], ["Felsberg", "Michael", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.11566", "submitter": "Luis Oala", "authors": "Luis Oala, Cosmas Hei{\\ss}, Jan Macdonald, Maximilian M\\\"arz, Wojciech\n  Samek and Gitta Kutyniok", "title": "Interval Neural Networks: Uncertainty Scores", "comments": "LO and CH contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast, non-Bayesian method for producing uncertainty scores in\nthe output of pre-trained deep neural networks (DNNs) using a data-driven\ninterval propagating network. This interval neural network (INN) has interval\nvalued parameters and propagates its input using interval arithmetic. The INN\nproduces sensible lower and upper bounds encompassing the ground truth. We\nprovide theoretical justification for the validity of these bounds.\nFurthermore, its asymmetric uncertainty scores offer additional, directional\ninformation beyond what Gaussian-based, symmetric variance estimation can\nprovide. We find that noise in the data is adequately captured by the intervals\nproduced with our method. In numerical experiments on an image reconstruction\ntask, we demonstrate the practical utility of INNs as a proxy for the\nprediction error in comparison to two state-of-the-art uncertainty\nquantification methods. In summary, INNs produce fast, theoretically justified\nuncertainty scores for DNNs that are easy to interpret, come with added\ninformation and pose as improved error proxies - features that may prove useful\nin advancing the usability of DNNs especially in sensitive applications such as\nhealth care.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 18:03:51 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Oala", "Luis", ""], ["Hei\u00df", "Cosmas", ""], ["Macdonald", "Jan", ""], ["M\u00e4rz", "Maximilian", ""], ["Samek", "Wojciech", ""], ["Kutyniok", "Gitta", ""]]}, {"id": "2003.11571", "submitter": "Tianfu Wu", "authors": "Wei Sun and Tianfu Wu", "title": "Learning Layout and Style Reconfigurable GANs for Controllable Image\n  Synthesis", "comments": "16 pages (w/o ref), 15 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the remarkable recent progress on learning deep generative models, it\nbecomes increasingly interesting to develop models for controllable image\nsynthesis from reconfigurable inputs. This paper focuses on a recent emerged\ntask, layout-to-image, to learn generative models that are capable of\nsynthesizing photo-realistic images from spatial layout (i.e., object bounding\nboxes configured in an image lattice) and style (i.e., structural and\nappearance variations encoded by latent vectors). This paper first proposes an\nintuitive paradigm for the task, layout-to-mask-to-image, to learn to unfold\nobject masks of given bounding boxes in an input layout to bridge the gap\nbetween the input layout and synthesized images. Then, this paper presents a\nmethod built on Generative Adversarial Networks for the proposed\nlayout-to-mask-to-image with style control at both image and mask levels.\nObject masks are learned from the input layout and iteratively refined along\nstages in the generator network. Style control at the image level is the same\nas in vanilla GANs, while style control at the object mask level is realized by\na proposed novel feature normalization scheme, Instance-Sensitive and\nLayout-Aware Normalization. In experiments, the proposed method is tested in\nthe COCO-Stuff dataset and the Visual Genome dataset with state-of-the-art\nperformance obtained.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 18:16:05 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 19:57:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sun", "Wei", ""], ["Wu", "Tianfu", ""]]}, {"id": "2003.11596", "submitter": "Mahmoud Afifi", "authors": "Mahmoud Afifi, Konstantinos G. Derpanis, Bj\\\"orn Ommer, Michael S.\n  Brown", "title": "Learning Multi-Scale Photo Exposure Correction", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing photographs with wrong exposures remains a major source of errors\nin camera-based imaging. Exposure problems are categorized as either: (i)\noverexposed, where the camera exposure was too long, resulting in bright and\nwashed-out image regions, or (ii) underexposed, where the exposure was too\nshort, resulting in dark regions. Both under- and overexposure greatly reduce\nthe contrast and visual appeal of an image. Prior work mainly focuses on\nunderexposed images or general image enhancement. In contrast, our proposed\nmethod targets both over- and underexposure errors in photographs. We formulate\nthe exposure correction problem as two main sub-problems: (i) color enhancement\nand (ii) detail enhancement. Accordingly, we propose a coarse-to-fine deep\nneural network (DNN) model, trainable in an end-to-end manner, that addresses\neach sub-problem separately. A key aspect of our solution is a new dataset of\nover 24,000 images exhibiting the broadest range of exposure values to date\nwith a corresponding properly exposed image. Our method achieves results on par\nwith existing state-of-the-art methods on underexposed images and yields\nsignificant improvements for images suffering from overexposure errors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 19:33:51 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 06:49:28 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 05:19:09 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Afifi", "Mahmoud", ""], ["Derpanis", "Konstantinos G.", ""], ["Ommer", "Bj\u00f6rn", ""], ["Brown", "Michael S.", ""]]}, {"id": "2003.11597", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Paul Morrison and Lan Dao", "title": "COVID-19 Image Data Collection", "comments": "Dataset available here:\n  https://github.com/ieee8023/covid-chestxray-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the initial COVID-19 open image data collection. It was\ncreated by assembling medical images from websites and publications and\ncurrently contains 123 frontal view X-rays.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 19:37:25 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Morrison", "Paul", ""], ["Dao", "Lan", ""]]}, {"id": "2003.11617", "submitter": "Ioannis Apostolopoulos", "authors": "Ioannis D. Apostolopoulos, Tzani Bessiana", "title": "Covid-19: Automatic detection from X-Ray images utilizing Transfer\n  Learning with Convolutional Neural Networks", "comments": null, "journal-ref": "Physical and Engineering Sciences in Medicine 43:635-40;2020", "doi": "10.1007/s13246-020-00865-4", "report-no": null, "categories": "eess.IV cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a dataset of X-Ray images from patients with common pneumonia,\nCovid-19, and normal incidents was utilized for the automatic detection of the\nCoronavirus. The aim of the study is to evaluate the performance of\nstate-of-the-art Convolutional Neural Network architectures proposed over\nrecent years for medical image classification. Specifically, the procedure\ncalled transfer learning was adopted. With transfer learning, the detection of\nvarious abnormalities in small medical image datasets is an achievable target,\noften yielding remarkable results. The dataset utilized in this experiment is a\ncollection of 1427 X-Ray images. 224 images with confirmed Covid-19, 700 images\nwith confirmed common pneumonia, and 504 images of normal conditions are\nincluded. The data was collected from the available X-Ray images on public\nmedical repositories. With transfer learning, an overall accuracy of 97.82% in\nthe detection of Covid-19 is achieved.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 20:34:30 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Apostolopoulos", "Ioannis D.", ""], ["Bessiana", "Tzani", ""]]}, {"id": "2003.11618", "submitter": "Jingzhou Liu", "authors": "Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan, Licheng Yu, Yiming Yang,\n  Jingjing Liu", "title": "VIOLIN: A Large-Scale Dataset for Video-and-Language Inference", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new task, Video-and-Language Inference, for joint multimodal\nunderstanding of video and text. Given a video clip with aligned subtitles as\npremise, paired with a natural language hypothesis based on the video content,\na model needs to infer whether the hypothesis is entailed or contradicted by\nthe given video clip. A new large-scale dataset, named Violin\n(VIdeO-and-Language INference), is introduced for this task, which consists of\n95,322 video-hypothesis pairs from 15,887 video clips, spanning over 582 hours\nof video. These video clips contain rich content with diverse temporal\ndynamics, event shifts, and people interactions, collected from two sources:\n(i) popular TV shows, and (ii) movie clips from YouTube channels. In order to\naddress our new multimodal inference task, a model is required to possess\nsophisticated reasoning skills, from surface-level grounding (e.g., identifying\nobjects and characters in the video) to in-depth commonsense reasoning (e.g.,\ninferring causal relations of events in the video). We present a detailed\nanalysis of the dataset and an extensive evaluation over many strong baselines,\nproviding valuable insights on the challenges of this new task.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 20:39:05 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Liu", "Jingzhou", ""], ["Chen", "Wenhu", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Yang", "Yiming", ""], ["Liu", "Jingjing", ""]]}, {"id": "2003.11632", "submitter": "Andrea Gayon Lombardo Miss", "authors": "Andrea Gayon-Lombardo, Lukas Mosser, Nigel P. Brandon, Samuel J.\n  Cooper", "title": "Pores for thought: The use of generative adversarial networks for the\n  stochastic reconstruction of 3D multi-phase electrode microstructures with\n  periodic boundaries", "comments": "37 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of multiphase porous electrode microstructures is a critical\nstep in the optimisation of electrochemical energy storage devices. This work\nimplements a deep convolutional generative adversarial network (DC-GAN) for\ngenerating realistic n-phase microstructural data. The same network\narchitecture is successfully applied to two very different three-phase\nmicrostructures: A lithium-ion battery cathode and a solid oxide fuel cell\nanode. A comparison between the real and synthetic data is performed in terms\nof the morphological properties (volume fraction, specific surface area,\ntriple-phase boundary) and transport properties (relative diffusivity), as well\nas the two-point correlation function. The results show excellent agreement\nbetween for datasets and they are also visually indistinguishable. By modifying\nthe input to the generator, we show that it is possible to generate\nmicrostructure with periodic boundaries in all three directions. This has the\npotential to significantly reduce the simulated volume required to be\nconsidered representative and therefore massively reduce the computational cost\nof the electrochemical simulations necessary to predict the performance of a\nparticular microstructure during optimisation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 17:38:27 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 21:37:20 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gayon-Lombardo", "Andrea", ""], ["Mosser", "Lukas", ""], ["Brandon", "Nigel P.", ""], ["Cooper", "Samuel J.", ""]]}, {"id": "2003.11647", "submitter": "Zhiheng Li", "authors": "Zhiheng Li, Wenxuan Bao, Jiayang Zheng, Chenliang Xu", "title": "Deep Grouping Model for Unified Perceptual Parsing", "comments": "Accepted by CVPR 2020", "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perceptual-based grouping process produces a hierarchical and\ncompositional image representation that helps both human and machine vision\nsystems recognize heterogeneous visual concepts. Examples can be found in the\nclassical hierarchical superpixel segmentation or image parsing works. However,\nthe grouping process is largely overlooked in modern CNN-based image\nsegmentation networks due to many challenges, including the inherent\nincompatibility between the grid-shaped CNN feature map and the\nirregular-shaped perceptual grouping hierarchy. Overcoming these challenges, we\npropose a deep grouping model (DGM) that tightly marries the two types of\nrepresentations and defines a bottom-up and a top-down process for feature\nexchanging. When evaluating the model on the recent Broden+ dataset for the\nunified perceptual parsing task, it achieves state-of-the-art results while\nhaving a small computational overhead compared to other contextual-based\nsegmentation models. Furthermore, the DGM has better interpretability compared\nwith modern CNN methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:16:09 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Li", "Zhiheng", ""], ["Bao", "Wenxuan", ""], ["Zheng", "Jiayang", ""], ["Xu", "Chenliang", ""]]}, {"id": "2003.11652", "submitter": "Jathushan Rajasegaran", "authors": "Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan,\n  Mubarak Shah", "title": "iTAML: An Incremental Task-Agnostic Meta-learning Approach", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can continuously learn new knowledge as their experience grows. In\ncontrast, previous learning in deep neural networks can quickly fade out when\nthey are trained on a new task. In this paper, we hypothesize this problem can\nbe avoided by learning a set of generalized parameters, that are neither\nspecific to old nor new tasks. In this pursuit, we introduce a novel\nmeta-learning approach that seeks to maintain an equilibrium between all the\nencountered tasks. This is ensured by a new meta-update rule which avoids\ncatastrophic forgetting. In comparison to previous meta-learning techniques,\nour approach is task-agnostic. When presented with a continuum of data, our\nmodel automatically identifies the task and quickly adapts to it with just a\nsingle update. We perform extensive experiments on five datasets in a\nclass-incremental setting, leading to significant improvements over the state\nof the art methods (e.g., a 21.3% boost on CIFAR100 with 10 incremental tasks).\nSpecifically, on large-scale datasets that generally prove difficult cases for\nincremental learning, our approach delivers absolute gains as high as 19.1% and\n7.4% on ImageNet and MS-Celeb datasets, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 21:42:48 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Rajasegaran", "Jathushan", ""], ["Khan", "Salman", ""], ["Hayat", "Munawar", ""], ["Khan", "Fahad Shahbaz", ""], ["Shah", "Mubarak", ""]]}, {"id": "2003.11670", "submitter": "Peng Zhou", "authors": "Peng Zhou, Brian Price, Scott Cohen, Gregg Wilensky and Larry S. Davis", "title": "DeepStrip: High Resolution Boundary Refinement", "comments": null, "journal-ref": "CVPR 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we target refining the boundaries in high resolution images\ngiven low resolution masks. For memory and computation efficiency, we propose\nto convert the regions of interest into strip images and compute a boundary\nprediction in the strip domain. To detect the target boundary, we present a\nframework with two prediction layers. First, all potential boundaries are\npredicted as an initial prediction and then a selection layer is used to pick\nthe target boundary and smooth the result. To encourage accurate prediction, a\nloss which measures the boundary distance in the strip domain is introduced. In\naddition, we enforce a matching consistency and C0 continuity regularization to\nthe network to reduce false alarms. Extensive experiments on both public and a\nnewly created high resolution dataset strongly validate our approach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 22:44:48 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhou", "Peng", ""], ["Price", "Brian", ""], ["Cohen", "Scott", ""], ["Wilensky", "Gregg", ""], ["Davis", "Larry S.", ""]]}, {"id": "2003.11690", "submitter": "Yandong Li", "authors": "Yandong Li, Yu Cheng, Zhe Gan, Licheng Yu, Liqiang Wang, and Jingjing\n  Liu", "title": "BachGAN: High-Resolution Image Synthesis from Salient Object Layout", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new task towards more practical application for image generation\n- high-quality image synthesis from salient object layout. This new setting\nallows users to provide the layout of salient objects only (i.e., foreground\nbounding boxes and categories), and lets the model complete the drawing with an\ninvented background and a matching foreground. Two main challenges spring from\nthis new task: (i) how to generate fine-grained details and realistic textures\nwithout segmentation map input; and (ii) how to create a background and weave\nit seamlessly into standalone objects. To tackle this, we propose Background\nHallucination Generative Adversarial Network (BachGAN), which first selects a\nset of segmentation maps from a large candidate pool via a background retrieval\nmodule, then encodes these candidate layouts via a background fusion module to\nhallucinate a suitable background for the given objects. By generating the\nhallucinated background representation dynamically, our model can synthesize\nhigh-resolution images with both photo-realistic foreground and integral\nbackground. Experiments on Cityscapes and ADE20K datasets demonstrate the\nadvantage of BachGAN over existing methods, measured on both visual fidelity of\ngenerated images and visual alignment between output images and input layouts.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 00:54:44 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 20:53:24 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Yandong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Yu", "Licheng", ""], ["Wang", "Liqiang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2003.11700", "submitter": "Vahid Abolghasemi", "authors": "Rasool Ameri, Ali Alameer, Saideh Ferdowsi, Kianoush Nazarpour, and\n  Vahid Abolghasemi", "title": "Classification of Chinese Handwritten Numbers with Labeled Projective\n  Dictionary Pair Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary learning is a cornerstone of image classification. We set out to\naddress a longstanding challenge in using dictionary learning for\nclassification; that is to simultaneously maximise the discriminability and\nsparse-representability power of the learned dictionaries. Upon this premise,\nwe designed class-specific dictionaries incorporating three factors:\ndiscriminability, sparsity and classification error. We integrated these\nmetrics into a unified cost function and adopted a new feature space, i.e.,\nhistogram of oriented gradients (HOG), to generate the dictionary atoms. The\nrationale of using HOG features for designing the dictionaries is their\nstrength in describing fine details of crowded images. The results of applying\nthe proposed method in the classification of Chinese handwritten numbers\ndemonstrated enhanced classification performance $(\\sim98\\%)$ compared to\nstate-of-the-art deep learning techniques (i.e., SqueezeNet, GoogLeNet and\nMobileNetV2), but with a fraction of parameters. Furthermore, combination of\nthe HOG features with dictionary learning enhances the accuracy by $11\\%$\ncompared to the case where only pixel domain data are used. These results were\nsupported when the proposed method was applied to both Arabic and English\nhandwritten number databases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 01:43:59 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 19:06:50 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 12:21:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Ameri", "Rasool", ""], ["Alameer", "Ali", ""], ["Ferdowsi", "Saideh", ""], ["Nazarpour", "Kianoush", ""], ["Abolghasemi", "Vahid", ""]]}, {"id": "2003.11712", "submitter": "Chunhua Shen", "authors": "Rufeng Zhang, Zhi Tian, Chunhua Shen, Mingyu You, Youliang Yan", "title": "Mask Encoding for Single Shot Instance Segmentation", "comments": "Accepted to Proc. IEEE Conf. Computer Vision and Pattern Recognition\n  (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To date, instance segmentation is dominated by twostage methods, as pioneered\nby Mask R-CNN. In contrast, one-stage alternatives cannot compete with Mask\nR-CNN in mask AP, mainly due to the difficulty of compactly representing masks,\nmaking the design of one-stage methods very challenging. In this work, we\npropose a simple singleshot instance segmentation framework, termed mask\nencoding based instance segmentation (MEInst). Instead of predicting the\ntwo-dimensional mask directly, MEInst distills it into a compact and\nfixed-dimensional representation vector, which allows the instance segmentation\ntask to be incorporated into one-stage bounding-box detectors and results in a\nsimple yet efficient instance segmentation framework. The proposed one-stage\nMEInst achieves 36.4% in mask AP with single-model (ResNeXt-101-FPN backbone)\nand single-scale testing on the MS-COCO benchmark. We show that the much\nsimpler and flexible one-stage instance segmentation method, can also achieve\ncompetitive performance. This framework can be easily adapted for other\ninstance-level recognition tasks. Code is available at:\nhttps://git.io/AdelaiDet\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 02:51:17 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 12:44:26 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Zhang", "Rufeng", ""], ["Tian", "Zhi", ""], ["Shen", "Chunhua", ""], ["You", "Mingyu", ""], ["Yan", "Youliang", ""]]}, {"id": "2003.11734", "submitter": "Xiaoye Sun", "authors": "Xiaoye Sun, Gongyan Li, Shaoyun Xu", "title": "Fastidious Attention Network for Navel Orange Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning achieves excellent performance in many domains, so we not only\napply it to the navel orange semantic segmentation task to solve the two\nproblems of distinguishing defect categories and identifying the stem end and\nblossom end, but also propose a fastidious attention mechanism to further\nimprove model performance. This lightweight attention mechanism includes two\nlearnable parameters, activations and thresholds, to capture long-range\ndependence. Specifically, the threshold picks out part of the spatial feature\nmap and the activation excite this area. Based on activations and thresholds\ntraining from different types of feature maps, we design fastidious\nself-attention module (FSAM) and fastidious inter-attention module (FIAM). And\nthen construct the Fastidious Attention Network (FANet), which uses U-Net as\nthe backbone and embeds these two modules, to solve the problems with semantic\nsegmentation for stem end, blossom end, flaw and ulcer. Compared with some\nstate-of-the-art deep-learning-based networks under our navel orange dataset,\nexperiments show that our network is the best performance with pixel accuracy\n99.105%, mean accuracy 77.468%, mean IU 70.375% and frequency weighted IU\n98.335%. And embedded modules show better discrimination of 5 categories\nincluding background, especially the IU of flaw is increased by 3.165%.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 03:59:22 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Sun", "Xiaoye", ""], ["Li", "Gongyan", ""], ["Xu", "Shaoyun", ""]]}, {"id": "2003.11743", "submitter": "Pranav Agarwal", "authors": "Pranav Agarwal, Alejandro Betancourt, Vana Panagiotou and Natalia\n  D\\'iaz-Rodr\\'iguez", "title": "Egoshots, an ego-vision life-logging dataset and semantic fidelity\n  metric to evaluate diversity in image captioning models", "comments": "15 pages, 25 figures, Accepted at Machine Learning in Real Life\n  (ML-IRL) ICLR 2020 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning models have been able to generate grammatically correct and\nhuman understandable sentences. However most of the captions convey limited\ninformation as the model used is trained on datasets that do not caption all\npossible objects existing in everyday life. Due to this lack of prior\ninformation most of the captions are biased to only a few objects present in\nthe scene, hence limiting their usage in daily life. In this paper, we attempt\nto show the biased nature of the currently existing image captioning models and\npresent a new image captioning dataset, Egoshots, consisting of 978 real life\nimages with no captions. We further exploit the state of the art pre-trained\nimage captioning and object recognition networks to annotate our images and\nshow the limitations of existing works. Furthermore, in order to evaluate the\nquality of the generated captions, we propose a new image captioning metric,\nobject based Semantic Fidelity (SF). Existing image captioning metrics can\nevaluate a caption only in the presence of their corresponding annotations;\nhowever, SF allows evaluating captions generated for images without\nannotations, making it highly useful for real life generated captions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 04:43:30 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 09:16:33 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Agarwal", "Pranav", ""], ["Betancourt", "Alejandro", ""], ["Panagiotou", "Vana", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""]]}, {"id": "2003.11753", "submitter": "Quanzeng You", "authors": "Quanzeng You, Hao Jiang", "title": "Real-time 3D Deep Multi-Camera Tracking", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking a crowd in 3D using multiple RGB cameras is a challenging task. Most\nprevious multi-camera tracking algorithms are designed for offline setting and\nhave high computational complexity. Robust real-time multi-camera 3D tracking\nis still an unsolved problem. In this work, we propose a novel end-to-end\ntracking pipeline, Deep Multi-Camera Tracking (DMCT), which achieves reliable\nreal-time multi-camera people tracking. Our DMCT consists of 1) a fast and\nnovel perspective-aware Deep GroudPoint Network, 2) a fusion procedure for\nground-plane occupancy heatmap estimation, 3) a novel Deep Glimpse Network for\nperson detection and 4) a fast and accurate online tracker. Our design fully\nunleashes the power of deep neural network to estimate the \"ground point\" of\neach person in each color image, which can be optimized to run efficiently and\nrobustly. Our fusion procedure, glimpse network and tracker merge the results\nfrom different views, find people candidates using multiple video frames and\nthen track people on the fused heatmap. Our system achieves the\nstate-of-the-art tracking results while maintaining real-time performance.\nApart from evaluation on the challenging WILDTRACK dataset, we also collect two\nmore tracking datasets with high-quality labels from two different environments\nand camera settings. Our experimental results confirm that our proposed\nreal-time pipeline gives superior results to previous approaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 06:08:19 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["You", "Quanzeng", ""], ["Jiang", "Hao", ""]]}, {"id": "2003.11756", "submitter": "Xuesong Niu", "authors": "Xiaobai Li, Hu Han, Hao Lu, Xuesong Niu, Zitong Yu, Antitza Dantcheva,\n  Guoying Zhao, Shiguang Shan", "title": "The 1st Challenge on Remote Physiological Signal Sensing (RePSS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote measurement of physiological signals from videos is an emerging topic.\nThe topic draws great interests, but the lack of publicly available benchmark\ndatabases and a fair validation platform are hindering its further development.\nFor this concern, we organize the first challenge on Remote Physiological\nSignal Sensing (RePSS), in which two databases of VIPL and OBF are provided as\nthe benchmark for kin researchers to evaluate their approaches. The 1st\nchallenge of RePSS focuses on measuring the average heart rate from facial\nvideos, which is the basic problem of remote physiological measurement. This\npaper presents an overview of the challenge, including data, protocol, analysis\nof results and discussion. The top ranked solutions are highlighted to provide\ninsights for researchers, and future directions are outlined for this topic and\nthis challenge.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 06:17:54 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Li", "Xiaobai", ""], ["Han", "Hu", ""], ["Lu", "Hao", ""], ["Niu", "Xuesong", ""], ["Yu", "Zitong", ""], ["Dantcheva", "Antitza", ""], ["Zhao", "Guoying", ""], ["Shan", "Shiguang", ""]]}, {"id": "2003.11766", "submitter": "Sai Krishna Bashetty", "authors": "Sai Krishna Bashetty, Heni Ben Amor, Georgios Fainekos", "title": "DeepCrashTest: Turning Dashcam Videos into Virtual Crash Tests for\n  Automated Driving Systems", "comments": "8 pages, 5 figures, ICRA 2020, Trajectory Extraction, Trajectory\n  Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to generate simulations with real-world collision\nscenarios for training and testing autonomous vehicles. We use numerous dashcam\ncrash videos uploaded on the internet to extract valuable collision data and\nrecreate the crash scenarios in a simulator. We tackle the problem of\nextracting 3D vehicle trajectories from videos recorded by an unknown and\nuncalibrated monocular camera source using a modular approach. A working\narchitecture and demonstration videos along with the open-source implementation\nare provided with the paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:03:45 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Bashetty", "Sai Krishna", ""], ["Amor", "Heni Ben", ""], ["Fainekos", "Georgios", ""]]}, {"id": "2003.11774", "submitter": "Khoa Doan", "authors": "Khoa D. Doan and Saurav Manchanda and Fengjiao Wang and Sathiya\n  Keerthi and Avradeep Bhowmik and Chandan K. Reddy", "title": "Image Generation Via Minimizing Fr\\'echet Distance in Discriminator\n  Feature Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given image generation problem, the intrinsic image manifold is often\nlow dimensional. We use the intuition that it is much better to train the GAN\ngenerator by minimizing the distributional distance between real and generated\nimages in a small dimensional feature space representing such a manifold than\non the original pixel-space. We use the feature space of the GAN discriminator\nfor such a representation. For distributional distance, we employ one of two\nchoices: the Fr\\'{e}chet distance or direct optimal transport (OT); these\nrespectively lead us to two new GAN methods: Fr\\'{e}chet-GAN and OT-GAN. The\nidea of employing Fr\\'{e}chet distance comes from the success of Fr\\'{e}chet\nInception Distance as a solid evaluation metric in image generation.\nFr\\'{e}chet-GAN is attractive in several ways. We propose an efficient,\nnumerically stable approach to calculate the Fr\\'{e}chet distance and its\ngradient. The Fr\\'{e}chet distance estimation requires a significantly less\ncomputation time than OT; this allows Fr\\'{e}chet-GAN to use much larger\nmini-batch size in training than OT. More importantly, we conduct experiments\non a number of benchmark datasets and show that Fr\\'{e}chet-GAN (in particular)\nand OT-GAN have significantly better image generation capabilities than the\nexisting representative primal and dual GAN approaches based on the Wasserstein\ndistance.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 07:37:18 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 20:35:11 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Doan", "Khoa D.", ""], ["Manchanda", "Saurav", ""], ["Wang", "Fengjiao", ""], ["Keerthi", "Sathiya", ""], ["Bhowmik", "Avradeep", ""], ["Reddy", "Chandan K.", ""]]}, {"id": "2003.11794", "submitter": "Yujie Zhong", "authors": "Yujie Zhong, Relja Arandjelovi\\'c, Andrew Zisserman", "title": "Compact Deep Aggregation for Set Retrieval", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this work is to learn a compact embedding of a set of\ndescriptors that is suitable for efficient retrieval and ranking, whilst\nmaintaining discriminability of the individual descriptors. We focus on a\nspecific example of this general problem -- that of retrieving images\ncontaining multiple faces from a large scale dataset of images. Here the set\nconsists of the face descriptors in each image, and given a query for multiple\nidentities, the goal is then to retrieve, in order, images which contain all\nthe identities, all but one, \\etc\n  To this end, we make the following contributions: first, we propose a CNN\narchitecture -- {\\em SetNet} -- to achieve the objective: it learns face\ndescriptors and their aggregation over a set to produce a compact fixed length\ndescriptor designed for set retrieval, and the score of an image is a count of\nthe number of identities that match the query; second, we show that this\ncompact descriptor has minimal loss of discriminability up to two faces per\nimage, and degrades slowly after that -- far exceeding a number of baselines;\nthird, we explore the speed vs.\\ retrieval quality trade-off for set retrieval\nusing this compact descriptor; and, finally, we collect and annotate a large\ndataset of images containing various number of celebrities, which we use for\nevaluation and is publicly released.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 08:43:15 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhong", "Yujie", ""], ["Arandjelovi\u0107", "Relja", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2003.11797", "submitter": "Kai Qiao", "authors": "Kai Qiao, Chi Zhang, Jian Chen, Linyuan Wang, Li Tong, Bin Yan", "title": "Neural encoding and interpretation for high-level visual cortices based\n  on fMRI using image caption features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On basis of functional magnetic resonance imaging (fMRI), researchers are\ndevoted to designing visual encoding models to predict the neuron activity of\nhuman in response to presented image stimuli and analyze inner mechanism of\nhuman visual cortices. Deep network structure composed of hierarchical\nprocessing layers forms deep network models by learning features of data on\nspecific task through big dataset. Deep network models have powerful and\nhierarchical representation of data, and have brought about breakthroughs for\nvisual encoding, while revealing hierarchical structural similarity with the\nmanner of information processing in human visual cortices. However, previous\nstudies almost used image features of those deep network models pre-trained on\nclassification task to construct visual encoding models. Except for deep\nnetwork structure, the task or corresponding big dataset is also important for\ndeep network models, but neglected by previous studies. Because image\nclassification is a relatively fundamental task, it is difficult to guide deep\nnetwork models to master high-level semantic representations of data, which\ncauses into that encoding performance for high-level visual cortices is\nlimited. In this study, we introduced one higher-level vision task: image\ncaption (IC) task and proposed the visual encoding model based on IC features\n(ICFVEM) to encode voxels of high-level visual cortices. Experiment\ndemonstrated that ICFVEM obtained better encoding performance than previous\ndeep network models pre-trained on classification task. In addition, the\ninterpretation of voxels was realized to explore the detailed characteristics\nof voxels based on the visualization of semantic words, and comparative\nanalysis implied that high-level visual cortices behaved the correlative\nrepresentation of image content.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 08:47:21 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Qiao", "Kai", ""], ["Zhang", "Chi", ""], ["Chen", "Jian", ""], ["Wang", "Linyuan", ""], ["Tong", "Li", ""], ["Yan", "Bin", ""]]}, {"id": "2003.11816", "submitter": "Jirong Yi", "authors": "Zain Khan, Jirong Yi, Raghu Mudumbai, Xiaodong Wu, Weiyu Xu", "title": "Do Deep Minds Think Alike? Selective Adversarial Attacks for\n  Fine-Grained Manipulation of Multiple Deep Neural Networks", "comments": "9 pages, submitted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have demonstrated the existence of {\\it adversarial examples}\ntargeting a single machine learning system. In this paper we ask a simple but\nfundamental question of \"selective fooling\": given {\\it multiple} machine\nlearning systems assigned to solve the same classification problem and taking\nthe same input signal, is it possible to construct a perturbation to the input\nsignal that manipulates the outputs of these {\\it multiple} machine learning\nsystems {\\it simultaneously} in arbitrary pre-defined ways? For example, is it\npossible to selectively fool a set of \"enemy\" machine learning systems but does\nnot fool the other \"friend\" machine learning systems? The answer to this\nquestion depends on the extent to which these different machine learning\nsystems \"think alike\". We formulate the problem of \"selective fooling\" as a\nnovel optimization problem, and report on a series of experiments on the MNIST\ndataset. Our preliminary findings from these experiments show that it is in\nfact very easy to selectively manipulate multiple MNIST classifiers\nsimultaneously, even when the classifiers are identical in their architectures,\ntraining algorithms and training datasets except for random initialization\nduring training. This suggests that two nominally equivalent machine learning\nsystems do not in fact \"think alike\" at all, and opens the possibility for many\nnovel applications and deeper understandings of the working principles of deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 10:00:33 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Khan", "Zain", ""], ["Yi", "Jirong", ""], ["Mudumbai", "Raghu", ""], ["Wu", "Xiaodong", ""], ["Xu", "Weiyu", ""]]}, {"id": "2003.11818", "submitter": "Jianyuan Guo", "authors": "Jianyuan Guo, Kai Han, Yunhe Wang, Chao Zhang, Zhaohui Yang, Han Wu,\n  Xinghao Chen and Chang Xu", "title": "Hit-Detector: Hierarchical Trinity Architecture Search for Object\n  Detection", "comments": "Accepted in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has achieved great success in image\nclassification task. Some recent works have managed to explore the automatic\ndesign of efficient backbone or feature fusion layer for object detection.\nHowever, these methods focus on searching only one certain component of object\ndetector while leaving others manually designed. We identify the inconsistency\nbetween searched component and manually designed ones would withhold the\ndetector of stronger performance. To this end, we propose a hierarchical\ntrinity search framework to simultaneously discover efficient architectures for\nall components (i.e. backbone, neck, and head) of object detector in an\nend-to-end manner. In addition, we empirically reveal that different parts of\nthe detector prefer different operators. Motivated by this, we employ a novel\nscheme to automatically screen different sub search spaces for different\ncomponents so as to perform the end-to-end search for each component on the\ncorresponding sub search space efficiently. Without bells and whistles, our\nsearched architecture, namely Hit-Detector, achieves 41.4\\% mAP on COCO minival\nset with 27M parameters. Our implementation is available at\nhttps://github.com/ggjy/HitDet.pytorch.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 10:20:52 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Guo", "Jianyuan", ""], ["Han", "Kai", ""], ["Wang", "Yunhe", ""], ["Zhang", "Chao", ""], ["Yang", "Zhaohui", ""], ["Wu", "Han", ""], ["Chen", "Xinghao", ""], ["Xu", "Chang", ""]]}, {"id": "2003.11842", "submitter": "Frank Glavin", "authors": "James Houston, Frank G. Glavin, Michael G. Madden", "title": "Robust Classification of High-Dimensional Spectroscopy Data Using Deep\n  Learning and Data Synthesis", "comments": "Journal of Chemical Information and Modeling", "journal-ref": null, "doi": "10.1021/acs.jcim.9b01037", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to classification of high dimensional\nspectroscopy data and demonstrates that it outperforms other current\nstate-of-the art approaches. The specific task we consider is identifying\nwhether samples contain chlorinated solvents or not, based on their Raman\nspectra. We also examine robustness to classification of outlier samples that\nare not represented in the training set (negative outliers). A novel\napplication of a locally-connected neural network (NN) for the binary\nclassification of spectroscopy data is proposed and demonstrated to yield\nimproved accuracy over traditionally popular algorithms. Additionally, we\npresent the ability to further increase the accuracy of the locally-connected\nNN algorithm through the use of synthetic training spectra and we investigate\nthe use of autoencoder based one-class classifiers and outlier detectors.\nFinally, a two-step classification process is presented as an alternative to\nthe binary and one-class classification paradigms. This process combines the\nlocally-connected NN classifier, the use of synthetic training data, and an\nautoencoder based outlier detector to produce a model which is shown to both\nproduce high classification accuracy, and be robust to the presence of negative\noutliers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 11:33:52 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Houston", "James", ""], ["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "2003.11844", "submitter": "Shailza Jolly", "authors": "Shailza Jolly, Sebastian Palacio, Joachim Folz, Federico Raue, Joern\n  Hees, Andreas Dengel", "title": "P $\\approx$ NP, at least in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, progress in the Visual Question Answering (VQA) field has\nlargely been driven by public challenges and large datasets. One of the most\nwidely-used of these is the VQA 2.0 dataset, consisting of polar (\"yes/no\") and\nnon-polar questions. Looking at the question distribution over all answers, we\nfind that the answers \"yes\" and \"no\" account for 38 % of the questions, while\nthe remaining 62% are spread over the more than 3000 remaining answers. While\nseveral sources of biases have already been investigated in the field, the\neffects of such an over-representation of polar vs. non-polar questions remain\nunclear. In this paper, we measure the potential confounding factors when polar\nand non-polar samples are used jointly to train a baseline VQA classifier, and\ncompare it to an upper bound where the over-representation of polar questions\nis excluded from the training. Further, we perform cross-over experiments to\nanalyze how well the feature spaces align. Contrary to expectations, we find no\nevidence of counterproductive effects in the joint training of unbalanced\nclasses. In fact, by exploring the intermediate feature space of visual-text\nembeddings, we find that the feature space of polar questions already encodes\nsufficient structure to answer many non-polar questions. Our results indicate\nthat the polar (P) and the non-polar (NP) feature spaces are strongly aligned,\nhence the expression P $\\approx$ NP\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 11:36:09 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 08:27:28 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Jolly", "Shailza", ""], ["Palacio", "Sebastian", ""], ["Folz", "Joachim", ""], ["Raue", "Federico", ""], ["Hees", "Joern", ""], ["Dengel", "Andreas", ""]]}, {"id": "2003.11846", "submitter": "Lu Wang", "authors": "Lu Wang, Dong-xue Liang, Xiao-lei Yin, Jing Qiu, Zhi-yun Yang, Jun-hui\n  Xing, Jian-zeng Dong, Zhao-yuan Ma", "title": "Weakly-supervised 3D coronary artery reconstruction from two-view\n  angiographic images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of three-dimensional models of coronary arteries is of\ngreat significance for the localization, evaluation and diagnosis of stenosis\nand plaque in the arteries, as well as for the assisted navigation of\ninterventional surgery. In the clinical practice, physicians use a few angles\nof coronary angiography to capture arterial images, so it is of great practical\nvalue to perform 3D reconstruction directly from coronary angiography images.\nHowever, this is a very difficult computer vision task due to the complex shape\nof coronary blood vessels, as well as the lack of data set and key point\nlabeling. With the rise of deep learning, more and more work is being done to\nreconstruct 3D models of human organs from medical images using deep neural\nnetworks. We propose an adversarial and generative way to reconstruct three\ndimensional coronary artery models, from two different views of angiographic\nimages of coronary arteries. With 3D fully supervised learning and 2D weakly\nsupervised learning schemes, we obtained reconstruction accuracies that\noutperform state-of-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 11:41:38 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wang", "Lu", ""], ["Liang", "Dong-xue", ""], ["Yin", "Xiao-lei", ""], ["Qiu", "Jing", ""], ["Yang", "Zhi-yun", ""], ["Xing", "Jun-hui", ""], ["Dong", "Jian-zeng", ""], ["Ma", "Zhao-yuan", ""]]}, {"id": "2003.11851", "submitter": "Lu Wang", "authors": "Lu Wang, Dong-xue Liang, Xiao-lei Yin, Jing Qiu, Zhi-yun Yang, Jun-hui\n  Xing, Jian-zeng Dong, Zhao-yuan Ma", "title": "Coronary Artery Segmentation in Angiographic Videos Using A 3D-2D CE-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary angiography is an indispensable assistive technique for cardiac\ninterventional surgery. Segmentation and extraction of blood vessels from\ncoronary angiography videos are very essential prerequisites for physicians to\nlocate, assess and diagnose the plaques and stenosis in blood vessels. This\narticle proposes a new video segmentation framework that can extract the\nclearest and most comprehensive coronary angiography images from a video\nsequence, thereby helping physicians to better observe the condition of blood\nvessels. This framework combines a 3D convolutional layer to extract\nspatial--temporal information from a video sequence and a 2D CE--Net to\naccomplish the segmentation task of an image sequence. The input is a few\ncontinuous frames of angiographic video, and the output is a mask of\nsegmentation result. From the results of segmentation and extraction, we can\nget good segmentation results despite the poor quality of coronary angiography\nvideo sequences.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 11:56:17 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 03:15:15 GMT"}, {"version": "v3", "created": "Mon, 18 May 2020 01:29:50 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wang", "Lu", ""], ["Liang", "Dong-xue", ""], ["Yin", "Xiao-lei", ""], ["Qiu", "Jing", ""], ["Yang", "Zhi-yun", ""], ["Xing", "Jun-hui", ""], ["Dong", "Jian-zeng", ""], ["Ma", "Zhao-yuan", ""]]}, {"id": "2003.11853", "submitter": "Yikai Wang", "authors": "Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu", "title": "Instance Credibility Inference for Few-Shot Learning", "comments": "accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot learning (FSL) aims to recognize new objects with extremely limited\ntraining data for each category. Previous efforts are made by either leveraging\nmeta-learning paradigm or novel principles in data augmentation to alleviate\nthis extremely data-scarce problem. In contrast, this paper presents a simple\nstatistical approach, dubbed Instance Credibility Inference (ICI) to exploit\nthe distribution support of unlabeled instances for few-shot learning.\nSpecifically, we first train a linear classifier with the labeled few-shot\nexamples and use it to infer the pseudo-labels for the unlabeled data. To\nmeasure the credibility of each pseudo-labeled instance, we then propose to\nsolve another linear regression hypothesis by increasing the sparsity of the\nincidental parameters and rank the pseudo-labeled instances with their sparsity\ndegree. We select the most trustworthy pseudo-labeled instances alongside the\nlabeled examples to re-train the linear classifier. This process is iterated\nuntil all the unlabeled samples are included in the expanded training set, i.e.\nthe pseudo-label is converged for unlabeled data pool. Extensive experiments\nunder two few-shot settings show that our simple approach can establish new\nstate-of-the-arts on four widely used few-shot learning benchmark datasets\nincluding miniImageNet, tieredImageNet, CIFAR-FS, and CUB. Our code is\navailable at: https://github.com/Yikai-Wang/ICI-FSL\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 12:01:15 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 01:40:28 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Wang", "Yikai", ""], ["Xu", "Chengming", ""], ["Liu", "Chen", ""], ["Zhang", "Li", ""], ["Fu", "Yanwei", ""]]}, {"id": "2003.11883", "submitter": "Xiong Zhang", "authors": "Xiong Zhang, Hongmin Xu, Hong Mo, Jianchao Tan, Cheng Yang, Lei Wang,\n  Wenqi Ren", "title": "DCNAS: Densely Connected Neural Architecture Search for Semantic Image\n  Segmentation", "comments": "accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) has shown great potentials in automatically\ndesigning scalable network architectures for dense image predictions. However,\nexisting NAS algorithms usually compromise on restricted search space and\nsearch on proxy task to meet the achievable computational demands. To allow as\nwide as possible network architectures and avoid the gap between target and\nproxy dataset, we propose a Densely Connected NAS (DCNAS) framework, which\ndirectly searches the optimal network structures for the multi-scale\nrepresentations of visual information, over a large-scale target dataset.\nSpecifically, by connecting cells with each other using learnable weights, we\nintroduce a densely connected search space to cover an abundance of mainstream\nnetwork designs. Moreover, by combining both path-level and channel-level\nsampling strategies, we design a fusion module to reduce the memory consumption\nof ample search space. We demonstrate that the architecture obtained from our\nDCNAS algorithm achieves state-of-the-art performances on public semantic image\nsegmentation benchmarks, including 84.3% on Cityscapes, and 86.9% on PASCAL VOC\n2012. We also retain leading performances when evaluating the architecture on\nthe more challenging ADE20K and Pascal Context dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 13:21:33 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 15:03:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Xiong", ""], ["Xu", "Hongmin", ""], ["Mo", "Hong", ""], ["Tan", "Jianchao", ""], ["Yang", "Cheng", ""], ["Wang", "Lei", ""], ["Ren", "Wenqi", ""]]}, {"id": "2003.11904", "submitter": "Dongxian Wu", "authors": "Xianbin Lv, Dongxian Wu, Shu-Tao Xia", "title": "Matrix Smoothing: A Regularization for DNN with Transition Matrix under\n  Noisy Labels", "comments": "ICME 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks (DNNs) in the presence of noisy labels is an\nimportant and challenging task. Probabilistic modeling, which consists of a\nclassifier and a transition matrix, depicts the transformation from true labels\nto noisy labels and is a promising approach. However, recent probabilistic\nmethods directly apply transition matrix to DNN, neglect DNN's susceptibility\nto overfitting, and achieve unsatisfactory performance, especially under the\nuniform noise. In this paper, inspired by label smoothing, we proposed a novel\nmethod, in which a smoothed transition matrix is used for updating DNN, to\nrestrict the overfitting of DNN in probabilistic modeling. Our method is termed\nMatrix Smoothing. We also empirically demonstrate that our method not only\nimproves the robustness of probabilistic modeling significantly, but also even\nobtains a better estimation of the transition matrix.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 13:49:37 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Lv", "Xianbin", ""], ["Wu", "Dongxian", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2003.11928", "submitter": "Gui-Song Xia", "authors": "Fudong Wang and Nan Xue and Jin-Gang Yu and Gui-Song Xia", "title": "Zero-Assignment Constraint for Graph Matching with Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching (GM), as a longstanding problem in computer vision and pattern\nrecognition, still suffers from numerous cluttered outliers in practical\napplications. To address this issue, we present the zero-assignment constraint\n(ZAC) for approaching the graph matching problem in the presence of outliers.\nThe underlying idea is to suppress the matchings of outliers by assigning\nzero-valued vectors to the potential outliers in the obtained optimal\ncorrespondence matrix. We provide elaborate theoretical analysis to the\nproblem, i.e., GM with ZAC, and figure out that the GM problem with and without\noutliers are intrinsically different, which enables us to put forward a\nsufficient condition to construct valid and reasonable objective function.\nConsequently, we design an efficient outlier-robust algorithm to significantly\nreduce the incorrect or redundant matchings caused by numerous outliers.\nExtensive experiments demonstrate that our method can achieve the\nstate-of-the-art performance in terms of accuracy and efficiency, especially in\nthe presence of numerous outliers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 14:11:10 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Wang", "Fudong", ""], ["Xue", "Nan", ""], ["Yu", "Jin-Gang", ""], ["Xia", "Gui-Song", ""]]}, {"id": "2003.11942", "submitter": "Yantao Shen", "authors": "Yantao Shen, Yuanjun Xiong, Wei Xia, and Stefano Soatto", "title": "Towards Backward-Compatible Representation Learning", "comments": "Accepted to CVPR 2020 as oral, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a way to learn visual features that are compatible with previously\ncomputed ones even when they have different dimensions and are learned via\ndifferent neural network architectures and loss functions. Compatible means\nthat, if such features are used to compare images, then \"new\" features can be\ncompared directly to \"old\" features, so they can be used interchangeably. This\nenables visual search systems to bypass computing new features for all\npreviously seen images when updating the embedding models, a process known as\nbackfilling. Backward compatibility is critical to quickly deploy new embedding\nmodels that leverage ever-growing large-scale training datasets and\nimprovements in deep learning architectures and training methods. We propose a\nframework to train embedding models, called backward-compatible training (BCT),\nas a first step towards backward compatible representation learning. In\nexperiments on learning embeddings for face recognition, models trained with\nBCT successfully achieve backward compatibility without sacrificing accuracy,\nthus enabling backfill-free model updates of visual embeddings.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 14:34:09 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 14:44:11 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 06:59:07 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Shen", "Yantao", ""], ["Xiong", "Yuanjun", ""], ["Xia", "Wei", ""], ["Soatto", "Stefano", ""]]}, {"id": "2003.11973", "submitter": "Ziyi Zhao", "authors": "Ziyi Zhao, Haowen Fang, Zhao Jin, Qinru Qiu", "title": "GISNet: Graph-Based Information Sharing Network For Vehicle Trajectory\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trajectory prediction is a critical and challenging problem in the design\nof an autonomous driving system. Many AI-oriented companies, such as Google\nWaymo, Uber and DiDi, are investigating more accurate vehicle trajectory\nprediction algorithms. However, the prediction performance is governed by lots\nof entangled factors, such as the stochastic behaviors of surrounding vehicles,\nhistorical information of self-trajectory, and relative positions of neighbors,\netc. In this paper, we propose a novel graph-based information sharing network\n(GISNet) that allows the information sharing between the target vehicle and its\nsurrounding vehicles. Meanwhile, the model encodes the historical trajectory\ninformation of all the vehicles in the scene. Experiments are carried out on\nthe public NGSIM US-101 and I-80 Dataset and the prediction performance is\nmeasured by the Root Mean Square Error (RMSE). The quantitative and qualitative\nexperimental results show that our model significantly improves the trajectory\nprediction accuracy, by up to 50.00%, compared to existing models.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2020 03:24:31 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Zhao", "Ziyi", ""], ["Fang", "Haowen", ""], ["Jin", "Zhao", ""], ["Qiu", "Qinru", ""]]}, {"id": "2003.11988", "submitter": "Feng Shi", "authors": "Zhenyu Tang, Wei Zhao, Xingzhi Xie, Zheng Zhong, Feng Shi, Jun Liu,\n  Dinggang Shen", "title": "Severity Assessment of Coronavirus Disease 2019 (COVID-19) Using\n  Quantitative Features from Chest CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Chest computed tomography (CT) is recognized as an important tool\nfor COVID-19 severity assessment. As the number of affected patients increase\nrapidly, manual severity assessment becomes a labor-intensive task, and may\nlead to delayed treatment. Purpose: Using machine learning method to realize\nautomatic severity assessment (non-severe or severe) of COVID-19 based on chest\nCT images, and to explore the severity-related features from the resulting\nassessment model. Materials and Method: Chest CT images of 176 patients (age\n45.3$\\pm$16.5 years, 96 male and 80 female) with confirmed COVID-19 are used,\nfrom which 63 quantitative features, e.g., the infection volume/ratio of the\nwhole lung and the volume of ground-glass opacity (GGO) regions, are\ncalculated. A random forest (RF) model is trained to assess the severity\n(non-severe or severe) based on quantitative features. Importance of each\nquantitative feature, which reflects the correlation to the severity of\nCOVID-19, is calculated from the RF model. Results: Using three-fold cross\nvalidation, the RF model shows promising results, i.e., 0.933 of true positive\nrate, 0.745 of true negative rate, 0.875 of accuracy, and 0.91 of area under\nreceiver operating characteristic curve (AUC). The resulting importance of\nquantitative features shows that the volume and its ratio (with respect to the\nwhole lung volume) of ground glass opacity (GGO) regions are highly related to\nthe severity of COVID-19, and the quantitative features calculated from the\nright lung are more related to the severity assessment than those of the left\nlung. Conclusion: The RF based model can achieve automatic severity assessment\n(non-severe or severe) of COVID-19 infection, and the performance is promising.\nSeveral quantitative features, which have the potential to reflect the severity\nof COVID-19, were revealed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 15:49:32 GMT"}], "update_date": "2020-03-28", "authors_parsed": [["Tang", "Zhenyu", ""], ["Zhao", "Wei", ""], ["Xie", "Xingzhi", ""], ["Zhong", "Zheng", ""], ["Shi", "Feng", ""], ["Liu", "Jun", ""], ["Shen", "Dinggang", ""]]}, {"id": "2003.12022", "submitter": "Geoffrey French", "authors": "Geoff French, Avital Oliver, Tim Salimans", "title": "Milking CowMask for Semi-Supervised Image Classification", "comments": "11 pages, 2 figures, submitted to NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Consistency regularization is a technique for semi-supervised learning that\nunderlies a number of strong results for classification with few labeled data.\nIt works by encouraging a learned model to be robust to perturbations on\nunlabeled data. Here, we present a novel mask-based augmentation method called\nCowMask. Using it to provide perturbations for semi-supervised consistency\nregularization, we achieve a state-of-the-art result on ImageNet with 10%\nlabeled data, with a top-5 error of 8.76% and top-1 error of 26.06%. Moreover,\nwe do so with a method that is much simpler than many alternatives. We further\ninvestigate the behavior of CowMask for semi-supervised learning by running\nmany smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets,\nwhere we achieve results competitive with the state of the art, indicating that\nCowMask is widely applicable. We open source our code at\nhttps://github.com/google-research/google-research/tree/master/milking_cowmask\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 16:42:22 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 08:42:54 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 18:11:20 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["French", "Geoff", ""], ["Oliver", "Avital", ""], ["Salimans", "Tim", ""]]}, {"id": "2003.12025", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Hieu Pham, Ye Han, Andy Kuhl, Wade Kent and Lizhi Wang", "title": "Convolutional Neural Networks for Image-based Corn Kernel Detection and\n  Counting", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": "10.3390/s20092721", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise in-season corn grain yield estimates enable farmers to make real-time\naccurate harvest and grain marketing decisions minimizing possible losses of\nprofitability. A well developed corn ear can have up to 800 kernels, but\nmanually counting the kernels on an ear of corn is labor-intensive, time\nconsuming and prone to human error. From an algorithmic perspective, the\ndetection of the kernels from a single corn ear image is challenging due to the\nlarge number of kernels at different angles and very small distance among the\nkernels. In this paper, we propose a kernel detection and counting method based\non a sliding window approach. The proposed method detect and counts all corn\nkernels in a single corn ear image taken in uncontrolled lighting conditions.\nThe sliding window approach uses a convolutional neural network (CNN) for\nkernel detection. Then, a non-maximum suppression (NMS) is applied to remove\noverlapping detections. Finally, windows that are classified as kernel are\npassed to another CNN regression model for finding the (x,y) coordinates of the\ncenter of kernel image patches. Our experiments indicate that the proposed\nmethod can successfully detect the corn kernels with a low detection error and\nis also able to detect kernels on a batch of corn ears positioned at different\nangles.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 16:46:23 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 02:02:19 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Khaki", "Saeed", ""], ["Pham", "Hieu", ""], ["Han", "Ye", ""], ["Kuhl", "Andy", ""], ["Kent", "Wade", ""], ["Wang", "Lizhi", ""]]}, {"id": "2003.12039", "submitter": "Zachary Teed", "authors": "Zachary Teed and Jia Deng", "title": "RAFT: Recurrent All-Pairs Field Transforms for Optical Flow", "comments": "fixed a formatting issue, Eq 7. no change in content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Recurrent All-Pairs Field Transforms (RAFT), a new deep network\narchitecture for optical flow. RAFT extracts per-pixel features, builds\nmulti-scale 4D correlation volumes for all pairs of pixels, and iteratively\nupdates a flow field through a recurrent unit that performs lookups on the\ncorrelation volumes. RAFT achieves state-of-the-art performance. On KITTI, RAFT\nachieves an F1-all error of 5.10%, a 16% error reduction from the best\npublished result (6.10%). On Sintel (final pass), RAFT obtains an\nend-point-error of 2.855 pixels, a 30% error reduction from the best published\nresult (4.098 pixels). In addition, RAFT has strong cross-dataset\ngeneralization as well as high efficiency in inference time, training speed,\nand parameter count. Code is available at https://github.com/princeton-vl/RAFT.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:12:42 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 14:48:09 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 15:49:48 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Teed", "Zachary", ""], ["Deng", "Jia", ""]]}, {"id": "2003.12040", "submitter": "Qilei Chen", "authors": "Qilei Chen, Ping Liu, Jing Ni, Yu Cao, Benyuan Liu, Honggang Zhang", "title": "Pseudo-Labeling for Small Lesion Detection on Diabetic Retinopathy\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) is a primary cause of blindness in working-age\npeople worldwide. About 3 to 4 million people with diabetes become blind\nbecause of DR every year. Diagnosis of DR through color fundus images is a\ncommon approach to mitigate such problem. However, DR diagnosis is a difficult\nand time consuming task, which requires experienced clinicians to identify the\npresence and significance of many small features on high resolution images.\nConvolutional Neural Network (CNN) has proved to be a promising approach for\nautomatic biomedical image analysis recently. In this work, we investigate\nlesion detection on DR fundus images with CNN-based object detection methods.\nLesion detection on fundus images faces two unique challenges. The first one is\nthat our dataset is not fully labeled, i.e., only a subset of all lesion\ninstances are marked. Not only will these unlabeled lesion instances not\ncontribute to the training of the model, but also they will be mistakenly\ncounted as false negatives, leading the model move to the opposite direction.\nThe second challenge is that the lesion instances are usually very small,\nmaking them difficult to be found by normal object detectors. To address the\nfirst challenge, we introduce an iterative training algorithm for the\nsemi-supervised method of pseudo-labeling, in which a considerable number of\nunlabeled lesion instances can be discovered to boost the performance of the\nlesion detector. For the small size targets problem, we extend both the input\nsize and the depth of feature pyramid network (FPN) to produce a large CNN\nfeature map, which can preserve the detail of small lesions and thus enhance\nthe effectiveness of the lesion detector. The experimental results show that\nour proposed methods significantly outperform the baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:13:48 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chen", "Qilei", ""], ["Liu", "Ping", ""], ["Ni", "Jing", ""], ["Cao", "Yu", ""], ["Liu", "Benyuan", ""], ["Zhang", "Honggang", ""]]}, {"id": "2003.12041", "submitter": "Marcos Baptista Rios", "authors": "Marcos Baptista Rios, Roberto J. L\\'opez-Sastre, Fabian Caba Heilbron,\n  Jan van Gemert, F. Javier Acevedo-Rodr\\'iguez, and S. Maldonado-Basc\\'on", "title": "Rethinking Online Action Detection in Untrimmed Videos: A Novel Online\n  Evaluation Protocol", "comments": "Published at IEEE Access journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Online Action Detection (OAD) problem needs to be revisited. Unlike\ntraditional offline action detection approaches, where the evaluation metrics\nare clear and well established, in the OAD setting we find very few works and\nno consensus on the evaluation protocols to be used. In this work we propose to\nrethink the OAD scenario, clearly defining the problem itself and the main\ncharacteristics that the models which are considered online must comply with.\nWe also introduce a novel metric: the Instantaneous Accuracy ($IA$). This new\nmetric exhibits an \\emph{online} nature and solves most of the limitations of\nthe previous metrics. We conduct a thorough experimental evaluation on 3\nchallenging datasets, where the performance of various baseline methods is\ncompared to that of the state-of-the-art. Our results confirm the problems of\nthe previous evaluation protocols, and suggest that an IA-based protocol is\nmore adequate to the online scenario. The baselines models and a development\nkit with the novel evaluation protocol are publicly available:\nhttps://github.com/gramuah/ia.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:13:55 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Rios", "Marcos Baptista", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""], ["Heilbron", "Fabian Caba", ""], ["van Gemert", "Jan", ""], ["Acevedo-Rodr\u00edguez", "F. Javier", ""], ["Maldonado-Basc\u00f3n", "S.", ""]]}, {"id": "2003.12045", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Shubham Tulsiani, Saurabh Gupta, Ali Farhadi, Abhinav\n  Gupta", "title": "Use the Force, Luke! Learning to Predict Physical Forces by Simulating\n  Effects", "comments": "CVPR 2020 -- (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we humans look at a video of human-object interaction, we can not only\ninfer what is happening but we can even extract actionable information and\nimitate those interactions. On the other hand, current recognition or geometric\napproaches lack the physicality of action representation. In this paper, we\ntake a step towards a more physical understanding of actions. We address the\nproblem of inferring contact points and the physical forces from videos of\nhumans interacting with objects. One of the main challenges in tackling this\nproblem is obtaining ground-truth labels for forces. We sidestep this problem\nby instead using a physics simulator for supervision. Specifically, we use a\nsimulator to predict effects and enforce that estimated forces must lead to the\nsame effect as depicted in the video. Our quantitative and qualitative results\nshow that (a) we can predict meaningful forces from videos whose effects lead\nto accurate imitation of the motions observed, (b) by jointly optimizing for\ncontact point and force prediction, we can improve the performance on both\ntasks in comparison to independent training, and (c) we can learn a\nrepresentation from this model that generalizes to novel objects using few shot\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:20:23 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Ehsani", "Kiana", ""], ["Tulsiani", "Shubham", ""], ["Gupta", "Saurabh", ""], ["Farhadi", "Ali", ""], ["Gupta", "Abhinav", ""]]}, {"id": "2003.12047", "submitter": "Yuda Qiu", "authors": "Yuda Qiu, Zhangyang Xiong, Kai Han, Zhongyuan Wang, Zixiang Xiong,\n  Xiaoguang Han", "title": "Learning Inverse Rendering of Faces from Real-world Videos", "comments": "First two authors contributed equally.\n  Code:https://github.com/RudyQ/InverseFaceRender", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the problem of inverse rendering of real face\nimages. Existing methods decompose a face image into three components (albedo,\nnormal, and illumination) by supervised training on synthetic face data.\nHowever, due to the domain gap between real and synthetic face images, a model\ntrained on synthetic data often does not generalize well to real data.\nMeanwhile, since no ground truth for any component is available for real\nimages, it is not feasible to conduct supervised learning on real face images.\nTo alleviate this problem, we propose a weakly supervised training approach to\ntrain our model on real face videos, based on the assumption of consistency of\nalbedo and normal across different frames, thus bridging the gap between real\nand synthetic face images. In addition, we introduce a learning framework,\ncalled IlluRes-SfSNet, to further extract the residual map to capture the\nglobal illumination effects that give the fine details that are largely ignored\nin existing methods. Our network is trained on both real and synthetic data,\nbenefiting from both. We comprehensively evaluate our methods on various\nbenchmarks, obtaining better inverse rendering results than the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:26:40 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Qiu", "Yuda", ""], ["Xiong", "Zhangyang", ""], ["Han", "Kai", ""], ["Wang", "Zhongyuan", ""], ["Xiong", "Zixiang", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2003.12056", "submitter": "Chenxi Liu", "authors": "Chenxi Liu, Piotr Doll\\'ar, Kaiming He, Ross Girshick, Alan Yuille,\n  Saining Xie", "title": "Are Labels Necessary for Neural Architecture Search?", "comments": "To appear in ECCV 2020 as spotlight. Code release:\n  https://github.com/facebookresearch/unnas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing neural network architectures in computer vision -- whether designed\nby humans or by machines -- were typically found using both images and their\nassociated labels. In this paper, we ask the question: can we find high-quality\nneural architectures using only images, but no human-annotated labels? To\nanswer this question, we first define a new setup called Unsupervised Neural\nArchitecture Search (UnNAS). We then conduct two sets of experiments. In\nsample-based experiments, we train a large number (500) of diverse\narchitectures with either supervised or unsupervised objectives, and find that\nthe architecture rankings produced with and without labels are highly\ncorrelated. In search-based experiments, we run a well-established NAS\nalgorithm (DARTS) using various unsupervised objectives, and report that the\narchitectures searched without labels can be competitive to their counterparts\nsearched with labels. Together, these results reveal the potentially surprising\nfinding that labels are not necessary, and the image statistics alone may be\nsufficient to identify good neural architectures.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:55:16 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 15:18:16 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Liu", "Chenxi", ""], ["Doll\u00e1r", "Piotr", ""], ["He", "Kaiming", ""], ["Girshick", "Ross", ""], ["Yuille", "Alan", ""], ["Xie", "Saining", ""]]}, {"id": "2003.12058", "submitter": "Sarah Pratt", "authors": "Sarah Pratt, Mark Yatskar, Luca Weihs, Ali Farhadi, Aniruddha Kembhavi", "title": "Grounded Situation Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Grounded Situation Recognition (GSR), a task that requires\nproducing structured semantic summaries of images describing: the primary\nactivity, entities engaged in the activity with their roles (e.g. agent, tool),\nand bounding-box groundings of entities. GSR presents important technical\nchallenges: identifying semantic saliency, categorizing and localizing a large\nand diverse set of entities, overcoming semantic sparsity, and disambiguating\nroles. Moreover, unlike in captioning, GSR is straightforward to evaluate. To\nstudy this new task we create the Situations With Groundings (SWiG) dataset\nwhich adds 278,336 bounding-box groundings to the 11,538 entity classes in the\nimsitu dataset. We propose a Joint Situation Localizer and find that jointly\npredicting situations and groundings with end-to-end training handily\noutperforms independent training on the entire grounding metric suite with\nrelative gains between 8% and 32%. Finally, we show initial findings on three\nexciting future directions enabled by our models: conditional querying, visual\nchaining, and grounded semantic aware image retrieval. Code and data available\nat https://prior.allenai.org/projects/gsr.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:57:52 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Pratt", "Sarah", ""], ["Yatskar", "Mark", ""], ["Weihs", "Luca", ""], ["Farhadi", "Ali", ""], ["Kembhavi", "Aniruddha", ""]]}, {"id": "2003.12059", "submitter": "Kai Han", "authors": "Shuda Li, Kai Han, Theo W. Costain, Henry Howard-Jenkins, and Victor\n  Prisacariu", "title": "Correspondence Networks with Adaptive Neighbourhood Consensus", "comments": "CVPR 2020. Project page: https://ancnet.avlcode.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the task of establishing dense visual\ncorrespondences between images containing objects of the same category. This is\na challenging task due to large intra-class variations and a lack of dense\npixel level annotations. We propose a convolutional neural network\narchitecture, called adaptive neighbourhood consensus network (ANC-Net), that\ncan be trained end-to-end with sparse key-point annotations, to handle this\nchallenge. At the core of ANC-Net is our proposed non-isotropic 4D convolution\nkernel, which forms the building block for the adaptive neighbourhood consensus\nmodule for robust matching. We also introduce a simple and efficient\nmulti-scale self-similarity module in ANC-Net to make the learned feature\nrobust to intra-class variations. Furthermore, we propose a novel orthogonal\nloss that can enforce the one-to-one matching constraint. We thoroughly\nevaluate the effectiveness of our method on various benchmarks, where it\nsubstantially outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:58:09 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Li", "Shuda", ""], ["Han", "Kai", ""], ["Costain", "Theo W.", ""], ["Howard-Jenkins", "Henry", ""], ["Prisacariu", "Victor", ""]]}, {"id": "2003.12060", "submitter": "Bin Liu", "authors": "Bin Liu, Yue Cao, Yutong Lin, Qi Li, Zheng Zhang, Mingsheng Long, Han\n  Hu", "title": "Negative Margin Matters: Understanding Margin in Few-shot Classification", "comments": "Code is available at https://github.com/bl0/negative-margin.few-shot", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a negative margin loss to metric learning based\nfew-shot learning methods. The negative margin loss significantly outperforms\nregular softmax loss, and achieves state-of-the-art accuracy on three standard\nfew-shot classification benchmarks with few bells and whistles. These results\nare contrary to the common practice in the metric learning field, that the\nmargin is zero or positive. To understand why the negative margin loss performs\nwell for the few-shot classification, we analyze the discriminability of\nlearned features w.r.t different margins for training and novel classes, both\nempirically and theoretically. We find that although negative margin reduces\nthe feature discriminability for training classes, it may also avoid falsely\nmapping samples of the same novel class to multiple peaks or clusters, and thus\nbenefit the discrimination of novel classes. Code is available at\nhttps://github.com/bl0/negative-margin.few-shot.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:59:05 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Liu", "Bin", ""], ["Cao", "Yue", ""], ["Lin", "Yutong", ""], ["Li", "Qi", ""], ["Zhang", "Zheng", ""], ["Long", "Mingsheng", ""], ["Hu", "Han", ""]]}, {"id": "2003.12063", "submitter": "Yihong Chen", "authors": "Yihong Chen, Yue Cao, Han Hu, Liwei Wang", "title": "Memory Enhanced Global-Local Aggregation for Video Object Detection", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do humans recognize an object in a piece of video? Due to the\ndeteriorated quality of single frame, it may be hard for people to identify an\noccluded object in this frame by just utilizing information within one image.\nWe argue that there are two important cues for humans to recognize objects in\nvideos: the global semantic information and the local localization information.\nRecently, plenty of methods adopt the self-attention mechanisms to enhance the\nfeatures in key frame with either global semantic information or local\nlocalization information. In this paper we introduce memory enhanced\nglobal-local aggregation (MEGA) network, which is among the first trials that\ntakes full consideration of both global and local information. Furthermore,\nempowered by a novel and carefully-designed Long Range Memory (LRM) module, our\nproposed MEGA could enable the key frame to get access to much more content\nthan any previous methods. Enhanced by these two sources of information, our\nmethod achieves state-of-the-art performance on ImageNet VID dataset. Code is\navailable at \\url{https://github.com/Scalsol/mega.pytorch}.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:59:38 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Chen", "Yihong", ""], ["Cao", "Yue", ""], ["Hu", "Han", ""], ["Wang", "Liwei", ""]]}, {"id": "2003.12065", "submitter": "Wen Liu", "authors": "Zhaoyang Sun, Wenxuan Liu, Feng Liu, Ryan Wen Liu, Shengwu Xiong", "title": "Local Facial Makeup Transfer via Disentangled Representation", "comments": "There's something wrong with the experiment. It's not complete", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial makeup transfer aims to render a non-makeup face image in an arbitrary\ngiven makeup one while preserving face identity. The most advanced method\nseparates makeup style information from face images to realize makeup transfer.\nHowever, makeup style includes several semantic clear local styles which are\nstill entangled together. In this paper, we propose a novel unified adversarial\ndisentangling network to further decompose face images into four independent\ncomponents, i.e., personal identity, lips makeup style, eyes makeup style and\nface makeup style. Owing to the further disentangling of makeup style, our\nmethod can not only control the degree of global makeup style, but also\nflexibly regulate the degree of local makeup styles which any other approaches\ncan't do. For makeup removal, different from other methods which regard makeup\nremoval as the reverse process of makeup, we integrate the makeup transfer with\nthe makeup removal into one uniform framework and obtain multiple makeup\nremoval results. Extensive experiments have demonstrated that our approach can\nproduce more realistic and accurate makeup transfer results compared to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 00:25:13 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 01:22:02 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Sun", "Zhaoyang", ""], ["Liu", "Wenxuan", ""], ["Liu", "Feng", ""], ["Liu", "Ryan Wen", ""], ["Xiong", "Shengwu", ""]]}, {"id": "2003.12103", "submitter": "Azadeh Nazemi", "authors": "Niloofar Tavakolian, Azadeh Nazemi, Donal Fitzpatrick", "title": "Real-time information retrieval from Identity cards", "comments": "6pages,10 figures,conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Information is frequently retrieved from valid personal ID cards by the\nauthorised organisation to address different purposes. The successful\ninformation retrieval (IR) depends on the accuracy and timing process. A\nprocess which necessitates a long time to respond is frustrating for both sides\nin the exchange of data. This paper aims to propose a series of\nstate-of-the-art methods for the journey of an Identification card (ID) from\nthe scanning or capture phase to the point before Optical character recognition\n(OCR). The key factors for this proposal are the accuracy and speed of the\nprocess during the journey. The experimental results of this research prove\nthat utilising the methods based on deep learning, such as Efficient and\nAccurate Scene Text (EAST) detector and Deep Neural Network (DNN) for face\ndetection, instead of traditional methods increase the efficiency considerably.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 18:37:29 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Tavakolian", "Niloofar", ""], ["Nazemi", "Azadeh", ""], ["Fitzpatrick", "Donal", ""]]}, {"id": "2003.12122", "submitter": "Kyungjun Lee", "authors": "Kyungjun Lee, Daisuke Sato, Saki Asakawa, Hernisa Kacorri, Chieko\n  Asakawa", "title": "Pedestrian Detection with Wearable Cameras for the Blind: A Two-way\n  Perspective", "comments": "The 2020 ACM CHI Conference on Human Factors in Computing Systems\n  (CHI 2020)", "journal-ref": null, "doi": "10.1145/3313831.3376398", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people have limited access to information about their surroundings,\nwhich is important for ensuring one's safety, managing social interactions, and\nidentifying approaching pedestrians. With advances in computer vision, wearable\ncameras can provide equitable access to such information. However, the\nalways-on nature of these assistive technologies poses privacy concerns for\nparties that may get recorded. We explore this tension from both perspectives,\nthose of sighted passersby and blind users, taking into account camera\nvisibility, in-person versus remote experience, and extracted visual\ninformation. We conduct two studies: an online survey with MTurkers (N=206) and\nan in-person experience study between pairs of blind (N=10) and sighted (N=40)\nparticipants, where blind participants wear a working prototype for pedestrian\ndetection and pass by sighted participants. Our results suggest that both of\nthe perspectives of users and bystanders and the several factors mentioned\nabove need to be carefully considered to mitigate potential social tensions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:34:54 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 19:17:38 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lee", "Kyungjun", ""], ["Sato", "Daisuke", ""], ["Asakawa", "Saki", ""], ["Kacorri", "Hernisa", ""], ["Asakawa", "Chieko", ""]]}, {"id": "2003.12125", "submitter": "Shiyi Lan", "authors": "Shiyi Lan, Zhou Ren, Yi Wu, Larry S. Davis, Gang Hua", "title": "SaccadeNet: A Fast and Accurate Object Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an essential step towards holistic scene understanding.\nMost existing object detection algorithms attend to certain object areas once\nand then predict the object locations. However, neuroscientists have revealed\nthat humans do not look at the scene in fixed steadiness. Instead, human eyes\nmove around, locating informative parts to understand the object location. This\nactive perceiving movement process is called \\textit{saccade}.\n  %In this paper, Inspired by such mechanism, we propose a fast and accurate\nobject detector called \\textit{SaccadeNet}. It contains four main modules, the\n\\cenam, the \\coram, the \\atm, and the \\aggatt, which allows it to attend to\ndifferent informative object keypoints, and predict object locations from\ncoarse to fine. The \\coram~is used only during training to extract more\ninformative corner features which brings free-lunch performance boost. On the\nMS COCO dataset, we achieve the performance of 40.4\\% mAP at 28 FPS and 30.5\\%\nmAP at 118 FPS. Among all the real-time object detectors, %that can run faster\nthan 25 FPS, our SaccadeNet achieves the best detection performance, which\ndemonstrates the effectiveness of the proposed detection mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:47:17 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Lan", "Shiyi", ""], ["Ren", "Zhou", ""], ["Wu", "Yi", ""], ["Davis", "Larry S.", ""], ["Hua", "Gang", ""]]}, {"id": "2003.12128", "submitter": "Ruben van Bergen", "authors": "Ruben S. van Bergen, Nikolaus Kriegeskorte", "title": "Going in circles is the way forward: the role of recurrence in visual\n  inference", "comments": null, "journal-ref": null, "doi": "10.1016/j.conb.2020.11.009", "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological visual systems exhibit abundant recurrent connectivity.\nState-of-the-art neural network models for visual recognition, by contrast,\nrely heavily or exclusively on feedforward computation. Any finite-time\nrecurrent neural network (RNN) can be unrolled along time to yield an\nequivalent feedforward neural network (FNN). This important insight suggests\nthat computational neuroscientists may not need to engage recurrent\ncomputation, and that computer-vision engineers may be limiting themselves to a\nspecial case of FNN if they build recurrent models. Here we argue, to the\ncontrary, that FNNs are a special case of RNNs and that computational\nneuroscientists and engineers should engage recurrence to understand how brains\nand machines can (1) achieve greater and more flexible computational depth, (2)\ncompress complex computations into limited hardware, (3) integrate priors and\npriorities into visual inference through expectation and attention, (4) exploit\nsequential dependencies in their data for better inference and prediction, and\n(5) leverage the power of iterative computation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:53:05 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 19:27:18 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:33:25 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["van Bergen", "Ruben S.", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "2003.12137", "submitter": "Samir Sen", "authors": "Trevor Tsue, Samir Sen, Jason Li", "title": "Cycle Text-To-Image GAN with BERT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We explore novel approaches to the task of image generation from their\nrespective captions, building on state-of-the-art GAN architectures.\nParticularly, we baseline our models with the Attention-based GANs that learn\nattention mappings from words to image features. To better capture the features\nof the descriptions, we then built a novel cyclic design that learns an inverse\nfunction to maps the image back to original caption. Additionally, we\nincorporated recently developed BERT pretrained word embeddings as our initial\ntext featurizer and observe a noticeable improvement in qualitative and\nquantitative performance compared to the Attention GAN baseline.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 20:17:55 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Tsue", "Trevor", ""], ["Sen", "Samir", ""], ["Li", "Jason", ""]]}, {"id": "2003.12163", "submitter": "Yunhe Xie", "authors": "Yunhe Xie and Gregory Sharp and David P. Gierga and Theodore S. Hong\n  and Thomas Bortfeld and Kongbin Kang", "title": "An improved 3D region detection network: automated detection of the 12th\n  thoracic vertebra in image guided radiation therapy", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract. Image guidance has been widely used in radiation therapy. Correctly\nidentifying anatomical landmarks, like the 12th thoracic vertebra (T12), is the\nkey to success. Until recently, the detection of those landmarks still requires\ntedious manual inspections and annotations; and superior-inferior misalignment\nto the wrong vertebral body is still relatively common in image guided\nradiation therapy. It is necessary to develop an automated approach to detect\nthose landmarks from images. There are three major challenges to identify T12\nvertebra automatically: 1) subtle difference in the structures with high\nsimilarity, 2) limited annotated training data, and 3) high memory usage of 3D\nnetworks.\n  Abstract. In this study, we propose a novel 3D full convolutional network\n(FCN) that is trained to detect anatomical structures from 3D volumetric data,\nrequiring only a small amount of training data. Comparing with existing\napproaches, the network architecture, target generation and loss functions were\nsignificantly improved to address the challenges specific to medical images. In\nour experiments, the proposed network, which was trained from a small amount of\nannotated images, demonstrated the capability of accurately detecting\nstructures with high similarity. Furthermore, the trained network showed the\ncapability of cross-modality learning. This is meaningful in the situation\nwhere image annotations in one modality are easier to obtain than others. The\ncross-modality learning ability also indicated that the learned features were\nrobust to noise in different image modalities. In summary, our approach has a\ngreat potential to be integrated into the clinical workflow to improve the\nsafety of image guided radiation therapy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 21:41:00 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Xie", "Yunhe", ""], ["Sharp", "Gregory", ""], ["Gierga", "David P.", ""], ["Hong", "Theodore S.", ""], ["Bortfeld", "Thomas", ""], ["Kang", "Kongbin", ""]]}, {"id": "2003.12181", "submitter": "Gopal Sharma", "authors": "Gopal Sharma, Difan Liu, Subhransu Maji, Evangelos Kalogerakis,\n  Siddhartha Chaudhuri, Radom\\'ir M\\v{e}ch", "title": "ParSeNet: A Parametric Surface Fitting Network for 3D Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel, end-to-end trainable, deep network called ParSeNet that\ndecomposes a 3D point cloud into parametric surface patches, including B-spline\npatches as well as basic geometric primitives. ParSeNet is trained on a\nlarge-scale dataset of man-made 3D shapes and captures high-level semantic\npriors for shape decomposition. It handles a much richer class of primitives\nthan prior work, and allows us to represent surfaces with higher fidelity. It\nalso produces repeatable and robust parametrizations of a surface compared to\npurely geometric approaches. We present extensive experiments to validate our\napproach against analytical and learning-based alternatives. Our source code is\npublicly available at: https://hippogriff.github.io/parsenet.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 22:54:18 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 02:59:16 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 16:47:09 GMT"}, {"version": "v4", "created": "Sun, 26 Jul 2020 02:08:57 GMT"}, {"version": "v5", "created": "Tue, 22 Sep 2020 16:05:16 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Sharma", "Gopal", ""], ["Liu", "Difan", ""], ["Maji", "Subhransu", ""], ["Kalogerakis", "Evangelos", ""], ["Chaudhuri", "Siddhartha", ""], ["M\u011bch", "Radom\u00edr", ""]]}, {"id": "2003.12185", "submitter": "Sathyanarayanan Aakur", "authors": "Sathyanarayanan N. Aakur, Sudeep Sarkar", "title": "Action Localization through Continual Predictive Learning", "comments": "18 pages, 4 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of action recognition involves locating the action in the video,\nboth over time and spatially in the image. The dominant current approaches use\nsupervised learning to solve this problem, and require large amounts of\nannotated training data, in the form of frame-level bounding box annotations\naround the region of interest. In this paper, we present a new approach based\non continual learning that uses feature-level predictions for self-supervision.\nIt does not require any training annotations in terms of frame-level bounding\nboxes. The approach is inspired by cognitive models of visual event perception\nthat propose a prediction-based approach to event understanding. We use a stack\nof LSTMs coupled with CNN encoder, along with novel attention mechanisms, to\nmodel the events in the video and use this model to predict high-level features\nfor the future frames. The prediction errors are used to continuously learn the\nparameters of the models. This self-supervised framework is not complicated as\nother approaches but is very effective in learning robust visual\nrepresentations for both labeling and localization. It should be noted that the\napproach outputs in a streaming fashion, requiring only a single pass through\nthe video, making it amenable for real-time processing. We demonstrate this on\nthree datasets - UCF Sports, JHMDB, and THUMOS'13 and show that the proposed\napproach outperforms weakly-supervised and unsupervised baselines and obtains\ncompetitive performance compared to fully supervised baselines. Finally, we\nshow that the proposed framework can generalize to egocentric videos and obtain\nstate-of-the-art results in unsupervised gaze prediction.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 23:32:43 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Aakur", "Sathyanarayanan N.", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "2003.12197", "submitter": "Vishnu Naresh Boddeti", "authors": "Joshua J. Engelsma and Anil K. Jain and Vishnu Naresh Boddeti", "title": "HERS: Homomorphically Encrypted Representation Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to search for a probe (or query) image representation\nagainst a large gallery in the encrypted domain. We require that the probe and\ngallery images be represented in terms of a fixed-length representation, which\nis typical for representations obtained from learned networks. Our encryption\nscheme is agnostic to how the fixed-length representation is obtained and can\ntherefore be applied to any fixed-length representation in any application\ndomain. Our method, dubbed HERS (Homomorphically Encrypted Representation\nSearch), operates by (i) compressing the representation towards its estimated\nintrinsic dimensionality with minimal loss of accuracy (ii) encrypting the\ncompressed representation using the proposed fully homomorphic encryption\nscheme, and (iii) efficiently searching against a gallery of encrypted\nrepresentations directly in the encrypted domain, without decrypting them.\nNumerical results on large galleries of face, fingerprint, and object datasets\nsuch as ImageNet show that, for the first time, accurate and fast image search\nwithin the encrypted domain is feasible at scale (500 seconds; $275\\times$\nspeed up over state-of-the-art for encrypted search against a gallery of 100\nmillion).\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 01:10:54 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 03:22:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Engelsma", "Joshua J.", ""], ["Jain", "Anil K.", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "2003.12224", "submitter": "Zhizheng Zhang", "authors": "Zhizheng Zhang, Cuiling Lan, Wenjun Zeng, Zhibo Chen", "title": "Multi-Granularity Reference-Aided Attentive Feature Aggregation for\n  Video-based Person Re-identification", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (reID) aims at matching the same person\nacross video clips. It is a challenging task due to the existence of redundancy\namong frames, newly revealed appearance, occlusion, and motion blurs. In this\npaper, we propose an attentive feature aggregation module, namely\nMulti-Granularity Reference-aided Attentive Feature Aggregation (MG-RAFA), to\ndelicately aggregate spatio-temporal features into a discriminative video-level\nfeature representation. In order to determine the contribution/importance of a\nspatial-temporal feature node, we propose to learn the attention from a global\nview with convolutional operations. Specifically, we stack its relations, i.e.,\npairwise correlations with respect to a representative set of reference feature\nnodes (S-RFNs) that represents global video information, together with the\nfeature itself to infer the attention. Moreover, to exploit the semantics of\ndifferent levels, we propose to learn multi-granularity attentions based on the\nrelations captured at different granularities. Extensive ablation studies\ndemonstrate the effectiveness of our attentive feature aggregation module\nMG-RAFA. Our framework achieves the state-of-the-art performance on three\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 03:49:21 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhang", "Zhizheng", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Chen", "Zhibo", ""]]}, {"id": "2003.12230", "submitter": "Yang Li", "authors": "Yang Li, Alja\\v{z} Bo\\v{z}i\\v{c}, Tianwei Zhang, Yanli Ji, Tatsuya\n  Harada, Matthias Nie{\\ss}ner", "title": "Learning to Optimize Non-Rigid Tracking", "comments": "Accepted to CVPR'2020 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the widespread solutions for non-rigid tracking has a nested-loop\nstructure: with Gauss-Newton to minimize a tracking objective in the outer\nloop, and Preconditioned Conjugate Gradient (PCG) to solve a sparse linear\nsystem in the inner loop. In this paper, we employ learnable optimizations to\nimprove tracking robustness and speed up solver convergence. First, we upgrade\nthe tracking objective by integrating an alignment data term on deep features\nwhich are learned end-to-end through CNN. The new tracking objective can\ncapture the global deformation which helps Gauss-Newton to jump over local\nminimum, leading to robust tracking on large non-rigid motions. Second, we\nbridge the gap between the preconditioning technique and learning method by\nintroducing a ConditionNet which is trained to generate a preconditioner such\nthat PCG can converge within a small number of steps. Experimental results\nindicate that the proposed learning method converges faster than the original\nPCG by a large margin.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 04:40:57 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Li", "Yang", ""], ["Bo\u017ei\u010d", "Alja\u017e", ""], ["Zhang", "Tianwei", ""], ["Ji", "Yanli", ""], ["Harada", "Tatsuya", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2003.12237", "submitter": "Shuhao Cui", "authors": "Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, Qi\n  Tian", "title": "Towards Discriminability and Diversity: Batch Nuclear-norm Maximization\n  under Label Insufficient Situations", "comments": "Accepted to CVPR 2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning of the deep networks largely relies on the data with\nhuman-annotated labels. In some label insufficient situations, the performance\ndegrades on the decision boundary with high data density. A common solution is\nto directly minimize the Shannon Entropy, but the side effect caused by entropy\nminimization, i.e., reduction of the prediction diversity, is mostly ignored.\nTo address this issue, we reinvestigate the structure of classification output\nmatrix of a randomly selected data batch. We find by theoretical analysis that\nthe prediction discriminability and diversity could be separately measured by\nthe Frobenius-norm and rank of the batch output matrix. Besides, the\nnuclear-norm is an upperbound of the Frobenius-norm, and a convex approximation\nof the matrix rank. Accordingly, to improve both discriminability and\ndiversity, we propose Batch Nuclear-norm Maximization (BNM) on the output\nmatrix. BNM could boost the learning under typical label insufficient learning\nscenarios, such as semi-supervised learning, domain adaptation and open domain\nrecognition. On these tasks, extensive experimental results show that BNM\noutperforms competitors and works well with existing well-known methods. The\ncode is available at https://github.com/cuishuhao/BNM.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 05:04:24 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Cui", "Shuhao", ""], ["Wang", "Shuhui", ""], ["Zhuo", "Junbao", ""], ["Li", "Liang", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2003.12238", "submitter": "Chaoyang He", "authors": "Chaoyang He, Haishan Ye, Li Shen, Tong Zhang", "title": "MiLeNAS: Efficient Neural Architecture Search via Mixed-Level\n  Reformulation", "comments": "This paper is published in CVPR 2020 (IEEE/CVF Conference on Computer\n  Vision and Pattern Recognition 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recently proposed methods for Neural Architecture Search (NAS) can be\nformulated as bilevel optimization. For efficient implementation, its solution\nrequires approximations of second-order methods. In this paper, we demonstrate\nthat gradient errors caused by such approximations lead to suboptimality, in\nthe sense that the optimization procedure fails to converge to a (locally)\noptimal solution. To remedy this, this paper proposes \\mldas, a mixed-level\nreformulation for NAS that can be optimized efficiently and reliably. It is\nshown that even when using a simple first-order method on the mixed-level\nformulation, \\mldas\\ can achieve a lower validation error for NAS problems.\nConsequently, architectures obtained by our method achieve consistently higher\naccuracies than those obtained from bilevel optimization. Moreover, \\mldas\\\nproposes a framework beyond DARTS. It is upgraded via model size-based search\nand early stopping strategies to complete the search process in around 5 hours.\nExtensive experiments within the convolutional architecture search space\nvalidate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 05:06:54 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["He", "Chaoyang", ""], ["Ye", "Haishan", ""], ["Shen", "Li", ""], ["Zhang", "Tong", ""]]}, {"id": "2003.12243", "submitter": "Jin Chen", "authors": "Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, Jian Sun", "title": "Dynamic Region-Aware Convolution", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new convolution called Dynamic Region-Aware Convolution\n(DRConv), which can automatically assign multiple filters to corresponding\nspatial regions where features have similar representation. In this way, DRConv\noutperforms standard convolution in modeling semantic variations. Standard\nconvolutional layer can increase the number of filers to extract more visual\nelements but results in high computational cost. More gracefully, our DRConv\ntransfers the increasing channel-wise filters to spatial dimension with\nlearnable instructor, which not only improve representation ability of\nconvolution, but also maintains computational cost and the\ntranslation-invariance as standard convolution dose. DRConv is an effective and\nelegant method for handling complex and variable spatial information\ndistribution. It can substitute standard convolution in any existing networks\nfor its plug-and-play property, especially to power convolution layers in\nefficient networks. We evaluate DRConv on a wide range of models (MobileNet\nseries, ShuffleNetV2, etc.) and tasks (Classification, Face Recognition,\nDetection and Segmentation). On ImageNet classification, DRConv-based\nShuffleNetV2-0.5x achieves state-of-the-art performance of 67.1% at 46M\nmultiply-adds level with 6.3% relative improvement.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 05:49:57 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 14:46:20 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 16:28:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Jin", ""], ["Wang", "Xijun", ""], ["Guo", "Zichao", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2003.12244", "submitter": "Hadi Mansourifar", "authors": "Hadi Mansourifar, Weidong Shi", "title": "One-Shot GAN Generated Fake Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake face detection is a significant challenge for intelligent systems as\ngenerative models become more powerful every single day. As the quality of fake\nfaces increases, the trained models become more and more inefficient to detect\nthe novel fake faces, since the corresponding training data is considered\noutdated. In this case, robust One-Shot learning methods is more compatible\nwith the requirements of changeable training data. In this paper, we propose a\nuniversal One-Shot GAN generated fake face detection method which can be used\nin significantly different areas of anomaly detection. The proposed method is\nbased on extracting out-of-context objects from faces via scene understanding\nmodels. To do so, we use state of the art scene understanding and object\ndetection methods as a pre-processing tool to detect the weird objects in the\nface. Second, we create a bag of words given all the detected out-of-context\nobjects per all training data. This way, we transform each image into a sparse\nvector where each feature represents the confidence score related to each\ndetected object in the image. Our experiments show that, we can discriminate\nfake faces from real ones in terms of out-of-context features. It means that,\ndifferent sets of objects are detected in fake faces comparing to real ones\nwhen we analyze them with scene understanding and object detection models. We\nprove that, the proposed method can outperform previous methods based on our\nexperiments on Style-GAN generated fake faces.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 05:51:14 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Mansourifar", "Hadi", ""], ["Shi", "Weidong", ""]]}, {"id": "2003.12255", "submitter": "Xiaomin Zhou", "authors": "Xiaomin Zhou, Chen Li, Md Mamunur Rahaman, Yudong Yao, Shiliang Ai,\n  Changhao Sun, Xiaoyan Li, Qian Wang, Tao Jiang", "title": "A Comprehensive Review for Breast Histopathology Image Analysis Using\n  Classical and Deep Neural Networks", "comments": "25 pages,19 figures", "journal-ref": "IEEE Access, Vol.8, page:90931-90956, 2020", "doi": "10.1109/ACCESS.2020.2993788", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is one of the most common and deadliest cancers among women.\nSince histopathological images contain sufficient phenotypic information, they\nplay an indispensable role in the diagnosis and treatment of breast cancers. To\nimprove the accuracy and objectivity of Breast Histopathological Image Analysis\n(BHIA), Artificial Neural Network (ANN) approaches are widely used in the\nsegmentation and classification tasks of breast histopathological images. In\nthis review, we present a comprehensive overview of the BHIA techniques based\non ANNs. First of all, we categorize the BHIA systems into classical and deep\nneural networks for in-depth investigation. Then, the relevant studies based on\nBHIA systems are presented. After that, we analyze the existing models to\ndiscover the most suitable algorithms. Finally, publicly accessible datasets,\nalong with their download links, are provided for the convenience of future\nresearchers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 06:53:41 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 12:48:38 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhou", "Xiaomin", ""], ["Li", "Chen", ""], ["Rahaman", "Md Mamunur", ""], ["Yao", "Yudong", ""], ["Ai", "Shiliang", ""], ["Sun", "Changhao", ""], ["Li", "Xiaoyan", ""], ["Wang", "Qian", ""], ["Jiang", "Tao", ""]]}, {"id": "2003.12263", "submitter": "Hirokatsu Kataoka", "authors": "Munetaka Minoguchi, Ken Okayama, Yutaka Satoh, Hirokatsu Kataoka", "title": "Weakly Supervised Dataset Collection for Robust Person Detection", "comments": "Project page:\n  https://github.com/cvpaperchallenge/FashionCultureDataBase_DLoader The paper\n  is under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To construct an algorithm that can provide robust person detection, we\npresent a dataset with over 8 million images that was produced in a weakly\nsupervised manner. Through labor-intensive human annotation, the person\ndetection research community has produced relatively small datasets containing\non the order of 100,000 images, such as the EuroCity Persons dataset, which\nincludes 240,000 bounding boxes. Therefore, we have collected 8.7 million\nimages of persons based on a two-step collection process, namely person\ndetection with an existing detector and data refinement for false positive\nsuppression. According to the experimental results, the Weakly Supervised\nPerson Dataset (WSPD) is simple yet effective for person detection\npre-training. In the context of pre-trained person detection algorithms, our\nWSPD pre-trained model has 13.38 and 6.38% better accuracy than the same model\ntrained on the fully supervised ImageNet and EuroCity Persons datasets,\nrespectively, when verified with the Caltech Pedestrian.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 07:36:59 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 07:35:26 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Minoguchi", "Munetaka", ""], ["Okayama", "Ken", ""], ["Satoh", "Yutaka", ""], ["Kataoka", "Hirokatsu", ""]]}, {"id": "2003.12267", "submitter": "Yifang Men", "authors": "Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, Zhouhui Lian", "title": "Controllable Person Image Synthesis with Attribute-Decomposed GAN", "comments": "Accepted by CVPR 2020 (Oral). Project Page:\n  https://menyifang.github.io/projects/ADGAN/ADGAN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Attribute-Decomposed GAN, a novel generative model\nfor controllable person image synthesis, which can produce realistic person\nimages with desired human attributes (e.g., pose, head, upper clothes and\npants) provided in various source inputs. The core idea of the proposed model\nis to embed human attributes into the latent space as independent codes and\nthus achieve flexible and continuous control of attributes via mixing and\ninterpolation operations in explicit style representations. Specifically, a new\narchitecture consisting of two encoding pathways with style block connections\nis proposed to decompose the original hard mapping into multiple more\naccessible subtasks. In source pathway, we further extract component layouts\nwith an off-the-shelf human parser and feed them into a shared global texture\nencoder for decomposed latent codes. This strategy allows for the synthesis of\nmore realistic output images and automatic separation of un-annotated\nattributes. Experimental results demonstrate the proposed method's superiority\nover the state of the art in pose transfer and its effectiveness in the\nbrand-new task of component attribute transfer.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 07:47:06 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 05:31:15 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 06:48:30 GMT"}, {"version": "v4", "created": "Sun, 19 Jul 2020 05:32:31 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Men", "Yifang", ""], ["Mao", "Yiming", ""], ["Jiang", "Yuning", ""], ["Ma", "Wei-Ying", ""], ["Lian", "Zhouhui", ""]]}, {"id": "2003.12294", "submitter": "Dely Yu", "authors": "Deli Yu, Xuan Li, Chengquan Zhang, Junyu Han, Jingtuo Liu, Errui Ding", "title": "Towards Accurate Scene Text Recognition with Semantic Reasoning Networks", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene text image contains two levels of contents: visual texture and semantic\ninformation. Although the previous scene text recognition methods have made\ngreat progress over the past few years, the research on mining semantic\ninformation to assist text recognition attracts less attention, only RNN-like\nstructures are explored to implicitly model semantic information. However, we\nobserve that RNN based methods have some obvious shortcomings, such as\ntime-dependent decoding manner and one-way serial transmission of semantic\ncontext, which greatly limit the help of semantic information and the\ncomputation efficiency. To mitigate these limitations, we propose a novel\nend-to-end trainable framework named semantic reasoning network (SRN) for\naccurate scene text recognition, where a global semantic reasoning module\n(GSRM) is introduced to capture global semantic context through multi-way\nparallel transmission. The state-of-the-art results on 7 public benchmarks,\nincluding regular text, irregular text and non-Latin long text, verify the\neffectiveness and robustness of the proposed method. In addition, the speed of\nSRN has significant advantages over the RNN based methods, demonstrating its\nvalue in practical use.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 09:19:25 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Yu", "Deli", ""], ["Li", "Xuan", ""], ["Zhang", "Chengquan", ""], ["Han", "Junyu", ""], ["Liu", "Jingtuo", ""], ["Ding", "Errui", ""]]}, {"id": "2003.12296", "submitter": "Lei Qi", "authors": "Jian Zhang, Lei Qi, Yinghuan Shi, Yang Gao", "title": "Generalizable Semantic Segmentation via Model-agnostic Learning and\n  Target-specific Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation methods in the supervised scenario have achieved\nsignificant improvement in recent years. However, when directly deploying the\ntrained model to segment the images of unseen (or new coming) domains, its\nperformance usually drops dramatically due to the data-distribution discrepancy\nbetween seen and unseen domains. To overcome this limitation, we propose a\nnovel domain generalization framework for the generalizable semantic\nsegmentation task, which enhances the generalization ability of the model from\ntwo different views, including the training paradigm and the data-distribution\ndiscrepancy. Concretely, we exploit the model-agnostic learning method to\nsimulate the domain shift problem, which deals with the domain generalization\nfrom the training scheme perspective. Besides, considering the\ndata-distribution discrepancy between source domains and unseen target domains,\nwe develop the target-specific normalization scheme to further boost the\ngeneralization ability in unseen target domains. Extensive experiments\nhighlight that the proposed method produces state-of-the-art performance for\nthe domain generalization of semantic segmentation on multiple benchmark\nsegmentation datasets (i.e., Cityscapes, Mapillary). Furthermore, we gain an\ninteresting observation that the target-specific normalization can benefit from\nthe model-agnostic learning scheme.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 09:25:19 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhang", "Jian", ""], ["Qi", "Lei", ""], ["Shi", "Yinghuan", ""], ["Gao", "Yang", ""]]}, {"id": "2003.12299", "submitter": "Youngjae Yu", "authors": "Youngjae Yu, Seunghwan Lee, Yuncheol Choi, Gunhee Kim", "title": "CurlingNet: Compositional Learning between Images and Text for Fashion\n  IQ Data", "comments": "4 pages, 4 figures, ICCV 2019 Linguistics Meets image and video\n  retrieval workshop, Fashion IQ challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach named CurlingNet that can measure the semantic\ndistance of composition of image-text embedding. In order to learn an effective\nimage-text composition for the data in the fashion domain, our model proposes\ntwo key components as follows. First, the Delivery makes the transition of a\nsource image in an embedding space. Second, the Sweeping emphasizes\nquery-related components of fashion images in the embedding space. We utilize a\nchannel-wise gating mechanism to make it possible. Our single model outperforms\nprevious state-of-the-art image-text composition models including TIRG and\nFiLM. We participate in the first fashion-IQ challenge in ICCV 2019, for which\nensemble of our model achieves one of the best performances.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 09:36:32 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 04:35:16 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yu", "Youngjae", ""], ["Lee", "Seunghwan", ""], ["Choi", "Yuncheol", ""], ["Kim", "Gunhee", ""]]}, {"id": "2003.12307", "submitter": "Juyong Zhang", "authors": "Xueying Wang, Yudong Guo, Bailin Deng, Juyong Zhang", "title": "Lightweight Photometric Stereo for Facial Details Recovery", "comments": "Accepted to CVPR2020. The source code is available\n  https://github.com/Juyong/FacePSNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D face reconstruction from a single image has achieved great\nsuccess with the help of deep learning and shape prior knowledge, but they\noften fail to produce accurate geometry details. On the other hand, photometric\nstereo methods can recover reliable geometry details, but require dense inputs\nand need to solve a complex optimization problem. In this paper, we present a\nlightweight strategy that only requires sparse inputs or even a single image to\nrecover high-fidelity face shapes with images captured under near-field lights.\nTo this end, we construct a dataset containing 84 different subjects with 29\nexpressions under 3 different lights. Data augmentation is applied to enrich\nthe data in terms of diversity in identity, lighting, expression, etc. With\nthis constructed dataset, we propose a novel neural network specially designed\nfor photometric stereo based 3D face reconstruction. Extensive experiments and\ncomparisons demonstrate that our method can generate high-quality\nreconstruction results with one to three facial images captured under\nnear-field lights. Our full framework is available at\nhttps://github.com/Juyong/FacePSNet.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 10:10:00 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Wang", "Xueying", ""], ["Guo", "Yudong", ""], ["Deng", "Bailin", ""], ["Zhang", "Juyong", ""]]}, {"id": "2003.12327", "submitter": "Lei Huang", "authors": "Lei Huang, Lei Zhao, Yi Zhou, Fan Zhu, Li Liu, Ling Shao", "title": "An Investigation into the Stochasticity of Batch Whitening", "comments": "Accepted to CVPR 2020. The Code is available at\n  https://github.com/huangleiBuaa/StochasticityBW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) is extensively employed in various network\narchitectures by performing standardization within mini-batches.\n  A full understanding of the process has been a central target in the deep\nlearning communities.\n  Unlike existing works, which usually only analyze the standardization\noperation, this paper investigates the more general Batch Whitening (BW). Our\nwork originates from the observation that while various whitening\ntransformations equivalently improve the conditioning, they show significantly\ndifferent behaviors in discriminative scenarios and training Generative\nAdversarial Networks (GANs).\n  We attribute this phenomenon to the stochasticity that BW introduces.\n  We quantitatively investigate the stochasticity of different whitening\ntransformations and show that it correlates well with the optimization\nbehaviors during training.\n  We also investigate how stochasticity relates to the estimation of population\nstatistics during inference.\n  Based on our analysis, we provide a framework for designing and comparing BW\nalgorithms in different scenarios.\n  Our proposed BW algorithm improves the residual networks by a significant\nmargin on ImageNet classification.\n  Besides, we show that the stochasticity of BW can improve the GAN's\nperformance with, however, the sacrifice of the training stability.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 11:06:32 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Huang", "Lei", ""], ["Zhao", "Lei", ""], ["Zhou", "Yi", ""], ["Zhu", "Fan", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""]]}, {"id": "2003.12338", "submitter": "Chunhua Shen", "authors": "Jianpeng Zhang, Yutong Xie, Guansong Pang, Zhibin Liao, Johan Verjans,\n  Wenxin Li, Zongji Sun, Jian He, Yi Li, Chunhua Shen, Yong Xia", "title": "Viral Pneumonia Screening on Chest X-ray Images Using Confidence-Aware\n  Anomaly Detection", "comments": "Accepted to IEEE Trans. Medical Imaging. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cluster of viral pneumonia occurrences during a short period of time may be a\nharbinger of an outbreak or pandemic, like SARS, MERS, and recent COVID-19.\nRapid and accurate detection of viral pneumonia using chest X-ray can be\nsignificantly useful in large-scale screening and epidemic prevention,\nparticularly when other chest imaging modalities are less available. Viral\npneumonia often have diverse causes and exhibit notably different visual\nappearances on X-ray images. The evolution of viruses and the emergence of\nnovel mutated viruses further result in substantial dataset shift, which\ngreatly limits the performance of classification approaches. In this paper, we\nformulate the task of differentiating viral pneumonia from non-viral pneumonia\nand healthy controls into an one-class classification-based anomaly detection\nproblem, and thus propose the confidence-aware anomaly detection (CAAD) model,\nwhich consists of a shared feature extractor, an anomaly detection module, and\na confidence prediction module. If the anomaly score produced by the anomaly\ndetection module is large enough or the confidence score estimated by the\nconfidence prediction module is small enough, we accept the input as an anomaly\ncase (i.e., viral pneumonia). The major advantage of our approach over binary\nclassification is that we avoid modeling individual viral pneumonia classes\nexplicitly and treat all known viral pneumonia cases as anomalies to reinforce\nthe one-class model. The proposed model outperforms binary classification\nmodels on the clinical X-VIRAL dataset that contains 5,977 viral pneumonia (no\nCOVID-19) cases, 18,619 non-viral pneumonia cases, and 18,774 healthy controls.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 11:32:18 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 04:16:00 GMT"}, {"version": "v3", "created": "Sun, 27 Sep 2020 01:29:10 GMT"}, {"version": "v4", "created": "Wed, 2 Dec 2020 00:05:40 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Jianpeng", ""], ["Xie", "Yutong", ""], ["Pang", "Guansong", ""], ["Liao", "Zhibin", ""], ["Verjans", "Johan", ""], ["Li", "Wenxin", ""], ["Sun", "Zongji", ""], ["He", "Jian", ""], ["Li", "Yi", ""], ["Shen", "Chunhua", ""], ["Xia", "Yong", ""]]}, {"id": "2003.12344", "submitter": "Juil Sock", "authors": "Juil Sock, Guillermo Garcia-Hernando, Anil Armagan, Tae-Kyun Kim", "title": "Introducing Pose Consistency and Warp-Alignment for Self-Supervised 6D\n  Object Pose Estimation in Color Images", "comments": "Accepted to 3DV'2020 as Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful approaches to estimate the 6D pose of an object typically\ntrain a neural network by supervising the learning with annotated poses in real\nworld images. These annotations are generally expensive to obtain and a common\nworkaround is to generate and train on synthetic scenes, with the drawback of\nlimited generalisation when the model is deployed in the real world. In this\nwork, a two-stage 6D object pose estimator framework that can be applied on top\nof existing neural-network-based approaches and that does not require pose\nannotations on real images is proposed. The first self-supervised stage\nenforces the pose consistency between rendered predictions and real input\nimages, narrowing the gap between the two domains. The second stage fine-tunes\nthe previously trained model by enforcing the photometric consistency between\npairs of different object views, where one image is warped and aligned to match\nthe view of the other and thus enabling their comparison. In the absence of\nboth real image annotations and depth information, applying the proposed\nframework on top of two recent approaches results in state-of-the-art\nperformance when compared to methods trained only on synthetic data, domain\nadaptation baselines and a concurrent self-supervised approach on LINEMOD,\nLINEMOD OCCLUSION and HomebrewedDB datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 11:53:38 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 09:49:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Sock", "Juil", ""], ["Garcia-Hernando", "Guillermo", ""], ["Armagan", "Anil", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2003.12346", "submitter": "Ali Samadzadeh", "authors": "Ali Samadzadeh, Fatemeh Sadat Tabatabaei Far, Ali Javadi, Ahmad\n  Nickabadi, Morteza Haghir Chehreghani", "title": "Convolutional Spiking Neural Networks for Spatio-Temporal Feature\n  Extraction", "comments": "10 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking neural networks (SNNs) can be used in low-power and embedded systems\n(such as emerging neuromorphic chips) due to their event-based nature. Also,\nthey have the advantage of low computation cost in contrast to conventional\nartificial neural networks (ANNs), while preserving ANN's properties. However,\ntemporal coding in layers of convolutional spiking neural networks and other\ntypes of SNNs has yet to be studied. In this paper, we provide insight into\nspatio-temporal feature extraction of convolutional SNNs in experiments\ndesigned to exploit this property. The shallow convolutional SNN outperforms\nstate-of-the-art spatio-temporal feature extractor methods such as C3D,\nConvLstm, and similar networks. Furthermore, we present a new deep spiking\narchitecture to tackle real-world problems (in particular classification tasks)\nwhich achieved superior performance compared to other SNN methods on NMNIST\n(99.6%), DVS-CIFAR10 (69.2%) and DVS-Gesture (96.7%) and ANN methods on UCF-101\n(42.1%) and HMDB-51 (21.5%) datasets. It is also worth noting that the training\nprocess is implemented based on variation of spatio-temporal backpropagation\nexplained in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 11:58:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 00:23:29 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Samadzadeh", "Ali", ""], ["Far", "Fatemeh Sadat Tabatabaei", ""], ["Javadi", "Ali", ""], ["Nickabadi", "Ahmad", ""], ["Chehreghani", "Morteza Haghir", ""]]}, {"id": "2003.12352", "submitter": "Ester Gonzalez-Sosa", "authors": "Ester Gonzalez-Sosa, Pablo Perez, Ruben Tolosana, Redouane Kachach,\n  Alvaro Villegas", "title": "Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation\n  and Database with Automatic Labelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we focus on the egocentric segmentation of arms to improve\nself-perception in Augmented Virtuality (AV). The main contributions of this\nwork are: i) a comprehensive survey of segmentation algorithms for AV; ii) an\nEgocentric Arm Segmentation Dataset, composed of more than 10, 000 images,\ncomprising variations of skin color, and gender, among others. We provide all\ndetails required for the automated generation of groundtruth and semi-synthetic\nimages; iii) the use of deep learning for the first time for segmenting arms in\nAV; iv) to showcase the usefulness of this database, we report results on\ndifferent real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands,\nEgo Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for\ndirect comparisons with existing approaches utilizing color or depth. Results\nconfirm the suitability of the EgoArm dataset for this task, achieving\nimprovement up to 40% with respect to the original network, depending on the\nparticular dataset. Results also suggest that, while approaches based on color\nor depth can work in controlled conditions (lack of occlusion, uniform\nlighting, only objects of interest in the near range, controlled background,\netc.), egocentric segmentation based on deep learning is more robust in real AV\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 12:09:27 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Gonzalez-Sosa", "Ester", ""], ["Perez", "Pablo", ""], ["Tolosana", "Ruben", ""], ["Kachach", "Redouane", ""], ["Villegas", "Alvaro", ""]]}, {"id": "2003.12382", "submitter": "Thomas Germer", "authors": "Thomas Germer, Tobias Uelwer, Stefan Conrad, Stefan Harmeling", "title": "PyMatting: A Python Library for Alpha Matting", "comments": null, "journal-ref": "Journal of Open Source Software (2020), 5(54), 2481", "doi": "10.21105/joss.02481", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step of many image editing tasks is to extract specific objects\nfrom an image in order to place them in a scene of a movie or compose them onto\nanother background. Alpha matting describes the problem of separating the\nobjects in the foreground from the background of an image given only a rough\nsketch. We introduce the PyMatting package for Python which implements various\napproaches to solve the alpha matting problem. Our toolbox is also able to\nextract the foreground of an image given the alpha matte. The implementation\naims to be computationally efficient and easy to use. The source code of\nPyMatting is available under an open-source license at\nhttps://github.com/pymatting/pymatting.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 18:46:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Germer", "Thomas", ""], ["Uelwer", "Tobias", ""], ["Conrad", "Stefan", ""], ["Harmeling", "Stefan", ""]]}, {"id": "2003.12383", "submitter": "Xander Wilcke", "authors": "W.X. Wilcke (1), P. Bloem (1), V. de Boer (1), R.H. van t Veer (2),\n  F.A.H. van Harmelen (1) ((1) Department of Computer Science Vrije\n  Universiteit Amsterdam The Netherlands, (2) Geodan Amsterdam The Netherlands)", "title": "End-to-End Entity Classification on Multimodal Knowledge Graphs", "comments": "Submitted to the 17th International Conference on Principles of\n  Knowledge Representation and Reasoning (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end multimodal learning on knowledge graphs has been left largely\nunaddressed. Instead, most end-to-end models such as message passing networks\nlearn solely from the relational information encoded in graphs' structure: raw\nvalues, or literals, are either omitted completely or are stripped from their\nvalues and treated as regular nodes. In either case we lose potentially\nrelevant information which could have otherwise been exploited by our learning\nmethods. To avoid this, we must treat literals and non-literals as separate\ncases. We must also address each modality separately and accordingly: numbers,\ntexts, images, geometries, et cetera. We propose a multimodal message passing\nnetwork which not only learns end-to-end from the structure of graphs, but also\nfrom their possibly divers set of multimodal node features. Our model uses\ndedicated (neural) encoders to naturally learn embeddings for node features\nbelonging to five different types of modalities, including images and\ngeometries, which are projected into a joint representation space together with\ntheir relational information. We demonstrate our model on a node classification\ntask, and evaluate the effect that each modality has on the overall\nperformance. Our result supports our hypothesis that including information from\nmultiple modalities can help our models obtain a better overall performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 14:57:52 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Wilcke", "W. X.", ""], ["Bloem", "P.", ""], ["de Boer", "V.", ""], ["Veer", "R. H. van t", ""], ["van Harmelen", "F. A. H.", ""]]}, {"id": "2003.12397", "submitter": "Cheng Lin", "authors": "Cheng Lin, Tingxiang Fan, Wenping Wang, Matthias Nie{\\ss}ner", "title": "Modeling 3D Shapes by Reinforcement Learning", "comments": "Accepted to ECCV 2020; Video: https://youtu.be/w5e9g_lvbyE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how to enable machines to model 3D shapes like human modelers\nusing deep reinforcement learning (RL). In 3D modeling software like Maya, a\nmodeler usually creates a mesh model in two steps: (1) approximating the shape\nusing a set of primitives; (2) editing the meshes of the primitives to create\ndetailed geometry. Inspired by such artist-based modeling, we propose a\ntwo-step neural framework based on RL to learn 3D modeling policies. By taking\nactions and collecting rewards in an interactive environment, the agents first\nlearn to parse a target shape into primitives and then to edit the geometry. To\neffectively train the modeling agents, we introduce a novel training algorithm\nthat combines heuristic policy, imitation learning and reinforcement learning.\nOur experiments show that the agents can learn good policies to produce regular\nand structure-aware mesh models, which demonstrates the feasibility and\neffectiveness of the proposed RL framework.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:05:39 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 04:40:18 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 04:45:27 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lin", "Cheng", ""], ["Fan", "Tingxiang", ""], ["Wang", "Wenping", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2003.12406", "submitter": "Michael Oechsle", "authors": "Michael Oechsle, Michael Niemeyer, Lars Mescheder, Thilo Strauss,\n  Andreas Geiger", "title": "Learning Implicit Surface Light Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit representations of 3D objects have recently achieved impressive\nresults on learning-based 3D reconstruction tasks. While existing works use\nsimple texture models to represent object appearance, photo-realistic image\nsynthesis requires reasoning about the complex interplay of light, geometry and\nsurface properties. In this work, we propose a novel implicit representation\nfor capturing the visual appearance of an object in terms of its surface light\nfield. In contrast to existing representations, our implicit model represents\nsurface light fields in a continuous fashion and independent of the geometry.\nMoreover, we condition the surface light field with respect to the location and\ncolor of a small light source. Compared to traditional surface light field\nmodels, this allows us to manipulate the light source and relight the object\nusing environment maps. We further demonstrate the capabilities of our model to\npredict the visual appearance of an unseen object from a single real RGB image\nand corresponding 3D shape information. As evidenced by our experiments, our\nmodel is able to infer rich visual appearance including shadows and specular\nreflections. Finally, we show that the proposed representation can be embedded\ninto a variational auto-encoder for generating novel appearances that conform\nto the specified illumination conditions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 13:17:45 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Oechsle", "Michael", ""], ["Niemeyer", "Michael", ""], ["Mescheder", "Lars", ""], ["Strauss", "Thilo", ""], ["Geiger", "Andreas", ""]]}, {"id": "2003.12424", "submitter": "Baifeng Shi", "authors": "Baifeng Shi, Qi Dai, Yadong Mu, Jingdong Wang", "title": "Weakly-Supervised Action Localization by Generative Attention Modeling", "comments": "CVPR2020. Code is available at\n  https://github.com/bfshi/DGAM-Weakly-Supervised-Action-Localization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization is a problem of learning an\naction localization model with only video-level action labeling available. The\ngeneral framework largely relies on the classification activation, which\nemploys an attention model to identify the action-related frames and then\ncategorizes them into different classes. Such method results in the\naction-context confusion issue: context frames near action clips tend to be\nrecognized as action frames themselves, since they are closely related to the\nspecific classes. To solve the problem, in this paper we propose to model the\nclass-agnostic frame-wise probability conditioned on the frame attention using\nconditional Variational Auto-Encoder (VAE). With the observation that the\ncontext exhibits notable difference from the action at representation level, a\nprobabilistic model, i.e., conditional VAE, is learned to model the likelihood\nof each frame given the attention. By maximizing the conditional probability\nwith respect to the attention, the action and non-action frames are well\nseparated. Experiments on THUMOS14 and ActivityNet1.2 demonstrate advantage of\nour method and effectiveness in handling action-context confusion problem. Code\nis now available on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 14:02:56 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 14:36:48 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Shi", "Baifeng", ""], ["Dai", "Qi", ""], ["Mu", "Yadong", ""], ["Wang", "Jingdong", ""]]}, {"id": "2003.12443", "submitter": "Yoichi Sato", "authors": "Yoichi Sato, Yasuhiko Takegami, Takamune Asamoto, Yutaro Ono, Tsugeno\n  Hidetoshi, Ryosuke Goto, Akira Kitamura, Seiwa Honda", "title": "A Computer-Aided Diagnosis System Using Artificial Intelligence for Hip\n  Fractures -Multi-Institutional Joint Development Research-", "comments": "9 pages, 4 tables, 7 figures. / author's homepage :\n  https://www.fracture-ai.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV eess.IV q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Objective] To develop a Computer-aided diagnosis (CAD) system for plane\nfrontal hip X-rays with a deep learning model trained on a large dataset\ncollected at multiple centers. [Materials and Methods]. We included 5295 cases\nwith neck fracture or trochanteric fracture who were diagnosed and treated by\northopedic surgeons using plane X-rays or computed tomography (CT) or magnetic\nresonance imaging (MRI) who visited each institution between April 2009 and\nMarch 2019 were enrolled. Cases in which both hips were not included in the\nphotographing range, femoral shaft fractures, and periprosthetic fractures were\nexcluded, and 5242 plane frontal pelvic X-rays obtained from 4,851 cases were\nused for machine learning. These images were divided into 5242 images including\nthe fracture side and 5242 images without the fracture side, and a total of\n10484 images were used for machine learning. A deep convolutional neural\nnetwork approach was used for machine learning. Pytorch 1.3 and Fast.ai 1.0\nwere used as frameworks, and EfficientNet-B4, which is pre-trained ImageNet\nmodel, was used. In the final evaluation, accuracy, sensitivity, specificity,\nF-value and area under the curve (AUC) were evaluated. Gradient-weighted class\nactivation mapping (Grad-CAM) was used to conceptualize the diagnostic basis of\nthe CAD system. [Results] The diagnostic accuracy of the learning model was\naccuracy of 96. 1 %, sensitivity of 95.2 %, specificity of 96.9 %, F-value of\n0.961, and AUC of 0.99. The cases who were correct for the diagnosis showed\ngenerally correct diagnostic basis using Grad-CAM. [Conclusions] The CAD system\nusing deep learning model which we developed was able to diagnose hip fracture\nin the plane X-ray with the high accuracy, and it was possible to present the\ndecision reason.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 11:16:39 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 14:15:19 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 11:22:28 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 05:41:46 GMT"}, {"version": "v5", "created": "Wed, 20 May 2020 04:29:20 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Sato", "Yoichi", ""], ["Takegami", "Yasuhiko", ""], ["Asamoto", "Takamune", ""], ["Ono", "Yutaro", ""], ["Hidetoshi", "Tsugeno", ""], ["Goto", "Ryosuke", ""], ["Kitamura", "Akira", ""], ["Honda", "Seiwa", ""]]}, {"id": "2003.12462", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, Amanpreet Singh", "title": "TextCaps: a Dataset for Image Captioning with Reading Comprehension", "comments": "To appear in ECCV 2020 (oral) Project page:\n  https://textvqa.org/textcaps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image descriptions can help visually impaired people to quickly understand\nthe image content. While we made significant progress in automatically\ndescribing images and optical character recognition, current approaches are\nunable to include written text in their descriptions, although text is\nomnipresent in human environments and frequently critical to understand our\nsurroundings. To study how to comprehend text in the context of an image we\ncollect a novel dataset, TextCaps, with 145k captions for 28k images. Our\ndataset challenges a model to recognize text, relate it to its visual context,\nand decide what part of the text to copy or paraphrase, requiring spatial,\nsemantic, and visual reasoning between multiple text tokens and visual\nentities, such as objects. We study baselines and adapt existing approaches to\nthis new task, which we refer to as image captioning with reading\ncomprehension. Our analysis with automatic and human studies shows that our new\nTextCaps dataset provides many new technical challenges over previous datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 02:38:35 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 04:08:02 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Hu", "Ronghang", ""], ["Rohrbach", "Marcus", ""], ["Singh", "Amanpreet", ""]]}, {"id": "2003.12464", "submitter": "Jianyu Chen", "authors": "Jianyu Chen, Zhuo Xu and Masayoshi Tomizuka", "title": "End-to-end Autonomous Driving Perception with Sequential Latent\n  Representation Learning", "comments": "8 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current autonomous driving systems are composed of a perception system and a\ndecision system. Both of them are divided into multiple subsystems built up\nwith lots of human heuristics. An end-to-end approach might clean up the system\nand avoid huge efforts of human engineering, as well as obtain better\nperformance with increasing data and computation resources. Compared to the\ndecision system, the perception system is more suitable to be designed in an\nend-to-end framework, since it does not require online driving exploration. In\nthis paper, we propose a novel end-to-end approach for autonomous driving\nperception. A latent space is introduced to capture all relevant features\nuseful for perception, which is learned through sequential latent\nrepresentation learning. The learned end-to-end perception model is able to\nsolve the detection, tracking, localization and mapping problems altogether\nwith only minimum human engineering efforts and without storing any maps\nonline. The proposed method is evaluated in a realistic urban driving\nsimulator, with both camera image and lidar point cloud as sensor inputs. The\ncodes and videos of this work are available at our github repo and project\nwebsite.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 05:37:44 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 03:40:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Chen", "Jianyu", ""], ["Xu", "Zhuo", ""], ["Tomizuka", "Masayoshi", ""]]}, {"id": "2003.12473", "submitter": "Saad Nadeem", "authors": "Shawn Mathew, Saad Nadeem, Sruti Kumari, Arie Kaufman", "title": "Augmenting Colonoscopy using Extended and Directional CycleGAN for Lossy\n  Image Translation", "comments": "CVPR 2020. **First two authors contributed equally to this work", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.00475", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer screening modalities, such as optical colonoscopy (OC) and\nvirtual colonoscopy (VC), are critical for diagnosing and ultimately removing\npolyps (precursors of colon cancer). The non-invasive VC is normally used to\ninspect a 3D reconstructed colon (from CT scans) for polyps and if found, the\nOC procedure is performed to physically traverse the colon via endoscope and\nremove these polyps. In this paper, we present a deep learning framework,\nExtended and Directional CycleGAN, for lossy unpaired image-to-image\ntranslation between OC and VC to augment OC video sequences with\nscale-consistent depth information from VC, and augment VC with\npatient-specific textures, color and specular highlights from OC (e.g, for\nrealistic polyp synthesis). Both OC and VC contain structural information, but\nit is obscured in OC by additional patient-specific texture and specular\nhighlights, hence making the translation from OC to VC lossy. The existing\nCycleGAN approaches do not handle lossy transformations. To address this\nshortcoming, we introduce an extended cycle consistency loss, which compares\nthe geometric structures from OC in the VC domain. This loss removes the need\nfor the CycleGAN to embed OC information in the VC domain. To handle a stronger\nremoval of the textures and lighting, a Directional Discriminator is introduced\nto differentiate the direction of translation (by creating paired information\nfor the discriminator), as opposed to the standard CycleGAN which is\ndirection-agnostic. Combining the extended cycle consistency loss and the\nDirectional Discriminator, we show state-of-the-art results on scale-consistent\ndepth inference for phantom, textured VC and for real polyp and normal colon\nvideo sequences. We also present results for realistic pendunculated and flat\npolyp synthesis from bumps introduced in 3D VC models.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 15:34:17 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 16:15:49 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Mathew", "Shawn", ""], ["Nadeem", "Saad", ""], ["Kumari", "Sruti", ""], ["Kaufman", "Arie", ""]]}, {"id": "2003.12506", "submitter": "Hongjie Zhang", "authors": "Hongjie Zhang, Ang Li, Jie Guo, Yanwen Guo", "title": "Hybrid Models for Open Set Recognition", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open set recognition requires a classifier to detect samples not belonging to\nany of the classes in its training set. Existing methods fit a probability\ndistribution to the training samples on their embedding space and detect\noutliers according to this distribution. The embedding space is often obtained\nfrom a discriminative classifier. However, such discriminative representation\nfocuses only on known classes, which may not be critical for distinguishing the\nunknown classes. We argue that the representation space should be jointly\nlearned from the inlier classifier and the density estimator (served as an\noutlier detector). We propose the OpenHybrid framework, which is composed of an\nencoder to encode the input data into a joint embedding space, a classifier to\nclassify samples to inlier classes, and a flow-based density estimator to\ndetect whether a sample belongs to the unknown category. A typical problem of\nexisting flow-based models is that they may assign a higher likelihood to\noutliers. However, we empirically observe that such an issue does not occur in\nour experiments when learning a joint representation for discriminative and\ngenerative components. Experiments on standard open set benchmarks also reveal\nthat an end-to-end trained OpenHybrid model significantly outperforms\nstate-of-the-art methods and flow-based baselines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 16:14:27 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 01:06:26 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Zhang", "Hongjie", ""], ["Li", "Ang", ""], ["Guo", "Jie", ""], ["Guo", "Yanwen", ""]]}, {"id": "2003.12511", "submitter": "Tai-Yin Chiu", "authors": "Tai-Yin Chiu, Yinan Zhao, Danna Gurari", "title": "Assessing Image Quality Issues for Real-World Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new large-scale dataset that links the assessment of image\nquality issues to two practical vision tasks: image captioning and visual\nquestion answering. First, we identify for 39,181 images taken by people who\nare blind whether each is sufficient quality to recognize the content as well\nas what quality flaws are observed from six options. These labels serve as a\ncritical foundation for us to make the following contributions: (1) a new\nproblem and algorithms for deciding whether an image is insufficient quality to\nrecognize the content and so not captionable, (2) a new problem and algorithms\nfor deciding which of six quality flaws an image contains, (3) a new problem\nand algorithms for deciding whether a visual question is unanswerable due to\nunrecognizable content versus the content of interest being missing from the\nfield of view, and (4) a novel application of more efficiently creating a\nlarge-scale image captioning dataset by automatically deciding whether an image\nis insufficient quality and so should not be captioned. We publicly-share our\ndatasets and code to facilitate future extensions of this work:\nhttps://vizwiz.org.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 16:21:44 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2020 16:47:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chiu", "Tai-Yin", ""], ["Zhao", "Yinan", ""], ["Gurari", "Danna", ""]]}, {"id": "2003.12563", "submitter": "Xiyang Dai", "authors": "Xiyang Dai and Dongdong Chen and Mengchen Liu and Yinpeng Chen and Lu\n  Yuan", "title": "DA-NAS: Data Adapted Pruning for Efficient Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient search is a core issue in Neural Architecture Search (NAS). It is\ndifficult for conventional NAS algorithms to directly search the architectures\non large-scale tasks like ImageNet. In general, the cost of GPU hours for NAS\ngrows with regard to training dataset size and candidate set size. One common\nway is searching on a smaller proxy dataset (e.g., CIFAR-10) and then\ntransferring to the target task (e.g., ImageNet). These architectures optimized\non proxy data are not guaranteed to be optimal on the target task. Another\ncommon way is learning with a smaller candidate set, which may require expert\nknowledge and indeed betrays the essence of NAS. In this paper, we present\nDA-NAS that can directly search the architecture for large-scale target tasks\nwhile allowing a large candidate set in a more efficient manner. Our method is\nbased on an interesting observation that the learning speed for blocks in deep\nneural networks is related to the difficulty of recognizing distinct\ncategories. We carefully design a progressive data adapted pruning strategy for\nefficient architecture search. It will quickly trim low performed blocks on a\nsubset of target dataset (e.g., easy classes), and then gradually find the best\nblocks on the whole target dataset. At this time, the original candidate set\nbecomes as compact as possible, providing a faster search in the target task.\nExperiments on ImageNet verify the effectiveness of our approach. It is 2x\nfaster than previous methods while the accuracy is currently state-of-the-art,\nat 76.2% under small FLOPs constraint. It supports an argument search space\n(i.e., more candidate blocks) to efficiently search the best-performing\narchitecture.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:55:21 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dai", "Xiyang", ""], ["Chen", "Dongdong", ""], ["Liu", "Mengchen", ""], ["Chen", "Yinpeng", ""], ["Yuan", "Lu", ""]]}, {"id": "2003.12565", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Luc Van Gool, Radu Timofte", "title": "Probabilistic Regression for Visual Tracking", "comments": "CVPR 2020. Includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual tracking is fundamentally the problem of regressing the state of the\ntarget in each video frame. While significant progress has been achieved,\ntrackers are still prone to failures and inaccuracies. It is therefore crucial\nto represent the uncertainty in the target estimation. Although current\nprominent paradigms rely on estimating a state-dependent confidence score, this\nvalue lacks a clear probabilistic interpretation, complicating its use.\n  In this work, we therefore propose a probabilistic regression formulation and\napply it to tracking. Our network predicts the conditional probability density\nof the target state given an input image. Crucially, our formulation is capable\nof modeling label noise stemming from inaccurate annotations and ambiguities in\nthe task. The regression network is trained by minimizing the Kullback-Leibler\ndivergence. When applied for tracking, our formulation not only allows a\nprobabilistic representation of the output, but also substantially improves the\nperformance. Our tracker sets a new state-of-the-art on six datasets, achieving\n59.8% AUC on LaSOT and 75.8% Success on TrackingNet. The code and models are\navailable at https://github.com/visionml/pytracking.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:58:37 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Danelljan", "Martin", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.12597", "submitter": "Dhruv Patel", "authors": "Dhruv V. Patel, Assad A. Oberai", "title": "GAN-based Priors for Quantifying Uncertainty", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.28806.32322", "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is used extensively to quantify the uncertainty in an\ninferred field given the measurement of a related field when the two are linked\nby a mathematical model. Despite its many applications, Bayesian inference\nfaces challenges when inferring fields that have discrete representations of\nlarge dimension, and/or have prior distributions that are difficult to\ncharacterize mathematically. In this work we demonstrate how the approximate\ndistribution learned by a deep generative adversarial network (GAN) may be used\nas a prior in a Bayesian update to address both these challenges. We\ndemonstrate the efficacy of this approach on two distinct, and remarkably\nbroad, classes of problems. The first class leads to supervised learning\nalgorithms for image classification with superior out of distribution detection\nand accuracy, and for image inpainting with built-in variance estimation. The\nsecond class leads to unsupervised learning algorithms for image denoising and\nfor solving physics-driven inverse problems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 18:52:54 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Patel", "Dhruv V.", ""], ["Oberai", "Assad A.", ""]]}, {"id": "2003.12602", "submitter": "Sharad Joshi", "authors": "Sharad Joshi, Suraj Saxena, Nitin Khanna", "title": "Source Printer Identification from Document Images Acquired using\n  Smartphone", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vast volumes of printed documents continue to be used for various important\nas well as trivial applications. Such applications often rely on the\ninformation provided in the form of printed text documents whose integrity\nverification poses a challenge due to time constraints and lack of resources.\nSource printer identification provides essential information about the origin\nand integrity of a printed document in a fast and cost-effective manner. Even\nwhen fraudulent documents are identified, information about their origin can\nhelp stop future frauds. If a smartphone camera replaces scanner for the\ndocument acquisition process, document forensics would be more economical,\nuser-friendly, and even faster in many applications where remote and\ndistributed analysis is beneficial. Building on existing methods, we propose to\nlearn a single CNN model from the fusion of letter images and their\nprinter-specific noise residuals. In the absence of any publicly available\ndataset, we created a new dataset consisting of 2250 document images of text\ndocuments printed by eighteen printers and acquired by a smartphone camera at\nfive acquisition settings. The proposed method achieves 98.42% document\nclassification accuracy using images of letter 'e' under a 5x2 cross-validation\napproach. Further, when tested using about half a million letters of all types,\nit achieves 90.33% and 98.01% letter and document classification accuracies,\nrespectively, thus highlighting the ability to learn a discriminative model\nwithout dependence on a single letter type. Also, classification accuracies are\nencouraging under various acquisition settings, including low illumination and\nchange in angle between the document and camera planes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 18:59:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Joshi", "Sharad", ""], ["Saxena", "Suraj", ""], ["Khanna", "Nitin", ""]]}, {"id": "2003.12618", "submitter": "Alex Golts", "authors": "Alex Golts and Yoav Y. Schechner", "title": "Image compression optimized for 3D reconstruction by utilizing deep\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvcir.2021.103208", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computer vision tasks are often expected to be executed on compressed images.\nClassical image compression standards like JPEG 2000 are widely used. However,\nthey do not account for the specific end-task at hand. Motivated by works on\nrecurrent neural network (RNN)-based image compression and three-dimensional\n(3D) reconstruction, we propose unified network architectures to solve both\ntasks jointly. These joint models provide image compression tailored for the\nspecific task of 3D reconstruction. Images compressed by our proposed models,\nyield 3D reconstruction performance superior as compared to using JPEG 2000\ncompression. Our models significantly extend the range of compression rates for\nwhich 3D reconstruction is possible. We also show that this can be done highly\nefficiently at almost no additional cost to obtain compression on top of the\ncomputation already required for performing the 3D reconstruction task.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 19:55:30 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 17:18:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Golts", "Alex", ""], ["Schechner", "Yoav Y.", ""]]}, {"id": "2003.12621", "submitter": "Shadrokh Samavi", "authors": "Kamran Chitsaz, Mohsen Hajabdollahi, Nader Karimi, Shadrokh Samavi,\n  Shahram Shirani", "title": "Acceleration of Convolutional Neural Network Using FFT-Based Split\n  Convolutions", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have a large number of variables and\nhence suffer from a complexity problem for their implementation. Different\nmethods and techniques have developed to alleviate the problem of CNN's\ncomplexity, such as quantization, pruning, etc. Among the different\nsimplification methods, computation in the Fourier domain is regarded as a new\nparadigm for the acceleration of CNNs. Recent studies on Fast Fourier Transform\n(FFT) based CNN aiming at simplifying the computations required for FFT.\nHowever, there is a lot of space for working on the reduction of the\ncomputational complexity of FFT. In this paper, a new method for CNN processing\nin the FFT domain is proposed, which is based on input splitting. There are\nproblems in the computation of FFT using small kernels in situations such as\nCNN. Splitting can be considered as an effective solution for such issues\naroused by small kernels. Using splitting redundancy, such as overlap-and-add,\nis reduced and, efficiency is increased. Hardware implementation of the\nproposed FFT method, as well as different analyses of the complexity, are\nperformed to demonstrate the proper performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:16:57 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 21:14:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chitsaz", "Kamran", ""], ["Hajabdollahi", "Mohsen", ""], ["Karimi", "Nader", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""]]}, {"id": "2003.12622", "submitter": "Armen Avetisyan", "authors": "Armen Avetisyan, Tatiana Khanova, Christopher Choy, Denver Dash,\n  Angela Dai, Matthias Nie{\\ss}ner", "title": "SceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans", "comments": "Video here https://youtu.be/F0DpggYByh0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to reconstructing lightweight, CAD-based\nrepresentations of scanned 3D environments from commodity RGB-D sensors. Our\nkey idea is to jointly optimize for both CAD model alignments as well as layout\nestimations of the scanned scene, explicitly modeling inter-relationships\nbetween objects-to-objects and objects-to-layout. Since object arrangement and\nscene layout are intrinsically coupled, we show that treating the problem\njointly significantly helps to produce globally-consistent representations of a\nscene. Object CAD models are aligned to the scene by establishing dense\ncorrespondences between geometry, and we introduce a hierarchical layout\nprediction approach to estimate layout planes from corners and edges of the\nscene.To this end, we propose a message-passing graph neural network to model\nthe inter-relationships between objects and layout, guiding generation of a\nglobally object alignment in a scene. By considering the global scene layout,\nwe achieve significantly improved CAD alignments compared to state-of-the-art\nmethods, improving from 41.83% to 58.41% alignment accuracy on SUNCG and from\n50.05% to 61.24% on ScanNet, respectively. The resulting CAD-based\nrepresentations makes our method well-suited for applications in content\ncreation such as augmented- or virtual reality.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:17:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Avetisyan", "Armen", ""], ["Khanova", "Tatiana", ""], ["Choy", "Christopher", ""], ["Dash", "Denver", ""], ["Dai", "Angela", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2003.12625", "submitter": "Qian Wang", "authors": "Qian Wang, Neelanjan Bhowmik, Toby P. Breckon", "title": "On the Evaluation of Prohibited Item Classification and Detection in\n  Volumetric 3D Computed Tomography Baggage Security Screening Imagery", "comments": "Accepted to IJCNN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  X-ray Computed Tomography (CT) based 3D imaging is widely used in airports\nfor aviation security screening whilst prior work on prohibited item detection\nfocuses primarily on 2D X-ray imagery. In this paper, we aim to evaluate the\npossibility of extending the automatic prohibited item detection from 2D X-ray\nimagery to volumetric 3D CT baggage security screening imagery. To these ends,\nwe take advantage of 3D Convolutional Neural Neworks (CNN) and popular object\ndetection frameworks such as RetinaNet and Faster R-CNN in our work. As the\nfirst attempt to use 3D CNN for volumetric 3D CT baggage security screening, we\nfirst evaluate different CNN architectures on the classification of isolated\nprohibited item volumes and compare against traditional methods which use\nhand-crafted features. Subsequently, we evaluate object detection performance\nof different architectures on volumetric 3D CT baggage images. The results of\nour experiments on Bottle and Handgun datasets demonstrate that 3D CNN models\ncan achieve comparable performance (98% true positive rate and 1.5% false\npositive rate) to traditional methods but require significantly less time for\ninference (0.014s per volume). Furthermore, the extended 3D object detection\nmodels achieve promising performance in detecting prohibited items within\nvolumetric 3D CT baggage imagery with 76% mAP for bottles and 88% mAP for\nhandguns, which shows both the challenge and promise of such threat detection\nwithin 3D CT X-ray security imagery.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:17:58 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wang", "Qian", ""], ["Bhowmik", "Neelanjan", ""], ["Breckon", "Toby P.", ""]]}, {"id": "2003.12628", "submitter": "Edgar A. Bernal", "authors": "Trevor W. Richardson, Wencheng Wu, Lei Lin, Beilei Xu, Edgar A. Bernal", "title": "MCFlow: Monte Carlo Flow Models for Data Imputation", "comments": null, "journal-ref": "2020 Computer Vision and Pattern Recognition (CVPR)", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the topic of data imputation, a foundational task in machine\nlearning that addresses issues with missing data. To that end, we propose\nMCFlow, a deep framework for imputation that leverages normalizing flow\ngenerative models and Monte Carlo sampling. We address the causality dilemma\nthat arises when training models with incomplete data by introducing an\niterative learning scheme which alternately updates the density estimate and\nthe values of the missing entries in the training data. We provide extensive\nempirical validation of the effectiveness of the proposed method on standard\nmultivariate and image datasets, and benchmark its performance against\nstate-of-the-art alternatives. We demonstrate that MCFlow is superior to\ncompeting methods in terms of the quality of the imputed data, as well as with\nregards to its ability to preserve the semantic structure of the data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:33:52 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Richardson", "Trevor W.", ""], ["Wu", "Wencheng", ""], ["Lin", "Lei", ""], ["Xu", "Beilei", ""], ["Bernal", "Edgar A.", ""]]}, {"id": "2003.12633", "submitter": "Davis Gilton", "authors": "Davis Gilton, Ruotian Luo, Rebecca Willett, Greg Shakhnarovich", "title": "Detection and Description of Change in Visual Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for the analysis of changes in visual\nstreams: ordered sequences of images, possibly separated by significant time\ngaps. We propose a new approach to incorporating unlabeled data into training\nto generate natural language descriptions of change. We also develop a\nframework for estimating the time of change in visual stream. We use learned\nrepresentations for change evidence and consistency of perceived change, and\ncombine these in a regularized graph cut based change detector. Experimental\nevaluation on visual stream datasets, which we release as part of our\ncontribution, shows that representation learning driven by natural language\ndescriptions significantly improves change detection accuracy, compared to\nmethods that do not rely on language.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 20:49:38 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 20:32:19 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Gilton", "Davis", ""], ["Luo", "Ruotian", ""], ["Willett", "Rebecca", ""], ["Shakhnarovich", "Greg", ""]]}, {"id": "2003.12638", "submitter": "Vinicius G. Goecks", "authors": "Vinicius G. Goecks, Grayson Woods, John Valasek", "title": "Combining Visible and Infrared Spectrum Imagery using Machine Learning\n  for Small Unmanned Aerial System Detection", "comments": "Project page: https://sites.google.com/view/tamudrone-spie2020/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in machine learning and deep neural networks for object detection,\ncoupled with lower cost and power requirements of cameras, led to promising\nvision-based solutions for sUAS detection. However, solely relying on the\nvisible spectrum has previously led to reliability issues in low contrast\nscenarios such as sUAS flying below the treeline and against bright sources of\nlight. Alternatively, due to the relatively high heat signatures emitted from\nsUAS during flight, a long-wave infrared (LWIR) sensor is able to produce\nimages that clearly contrast the sUAS from its background. However, compared to\nwidely available visible spectrum sensors, LWIR sensors have lower resolution\nand may produce more false positives when exposed to birds or other heat\nsources. This research work proposes combining the advantages of the LWIR and\nvisible spectrum sensors using machine learning for vision-based detection of\nsUAS. Utilizing the heightened background contrast from the LWIR sensor\ncombined and synchronized with the relatively increased resolution of the\nvisible spectrum sensor, a deep learning model was trained to detect the sUAS\nthrough previously difficult environments. More specifically, the approach\ndemonstrated effective detection of multiple sUAS flying above and below the\ntreeline, in the presence of heat sources, and glare from the sun. Our approach\nachieved a detection rate of 71.2 +- 8.3%, improving by 69% when compared to\nLWIR and by 30.4% when visible spectrum alone, and achieved false alarm rate of\n2.7 +- 2.6%, decreasing by 74.1% and by 47.1% when compared to LWIR and visible\nspectrum alone, respectively, on average, for single and multiple drone\nscenarios, controlled for the same confidence metric of the machine learning\nobject detector of at least 50%. Videos of the solution's performance can be\nseen at https://sites.google.com/view/tamudrone-spie2020/.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:06:14 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 19:08:02 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Goecks", "Vinicius G.", ""], ["Woods", "Grayson", ""], ["Valasek", "John", ""]]}, {"id": "2003.12641", "submitter": "Idan Achituve", "authors": "Idan Achituve, Haggai Maron and Gal Chechik", "title": "Self-Supervised Learning for Domain Adaptation on Point-Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SSL) is a technique for learning useful\nrepresentations from unlabeled data. It has been applied effectively to domain\nadaptation (DA) on images and videos. It is still unknown if and how it can be\nleveraged for domain adaptation in 3D perception problems. Here we describe the\nfirst study of SSL for DA on point clouds. We introduce a new family of pretext\ntasks, Deformation Reconstruction, inspired by the deformations encountered in\nsim-to-real transformations. In addition, we propose a novel training procedure\nfor labeled point cloud data motivated by the MixUp method called Point cloud\nMixup (PCM). Evaluations on domain adaptations datasets for classification and\nsegmentation, demonstrate a large improvement over existing and baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:19:15 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 16:40:13 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 22:53:54 GMT"}, {"version": "v4", "created": "Mon, 8 Mar 2021 14:04:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Achituve", "Idan", ""], ["Maron", "Haggai", ""], ["Chechik", "Gal", ""]]}, {"id": "2003.12642", "submitter": "Sai Bi", "authors": "Sai Bi, Zexiang Xu, Kalyan Sunkavalli, David Kriegman, Ravi\n  Ramamoorthi", "title": "Deep 3D Capture: Geometry and Reflectance from Sparse Multi-View Images", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel learning-based method to reconstruct the high-quality\ngeometry and complex, spatially-varying BRDF of an arbitrary object from a\nsparse set of only six images captured by wide-baseline cameras under\ncollocated point lighting. We first estimate per-view depth maps using a deep\nmulti-view stereo network; these depth maps are used to coarsely align the\ndifferent views. We propose a novel multi-view reflectance estimation network\narchitecture that is trained to pool features from these coarsely aligned\nimages and predict per-view spatially-varying diffuse albedo, surface normals,\nspecular roughness and specular albedo. We do this by jointly optimizing the\nlatent space of our multi-view reflectance network to minimize the photometric\nerror between images rendered with our predictions and the input images. While\nprevious state-of-the-art methods fail on such sparse acquisition setups, we\ndemonstrate, via extensive experiments on synthetic and real data, that our\nmethod produces high-quality reconstructions that can be used to render\nphotorealistic images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:28:54 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 07:48:28 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bi", "Sai", ""], ["Xu", "Zexiang", ""], ["Sunkavalli", "Kalyan", ""], ["Kriegman", "David", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2003.12645", "submitter": "Yuteng Zhu", "authors": "Graham D. Finlayson and Yuteng Zhu", "title": "Designing Color Filters that Make Cameras MoreColorimetric", "comments": "13 pages, 5 figures, 3 algorithms, journal", "journal-ref": null, "doi": "10.1109/TIP.2020.3038523", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we place a colored filter in front of a camera the effective camera\nresponse functions are equal to the given camera spectral sensitivities\nmultiplied by the filter spectral transmittance. In this paper, we solve for\nthe filter which returns the modified sensitivities as close to being a linear\ntransformation from the color matching functions of human visual system as\npossible. When this linearity condition - sometimes called the Luther condition\n- is approximately met, the `camera+filter' system can be used for accurate\ncolor measurement. Then, we reformulate our filter design optimisation for\nmaking the sensor responses as close to the CIEXYZ tristimulus values as\npossible given the knowledge of real measured surfaces and illuminants spectra\ndata. This data-driven method in turn is extended to incorporate constraints on\nthe filter (smoothness and bounded transmission). Also, because how the\noptimisation is initialised is shown to impact on the performance of the\nsolved-for filters, a multi-initialisation optimisation is developed.\n  Experiments demonstrate that, by taking pictures through our optimised color\nfilters we can make cameras significantly more colorimetric.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:41:30 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Finlayson", "Graham D.", ""], ["Zhu", "Yuteng", ""]]}, {"id": "2003.12649", "submitter": "Sai Bi", "authors": "Sai Bi, Kalyan Sunkavalli, Federico Perazzi, Eli Shechtman, Vladimir\n  Kim, Ravi Ramamoorthi", "title": "Deep CG2Real: Synthetic-to-Real Translation via Image Disentanglement", "comments": "Accepted to ICCV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to improve the visual realism of low-quality, synthetic\nimages, e.g. OpenGL renderings. Training an unpaired synthetic-to-real\ntranslation network in image space is severely under-constrained and produces\nvisible artifacts. Instead, we propose a semi-supervised approach that operates\non the disentangled shading and albedo layers of the image. Our two-stage\npipeline first learns to predict accurate shading in a supervised fashion using\nphysically-based renderings as targets, and further increases the realism of\nthe textures and shading with an improved CycleGAN network. Extensive\nevaluations on the SUNCG indoor scene dataset demonstrate that our approach\nyields more realistic images compared to other state-of-the-art approaches.\nFurthermore, networks trained on our generated \"real\" images predict more\naccurate depth and normals than domain adaptation approaches, suggesting that\nimproving the visual realism of the images can be more effective than imposing\ntask-specific losses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 21:45:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Bi", "Sai", ""], ["Sunkavalli", "Kalyan", ""], ["Perazzi", "Federico", ""], ["Shechtman", "Eli", ""], ["Kim", "Vladimir", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "2003.12673", "submitter": "Amit Kohli", "authors": "Amit Kohli, Vincent Sitzmann, Gordon Wetzstein", "title": "Semantic Implicit Neural Scene Representations With Semi-Supervised\n  Training", "comments": "3DV 2020 Camera Ready\n  https://www.computationalimaging.org/publications/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of implicit neural scene representations has presented a\nviable new method for how we capture and store 3D scenes. Unlike conventional\n3D representations, such as point clouds, which explicitly store scene\nproperties in discrete, localized units, these implicit representations encode\na scene in the weights of a neural network which can be queried at any\ncoordinate to produce these same scene properties. Thus far, implicit\nrepresentations have primarily been optimized to estimate only the appearance\nand/or 3D geometry information in a scene. We take the next step and\ndemonstrate that an existing implicit representation (SRNs) is actually\nmulti-modal; it can be further leveraged to perform per-point semantic\nsegmentation while retaining its ability to represent appearance and geometry.\nTo achieve this multi-modal behavior, we utilize a semi-supervised learning\nstrategy atop the existing pre-trained scene representation. Our method is\nsimple, general, and only requires a few tens of labeled 2D segmentation masks\nin order to achieve dense 3D semantic segmentation. We explore two novel\napplications for this semantically aware implicit neural scene representation:\n3D novel view and semantic label synthesis given only a single input RGB image\nor 2D label mask, as well as 3D interpolation of appearance and semantics.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 00:43:17 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 01:53:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kohli", "Amit", ""], ["Sitzmann", "Vincent", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2003.12693", "submitter": "Huizhu Pan", "authors": "Huizhu Pan, Jintao Song, Wanquan Liu, Ling Li, Guanglu Zhou, Lu Tan,\n  Shichu Chen", "title": "Using the Split Bregman Algorithm to Solve the Self-repelling Snake\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preserving contour topology during image segmentation is useful in many\npractical scenarios. By keeping the contours isomorphic, it is possible to\nprevent over-segmentation and under-segmentation, as well as to adhere to given\ntopologies. The Self-repelling Snake model (SR) is a variational model that\npreserves contour topology by combining a non-local repulsion term with the\ngeodesic active contour model (GAC). The SR is traditionally solved using the\nadditive operator splitting (AOS) scheme. In our paper, we propose an\nalternative solution to the SR using the Split Bregman method. Our algorithm\nbreaks the problem down into simpler sub-problems to use lower-order evolution\nequations and a simple projection scheme rather than re-initialization. The\nsub-problems can be solved via fast Fourier transform (FFT) or an approximate\nsoft thresholding formula which maintains stability, shortening the convergence\ntime, and reduces the memory requirement. The Split Bregman and AOS algorithms\nare compared theoretically and experimentally.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 03:41:47 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 02:59:58 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Pan", "Huizhu", ""], ["Song", "Jintao", ""], ["Liu", "Wanquan", ""], ["Li", "Ling", ""], ["Zhou", "Guanglu", ""], ["Tan", "Lu", ""], ["Chen", "Shichu", ""]]}, {"id": "2003.12697", "submitter": "Zhen Zhu", "authors": "Zhen Zhu, Zhiliang Xu, Ansheng You, Xiang Bai", "title": "Semantically Multi-modal Image Synthesis", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on semantically multi-modal image synthesis (SMIS)\ntask, namely, generating multi-modal images at the semantic level. Previous\nwork seeks to use multiple class-specific generators, constraining its usage in\ndatasets with a small number of classes. We instead propose a novel Group\nDecreasing Network (GroupDNet) that leverages group convolutions in the\ngenerator and progressively decreases the group numbers of the convolutions in\nthe decoder. Consequently, GroupDNet is armed with much more controllability on\ntranslating semantic labels to natural images and has plausible high-quality\nyields for datasets with many classes. Experiments on several challenging\ndatasets demonstrate the superiority of GroupDNet on performing the SMIS task.\nWe also show that GroupDNet is capable of performing a wide range of\ninteresting synthesis applications. Codes and models are available at:\nhttps://github.com/Seanseattle/SMIS.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 04:03:46 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 06:50:49 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 09:07:29 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Zhu", "Zhen", ""], ["Xu", "Zhiliang", ""], ["You", "Ansheng", ""], ["Bai", "Xiang", ""]]}, {"id": "2003.12698", "submitter": "Ashwin Balakrishna", "authors": "Aditya Ganapathi, Priya Sundaresan, Brijen Thananjeyan, Ashwin\n  Balakrishna, Daniel Seita, Jennifer Grannen, Minho Hwang, Ryan Hoque, Joseph\n  E. Gonzalez, Nawid Jamali, Katsu Yamane, Soshi Iba, Ken Goldberg", "title": "Learning Dense Visual Correspondences in Simulation to Smooth and Fold\n  Real Fabrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic fabric manipulation is challenging due to the infinite dimensional\nconfiguration space, self-occlusion, and complex dynamics of fabrics. There has\nbeen significant prior work on learning policies for specific deformable\nmanipulation tasks, but comparatively less focus on algorithms which can\nefficiently learn many different tasks. In this paper, we learn visual\ncorrespondences for deformable fabrics across different configurations in\nsimulation and show that this representation can be used to design policies for\na variety of tasks. Given a single demonstration of a new task from an initial\nfabric configuration, the learned correspondences can be used to compute\ngeometrically equivalent actions in a new fabric configuration. This makes it\npossible to robustly imitate a broad set of multi-step fabric smoothing and\nfolding tasks on multiple physical robotic systems. The resulting policies\nachieve 80.3% average task success rate across 10 fabric manipulation tasks on\ntwo different robotic systems, the da Vinci surgical robot and the ABB YuMi.\nResults also suggest robustness to fabrics of various colors, sizes, and\nshapes. See https://tinyurl.com/fabric-descriptors for supplementary material\nand videos.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 04:06:20 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 01:01:59 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Ganapathi", "Aditya", ""], ["Sundaresan", "Priya", ""], ["Thananjeyan", "Brijen", ""], ["Balakrishna", "Ashwin", ""], ["Seita", "Daniel", ""], ["Grannen", "Jennifer", ""], ["Hwang", "Minho", ""], ["Hoque", "Ryan", ""], ["Gonzalez", "Joseph E.", ""], ["Jamali", "Nawid", ""], ["Yamane", "Katsu", ""], ["Iba", "Soshi", ""], ["Goldberg", "Ken", ""]]}, {"id": "2003.12703", "submitter": "Mingyi Zhou", "authors": "Mingyi Zhou, Jing Wu, Yipeng Liu, Shuaicheng Liu, Ce Zhu", "title": "DaST: Data-free Substitute Training for Adversarial Attacks", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to adversarial examples. For the\nblack-box setting, current substitute attacks need pre-trained models to\ngenerate adversarial examples. However, pre-trained models are hard to obtain\nin real-world tasks. In this paper, we propose a data-free substitute training\nmethod (DaST) to obtain substitute models for adversarial black-box attacks\nwithout the requirement of any real data. To achieve this, DaST utilizes\nspecially designed generative adversarial networks (GANs) to train the\nsubstitute models. In particular, we design a multi-branch architecture and\nlabel-control loss for the generative model to deal with the uneven\ndistribution of synthetic samples. The substitute model is then trained by the\nsynthetic samples generated by the generative model, which are labeled by the\nattacked model subsequently. The experiments demonstrate the substitute models\nproduced by DaST can achieve competitive performance compared with the baseline\nmodels which are trained by the same train set with attacked models.\nAdditionally, to evaluate the practicability of the proposed method on the\nreal-world task, we attack an online machine learning model on the Microsoft\nAzure platform. The remote model misclassifies 98.35% of the adversarial\nexamples crafted by our method. To the best of our knowledge, we are the first\nto train a substitute model for adversarial attacks without any real data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 04:28:13 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 15:25:06 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhou", "Mingyi", ""], ["Wu", "Jing", ""], ["Liu", "Yipeng", ""], ["Liu", "Shuaicheng", ""], ["Zhu", "Ce", ""]]}, {"id": "2003.12724", "submitter": "Zhenzhong Chen", "authors": "Yaochen Zhu, Jiayi Xie, Zhenzhong Chen", "title": "Predicting the Popularity of Micro-videos with Multimodal Variational\n  Encoder-Decoder Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emerging type of user-generated content, micro-video drastically\nenriches people's entertainment experiences and social interactions. However,\nthe popularity pattern of an individual micro-video still remains elusive among\nthe researchers. One of the major challenges is that the potential popularity\nof a micro-video tends to fluctuate under the impact of various external\nfactors, which makes it full of uncertainties. In addition, since micro-videos\nare mainly uploaded by individuals that lack professional techniques, multiple\ntypes of noise could exist that obscure useful information. In this paper, we\npropose a multimodal variational encoder-decoder (MMVED) framework for\nmicro-video popularity prediction tasks. MMVED learns a stochastic Gaussian\nembedding of a micro-video that is informative to its popularity level while\npreserves the inherent uncertainties simultaneously. Moreover, through the\noptimization of a deep variational information bottleneck lower-bound (IBLBO),\nthe learned hidden representation is shown to be maximally expressive about the\npopularity target while maximally compressive to the noise in micro-video\nfeatures. Furthermore, the Bayesian product-of-experts principle is applied to\nthe multimodal encoder, where the decision for information keeping or\ndiscarding is made comprehensively with all available modalities. Extensive\nexperiments conducted on a public dataset and a dataset we collect from Xigua\ndemonstrate the effectiveness of the proposed MMVED framework.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 06:08:16 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhu", "Yaochen", ""], ["Xie", "Jiayi", ""], ["Chen", "Zhenzhong", ""]]}, {"id": "2003.12729", "submitter": "Zheng Ge", "authors": "Xin Huang, Zheng Ge, Zequn Jie and Osamu Yoshie", "title": "NMS by Representative Region: Towards Crowded Pedestrian Detection by\n  Proposal Pairing", "comments": "Accepted by CVPR2020. The first two authors contributed equally, and\n  are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although significant progress has been made in pedestrian detection recently,\npedestrian detection in crowded scenes is still challenging. The heavy\nocclusion between pedestrians imposes great challenges to the standard\nNon-Maximum Suppression (NMS). A relative low threshold of intersection over\nunion (IoU) leads to missing highly overlapped pedestrians, while a higher one\nbrings in plenty of false positives. To avoid such a dilemma, this paper\nproposes a novel Representative Region NMS approach leveraging the less\noccluded visible parts, effectively removing the redundant boxes without\nbringing in many false positives. To acquire the visible parts, a novel\nPaired-Box Model (PBM) is proposed to simultaneously predict the full and\nvisible boxes of a pedestrian. The full and visible boxes constitute a pair\nserving as the sample unit of the model, thus guaranteeing a strong\ncorrespondence between the two boxes throughout the detection pipeline.\nMoreover, convenient feature integration of the two boxes is allowed for the\nbetter performance on both full and visible pedestrian detection tasks.\nExperiments on the challenging CrowdHuman and CityPersons benchmarks\nsufficiently validate the effectiveness of the proposed approach on pedestrian\ndetection in the crowded situation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 06:33:54 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 09:05:54 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Huang", "Xin", ""], ["Ge", "Zheng", ""], ["Jie", "Zequn", ""], ["Yoshie", "Osamu", ""]]}, {"id": "2003.12735", "submitter": "Chih-Hui Ho", "authors": "Chih-Hui Ho, Bo Liu, Tz-Ying Wu, Nuno Vasconcelos", "title": "Exploit Clues from Views: Self-Supervised and Regularized Learning for\n  Multiview Object Recognition", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview recognition has been well studied in the literature and achieves\ndecent performance in object recognition and retrieval task. However, most\nprevious works rely on supervised learning and some impractical underlying\nassumptions, such as the availability of all views in training and inference\ntime. In this work, the problem of multiview self-supervised learning (MV-SSL)\nis investigated, where only image to object association is given. Given this\nsetup, a novel surrogate task for self-supervised learning is proposed by\npursuing \"object invariant\" representation. This is solved by randomly\nselecting an image feature of an object as object prototype, accompanied with\nmultiview consistency regularization, which results in view invariant\nstochastic prototype embedding (VISPE). Experiments shows that the recognition\nand retrieval results using VISPE outperform that of other self-supervised\nlearning methods on seen and unseen data. VISPE can also be applied to\nsemi-supervised scenario and demonstrates robust performance with limited data\navailable. Code is available at https://github.com/chihhuiho/VISPE\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 07:06:06 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ho", "Chih-Hui", ""], ["Liu", "Bo", ""], ["Wu", "Tz-Ying", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "2003.12737", "submitter": "Kirill Gavrilyuk", "authors": "Kirill Gavrilyuk, Ryan Sanford, Mehrsan Javan, Cees G. M. Snoek", "title": "Actor-Transformers for Group Activity Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives to recognize individual actions and group activities from\nvideos. While existing solutions for this challenging problem explicitly model\nspatial and temporal relationships based on location of individual actors, we\npropose an actor-transformer model able to learn and selectively extract\ninformation relevant for group activity recognition. We feed the transformer\nwith rich actor-specific static and dynamic representations expressed by\nfeatures from a 2D pose network and 3D CNN, respectively. We empirically study\ndifferent ways to combine these representations and show their complementary\nbenefits. Experiments show what is important to transform and how it should be\ntransformed. What is more, actor-transformers achieve state-of-the-art results\non two publicly available benchmarks for group activity recognition,\noutperforming the previous best published results by a considerable margin.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 07:21:58 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gavrilyuk", "Kirill", ""], ["Sanford", "Ryan", ""], ["Javan", "Mehrsan", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "2003.12739", "submitter": "Deniz Yuret", "authors": "Ozan Arkan Can, \\.Ilker Kesen, Deniz Yuret", "title": "BiLingUNet: Image Segmentation by Modulating Top-Down and Bottom-Up\n  Visual Processing with Referring Expressions", "comments": "18 pages, 3 figures, submitted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present BiLingUNet, a state-of-the-art model for image segmentation using\nreferring expressions. BiLingUNet uses language to customize visual filters and\noutperforms approaches that concatenate a linguistic representation to the\nvisual input. We find that using language to modulate both bottom-up and\ntop-down visual processing works better than just making the top-down\nprocessing language-conditional. We argue that common 1x1 language-conditional\nfilters cannot represent relational concepts and experimentally demonstrate\nthat wider filters work better. Our model achieves state-of-the-art performance\non four referring expression datasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 07:54:03 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Can", "Ozan Arkan", ""], ["Kesen", "\u0130lker", ""], ["Yuret", "Deniz", ""]]}, {"id": "2003.12751", "submitter": "Kaixuan Wei", "authors": "Kaixuan Wei, Ying Fu, Jiaolong Yang, Hua Huang", "title": "A Physics-based Noise Formation Model for Extreme Low-light Raw\n  Denoising", "comments": "Accepted to CVPR 2020 (oral); code is available at\n  https://github.com/Vandermode/NoiseModel", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lacking rich and realistic data, learned single image denoising algorithms\ngeneralize poorly to real raw images that do not resemble the data used for\ntraining. Although the problem can be alleviated by the heteroscedastic\nGaussian model for noise synthesis, the noise sources caused by digital camera\nelectronics are still largely overlooked, despite their significant effect on\nraw measurement, especially under extremely low-light condition. To address\nthis issue, we present a highly accurate noise formation model based on the\ncharacteristics of CMOS photosensors, thereby enabling us to synthesize\nrealistic samples that better match the physics of image formation process.\nGiven the proposed noise model, we additionally propose a method to calibrate\nthe noise parameters for available modern digital cameras, which is simple and\nreproducible for any new device. We systematically study the generalizability\nof a neural network trained with existing schemes, by introducing a new\nlow-light denoising dataset that covers many modern digital cameras from\ndiverse brands. Extensive empirical results collectively show that by utilizing\nour proposed noise formation model, a network can reach the capability as if it\nhad been trained with rich real data, which demonstrates the effectiveness of\nour noise formation model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 09:16:48 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 04:58:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Wei", "Kaixuan", ""], ["Fu", "Ying", ""], ["Yang", "Jiaolong", ""], ["Huang", "Hua", ""]]}, {"id": "2003.12753", "submitter": "Heming Zhu", "authors": "Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang,\n  Shuguang Cui, Xiaoguang Han", "title": "Deep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction\n  from Single Images", "comments": "23 pages, 9 figures. For project page, see\n  https://kv2000.github.io/2020/03/25/deepFashion3DRevisited/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-fidelity clothing reconstruction is the key to achieving photorealism in\na wide range of applications including human digitization, virtual try-on, etc.\nRecent advances in learning-based approaches have accomplished unprecedented\naccuracy in recovering unclothed human shape and pose from single images,\nthanks to the availability of powerful statistical models, e.g. SMPL, learned\nfrom a large number of body scans. In contrast, modeling and recovering clothed\nhuman and 3D garments remains notoriously difficult, mostly due to the lack of\nlarge-scale clothing models available for the research community. We propose to\nfill this gap by introducing Deep Fashion3D, the largest collection to date of\n3D garment models, with the goal of establishing a novel benchmark and dataset\nfor the evaluation of image-based garment reconstruction systems. Deep\nFashion3D contains 2078 models reconstructed from real garments, which covers\n10 different categories and 563 garment instances. It provides rich annotations\nincluding 3D feature lines, 3D body pose and the corresponded multi-view real\nimages. In addition, each garment is randomly posed to enhance the variety of\nreal clothing deformations. To demonstrate the advantage of Deep Fashion3D, we\npropose a novel baseline approach for single-view garment reconstruction, which\nleverages the merits of both mesh and implicit representations. A novel\nadaptable template is proposed to enable the learning of all types of clothing\nin a single network. Extensive experiments have been conducted on the proposed\ndataset to verify its significance and usefulness. We will make Deep Fashion3D\npublicly available upon publication.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 09:20:04 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 12:43:49 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Zhu", "Heming", ""], ["Cao", "Yu", ""], ["Jin", "Hang", ""], ["Chen", "Weikai", ""], ["Du", "Dong", ""], ["Wang", "Zhangye", ""], ["Cui", "Shuguang", ""], ["Han", "Xiaoguang", ""]]}, {"id": "2003.12760", "submitter": "Mingyi Zhou", "authors": "Mingyi Zhou, Jing Wu, Yipeng Liu, Xiaolin Huang, Shuaicheng Liu, Xiang\n  Zhang, Ce Zhu", "title": "Adversarial Imitation Attack", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are known to be vulnerable to adversarial examples. A\npractical adversarial attack should require as little as possible knowledge of\nattacked models. Current substitute attacks need pre-trained models to generate\nadversarial examples and their attack success rates heavily rely on the\ntransferability of adversarial examples. Current score-based and decision-based\nattacks require lots of queries for the attacked models. In this study, we\npropose a novel adversarial imitation attack. First, it produces a replica of\nthe attacked model by a two-player game like the generative adversarial\nnetworks (GANs). The objective of the generative model is to generate examples\nthat lead the imitation model returning different outputs with the attacked\nmodel. The objective of the imitation model is to output the same labels with\nthe attacked model under the same inputs. Then, the adversarial examples\ngenerated by the imitation model are utilized to fool the attacked model.\nCompared with the current substitute attacks, imitation attacks can use less\ntraining data to produce a replica of the attacked model and improve the\ntransferability of adversarial examples. Experiments demonstrate that our\nimitation attack requires less training data than the black-box substitute\nattacks, but achieves an attack success rate close to the white-box attack on\nunseen data with no query.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 10:02:49 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 05:10:40 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhou", "Mingyi", ""], ["Wu", "Jing", ""], ["Liu", "Yipeng", ""], ["Huang", "Xiaolin", ""], ["Liu", "Shuaicheng", ""], ["Zhang", "Xiang", ""], ["Zhu", "Ce", ""]]}, {"id": "2003.12767", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson, Jason L. Williams,\n  Yuxuan Xia, Karl Granstr\\\"om", "title": "Trajectory Poisson multi-Bernoulli filters", "comments": "Matlab code is provided at https://github.com/Agarciafernandez/MTT", "journal-ref": "in IEEE Transactions on Signal Processing, vol. 68, pp. 4933-4945,\n  2020", "doi": "10.1109/TSP.2020.3017046", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two trajectory Poisson multi-Bernoulli (TPMB) filters for\nmulti-target tracking: one to estimate the set of alive trajectories at each\ntime step and another to estimate the set of all trajectories, which includes\nalive and dead trajectories, at each time step. The filters are based on\npropagating a Poisson multi-Bernoulli (PMB) density on the corresponding set of\ntrajectories through the filtering recursion. After the update step, the\nposterior is a PMB mixture (PMBM) so, in order to obtain a PMB density, a\nKullback-Leibler divergence minimisation on an augmented space is performed.\nThe developed filters are computationally lighter alternatives to the\ntrajectory PMBM filters, which provide the closed-form recursion for sets of\ntrajectories with Poisson birth model, and are shown to outperform previous\nmulti-target tracking algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 11:04:45 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:07:32 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 12:59:16 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""], ["Williams", "Jason L.", ""], ["Xia", "Yuxuan", ""], ["Granstr\u00f6m", "Karl", ""]]}, {"id": "2003.12769", "submitter": "Wenchao Du", "authors": "Wenchao Du, Hu Chen and Hongyu Yang", "title": "Learning Invariant Representation for Unsupervised Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, cross domain transfer has been applied for unsupervised image\nrestoration tasks. However, directly applying existing frameworks would lead to\ndomain-shift problems in translated images due to lack of effective\nsupervision. Instead, we propose an unsupervised learning method that\nexplicitly learns invariant presentation from noisy data and reconstructs clear\nobservations. To do so, we introduce discrete disentangling representation and\nadversarial domain adaption into general domain transfer framework, aided by\nextra self-supervised modules including background and semantic consistency\nconstraints, learning robust representation under dual domain constraints, such\nas feature and image domains. Experiments on synthetic and real noise removal\ntasks show the proposed method achieves comparable performance with other\nstate-of-the-art supervised and unsupervised methods, while having faster and\nstable convergence than other domain adaption methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 11:20:21 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Du", "Wenchao", ""], ["Chen", "Hu", ""], ["Yang", "Hongyu", ""]]}, {"id": "2003.12779", "submitter": "Haoyu Ma", "authors": "Juncheng Zhang, Qingmin Liao, Shaojun Liu, Haoyu Ma, Wenming Yang,\n  Jing-Hao Xue", "title": "Real-MFF: A Large Realistic Multi-focus Image Dataset with Ground Truth", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2020.08.002", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-focus image fusion, a technique to generate an all-in-focus image from\ntwo or more partially-focused source images, can benefit many computer vision\ntasks. However, currently there is no large and realistic dataset to perform\nconvincing evaluation and comparison of algorithms in multi-focus image fusion.\nMoreover, it is difficult to train a deep neural network for multi-focus image\nfusion without a suitable dataset. In this letter, we introduce a large and\nrealistic multi-focus dataset called Real-MFF, which contains 710 pairs of\nsource images with corresponding ground truth images. The dataset is generated\nby light field images, and both the source images and the ground truth images\nare realistic. To serve as both a well-established benchmark for existing\nmulti-focus image fusion algorithms and an appropriate training dataset for\nfuture development of deep-learning-based methods, the dataset contains a\nvariety of scenes, including buildings, plants, humans, shopping malls, squares\nand so on. We also evaluate 10 typical multi-focus algorithms on this dataset\nfor the purpose of illustration.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 12:33:46 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 03:31:32 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 11:25:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Zhang", "Juncheng", ""], ["Liao", "Qingmin", ""], ["Liu", "Shaojun", ""], ["Ma", "Haoyu", ""], ["Yang", "Wenming", ""], ["Xue", "Jing-Hao", ""]]}, {"id": "2003.12783", "submitter": "Guangshuai Gao", "authors": "Guangshuai Gao, Junyu Gao, Qingjie Liu, Qi Wang, Yunhong Wang", "title": "CNN-based Density Estimation and Crowd Counting: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately estimating the number of objects in a single image is a\nchallenging yet meaningful task and has been applied in many applications such\nas urban planning and public safety. In the various object counting tasks,\ncrowd counting is particularly prominent due to its specific significance to\nsocial security and development. Fortunately, the development of the techniques\nfor crowd counting can be generalized to other related fields such as vehicle\ncounting and environment survey, if without taking their characteristics into\naccount. Therefore, many researchers are devoting to crowd counting, and many\nexcellent works of literature and works have spurted out. In these works, they\nare must be helpful for the development of crowd counting. However, the\nquestion we should consider is why they are effective for this task. Limited by\nthe cost of time and energy, we cannot analyze all the algorithms. In this\npaper, we have surveyed over 220 works to comprehensively and systematically\nstudy the crowd counting models, mainly CNN-based density map estimation\nmethods. Finally, according to the evaluation metrics, we select the top three\nperformers on their crowd counting datasets and analyze their merits and\ndrawbacks. Through our analysis, we expect to make reasonable inference and\nprediction for the future development of crowd counting, and meanwhile, it can\nalso provide feasible solutions for the problem of object counting in other\nfields. We provide the density maps and prediction results of some mainstream\nalgorithm in the validation set of NWPU dataset for comparison and testing.\nMeanwhile, density map generation and evaluation tools are also provided. All\nthe codes and evaluation results are made publicly available at\nhttps://github.com/gaoguangshuai/survey-for-crowd-counting.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 13:17:30 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gao", "Guangshuai", ""], ["Gao", "Junyu", ""], ["Liu", "Qingjie", ""], ["Wang", "Qi", ""], ["Wang", "Yunhong", ""]]}, {"id": "2003.12789", "submitter": "Chenyang Lei", "authors": "Chenyang Lei, Xuhua Huang, Mengdi Zhang, Qiong Yan, Wenxiu Sun and\n  Qifeng Chen", "title": "Polarized Reflection Removal with Perfect Alignment in the Wild", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel formulation to removing reflection from polarized images\nin the wild. We first identify the misalignment issues of existing reflection\nremoval datasets where the collected reflection-free images are not perfectly\naligned with input mixed images due to glass refraction. Then we build a new\ndataset with more than 100 types of glass in which obtained transmission images\nare perfectly aligned with input mixed images. Second, capitalizing on the\nspecial relationship between reflection and polarized light, we propose a\npolarized reflection removal model with a two-stage architecture. In addition,\nwe design a novel perceptual NCC loss that can improve the performance of\nreflection removal and general image decomposition tasks. We conduct extensive\nexperiments, and results suggest that our model outperforms state-of-the-art\nmethods on reflection removal.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 13:29:31 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Lei", "Chenyang", ""], ["Huang", "Xuhua", ""], ["Zhang", "Mengdi", ""], ["Yan", "Qiong", ""], ["Sun", "Wenxiu", ""], ["Chen", "Qifeng", ""]]}, {"id": "2003.12798", "submitter": "Qihang Yu", "authors": "Qihang Yu, Yingwei Li, Jieru Mei, Yuyin Zhou, Alan L. Yuille", "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Convolution Neural Networks (CNNs) have been widely applied to 3D scene\nunderstanding, such as video analysis and volumetric image recognition.\nHowever, 3D networks can easily lead to over-parameterization which incurs\nexpensive computation cost. In this paper, we propose Channel-wise Automatic\nKErnel Shrinking (CAKES), to enable efficient 3D learning by shrinking standard\n3D convolutions into a set of economic operations e.g., 1D, 2D convolutions.\nUnlike previous methods, CAKES performs channel-wise kernel shrinkage, which\nenjoys the following benefits: 1) enabling operations deployed in every layer\nto be heterogeneous, so that they can extract diverse and complementary\ninformation to benefit the learning process; and 2) allowing for an efficient\nand flexible replacement design, which can be generalized to both\nspatial-temporal and volumetric data. Further, we propose a new search space\nbased on CAKES, so that the replacement configuration can be determined\nautomatically for simplifying 3D networks. CAKES shows superior performance to\nother methods with similar model size, and it also achieves comparable\nperformance to state-of-the-art with much fewer parameters and computational\ncosts on tasks including 3D medical imaging segmentation and video action\nrecognition. Codes and models are available at\nhttps://github.com/yucornetto/CAKES\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 14:21:12 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 21:46:21 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2020 19:03:30 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yu", "Qihang", ""], ["Li", "Yingwei", ""], ["Mei", "Jieru", ""], ["Zhou", "Yuyin", ""], ["Yuille", "Alan L.", ""]]}, {"id": "2003.12824", "submitter": "Hiroshi Kaizuka", "authors": "Hiroshi Kaizuka", "title": "Gradient-based Data Augmentation for Semi-Supervised Learning", "comments": "The lower bound of the inequality (line 2 on page 6 ) changed to fit\n  fact 1 (2). Typos in (9) corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semi-supervised learning (SSL), a technique called consistency\nregularization (CR) achieves high performance. It has been proved that the\ndiversity of data used in CR is extremely important to obtain a model with high\ndiscrimination performance by CR. We propose a new data augmentation\n(Gradient-based Data Augmentation (GDA)) that is deterministically calculated\nfrom the image pixel value gradient of the posterior probability distribution\nthat is the model output. We aim to secure effective data diversity for CR by\nutilizing three types of GDA. On the other hand, it has been demonstrated that\nthe mixup method for labeled data and unlabeled data is also effective in SSL.\nWe propose an SSL method named MixGDA by combining various mixup methods and\nGDA. The discrimination performance achieved by MixGDA is evaluated against the\n13-layer CNN that is used as standard in SSL research. As a result, for\nCIFAR-10 (4000 labels), MixGDA achieves the same level of performance as the\nbest performance ever achieved. For SVHN (250 labels, 500 labels and 1000\nlabels) and CIFAR-100 (10000 labels), MixGDA achieves state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 15:57:20 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 01:10:27 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Kaizuka", "Hiroshi", ""]]}, {"id": "2003.12841", "submitter": "Simone Fontana", "authors": "Simone Fontana, Daniele Cattaneo, Augusto Luis Ballardini, Matteo\n  Vaghi and Domenico Giorgio Sorrenti", "title": "A Benchmark for Point Clouds Registration Algorithms", "comments": null, "journal-ref": null, "doi": "10.1016/j.robot.2021.103734", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Point clouds registration is a fundamental step of many point clouds\nprocessing pipelines; however, most algorithms are tested on data collected\nad-hoc and not shared with the research community. These data often cover only\na very limited set of use cases; therefore, the results cannot be generalised.\nPublic datasets proposed until now, taken individually, cover only a few kinds\nof environment and mostly a single sensor. For these reasons, we developed a\nbenchmark, for localization and mapping applications, using multiple publicly\navailable datasets. In this way, we have been able to cover many kinds of\nenvironments and many kinds of sensor that can produce point clouds.\nFurthermore, the ground truth has been thoroughly inspected and evaluated to\nensure its quality. For some of the datasets, the accuracy of the ground truth\nsystem was not reported by the original authors, therefore we estimated it with\nour own novel method, based on an iterative registration algorithm. Along with\nthe data, we provide a broad set of registration problems, chosen to cover\ndifferent types of initial misalignment, various degrees of overlap, and\ndifferent kinds of registration problems. Lastly, we propose a metric to\nmeasure the performances of registration algorithms: it combines the commonly\nused rotation and translation errors together, to allow an objective comparison\nof the alignments. This work aims at encouraging authors to use a public and\nshared benchmark, instead than data collected ad-hoc, to ensure objectivity and\nrepeatability, two fundamental characteristics in any scientific field.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:02:26 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 09:11:23 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Fontana", "Simone", ""], ["Cattaneo", "Daniele", ""], ["Ballardini", "Augusto Luis", ""], ["Vaghi", "Matteo", ""], ["Sorrenti", "Domenico Giorgio", ""]]}, {"id": "2003.12849", "submitter": "Minghao Xu", "authors": "Minghao Xu, Hang Wang, Bingbing Ni, Qi Tian, Wenjun Zhang", "title": "Cross-domain Detection via Graph-induced Prototype Alignment", "comments": "Accepted as ORAL presentation at IEEE Conference on Computer Vision\n  and Pattern Recognition (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying the knowledge of an object detector trained on a specific domain\ndirectly onto a new domain is risky, as the gap between two domains can\nseverely degrade model's performance. Furthermore, since different instances\ncommonly embody distinct modal information in object detection scenario, the\nfeature alignment of source and target domain is hard to be realized. To\nmitigate these problems, we propose a Graph-induced Prototype Alignment (GPA)\nframework to seek for category-level domain alignment via elaborate prototype\nrepresentations. In the nutshell, more precise instance-level features are\nobtained through graph-based information propagation among region proposals,\nand, on such basis, the prototype representation of each class is derived for\ncategory-level domain alignment. In addition, in order to alleviate the\nnegative effect of class-imbalance on domain adaptation, we design a\nClass-reweighted Contrastive Loss to harmonize the adaptation training process.\nCombining with Faster R-CNN, the proposed framework conducts feature alignment\nin a two-stage manner. Comprehensive results on various cross-domain detection\ntasks demonstrate that our approach outperforms existing methods with a\nremarkable margin. Our code is available at\nhttps://github.com/ChrisAllenMing/GPA-detection.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:46:55 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Xu", "Minghao", ""], ["Wang", "Hang", ""], ["Ni", "Bingbing", ""], ["Tian", "Qi", ""], ["Zhang", "Wenjun", ""]]}, {"id": "2003.12857", "submitter": "Chen Wei", "authors": "Chen Wei, Chuang Niu, Yiping Tang, Yue Wang, Haihong Hu, Jimin Liang", "title": "NPENAS: Neural Predictor Guided Evolution for Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) is a promising method for automatically\ndesign neural architectures. NAS adopts a search strategy to explore the\npredefined search space to find outstanding performance architecture with the\nminimum searching costs. Bayesian optimization and evolutionary algorithms are\ntwo commonly used search strategies, but they suffer from computationally\nexpensive, challenge to implement or inefficient exploration ability. In this\npaper, we propose a neural predictor guided evolutionary algorithm to enhance\nthe exploration ability of EA for NAS (NPENAS) and design two kinds of neural\npredictors. The first predictor is defined from Bayesian optimization and we\npropose a graph-based uncertainty estimation network as a surrogate model that\nis easy to implement and computationally efficient. The second predictor is a\ngraph-based neural network that directly outputs the performance prediction of\nthe input neural architecture. The NPENAS using the two neural predictors are\ndenoted as NPENAS-BO and NPENAS-NP respectively. In addition, we introduce a\nnew random architecture sampling method to overcome the drawbacks of the\nexisting sampling method. Extensive experiments demonstrate the superiority of\nNPENAS. Quantitative results on three NAS search spaces indicate that both\nNPENAS-BO and NPENAS-NP outperform most existing NAS algorithms, with NPENAS-BO\nachieving state-of-the-art performance on NASBench-201 and NPENAS-NP on\nNASBench-101 and DARTS, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 17:56:31 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 16:32:49 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 06:22:06 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Wei", "Chen", ""], ["Niu", "Chuang", ""], ["Tang", "Yiping", ""], ["Wang", "Yue", ""], ["Hu", "Haihong", ""], ["Liang", "Jimin", ""]]}, {"id": "2003.12862", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Sijia Liu, Shiyu Chang, Yu Cheng, Lisa Amini and\n  Zhangyang Wang", "title": "Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretrained models from self-supervision are prevalently used in fine-tuning\ndownstream tasks faster or for better accuracy. However, gaining robustness\nfrom pretraining is left unexplored. We introduce adversarial training into\nself-supervision, to provide general-purpose robust pre-trained models for the\nfirst time. We find these robust pre-trained models can benefit the subsequent\nfine-tuning in two ways: i) boosting final model robustness; ii) saving the\ncomputation cost, if proceeding towards adversarial fine-tuning. We conduct\nextensive experiments to demonstrate that the proposed framework achieves large\nperformance margins (eg, 3.83% on robust accuracy and 1.3% on standard\naccuracy, on the CIFAR-10 dataset), compared with the conventional end-to-end\nadversarial training baseline. Moreover, we find that different self-supervised\npre-trained models have a diverse adversarial vulnerability. It inspires us to\nensemble several pretraining tasks, which boosts robustness more. Our ensemble\nstrategy contributes to a further improvement of 3.59% on robust accuracy,\nwhile maintaining a slightly higher standard accuracy on CIFAR-10. Our codes\nare available at https://github.com/TAMU-VITA/Adv-SS-Pretraining.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 18:28:33 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Tianlong", ""], ["Liu", "Sijia", ""], ["Chang", "Shiyu", ""], ["Cheng", "Yu", ""], ["Amini", "Lisa", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2003.12869", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Ser-Nam Lim", "title": "One-Shot Domain Adaptation For Face Generation", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework capable of generating face images that\nfall into the same distribution as that of a given one-shot example. We\nleverage a pre-trained StyleGAN model that already learned the generic face\ndistribution. Given the one-shot target, we develop an iterative optimization\nscheme that rapidly adapts the weights of the model to shift the output's\nhigh-level distribution to the target's. To generate images of the same\ndistribution, we introduce a style-mixing technique that transfers the\nlow-level statistics from the target to faces randomly generated with the\nmodel. With that, we are able to generate an unlimited number of faces that\ninherit from the distribution of both generic human faces and the one-shot\nexample. The newly generated faces can serve as augmented training data for\nother downstream tasks. Such setting is appealing as it requires labeling very\nfew, or even one example, in the target domain, which is often the case of\nreal-world face manipulations that result from a variety of unknown and unique\ndistributions, each with extremely low prevalence. We show the effectiveness of\nour one-shot approach for detecting face manipulations and compare it with\nother few-shot domain adaptation methods qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 18:50:13 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yang", "Chao", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2003.12870", "submitter": "Alexander Naumann", "authors": "Alexander Naumann, Laura D\\\"orr, Niels Ole Salscheider, Kai Furmans", "title": "Refined Plane Segmentation for Cuboid-Shaped Objects by Leveraging Edge\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the area of plane segmentation from single RGB images show\nstrong accuracy improvements and now allow a reliable segmentation of indoor\nscenes into planes. Nonetheless, fine-grained details of these segmentation\nmasks are still lacking accuracy, thus restricting the usability of such\ntechniques on a larger scale in numerous applications, such as inpainting for\nAugmented Reality use cases. We propose a post-processing algorithm to align\nthe segmented plane masks with edges detected in the image. This allows us to\nincrease the accuracy of state-of-the-art approaches, while limiting ourselves\nto cuboid-shaped objects. Our approach is motivated by logistics, where this\nassumption is valid and refined planes can be used to perform robust object\ndetection without the need for supervised learning. Results for two baselines\nand our approach are reported on our own dataset, which we made publicly\navailable. The results show a consistent improvement over the state-of-the-art.\nThe influence of the prior segmentation and the edge detection is investigated\nand finally, areas for future research are proposed.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 18:51:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Naumann", "Alexander", ""], ["D\u00f6rr", "Laura", ""], ["Salscheider", "Niels Ole", ""], ["Furmans", "Kai", ""]]}, {"id": "2003.12929", "submitter": "Fengting Yang", "authors": "Fengting Yang, Qian Sun, Hailin Jin, Zihan Zhou", "title": "Superpixel Segmentation with Fully Convolutional Networks", "comments": "16 pages, 15 figures, to be published in CVPR'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision, superpixels have been widely used as an effective way to\nreduce the number of image primitives for subsequent processing. But only a few\nattempts have been made to incorporate them into deep neural networks. One main\nreason is that the standard convolution operation is defined on regular grids\nand becomes inefficient when applied to superpixels. Inspired by an\ninitialization strategy commonly adopted by traditional superpixel algorithms,\nwe present a novel method that employs a simple fully convolutional network to\npredict superpixels on a regular image grid. Experimental results on benchmark\ndatasets show that our method achieves state-of-the-art superpixel segmentation\nperformance while running at about 50fps. Based on the predicted superpixels,\nwe further develop a downsampling/upsampling scheme for deep networks with the\ngoal of generating high-resolution outputs for dense prediction tasks.\nSpecifically, we modify a popular network architecture for stereo matching to\nsimultaneously predict superpixels and disparities. We show that improved\ndisparity estimation accuracy can be obtained on public datasets.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 02:42:07 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yang", "Fengting", ""], ["Sun", "Qian", ""], ["Jin", "Hailin", ""], ["Zhou", "Zihan", ""]]}, {"id": "2003.12931", "submitter": "Wenjun Zhou", "authors": "Wenjun Zhou, Yuheng Deng, Bo Peng, Dong Liang and Shun'ichi Kaneko", "title": "Co-occurrence Background Model with Superpixels for Robust Background\n  Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background initialization is an important step in many high-level\napplications of video processing,ranging from video surveillance to video\ninpainting.However,this process is often affected by practical challenges such\nas illumination changes,background motion,camera jitter and intermittent\nmovement,etc.In this paper,we develop a co-occurrence background model with\nsuperpixel segmentation for robust background initialization. We first\nintroduce a novel co-occurrence background modeling method called as\nCo-occurrence Pixel-Block Pairs(CPB)to generate a reliable initial background\nmodel,and the superpixel segmentation is utilized to further acquire the\nspatial texture Information of foreground and background.Then,the initial\nbackground can be determined by combining the foreground extraction results\nwith the superpixel segmentation information.Experimental results obtained from\nthe dataset of the challenging benchmark(SBMnet)validate it's performance under\nvarious challenges.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 02:48:41 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhou", "Wenjun", ""], ["Deng", "Yuheng", ""], ["Peng", "Bo", ""], ["Liang", "Dong", ""], ["Kaneko", "Shun'ichi", ""]]}, {"id": "2003.12943", "submitter": "Yuhong Guo", "authors": "Zhen Zhao, Yuhong Guo, Haifeng Shen, Jieping Ye", "title": "Adaptive Object Detection with Dual Multi-Label Prediction", "comments": "ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel end-to-end unsupervised deep domain\nadaptation model for adaptive object detection by exploiting multi-label object\nrecognition as a dual auxiliary task. The model exploits multi-label prediction\nto reveal the object category information in each image and then uses the\nprediction results to perform conditional adversarial global feature alignment,\nsuch that the multi-modal structure of image features can be tackled to bridge\nthe domain divergence at the global feature level while preserving the\ndiscriminability of the features. Moreover, we introduce a prediction\nconsistency regularization mechanism to assist object detection, which uses the\nmulti-label prediction results as an auxiliary regularization information to\nensure consistent object category discoveries between the object recognition\ntask and the object detection task. Experiments are conducted on a few\nbenchmark datasets and the results show the proposed model outperforms the\nstate-of-the-art comparison methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 04:23:22 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 00:25:27 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Zhao", "Zhen", ""], ["Guo", "Yuhong", ""], ["Shen", "Haifeng", ""], ["Ye", "Jieping", ""]]}, {"id": "2003.12944", "submitter": "Yuhong Guo", "authors": "Zhenpeng Li, Zhen Zhao, Yuhong Guo, Haifeng Shen, Jieping Ye", "title": "Mutual Learning Network for Multi-Source Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early Unsupervised Domain Adaptation (UDA) methods have mostly assumed the\nsetting of a single source domain, where all the labeled source data come from\nthe same distribution. However, in practice the labeled data can come from\nmultiple source domains with different distributions. In such scenarios, the\nsingle source domain adaptation methods can fail due to the existence of domain\nshifts across different source domains and multi-source domain adaptation\nmethods need to be designed. In this paper, we propose a novel multi-source\ndomain adaptation method, Mutual Learning Network for Multiple Source Domain\nAdaptation (ML-MSDA). Under the framework of mutual learning, the proposed\nmethod pairs the target domain with each single source domain to train a\nconditional adversarial domain adaptation network as a branch network, while\ntaking the pair of the combined multi-source domain and target domain to train\na conditional adversarial adaptive network as the guidance network. The\nmultiple branch networks are aligned with the guidance network to achieve\nmutual learning by enforcing JS-divergence regularization over their prediction\nprobability distributions on the corresponding target data. We conduct\nextensive experiments on multiple multi-source domain adaptation benchmark\ndatasets. The results show the proposed ML-MSDA method outperforms the\ncomparison methods and achieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 04:31:43 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Zhenpeng", ""], ["Zhao", "Zhen", ""], ["Guo", "Yuhong", ""], ["Shen", "Haifeng", ""], ["Ye", "Jieping", ""]]}, {"id": "2003.12949", "submitter": "Yiming Li", "authors": "Yiming Li, Changhong Fu, Fangqiang Ding, Ziyuan Huang, Geng Lu", "title": "AutoTrack: Towards High-Performance Visual Tracking for UAV with\n  Automatic Spatio-Temporal Regularization", "comments": "2020 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing trackers based on discriminative correlation filters (DCF) try\nto introduce predefined regularization term to improve the learning of target\nobjects, e.g., by suppressing background learning or by restricting change rate\nof correlation filters. However, predefined parameters introduce much effort in\ntuning them and they still fail to adapt to new situations that the designer\ndid not think of. In this work, a novel approach is proposed to online\nautomatically and adaptively learn spatio-temporal regularization term.\nSpatially local response map variation is introduced as spatial regularization\nto make DCF focus on the learning of trust-worthy parts of the object, and\nglobal response map variation determines the updating rate of the filter.\nExtensive experiments on four UAV benchmarks have proven the superiority of our\nmethod compared to the state-of-the-art CPU- and GPU-based trackers, with a\nspeed of ~60 frames per second running on a single CPU.\n  Our tracker is additionally proposed to be applied in UAV localization.\nConsiderable tests in the indoor practical scenarios have proven the\neffectiveness and versatility of our localization method. The code is available\nat https://github.com/vision4robotics/AutoTrack.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 05:02:25 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Yiming", ""], ["Fu", "Changhong", ""], ["Ding", "Fangqiang", ""], ["Huang", "Ziyuan", ""], ["Lu", "Geng", ""]]}, {"id": "2003.12957", "submitter": "Xianfang Zeng", "authors": "Xianfang Zeng, Yusu Pan, Mengmeng Wang, Jiangning Zhang, Yong Liu", "title": "Realistic Face Reenactment via Self-Supervised Disentangling of Identity\n  and Pose", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have shown how realistic talking face images can be obtained\nunder the supervision of geometry guidance, e.g., facial landmark or boundary.\nTo alleviate the demand for manual annotations, in this paper, we propose a\nnovel self-supervised hybrid model (DAE-GAN) that learns how to reenact face\nnaturally given large amounts of unlabeled videos. Our approach combines two\ndeforming autoencoders with the latest advances in the conditional generation.\nOn the one hand, we adopt the deforming autoencoder to disentangle identity and\npose representations. A strong prior in talking face videos is that each frame\ncan be encoded as two parts: one for video-specific identity and the other for\nvarious poses. Inspired by that, we utilize a multi-frame deforming autoencoder\nto learn a pose-invariant embedded face for each video. Meanwhile, a\nmulti-scale deforming autoencoder is proposed to extract pose-related\ninformation for each frame. On the other hand, the conditional generator allows\nfor enhancing fine details and overall reality. It leverages the disentangled\nfeatures to generate photo-realistic and pose-alike face images. We evaluate\nour model on VoxCeleb1 and RaFD dataset. Experiment results demonstrate the\nsuperior quality of reenacted images and the flexibility of transferring facial\nmovements between identities.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 06:45:17 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zeng", "Xianfang", ""], ["Pan", "Yusu", ""], ["Wang", "Mengmeng", ""], ["Zhang", "Jiangning", ""], ["Liu", "Yong", ""]]}, {"id": "2003.12962", "submitter": "Changxing Ding", "authors": "Xin Lin, Changxing Ding, Jinquan Zeng, Dacheng Tao", "title": "GPS-Net: Graph Property Sensing Network for Scene Graph Generation", "comments": "Accepted by CVPR 2020 as Oral. Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scene graph generation (SGG) aims to detect objects in an image along with\ntheir pairwise relationships. There are three key properties of scene graph\nthat have been underexplored in recent works: namely, the edge direction\ninformation, the difference in priority between nodes, and the long-tailed\ndistribution of relationships. Accordingly, in this paper, we propose a Graph\nProperty Sensing Network (GPS-Net) that fully explores these three properties\nfor SGG. First, we propose a novel message passing module that augments the\nnode feature with node-specific contextual information and encodes the edge\ndirection information via a tri-linear model. Second, we introduce a node\npriority sensitive loss to reflect the difference in priority between nodes\nduring training. This is achieved by designing a mapping function that adjusts\nthe focusing parameter in the focal loss. Third, since the frequency of\nrelationships is affected by the long-tailed distribution problem, we mitigate\nthis issue by first softening the distribution and then enabling it to be\nadjusted for each subject-object pair according to their visual appearance.\nSystematic experiments demonstrate the effectiveness of the proposed\ntechniques. Moreover, GPS-Net achieves state-of-the-art performance on three\npopular databases: VG, OI, and VRD by significant gains under various settings\nand metrics. The code and models are available at\n\\url{https://github.com/taksau/GPS-Net}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 07:22:31 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Lin", "Xin", ""], ["Ding", "Changxing", ""], ["Zeng", "Jinquan", ""], ["Tao", "Dacheng", ""]]}, {"id": "2003.12971", "submitter": "Yongming Rao", "authors": "Yongming Rao, Jiwen Lu, Jie Zhou", "title": "Global-Local Bidirectional Reasoning for Unsupervised Representation\n  Learning of 3D Point Clouds", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local and global patterns of an object are closely related. Although each\npart of an object is incomplete, the underlying attributes about the object are\nshared among all parts, which makes reasoning the whole object from a single\npart possible. We hypothesize that a powerful representation of a 3D object\nshould model the attributes that are shared between parts and the whole object,\nand distinguishable from other objects. Based on this hypothesis, we propose to\nlearn point cloud representation by bidirectional reasoning between the local\nstructures at different abstraction hierarchies and the global shape without\nhuman supervision. Experimental results on various benchmark datasets\ndemonstrate the unsupervisedly learned representation is even better than\nsupervised representation in discriminative power, generalization ability, and\nrobustness. We show that unsupervisedly trained point cloud models can\noutperform their supervised counterparts on downstream classification tasks.\nMost notably, by simply increasing the channel width of an SSG PointNet++, our\nunsupervised model surpasses the state-of-the-art supervised methods on both\nsynthetic and real-world 3D object classification datasets. We expect our\nobservations to offer a new perspective on learning better representation from\ndata structures instead of human annotations for point cloud understanding.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 08:26:08 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Rao", "Yongming", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2003.12979", "submitter": "Congcong Li", "authors": "Congcong Li, Dawei Du, Libo Zhang, Longyin Wen, Tiejian Luo, Yanjun\n  Wu, Pengfei Zhu", "title": "Spatial Attention Pyramid Network for Unsupervised Domain Adaptation", "comments": "Accepted to ECCV 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised domain adaptation is critical in various computer vision tasks,\nsuch as object detection, instance segmentation, and semantic segmentation,\nwhich aims to alleviate performance degradation caused by domain-shift. Most of\nprevious methods rely on a single-mode distribution of source and target\ndomains to align them with adversarial learning, leading to inferior results in\nvarious scenarios. To that end, in this paper, we design a new spatial\nattention pyramid network for unsupervised domain adaptation. Specifically, we\nfirst build the spatial pyramid representation to capture context information\nof objects at different scales. Guided by the task-specific information, we\ncombine the dense global structure representation and local texture patterns at\neach spatial location effectively using the spatial attention mechanism. In\nthis way, the network is enforced to focus on the discriminative regions with\ncontext information for domain adaption. We conduct extensive experiments on\nvarious challenging datasets for unsupervised domain adaptation on object\ndetection, instance segmentation, and semantic segmentation, which demonstrates\nthat our method performs favorably against the state-of-the-art methods by a\nlarge margin. Our source code is available at\nhttps://isrc.iscas.ac.cn/gitlab/research/domain-adaption.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 09:03:23 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 04:23:21 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 14:50:31 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Congcong", ""], ["Du", "Dawei", ""], ["Zhang", "Libo", ""], ["Wen", "Longyin", ""], ["Luo", "Tiejian", ""], ["Wu", "Yanjun", ""], ["Zhu", "Pengfei", ""]]}, {"id": "2003.12980", "submitter": "Jiahui Huang", "authors": "Jiahui Huang, Sheng Yang, Tai-Jiang Mu, Shi-Min Hu", "title": "ClusterVO: Clustering Moving Instances and Estimating Visual Odometry\n  for Self and Surroundings", "comments": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ClusterVO, a stereo Visual Odometry which simultaneously clusters\nand estimates the motion of both ego and surrounding rigid clusters/objects.\nUnlike previous solutions relying on batch input or imposing priors on scene\nstructure or dynamic object models, ClusterVO is online, general and thus can\nbe used in various scenarios including indoor scene understanding and\nautonomous driving. At the core of our system lies a multi-level probabilistic\nassociation mechanism and a heterogeneous Conditional Random Field (CRF)\nclustering approach combining semantic, spatial and motion information to\njointly infer cluster segmentations online for every frame. The poses of camera\nand dynamic objects are instantly solved through a sliding-window optimization.\nOur system is evaluated on Oxford Multimotion and KITTI dataset both\nquantitatively and qualitatively, reaching comparable results to\nstate-of-the-art solutions on both odometry and dynamic trajectory recovery.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 09:06:28 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Huang", "Jiahui", ""], ["Yang", "Sheng", ""], ["Mu", "Tai-Jiang", ""], ["Hu", "Shi-Min", ""]]}, {"id": "2003.13006", "submitter": "Tobi Delbruck", "authors": "Tobi Delbruck, Shih-Chii Liu", "title": "Data-Driven Neuromorphic DRAM-based CNN and RNN Accelerators", "comments": "To appear in 2019 IEEE Sig. Proc. Soc. Asilomar Conference on\n  Signals, Systems, and Computers Session MP6b: Neuromorphic Computing\n  (Invited)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy consumed by running large deep neural networks (DNNs) on hardware\naccelerators is dominated by the need for lots of fast memory to store both\nstates and weights. This large required memory is currently only economically\nviable through DRAM. Although DRAM is high-throughput and low-cost memory\n(costing 20X less than SRAM), its long random access latency is bad for the\nunpredictable access patterns in spiking neural networks (SNNs). In addition,\naccessing data from DRAM costs orders of magnitude more energy than doing\narithmetic with that data. SNNs are energy-efficient if local memory is\navailable and few spikes are generated. This paper reports on our developments\nover the last 5 years of convolutional and recurrent deep neural network\nhardware accelerators that exploit either spatial or temporal sparsity similar\nto SNNs but achieve SOA throughput, power efficiency and latency even with the\nuse of DRAM for the required storage of the weights and states of large DNNs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 11:45:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Delbruck", "Tobi", ""], ["Liu", "Shih-Chii", ""]]}, {"id": "2003.13017", "submitter": "Zehao Yu", "authors": "Zehao Yu, Shenghua Gao", "title": "Fast-MVSNet: Sparse-to-Dense Multi-View Stereo With Learned Propagation\n  and Gauss-Newton Refinement", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all previous deep learning-based multi-view stereo (MVS) approaches\nfocus on improving reconstruction quality. Besides quality, efficiency is also\na desirable feature for MVS in real scenarios. Towards this end, this paper\npresents a Fast-MVSNet, a novel sparse-to-dense coarse-to-fine framework, for\nfast and accurate depth estimation in MVS. Specifically, in our Fast-MVSNet, we\nfirst construct a sparse cost volume for learning a sparse and high-resolution\ndepth map. Then we leverage a small-scale convolutional neural network to\nencode the depth dependencies for pixels within a local region to densify the\nsparse high-resolution depth map. At last, a simple but efficient Gauss-Newton\nlayer is proposed to further optimize the depth map. On one hand, the\nhigh-resolution depth map, the data-adaptive propagation method and the\nGauss-Newton layer jointly guarantee the effectiveness of our method. On the\nother hand, all modules in our Fast-MVSNet are lightweight and thus guarantee\nthe efficiency of our approach. Besides, our approach is also memory-friendly\nbecause of the sparse depth representation. Extensive experimental results show\nthat our method is 5$\\times$ and 14$\\times$ faster than Point-MVSNet and\nR-MVSNet, respectively, while achieving comparable or even better results on\nthe challenging Tanks and Temples dataset as well as the DTU dataset. Code is\navailable at https://github.com/svip-lab/FastMVSNet.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 13:31:00 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yu", "Zehao", ""], ["Gao", "Shenghua", ""]]}, {"id": "2003.13035", "submitter": "Jiacheng Wei", "authors": "Jiacheng Wei, Guosheng Lin, Kim-Hui Yap, Tzu-Yi Hung, Lihua Xie", "title": "Multi-Path Region Mining For Weakly Supervised 3D Semantic Segmentation\n  on Point Clouds", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds provide intrinsic geometric information and surface context for\nscene understanding. Existing methods for point cloud segmentation require a\nlarge amount of fully labeled data. Using advanced depth sensors, collection of\nlarge scale 3D dataset is no longer a cumbersome process. However, manually\nproducing point-level label on the large scale dataset is time and\nlabor-intensive. In this paper, we propose a weakly supervised approach to\npredict point-level results using weak labels on 3D point clouds. We introduce\nour multi-path region mining module to generate pseudo point-level label from a\nclassification network trained with weak labels. It mines the localization cues\nfor each class from various aspects of the network feature using different\nattention modules. Then, we use the point-level pseudo labels to train a point\ncloud segmentation network in a fully supervised manner. To the best of our\nknowledge, this is the first method that uses cloud-level weak labels on raw 3D\nspace to train a point cloud semantic segmentation network. In our setting, the\n3D weak labels only indicate the classes that appeared in our input sample. We\ndiscuss both scene- and subcloud-level weakly labels on raw 3D point cloud data\nand perform in-depth experiments on them. On ScanNet dataset, our result\ntrained with subcloud-level labels is compatible with some fully supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:13:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wei", "Jiacheng", ""], ["Lin", "Guosheng", ""], ["Yap", "Kim-Hui", ""], ["Hung", "Tzu-Yi", ""], ["Xie", "Lihua", ""]]}, {"id": "2003.13042", "submitter": "Haodong Duan", "authors": "Haodong Duan, Yue Zhao, Yuanjun Xiong, Wentao Liu, Dahua Lin", "title": "Omni-sourced Webly-supervised Learning for Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce OmniSource, a novel framework for leveraging web data to train\nvideo recognition models. OmniSource overcomes the barriers between data\nformats, such as images, short videos, and long untrimmed videos for\nwebly-supervised learning. First, data samples with multiple formats, curated\nby task-specific data collection and automatically filtered by a teacher model,\nare transformed into a unified form. Then a joint-training strategy is proposed\nto deal with the domain gaps between multiple data sources and formats in\nwebly-supervised learning. Several good practices, including data balancing,\nresampling, and cross-dataset mixup are adopted in joint training. Experiments\nshow that by utilizing data from multiple sources and formats, OmniSource is\nmore data-efficient in training. With only 3.5M images and 800K minutes videos\ncrawled from the internet without human labeling (less than 2% of prior works),\nour models learned with OmniSource improve Top-1 accuracy of 2D- and 3D-ConvNet\nbaseline models by 3.0% and 3.9%, respectively, on the Kinetics-400 benchmark.\nWith OmniSource, we establish new records with different pretraining strategies\nfor video recognition. Our best models achieve 80.4%, 80.5%, and 83.6 Top-1\naccuracies on the Kinetics-400 benchmark respectively for\ntraining-from-scratch, ImageNet pre-training and IG-65M pre-training.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:47:31 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 06:36:16 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Duan", "Haodong", ""], ["Zhao", "Yue", ""], ["Xiong", "Yuanjun", ""], ["Liu", "Wentao", ""], ["Lin", "Dahua", ""]]}, {"id": "2003.13043", "submitter": "Joel Stehouwer", "authors": "Joel Stehouwer, Amin Jourabloo, Yaojie Liu, Xiaoming Liu", "title": "Noise Modeling, Synthesis and Classification for Generic Object\n  Anti-Spoofing", "comments": "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using printed photograph and replaying videos of biometric modalities, such\nas iris, fingerprint and face, are common attacks to fool the recognition\nsystems for granting access as the genuine user. With the growing online\nperson-to-person shopping (e.g., Ebay and Craigslist), such attacks also\nthreaten those services, where the online photo illustration might not be\ncaptured from real items but from paper or digital screen. Thus, the study of\nanti-spoofing should be extended from modality-specific solutions to\ngeneric-object-based ones. In this work, we define and tackle the problem of\nGeneric Object Anti-Spoofing (GOAS) for the first time. One significant cue to\ndetect these attacks is the noise patterns introduced by the capture sensors\nand spoof mediums. Different sensor/medium combinations can result in diverse\nnoise patterns. We propose a GAN-based architecture to synthesize and identify\nthe noise patterns from seen and unseen medium/sensor combinations. We show\nthat the procedure of synthesis and identification are mutually beneficial. We\nfurther demonstrate the learned GOAS models can directly contribute to\nmodality-specific anti-spoofing without domain transfer. The code and GOSet\ndataset are available at cvlab.cse.msu.edu/project-goas.html.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:52:36 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 16:15:59 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Stehouwer", "Joel", ""], ["Jourabloo", "Amin", ""], ["Liu", "Yaojie", ""], ["Liu", "Xiaoming", ""]]}, {"id": "2003.13045", "submitter": "Liang Liu", "authors": "Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying\n  Tai, Donghao Luo, Chengjie Wang, Jilin Li, Feiyue Huang", "title": "Learning by Analogy: Reliable Supervision from Transformations for\n  Unsupervised Optical Flow Estimation", "comments": "Accepted to CVPR 2020, https://github.com/lliuz/ARFlow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of optical flow, which leverages the supervision from\nview synthesis, has emerged as a promising alternative to supervised methods.\nHowever, the objective of unsupervised learning is likely to be unreliable in\nchallenging scenes. In this work, we present a framework to use more reliable\nsupervision from transformations. It simply twists the general unsupervised\nlearning pipeline by running another forward pass with transformed data from\naugmentation, along with using transformed predictions of original data as the\nself-supervision signal. Besides, we further introduce a lightweight network\nwith multiple frames by a highly-shared flow decoder. Our method consistently\ngets a leap of performance on several benchmarks with the best accuracy among\ndeep unsupervised methods. Also, our method achieves competitive results to\nrecent fully supervised methods while with much fewer parameters.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 14:55:24 GMT"}, {"version": "v2", "created": "Sun, 29 Nov 2020 12:26:25 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Liu", "Liang", ""], ["Zhang", "Jiangning", ""], ["He", "Ruifei", ""], ["Liu", "Yong", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Luo", "Donghao", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2003.13048", "submitter": "Devesh Walawalkar", "authors": "Devesh Walawalkar, Zhiqiang Shen, Zechun Liu, Marios Savvides", "title": "Attentive CutMix: An Enhanced Data Augmentation Approach for Deep\n  Learning Based Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNN) are capable of learning robust\nrepresentation with different regularization methods and activations as\nconvolutional layers are spatially correlated. Based on this property, a large\nvariety of regional dropout strategies have been proposed, such as Cutout,\nDropBlock, CutMix, etc. These methods aim to promote the network to generalize\nbetter by partially occluding the discriminative parts of objects. However, all\nof them perform this operation randomly, without capturing the most important\nregion(s) within an object. In this paper, we propose Attentive CutMix, a\nnaturally enhanced augmentation strategy based on CutMix. In each training\niteration, we choose the most descriptive regions based on the intermediate\nattention maps from a feature extractor, which enables searching for the most\ndiscriminative parts in an image. Our proposed method is simple yet effective,\neasy to implement and can boost the baseline significantly. Extensive\nexperiments on CIFAR-10/100, ImageNet datasets with various CNN architectures\n(in a unified setting) demonstrate the effectiveness of our proposed method,\nwhich consistently outperforms the baseline CutMix and other methods by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 15:01:05 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 13:35:20 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Walawalkar", "Devesh", ""], ["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Savvides", "Marios", ""]]}, {"id": "2003.13063", "submitter": "Cheng Ma", "authors": "Cheng Ma, Zhenyu Jiang, Yongming Rao, Jiwen Lu, Jie Zhou", "title": "Deep Face Super-Resolution with Iterative Collaboration between\n  Attentive Recovery and Landmark Estimation", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works based on deep learning and facial priors have succeeded in\nsuper-resolving severely degraded facial images. However, the prior knowledge\nis not fully exploited in existing methods, since facial priors such as\nlandmark and component maps are always estimated by low-resolution or coarsely\nsuper-resolved images, which may be inaccurate and thus affect the recovery\nperformance. In this paper, we propose a deep face super-resolution (FSR)\nmethod with iterative collaboration between two recurrent networks which focus\non facial image recovery and landmark estimation respectively. In each\nrecurrent step, the recovery branch utilizes the prior knowledge of landmarks\nto yield higher-quality images which facilitate more accurate landmark\nestimation in turn. Therefore, the iterative information interaction between\ntwo processes boosts the performance of each other progressively. Moreover, a\nnew attentive fusion module is designed to strengthen the guidance of landmark\nmaps, where facial components are generated individually and aggregated\nattentively for better restoration. Quantitative and qualitative experimental\nresults show the proposed method significantly outperforms state-of-the-art FSR\nmethods in recovering high-quality face images.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 16:04:48 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ma", "Cheng", ""], ["Jiang", "Zhenyu", ""], ["Rao", "Yongming", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2003.13081", "submitter": "Cheng Ma", "authors": "Cheng Ma, Yongming Rao, Yean Cheng, Ce Chen, Jiwen Lu, Jie Zhou", "title": "Structure-Preserving Super Resolution with Gradient Guidance", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structures matter in single image super resolution (SISR). Recent studies\nbenefiting from generative adversarial network (GAN) have promoted the\ndevelopment of SISR by recovering photo-realistic images. However, there are\nalways undesired structural distortions in the recovered images. In this paper,\nwe propose a structure-preserving super resolution method to alleviate the\nabove issue while maintaining the merits of GAN-based methods to generate\nperceptual-pleasant details. Specifically, we exploit gradient maps of images\nto guide the recovery in two aspects. On the one hand, we restore\nhigh-resolution gradient maps by a gradient branch to provide additional\nstructure priors for the SR process. On the other hand, we propose a gradient\nloss which imposes a second-order restriction on the super-resolved images.\nAlong with the previous image-space loss functions, the gradient-space\nobjectives help generative networks concentrate more on geometric structures.\nMoreover, our method is model-agnostic, which can be potentially used for\noff-the-shelf SR networks. Experimental results show that we achieve the best\nPI and LPIPS performance and meanwhile comparable PSNR and SSIM compared with\nstate-of-the-art perceptual-driven SR methods. Visual results demonstrate our\nsuperiority in restoring structures while generating natural SR images.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:26:58 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ma", "Cheng", ""], ["Rao", "Yongming", ""], ["Cheng", "Yean", ""], ["Chen", "Ce", ""], ["Lu", "Jiwen", ""], ["Zhou", "Jie", ""]]}, {"id": "2003.13088", "submitter": "Qianqian Wang", "authors": "Qianqian Wang, Zhengming Ding, Zhiqiang Tao, Quanxue Gao, Yun Fu", "title": "Generative Partial Multi-View Clustering", "comments": "This paper is an extension to our previous work: \"Wang Q, Ding Z, Tao\n  Z, et al. Partial multi-view clustering via consistent GAN[C]//2018 IEEE\n  International Conference on Data Mining (ICDM). IEEE, 2018: 1290-1295.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, with the rapid development of data collection sources and feature\nextraction methods, multi-view data are getting easy to obtain and have\nreceived increasing research attention in recent years, among which, multi-view\nclustering (MVC) forms a mainstream research direction and is widely used in\ndata analysis. However, existing MVC methods mainly assume that each sample\nappears in all the views, without considering the incomplete view case due to\ndata corruption, sensor failure, equipment malfunction, etc. In this study, we\ndesign and build a generative partial multi-view clustering model, named as\nGP-MVC, to address the incomplete multi-view problem by explicitly generating\nthe data of missing views. The main idea of GP-MVC lies at two-fold. First,\nmulti-view encoder networks are trained to learn common low-dimensional\nrepresentations, followed by a clustering layer to capture the consistent\ncluster structure across multiple views. Second, view-specific generative\nadversarial networks are developed to generate the missing data of one view\nconditioning on the shared representation given by other views. These two steps\ncould be promoted mutually, where learning common representations facilitates\ndata imputation and the generated data could further explores the view\nconsistency. Moreover, an weighted adaptive fusion scheme is implemented to\nexploit the complementary information among different views. Experimental\nresults on four benchmark datasets are provided to show the effectiveness of\nthe proposed GP-MVC over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:48:27 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wang", "Qianqian", ""], ["Ding", "Zhengming", ""], ["Tao", "Zhiqiang", ""], ["Gao", "Quanxue", ""], ["Fu", "Yun", ""]]}, {"id": "2003.13089", "submitter": "Mingkui Tan", "authors": "Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yong Guo, Peilin Zhao,\n  Junzhou Huang, Mingkui Tan", "title": "Disturbance-immune Weight Sharing for Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has gained increasing attention in the\ncommunity of architecture design. One of the key factors behind the success\nlies in the training efficiency created by the weight sharing (WS) technique.\nHowever, WS-based NAS methods often suffer from a performance disturbance (PD)\nissue. That is, the training of subsequent architectures inevitably disturbs\nthe performance of previously trained architectures due to the partially shared\nweights. This leads to inaccurate performance estimation for the previous\narchitectures, which makes it hard to learn a good search strategy. To\nalleviate the performance disturbance issue, we propose a new\ndisturbance-immune update strategy for model updating. Specifically, to\npreserve the knowledge learned by previous architectures, we constrain the\ntraining of subsequent architectures in an orthogonal space via orthogonal\ngradient descent. Equipped with this strategy, we propose a novel\ndisturbance-immune training scheme for NAS. We theoretically analyze the\neffectiveness of our strategy in alleviating the PD risk. Extensive experiments\non CIFAR-10 and ImageNet verify the superiority of our method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 17:54:49 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Niu", "Shuaicheng", ""], ["Wu", "Jiaxiang", ""], ["Zhang", "Yifan", ""], ["Guo", "Yong", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.13094", "submitter": "Nan Meng", "authors": "Nan Meng, Xiaofei Wu, Jianzhuang Liu, Edmund Y. Lam", "title": "High-Order Residual Network for Light Field Super-Resolution", "comments": "9 pages, 14 figures, accepted by the thirty-fourth AAAI Conference on\n  Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plenoptic cameras usually sacrifice the spatial resolution of their SAIs to\nacquire geometry information from different viewpoints. Several methods have\nbeen proposed to mitigate such spatio-angular trade-off, but seldom make use of\nthe structural properties of the light field (LF) data efficiently. In this\npaper, we propose a novel high-order residual network to learn the geometric\nfeatures hierarchically from the LF for reconstruction. An important component\nin the proposed network is the high-order residual block (HRB), which learns\nthe local geometric features by considering the information from all input\nviews. After fully obtaining the local features learned from each HRB, our\nmodel extracts the representative geometric features for spatio-angular\nupsampling through the global residual learning. Additionally, a refinement\nnetwork is followed to further enhance the spatial details by minimizing a\nperceptual loss. Compared with previous work, our model is tailored to the rich\nstructure inherent in the LF, and therefore can reduce the artifacts near\nnon-Lambertian and occlusion regions. Experimental results show that our\napproach enables high-quality reconstruction even in challenging regions and\noutperforms state-of-the-art single image or LF reconstruction methods with\nboth quantitative measurements and visual evaluation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 18:06:05 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Meng", "Nan", ""], ["Wu", "Xiaofei", ""], ["Liu", "Jianzhuang", ""], ["Lam", "Edmund Y.", ""]]}, {"id": "2003.13096", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Hyungjin Chung, Eung Yeop Kim, and Jong Chul Ye", "title": "Unsupervised Deep Learning for MR Angiography with Flexible Temporal\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-resolved MR angiography (tMRA) has been widely used for dynamic contrast\nenhanced MRI (DCE-MRI) due to its highly accelerated acquisition. In tMRA, the\nperiphery of the k-space data are sparsely sampled so that neighbouring frames\ncan be merged to construct one temporal frame. However, this view-sharing\nscheme fundamentally limits the temporal resolution, and it is not possible to\nchange the view-sharing number to achieve different spatio-temporal resolution\ntrade-off. Although many deep learning approaches have been recently proposed\nfor MR reconstruction from sparse samples, the existing approaches usually\nrequire matched fully sampled k-space reference data for supervised training,\nwhich is not suitable for tMRA. This is because high spatio-temporal resolution\nground-truth images are not available for tMRA. To address this problem, here\nwe propose a novel unsupervised deep learning using optimal transport driven\ncycle-consistent generative adversarial network (cycleGAN). In contrast to the\nconventional cycleGAN with two pairs of generator and discriminator, the new\narchitecture requires just a single pair of generator and discriminator, which\nmakes the training much simpler and improves the performance. Reconstruction\nresults using in vivo tMRA data set confirm that the proposed method can\nimmediately generate high quality reconstruction results at various choices of\nview-sharing numbers, allowing us to exploit better trade-off between spatial\nand temporal resolution in time-resolved MR angiography.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 18:08:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Cha", "Eunju", ""], ["Chung", "Hyungjin", ""], ["Kim", "Eung Yeop", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2003.13098", "submitter": "Seyed Ali Rokni", "authors": "Marjan Nourollahi, Seyed Ali Rokni, Hassan Ghasemzadeh", "title": "Proximity-Based Active Learning on Streaming Data: A Personalized Eating\n  Moment Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting when eating occurs is an essential step toward automatic dietary\nmonitoring, medication adherence assessment, and diet-related health\ninterventions. Wearable technologies play a central role in designing\nunubtrusive diet monitoring solutions by leveraging machine learning algorithms\nthat work on time-series sensor data to detect eating moments. While much\nresearch has been done on developing activity recognition and eating moment\ndetection algorithms, the performance of the detection algorithms drops\nsubstantially when the model trained with one user is utilized by a new user.\nTo facilitate development of personalized models, we propose PALS,\nProximity-based Active Learning on Streaming data, a novel proximity-based\nmodel for recognizing eating gestures with the goal of significantly decreasing\nthe need for labeled data with new users. Particularly, we propose an\noptimization problem to perform active learning under limited query budget by\nleveraging unlabeled data. Our extensive analysis on data collected in both\ncontrolled and uncontrolled settings indicates that the F-score of PLAS ranges\nfrom 22% to 39% for a budget that varies from 10 to 60 query. Furthermore,\ncompared to the state-of-the-art approaches, off-line PALS, on average,\nachieves to 40% higher recall and 12\\% higher f-score in detecting eating\ngestures.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 18:17:29 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Nourollahi", "Marjan", ""], ["Rokni", "Seyed Ali", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2003.13120", "submitter": "Senlin Yang", "authors": "Senlin Yang, Zhengfang Wang, Jing Wang, Anthony G. Cohn, Jiaqi Zhang,\n  Peng Jiang, Peng Jiang, Qingmei Sui", "title": "Defect segmentation: Mapping tunnel lining internal defects with ground\n  penetrating radar data using a convolutional neural network", "comments": "24 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a Ground Penetrating Radar (GPR) data processing\nmethod for non-destructive detection of tunnel lining internal defects, called\ndefect segmentation. To perform this critical step of automatic tunnel lining\ndetection, the method uses a CNN called Segnet combined with the Lov\\'asz\nsoftmax loss function to map the internal defect structure with GPR synthetic\ndata, which improves the accuracy, automation and efficiency of defects\ndetection. The novel method we present overcomes several difficulties of\ntraditional GPR data interpretation as demonstrated by an evaluation on both\nsynthetic and real datas -- to verify the method on real data, a test model\ncontaining a known defect was designed and built and GPR data was obtained and\nanalyzed.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:30:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yang", "Senlin", ""], ["Wang", "Zhengfang", ""], ["Wang", "Jing", ""], ["Cohn", "Anthony G.", ""], ["Zhang", "Jiaqi", ""], ["Jiang", "Peng", ""], ["Jiang", "Peng", ""], ["Sui", "Qingmei", ""]]}, {"id": "2003.13137", "submitter": "Viktor Kocur", "authors": "Viktor Kocur and Milan Ft\\'a\\v{c}nik", "title": "Detection of 3D Bounding Boxes of Vehicles Using Perspective\n  Transformation for Accurate Speed Measurement", "comments": "Submitted to Machine Vision and Applications", "journal-ref": null, "doi": "10.1007/s00138-020-01117-x", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection and tracking of vehicles captured by traffic surveillance cameras\nis a key component of intelligent transportation systems. We present an\nimproved version of our algorithm for detection of 3D bounding boxes of\nvehicles, their tracking and subsequent speed estimation. Our algorithm\nutilizes the known geometry of vanishing points in the surveilled scene to\nconstruct a perspective transformation. The transformation enables an intuitive\nsimplification of the problem of detecting 3D bounding boxes to detection of 2D\nbounding boxes with one additional parameter using a standard 2D object\ndetector. Main contribution of this paper is an improved construction of the\nperspective transformation which is more robust and fully automatic and an\nextended experimental evaluation of speed estimation. We test our algorithm on\nthe speed estimation task of the BrnoCompSpeed dataset. We evaluate our\napproach with different configurations to gauge the relationship between\naccuracy and computational costs and benefits of 3D bounding box detection over\n2D detection. All of the tested configurations run in real-time and are fully\nautomatic. Compared to other published state-of-the-art fully automatic results\nour algorithm reduces the mean absolute speed measurement error by 32% (1.10\nkm/h to 0.75 km/h) and the absolute median error by 40% (0.97 km/h to 0.58\nkm/h).\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 21:01:25 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 23:19:58 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Kocur", "Viktor", ""], ["Ft\u00e1\u010dnik", "Milan", ""]]}, {"id": "2003.13141", "submitter": "Jie Chen", "authors": "Jie Chen, Zhiheng Li, Jiebo Luo, and Chenliang Xu", "title": "Learning a Weakly-Supervised Video Actor-Action Segmentation Model with\n  a Wise Selection", "comments": "11 pages, 8 figures, cvpr-2020 supplementary video:\n  https://youtu.be/CX1hEOV9tlo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address weakly-supervised video actor-action segmentation (VAAS), which\nextends general video object segmentation (VOS) to additionally consider action\nlabels of the actors. The most successful methods on VOS synthesize a pool of\npseudo-annotations (PAs) and then refine them iteratively. However, they face\nchallenges as to how to select from a massive amount of PAs high-quality ones,\nhow to set an appropriate stop condition for weakly-supervised training, and\nhow to initialize PAs pertaining to VAAS. To overcome these challenges, we\npropose a general Weakly-Supervised framework with a Wise Selection of training\nsamples and model evaluation criterion (WS^2). Instead of blindly trusting\nquality-inconsistent PAs, WS^2 employs a learning-based selection to select\neffective PAs and a novel region integrity criterion as a stopping condition\nfor weakly-supervised training. In addition, a 3D-Conv GCAM is devised to adapt\nto the VAAS task. Extensive experiments show that WS^2 achieves\nstate-of-the-art performance on both weakly-supervised VOS and VAAS tasks and\nis on par with the best fully-supervised method on VAAS.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 21:15:18 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Chen", "Jie", ""], ["Li", "Zhiheng", ""], ["Luo", "Jiebo", ""], ["Xu", "Chenliang", ""]]}, {"id": "2003.13145", "submitter": "Muhammad E. H. Chowdhury", "authors": "Muhammad E. H. Chowdhury, Tawsifur Rahman, Amith Khandakar, Rashid\n  Mazhar, Muhammad Abdul Kadir, Zaid Bin Mahbub, Khandaker Reajul Islam,\n  Muhammad Salman Khan, Atif Iqbal, Nasser Al-Emadi, Mamun Bin Ibne Reaz, T. I.\n  Islam", "title": "Can AI help in screening Viral and COVID-19 pneumonia?", "comments": "12 pages, 9 Figures", "journal-ref": "IEEE Access 2020", "doi": "10.1109/ACCESS.2020.3010287", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus disease (COVID-19) is a pandemic disease, which has already\ncaused thousands of causalities and infected several millions of people\nworldwide. Any technological tool enabling rapid screening of the COVID-19\ninfection with high accuracy can be crucially helpful to healthcare\nprofessionals. The main clinical tool currently in use for the diagnosis of\nCOVID-19 is the Reverse transcription polymerase chain reaction (RT-PCR), which\nis expensive, less-sensitive and requires specialized medical personnel. X-ray\nimaging is an easily accessible tool that can be an excellent alternative in\nthe COVID-19 diagnosis. This research was taken to investigate the utility of\nartificial intelligence (AI) in the rapid and accurate detection of COVID-19\nfrom chest X-ray images. The aim of this paper is to propose a robust technique\nfor automatic detection of COVID-19 pneumonia from digital chest X-ray images\napplying pre-trained deep-learning algorithms while maximizing the detection\naccuracy. A public database was created by the authors combining several public\ndatabases and also by collecting images from recently published articles. The\ndatabase contains a mixture of 423 COVID-19, 1485 viral pneumonia, and 1579\nnormal chest X-ray images. Transfer learning technique was used with the help\nof image augmentation to train and validate several pre-trained deep\nConvolutional Neural Networks (CNNs). The networks were trained to classify two\ndifferent schemes: i) normal and COVID-19 pneumonia; ii) normal, viral and\nCOVID-19 pneumonia with and without image augmentation. The classification\naccuracy, precision, sensitivity, and specificity for both the schemes were\n99.7%, 99.7%, 99.7% and 99.55% and 97.9%, 97.95%, 97.9%, and 98.8%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 21:37:21 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 08:48:18 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 08:43:36 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Chowdhury", "Muhammad E. H.", ""], ["Rahman", "Tawsifur", ""], ["Khandakar", "Amith", ""], ["Mazhar", "Rashid", ""], ["Kadir", "Muhammad Abdul", ""], ["Mahbub", "Zaid Bin", ""], ["Islam", "Khandaker Reajul", ""], ["Khan", "Muhammad Salman", ""], ["Iqbal", "Atif", ""], ["Al-Emadi", "Nasser", ""], ["Reaz", "Mamun Bin Ibne", ""], ["Islam", "T. I.", ""]]}, {"id": "2003.13158", "submitter": "Anna Kukleva", "authors": "Anna Kukleva and Makarand Tapaswi and Ivan Laptev", "title": "Learning Interactions and Relationships between Movie Characters", "comments": "CVPR 2020 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between people are often governed by their relationships. On the\nflip side, social relationships are built upon several interactions. Two\nstrangers are more likely to greet and introduce themselves while becoming\nfriends over time. We are fascinated by this interplay between interactions and\nrelationships, and believe that it is an important aspect of understanding\nsocial situations. In this work, we propose neural models to learn and jointly\npredict interactions, relationships, and the pair of characters that are\ninvolved. We note that interactions are informed by a mixture of visual and\ndialog cues, and present a multimodal architecture to extract meaningful\ninformation from them. Localizing the pair of interacting characters in video\nis a time-consuming process, instead, we train our model to learn from\nclip-level weak labels. We evaluate our models on the MovieGraphs dataset and\nshow the impact of modalities, use of longer temporal context for predicting\nrelationships, and achieve encouraging performance using weak labels as\ncompared with ground-truth labels. Code is online.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 23:11:24 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kukleva", "Anna", ""], ["Tapaswi", "Makarand", ""], ["Laptev", "Ivan", ""]]}, {"id": "2003.13170", "submitter": "Muhammad Haris", "authors": "Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita", "title": "Space-Time-Aware Multi-Resolution Video Enhancement", "comments": "To appear in CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of space-time super-resolution (ST-SR): increasing\nspatial resolution of video frames and simultaneously interpolating frames to\nincrease the frame rate. Modern approaches handle these axes one at a time. In\ncontrast, our proposed model called STARnet super-resolves jointly in space and\ntime. This allows us to leverage mutually informative relationships between\ntime and space: higher resolution can provide more detailed information about\nmotion, and higher frame-rate can provide better pixel alignment. The\ncomponents of our model that generate latent low- and high-resolution\nrepresentations during ST-SR can be used to finetune a specialized mechanism\nfor just spatial or just temporal super-resolution. Experimental results\ndemonstrate that STARnet improves the performances of space-time, spatial, and\ntemporal video super-resolution by substantial margins on publicly available\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 00:33:17 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Haris", "Muhammad", ""], ["Shakhnarovich", "Greg", ""], ["Ukita", "Norimichi", ""]]}, {"id": "2003.13183", "submitter": "Shuhao Cui", "authors": "Shuhao Cui, Shuhui Wang, Junbao Zhuo, Chi Su, Qingming Huang, Qi Tian", "title": "Gradually Vanishing Bridge for Adversarial Domain Adaptation", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In unsupervised domain adaptation, rich domain-specific characteristics bring\ngreat challenge to learn domain-invariant representations. However, domain\ndiscrepancy is considered to be directly minimized in existing solutions, which\nis difficult to achieve in practice. Some methods alleviate the difficulty by\nexplicitly modeling domain-invariant and domain-specific parts in the\nrepresentations, but the adverse influence of the explicit construction lies in\nthe residual domain-specific characteristics in the constructed\ndomain-invariant representations. In this paper, we equip adversarial domain\nadaptation with Gradually Vanishing Bridge (GVB) mechanism on both generator\nand discriminator. On the generator, GVB could not only reduce the overall\ntransfer difficulty, but also reduce the influence of the residual\ndomain-specific characteristics in domain-invariant representations. On the\ndiscriminator, GVB contributes to enhance the discriminating ability, and\nbalance the adversarial training process. Experiments on three challenging\ndatasets show that our GVB methods outperform strong competitors, and cooperate\nwell with other adversarial methods. The code is available at\nhttps://github.com/cuishuhao/GVB.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 01:36:13 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Cui", "Shuhao", ""], ["Wang", "Shuhui", ""], ["Zhuo", "Junbao", ""], ["Su", "Chi", ""], ["Huang", "Qingming", ""], ["Tian", "Qi", ""]]}, {"id": "2003.13191", "submitter": "Jiangpeng He", "authors": "Jiangpeng He, Runyu Mao, Zeman Shao and Fengqing Zhu", "title": "Incremental Learning In Online Scenario", "comments": "Accepted paper at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning approaches have achieved great success in many vision\napplications by training a model using all available task-specific data.\nHowever, there are two major obstacles making it challenging to implement for\nreal life applications: (1) Learning new classes makes the trained model\nquickly forget old classes knowledge, which is referred to as catastrophic\nforgetting. (2) As new observations of old classes come sequentially over time,\nthe distribution may change in unforeseen way, making the performance degrade\ndramatically on future data, which is referred to as concept drift. Current\nstate-of-the-art incremental learning methods require a long time to train the\nmodel whenever new classes are added and none of them takes into consideration\nthe new observations of old classes. In this paper, we propose an incremental\nlearning framework that can work in the challenging online learning scenario\nand handle both new classes data and new observations of old classes. We\naddress problem (1) in online mode by introducing a modified cross-distillation\nloss together with a two-step learning technique. Our method outperforms the\nresults obtained from current state-of-the-art offline incremental learning\nmethods on the CIFAR-100 and ImageNet-1000 (ILSVRC 2012) datasets under the\nsame experiment protocol but in online scenario. We also provide a simple yet\neffective method to mitigate problem (2) by updating exemplar set using the\nfeature of each new observation of old classes and demonstrate a real life\napplication of online food image classification based on our complete framework\nusing the Food-101 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 02:24:26 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 00:50:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["He", "Jiangpeng", ""], ["Mao", "Runyu", ""], ["Shao", "Zeman", ""], ["Zhu", "Fengqing", ""]]}, {"id": "2003.13193", "submitter": "Kai Li", "authors": "Kai Li, Yulun Zhang, Kunpeng Li, Yun Fu", "title": "Adversarial Feature Hallucination Networks for Few-Shot Learning", "comments": "Correct some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent flourish of deep learning in various tasks is largely accredited\nto the rich and accessible labeled data. Nonetheless, massive supervision\nremains a luxury for many real applications, boosting great interest in\nlabel-scarce techniques such as few-shot learning (FSL), which aims to learn\nconcept of new classes with a few labeled samples. A natural approach to FSL is\ndata augmentation and many recent works have proved the feasibility by\nproposing various data synthesis models. However, these models fail to well\nsecure the discriminability and diversity of the synthesized data and thus\noften produce undesirable results. In this paper, we propose Adversarial\nFeature Hallucination Networks (AFHN) which is based on conditional Wasserstein\nGenerative Adversarial networks (cWGAN) and hallucinates diverse and\ndiscriminative features conditioned on the few labeled samples. Two novel\nregularizers, i.e., the classification regularizer and the anti-collapse\nregularizer, are incorporated into AFHN to encourage discriminability and\ndiversity of the synthesized features, respectively. Ablation study verifies\nthe effectiveness of the proposed cWGAN based feature hallucination framework\nand the proposed regularizers. Comparative results on three common benchmark\ndatasets substantiate the superiority of AFHN to existing data augmentation\nbased FSL approaches and other state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 02:43:16 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 19:16:50 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Li", "Kai", ""], ["Zhang", "Yulun", ""], ["Li", "Kunpeng", ""], ["Fu", "Yun", ""]]}, {"id": "2003.13194", "submitter": "Suichan Li", "authors": "Suichan Li, Bin Liu, Dongdong Chen, Qi Chu, Lu Yuan, Nenghai Yu", "title": "Density-Aware Graph for Deep Semi-Supervised Visual Recognition", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) has been extensively studied to improve the\ngeneralization ability of deep neural networks for visual recognition. To\ninvolve the unlabelled data, most existing SSL methods are based on common\ndensity-based cluster assumption: samples lying in the same high-density region\nare likely to belong to the same class, including the methods performing\nconsistency regularization or generating pseudo-labels for the unlabelled\nimages. Despite their impressive performance, we argue three limitations exist:\n1) Though the density information is demonstrated to be an important clue, they\nall use it in an implicit way and have not exploited it in depth. 2) For\nfeature learning, they often learn the feature embedding based on the single\ndata sample and ignore the neighborhood information. 3) For label-propagation\nbased pseudo-label generation, it is often done offline and difficult to be\nend-to-end trained with feature learning. Motivated by these limitations, this\npaper proposes to solve the SSL problem by building a novel density-aware\ngraph, based on which the neighborhood information can be easily leveraged and\nthe feature learning and label propagation can also be trained in an end-to-end\nway. Specifically, we first propose a new Density-aware Neighborhood\nAggregation(DNA) module to learn more discriminative features by incorporating\nthe neighborhood information in a density-aware manner. Then a novel\nDensity-ascending Path based Label Propagation(DPLP) module is proposed to\ngenerate the pseudo-labels for unlabeled samples more efficiently according to\nthe feature distribution characterized by density. Finally, the DNA module and\nDPLP module evolve and improve each other end-to-end.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 02:52:40 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Suichan", ""], ["Liu", "Bin", ""], ["Chen", "Dongdong", ""], ["Chu", "Qi", ""], ["Yuan", "Lu", ""], ["Yu", "Nenghai", ""]]}, {"id": "2003.13197", "submitter": "Kai Li", "authors": "Kai Li, Curtis Wigington, Chris Tensmeyer, Handong Zhao, Nikolaos\n  Barmpalios, Vlad I. Morariu, Varun Manjunatha, Tong Sun, Yun Fu", "title": "Cross-Domain Document Object Detection: Benchmark Suite and Method", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decomposing images of document pages into high-level semantic regions (e.g.,\nfigures, tables, paragraphs), document object detection (DOD) is fundamental\nfor downstream tasks like intelligent document editing and understanding. DOD\nremains a challenging problem as document objects vary significantly in layout,\nsize, aspect ratio, texture, etc. An additional challenge arises in practice\nbecause large labeled training datasets are only available for domains that\ndiffer from the target domain. We investigate cross-domain DOD, where the goal\nis to learn a detector for the target domain using labeled data from the source\ndomain and only unlabeled data from the target domain. Documents from the two\ndomains may vary significantly in layout, language, and genre. We establish a\nbenchmark suite consisting of different types of PDF document datasets that can\nbe utilized for cross-domain DOD model training and evaluation. For each\ndataset, we provide the page images, bounding box annotations, PDF files, and\nthe rendering layers extracted from the PDF files. Moreover, we propose a novel\ncross-domain DOD model which builds upon the standard detection model and\naddresses domain shifts by incorporating three novel alignment modules: Feature\nPyramid Alignment (FPA) module, Region Alignment (RA) module and Rendering\nLayer alignment (RLA) module. Extensive experiments on the benchmark suite\nsubstantiate the efficacy of the three proposed modules and the proposed method\nsignificantly outperforms the baseline methods. The project page is at\n\\url{https://github.com/kailigo/cddod}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 03:04:51 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Li", "Kai", ""], ["Wigington", "Curtis", ""], ["Tensmeyer", "Chris", ""], ["Zhao", "Handong", ""], ["Barmpalios", "Nikolaos", ""], ["Morariu", "Vlad I.", ""], ["Manjunatha", "Varun", ""], ["Sun", "Tong", ""], ["Fu", "Yun", ""]]}, {"id": "2003.13198", "submitter": "An Yang", "authors": "Junyang Lin, An Yang, Yichang Zhang, Jie Liu, Jingren Zhou, Hongxia\n  Yang", "title": "InterBERT: Vision-and-Language Interaction for Multi-modal Pretraining", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-modal pretraining for learning high-level multi-modal representation is\na further step towards deep learning and artificial intelligence. In this work,\nwe propose a novel model, namely InterBERT (BERT for Interaction), which is the\nfirst model of our series of multimodal pretraining methods M6\n(MultiModality-to-MultiModality Multitask Mega-transformer). The model owns\nstrong capability of modeling interaction between the information flows of\ndifferent modalities. The single-stream interaction module is capable of\neffectively processing information of multiple modalilties, and the two-stream\nmodule on top preserves the independence of each modality to avoid performance\ndowngrade in single-modal tasks. We pretrain the model with three pretraining\ntasks, including masked segment modeling (MSM), masked region modeling (MRM)\nand image-text matching (ITM); and finetune the model on a series of\nvision-and-language downstream tasks. Experimental results demonstrate that\nInterBERT outperforms a series of strong baselines, including the most recent\nmulti-modal pretraining methods, and the analysis shows that MSM and MRM are\neffective for pretraining and our method can achieve performances comparable to\nBERT in single-modal tasks. Besides, we propose a large-scale dataset for\nmulti-modal pretraining in Chinese, and we develop the Chinese InterBERT which\nis the first Chinese multi-modal pretrained model. We pretrain the Chinese\nInterBERT on our proposed dataset of 3.1M image-text pairs from the mobile\nTaobao, the largest Chinese e-commerce platform. We finetune the model for\ntext-based image retrieval, and recently we deployed the model online for\ntopic-based recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 03:13:22 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 02:14:00 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 16:19:34 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 11:20:26 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lin", "Junyang", ""], ["Yang", "An", ""], ["Zhang", "Yichang", ""], ["Liu", "Jie", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2003.13216", "submitter": "Fengchun Qiao", "authors": "Fengchun Qiao, Long Zhao, Xi Peng", "title": "Learning to Learn Single Domain Generalization", "comments": "In CVPR 2020 (13 pages including supplementary material). The source\n  code and pre-trained models are publicly available at:\n  https://github.com/joffery/M-ADA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with a worst-case scenario in model generalization, in the\nsense that a model aims to perform well on many unseen domains while there is\nonly one single domain available for training. We propose a new method named\nadversarial domain augmentation to solve this Out-of-Distribution (OOD)\ngeneralization problem. The key idea is to leverage adversarial training to\ncreate \"fictitious\" yet \"challenging\" populations, from which a model can learn\nto generalize with theoretical guarantees. To facilitate fast and desirable\ndomain augmentation, we cast the model training in a meta-learning scheme and\nuse a Wasserstein Auto-Encoder (WAE) to relax the widely used worst-case\nconstraint. Detailed theoretical analysis is provided to testify our\nformulation, while extensive experiments on multiple benchmark datasets\nindicate its superior performance in tackling single domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 04:39:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Qiao", "Fengchun", ""], ["Zhao", "Long", ""], ["Peng", "Xi", ""]]}, {"id": "2003.13228", "submitter": "Hyunjong Park", "authors": "Hyunjong Park, Jongyoun Noh, Bumsub Ham", "title": "Learning Memory-guided Normality for Anomaly Detection", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of anomaly detection, that is, detecting anomalous\nevents in a video sequence. Anomaly detection methods based on convolutional\nneural networks (CNNs) typically leverage proxy tasks, such as reconstructing\ninput video frames, to learn models describing normality without seeing\nanomalous samples at training time, and quantify the extent of abnormalities\nusing the reconstruction error at test time. The main drawbacks of these\napproaches are that they do not consider the diversity of normal patterns\nexplicitly, and the powerful representation capacity of CNNs allows to\nreconstruct abnormal video frames. To address this problem, we present an\nunsupervised learning approach to anomaly detection that considers the\ndiversity of normal patterns explicitly, while lessening the representation\ncapacity of CNNs. To this end, we propose to use a memory module with a new\nupdate scheme where items in the memory record prototypical patterns of normal\ndata. We also present novel feature compactness and separateness losses to\ntrain the memory, boosting the discriminative power of both memory items and\ndeeply learned features from normal data. Experimental results on standard\nbenchmarks demonstrate the effectiveness and efficiency of our approach, which\noutperforms the state of the art.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 05:30:09 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Park", "Hyunjong", ""], ["Noh", "Jongyoun", ""], ["Ham", "Bumsub", ""]]}, {"id": "2003.13239", "submitter": "Ronchang Xie", "authors": "Rongchang Xie, Chunyu Wang, Yizhou Wang", "title": "MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross view feature fusion is the key to address the occlusion problem in\nhuman pose estimation. The current fusion methods need to train a separate\nmodel for every pair of cameras making them difficult to scale. In this work,\nwe introduce MetaFuse, a pre-trained fusion model learned from a large number\nof cameras in the Panoptic dataset. The model can be efficiently adapted or\nfinetuned for a new pair of cameras using a small number of labeled images. The\nstrong adaptation power of MetaFuse is due in large part to the proposed\nfactorization of the original fusion model into two parts (1) a generic fusion\nmodel shared by all cameras, and (2) lightweight camera-dependent\ntransformations. Furthermore, the generic model is learned from many cameras by\na meta-learning style algorithm to maximize its adaptation capability to\nvarious camera poses. We observe in experiments that MetaFuse finetuned on the\npublic datasets outperforms the state-of-the-arts by a large margin which\nvalidates its value in practice.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 06:54:50 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Xie", "Rongchang", ""], ["Wang", "Chunyu", ""], ["Wang", "Yizhou", ""]]}, {"id": "2003.13242", "submitter": "Cong Wang", "authors": "Honghe Zhu and Cong Wang and Yajie Zhang and Zhixun Su and Guohui Zhao", "title": "Physical Model Guided Deep Image Deraining", "comments": "IEEE ICME2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image deraining is an urgent task because the degraded rainy image\nmakes many computer vision systems fail to work, such as video surveillance and\nautonomous driving.\n  So, deraining becomes important and an effective deraining algorithm is\nneeded.\n  In this paper, we propose a novel network based on physical model guided\nlearning for single image deraining, which consists of three sub-networks: rain\nstreaks network, rain-free network, and guide-learning network.\n  The concatenation of rain streaks and rain-free image that are estimated by\nrain streaks network, rain-free network, respectively, is input to the\nguide-learning network to guide further learning and the direct sum of the two\nestimated images is constrained with the input rainy image based on the\nphysical model of rainy image.\n  Moreover, we further develop the Multi-Scale Residual Block (MSRB) to better\nutilize multi-scale information and it is proved to boost the deraining\nperformance.\n  Quantitative and qualitative experimental results demonstrate that the\nproposed method outperforms the state-of-the-art deraining methods.\n  The source code will be available at\n\\url{https://supercong94.wixsite.com/supercong94}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 07:08:13 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Zhu", "Honghe", ""], ["Wang", "Cong", ""], ["Zhang", "Yajie", ""], ["Su", "Zhixun", ""], ["Zhao", "Guohui", ""]]}, {"id": "2003.13246", "submitter": "Jiaxu Miao", "authors": "Jiaxu Miao, Yunchao Wei and Yi Yang", "title": "Memory Aggregation Networks for Efficient Interactive Video Object\n  Segmentation", "comments": "Accepted to CVPR 2020. 10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive video object segmentation (iVOS) aims at efficiently harvesting\nhigh-quality segmentation masks of the target object in a video with user\ninteractions. Most previous state-of-the-arts tackle the iVOS with two\nindependent networks for conducting user interaction and temporal propagation,\nrespectively, leading to inefficiencies during the inference stage. In this\nwork, we propose a unified framework, named Memory Aggregation Networks\n(MA-Net), to address the challenging iVOS in a more efficient way. Our MA-Net\nintegrates the interaction and the propagation operations into a single\nnetwork, which significantly promotes the efficiency of iVOS in the scheme of\nmulti-round interactions. More importantly, we propose a simple yet effective\nmemory aggregation mechanism to record the informative knowledge from the\nprevious interaction rounds, improving the robustness in discovering\nchallenging objects of interest greatly. We conduct extensive experiments on\nthe validation set of DAVIS Challenge 2018 benchmark. In particular, our MA-Net\nachieves the J@60 score of 76.1% without any bells and whistles, outperforming\nthe state-of-the-arts with more than 2.7%.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 07:25:26 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Miao", "Jiaxu", ""], ["Wei", "Yunchao", ""], ["Yang", "Yi", ""]]}, {"id": "2003.13253", "submitter": "Sebastian Feld", "authors": "Sebastian Feld, Markus Friedrich, Claudia Linnhoff-Popien", "title": "Optimizing Geometry Compression using Quantum Annealing", "comments": "6 pages, 3 figures", "journal-ref": "2018 IEEE Globecom Workshops (GC Wkshps), Abu Dhabi, United Arab\n  Emirates, 2018, pp. 1-6", "doi": "10.1109/GLOCOMW.2018.8644358", "report-no": null, "categories": "quant-ph cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compression of geometry data is an important aspect of\nbandwidth-efficient data transfer for distributed 3d computer vision\napplications. We propose a quantum-enabled lossy 3d point cloud compression\npipeline based on the constructive solid geometry (CSG) model representation.\nKey parts of the pipeline are mapped to NP-complete problems for which an\nefficient Ising formulation suitable for the execution on a Quantum Annealer\nexists. We describe existing Ising formulations for the maximum clique search\nproblem and the smallest exact cover problem, both of which are important\nbuilding blocks of the proposed compression pipeline. Additionally, we discuss\nthe properties of the overall pipeline regarding result optimality and\ndescribed Ising formulations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 07:56:34 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Feld", "Sebastian", ""], ["Friedrich", "Markus", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2003.13260", "submitter": "Xi Li", "authors": "Junyi Feng, Songyuan Li, Xi Li, Fei Wu, Qi Tian, Ming-Hsuan Yang, and\n  Haibin Ling", "title": "TapLab: A Fast Framework for Semantic Video Segmentation Tapping into\n  Compressed-Domain Knowledge", "comments": "Accepted to TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time semantic video segmentation is a challenging task due to the strict\nrequirements of inference speed. Recent approaches mainly devote great efforts\nto reducing the model size for high efficiency. In this paper, we rethink this\nproblem from a different viewpoint: using knowledge contained in compressed\nvideos. We propose a simple and effective framework, dubbed TapLab, to tap into\nresources from the compressed domain. Specifically, we design a fast feature\nwarping module using motion vectors for acceleration. To reduce the noise\nintroduced by motion vectors, we design a residual-guided correction module and\na residual-guided frame selection module using residuals. TapLab significantly\nreduces redundant computations of the state-of-the-art fast semantic image\nsegmentation models, running 3 to 10 times faster with controllable accuracy\ndegradation. The experimental results show that TapLab achieves 70.6% mIoU on\nthe Cityscapes dataset at 99.8 FPS with a single GPU card for the 1024x2048\nvideos. A high-speed version even reaches the speed of 160+ FPS. Codes will be\navailable soon at https://github.com/Sixkplus/TapLab.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:13:47 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 12:26:36 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 06:52:41 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Feng", "Junyi", ""], ["Li", "Songyuan", ""], ["Li", "Xi", ""], ["Wu", "Fei", ""], ["Tian", "Qi", ""], ["Yang", "Ming-Hsuan", ""], ["Ling", "Haibin", ""]]}, {"id": "2003.13261", "submitter": "Shaobo Min", "authors": "Shaobo Min, Hantao Yao, Hongtao Xie, Chaoqun Wang, Zheng-Jun Zha, and\n  Yongdong Zhang", "title": "Domain-aware Visual Bias Eliminating for Generalized Zero-Shot Learning", "comments": "Accepted by CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods focus on learning a unified semantic-aligned visual\nrepresentation to transfer knowledge between two domains, while ignoring the\neffect of semantic-free visual representation in alleviating the biased\nrecognition problem. In this paper, we propose a novel Domain-aware Visual Bias\nEliminating (DVBE) network that constructs two complementary visual\nrepresentations, i.e., semantic-free and semantic-aligned, to treat seen and\nunseen domains separately. Specifically, we explore cross-attentive\nsecond-order visual statistics to compact the semantic-free representation, and\ndesign an adaptive margin Softmax to maximize inter-class divergences. Thus,\nthe semantic-free representation becomes discriminative enough to not only\npredict seen class accurately but also filter out unseen images, i.e., domain\ndetection, based on the predicted class entropy. For unseen images, we\nautomatically search an optimal semantic-visual alignment architecture, rather\nthan manual designs, to predict unseen classes. With accurate domain detection,\nthe biased recognition problem towards the seen domain is significantly\nreduced. Experiments on five benchmarks for classification and segmentation\nshow that DVBE outperforms existing methods by averaged 5.7% improvement.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:17:04 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 07:12:53 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Min", "Shaobo", ""], ["Yao", "Hantao", ""], ["Xie", "Hongtao", ""], ["Wang", "Chaoqun", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2003.13266", "submitter": "Yingyi Zhang", "authors": "Yingyi Zhang, Lin Zhang, Ruixin Zhang, Shaoxin Li, Jilin Li, Feiyue\n  Huang", "title": "Towards Palmprint Verification On Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of mobile devices, smartphones have gradually\nbecome an indispensable part of people's lives. Meanwhile, biometric\nauthentication has been corroborated to be an effective method for establishing\na person's identity with high confidence. Hence, recently, biometric\ntechnologies for smartphones have also become increasingly sophisticated and\npopular. But it is noteworthy that the application potential of palmprints for\nsmartphones is seriously underestimated. Studies in the past two decades have\nshown that palmprints have outstanding merits in uniqueness and permanence, and\nhave high user acceptance. However, currently, studies specializing in\npalmprint verification for smartphones are still quite sporadic, especially\nwhen compared to face- or fingerprint-oriented ones. In this paper, aiming to\nfill the aforementioned research gap, we conducted a thorough study of\npalmprint verification on smartphones and our contributions are twofold. First,\nto facilitate the study of palmprint verification on smartphones, we\nestablished an annotated palmprint dataset named MPD, which was collected by\nmulti-brand smartphones in two separate sessions with various backgrounds and\nillumination conditions. As the largest dataset in this field, MPD contains\n16,000 palm images collected from 200 subjects. Second, we built a DCNN-based\npalmprint verification system named DeepMPV+ for smartphones. In DeepMPV+, two\nkey steps, ROI extraction and ROI matching, are both formulated as learning\nproblems and then solved naturally by modern DCNN models. The efficiency and\nefficacy of DeepMPV+ have been corroborated by extensive experiments. To make\nour results fully reproducible, the labeled dataset and the relevant source\ncodes have been made publicly available at\nhttps://cslinzhang.github.io/MobilePalmPrint/.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:31:03 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 04:08:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Zhang", "Yingyi", ""], ["Zhang", "Lin", ""], ["Zhang", "Ruixin", ""], ["Li", "Shaoxin", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""]]}, {"id": "2003.13268", "submitter": "Jie Hu", "authors": "Jie Hu, Liujuan Cao, Qixiang Ye, Tong Tong, ShengChuan Zhang, Ke Li,\n  Feiyue Huang, Rongrong Ji, Ling Shao", "title": "Architecture Disentanglement for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the inner workings of deep neural networks (DNNs) is essential\nto provide trustworthy artificial intelligence techniques for practical\napplications. Existing studies typically involve linking semantic concepts to\nunits or layers of DNNs, but fail to explain the inference process. In this\npaper, we introduce neural architecture disentanglement (NAD) to fill the gap.\nSpecifically, NAD learns to disentangle a pre-trained DNN into\nsub-architectures according to independent tasks, forming information flows\nthat describe the inference processes. We investigate whether, where, and how\nthe disentanglement occurs through experiments conducted with handcrafted and\nautomatically-searched network architectures, on both object-based and\nscene-based datasets. Based on the experimental results, we present three new\nfindings that provide fresh insights into the inner logic of DNNs. First, DNNs\ncan be divided into sub-architectures for independent tasks. Second, deeper\nlayers do not always correspond to higher semantics. Third, the connection type\nin a DNN affects how the information flows across layers, leading to different\ndisentanglement behaviors. With NAD, we further explain why DNNs sometimes give\nwrong predictions. Experimental results show that misclassified images have a\nhigh probability of being assigned to task sub-architectures similar to the\ncorrect ones. Code will be available at: https://github.com/hujiecpp/NAD.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:34:33 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 03:03:54 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hu", "Jie", ""], ["Cao", "Liujuan", ""], ["Ye", "Qixiang", ""], ["Tong", "Tong", ""], ["Zhang", "ShengChuan", ""], ["Li", "Ke", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Shao", "Ling", ""]]}, {"id": "2003.13272", "submitter": "Shaobo Min", "authors": "Shaobo Min, Hantao Yao, Hongtao Xie, Zheng-Jun Zha, and Yongdong Zhang", "title": "Multi-Objective Matrix Normalization for Fine-grained Visual Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2020.2977457", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bilinear pooling achieves great success in fine-grained visual recognition\n(FGVC). Recent methods have shown that the matrix power normalization can\nstabilize the second-order information in bilinear features, but some problems,\ne.g., redundant information and over-fitting, remain to be resolved. In this\npaper, we propose an efficient Multi-Objective Matrix Normalization (MOMN)\nmethod that can simultaneously normalize a bilinear representation in terms of\nsquare-root, low-rank, and sparsity. These three regularizers can not only\nstabilize the second-order information, but also compact the bilinear features\nand promote model generalization. In MOMN, a core challenge is how to jointly\noptimize three non-smooth regularizers of different convex properties. To this\nend, MOMN first formulates them into an augmented Lagrange formula with\napproximated regularizer constraints. Then, auxiliary variables are introduced\nto relax different constraints, which allow each regularizer to be solved\nalternately. Finally, several updating strategies based on gradient descent are\ndesigned to obtain consistent convergence and efficient implementation.\nConsequently, MOMN is implemented with only matrix multiplication, which is\nwell-compatible with GPU acceleration, and the normalized bilinear features are\nstabilized and discriminative. Experiments on five public benchmarks for FGVC\ndemonstrate that the proposed MOMN is superior to existing normalization-based\nmethods in terms of both accuracy and efficiency. The code is available:\nhttps://github.com/mboboGO/MOMN.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:40:35 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 07:33:42 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Min", "Shaobo", ""], ["Yao", "Hantao", ""], ["Xie", "Hongtao", ""], ["Zha", "Zheng-Jun", ""], ["Zhang", "Yongdong", ""]]}, {"id": "2003.13274", "submitter": "Dapeng Hu", "authors": "Dapeng Hu, Jian Liang, Qibin Hou, Hanshu Yan, Yunpeng Chen, Shuicheng\n  Yan, Jiashi Feng", "title": "Semantic Domain Adversarial Networks for Unsupervised Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adversarial training has become a prevailing and effective paradigm\nfor unsupervised domain adaptation (UDA). To successfully align the multi-modal\ndata structures across domains, the following works exploit discriminative\ninformation in the adversarial training process, e.g., using multiple\nclass-wise discriminators and introducing conditional information in input or\noutput of the domain discriminator. However, these methods either require\nnon-trivial model designs or are inefficient for UDA tasks. In this work, we\nattempt to address this dilemma by devising simple and compact conditional\ndomain adversarial training methods. We first show that the previous failure of\nthe concatenation conditioning strategy mainly accounts for the weak support of\nthe conditioning. Thus we propose an effective concatenation conditioning\nstrategy by introducing a norm control factor to strengthen the conditioning\nand term the derived method as \\underline{S}emantic \\underline{D}omain\n\\underline{A}dversarial \\underline{N}etworks~(SDAN). However, directly applying\npredictions for conditional domain alignment, SDAN still suffers from\ninaccurate target predictions. We further propose a novel structure-aware\nconditioning strategy to enhance SDAN by conditioning the cross-domain feature\nalignment in the structure-aware semantic space rather than in the prediction\nspace. We term the enhanced method as \\underline{S}tructure-aware\n\\underline{S}emantic \\underline{D}omain \\underline{A}dversarial\n\\underline{N}etworks~(SSDAN). Experiments on both object recognition and\nsemantic segmentation show that SDAN effectively aligns the multi-modal\nstructures across domains and even outperforms state-of-the-art domain\nadversarial training methods. With structure-aware semantic conditioning, SSDAN\nfurther improves the adaptation performance over SDAN on multiple object\nrecognition benchmarks for UDA.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 08:50:32 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 04:50:40 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 10:42:43 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Hu", "Dapeng", ""], ["Liang", "Jian", ""], ["Hou", "Qibin", ""], ["Yan", "Hanshu", ""], ["Chen", "Yunpeng", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "2003.13296", "submitter": "Matthias De Lange", "authors": "Matthias De Lange, Xu Jia, Sarah Parisot, Ales Leonardis, Gregory\n  Slabaugh, Tinne Tuytelaars", "title": "Unsupervised Model Personalization while Preserving Privacy and\n  Scalability: An Open Problem", "comments": "CVPR 2020", "journal-ref": "Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition (CVPR), June 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the task of unsupervised model personalization,\nadapted to continually evolving, unlabeled local user images. We consider the\npractical scenario where a high capacity server interacts with a myriad of\nresource-limited edge devices, imposing strong requirements on scalability and\nlocal data privacy. We aim to address this challenge within the continual\nlearning paradigm and provide a novel Dual User-Adaptation framework (DUA) to\nexplore the problem. This framework flexibly disentangles user-adaptation into\nmodel personalization on the server and local data regularization on the user\ndevice, with desirable properties regarding scalability and privacy\nconstraints. First, on the server, we introduce incremental learning of\ntask-specific expert models, subsequently aggregated using a concealed\nunsupervised user prior. Aggregation avoids retraining, whereas the user prior\nconceals sensitive raw user data, and grants unsupervised adaptation. Second,\nlocal user-adaptation incorporates a domain adaptation point of view, adapting\nregularizing batch normalization parameters to the user data. We explore\nvarious empirical user configurations with different priors in categories and a\ntenfold of transforms for MIT Indoor Scene recognition, and classify numbers in\na combined MNIST and SVHN setup. Extensive experiments yield promising results\nfor data-driven local adaptation and elicit user priors for server adaptation\nto depend on the model rather than user data. Hence, although user-adaptation\nremains a challenging open problem, the DUA framework formalizes a principled\nfoundation for personalizing both on server and user device, while maintaining\nprivacy and scalability.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 09:35:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["De Lange", "Matthias", ""], ["Jia", "Xu", ""], ["Parisot", "Sarah", ""], ["Leonardis", "Ales", ""], ["Slabaugh", "Gregory", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "2003.13298", "submitter": "Hanwen Kang", "authors": "Hanwen Kang, Chao Chen", "title": "Real-Time Fruit Recognition and Grasping Estimation for Autonomous Apple\n  Harvesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, a fully neural network based visual perception framework\nfor autonomous apple harvesting is proposed. The proposed framework includes a\nmulti-function neural network for fruit recognition and a Pointnet grasp\nestimation to determine the proper grasp pose to guide the robotic execution.\nFruit recognition takes raw input of RGB images from the RGB-D camera to\nperform fruit detection and instance segmentation, and Pointnet grasp\nestimation take point cloud of each fruit as input and output the prediction of\ngrasp pose for each of fruits. The proposed framework is validated by using\nRGB-D images collected from laboratory and orchard environments, a robotic\ngrasping test in a controlled environment is also included in the experiments.\nExperimental shows that the proposed framework can accurately localise and\nestimate the grasp pose for robotic grasping.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 09:37:55 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 12:07:21 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kang", "Hanwen", ""], ["Chen", "Chao", ""]]}, {"id": "2003.13322", "submitter": "Zhenzhou Wang", "authors": "Yongcan Shuang and Zhenzhou Wang", "title": "Active stereo vision three-dimensional reconstruction by RGB dot pattern\n  projection and ray intersection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active stereo vision is important in reconstructing objects without obvious\ntextures. However, it is still very challenging to extract and match the\nprojected patterns from two camera views automatically and robustly. In this\npaper, we propose a new pattern extraction method and a new stereo vision\nmatching method based on our novel structured light pattern. Instead of using\nthe widely used 2D disparity to calculate the depths of the objects, we use the\nray intersection to compute the 3D shapes directly. Experimental results showed\nthat the proposed approach could reconstruct the 3D shape of the object\nsignificantly more robustly than state of the art methods that include the\nwidely used disparity based active stereo vision method, the time of flight\nmethod and the structured light method. In addition, experimental results also\nshowed that the proposed approach could reconstruct the 3D motions of the\ndynamic shapes robustly.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:13:28 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 01:44:41 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Shuang", "Yongcan", ""], ["Wang", "Zhenzhou", ""]]}, {"id": "2003.13326", "submitter": "Amir Hertz", "authors": "Amir Hertz, Rana Hanocka, Raja Giryes, Daniel Cohen-Or", "title": "PointGMM: a Neural GMM Network for Point Clouds", "comments": "CVPR 2020 -- final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds are a popular representation for 3D shapes. However, they encode\na particular sampling without accounting for shape priors or non-local\ninformation. We advocate for the use of a hierarchical Gaussian mixture model\n(hGMM), which is a compact, adaptive and lightweight representation that\nprobabilistically defines the underlying 3D surface. We present PointGMM, a\nneural network that learns to generate hGMMs which are characteristic of the\nshape class, and also coincide with the input point cloud. PointGMM is trained\nover a collection of shapes to learn a class-specific prior. The hierarchical\nrepresentation has two main advantages: (i) coarse-to-fine learning, which\navoids converging to poor local-minima; and (ii) (an unsupervised) consistent\npartitioning of the input shape. We show that as a generative model, PointGMM\nlearns a meaningful latent space which enables generating consistent\ninterpolations between existing shapes, as well as synthesizing novel shapes.\nWe also present a novel framework for rigid registration using PointGMM, that\nlearns to disentangle orientation from structure of an input shape.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:34:59 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hertz", "Amir", ""], ["Hanocka", "Rana", ""], ["Giryes", "Raja", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "2003.13328", "submitter": "Qibin Hou", "authors": "Qibin Hou, Li Zhang, Ming-Ming Cheng, Jiashi Feng", "title": "Strip Pooling: Rethinking Spatial Pooling for Scene Parsing", "comments": "Published as a CVPR2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial pooling has been proven highly effective in capturing long-range\ncontextual information for pixel-wise prediction tasks, such as scene parsing.\nIn this paper, beyond conventional spatial pooling that usually has a regular\nshape of NxN, we rethink the formulation of spatial pooling by introducing a\nnew pooling strategy, called strip pooling, which considers a long but narrow\nkernel, i.e., 1xN or Nx1. Based on strip pooling, we further investigate\nspatial pooling architecture design by 1) introducing a new strip pooling\nmodule that enables backbone networks to efficiently model long-range\ndependencies, 2) presenting a novel building block with diverse spatial pooling\nas a core, and 3) systematically comparing the performance of the proposed\nstrip pooling and conventional spatial pooling techniques. Both novel\npooling-based designs are lightweight and can serve as an efficient\nplug-and-play module in existing scene parsing networks. Extensive experiments\non popular benchmarks (e.g., ADE20K and Cityscapes) demonstrate that our simple\napproach establishes new state-of-the-art results. Code is made available at\nhttps://github.com/Andrew-Qibin/SPNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 10:40:11 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Hou", "Qibin", ""], ["Zhang", "Li", ""], ["Cheng", "Ming-Ming", ""], ["Feng", "Jiashi", ""]]}, {"id": "2003.13401", "submitter": "Ronak Kosti", "authors": "Ronak Kosti, Jose M. Alvarez, Adria Recasens, Agata Lapedriza", "title": "Context Based Emotion Recognition using EMOTIC Dataset", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2019.2916866", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our everyday lives and social interactions we often try to perceive the\nemotional states of people. There has been a lot of research in providing\nmachines with a similar capacity of recognizing emotions. From a computer\nvision perspective, most of the previous efforts have been focusing in\nanalyzing the facial expressions and, in some cases, also the body pose. Some\nof these methods work remarkably well in specific settings. However, their\nperformance is limited in natural, unconstrained environments. Psychological\nstudies show that the scene context, in addition to facial expression and body\npose, provides important information to our perception of people's emotions.\nHowever, the processing of the context for automatic emotion recognition has\nnot been explored in depth, partly due to the lack of proper data. In this\npaper we present EMOTIC, a dataset of images of people in a diverse set of\nnatural situations, annotated with their apparent emotion. The EMOTIC dataset\ncombines two different types of emotion representation: (1) a set of 26\ndiscrete categories, and (2) the continuous dimensions Valence, Arousal, and\nDominance. We also present a detailed statistical and algorithmic analysis of\nthe dataset along with annotators' agreement analysis. Using the EMOTIC dataset\nwe train different CNN models for emotion recognition, combining the\ninformation of the bounding box containing the person with the contextual\ninformation extracted from the scene. Our results show how scene context\nprovides important information to automatically recognize emotional states and\nmotivate further research in this direction. Dataset and code is open-sourced\nand available at: https://github.com/rkosti/emotic and link for the\npeer-reviewed published article: https://ieeexplore.ieee.org/document/8713881\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 12:38:50 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Kosti", "Ronak", ""], ["Alvarez", "Jose M.", ""], ["Recasens", "Adria", ""], ["Lapedriza", "Agata", ""]]}, {"id": "2003.13402", "submitter": "Thomas Roddick", "authors": "Thomas Roddick, Roberto Cipolla", "title": "Predicting Semantic Map Representations from Images using Pyramid\n  Occupancy Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles commonly rely on highly detailed birds-eye-view maps of\ntheir environment, which capture both static elements of the scene such as road\nlayout as well as dynamic elements such as other cars and pedestrians.\nGenerating these map representations on the fly is a complex multi-stage\nprocess which incorporates many important vision-based elements, including\nground plane estimation, road segmentation and 3D object detection. In this\nwork we present a simple, unified approach for estimating maps directly from\nmonocular images using a single end-to-end deep learning architecture. For the\nmaps themselves we adopt a semantic Bayesian occupancy grid framework, allowing\nus to trivially accumulate information over multiple cameras and timesteps. We\ndemonstrate the effectiveness of our approach by evaluating against several\nchallenging baselines on the NuScenes and Argoverse datasets, and show that we\nare able to achieve a relative improvement of 9.1% and 22.3% respectively\ncompared to the best-performing existing method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 12:39:44 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Roddick", "Thomas", ""], ["Cipolla", "Roberto", ""]]}, {"id": "2003.13431", "submitter": "Jaime Spencer Martin Mr.", "authors": "Jaime Spencer, Richard Bowden, Simon Hadfield", "title": "Same Features, Different Day: Weakly Supervised Feature Learning for\n  Seasonal Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"Like night and day\" is a commonly used expression to imply that two things\nare completely different. Unfortunately, this tends to be the case for current\nvisual feature representations of the same scene across varying seasons or\ntimes of day. The aim of this paper is to provide a dense feature\nrepresentation that can be used to perform localization, sparse matching or\nimage retrieval, regardless of the current seasonal or temporal appearance.\n  Recently, there have been several proposed methodologies for deep learning\ndense feature representations. These methods make use of ground truth\npixel-wise correspondences between pairs of images and focus on the spatial\nproperties of the features. As such, they don't address temporal or seasonal\nvariation. Furthermore, obtaining the required pixel-wise correspondence data\nto train in cross-seasonal environments is highly complex in most scenarios.\n  We propose Deja-Vu, a weakly supervised approach to learning season invariant\nfeatures that does not require pixel-wise ground truth data. The proposed\nsystem only requires coarse labels indicating if two images correspond to the\nsame location or not. From these labels, the network is trained to produce\n\"similar\" dense feature maps for corresponding locations despite environmental\nchanges. Code will be made available at:\nhttps://github.com/jspenmar/DejaVu_Features\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 12:56:44 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Spencer", "Jaime", ""], ["Bowden", "Richard", ""], ["Hadfield", "Simon", ""]]}, {"id": "2003.13440", "submitter": "German Gonzalez", "authors": "Germ\\'an Gonz\\'alez, Daniel Jimenez-Carretero, Sara\n  Rodr\\'iguez-L\\'opez, Carlos Cano-Espinosa, Miguel Cazorla, Tanya Agarwal,\n  Vinit Agarwal, Nima Tajbakhsh, Michael B. Gotway, Jianming Liang, Mojtaba\n  Masoudi, Noushin Eftekhari, Mahdi Saadatmand, Hamid-Reza Pourreza, Patricia\n  Fraga-Rivas, Eduardo Fraile, Frank J. Rybicki, Ara Kassarjian, Ra\\'ul San\n  Jos\\'e Est\\'epar and Maria J. Ledesma-Carbayo", "title": "Computer Aided Detection for Pulmonary Embolism Challenge (CAD-PE)", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rationale: Computer aided detection (CAD) algorithms for Pulmonary Embolism\n(PE) algorithms have been shown to increase radiologists' sensitivity with a\nsmall increase in specificity. However, CAD for PE has not been adopted into\nclinical practice, likely because of the high number of false positives current\nCAD software produces. Objective: To generate a database of annotated computed\ntomography pulmonary angiographies, use it to compare the sensitivity and false\npositive rate of current algorithms and to develop new methods that improve\nsuch metrics. Methods: 91 Computed tomography pulmonary angiography scans were\nannotated by at least one radiologist by segmenting all pulmonary emboli\nvisible on the study. 20 annotated CTPAs were open to the public in the form of\na medical image analysis challenge. 20 more were kept for evaluation purposes.\n51 were made available post-challenge. 8 submissions, 6 of them novel, were\nevaluated on the 20 evaluation CTPAs. Performance was measured as per embolus\nsensitivity vs. false positives per scan curve. Results: The best algorithms\nachieved a per-embolus sensitivity of 75% at 2 false positives per scan (fps)\nor of 70% at 1 fps, outperforming the state of the art. Deep learning\napproaches outperformed traditional machine learning ones, and their\nperformance improved with the number of training cases. Significance: Through\nthis work and challenge we have improved the state-of-the art of computer aided\ndetection algorithms for pulmonary embolism. An open database and an evaluation\nbenchmark for such algorithms have been generated, easing the development of\nfurther improvements. Implications on clinical practice will need further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:05:07 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gonz\u00e1lez", "Germ\u00e1n", ""], ["Jimenez-Carretero", "Daniel", ""], ["Rodr\u00edguez-L\u00f3pez", "Sara", ""], ["Cano-Espinosa", "Carlos", ""], ["Cazorla", "Miguel", ""], ["Agarwal", "Tanya", ""], ["Agarwal", "Vinit", ""], ["Tajbakhsh", "Nima", ""], ["Gotway", "Michael B.", ""], ["Liang", "Jianming", ""], ["Masoudi", "Mojtaba", ""], ["Eftekhari", "Noushin", ""], ["Saadatmand", "Mahdi", ""], ["Pourreza", "Hamid-Reza", ""], ["Fraga-Rivas", "Patricia", ""], ["Fraile", "Eduardo", ""], ["Rybicki", "Frank J.", ""], ["Kassarjian", "Ara", ""], ["Est\u00e9par", "Ra\u00fal San Jos\u00e9", ""], ["Ledesma-Carbayo", "Maria J.", ""]]}, {"id": "2003.13446", "submitter": "Jaime Spencer Martin Mr.", "authors": "Jaime Spencer, Richard Bowden, Simon Hadfield", "title": "DeFeat-Net: General Monocular Depth via Simultaneous Unsupervised\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the current monocular depth research, the dominant approach is to employ\nunsupervised training on large datasets, driven by warped photometric\nconsistency. Such approaches lack robustness and are unable to generalize to\nchallenging domains such as nighttime scenes or adverse weather conditions\nwhere assumptions about photometric consistency break down.\n  We propose DeFeat-Net (Depth & Feature network), an approach to\nsimultaneously learn a cross-domain dense feature representation, alongside a\nrobust depth-estimation framework based on warped feature consistency. The\nresulting feature representation is learned in an unsupervised manner with no\nexplicit ground-truth correspondences required.\n  We show that within a single domain, our technique is comparable to both the\ncurrent state of the art in monocular depth estimation and supervised feature\nrepresentation learning. However, by simultaneously learning features, depth\nand motion, our technique is able to generalize to challenging domains,\nallowing DeFeat-Net to outperform the current state-of-the-art with around 10%\nreduction in all error measures on more challenging sequences such as nighttime\ndriving.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:10:32 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Spencer", "Jaime", ""], ["Bowden", "Richard", ""], ["Hadfield", "Simon", ""]]}, {"id": "2003.13471", "submitter": "Luis Oala", "authors": "Jan Macdonald, Maximilian M\\\"arz, Luis Oala and Wojciech Samek", "title": "Interval Neural Networks as Instability Detectors for Image\n  Reconstructions", "comments": "JM, MM and LO contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the detection of instabilities that may occur when\nutilizing deep learning models for image reconstruction tasks. Although neural\nnetworks often empirically outperform traditional reconstruction methods, their\nusage for sensitive medical applications remains controversial. Indeed, in a\nrecent series of works, it has been demonstrated that deep learning approaches\nare susceptible to various types of instabilities, caused for instance by\nadversarial noise or out-of-distribution features. It is argued that this\nphenomenon can be observed regardless of the underlying architecture and that\nthere is no easy remedy. Based on this insight, the present work demonstrates\non two use cases how uncertainty quantification methods can be employed as\ninstability detectors. In particular, it is shown that the recently proposed\nInterval Neural Networks are highly effective in revealing instabilities of\nreconstructions. Such an ability is crucial to ensure a safe use of deep\nlearning-based methods for medical image reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 01:34:16 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Macdonald", "Jan", ""], ["M\u00e4rz", "Maximilian", ""], ["Oala", "Luis", ""], ["Samek", "Wojciech", ""]]}, {"id": "2003.13479", "submitter": "Zi Jian Yew", "authors": "Zi Jian Yew and Gim Hee Lee", "title": "RPM-Net: Robust Point Matching using Learned Features", "comments": "10 pages, 4 figures. To appear in CVPR2020", "journal-ref": null, "doi": "10.1109/CVPR42600.2020.01184", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative Closest Point (ICP) solves the rigid point cloud registration\nproblem iteratively in two steps: (1) make hard assignments of spatially\nclosest point correspondences, and then (2) find the least-squares rigid\ntransformation. The hard assignments of closest point correspondences based on\nspatial distances are sensitive to the initial rigid transformation and\nnoisy/outlier points, which often cause ICP to converge to wrong local minima.\nIn this paper, we propose the RPM-Net -- a less sensitive to initialization and\nmore robust deep learning-based approach for rigid point cloud registration. To\nthis end, our network uses the differentiable Sinkhorn layer and annealing to\nget soft assignments of point correspondences from hybrid features learned from\nboth spatial coordinates and local geometry. To further improve registration\nperformance, we introduce a secondary network to predict optimal annealing\nparameters. Unlike some existing methods, our RPM-Net handles missing\ncorrespondences and point clouds with partial visibility. Experimental results\nshow that our RPM-Net achieves state-of-the-art performance compared to\nexisting non-deep learning and recent deep learning methods. Our source code is\navailable at the project website https://github.com/yewzijian/RPMNet .\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:45:27 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yew", "Zi Jian", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2003.13493", "submitter": "Philipp Foehn", "authors": "Balazs Nagy, Philipp Foehn, Davide Scaramuzza", "title": "Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO", "comments": "IEEE International Conference on Intelligent Robots and Systems\n  (IROS), 2020. Open-source implementation available at\n  https://github.com/uzh-rpg/vilib", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent introduction of powerful embedded graphics processing units (GPUs)\nhas allowed for unforeseen improvements in real-time computer vision\napplications. It has enabled algorithms to run onboard, well above the standard\nvideo rates, yielding not only higher information processing capability, but\nalso reduced latency. This work focuses on the applicability of efficient\nlow-level, GPU hardware-specific instructions to improve on existing computer\nvision algorithms in the field of visual-inertial odometry (VIO). While most\nsteps of a VIO pipeline work on visual features, they rely on image data for\ndetection and tracking, of which both steps are well suited for\nparallelization. Especially non-maxima suppression and the subsequent feature\nselection are prominent contributors to the overall image processing latency.\nOur work first revisits the problem of non-maxima suppression for feature\ndetection specifically on GPUs, and proposes a solution that selects local\nresponse maxima, imposes spatial feature distribution, and extracts features\nsimultaneously. Our second contribution introduces an enhanced FAST feature\ndetector that applies the aforementioned non-maxima suppression method.\nFinally, we compare our method to other state-of-the-art CPU and GPU\nimplementations, where we always outperform all of them in feature tracking and\ndetection, resulting in over 1000fps throughput on an embedded Jetson TX2\nplatform. Additionally, we demonstrate our work integrated in a VIO pipeline\nachieving a metric state estimation at ~200fps.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:16:23 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 12:30:38 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 09:22:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nagy", "Balazs", ""], ["Foehn", "Philipp", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2003.13502", "submitter": "Mohamed Abdelhack", "authors": "Mohamed Abdelhack", "title": "An Open-source Tool for Hyperspectral Image Augmentation in Tensorflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Satellite imagery allows a plethora of applications ranging from weather\nforecasting to land surveying. The rapid development of computer vision systems\ncould open new horizons to the utilization of satellite data due to the\nabundance of large volumes of data. However, current state-of-the-art computer\nvision systems mainly cater to applications that mainly involve natural images.\nWhile useful, those images exhibit a different distribution from satellite\nimages in addition to having more spectral channels. This allows the use of\npretrained deep learning models only in a subset of spectral channels that are\nequivalent to natural images thus discarding valuable information from other\nspectral channels. This calls for research effort to optimize deep learning\nmodels for satellite imagery to enable the assessment of their utility in the\ndomain of remote sensing. Tensorflow tool allows for rapid prototyping and\ntesting of deep learning models, however, its built-in image generator is\ndesigned to handle a maximum of four spectral channels. This manuscript\nintroduces an open-source tool that allows the implementation of image\naugmentation for hyperspectral images in Tensorflow. Given how accessible and\neasy-to-use Tensorflow is, this tool would provide many researchers with the\nmeans to implement, test, and deploy deep learning models for remote sensing\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:28:12 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 17:25:26 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Abdelhack", "Mohamed", ""]]}, {"id": "2003.13503", "submitter": "Prajoy Podder", "authors": "Aditya Khamparia, Subrato Bharati, Prajoy Podder, Deepak Gupta, Ashish\n  Khanna, Thai Kim Phung, Dang N. H. Thanh", "title": "Diagnosis of Breast Cancer Based on Modern Mammography using Hybrid\n  Transfer Learning", "comments": "24 pages, 11 figures", "journal-ref": "Multidimensional Systems and Signal Processing, 2021", "doi": "10.1007/s11045-020-00756-7", "report-no": null, "categories": "eess.IV cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breast cancer is a common cancer for women. Early detection of breast cancer\ncan considerably increase the survival rate of women. This paper mainly focuses\non transfer learning process to detect breast cancer. Modified VGG (MVGG),\nresidual network, mobile network is proposed and implemented in this paper.\nDDSM dataset is used in this paper. Experimental results show that our proposed\nhybrid transfers learning model (Fusion of MVGG16 and ImageNet) provides an\naccuracy of 88.3% where the number of epoch is 15. On the other hand, only\nmodified VGG 16 architecture (MVGG 16) provides an accuracy 80.8% and MobileNet\nprovides an accuracy of 77.2%. So, it is clearly stated that the proposed\nhybrid pre-trained network outperforms well compared to single architecture.\nThis architecture can be considered as an effective tool for the radiologists\nin order to reduce the false negative and false positive rate. Therefore, the\nefficiency of mammography analysis will be improved.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 05:16:34 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 05:39:57 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 05:53:17 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Khamparia", "Aditya", ""], ["Bharati", "Subrato", ""], ["Podder", "Prajoy", ""], ["Gupta", "Deepak", ""], ["Khanna", "Ashish", ""], ["Phung", "Thai Kim", ""], ["Thanh", "Dang N. H.", ""]]}, {"id": "2003.13511", "submitter": "Kartik Gupta", "authors": "Kartik Gupta, Thalaiyasingam Ajanthan", "title": "Improved Gradient based Adversarial Attacks for Quantized Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network quantization has become increasingly popular due to efficient\nmemory consumption and faster computation resulting from bitwise operations on\nthe quantized networks. Even though they exhibit excellent generalization\ncapabilities, their robustness properties are not well-understood. In this\nwork, we systematically study the robustness of quantized networks against\ngradient based adversarial attacks and demonstrate that these quantized models\nsuffer from gradient vanishing issues and show a fake sense of security. By\nattributing gradient vanishing to poor forward-backward signal propagation in\nthe trained network, we introduce a simple temperature scaling approach to\nmitigate this issue while preserving the decision boundary. Despite being a\nsimple modification to existing gradient based adversarial attacks, experiments\non CIFAR-10/100 datasets with VGG-16 and ResNet-18 networks demonstrate that\nour temperature scaled attacks obtain near-perfect success rate on quantized\nnetworks while outperforming original attacks on adversarially trained models\nas well as floating-point networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:34:08 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Gupta", "Kartik", ""], ["Ajanthan", "Thalaiyasingam", ""]]}, {"id": "2003.13516", "submitter": "Fu-En Wang", "authors": "Fu-En Wang, Yu-Hsuan Yeh, Min Sun, Wei-Chen Chiu, Yi-Hsuan Tsai", "title": "LayoutMP3D: Layout Annotation of Matterport3D", "comments": "Annotation is available at https://github.com/fuenwang/LayoutMP3D", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the information of 3D layout from a single equirectangular panorama\nis crucial for numerous applications of virtual reality or robotics (e.g.,\nscene understanding and navigation). To achieve this, several datasets are\ncollected for the task of 360 layout estimation. To facilitate the learning\nalgorithms for autonomous systems in indoor scenarios, we consider the\nMatterport3D dataset with their originally provided depth map ground truths and\nfurther release our annotations for layout ground truths from a subset of\nMatterport3D. As Matterport3D contains accurate depth ground truths from\ntime-of-flight (ToF) sensors, our dataset provides both the layout and depth\ninformation, which enables the opportunity to explore the environment by\nintegrating both cues.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:40:56 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Wang", "Fu-En", ""], ["Yeh", "Yu-Hsuan", ""], ["Sun", "Min", ""], ["Chiu", "Wei-Chen", ""], ["Tsai", "Yi-Hsuan", ""]]}, {"id": "2003.13524", "submitter": "Riccardo La Grassa", "authors": "Riccardo La Grassa, Ignazio Gallo, Nicola Landro", "title": "OCmst: One-class Novelty Detection using Convolutional Neural Network\n  and Minimum Spanning Trees", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel model called One Class Minimum Spanning Tree (OCmst) for\nnovelty detection problem that uses a Convolutional Neural Network (CNN) as\ndeep feature extractor and graph-based model based on Minimum Spanning Tree\n(MST). In a novelty detection scenario, the training data is no polluted by\noutliers (abnormal class) and the goal is to recognize if a test instance\nbelongs to the normal class or to the abnormal class. Our approach uses the\ndeep features from CNN to feed a pair of MSTs built starting from each test\ninstance. To cut down the computational time we use a parameter $\\gamma$ to\nspecify the size of the MST's starting to the neighbours from the test\ninstance. To prove the effectiveness of the proposed approach we conducted\nexperiments on two publicly available datasets, well-known in literature and we\nachieved the state-of-the-art results on CIFAR10 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:55:39 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["La Grassa", "Riccardo", ""], ["Gallo", "Ignazio", ""], ["Landro", "Nicola", ""]]}, {"id": "2003.13525", "submitter": "Isabela Maria Carneiro de Albuquerque", "authors": "Isabela Albuquerque, Nikhil Naik, Junnan Li, Nitish Keskar, and\n  Richard Socher", "title": "Improving out-of-distribution generalization via multi-task\n  self-supervised pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised feature representations have been shown to be useful for\nsupervised classification, few-shot learning, and adversarial robustness. We\nshow that features obtained using self-supervised learning are comparable to,\nor better than, supervised learning for domain generalization in computer\nvision. We introduce a new self-supervised pretext task of predicting responses\nto Gabor filter banks and demonstrate that multi-task learning of compatible\npretext tasks improves domain generalization performance as compared to\ntraining individual tasks alone. Features learnt through self-supervision\nobtain better generalization to unseen domains when compared to their\nsupervised counterpart when there is a larger domain shift between training and\ntest distributions and even show better localization ability for objects of\ninterest. Self-supervised feature representations can also be combined with\nother domain generalization methods to further boost performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:55:53 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Albuquerque", "Isabela", ""], ["Naik", "Nikhil", ""], ["Li", "Junnan", ""], ["Keskar", "Nitish", ""], ["Socher", "Richard", ""]]}, {"id": "2003.13528", "submitter": "Habtamu Fanta Mr", "authors": "Habtamu Fanta, Zhiwen Shao, Lizhuang Ma", "title": "SiTGRU: Single-Tunnelled Gated Recurrent Unit for Abnormality Detection", "comments": "14 pages, 11 figures, 13 tables, this paper is accepted on Journal of\n  Information Sciences", "journal-ref": "Journal of Information Sciences 524 (2020) 15-32", "doi": "10.1016/j.ins.2020.03.034", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abnormality detection is a challenging task due to the dependence on a\nspecific context and the unconstrained variability of practical scenarios. In\nrecent years, it has benefited from the powerful features learnt by deep neural\nnetworks, and handcrafted features specialized for abnormality detectors.\nHowever, these approaches with large complexity still have limitations in\nhandling long term sequential data (e.g., videos), and their learnt features do\nnot thoroughly capture useful information. Recurrent Neural Networks (RNNs)\nhave been shown to be capable of robustly dealing with temporal data in long\nterm sequences. In this paper, we propose a novel version of Gated Recurrent\nUnit (GRU), called Single Tunnelled GRU for abnormality detection.\nParticularly, the Single Tunnelled GRU discards the heavy weighted reset gate\nfrom GRU cells that overlooks the importance of past content by only favouring\ncurrent input to obtain an optimized single gated cell model. Moreover, we\nsubstitute the hyperbolic tangent activation in standard GRUs with sigmoid\nactivation, as the former suffers from performance loss in deeper networks.\nEmpirical results show that our proposed optimized GRU model outperforms\nstandard GRU and Long Short Term Memory (LSTM) networks on most metrics for\ndetection and generalization tasks on CUHK Avenue and UCSD datasets. The model\nis also computationally efficient with reduced training and testing time over\nstandard RNNs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:58:13 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Fanta", "Habtamu", ""], ["Shao", "Zhiwen", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2003.13537", "submitter": "Jose F. Ruiz-Munoz", "authors": "Jose F. Ruiz-Munoz, Jyothier K. Nimmagadda, Tyler G. Dowd, and James\n  E. Baciak, Alina Zare", "title": "Super Resolution for Root Imaging", "comments": "Under review. Submitted to Applications in Plant Sciences (APPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution cameras have become very helpful for plant phenotyping by\nproviding a mechanism for tasks such as target versus background\ndiscrimination, and the measurement and analysis of fine-above-ground plant\nattributes. However, the acquisition of high-resolution (HR) imagery of plant\nroots is more challenging than above-ground data collection. Thus, an effective\nsuper-resolution (SR) algorithm is desired for overcoming resolution\nlimitations of sensors, reducing storage space requirements, and boosting the\nperformance of later analysis, such as automatic segmentation. We propose a SR\nframework for enhancing images of plant roots by using convolutional neural\nnetworks (CNNs). We compare three alternatives for training the SR model: i)\ntraining with non-plant-root images, ii) training with plant-root images, and\niii) pretraining the model with non-plant-root images and fine-tuning with\nplant-root images. We demonstrate on a collection of publicly available\ndatasets that the SR models outperform the basic bicubic interpolation even\nwhen trained with non-root datasets. Also, our segmentation experiments show\nthat high performance on this task can be achieved independently of the SNR.\nTherefore, we conclude that the quality of the image enhancement depends on the\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:11:15 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 12:23:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Ruiz-Munoz", "Jose F.", ""], ["Nimmagadda", "Jyothier K.", ""], ["Dowd", "Tyler G.", ""], ["Baciak", "James E.", ""], ["Zare", "Alina", ""]]}, {"id": "2003.13549", "submitter": "Manuel Amthor", "authors": "Daniel Haase and Manuel Amthor", "title": "Rethinking Depthwise Separable Convolutions: How Intra-Kernel\n  Correlations Lead to Improved MobileNets", "comments": "Published at CVPR 2020. Code and models are available under\n  https://github.com/zeiss-microscopy/BSConv", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2020, pp. 14600-14609", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce blueprint separable convolutions (BSConv) as highly efficient\nbuilding blocks for CNNs. They are motivated by quantitative analyses of kernel\nproperties from trained models, which show the dominance of correlations along\nthe depth axis. Based on our findings, we formulate a theoretical foundation\nfrom which we derive efficient implementations using only standard layers.\nMoreover, our approach provides a thorough theoretical derivation,\ninterpretation, and justification for the application of depthwise separable\nconvolutions (DSCs) in general, which have become the basis of many modern\nnetwork architectures. Ultimately, we reveal that DSC-based architectures such\nas MobileNets implicitly rely on cross-kernel correlations, while our BSConv\nformulation is based on intra-kernel correlations and thus allows for a more\nefficient separation of regular convolutions. Extensive experiments on\nlarge-scale and fine-grained classification datasets show that BSConvs clearly\nand consistently improve MobileNets and other DSC-based architectures without\nintroducing any further complexity. For fine-grained datasets, we achieve an\nimprovement of up to 13.7 percentage points. In addition, if used as drop-in\nreplacement for standard architectures such as ResNets, BSConv variants also\noutperform their vanilla counterparts by up to 9.5 percentage points on\nImageNet. Code and models are available under\nhttps://github.com/zeiss-microscopy/BSConv.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:23:27 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 08:48:48 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 14:57:16 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Haase", "Daniel", ""], ["Amthor", "Manuel", ""]]}, {"id": "2003.13552", "submitter": "Di Ma", "authors": "Di Ma, Fan Zhang, and David R. Bull", "title": "BVI-DVC: A Training Database for Deep Video Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep learning methods are increasingly being applied in the optimisation of\nvideo compression algorithms and can achieve significantly enhanced coding\ngains, compared to conventional approaches. Such approaches often employ\nConvolutional Neural Networks (CNNs) which are trained on databases with\nrelatively limited content coverage. In this paper, a new extensive and\nrepresentative video database, BVI-DVC, is presented for training CNN-based\nvideo compression systems, with specific emphasis on machine learning tools\nthat enhance conventional coding architectures, including spatial resolution\nand bit depth up-sampling, post-processing and in-loop filtering. BVI-DVC\ncontains 800 sequences at various spatial resolutions from 270p to 2160p and\nhas been evaluated on ten existing network architectures for four different\ncoding tools. Experimental results show that this database produces significant\nimprovements in terms of coding gains over three existing (commonly used)\nimage/video training databases under the same training and evaluation\nconfigurations. The overall additional coding improvements by using the\nproposed database for all tested coding modules and CNN architectures are up to\n10.3% based on the assessment of PSNR and 8.1% based on VMAF.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 15:26:16 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 10:24:30 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ma", "Di", ""], ["Zhang", "Fan", ""], ["Bull", "David R.", ""]]}, {"id": "2003.13586", "submitter": "Heitor Felix BSc", "authors": "Heitor Felix, Walber M. Rodrigues, David Mac\\^edo, Francisco Sim\\~oes,\n  Adriano L. I. Oliveira, Veronica Teichrieb and Cleber Zanchettin", "title": "Squeezed Deep 6DoF Object Detection Using Knowledge Distillation", "comments": "This paper was accepted by 2020 International Joint Conference on\n  Neural Networks (IJCNN)", "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN)", "doi": "10.1109/IJCNN48605.2020.9207459", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of objects considering a 6DoF pose is a common requirement to\nbuild virtual and augmented reality applications. It is usually a complex task\nwhich requires real-time processing and high precision results for adequate\nuser experience. Recently, different deep learning techniques have been\nproposed to detect objects in 6DoF in RGB images. However, they rely on high\ncomplexity networks, requiring a computational power that prevents them from\nworking on mobile devices. In this paper, we propose an approach to reduce the\ncomplexity of 6DoF detection networks while maintaining accuracy. We used\nKnowledge Distillation to teach portables Convolutional Neural Networks (CNN)\nto learn from a real-time 6DoF detection CNN. The proposed method allows\nreal-time applications using only RGB images while decreasing the hardware\nrequirements. We used the LINEMOD dataset to evaluate the proposed method, and\nthe experimental results show that the proposed method reduces the memory\nrequirement by almost 99\\% in comparison to the original architecture with the\ncost of reducing half the accuracy in one of the metrics. Code is available at\nhttps://github.com/heitorcfelix/singleshot6Dpose.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:03:03 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 20:04:33 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 22:41:21 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Felix", "Heitor", ""], ["Rodrigues", "Walber M.", ""], ["Mac\u00eado", "David", ""], ["Sim\u00f5es", "Francisco", ""], ["Oliveira", "Adriano L. I.", ""], ["Teichrieb", "Veronica", ""], ["Zanchettin", "Cleber", ""]]}, {"id": "2003.13593", "submitter": "Tai Vu", "authors": "Tai Vu, Emily Wen, Roy Nehoran", "title": "How Not to Give a FLOP: Combining Regularization and Pruning for\n  Efficient Inference", "comments": "Citations added, typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The challenge of speeding up deep learning models during the deployment phase\nhas been a large, expensive bottleneck in the modern tech industry. In this\npaper, we examine the use of both regularization and pruning for reduced\ncomputational complexity and more efficient inference in Deep Neural Networks\n(DNNs). In particular, we apply mixup and cutout regularizations and soft\nfilter pruning to the ResNet architecture, focusing on minimizing\nfloating-point operations (FLOPs). Furthermore, by using regularization in\nconjunction with network pruning, we show that such a combination makes a\nsubstantial improvement over each of the two techniques individually.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:20:46 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 11:21:07 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Vu", "Tai", ""], ["Wen", "Emily", ""], ["Nehoran", "Roy", ""]]}, {"id": "2003.13594", "submitter": "Arsha Nagrani", "authors": "Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia\n  Schmid, Andrew Zisserman", "title": "Speech2Action: Cross-modal Supervision for Action Recognition", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to guess human action from dialogue alone? In this work we\ninvestigate the link between spoken words and actions in movies. We note that\nmovie screenplays describe actions, as well as contain the speech of characters\nand hence can be used to learn this correlation with no additional supervision.\nWe train a BERT-based Speech2Action classifier on over a thousand movie\nscreenplays, to predict action labels from transcribed speech segments. We then\napply this model to the speech segments of a large unlabelled movie corpus\n(188M speech segments from 288K movies). Using the predictions of this model,\nwe obtain weak action labels for over 800K video clips. By training on these\nvideo clips, we demonstrate superior action recognition performance on standard\naction recognition benchmarks, without using a single manually labelled action\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:22:39 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Nagrani", "Arsha", ""], ["Sun", "Chen", ""], ["Ross", "David", ""], ["Sukthankar", "Rahul", ""], ["Schmid", "Cordelia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "2003.13623", "submitter": "Jianbo Jiao", "authors": "Jianbo Jiao, Linchao Bao, Yunchao Wei, Shengfeng He, Honghui Shi,\n  Rynson Lau and Thomas S. Huang", "title": "Laplacian Denoising Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have been shown to perform remarkably well in many\nmachine learning tasks, labeling a large amount of ground truth data for\nsupervised training is usually very costly to scale. Therefore, learning robust\nrepresentations with unlabeled data is critical in relieving human effort and\nvital for many downstream tasks. Recent advances in unsupervised and\nself-supervised learning approaches for visual data have benefited greatly from\ndomain knowledge. Here we are interested in a more generic unsupervised\nlearning framework that can be easily generalized to other domains. In this\npaper, we propose to learn data representations with a novel type of denoising\nautoencoder, where the noisy input data is generated by corrupting latent clean\ndata in the gradient domain. This can be naturally generalized to span multiple\nscales with a Laplacian pyramid representation of the input data. In this way,\nthe agent learns more robust representations that exploit the underlying data\nstructures across multiple scales. Experiments on several visual benchmarks\ndemonstrate that better representations can be learned with the proposed\napproach, compared to its counterpart with single-scale corruption and other\napproaches. Furthermore, we also demonstrate that the learned representations\nperform well when transferring to other downstream vision tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 16:52:39 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Jiao", "Jianbo", ""], ["Bao", "Linchao", ""], ["Wei", "Yunchao", ""], ["He", "Shengfeng", ""], ["Shi", "Honghui", ""], ["Lau", "Rynson", ""], ["Huang", "Thomas S.", ""]]}, {"id": "2003.13630", "submitter": "Tal Ridnik", "authors": "Tal Ridnik, Hussam Lawen, Asaf Noy, Emanuel Ben Baruch, Gilad Sharir,\n  Itamar Friedman", "title": "TResNet: High Performance GPU-Dedicated Architecture", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many deep learning models, developed in recent years, reach higher ImageNet\naccuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are\noften seen as a proxy for network efficiency, when measuring actual GPU\ntraining and inference throughput, vanilla ResNet50 is usually significantly\nfaster than its recent competitors, offering better throughput-accuracy\ntrade-off.\n  In this work, we introduce a series of architecture modifications that aim to\nboost neural networks' accuracy, while retaining their GPU training and\ninference efficiency. We first demonstrate and discuss the bottlenecks induced\nby FLOPs-optimizations. We then suggest alternative designs that better utilize\nGPU structure and assets. Finally, we introduce a new family of GPU-dedicated\nmodels, called TResNet, which achieve better accuracy and efficiency than\nprevious ConvNets.\n  Using a TResNet model, with similar GPU throughput to ResNet50, we reach 80.8\ntop-1 accuracy on ImageNet. Our TResNet models also transfer well and achieve\nstate-of-the-art accuracy on competitive single-label classification datasets\nsuch as Stanford cars (96.0%), CIFAR-10 (99.0%), CIFAR-100 (91.5%) and\nOxford-Flowers (99.1%). They also perform well on multi-label classification\nand object detection tasks. Implementation is available at:\nhttps://github.com/mrT23/TResNet.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:04:47 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 06:16:27 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 05:36:43 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ridnik", "Tal", ""], ["Lawen", "Hussam", ""], ["Noy", "Asaf", ""], ["Baruch", "Emanuel Ben", ""], ["Sharir", "Gilad", ""], ["Friedman", "Itamar", ""]]}, {"id": "2003.13644", "submitter": "Hui-Lee Ooi", "authors": "Hui-Lee Ooi, Guillaume-Alexandre Bilodeau, and Nicolas Saunier", "title": "Supervised and Unsupervised Detections for Multiple Object Tracking in\n  Traffic Scenes: A Comparative Study", "comments": "Accepted for ICIAR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multiple object tracker, called MF-Tracker, that\nintegrates multiple classical features (spatial distances and colours) and\nmodern features (detection labels and re-identification features) in its\ntracking framework. Since our tracker can work with detections coming either\nfrom unsupervised and supervised object detectors, we also investigated the\nimpact of supervised and unsupervised detection inputs in our method and for\ntracking road users in general. We also compared our results with existing\nmethods that were applied on the UA-Detrac and the UrbanTracker datasets.\nResults show that our proposed method is performing very well in both datasets\nwith different inputs (MOTA ranging from 0:3491 to 0:5805 for unsupervised\ninputs on the UrbanTracker dataset and an average MOTA of 0:7638 for supervised\ninputs on the UA Detrac dataset) under different circumstances. A well-trained\nsupervised object detector can give better results in challenging scenarios.\nHowever, in simpler scenarios, if good training data is not available,\nunsupervised method can perform well and can be a good alternative.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:27:04 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Ooi", "Hui-Lee", ""], ["Bilodeau", "Guillaume-Alexandre", ""], ["Saunier", "Nicolas", ""]]}, {"id": "2003.13648", "submitter": "Sheng Sun", "authors": "Sheng Sun, Armando Marino, Wenze Shui, Zhongwen Hu", "title": "Weakly-supervised land classification for coastal zone based on deep\n  convolutional neural networks by incorporating dual-polarimetric\n  characteristics into training dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the performance of DCNNs on semantic segmentation\nusing spaceborne polarimetric synthetic aperture radar (PolSAR) datasets. The\nsemantic segmentation task using PolSAR data can be categorized as weakly\nsupervised learning when the characteristics of SAR data and data annotating\nprocedures are factored in. Datasets are initially analyzed for selecting\nfeasible pre-training images. Then the differences between spaceborne and\nairborne datasets are examined in terms of spatial resolution and viewing\ngeometry. In this study we used two dual-polarimetric images acquired by\nTerraSAR-X DLR. A novel method to produce training dataset with more supervised\ninformation is developed. Specifically, a series of typical classified images\nas well as intensity images serve as training datasets. A field survey is\nconducted for an area of about 20 square kilometers to obtain a ground truth\ndataset used for accuracy evaluation. Several transfer learning strategies are\nmade for aforementioned training datasets which will be combined in a\npracticable order. Three DCNN models, including SegNet, U-Net, and LinkNet, are\nimplemented next.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:32:49 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Sun", "Sheng", ""], ["Marino", "Armando", ""], ["Shui", "Wenze", ""], ["Hu", "Zhongwen", ""]]}, {"id": "2003.13653", "submitter": "Marco Domenico Cirillo", "authors": "Marco Domenico Cirillo and David Abramian and Anders Eklund", "title": "Vox2Vox: 3D-GAN for Brain Tumour Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gliomas are the most common primary brain malignancies, with different\ndegrees of aggressiveness, variable prognosis and various heterogeneous\nhistological sub-regions, i.e., peritumoral edema, necrotic core, enhancing and\nnon-enhancing tumour core. Although brain tumours can easily be detected using\nmulti-modal MRI, accurate tumor segmentation is a challenging task. Hence,\nusing the data provided by the BraTS Challenge 2020, we propose a 3D\nvolume-to-volume Generative Adversarial Network for segmentation of brain\ntumours. The model, called Vox2Vox, generates realistic segmentation outputs\nfrom multi-channel 3D MR images, segmenting the whole, core and enhancing tumor\nwith mean values of 87.20%, 81.14%, and 78.67% as dice scores and 6.44mm,\n24.36mm, and 18.95mm for Hausdorff distance 95 percentile for the BraTS testing\nset after ensembling 10 Vox2Vox models obtained with a 10-fold\ncross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 18:57:08 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 19:19:13 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 11:38:25 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Cirillo", "Marco Domenico", ""], ["Abramian", "David", ""], ["Eklund", "Anders", ""]]}, {"id": "2003.13654", "submitter": "Xin Yuan", "authors": "Xin Yuan, Yang Liu, Jinli Suo and Qionghai Dai", "title": "Plug-and-Play Algorithms for Large-scale Snapshot Compressive Imaging", "comments": "CVPR 2020. Corrected a proof of convergence in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snapshot compressive imaging (SCI) aims to capture the high-dimensional\n(usually 3D) images using a 2D sensor (detector) in a single snapshot. Though\nenjoying the advantages of low-bandwidth, low-power and low-cost, applying SCI\nto large-scale problems (HD or UHD videos) in our daily life is still\nchallenging. The bottleneck lies in the reconstruction algorithms; they are\neither too slow (iterative optimization algorithms) or not flexible to the\nencoding process (deep learning based end-to-end networks). In this paper, we\ndevelop fast and flexible algorithms for SCI based on the plug-and-play (PnP)\nframework. In addition to the widely used PnP-ADMM method, we further propose\nthe PnP-GAP (generalized alternating projection) algorithm with a lower\ncomputational workload and prove the convergence of PnP-GAP under the SCI\nhardware constraints. By employing deep denoising priors, we first time show\nthat PnP can recover a UHD color video ($3840\\times 1644\\times 48$ with PNSR\nabove 30dB) from a snapshot 2D measurement. Extensive results on both\nsimulation and real datasets verify the superiority of our proposed algorithm.\nThe code is available at https://github.com/liuyang12/PnP-SCI.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:41:12 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 21:10:04 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Yuan", "Xin", ""], ["Liu", "Yang", ""], ["Suo", "Jinli", ""], ["Dai", "Qionghai", ""]]}, {"id": "2003.13659", "submitter": "Xingang Pan", "authors": "Xingang Pan, Xiaohang Zhan, Bo Dai, Dahua Lin, Chen Change Loy, Ping\n  Luo", "title": "Exploiting Deep Generative Prior for Versatile Image Restoration and\n  Manipulation", "comments": "Accepted to ECCV2020 as oral. 1) Precise GAN-inversion by\n  discriminator-guided generator finetuning. 2) A versatile way for\n  high-quality image restoration and manipulation. Code:\n  https://github.com/XingangPan/deep-generative-prior", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a good image prior is a long-term goal for image restoration and\nmanipulation. While existing methods like deep image prior (DIP) capture\nlow-level image statistics, there are still gaps toward an image prior that\ncaptures rich image semantics including color, spatial coherence, textures, and\nhigh-level concepts. This work presents an effective way to exploit the image\nprior captured by a generative adversarial network (GAN) trained on large-scale\nnatural images. As shown in Fig.1, the deep generative prior (DGP) provides\ncompelling results to restore missing semantics, e.g., color, patch,\nresolution, of various degraded images. It also enables diverse image\nmanipulation including random jittering, image morphing, and category transfer.\nSuch highly flexible restoration and manipulation are made possible through\nrelaxing the assumption of existing GAN-inversion methods, which tend to fix\nthe generator. Notably, we allow the generator to be fine-tuned on-the-fly in a\nprogressive manner regularized by feature distance obtained by the\ndiscriminator in GAN. We show that these easy-to-implement and practical\nchanges help preserve the reconstruction to remain in the manifold of nature\nimage, and thus lead to more precise and faithful reconstruction for real\nimages. Code is available at\nhttps://github.com/XingangPan/deep-generative-prior.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:45:07 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 14:20:37 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 06:41:17 GMT"}, {"version": "v4", "created": "Mon, 20 Jul 2020 10:06:24 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Pan", "Xingang", ""], ["Zhan", "Xiaohang", ""], ["Dai", "Bo", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""], ["Luo", "Ping", ""]]}, {"id": "2003.13678", "submitter": "Ilija Radosavovic", "authors": "Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He,\n  Piotr Doll\\'ar", "title": "Designing Network Design Spaces", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a new network design paradigm. Our goal is to help\nadvance the understanding of network design and discover design principles that\ngeneralize across settings. Instead of focusing on designing individual network\ninstances, we design network design spaces that parametrize populations of\nnetworks. The overall process is analogous to classic manual design of\nnetworks, but elevated to the design space level. Using our methodology we\nexplore the structure aspect of network design and arrive at a low-dimensional\ndesign space consisting of simple, regular networks that we call RegNet. The\ncore insight of the RegNet parametrization is surprisingly simple: widths and\ndepths of good networks can be explained by a quantized linear function. We\nanalyze the RegNet design space and arrive at interesting findings that do not\nmatch the current practice of network design. The RegNet design space provides\nsimple and fast networks that work well across a wide range of flop regimes.\nUnder comparable training settings and flops, the RegNet models outperform the\npopular EfficientNet models while being up to 5x faster on GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:57:47 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Radosavovic", "Ilija", ""], ["Kosaraju", "Raj Prateek", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""], ["Doll\u00e1r", "Piotr", ""]]}, {"id": "2003.13683", "submitter": "Yawei Li", "authors": "Yawei Li, Shuhang Gu, Kai Zhang, Luc Van Gool, Radu Timofte", "title": "DHP: Differentiable Meta Pruning via HyperNetworks", "comments": "ECCV camera-ready. Code is available at\n  https://github.com/ofsoundof/dhp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network pruning has been the driving force for the acceleration of neural\nnetworks and the alleviation of model storage/transmission burden. With the\nadvent of AutoML and neural architecture search (NAS), pruning has become\ntopical with automatic mechanism and searching based architecture optimization.\nYet, current automatic designs rely on either reinforcement learning or\nevolutionary algorithm. Due to the non-differentiability of those algorithms,\nthe pruning algorithm needs a long searching stage before reaching the\nconvergence.\n  To circumvent this problem, this paper introduces a differentiable pruning\nmethod via hypernetworks for automatic network pruning. The specifically\ndesigned hypernetworks take latent vectors as input and generate the weight\nparameters of the backbone network. The latent vectors control the output\nchannels of the convolutional layers in the backbone network and act as a\nhandle for the pruning of the layers. By enforcing $\\ell_1$ sparsity\nregularization to the latent vectors and utilizing proximal gradient solver,\nsparse latent vectors can be obtained. Passing the sparsified latent vectors\nthrough the hypernetworks, the corresponding slices of the generated weight\nparameters can be removed, achieving the effect of network pruning. The latent\nvectors of all the layers are pruned together, resulting in an automatic layer\nconfiguration. Extensive experiments are conducted on various networks for\nimage classification, single image super-resolution, and denoising. And the\nexperimental results validate the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 17:59:18 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 11:16:27 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 10:59:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Yawei", ""], ["Gu", "Shuhang", ""], ["Zhang", "Kai", ""], ["Van Gool", "Luc", ""], ["Timofte", "Radu", ""]]}, {"id": "2003.13732", "submitter": "Mercedes Garcia-Salguero", "authors": "Mercedes Garcia-Salguero, Jesus Briales and Javier Gonzalez-Jimenez", "title": "Certifiable Relative Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first fast optimality certifier for the\nnon-minimal version of the Relative Pose problem for calibrated cameras from\nepipolar constraints. The proposed certifier is based on Lagrangian duality and\nrelies on a novel closed-form expression for dual points. We also leverage an\nefficient solver that performs local optimization on the manifold of the\noriginal problem's non-convex domain. The optimality of the solution is then\nchecked via our novel fast certifier. The extensive conducted experiments\ndemonstrate that, despite its simplicity, this certifiable solver performs\nexcellently on synthetic data, repeatedly attaining the (certified \\textit{a\nposteriori}) optimal solution and shows a satisfactory performance on real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:26:04 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 09:45:53 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Garcia-Salguero", "Mercedes", ""], ["Briales", "Jesus", ""], ["Gonzalez-Jimenez", "Javier", ""]]}, {"id": "2003.13736", "submitter": "Carl Leake", "authors": "Carl Leake, David Arnas and Daniele Mortari", "title": "Non-dimensional Star-Identification", "comments": "17 pages, 10 figures, 4 tables", "journal-ref": "Sensors. 2020; 20(9):2697", "doi": "10.3390/s20092697", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces a new \"Non-Dimensional\" star identification algorithm\nto reliably identify the stars observed by a wide field-of-view star tracker\nwhen the focal length and optical axis offset values are known with poor\naccuracy. This algorithm is particularly suited to complement nominal\nlost-in-space algorithms, which may identify stars incorrectly when the focal\nlength and/or optical axis offset deviate from their nominal operational\nranges. These deviations may be caused, for example, by launch vibrations or\nthermal variations in orbit. The algorithm performance is compared in terms of\naccuracy, speed, and robustness to the Pyramid algorithm. These comparisons\nhighlight the clear advantages that a combined approach of these methodologies\nprovides.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:33:20 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 15:32:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Leake", "Carl", ""], ["Arnas", "David", ""], ["Mortari", "Daniele", ""]]}, {"id": "2003.13743", "submitter": "Manchen Wang", "authors": "Manchen Wang, Joseph Tighe, Davide Modolo", "title": "Combining detection and tracking for human pose estimation in videos", "comments": "Accepted to CVPR 2020 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel top-down approach that tackles the problem of multi-person\nhuman pose estimation and tracking in videos. In contrast to existing top-down\napproaches, our method is not limited by the performance of its person detector\nand can predict the poses of person instances not localized. It achieves this\ncapability by propagating known person locations forward and backward in time\nand searching for poses in those regions. Our approach consists of three\ncomponents: (i) a Clip Tracking Network that performs body joint detection and\ntracking simultaneously on small video clips; (ii) a Video Tracking Pipeline\nthat merges the fixed-length tracklets produced by the Clip Tracking Network to\narbitrary length tracks; and (iii) a Spatial-Temporal Merging procedure that\nrefines the joint locations based on spatial and temporal smoothing terms.\nThanks to the precision of our Clip Tracking Network and our merging procedure,\nour approach produces very accurate joint predictions and can fix common\nmistakes on hard scenarios like heavily entangled people. Our approach achieves\nstate-of-the-art results on both joint detection and tracking, on both the\nPoseTrack 2017 and 2018 datasets, and against all top-down and bottom-down\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 18:45:31 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wang", "Manchen", ""], ["Tighe", "Joseph", ""], ["Modolo", "Davide", ""]]}, {"id": "2003.13759", "submitter": "Davide Modolo", "authors": "Davide Modolo, Bing Shuai, Rahul Rama Varior, Joseph Tighe", "title": "Understanding the impact of mistakes on background regions in crowd\n  counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every crowd counting researcher has likely observed their model output wrong\npositive predictions on image regions not containing any person. But how often\ndo these mistakes happen? Are our models negatively affected by this? In this\npaper we analyze this problem in depth. In order to understand its magnitude,\nwe present an extensive analysis on five of the most important crowd counting\ndatasets. We present this analysis in two parts. First, we quantify the number\nof mistakes made by popular crowd counting approaches. Our results show that\n(i) mistakes on background are substantial and they are responsible for 18-49%\nof the total error, (ii) models do not generalize well to different kinds of\nbackgrounds and perform poorly on completely background images, and (iii)\nmodels make many more mistakes than those captured by the standard Mean\nAbsolute Error (MAE) metric, as counting on background compensates considerably\nfor misses on foreground. And second, we quantify the performance change gained\nby helping the model better deal with this problem. We enrich a typical crowd\ncounting network with a segmentation branch trained to suppress background\npredictions. This simple addition (i) reduces background error by 10-83%, (ii)\nreduces foreground error by up to 26% and (iii) improves overall crowd counting\nperformance up to 20%. When compared against the literature, this simple\ntechnique achieves very competitive results on all datasets, on par with the\nstate-of-the-art, showing the importance of tackling the background problem.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:16:18 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Modolo", "Davide", ""], ["Shuai", "Bing", ""], ["Varior", "Rahul Rama", ""], ["Tighe", "Joseph", ""]]}, {"id": "2003.13764", "submitter": "Anil Armagan", "authors": "Anil Armagan, Guillermo Garcia-Hernando, Seungryul Baek, Shreyas\n  Hampali, Mahdi Rad, Zhaohui Zhang, Shipeng Xie, MingXiu Chen, Boshen Zhang,\n  Fu Xiong, Yang Xiao, Zhiguo Cao, Junsong Yuan, Pengfei Ren, Weiting Huang,\n  Haifeng Sun, Marek Hr\\'uz, Jakub Kanis, Zden\\v{e}k Kr\\v{n}oul, Qingfu Wan,\n  Shile Li, Linlin Yang, Dongheui Lee, Angela Yao, Weiguo Zhou, Sijia Mei,\n  Yunhui Liu, Adrian Spurr, Umar Iqbal, Pavlo Molchanov, Philippe Weinzaepfel,\n  Romain Br\\'egier, Gr\\'egory Rogez, Vincent Lepetit, Tae-Kyun Kim", "title": "Measuring Generalisation to Unseen Viewpoints, Articulations, Shapes and\n  Objects for 3D Hand Pose Estimation under Hand-Object Interaction", "comments": "European Conference on Computer Vision (ECCV), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how well different types of approaches generalise in the task of 3D\nhand pose estimation under single hand scenarios and hand-object interaction.\nWe show that the accuracy of state-of-the-art methods can drop, and that they\nfail mostly on poses absent from the training set. Unfortunately, since the\nspace of hand poses is highly dimensional, it is inherently not feasible to\ncover the whole space densely, despite recent efforts in collecting large-scale\ntraining datasets. This sampling problem is even more severe when hands are\ninteracting with objects and/or inputs are RGB rather than depth images, as RGB\nimages also vary with lighting conditions and colors. To address these issues,\nwe designed a public challenge (HANDS'19) to evaluate the abilities of current\n3D hand pose estimators (HPEs) to interpolate and extrapolate the poses of a\ntraining set. More exactly, HANDS'19 is designed (a) to evaluate the influence\nof both depth and color modalities on 3D hand pose estimation, under the\npresence or absence of objects; (b) to assess the generalisation abilities\nw.r.t. four main axes: shapes, articulations, viewpoints, and objects; (c) to\nexplore the use of a synthetic hand model to fill the gaps of current datasets.\nThrough the challenge, the overall accuracy has dramatically improved over the\nbaseline, especially on extrapolation tasks, from 27mm to 13mm mean joint\nerror. Our analyses highlight the impacts of: Data pre-processing, ensemble\napproaches, the use of a parametric 3D hand model (MANO), and different HPE\nmethods/backbones.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 19:28:13 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 15:35:17 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Armagan", "Anil", ""], ["Garcia-Hernando", "Guillermo", ""], ["Baek", "Seungryul", ""], ["Hampali", "Shreyas", ""], ["Rad", "Mahdi", ""], ["Zhang", "Zhaohui", ""], ["Xie", "Shipeng", ""], ["Chen", "MingXiu", ""], ["Zhang", "Boshen", ""], ["Xiong", "Fu", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""], ["Yuan", "Junsong", ""], ["Ren", "Pengfei", ""], ["Huang", "Weiting", ""], ["Sun", "Haifeng", ""], ["Hr\u00faz", "Marek", ""], ["Kanis", "Jakub", ""], ["Kr\u0148oul", "Zden\u011bk", ""], ["Wan", "Qingfu", ""], ["Li", "Shile", ""], ["Yang", "Linlin", ""], ["Lee", "Dongheui", ""], ["Yao", "Angela", ""], ["Zhou", "Weiguo", ""], ["Mei", "Sijia", ""], ["Liu", "Yunhui", ""], ["Spurr", "Adrian", ""], ["Iqbal", "Umar", ""], ["Molchanov", "Pavlo", ""], ["Weinzaepfel", "Philippe", ""], ["Br\u00e9gier", "Romain", ""], ["Rogez", "Gr\u00e9gory", ""], ["Lepetit", "Vincent", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "2003.13791", "submitter": "Dong Cao", "authors": "Dong Cao, Xiangyu Zhu, Xingyu Huang, Jianzhu Guo, Zhen Lei", "title": "Domain Balancing: Face Recognition on Long-Tailed Domains", "comments": "Accepted to CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-tailed problem has been an important topic in face recognition task.\nHowever, existing methods only concentrate on the long-tailed distribution of\nclasses. Differently, we devote to the long-tailed domain distribution problem,\nwhich refers to the fact that a small number of domains frequently appear while\nother domains far less existing. The key challenge of the problem is that\ndomain labels are too complicated (related to race, age, pose, illumination,\netc.) and inaccessible in real applications. In this paper, we propose a novel\nDomain Balancing (DB) mechanism to handle this problem. Specifically, we first\npropose a Domain Frequency Indicator (DFI) to judge whether a sample is from\nhead domains or tail domains. Secondly, we formulate a light-weighted Residual\nBalancing Mapping (RBM) block to balance the domain distribution by adjusting\nthe network according to DFI. Finally, we propose a Domain Balancing Margin\n(DBM) in the loss function to further optimize the feature space of the tail\ndomains to improve generalization. Extensive analysis and experiments on\nseveral face recognition benchmarks demonstrate that the proposed method\neffectively enhances the generalization capacities and achieves superior\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 20:16:31 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Cao", "Dong", ""], ["Zhu", "Xiangyu", ""], ["Huang", "Xingyu", ""], ["Guo", "Jianzhu", ""], ["Lei", "Zhen", ""]]}, {"id": "2003.13815", "submitter": "Mohammed Abdelsamea", "authors": "Asmaa Abbas, Mohammed M. Abdelsamea, Mohamed Medhat Gaber", "title": "Classification of COVID-19 in chest X-ray images using DeTraC deep\n  convolutional neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-ray is the first imaging technique that plays an important role in\nthe diagnosis of COVID-19 disease. Due to the high availability of large-scale\nannotated image datasets, great success has been achieved using convolutional\nneural networks (CNNs) for image recognition and classification. However, due\nto the limited availability of annotated medical images, the classification of\nmedical images remains the biggest challenge in medical diagnosis. Thanks to\ntransfer learning, an effective mechanism that can provide a promising solution\nby transferring knowledge from generic object recognition tasks to\ndomain-specific tasks. In this paper, we validate and adapt our previously\ndeveloped CNN, called Decompose, Transfer, and Compose (DeTraC), for the\nclassification of COVID-19 chest X-ray images. DeTraC can deal with any\nirregularities in the image dataset by investigating its class boundaries using\na class decomposition mechanism. The experimental results showed the capability\nof DeTraC in the detection of COVID-19 cases from a comprehensive image dataset\ncollected from several hospitals around the world. High accuracy of 95.12%\n(with a sensitivity of 97.91%, a specificity of 91.87%, and a precision of\n93.36%) was achieved by DeTraC in the detection of COVID-19 X-ray images from\nnormal, and severe acute respiratory syndrome cases.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 15:18:45 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 08:54:31 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 12:02:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Abbas", "Asmaa", ""], ["Abdelsamea", "Mohammed M.", ""], ["Gaber", "Mohamed Medhat", ""]]}, {"id": "2003.13820", "submitter": "Nathaniel Chodosh", "authors": "Nathaniel Chodosh, Simon Lucey", "title": "When to Use Convolutional Neural Networks for Inverse Problems", "comments": "CVPR 2020 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction tasks in computer vision aim fundamentally to recover an\nundetermined signal from a set of noisy measurements. Examples include\nsuper-resolution, image denoising, and non-rigid structure from motion, all of\nwhich have seen recent advancements through deep learning. However, earlier\nwork made extensive use of sparse signal reconstruction frameworks (e.g\nconvolutional sparse coding). While this work was ultimately surpassed by deep\nlearning, it rested on a much more developed theoretical framework. Recent work\nby Papyan et. al provides a bridge between the two approaches by showing how a\nconvolutional neural network (CNN) can be viewed as an approximate solution to\na convolutional sparse coding (CSC) problem. In this work we argue that for\nsome types of inverse problems the CNN approximation breaks down leading to\npoor performance. We argue that for these types of problems the CSC approach\nshould be used instead and validate this argument with empirical evidence.\nSpecifically we identify JPEG artifact reduction and non-rigid trajectory\nreconstruction as challenging inverse problems for CNNs and demonstrate state\nof the art performance on them using a CSC method. Furthermore, we offer some\npractical improvements to this model and its application, and also show how\ninsights from the CSC model can be used to make CNNs effective in tasks where\ntheir naive application fails.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:08:14 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chodosh", "Nathaniel", ""], ["Lucey", "Simon", ""]]}, {"id": "2003.13827", "submitter": "Juan I. Forcen", "authors": "J.I.Forcen, Miguel Pagola, Edurne Barrenechea and Humberto Bustince", "title": "Co-occurrence of deep convolutional features for image search", "comments": null, "journal-ref": null, "doi": "10.1016/j.imavis.2020.103909", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image search can be tackled using deep features from pre-trained\nConvolutional Neural Networks (CNN). The feature map from the last\nconvolutional layer of a CNN encodes descriptive information from which a\ndiscriminative global descriptor can be obtained. We propose a new\nrepresentation of co-occurrences from deep convolutional features to extract\nadditional relevant information from this last convolutional layer. Combining\nthis co-occurrence map with the feature map, we achieve an improved image\nrepresentation. We present two different methods to get the co-occurrence\nrepresentation, the first one based on direct aggregation of activations, and\nthe second one, based on a trainable co-occurrence representation. The image\ndescriptors derived from our methodology improve the performance in very\nwell-known image retrieval datasets as we prove in the experiments.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:27:22 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 16:47:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Forcen", "J. I.", ""], ["Pagola", "Miguel", ""], ["Barrenechea", "Edurne", ""], ["Bustince", "Humberto", ""]]}, {"id": "2003.13830", "submitter": "Necati Cihan Camgoz", "authors": "Necati Cihan Camgoz, Oscar Koller, Simon Hadfield, Richard Bowden", "title": "Sign Language Transformers: Joint End-to-end Sign Language Recognition\n  and Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work on Sign Language Translation has shown that having a mid-level\nsign gloss representation (effectively recognizing the individual signs)\nimproves the translation performance drastically. In fact, the current\nstate-of-the-art in translation requires gloss level tokenization in order to\nwork. We introduce a novel transformer based architecture that jointly learns\nContinuous Sign Language Recognition and Translation while being trainable in\nan end-to-end manner. This is achieved by using a Connectionist Temporal\nClassification (CTC) loss to bind the recognition and translation problems into\na single unified architecture. This joint approach does not require any\nground-truth timing information, simultaneously solving two co-dependant\nsequence-to-sequence learning problems and leads to significant performance\ngains.\n  We evaluate the recognition and translation performances of our approaches on\nthe challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report\nstate-of-the-art sign language recognition and translation results achieved by\nour Sign Language Transformers. Our translation networks outperform both sign\nvideo to spoken language and gloss to spoken language translation models, in\nsome cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We\nalso share new baseline translation results using transformer networks for\nseveral other text-to-text sign language translation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:35:09 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Camgoz", "Necati Cihan", ""], ["Koller", "Oscar", ""], ["Hadfield", "Simon", ""], ["Bowden", "Richard", ""]]}, {"id": "2003.13834", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Aruni RoyChowdhury, Gopal Sharma, Evangelos\n  Kalogerakis, Liangliang Cao, Erik Learned-Miller, Rui Wang, Subhransu Maji", "title": "Label-Efficient Learning on Point Clouds using Approximate Convex\n  Decompositions", "comments": "First two authors had equal contribution. ECCV'20 version. 19 pages,\n  5 figures", "journal-ref": "16th European Conference on Computer Vision (ECCV 2020)", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problems of shape classification and part segmentation from 3D point\nclouds have garnered increasing attention in the last few years. Both of these\nproblems, however, suffer from relatively small training sets, creating the\nneed for statistically efficient methods to learn 3D shape representations. In\nthis paper, we investigate the use of Approximate Convex Decompositions (ACD)\nas a self-supervisory signal for label-efficient learning of point cloud\nrepresentations. We show that using ACD to approximate ground truth\nsegmentation provides excellent self-supervision for learning 3D point cloud\nrepresentations that are highly effective on downstream tasks. We report\nimprovements over the state-of-the-art for unsupervised representation learning\non the ModelNet40 shape classification dataset and significant gains in\nfew-shot part segmentation on the ShapeNetPart dataset.Code available at\nhttps://github.com/matheusgadelha/PointCloudLearningACD\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 21:44:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 21:01:08 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Gadelha", "Matheus", ""], ["RoyChowdhury", "Aruni", ""], ["Sharma", "Gopal", ""], ["Kalogerakis", "Evangelos", ""], ["Cao", "Liangliang", ""], ["Learned-Miller", "Erik", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "2003.13840", "submitter": "Marian Petruk", "authors": "Ivan Kosarevych, Marian Petruk, Markian Kostiv, Orest Kupyn, Mykola\n  Maksymenko, Volodymyr Budzan", "title": "ActGAN: Flexible and Efficient One-shot Face Reenactment", "comments": "accepted by IWBF2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces ActGAN - a novel end-to-end generative adversarial\nnetwork (GAN) for one-shot face reenactment. Given two images, the goal is to\ntransfer the facial expression of the source actor onto a target person in a\nphoto-realistic fashion. While existing methods require target identity to be\npredefined, we address this problem by introducing a \"many-to-many\" approach,\nwhich allows arbitrary persons both for source and target without additional\nretraining. To this end, we employ the Feature Pyramid Network (FPN) as a core\ngenerator building block - the first application of FPN in face reenactment,\nproducing finer results. We also introduce a solution to preserve a person's\nidentity between synthesized and target person by adopting the state-of-the-art\napproach in deep face recognition domain. The architecture readily supports\nreenactment in different scenarios: \"many-to-many\", \"one-to-one\",\n\"one-to-another\" in terms of expression accuracy, identity preservation, and\noverall image quality. We demonstrate that ActGAN achieves competitive\nperformance against recent works concerning visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:03:16 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Kosarevych", "Ivan", ""], ["Petruk", "Marian", ""], ["Kostiv", "Markian", ""], ["Kupyn", "Orest", ""], ["Maksymenko", "Mykola", ""], ["Budzan", "Volodymyr", ""]]}, {"id": "2003.13845", "submitter": "Alexandros Lattas", "authors": "Alexandros Lattas, Stylianos Moschoglou, Baris Gecer, Stylianos\n  Ploumpis, Vasileios Triantafyllou, Abhijeet Ghosh, Stefanos Zafeiriou", "title": "AvatarMe: Realistically Renderable 3D Facial Reconstruction\n  \"in-the-wild\"", "comments": "Accepted to CVPR2020. Project page: github.com/lattas/AvatarMe with\n  high resolution results, data and more. 10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, with the advent of Generative Adversarial Networks\n(GANs), many face analysis tasks have accomplished astounding performance, with\napplications including, but not limited to, face generation and 3D face\nreconstruction from a single \"in-the-wild\" image. Nevertheless, to the best of\nour knowledge, there is no method which can produce high-resolution\nphotorealistic 3D faces from \"in-the-wild\" images and this can be attributed to\nthe: (a) scarcity of available data for training, and (b) lack of robust\nmethodologies that can successfully be applied on very high-resolution data. In\nthis paper, we introduce AvatarMe, the first method that is able to reconstruct\nphotorealistic 3D faces from a single \"in-the-wild\" image with an increasing\nlevel of detail. To achieve this, we capture a large dataset of facial shape\nand reflectance and build on a state-of-the-art 3D texture and shape\nreconstruction method and successively refine its results, while generating the\nper-pixel diffuse and specular components that are required for realistic\nrendering. As we demonstrate in a series of qualitative and quantitative\nexperiments, AvatarMe outperforms the existing arts by a significant margin and\nreconstructs authentic, 4K by 6K-resolution 3D faces from a single\nlow-resolution image that, for the first time, bridges the uncanny valley.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:17:54 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lattas", "Alexandros", ""], ["Moschoglou", "Stylianos", ""], ["Gecer", "Baris", ""], ["Ploumpis", "Stylianos", ""], ["Triantafyllou", "Vasileios", ""], ["Ghosh", "Abhijeet", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2003.13852", "submitter": "Vincent Jacquot", "authors": "Vincent Jacquot, Zhuofan Ying, Gabriel Kreiman", "title": "Can Deep Learning Recognize Subtle Human Activities?", "comments": "poster at CVPR 2020, includes supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has driven recent and exciting progress in computer vision,\ninstilling the belief that these algorithms could solve any visual task. Yet,\ndatasets commonly used to train and test computer vision algorithms have\npervasive confounding factors. Such biases make it difficult to truly estimate\nthe performance of those algorithms and how well computer vision models can\nextrapolate outside the distribution in which they were trained. In this work,\nwe propose a new action classification challenge that is performed well by\nhumans, but poorly by state-of-the-art Deep Learning models. As a\nproof-of-principle, we consider three exemplary tasks: drinking, reading, and\nsitting. The best accuracies reached using state-of-the-art computer vision\nmodels were 61.7%, 62.8%, and 76.8%, respectively, while human participants\nscored above 90% accuracy on the three tasks. We propose a rigorous method to\nreduce confounds when creating datasets, and when comparing human versus\ncomputer vision performance. Source code and datasets are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:45:43 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Jacquot", "Vincent", ""], ["Ying", "Zhuofan", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2003.13853", "submitter": "Yaxing Wang", "authors": "Yaxing Wang, Salman Khan, Abel Gonzalez-Garcia, Joost van de Weijer,\n  Fahad Shahbaz Khan", "title": "Semi-supervised Learning for Few-shot Image-to-Image Translation", "comments": "CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, unpaired image-to-image translation has witnessed\nremarkable progress. Although the latest methods are able to generate realistic\nimages, they crucially rely on a large number of labeled images. Recently, some\nmethods have tackled the challenging setting of few-shot image-to-image\ntranslation, reducing the labeled data requirements for the target domain\nduring inference. In this work, we go one step further and reduce the amount of\nrequired labeled data also from the source domain during training. To do so, we\npropose applying semi-supervised learning via a noise-tolerant pseudo-labeling\nprocedure. We also apply a cycle consistency constraint to further exploit the\ninformation from unlabeled images, either from the same dataset or external.\nAdditionally, we propose several structural modifications to facilitate the\nimage translation task under these circumstances. Our semi-supervised method\nfor few-shot image translation, called SEMIT, achieves excellent results on\nfour different datasets using as little as 10% of the source labels, and\nmatches the performance of the main fully-supervised competitor using only 20%\nlabeled data. Our code and models are made public at:\nhttps://github.com/yaxingwang/SEMIT.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:46:49 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 09:09:19 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Wang", "Yaxing", ""], ["Khan", "Salman", ""], ["Gonzalez-Garcia", "Abel", ""], ["van de Weijer", "Joost", ""], ["Khan", "Fahad Shahbaz", ""]]}, {"id": "2003.13865", "submitter": "Pengtao Xie", "authors": "Xingyi Yang, Xuehai He, Jinyu Zhao, Yichen Zhang, Shanghang Zhang,\n  Pengtao Xie", "title": "COVID-CT-Dataset: A CT Scan Dataset about COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the outbreak time of COVID-19, computed tomography (CT) is a useful\nmanner for diagnosing COVID-19 patients. Due to privacy issues, publicly\navailable COVID-19 CT datasets are highly difficult to obtain, which hinders\nthe research and development of AI-powered diagnosis methods of COVID-19 based\non CTs. To address this issue, we build an open-sourced dataset -- COVID-CT,\nwhich contains 349 COVID-19 CT images from 216 patients and 463 non-COVID-19\nCTs. The utility of this dataset is confirmed by a senior radiologist who has\nbeen diagnosing and treating COVID-19 patients since the outbreak of this\npandemic. We also perform experimental studies which further demonstrate that\nthis dataset is useful for developing AI-based diagnosis models of COVID-19.\nUsing this dataset, we develop diagnosis methods based on multi-task learning\nand self-supervised learning, that achieve an F1 of 0.90, an AUC of 0.98, and\nan accuracy of 0.89. According to the senior radiologist, models with such\nperformance are good enough for clinical usage. The data and code are available\nat https://github.com/UCSD-AI4H/COVID-CT\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:27:24 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 02:08:25 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 20:14:22 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yang", "Xingyi", ""], ["He", "Xuehai", ""], ["Zhao", "Jinyu", ""], ["Zhang", "Yichen", ""], ["Zhang", "Shanghang", ""], ["Xie", "Pengtao", ""]]}, {"id": "2003.13866", "submitter": "Calvin Murdock", "authors": "Calvin Murdock, Simon Lucey", "title": "Dataless Model Selection with the Deep Frame Potential", "comments": "Oral presentation at the Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a deep neural network architecture is a fundamental problem in\napplications that require balancing performance and parameter efficiency.\nStandard approaches rely on ad-hoc engineering or computationally expensive\nvalidation on a specific dataset. We instead attempt to quantify networks by\ntheir intrinsic capacity for unique and robust representations, enabling\nefficient architecture comparisons without requiring any data. Building upon\ntheoretical connections between deep learning and sparse approximation, we\npropose the deep frame potential: a measure of coherence that is approximately\nrelated to representation stability but has minimizers that depend only on\nnetwork structure. This provides a framework for jointly quantifying the\ncontributions of architectural hyper-parameters such as depth, width, and skip\nconnections. We validate its use as a criterion for model selection and\ndemonstrate correlation with generalization error on a variety of common\nresidual and densely connected network architectures.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:27:25 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Murdock", "Calvin", ""], ["Lucey", "Simon", ""]]}, {"id": "2003.13867", "submitter": "Francis Engelmann", "authors": "Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe,\n  Matthias Nie{\\ss}ner", "title": "3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation", "comments": "CVPR2020, Video: https://youtu.be/ifL8yTbRFDk Project Page:\n  https://www.vision.rwth-aachen.de/3d_instance_segmentation/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3D-MPA, a method for instance segmentation on 3D point clouds.\nGiven an input point cloud, we propose an object-centric approach where each\npoint votes for its object center. We sample object proposals from the\npredicted object centers. Then, we learn proposal features from grouped point\nfeatures that voted for the same object center. A graph convolutional network\nintroduces inter-proposal relations, providing higher-level feature learning in\naddition to the lower-level point features. Each proposal comprises a semantic\nlabel, a set of associated points over which we define a foreground-background\nmask, an objectness score and aggregation features. Previous works usually\nperform non-maximum-suppression (NMS) over proposals to obtain the final object\ndetections or semantic instances. However, NMS can discard potentially correct\npredictions. Instead, our approach keeps all proposals and groups them together\nbased on the learned aggregation features. We show that grouping proposals\nimproves over NMS and outperforms previous state-of-the-art methods on the\ntasks of 3D object detection and semantic instance segmentation on the\nScanNetV2 benchmark and the S3DIS dataset.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:28:50 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Engelmann", "Francis", ""], ["Bokeloh", "Martin", ""], ["Fathi", "Alireza", ""], ["Leibe", "Bastian", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "2003.13868", "submitter": "Manohar Karki", "authors": "Manohar Karki, Junghwan Cho, Seokhwan Ko", "title": "Lesion Conditional Image Generation for Improved Segmentation of\n  Intracranial Hemorrhage from CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data augmentation can effectively resolve a scarcity of images when training\nmachine-learning algorithms. It can make them more robust to unseen images. We\npresent a lesion conditional Generative Adversarial Network LcGAN to generate\nsynthetic Computed Tomography (CT) images for data augmentation. A lesion\nconditional image (segmented mask) is an input to both the generator and the\ndiscriminator of the LcGAN during training. The trained model generates\ncontextual CT images based on input masks. We quantify the quality of the\nimages by using a fully convolutional network (FCN) score and blurriness. We\nalso train another classification network to select better synthetic images.\nThese synthetic CT images are then augmented to our hemorrhagic lesion\nsegmentation network. By applying this augmentation method on 2.5%, 10% and 25%\nof original data, segmentation improved by 12.8%, 6% and 1.6% respectively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:32:54 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Karki", "Manohar", ""], ["Cho", "Junghwan", ""], ["Ko", "Seokhwan", ""]]}, {"id": "2003.13870", "submitter": "Jonathan Huang", "authors": "Zhichao Lu, Vivek Rathod, Ronny Votel, Jonathan Huang", "title": "RetinaTrack: Online Single Stage Joint Detection and Tracking", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally multi-object tracking and object detection are performed using\nseparate systems with most prior works focusing exclusively on one of these\naspects over the other. Tracking systems clearly benefit from having access to\naccurate detections, however and there is ample evidence in literature that\ndetectors can benefit from tracking which, for example, can help to smooth\npredictions over time. In this paper we focus on the tracking-by-detection\nparadigm for autonomous driving where both tasks are mission critical. We\npropose a conceptually simple and efficient joint model of detection and\ntracking, called RetinaTrack, which modifies the popular single stage RetinaNet\napproach such that it is amenable to instance-level embedding training. We\nshow, via evaluations on the Waymo Open Dataset, that we outperform a recent\nstate of the art tracking algorithm while requiring significantly less\ncomputation. We believe that our simple yet effective approach can serve as a\nstrong baseline for future work in this area.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:46:29 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Lu", "Zhichao", ""], ["Rathod", "Vivek", ""], ["Votel", "Ronny", ""], ["Huang", "Jonathan", ""]]}, {"id": "2003.13874", "submitter": "Zitao Chen", "authors": "Zitao Chen, Guanpeng Li and Karthik Pattabiraman", "title": "A Low-cost Fault Corrector for Deep Neural Networks through Range\n  Restriction", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of deep neural networks (DNNs) in safety-critical domains has\nengendered serious reliability concerns. A prominent example is hardware\ntransient faults that are growing in frequency due to the progressive\ntechnology scaling, and can lead to failures in DNNs.\n  This work proposes Ranger, a low-cost fault corrector, which directly\nrectifies the faulty output due to transient faults without re-computation.\nDNNs are inherently resilient to benign faults (which will not cause output\ncorruption), but not to critical faults (which can result in erroneous output).\nRanger is an automated transformation to selectively restrict the value ranges\nin DNNs, which reduces the large deviations caused by critical faults and\ntransforms them to benign faults that can be tolerated by the inherent\nresilience of the DNNs. Our evaluation on 8 DNNs demonstrates Ranger\nsignificantly increases the error resilience of the DNNs (by 3x to 50x), with\nno loss in accuracy, and with negligible overheads.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:53:55 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 08:07:30 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 08:23:57 GMT"}, {"version": "v4", "created": "Mon, 29 Mar 2021 01:47:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Zitao", ""], ["Li", "Guanpeng", ""], ["Pattabiraman", "Karthik", ""]]}, {"id": "2003.13880", "submitter": "Vishnu Naresh Boddeti", "authors": "Zhichao Lu and Kalyanmoy Deb and Vishnu Naresh Boddeti", "title": "MUXConv: Information Multiplexing in Convolutional Neural Networks", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have witnessed remarkable improvements in\ncomputational efficiency in recent years. A key driving force has been the idea\nof trading-off model expressivity and efficiency through a combination of\n$1\\times 1$ and depth-wise separable convolutions in lieu of a standard\nconvolutional layer. The price of the efficiency, however, is the sub-optimal\nflow of information across space and channels in the network. To overcome this\nlimitation, we present MUXConv, a layer that is designed to increase the flow\nof information by progressively multiplexing channel and spatial information in\nthe network, while mitigating computational complexity. Furthermore, to\ndemonstrate the effectiveness of MUXConv, we integrate it within an efficient\nmulti-objective evolutionary algorithm to search for the optimal model\nhyper-parameters while simultaneously optimizing accuracy, compactness, and\ncomputational efficiency. On ImageNet, the resulting models, dubbed MUXNets,\nmatch the performance (75.3% top-1 accuracy) and multiply-add operations (218M)\nof MobileNetV3 while being 1.6$\\times$ more compact, and outperform other\nmobile models in all the three criteria. MUXNet also performs well under\ntransfer learning and when adapted to object detection. On the ChestX-Ray 14\nbenchmark, its accuracy is comparable to the state-of-the-art while being\n$3.3\\times$ more compact and $14\\times$ more efficient. Similarly, detection on\nPASCAL VOC 2007 is 1.2% more accurate, 28% faster and 6% more compact compared\nto MobileNetV2. Code is available from\nhttps://github.com/human-analysis/MUXConv\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:09:47 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 17:27:20 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Lu", "Zhichao", ""], ["Deb", "Kalyanmoy", ""], ["Boddeti", "Vishnu Naresh", ""]]}, {"id": "2003.13886", "submitter": "Srikanth Malla", "authors": "Srikanth Malla and Behzad Dariush and Chiho Choi", "title": "TITAN: Future Forecast using Action Priors", "comments": "CVPR 2020 [oral], dataset url: https://usa.honda-ri.com/titan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of predicting the future trajectory of scene agents\nfrom egocentric views obtained from a moving platform. This problem is\nimportant in a variety of domains, particularly for autonomous systems making\nreactive or strategic decisions in navigation. In an attempt to address this\nproblem, we introduce TITAN (Trajectory Inference using Targeted Action priors\nNetwork), a new model that incorporates prior positions, actions, and context\nto forecast future trajectory of agents and future ego-motion. In the absence\nof an appropriate dataset for this task, we created the TITAN dataset that\nconsists of 700 labeled video-clips (with odometry) captured from a moving\nvehicle on highly interactive urban traffic scenes in Tokyo. Our dataset\nincludes 50 labels including vehicle states and actions, pedestrian age groups,\nand targeted pedestrian action attributes that are organized hierarchically\ncorresponding to atomic, simple/complex-contextual, transportive, and\ncommunicative actions. To evaluate our model, we conducted extensive\nexperiments on the TITAN dataset, revealing significant performance improvement\nagainst baselines and state-of-the-art algorithms. We also report promising\nresults from our Agent Importance Mechanism (AIM), a module which provides\ninsight into assessment of perceived risk by calculating the relative influence\nof each agent on the future ego-trajectory. The dataset is available at\nhttps://usa.honda-ri.com/titan\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 00:32:12 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 04:01:05 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 20:29:16 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Malla", "Srikanth", ""], ["Dariush", "Behzad", ""], ["Choi", "Chiho", ""]]}, {"id": "2003.13898", "submitter": "Hao Tang", "authors": "Hao Tang, Xiaojuan Qi, Dan Xu, Philip H. S. Torr, Nicu Sebe", "title": "Edge Guided GANs with Semantic Preserving for Semantic Image Synthesis", "comments": "40 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Edge guided Generative Adversarial Network (EdgeGAN) for\nphoto-realistic image synthesis from semantic layouts. Although considerable\nimprovement has been achieved, the quality of synthesized images is far from\nsatisfactory due to two largely unresolved challenges. First, the semantic\nlabels do not provide detailed structural information, making it difficult to\nsynthesize local details and structures. Second, the widely adopted CNN\noperations such as convolution, down-sampling and normalization usually cause\nspatial resolution loss and thus are unable to fully preserve the original\nsemantic information, leading to semantically inconsistent results (e.g.,\nmissing small objects). To tackle the first challenge, we propose to use the\nedge as an intermediate representation which is further adopted to guide image\ngeneration via a proposed attention guided edge transfer module. Edge\ninformation is produced by a convolutional generator and introduces detailed\nstructure information. Further, to preserve the semantic information, we design\nan effective module to selectively highlight class-dependent feature maps\naccording to the original semantic layout. Extensive experiments on two\nchallenging datasets show that the proposed EdgeGAN can generate significantly\nbetter results than state-of-the-art methods. The source code and trained\nmodels are available at https://github.com/Ha0Tang/EdgeGAN.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 01:23:21 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Tang", "Hao", ""], ["Qi", "Xiaojuan", ""], ["Xu", "Dan", ""], ["Torr", "Philip H. S.", ""], ["Sebe", "Nicu", ""]]}, {"id": "2003.13903", "submitter": "Changxing Ding", "authors": "Tong Zhou, Changxing Ding, Shaowen Lin, Xinchao Wang and Dacheng Tao", "title": "Learning Oracle Attention for High-fidelity Face Completion", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-fidelity face completion is a challenging task due to the rich and\nsubtle facial textures involved. What makes it more complicated is the\ncorrelations between different facial components, for example, the symmetry in\ntexture and structure between both eyes. While recent works adopted the\nattention mechanism to learn the contextual relations among elements of the\nface, they have largely overlooked the disastrous impacts of inaccurate\nattention scores; in addition, they fail to pay sufficient attention to key\nfacial components, the completion results of which largely determine the\nauthenticity of a face image. Accordingly, in this paper, we design a\ncomprehensive framework for face completion based on the U-Net structure.\nSpecifically, we propose a dual spatial attention module to efficiently learn\nthe correlations between facial textures at multiple scales; moreover, we\nprovide an oracle supervision signal to the attention module to ensure that the\nobtained attention scores are reasonable. Furthermore, we take the location of\nthe facial components as prior knowledge and impose a multi-discriminator on\nthese regions, with which the fidelity of facial components is significantly\npromoted. Extensive experiments on two high-resolution face datasets including\nCelebA-HQ and Flickr-Faces-HQ demonstrate that the proposed approach\noutperforms state-of-the-art methods by large margins.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 01:37:10 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhou", "Tong", ""], ["Ding", "Changxing", ""], ["Lin", "Shaowen", ""], ["Wang", "Xinchao", ""], ["Tao", "Dacheng", ""]]}, {"id": "2003.13910", "submitter": "Siqi Li", "authors": "Siqi Li, Changqing Zou, Yipeng Li, Xibin Zhao and Yue Gao", "title": "Attention-based Multi-modal Fusion Network for Semantic Scene Completion", "comments": "Accepted by AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end 3D convolutional network named\nattention-based multi-modal fusion network (AMFNet) for the semantic scene\ncompletion (SSC) task of inferring the occupancy and semantic labels of a\nvolumetric 3D scene from single-view RGB-D images. Compared with previous\nmethods which use only the semantic features extracted from RGB-D images, the\nproposed AMFNet learns to perform effective 3D scene completion and semantic\nsegmentation simultaneously via leveraging the experience of inferring 2D\nsemantic segmentation from RGB-D images as well as the reliable depth cues in\nspatial dimension. It is achieved by employing a multi-modal fusion\narchitecture boosted from 2D semantic segmentation and a 3D semantic completion\nnetwork empowered by residual attention blocks. We validate our method on both\nthe synthetic SUNCG-RGBD dataset and the real NYUv2 dataset and the results\nshow that our method respectively achieves the gains of 2.5% and 2.6% on the\nsynthetic SUNCG-RGBD dataset and the real NYUv2 dataset against the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:00:03 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2020 03:39:05 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Li", "Siqi", ""], ["Zou", "Changqing", ""], ["Li", "Yipeng", ""], ["Zhao", "Xibin", ""], ["Gao", "Yue", ""]]}, {"id": "2003.13911", "submitter": "Sungyeon Kim", "authors": "Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak", "title": "Proxy Anchor Loss for Deep Metric Learning", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing metric learning losses can be categorized into two classes:\npair-based and proxy-based losses. The former class can leverage fine-grained\nsemantic relations between data points, but slows convergence in general due to\nits high training complexity. In contrast, the latter class enables fast and\nreliable convergence, but cannot consider the rich data-to-data relations. This\npaper presents a new proxy-based loss that takes advantages of both pair- and\nproxy-based methods and overcomes their limitations. Thanks to the use of\nproxies, our loss boosts the speed of convergence and is robust against noisy\nlabels and outliers. At the same time, it allows embedding vectors of data to\ninteract with each other in its gradients to exploit data-to-data relations.\nOur method is evaluated on four public benchmarks, where a standard network\ntrained with our loss achieves state-of-the-art performance and most quickly\nconverges.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:05:27 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Kim", "Sungyeon", ""], ["Kim", "Dongwon", ""], ["Cho", "Minsu", ""], ["Kwak", "Suha", ""]]}, {"id": "2003.13912", "submitter": "C.-H. Huck Yang", "authors": "Hao-Hsiang Yang, Chao-Han Huck Yang, Yi-Chang James Tsai", "title": "Y-net: Multi-scale feature aggregation network with wavelet structure\n  similarity loss function for single image dehazing", "comments": "Accepted to IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image dehazing is the ill-posed two-dimensional signal reconstruction\nproblem. Recently, deep convolutional neural networks (CNN) have been\nsuccessfully used in many computer vision problems. In this paper, we propose a\nY-net that is named for its structure. This network reconstructs clear images\nby aggregating multi-scale features maps. Additionally, we propose a Wavelet\nStructure SIMilarity (W-SSIM) loss function in the training step. In the\nproposed loss function, discrete wavelet transforms are applied repeatedly to\ndivide the image into differently sized patches with different frequencies and\nscales. The proposed loss function is the accumulation of SSIM loss of various\npatches with respective ratios. Extensive experimental results demonstrate that\nthe proposed Y-net with the W-SSIM loss function restores high-quality clear\nimages and outperforms state-of-the-art algorithms. Code and models are\navailable at https://github.com/dectrfov/Y-net.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:07:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yang", "Hao-Hsiang", ""], ["Yang", "Chao-Han Huck", ""], ["Tsai", "Yi-Chang James", ""]]}, {"id": "2003.13924", "submitter": "Jiachen Li", "authors": "Jiachen Li and Fan Yang and Masayoshi Tomizuka and Chiho Choi", "title": "EvolveGraph: Multi-Agent Trajectory Prediction with Dynamic Relational\n  Reasoning", "comments": "NeurIPS 2020. Website:\n  https://jiachenli94.github.io/publications/Evolvegraph/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent interacting systems are prevalent in the world, from pure\nphysical systems to complicated social dynamic systems. In many applications,\neffective understanding of the situation and accurate trajectory prediction of\ninteractive agents play a significant role in downstream tasks, such as\ndecision making and planning. In this paper, we propose a generic trajectory\nforecasting framework (named EvolveGraph) with explicit relational structure\nrecognition and prediction via latent interaction graphs among multiple\nheterogeneous, interactive agents. Considering the uncertainty of future\nbehaviors, the model is designed to provide multi-modal prediction hypotheses.\nSince the underlying interactions may evolve even with abrupt changes, and\ndifferent modalities of evolution may lead to different outcomes, we address\nthe necessity of dynamic relational reasoning and adaptively evolving the\ninteraction graphs. We also introduce a double-stage training pipeline which\nnot only improves training efficiency and accelerates convergence, but also\nenhances model performance. The proposed framework is evaluated on both\nsynthetic physics simulations and multiple real-world benchmark datasets in\nvarious areas. The experimental results illustrate that our approach achieves\nstate-of-the-art performance in terms of prediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 02:49:23 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 07:28:20 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 17:42:10 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 16:02:44 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Li", "Jiachen", ""], ["Yang", "Fan", ""], ["Tomizuka", "Masayoshi", ""], ["Choi", "Chiho", ""]]}, {"id": "2003.13930", "submitter": "Shaochi Hu", "authors": "Shaochi Hu, Donghao Xu, Huijing Zhao", "title": "Cross Scene Prediction via Modeling Dynamic Correlation using Latent\n  Space Shared Auto-Encoders", "comments": "8 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses on the following problem: given a set of unsynchronized\nhistory observations of two scenes that are correlative on their dynamic\nchanges, the purpose is to learn a cross-scene predictor, so that with the\nobservation of one scene, a robot can onlinely predict the dynamic state of\nanother. A method is proposed to solve the problem via modeling dynamic\ncorrelation using latent space shared auto-encoders. Assuming that the inherent\ncorrelation of scene dynamics can be represented by shared latent space, where\na common latent state is reached if the observations of both scenes are at an\napproximate time, a learning model is developed by connecting two auto-encoders\nthrough the latent space, and a prediction model is built by concatenating the\nencoder of the input scene with the decoder of the target one. Simulation\ndatasets are generated imitating the dynamic flows at two adjacent gates of a\ncampus, where the dynamic changes are triggered by a common working and\nteaching schedule. Similar scenarios can also be found at successive\nintersections on a single road, gates of a subway station, etc. Accuracy of\ncross-scene prediction is examined at various conditions of scene correlation\nand pairwise observations. Potentials of the proposed method are demonstrated\nby comparing with conventional end-to-end methods and linear predictions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:08:23 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Hu", "Shaochi", ""], ["Xu", "Donghao", ""], ["Zhao", "Huijing", ""]]}, {"id": "2003.13942", "submitter": "Boxiao Pan", "authors": "Boxiao Pan, Haoye Cai, De-An Huang, Kuan-Hui Lee, Adrien Gaidon, Ehsan\n  Adeli, Juan Carlos Niebles", "title": "Spatio-Temporal Graph for Video Captioning with Knowledge Distillation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning is a challenging task that requires a deep understanding of\nvisual scenes. State-of-the-art methods generate captions using either\nscene-level or object-level information but without explicitly modeling object\ninteractions. Thus, they often fail to make visually grounded predictions, and\nare sensitive to spurious correlations. In this paper, we propose a novel\nspatio-temporal graph model for video captioning that exploits object\ninteractions in space and time. Our model builds interpretable links and is\nable to provide explicit visual grounding. To avoid unstable performance caused\nby the variable number of objects, we further propose an object-aware knowledge\ndistillation mechanism, in which local object information is used to regularize\nglobal scene features. We demonstrate the efficacy of our approach through\nextensive experiments on two benchmarks, showing our approach yields\ncompetitive performance with interpretable predictions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:58:11 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Pan", "Boxiao", ""], ["Cai", "Haoye", ""], ["Huang", "De-An", ""], ["Lee", "Kuan-Hui", ""], ["Gaidon", "Adrien", ""], ["Adeli", "Ehsan", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "2003.13947", "submitter": "Hongjoon Ahn", "authors": "Hongjoon Ahn, Jihwan Kwak, Subin Lim, Hyeonsu Bang, Hyojun Kim and\n  Taesup Moon", "title": "SS-IL: Separated Softmax for Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider class incremental learning (CIL) problem, in which a learning\nagent continuously learns new classes from incrementally arriving training data\nbatches and aims to predict well on all the classes learned so far. The main\nchallenge of the problem is the catastrophic forgetting, and for the\nexemplar-memory based CIL methods, it is generally known that the forgetting is\ncommonly caused by the prediction score bias that is injected due to the data\nimbalance between the new classes and the old classes (in the exemplar-memory).\nWhile several methods have been proposed to correct such score bias by some\nadditional post-processing, e.g., score re-scaling or balanced fine-tuning, no\nsystematic analysis on the root cause of such bias has been done. To that end,\nwe analyze that computing the softmax probabilities by combining the output\nscores for all old and new classes could be the main source of the bias and\npropose a new CIL method, Separated Softmax for Incremental Learning (SS-IL).\nOur SS-IL consists of separated softmax (SS) output layer and ratio-preserving\n(RP) mini-batches combined with task-wise knowledge distillation (TKD), and\nthrough extensive experimental results, we show our SS-IL achieves very strong\nstate-of-the-art accuracy on several large-scale benchmarks. We also show SS-IL\nmakes much more balanced prediction, without any additional post-processing\nsteps as is done in other baselines.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 04:36:59 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 06:18:29 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ahn", "Hongjoon", ""], ["Kwak", "Jihwan", ""], ["Lim", "Subin", ""], ["Bang", "Hyeonsu", ""], ["Kim", "Hyojun", ""], ["Moon", "Taesup", ""]]}, {"id": "2003.13948", "submitter": "Enze Xie", "authors": "Enze Xie, Wenjia Wang, Wenhai Wang, Mingyu Ding, Chunhua Shen, Ping\n  Luo", "title": "Segmenting Transparent Objects in the Wild", "comments": "ECCV2020 Accept", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparent objects such as windows and bottles made by glass widely exist in\nthe real world. Segmenting transparent objects is challenging because these\nobjects have diverse appearance inherited from the image background, making\nthem had similar appearance with their surroundings. Besides the technical\ndifficulty of this task, only a few previous datasets were specially designed\nand collected to explore this task and most of the existing datasets have major\ndrawbacks. They either possess limited sample size such as merely a thousand of\nimages without manual annotations, or they generate all images by using\ncomputer graphics method (i.e. not real image). To address this important\nproblem, this work proposes a large-scale dataset for transparent object\nsegmentation, named Trans10K, consisting of 10,428 images of real scenarios\nwith carefully manual annotations, which are 10 times larger than the existing\ndatasets. The transparent objects in Trans10K are extremely challenging due to\nhigh diversity in scale, viewpoint and occlusion as shown in Fig. 1. To\nevaluate the effectiveness of Trans10K, we propose a novel boundary-aware\nsegmentation method, termed TransLab, which exploits boundary as the clue to\nimprove segmentation of transparent objects. Extensive experiments and ablation\nstudies demonstrate the effectiveness of Trans10K and validate the practicality\nof learning object boundary in TransLab. For example, TransLab significantly\noutperforms 20 recent object segmentation methods based on deep learning,\nshowing that this task is largely unsolved. We believe that both Trans10K and\nTransLab have important contributions to both the academia and industry,\nfacilitating future researches and applications.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 04:44:31 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 06:19:36 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2020 03:32:48 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Xie", "Enze", ""], ["Wang", "Wenjia", ""], ["Wang", "Wenhai", ""], ["Ding", "Mingyu", ""], ["Shen", "Chunhua", ""], ["Luo", "Ping", ""]]}, {"id": "2003.13951", "submitter": "Adrian Johnston", "authors": "Adrian Johnston and Gustavo Carneiro", "title": "Self-supervised Monocular Trained Depth Estimation using Self-attention\n  and Discrete Disparity Volume", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular depth estimation has become one of the most studied applications in\ncomputer vision, where the most accurate approaches are based on fully\nsupervised learning models. However, the acquisition of accurate and large\nground truth data sets to model these fully supervised methods is a major\nchallenge for the further development of the area. Self-supervised methods\ntrained with monocular videos constitute one the most promising approaches to\nmitigate the challenge mentioned above due to the wide-spread availability of\ntraining data. Consequently, they have been intensively studied, where the main\nideas explored consist of different types of model architectures, loss\nfunctions, and occlusion masks to address non-rigid motion. In this paper, we\npropose two new ideas to improve self-supervised monocular trained depth\nestimation: 1) self-attention, and 2) discrete disparity prediction. Compared\nwith the usual localised convolution operation, self-attention can explore a\nmore general contextual information that allows the inference of similar\ndisparity values at non-contiguous regions of the image. Discrete disparity\nprediction has been shown by fully supervised methods to provide a more robust\nand sharper depth estimation than the more common continuous disparity\nprediction, besides enabling the estimation of depth uncertainty. We show that\nthe extension of the state-of-the-art self-supervised monocular trained depth\nestimator Monodepth2 with these two ideas allows us to design a model that\nproduces the best results in the field in KITTI 2015 and Make3D, closing the\ngap with respect self-supervised stereo training and fully supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 04:48:16 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Johnston", "Adrian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "2003.13954", "submitter": "Changxin Gao", "authors": "Zhibo Fan, Jin-Gang Yu, Zhihao Liang, Jiarong Ou, Changxin Gao,\n  Gui-Song Xia, Yuanqing Li", "title": "FGN: Fully Guided Network for Few-Shot Instance Segmentation", "comments": "Accepted by CVPR 2020, 10 pages, 6 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot instance segmentation (FSIS) conjoins the few-shot learning paradigm\nwith general instance segmentation, which provides a possible way of tackling\ninstance segmentation in the lack of abundant labeled data for training. This\npaper presents a Fully Guided Network (FGN) for few-shot instance segmentation.\nFGN perceives FSIS as a guided model where a so-called support set is encoded\nand utilized to guide the predictions of a base instance segmentation network\n(i.e., Mask R-CNN), critical to which is the guidance mechanism. In this view,\nFGN introduces different guidance mechanisms into the various key components in\nMask R-CNN, including Attention-Guided RPN, Relation-Guided Detector, and\nAttention-Guided FCN, in order to make full use of the guidance effect from the\nsupport set and adapt better to the inter-class generalization. Experiments on\npublic datasets demonstrate that our proposed FGN can outperform the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 05:02:20 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Fan", "Zhibo", ""], ["Yu", "Jin-Gang", ""], ["Liang", "Zhihao", ""], ["Ou", "Jiarong", ""], ["Gao", "Changxin", ""], ["Xia", "Gui-Song", ""], ["Li", "Yuanqing", ""]]}, {"id": "2003.13958", "submitter": "Jiahong Ouyang", "authors": "Jiahong Ouyang, Qingyu Zhao, Edith V Sullivan, Adolf Pfefferbaum,\n  Susan F. Tapert, Ehsan Adeli, Kilian M Pohl", "title": "Longitudinal Pooling & Consistency Regularization to Model Disease\n  Progression from MRIs", "comments": "Accepted by Journal of Biomedical and Health Informatics (JBHI)", "journal-ref": "IEEE Journal of Biomedical and Health Informatics 2020", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many neurological diseases are characterized by gradual deterioration of\nbrain structure and function. Large longitudinal MRI datasets have revealed\nsuch deterioration, in part, by applying machine and deep learning to predict\ndiagnosis. A popular approach is to apply Convolutional Neural Networks (CNN)\nto extract informative features from each visit of the longitudinal MRI and\nthen use those features to classify each visit via Recurrent Neural Networks\n(RNNs). Such modeling neglects the progressive nature of the disease, which may\nresult in clinically implausible classifications across visits. To avoid this\nissue, we propose to combine features across visits by coupling feature\nextraction with a novel longitudinal pooling layer and enforce consistency of\nthe classification across visits in line with disease progression. We evaluate\nthe proposed method on the longitudinal structural MRIs from three neuroimaging\ndatasets: Alzheimer's Disease Neuroimaging Initiative (ADNI, N=404), a dataset\ncomposed of 274 normal controls and 329 patients with Alcohol Use Disorder\n(AUD), and 255 youths from the National Consortium on Alcohol and\nNeuroDevelopment in Adolescence (NCANDA). In all three experiments our method\nis superior to other widely used approaches for longitudinal classification\nthus making a unique contribution towards more accurate tracking of the impact\nof conditions on the brain. The code is available at\nhttps://github.com/ouyangjiahong/longitudinal-pooling.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 05:28:51 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 17:36:39 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Ouyang", "Jiahong", ""], ["Zhao", "Qingyu", ""], ["Sullivan", "Edith V", ""], ["Pfefferbaum", "Adolf", ""], ["Tapert", "Susan F.", ""], ["Adeli", "Ehsan", ""], ["Pohl", "Kilian M", ""]]}, {"id": "2003.13960", "submitter": "Yandong Li", "authors": "Dongdong Wang, Yandong Li, Liqiang Wang, Boqing Gong", "title": "Neural Networks Are More Productive Teachers Than Human Raters: Active\n  Mixup for Data-Efficient Knowledge Distillation from a Blackbox Model", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study how to train a student deep neural network for visual recognition by\ndistilling knowledge from a blackbox teacher model in a data-efficient manner.\nProgress on this problem can significantly reduce the dependence on large-scale\ndatasets for learning high-performing visual recognition models. There are two\nmajor challenges. One is that the number of queries into the teacher model\nshould be minimized to save computational and/or financial costs. The other is\nthat the number of images used for the knowledge distillation should be small;\notherwise, it violates our expectation of reducing the dependence on\nlarge-scale datasets. To tackle these challenges, we propose an approach that\nblends mixup and active learning. The former effectively augments the few\nunlabeled images by a big pool of synthetic images sampled from the convex hull\nof the original images, and the latter actively chooses from the pool hard\nexamples for the student neural network and query their labels from the teacher\nmodel. We validate our approach with extensive experiments.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 05:44:55 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wang", "Dongdong", ""], ["Li", "Yandong", ""], ["Wang", "Liqiang", ""], ["Gong", "Boqing", ""]]}, {"id": "2003.13962", "submitter": "Difei Gao", "authors": "Difei Gao, Ke Li, Ruiping Wang, Shiguang Shan, Xilin Chen", "title": "Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene\n  Text", "comments": "Published as a CVPR2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answering questions that require reading texts in an image is challenging for\ncurrent models. One key difficulty of this task is that rare, polysemous, and\nambiguous words frequently appear in images, e.g., names of places, products,\nand sports teams. To overcome this difficulty, only resorting to pre-trained\nword embedding models is far from enough. A desired model should utilize the\nrich information in multiple modalities of the image to help understand the\nmeaning of scene texts, e.g., the prominent text on a bottle is most likely to\nbe the brand. Following this idea, we propose a novel VQA approach, Multi-Modal\nGraph Neural Network (MM-GNN). It first represents an image as a graph\nconsisting of three sub-graphs, depicting visual, semantic, and numeric\nmodalities respectively. Then, we introduce three aggregators which guide the\nmessage passing from one graph to another to utilize the contexts in various\nmodalities, so as to refine the features of nodes. The updated nodes have\nbetter features for the downstream question answering module. Experimental\nevaluations show that our MM-GNN represents the scene texts better and\nobviously facilitates the performances on two VQA tasks that require reading\nscene texts.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 05:56:59 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gao", "Difei", ""], ["Li", "Ke", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "2003.13964", "submitter": "Sukmin Yun", "authors": "Sukmin Yun, Jongjin Park, Kimin Lee, Jinwoo Shin", "title": "Regularizing Class-wise Predictions via Self-knowledge Distillation", "comments": "Accepted to CVPR 2020. Code is available at\n  https://github.com/alinlab/cs-kd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with millions of parameters may suffer from poor\ngeneralization due to overfitting. To mitigate the issue, we propose a new\nregularization method that penalizes the predictive distribution between\nsimilar samples. In particular, we distill the predictive distribution between\ndifferent samples of the same label during training. This results in\nregularizing the dark knowledge (i.e., the knowledge on wrong predictions) of a\nsingle network (i.e., a self-knowledge distillation) by forcing it to produce\nmore meaningful and consistent predictions in a class-wise manner.\nConsequently, it mitigates overconfident predictions and reduces intra-class\nvariations. Our experimental results on various image classification tasks\ndemonstrate that the simple yet powerful method can significantly improve not\nonly the generalization ability but also the calibration performance of modern\nconvolutional neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:03:51 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 05:28:07 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Yun", "Sukmin", ""], ["Park", "Jongjin", ""], ["Lee", "Kimin", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2003.13969", "submitter": "Mingkui Tan", "authors": "Chendi Rao, Jiezhang Cao, Runhao Zeng, Qi Chen, Huazhu Fu, Yanwu Xu,\n  Mingkui Tan", "title": "A Thorough Comparison Study on Adversarial Attacks and Defenses for\n  Common Thorax Disease Classification in Chest X-rays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks (DNNs) have made great progress on automated\ndiagnosis with chest X-rays images. However, DNNs are vulnerable to adversarial\nexamples, which may cause misdiagnoses to patients when applying the DNN based\nmethods in disease detection. Recently, there is few comprehensive studies\nexploring the influence of attack and defense methods on disease detection,\nespecially for the multi-label classification problem. In this paper, we aim to\nreview various adversarial attack and defense methods on chest X-rays. First,\nthe motivations and the mathematical representations of attack and defense\nmethods are introduced in details. Second, we evaluate the influence of several\nstate-of-the-art attack and defense methods for common thorax disease\nclassification in chest X-rays. We found that the attack and defense methods\nhave poor performance with excessive iterations and large perturbations. To\naddress this, we propose a new defense method that is robust to different\ndegrees of perturbations. This study could provide new insights into\nmethodological development for the community.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:21:03 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Rao", "Chendi", ""], ["Cao", "Jiezhang", ""], ["Zeng", "Runhao", ""], ["Chen", "Qi", ""], ["Fu", "Huazhu", ""], ["Xu", "Yanwu", ""], ["Tan", "Mingkui", ""]]}, {"id": "2003.13985", "submitter": "Sean Moran", "authors": "Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, Gregory\n  Slabaugh", "title": "DeepLPF: Deep Local Parametric Filters for Image Enhancement", "comments": "Accepted for publication at CVPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital artists often improve the aesthetic quality of digital photographs\nthrough manual retouching. Beyond global adjustments, professional image\nediting programs provide local adjustment tools operating on specific parts of\nan image. Options include parametric (graduated, radial filters) and\nunconstrained brush tools. These highly expressive tools enable a diverse set\nof local image enhancements. However, their use can be time consuming, and\nrequires artistic capability. State-of-the-art automated image enhancement\napproaches typically focus on learning pixel-level or global enhancements. The\nformer can be noisy and lack interpretability, while the latter can fail to\ncapture fine-grained adjustments. In this paper, we introduce a novel approach\nto automatically enhance images using learned spatially local filters of three\ndifferent types (Elliptical Filter, Graduated Filter, Polynomial Filter). We\nintroduce a deep neural network, dubbed Deep Local Parametric Filters\n(DeepLPF), which regresses the parameters of these spatially localized filters\nthat are then automatically applied to enhance the image. DeepLPF provides a\nnatural form of model regularization and enables interpretable, intuitive\nadjustments that lead to visually pleasing results. We report on multiple\nbenchmarks and show that DeepLPF produces state-of-the-art performance on two\nvariants of the MIT-Adobe-5K dataset, often using a fraction of the parameters\nrequired for competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 06:51:21 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Moran", "Sean", ""], ["Marza", "Pierre", ""], ["McDonagh", "Steven", ""], ["Parisot", "Sarah", ""], ["Slabaugh", "Gregory", ""]]}, {"id": "2003.13988", "submitter": "Chieh-Yun Chen", "authors": "Wen-Huang Cheng, Sijie Song, Chieh-Yun Chen, Shintami Chusnul\n  Hidayati, and Jiaying Liu", "title": "Fashion Meets Computer Vision: A Survey", "comments": "Accepted by ACM Computing Surveys (2021). 39 pages including 2 pages\n  of supplementary materials and 7 pages of reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is the way we present ourselves to the world and has become one of\nthe world's largest industries. Fashion, mainly conveyed by vision, has thus\nattracted much attention from computer vision researchers in recent years.\nGiven the rapid development, this paper provides a comprehensive survey of more\nthan 200 major fashion-related works covering four main aspects for enabling\nintelligent fashion: (1) Fashion detection includes landmark detection, fashion\nparsing, and item retrieval, (2) Fashion analysis contains attribute\nrecognition, style learning, and popularity prediction, (3) Fashion synthesis\ninvolves style transfer, pose transformation, and physical simulation, and (4)\nFashion recommendation comprises fashion compatibility, outfit matching, and\nhairstyle suggestion. For each task, the benchmark datasets and the evaluation\nprotocols are summarized. Furthermore, we highlight promising directions for\nfuture research.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 07:08:23 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 12:13:58 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cheng", "Wen-Huang", ""], ["Song", "Sijie", ""], ["Chen", "Chieh-Yun", ""], ["Hidayati", "Shintami Chusnul", ""], ["Liu", "Jiaying", ""]]}, {"id": "2003.13989", "submitter": "Haotian Yang", "authors": "Haotian Yang, Hao Zhu, Yanru Wang, Mingkai Huang, Qiu Shen, Ruigang\n  Yang, Xun Cao", "title": "FaceScape: a Large-scale High Quality 3D Face Dataset and Detailed\n  Riggable 3D Face Prediction", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a large-scale detailed 3D face dataset, FaceScape,\nand propose a novel algorithm that is able to predict elaborate riggable 3D\nface models from a single image input. FaceScape dataset provides 18,760\ntextured 3D faces, captured from 938 subjects and each with 20 specific\nexpressions. The 3D models contain the pore-level facial geometry that is also\nprocessed to be topologically uniformed. These fine 3D facial models can be\nrepresented as a 3D morphable model for rough shapes and displacement maps for\ndetailed geometry. Taking advantage of the large-scale and high-accuracy\ndataset, a novel algorithm is further proposed to learn the expression-specific\ndynamic details using a deep neural network. The learned relationship serves as\nthe foundation of our 3D face prediction system from a single image input.\nDifferent than the previous methods, our predicted 3D models are riggable with\nhighly detailed geometry under different expressions. The unprecedented dataset\nand code will be released to public for research purpose.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 07:11:08 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 12:56:48 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 17:18:48 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Yang", "Haotian", ""], ["Zhu", "Hao", ""], ["Wang", "Yanru", ""], ["Huang", "Mingkai", ""], ["Shen", "Qiu", ""], ["Yang", "Ruigang", ""], ["Cao", "Xun", ""]]}, {"id": "2003.14013", "submitter": "Cong Cao", "authors": "Huanjing Yue, Cong Cao, Lei Liao, Ronghe Chu, Jingyu Yang", "title": "Supervised Raw Video Denoising with a Benchmark Dataset on Dynamic\n  Scenes", "comments": "CVPR2020 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the supervised learning strategy for real noisy image\ndenoising has been emerging and has achieved promising results. In contrast,\nrealistic noise removal for raw noisy videos is rarely studied due to the lack\nof noisy-clean pairs for dynamic scenes. Clean video frames for dynamic scenes\ncannot be captured with a long-exposure shutter or averaging multi-shots as was\ndone for static images. In this paper, we solve this problem by creating\nmotions for controllable objects, such as toys, and capturing each static\nmoment for multiple times to generate clean video frames. In this way, we\nconstruct a dataset with 55 groups of noisy-clean videos with ISO values\nranging from 1600 to 25600. To our knowledge, this is the first dynamic video\ndataset with noisy-clean pairs. Correspondingly, we propose a raw video\ndenoising network (RViDeNet) by exploring the temporal, spatial, and channel\ncorrelations of video frames. Since the raw video has Bayer patterns, we pack\nit into four sub-sequences, i.e RGBG sequences, which are denoised by the\nproposed RViDeNet separately and finally fused into a clean video. In addition,\nour network not only outputs a raw denoising result, but also the sRGB result\nby going through an image signal processing (ISP) module, which enables users\nto generate the sRGB result with their favourite ISPs. Experimental results\ndemonstrate that our method outperforms state-of-the-art video and raw image\ndenoising algorithms on both indoor and outdoor videos.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:08:59 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yue", "Huanjing", ""], ["Cao", "Cong", ""], ["Liao", "Lei", ""], ["Chu", "Ronghe", ""], ["Yang", "Jingyu", ""]]}, {"id": "2003.14014", "submitter": "Weikun Wu", "authors": "Weikun Wu, Yan Zhang, David Wang, Yunqi Lei", "title": "SK-Net: Deep Learning on Point Cloud via End-to-end Discovery of Spatial\n  Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the PointNet was proposed, deep learning on point cloud has been the\nconcentration of intense 3D research. However, existing point-based methods\nusually are not adequate to extract the local features and the spatial pattern\nof a point cloud for further shape understanding. This paper presents an\nend-to-end framework, SK-Net, to jointly optimize the inference of spatial\nkeypoint with the learning of feature representation of a point cloud for a\nspecific point cloud task. One key process of SK-Net is the generation of\nspatial keypoints (Skeypoints). It is jointly conducted by two proposed\nregulating losses and a task objective function without knowledge of Skeypoint\nlocation annotations and proposals. Specifically, our Skeypoints are not\nsensitive to the location consistency but are acutely aware of shape. Another\nkey process of SK-Net is the extraction of the local structure of Skeypoints\n(detail feature) and the local spatial pattern of normalized Skeypoints\n(pattern feature). This process generates a comprehensive representation,\npattern-detail (PD) feature, which comprises the local detail information of a\npoint cloud and reveals its spatial pattern through the part district\nreconstruction on normalized Skeypoints. Consequently, our network is prompted\nto effectively understand the correlation between different regions of a point\ncloud and integrate contextual information of the point cloud. In point cloud\ntasks, such as classification and segmentation, our proposed method performs\nbetter than or comparable with the state-of-the-art approaches. We also present\nan ablation study to demonstrate the advantages of SK-Net.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:15:40 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wu", "Weikun", ""], ["Zhang", "Yan", ""], ["Wang", "David", ""], ["Lei", "Yunqi", ""]]}, {"id": "2003.14023", "submitter": "Tiancai Wang", "authors": "Tiancai Wang and Tong Yang and Martin Danelljan and Fahad Shahbaz Khan\n  and Xiangyu Zhang and Jian Sun", "title": "Learning Human-Object Interaction Detection using Interaction Points", "comments": "Accepted to CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding interactions between humans and objects is one of the\nfundamental problems in visual classification and an essential step towards\ndetailed scene understanding. Human-object interaction (HOI) detection strives\nto localize both the human and an object as well as the identification of\ncomplex interactions between them. Most existing HOI detection approaches are\ninstance-centric where interactions between all possible human-object pairs are\npredicted based on appearance features and coarse spatial information. We argue\nthat appearance features alone are insufficient to capture complex human-object\ninteractions. In this paper, we therefore propose a novel fully-convolutional\napproach that directly detects the interactions between human-object pairs. Our\nnetwork predicts interaction points, which directly localize and classify the\ninter-action. Paired with the densely predicted interaction vectors, the\ninteractions are associated with human and object detections to obtain final\npredictions. To the best of our knowledge, we are the first to propose an\napproach where HOI detection is posed as a keypoint detection and grouping\nproblem. Experiments are performed on two popular benchmarks: V-COCO and\nHICO-DET. Our approach sets a new state-of-the-art on both datasets. Code is\navailable at https://github.com/vaesl/IP-Net.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:42:06 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wang", "Tiancai", ""], ["Yang", "Tong", ""], ["Danelljan", "Martin", ""], ["Khan", "Fahad Shahbaz", ""], ["Zhang", "Xiangyu", ""], ["Sun", "Jian", ""]]}, {"id": "2003.14030", "submitter": "Matteo Poggi", "authors": "Fabio Tosi, Filippo Aleotti, Pierluigi Zama Ramirez, Matteo Poggi,\n  Samuele Salti, Luigi Di Stefano and Stefano Mattoccia", "title": "Distilled Semantics for Comprehensive Scene Understanding from Videos", "comments": "CVPR 2020. Code will be available at\n  https://github.com/CVLAB-Unibo/omeganet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole understanding of the surroundings is paramount to autonomous systems.\nRecent works have shown that deep neural networks can learn geometry (depth)\nand motion (optical flow) from a monocular video without any explicit\nsupervision from ground truth annotations, particularly hard to source for\nthese two tasks. In this paper, we take an additional step toward holistic\nscene understanding with monocular cameras by learning depth and motion\nalongside with semantics, with supervision for the latter provided by a\npre-trained network distilling proxy ground truth images. We address the three\ntasks jointly by a) a novel training protocol based on knowledge distillation\nand self-supervision and b) a compact network architecture which enables\nefficient scene understanding on both power hungry GPUs and low-power embedded\nplatforms. We thoroughly assess the performance of our framework and show that\nit yields state-of-the-art results for monocular depth estimation, optical flow\nand motion segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:52:13 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Tosi", "Fabio", ""], ["Aleotti", "Filippo", ""], ["Ramirez", "Pierluigi Zama", ""], ["Poggi", "Matteo", ""], ["Salti", "Samuele", ""], ["Di Stefano", "Luigi", ""], ["Mattoccia", "Stefano", ""]]}, {"id": "2003.14031", "submitter": "Xi Li", "authors": "Yifeng Chen, Guangchen Lin, Songyuan Li, Bourahla Omar, Yiming Wu,\n  Fangfang Wang, Junyi Feng, Mingliang Xu, and Xi Li", "title": "BANet: Bidirectional Aggregation Network with Occlusion Handling for\n  Panoptic Segmentation", "comments": "to be published in CVPR2020, oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoptic segmentation aims to perform instance segmentation for foreground\ninstances and semantic segmentation for background stuff simultaneously. The\ntypical top-down pipeline concentrates on two key issues: 1) how to effectively\nmodel the intrinsic interaction between semantic segmentation and instance\nsegmentation, and 2) how to properly handle occlusion for panoptic\nsegmentation. Intuitively, the complementarity between semantic segmentation\nand instance segmentation can be leveraged to improve the performance. Besides,\nwe notice that using detection/mask scores is insufficient for resolving the\nocclusion problem. Motivated by these observations, we propose a novel deep\npanoptic segmentation scheme based on a bidirectional learning pipeline.\nMoreover, we introduce a plug-and-play occlusion handling algorithm to deal\nwith the occlusion between different object instances. The experimental results\non COCO panoptic benchmark validate the effectiveness of our proposed method.\nCodes will be released soon at https://github.com/Mooonside/BANet.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:57:14 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chen", "Yifeng", ""], ["Lin", "Guangchen", ""], ["Li", "Songyuan", ""], ["Omar", "Bourahla", ""], ["Wu", "Yiming", ""], ["Wang", "Fangfang", ""], ["Feng", "Junyi", ""], ["Xu", "Mingliang", ""], ["Li", "Xi", ""]]}, {"id": "2003.14032", "submitter": "Yang Zhang", "authors": "Yang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing\n  Gong, Hassan Foroosh", "title": "PolarNet: An Improved Grid Representation for Online LiDAR Point Clouds\n  Semantic Segmentation", "comments": "Accepted by CVPR 2020; Code at\n  https://github.com/edwardzhou130/PolarSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for fine-grained perception in autonomous driving systems has\nresulted in recently increased research on online semantic segmentation of\nsingle-scan LiDAR. Despite the emerging datasets and technological\nadvancements, it remains challenging due to three reasons: (1) the need for\nnear-real-time latency with limited hardware; (2) uneven or even long-tailed\ndistribution of LiDAR points across space; and (3) an increasing number of\nextremely fine-grained semantic classes. In an attempt to jointly tackle all\nthe aforementioned challenges, we propose a new LiDAR-specific,\nnearest-neighbor-free segmentation algorithm - PolarNet. Instead of using\ncommon spherical or bird's-eye-view projection, our polar bird's-eye-view\nrepresentation balances the points across grid cells in a polar coordinate\nsystem, indirectly aligning a segmentation network's attention with the\nlong-tailed distribution of the points along the radial axis. We find that our\nencoding scheme greatly increases the mIoU in three drastically different\nsegmentation datasets of real urban LiDAR single scans while retaining near\nreal-time throughput.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 08:58:45 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 08:44:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhang", "Yang", ""], ["Zhou", "Zixiang", ""], ["David", "Philip", ""], ["Yue", "Xiangyu", ""], ["Xi", "Zerong", ""], ["Gong", "Boqing", ""], ["Foroosh", "Hassan", ""]]}, {"id": "2003.14034", "submitter": "Siyuan Xiang", "authors": "Wenyu Han, Siyuan Xiang, Chenhui Liu, Ruoyu Wang, Chen Feng", "title": "SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings", "comments": "This paper has been accepted in CVPR'20. The first two authors\n  contributed equally. Chen Feng is the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial reasoning is an important component of human intelligence. We can\nimagine the shapes of 3D objects and reason about their spatial relations by\nmerely looking at their three-view line drawings in 2D, with different levels\nof competence. Can deep networks be trained to perform spatial reasoning tasks?\nHow can we measure their \"spatial intelligence\"? To answer these questions, we\npresent the SPARE3D dataset. Based on cognitive science and psychometrics,\nSPARE3D contains three types of 2D-3D reasoning tasks on view consistency,\ncamera pose, and shape generation, with increasing difficulty. We then design a\nmethod to automatically generate a large number of challenging questions with\nground truth answers for each task. They are used to provide supervision for\ntraining our baseline models using state-of-the-art architectures like ResNet.\nOur experiments show that although convolutional networks have achieved\nsuperhuman performance in many visual learning tasks, their spatial reasoning\nperformance on SPARE3D tasks is either lower than average human performance or\neven close to random guesses. We hope SPARE3D can stimulate new problem\nformulations and network designs for spatial reasoning to empower intelligent\nrobots to operate effectively in the 3D world via 2D sensors. The dataset and\ncode are available at https://ai4ce.github.io/SPARE3D.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:01:27 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 14:18:47 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Han", "Wenyu", ""], ["Xiang", "Siyuan", ""], ["Liu", "Chenhui", ""], ["Wang", "Ruoyu", ""], ["Feng", "Chen", ""]]}, {"id": "2003.14043", "submitter": "Mark Philip Philipsen", "authors": "Mark Philip Philipsen and Thomas Baltzer Moeslund", "title": "Distance in Latent Space as Novelty Measure", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Learning performs well when training data densely covers the experience\nspace. For complex problems this makes data collection prohibitively expensive.\nWe propose to intelligently select samples when constructing data sets in order\nto best utilize the available labeling budget. The selection methodology is\nbased on the presumption that two dissimilar samples are worth more than two\nsimilar samples in a data set. Similarity is measured based on the Euclidean\ndistance between samples in the latent space produced by a DNN. By using a\nself-supervised method to construct the latent space, it is ensured that the\nspace fits the data well and that any upfront labeling effort can be avoided.\nThe result is more efficient, diverse, and balanced data set, which produce\nequal or superior results with fewer labeled examples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:14:56 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Philipsen", "Mark Philip", ""], ["Moeslund", "Thomas Baltzer", ""]]}, {"id": "2003.14047", "submitter": "Mark Philip Philipsen", "authors": "Mark Philip Philipsen and Thomas Baltzer Moeslund", "title": "Prediction Confidence from Neighbors", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The inability of Machine Learning (ML) models to successfully extrapolate\ncorrect predictions from out-of-distribution (OoD) samples is a major hindrance\nto the application of ML in critical applications. Until the generalization\nability of ML methods is improved it is necessary to keep humans in the loop.\nThe need for human supervision can only be reduced if it is possible to\ndetermining a level of confidence in predictions, which can be used to either\nask for human assistance or to abstain from making predictions. We show that\nfeature space distance is a meaningful measure that can provide confidence in\npredictions. The distance between unseen samples and nearby training samples\nproves to be correlated to the prediction error of unseen samples. Depending on\nthe acceptable degree of error, predictions can either be trusted or rejected\nbased on the distance to training samples. %Additionally, a novelty threshold\ncan be used to decide whether a sample is worth adding to the training set.\nThis enables earlier and safer deployment of models in critical applications\nand is vital for deploying models under ever-changing conditions.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:26:09 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Philipsen", "Mark Philip", ""], ["Moeslund", "Thomas Baltzer", ""]]}, {"id": "2003.14052", "submitter": "Xiaokang Chen", "authors": "Xiaokang Chen, Kwan-Yee Lin, Chen Qian, Gang Zeng and Hongsheng Li", "title": "3D Sketch-aware Semantic Scene Completion via Semi-supervised Structure\n  Prior", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the Semantic Scene Completion (SSC) task is to simultaneously\npredict a completed 3D voxel representation of volumetric occupancy and\nsemantic labels of objects in the scene from a single-view observation. Since\nthe computational cost generally increases explosively along with the growth of\nvoxel resolution, most current state-of-the-arts have to tailor their framework\ninto a low-resolution representation with the sacrifice of detail prediction.\nThus, voxel resolution becomes one of the crucial difficulties that lead to the\nperformance bottleneck.\n  In this paper, we propose to devise a new geometry-based strategy to embed\ndepth information with low-resolution voxel representation, which could still\nbe able to encode sufficient geometric information, e.g., room layout, object's\nsizes and shapes, to infer the invisible areas of the scene with well\nstructure-preserving details. To this end, we first propose a novel 3D\nsketch-aware feature embedding to explicitly encode geometric information\neffectively and efficiently. With the 3D sketch in hand, we further devise a\nsimple yet effective semantic scene completion framework that incorporates a\nlight-weight 3D Sketch Hallucination module to guide the inference of occupancy\nand the semantic labels via a semi-supervised structure prior learning\nstrategy. We demonstrate that our proposed geometric embedding works better\nthan the depth feature learning from habitual SSC frameworks. Our final model\nsurpasses state-of-the-arts consistently on three public benchmarks, which only\nrequires 3D volumes of 60 x 36 x 60 resolution for both input and output. The\ncode and the supplementary material will be available at\nhttps://charlesCXK.github.io.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:33:46 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Chen", "Xiaokang", ""], ["Lin", "Kwan-Yee", ""], ["Qian", "Chen", ""], ["Zeng", "Gang", ""], ["Li", "Hongsheng", ""]]}, {"id": "2003.14053", "submitter": "Jonas Geiping", "authors": "Jonas Geiping, Hartmut Bauermeister, Hannah Dr\\\"oge, Michael Moeller", "title": "Inverting Gradients -- How easy is it to break privacy in federated\n  learning?", "comments": "23 pages, 20 figures. The first three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of federated learning is to collaboratively train a neural network\non a server. Each user receives the current weights of the network and in turns\nsends parameter updates (gradients) based on local data. This protocol has been\ndesigned not only to train neural networks data-efficiently, but also to\nprovide privacy benefits for users, as their input data remains on device and\nonly parameter gradients are shared. But how secure is sharing parameter\ngradients? Previous attacks have provided a false sense of security, by\nsucceeding only in contrived settings - even for a single image. However, by\nexploiting a magnitude-invariant loss along with optimization strategies based\non adversarial attacks, we show that is is actually possible to faithfully\nreconstruct images at high resolution from the knowledge of their parameter\ngradients, and demonstrate that such a break of privacy is possible even for\ntrained deep networks. We analyze the effects of architecture as well as\nparameters on the difficulty of reconstructing an input image and prove that\nany input to a fully connected layer can be reconstructed analytically\nindependent of the remaining architecture. Finally we discuss settings\nencountered in practice and show that even averaging gradients over several\niterations or several images does not protect the user's privacy in federated\nlearning applications in computer vision.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:35:02 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 11:41:10 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Geiping", "Jonas", ""], ["Bauermeister", "Hartmut", ""], ["Dr\u00f6ge", "Hannah", ""], ["Moeller", "Michael", ""]]}, {"id": "2003.14058", "submitter": "Yuan Gao", "authors": "Yuan Gao, Haoping Bai, Zequn Jie, Jiayi Ma, Kui Jia, and Wei Liu", "title": "MTL-NAS: Task-Agnostic Neural Architecture Search towards\n  General-Purpose Multi-Task Learning", "comments": "Accepted to CVPR2020. The first two authors contribute equally", "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to incorporate neural architecture search (NAS) into\ngeneral-purpose multi-task learning (GP-MTL). Existing NAS methods typically\ndefine different search spaces according to different tasks. In order to adapt\nto different task combinations (i.e., task sets), we disentangle the GP-MTL\nnetworks into single-task backbones (optionally encode the task priors), and a\nhierarchical and layerwise features sharing/fusing scheme across them. This\nenables us to design a novel and general task-agnostic search space, which\ninserts cross-task edges (i.e., feature fusion connections) into fixed\nsingle-task network backbones. Moreover, we also propose a novel single-shot\ngradient-based search algorithm that closes the performance gap between the\nsearched architectures and the final evaluation architecture. This is realized\nwith a minimum entropy regularization on the architecture weights during the\nsearch phase, which makes the architecture weights converge to near-discrete\nvalues and therefore achieves a single model. As a result, our searched model\ncan be directly used for evaluation without (re-)training from scratch. We\nperform extensive experiments using different single-task backbones on various\ntask sets, demonstrating the promising performance obtained by exploiting the\nhierarchical and layerwise features, as well as the desirable generalizability\nto different i) task sets and ii) single-task backbones. The code of our paper\nis available at https://github.com/bhpfelix/MTLNAS.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 09:49:14 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Gao", "Yuan", ""], ["Bai", "Haoping", ""], ["Jie", "Zequn", ""], ["Ma", "Jiayi", ""], ["Jia", "Kui", ""], ["Liu", "Wei", ""]]}, {"id": "2003.14065", "submitter": "Ting Yao", "authors": "Dong Li and Ting Yao and Zhaofan Qiu and Houqiang Li and Tao Mei", "title": "Long Short-Term Relation Networks for Video Action Detection", "comments": "Accepted as a full paper for ACMMM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been well recognized that modeling human-object or object-object\nrelations would be helpful for detection task. Nevertheless, the problem is not\ntrivial especially when exploring the interactions between human actor, object\nand scene (collectively as human-context) to boost video action detectors. The\ndifficulty originates from the aspect that reliable relations in a video should\ndepend on not only short-term human-context relation in the present clip but\nalso the temporal dynamics distilled over a long-range span of the video. This\nmotivates us to capture both short-term and long-term relations in a video. In\nthis paper, we present a new Long Short-Term Relation Networks, dubbed as LSTR,\nthat novelly aggregates and propagates relation to augment features for video\naction detection. Technically, Region Proposal Networks (RPN) is remoulded to\nfirst produce 3D bounding boxes, i.e., tubelets, in each video clip. LSTR then\nmodels short-term human-context interactions within each clip through\nspatio-temporal attention mechanism and reasons long-term temporal dynamics\nacross video clips via Graph Convolutional Networks (GCN) in a cascaded manner.\nExtensive experiments are conducted on four benchmark datasets, and superior\nresults are reported when comparing to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 10:02:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Li", "Dong", ""], ["Yao", "Ting", ""], ["Qiu", "Zhaofan", ""], ["Li", "Houqiang", ""], ["Mei", "Tao", ""]]}, {"id": "2003.14080", "submitter": "Ting Yao", "authors": "Yingwei Pan and Ting Yao and Yehao Li and Tao Mei", "title": "X-Linear Attention Networks for Image Captioning", "comments": "CVPR 2020; The source code and model are publicly available at:\n  https://github.com/Panda-Peter/image-captioning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on fine-grained visual recognition and visual question\nanswering has featured Bilinear Pooling, which effectively models the 2$^{nd}$\norder interactions across multi-modal inputs. Nevertheless, there has not been\nevidence in support of building such interactions concurrently with attention\nmechanism for image captioning. In this paper, we introduce a unified attention\nblock -- X-Linear attention block, that fully employs bilinear pooling to\nselectively capitalize on visual information or perform multi-modal reasoning.\nTechnically, X-Linear attention block simultaneously exploits both the spatial\nand channel-wise bilinear attention distributions to capture the 2$^{nd}$ order\ninteractions between the input single-modal or multi-modal features. Higher and\neven infinity order feature interactions are readily modeled through stacking\nmultiple X-Linear attention blocks and equipping the block with Exponential\nLinear Unit (ELU) in a parameter-free fashion, respectively. Furthermore, we\npresent X-Linear Attention Networks (dubbed as X-LAN) that novelly integrates\nX-Linear attention block(s) into image encoder and sentence decoder of image\ncaptioning model to leverage higher order intra- and inter-modal interactions.\nThe experiments on COCO benchmark demonstrate that our X-LAN obtains to-date\nthe best published CIDEr performance of 132.0% on COCO Karpathy test split.\nWhen further endowing Transformer with X-Linear attention blocks, CIDEr is\nboosted up to 132.8%. Source code is available at\n\\url{https://github.com/Panda-Peter/image-captioning}.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 10:35:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Li", "Yehao", ""], ["Mei", "Tao", ""]]}, {"id": "2003.14105", "submitter": "Jianyang Zhang", "authors": "Jianyang Zhang, Fengmao Lv, Guowu Yang, Lei Feng, Yufeng Yu, Lixin\n  Duan", "title": "Learning Cross-domain Semantic-Visual Relation for Transductive\n  Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) aims to learn recognition models for recognizing new\nclasses without labeled data. In this work, we propose a novel approach dubbed\nTransferrable Semantic-Visual Relation (TSVR) to facilitate the cross-category\ntransfer in transductive ZSL. Our approach draws on an intriguing insight\nconnecting two challenging problems, i.e. domain adaptation and zero-shot\nlearning. Domain adaptation aims to transfer knowledge across two different\ndomains (i.e., source domain and target domain) that share the identical\ntask/label space. For ZSL, the source and target domains have different\ntasks/label spaces. Hence, ZSL is usually considered as a more difficult\ntransfer setting compared with domain adaptation. Although the existing ZSL\napproaches use semantic attributes of categories to bridge the source and\ntarget domains, their performances are far from satisfactory due to the large\ndomain gap between different categories. In contrast, our method directly\ntransforms ZSL into a domain adaptation task through redrawing ZSL as\npredicting the similarity/dissimilarity labels for the pairs of semantic\nattributes and visual features. For this redrawn domain adaptation problem, we\npropose to use a domain-specific batch normalization component to reduce the\ndomain discrepancy of semantic-visual pairs. Experimental results over diverse\nZSL benchmarks clearly demonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:26:49 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhang", "Jianyang", ""], ["Lv", "Fengmao", ""], ["Yang", "Guowu", ""], ["Feng", "Lei", ""], ["Yu", "Yufeng", ""], ["Duan", "Lixin", ""]]}, {"id": "2003.14109", "submitter": "Leonardo Citraro", "authors": "Leonardo Citraro, Pablo M\\'arquez-Neila, Stefano Savar\\`e, Vivek\n  Jayaram, Charles Dubout, F\\'elix Renaut, Andr\\'es Hasfura, Horesh Ben\n  Shitrit, Pascal Fua", "title": "Real-Time Camera Pose Estimation for Sports Fields", "comments": null, "journal-ref": "Machine Vision and Applications 31, 16 (2020)", "doi": "10.1007/s00138-020-01064-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image sequence featuring a portion of a sports field filmed by a\nmoving and uncalibrated camera, such as the one of the smartphones, our goal is\nto compute automatically in real time the focal length and extrinsic camera\nparameters for each image in the sequence without using a priori knowledges of\nthe position and orientation of the camera. To this end, we propose a novel\nframework that combines accurate localization and robust identification of\nspecific keypoints in the image by using a fully convolutional deep\narchitecture. Our algorithm exploits both the field lines and the players'\nimage locations, assuming their ground plane positions to be given, to achieve\naccuracy and robustness that is beyond the current state of the art. We will\ndemonstrate its effectiveness on challenging soccer, basketball, and volleyball\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:27:33 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Citraro", "Leonardo", ""], ["M\u00e1rquez-Neila", "Pablo", ""], ["Savar\u00e8", "Stefano", ""], ["Jayaram", "Vivek", ""], ["Dubout", "Charles", ""], ["Renaut", "F\u00e9lix", ""], ["Hasfura", "Andr\u00e9s", ""], ["Shitrit", "Horesh Ben", ""], ["Fua", "Pascal", ""]]}, {"id": "2003.14111", "submitter": "Ziyu Liu", "authors": "Ziyu Liu, Hongwen Zhang, Zhenghao Chen, Zhiyong Wang, Wanli Ouyang", "title": "Disentangling and Unifying Graph Convolutions for Skeleton-Based Action\n  Recognition", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal graphs have been widely used by skeleton-based action\nrecognition algorithms to model human action dynamics. To capture robust\nmovement patterns from these graphs, long-range and multi-scale context\naggregation and spatial-temporal dependency modeling are critical aspects of a\npowerful feature extractor. However, existing methods have limitations in\nachieving (1) unbiased long-range joint relationship modeling under multi-scale\noperators and (2) unobstructed cross-spacetime information flow for capturing\ncomplex spatial-temporal dependencies. In this work, we present (1) a simple\nmethod to disentangle multi-scale graph convolutions and (2) a unified\nspatial-temporal graph convolutional operator named G3D. The proposed\nmulti-scale aggregation scheme disentangles the importance of nodes in\ndifferent neighborhoods for effective long-range modeling. The proposed G3D\nmodule leverages dense cross-spacetime edges as skip connections for direct\ninformation propagation across the spatial-temporal graph. By coupling these\nproposals, we develop a powerful feature extractor named MS-G3D based on which\nour model outperforms previous state-of-the-art methods on three large-scale\ndatasets: NTU RGB+D 60, NTU RGB+D 120, and Kinetics Skeleton 400.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:28:25 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 07:04:42 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Liu", "Ziyu", ""], ["Zhang", "Hongwen", ""], ["Chen", "Zhenghao", ""], ["Wang", "Zhiyong", ""], ["Ouyang", "Wanli", ""]]}, {"id": "2003.14119", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra, Behzad Bozorgtabar, Jean-Philippe Thiran and\n  Ling Shao", "title": "Pathological Retinal Region Segmentation From OCT Images Using Geometric\n  Relation Based Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image segmentation is an important task for computer aided diagnosis.\nPixelwise manual annotations of large datasets require high expertise and is\ntime consuming. Conventional data augmentations have limited benefit by not\nfully representing the underlying distribution of the training set, thus\naffecting model robustness when tested on images captured from different\nsources. Prior work leverages synthetic images for data augmentation ignoring\nthe interleaved geometric relationship between different anatomical labels. We\npropose improvements over previous GAN-based medical image synthesis methods by\njointly encoding the intrinsic relationship of geometry and shape. Latent space\nvariable sampling results in diverse generated images from a base image and\nimproves robustness. Given those augmented images generated by our method, we\ntrain the segmentation network to enhance the segmentation performance of\nretinal optical coherence tomography (OCT) images. The proposed method\noutperforms state-of-the-art segmentation methods on the public RETOUCH dataset\nhaving images captured from different acquisition procedures. Ablation studies\nand visual analysis also demonstrate benefits of integrating geometry and\ndiversity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:50:43 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 12:06:48 GMT"}, {"version": "v3", "created": "Sat, 25 Apr 2020 14:37:07 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Bozorgtabar", "Behzad", ""], ["Thiran", "Jean-Philippe", ""], ["Shao", "Ling", ""]]}, {"id": "2003.14142", "submitter": "Mohan Zhou", "authors": "Mohan Zhou, Yalong Bai, Wei Zhang, Tiejun Zhao, Tao Mei", "title": "Look-into-Object: Self-supervised Structure Modeling for Object\n  Recognition", "comments": "10 pages, 7 figures, accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most object recognition approaches predominantly focus on learning\ndiscriminative visual patterns while overlooking the holistic object structure.\nThough important, structure modeling usually requires significant manual\nannotations and therefore is labor-intensive. In this paper, we propose to\n\"look into object\" (explicitly yet intrinsically model the object structure)\nthrough incorporating self-supervisions into the traditional framework. We show\nthe recognition backbone can be substantially enhanced for more robust\nrepresentation learning, without any cost of extra annotation and inference\nspeed. Specifically, we first propose an object-extent learning module for\nlocalizing the object according to the visual patterns shared among the\ninstances in the same category. We then design a spatial context learning\nmodule for modeling the internal structures of the object, through predicting\nthe relative positions within the extent. These two modules can be easily\nplugged into any backbone networks during training and detached at inference\ntime. Extensive experiments show that our look-into-object approach (LIO)\nachieves large performance gain on a number of benchmarks, including generic\nobject recognition (ImageNet) and fine-grained object recognition tasks (CUB,\nCars, Aircraft). We also show that this learning paradigm is highly\ngeneralizable to other tasks such as object detection and segmentation (MS\nCOCO). Project page: https://github.com/JDAI-CV/LIO.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 12:22:51 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhou", "Mohan", ""], ["Bai", "Yalong", ""], ["Zhang", "Wei", ""], ["Zhao", "Tiejun", ""], ["Mei", "Tao", ""]]}, {"id": "2003.14166", "submitter": "Florian Golemo", "authors": "Sai Rajeswar, Fahim Mannan, Florian Golemo, J\\'er\\^ome\n  Parent-L\\'evesque, David Vazquez, Derek Nowrouzezahrai, Aaron Courville", "title": "Pix2Shape: Towards Unsupervised Learning of 3D Scenes from Images using\n  a View-based Representation", "comments": "This is a pre-print of an article published in International Journal\n  of Computer Vision. The final authenticated version is available online at:\n  https://doi.org/10.1007/s11263-020-01322-1", "journal-ref": "International Journal of Computer Vision, (2020), 1-16", "doi": "10.1007/s11263-020-01322-1", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We infer and generate three-dimensional (3D) scene information from a single\ninput image and without supervision. This problem is under-explored, with most\nprior work relying on supervision from, e.g., 3D ground-truth, multiple images\nof a scene, image silhouettes or key-points. We propose Pix2Shape, an approach\nto solve this problem with four components: (i) an encoder that infers the\nlatent 3D representation from an image, (ii) a decoder that generates an\nexplicit 2.5D surfel-based reconstruction of a scene from the latent code (iii)\na differentiable renderer that synthesizes a 2D image from the surfel\nrepresentation, and (iv) a critic network trained to discriminate between\nimages generated by the decoder-renderer and those from a training\ndistribution. Pix2Shape can generate complex 3D scenes that scale with the\nview-dependent on-screen resolution, unlike representations that capture\nworld-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a\nconsistent scene representation in its encoded latent space and that the\ndecoder can then be applied to this latent representation in order to\nsynthesize the scene from a novel viewpoint. We evaluate Pix2Shape with\nexperiments on the ShapeNet dataset as well as on a novel benchmark we\ndeveloped, called 3D-IQTT, to evaluate models based on their ability to enable\n3d spatial reasoning. Qualitative and quantitative evaluation demonstrate\nPix2Shape's ability to solve scene reconstruction, generation, and\nunderstanding tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 03:01:34 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:22:58 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Rajeswar", "Sai", ""], ["Mannan", "Fahim", ""], ["Golemo", "Florian", ""], ["Parent-L\u00e9vesque", "J\u00e9r\u00f4me", ""], ["Vazquez", "David", ""], ["Nowrouzezahrai", "Derek", ""], ["Courville", "Aaron", ""]]}, {"id": "2003.14171", "submitter": "Ronak Kosti", "authors": "Prathmesh Madhu, Ronak Kosti, Lara M\\\"uhrenberg, Peter Bell, Andreas\n  Maier, Vincent Christlein", "title": "Recognizing Characters in Art History Using Deep Learning", "comments": null, "journal-ref": "In Proceedings of the 1st Workshop on Structuring and\n  Understanding of Multimedia heritAge Contents, pp. 15-22 (2019, October)", "doi": "10.1145/3347317.3357242", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of Art History, images of artworks and their contexts are core\nto understanding the underlying semantic information. However, the highly\ncomplex and sophisticated representation of these artworks makes it difficult,\neven for the experts, to analyze the scene. From the computer vision\nperspective, the task of analyzing such artworks can be divided into\nsub-problems by taking a bottom-up approach. In this paper, we focus on the\nproblem of recognizing the characters in Art History. From the iconography of\n$Annunciation$ $of$ $the$ $Lord$ (Figure 1), we consider the representation of\nthe main protagonists, $Mary$ and $Gabriel$, across different artworks and\nstyles. We investigate and present the findings of training a character\nclassifier on features extracted from their face images. The limitations of\nthis method, and the inherent ambiguity in the representation of $Gabriel$,\nmotivated us to consider their bodies (a bigger context) to analyze in order to\nrecognize the characters. Convolutional Neural Networks (CNN) trained on the\nbodies of $Mary$ and $Gabriel$ are able to learn person related features and\nultimately improve the performance of character recognition. We introduce a new\ntechnique that generates more data with similar styles, effectively creating\ndata in the similar domain. We present experiments and analysis on three\ndifferent models and show that the model trained on domain related data gives\nthe best performance for recognizing character. Additionally, we analyze the\nlocalized image regions for the network predictions. Code is open-sourced and\navailable at\nhttps://github.com/prathmeshrmadhu/recognize_characters_art_history and the\nlink to the published peer-reviewed article is\nhttps://dl.acm.org/citation.cfm?id=3357242.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 13:15:00 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 11:37:40 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Madhu", "Prathmesh", ""], ["Kosti", "Ronak", ""], ["M\u00fchrenberg", "Lara", ""], ["Bell", "Peter", ""], ["Maier", "Andreas", ""], ["Christlein", "Vincent", ""]]}, {"id": "2003.14179", "submitter": "Juan Rojas", "authors": "Junfa Liu, Juan Rojas, Zhijun Liang, Yihui Li, and Yisheng Guan", "title": "A Graph Attention Spatio-temporal Convolutional Network for 3D Human\n  Pose Estimation in Video", "comments": "8 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal information is key to resolve occlusion and depth ambiguity\nin 3D pose estimation. Previous methods have focused on either temporal\ncontexts or local-to-global architectures that embed fixed-length\nspatio-temporal information. To date, there have not been effective proposals\nto simultaneously and flexibly capture varying spatio-temporal sequences and\neffectively achieves real-time 3D pose estimation. In this work, we improve the\nlearning of kinematic constraints in the human skeleton: posture, local\nkinematic connections, and symmetry by modeling local and global spatial\ninformation via attention mechanisms. To adapt to single- and multi-frame\nestimation, the dilated temporal model is employed to process varying skeleton\nsequences. Also, importantly, we carefully design the interleaving of spatial\nsemantics with temporal dependencies to achieve a synergistic effect. To this\nend, we propose a simple yet effective graph attention spatio-temporal\nconvolutional network (GAST-Net) that comprises of interleaved temporal\nconvolutional and graph attention blocks. Experiments on two challenging\nbenchmark datasets (Human3.6M and HumanEva-I) and YouTube videos demonstrate\nthat our approach effectively mitigates depth ambiguity and self-occlusion,\ngeneralizes to half upper body estimation, and achieves competitive performance\non 2D-to-3D video pose estimation. Code, video, and supplementary information\nis available at:\n\\href{http://www.juanrojas.net/gast/}{http://www.juanrojas.net/gast/}\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 14:54:40 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 03:29:31 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 11:49:40 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 01:19:08 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Liu", "Junfa", ""], ["Rojas", "Juan", ""], ["Liang", "Zhijun", ""], ["Li", "Yihui", ""], ["Guan", "Yisheng", ""]]}, {"id": "2003.14194", "submitter": "Saeed Masoudnia", "authors": "Saeed Masoudnia, Melika Kheirieh, Abdol-Hossein Vahabie, Babak Nadjar\n  Araabi", "title": "Attention-based Assisted Excitation for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention brings significant progress for Convolution Neural Networks\n(CNNs) in various applications. In this paper, object-based attention in human\nvisual cortex inspires us to introduce a mechanism for modification of\nactivations in feature maps of CNNs. In this mechanism, the activations of\nobject locations are excited in feature maps. This mechanism is specifically\ninspired by attention-based gain modulation in object-based attention in brain.\nIt facilitates figure-ground segregation in the visual cortex. Similar to\nbrain, we use the idea to address two challenges in salient object detection:\ngathering object interior parts while segregation from background with concise\nboundaries. We implement the object-based attention in the U-net model using\ndifferent architectures in the encoder parts, including AlexNet, VGG, and\nResNet. The proposed method was examined on three benchmark datasets: HKU-IS,\nMSRB, and PASCAL-S. Experimental results showed that our inspired method could\nsignificantly improve the results in terms of mean absolute error and\nF-measure. The results also showed that our proposed method better captured not\nonly the boundary but also the object interior. Thus, it can tackle the\nmentioned challenges.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 13:33:33 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 05:42:16 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Masoudnia", "Saeed", ""], ["Kheirieh", "Melika", ""], ["Vahabie", "Abdol-Hossein", ""], ["Araabi", "Babak Nadjar", ""]]}, {"id": "2003.14200", "submitter": "Gaston Lenczner", "authors": "Gaston Lenczner, Bertrand Le Saux, Nicola Luminari, Adrien Chan Hon\n  Tong and Guy Le Besnerais", "title": "DISIR: Deep Image Segmentation with Interactive Refinement", "comments": "8 pages, 12 figures. Published in the ISPRS Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences", "journal-ref": "XXIV ISPRS Congress, Commission II (Volume V-2-2020)", "doi": "10.5194/isprs-annals-V-2-2020-877-2020", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an interactive approach for multi-class segmentation of\naerial images. Precisely, it is based on a deep neural network which exploits\nboth RGB images and annotations. Starting from an initial output based on the\nimage only, our network then interactively refines this segmentation map using\na concatenation of the image and user annotations. Importantly, user\nannotations modify the inputs of the network - not its weights - enabling a\nfast and smooth process. Through experiments on two public aerial datasets, we\nshow that user annotations are extremely rewarding: each click corrects roughly\n5000 pixels. We analyze the impact of different aspects of our framework such\nas the representation of the annotations, the volume of training data or the\nnetwork architecture. Code is available at https://github.com/delair-ai/DISIR.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 13:37:42 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 14:04:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Lenczner", "Gaston", ""], ["Saux", "Bertrand Le", ""], ["Luminari", "Nicola", ""], ["Tong", "Adrien Chan Hon", ""], ["Besnerais", "Guy Le", ""]]}, {"id": "2003.14226", "submitter": "Xi Li", "authors": "Peng Sun, Jiaxiang Wu, Songyuan Li, Peiwen Lin, Junzhou Huang, and Xi\n  Li", "title": "Real-Time Semantic Segmentation via Auto Depth, Downsampling Joint\n  Decision and Feature Aggregation", "comments": "submitted to IJCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To satisfy the stringent requirements on computational resources in the field\nof real-time semantic segmentation, most approaches focus on the hand-crafted\ndesign of light-weight segmentation networks. Recently, Neural Architecture\nSearch (NAS) has been used to search for the optimal building blocks of\nnetworks automatically, but the network depth, downsampling strategy, and\nfeature aggregation way are still set in advance by trial and error. In this\npaper, we propose a joint search framework, called AutoRTNet, to automate the\ndesign of these strategies. Specifically, we propose hyper-cells to jointly\ndecide the network depth and downsampling strategy, and an aggregation cell to\nachieve automatic multi-scale feature aggregation. Experimental results show\nthat AutoRTNet achieves 73.9% mIoU on the Cityscapes test set and 110.0 FPS on\nan NVIDIA TitanXP GPU card with 768x1536 input images.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:02:25 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Sun", "Peng", ""], ["Wu", "Jiaxiang", ""], ["Li", "Songyuan", ""], ["Lin", "Peiwen", ""], ["Huang", "Junzhou", ""], ["Li", "Xi", ""]]}, {"id": "2003.14229", "submitter": "Washington Luis De Souza Ramos", "authors": "Washington Ramos, Michel Silva, Edson Araujo, Leandro Soriano\n  Marcolino, Erickson Nascimento", "title": "Straight to the Point: Fast-forwarding Videos via Reinforcement Learning\n  Using Textual Data", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid increase in the amount of published visual data and the limited\ntime of users bring the demand for processing untrimmed videos to produce\nshorter versions that convey the same information. Despite the remarkable\nprogress that has been made by summarization methods, most of them can only\nselect a few frames or skims, which creates visual gaps and breaks the video\ncontext. In this paper, we present a novel methodology based on a reinforcement\nlearning formulation to accelerate instructional videos. Our approach can\nadaptively select frames that are not relevant to convey the information\nwithout creating gaps in the final video. Our agent is textually and visually\noriented to select which frames to remove to shrink the input video.\nAdditionally, we propose a novel network, called Visually-guided Document\nAttention Network (VDAN), able to generate a highly discriminative embedding\nspace to represent both textual and visual data. Our experiments show that our\nmethod achieves the best performance in terms of F1 Score and coverage at the\nvideo segment level.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:07:45 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Ramos", "Washington", ""], ["Silva", "Michel", ""], ["Araujo", "Edson", ""], ["Marcolino", "Leandro Soriano", ""], ["Nascimento", "Erickson", ""]]}, {"id": "2003.14247", "submitter": "Liangliang Li", "authors": "Ling Yang, Liangliang Li, Zilun Zhang, Xinyu Zhou, Erjin Zhou, and Yu\n  Liu", "title": "DPGN: Distribution Propagation Graph Network for Few-shot Learning", "comments": "Accepted by CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most graph-network-based meta-learning approaches model instance-level\nrelation of examples. We extend this idea further to explicitly model the\ndistribution-level relation of one example to all other examples in a 1-vs-N\nmanner. We propose a novel approach named distribution propagation graph\nnetwork (DPGN) for few-shot learning. It conveys both the distribution-level\nrelations and instance-level relations in each few-shot learning task. To\ncombine the distribution-level relations and instance-level relations for all\nexamples, we construct a dual complete graph network which consists of a point\ngraph and a distribution graph with each node standing for an example. Equipped\nwith dual graph architecture, DPGN propagates label information from labeled\nexamples to unlabeled examples within several update generations. In extensive\nexperiments on few-shot learning benchmarks, DPGN outperforms state-of-the-art\nresults by a large margin in 5% $\\sim$ 12% under supervised setting and 7%\n$\\sim$ 13% under semi-supervised setting. Code will be released.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:32:05 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 16:57:03 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yang", "Ling", ""], ["Li", "Liangliang", ""], ["Zhang", "Zilun", ""], ["Zhou", "Xinyu", ""], ["Zhou", "Erjin", ""], ["Liu", "Yu", ""]]}, {"id": "2003.14266", "submitter": "Mohsen Fayyaz", "authors": "Mohsen Fayyaz and Juergen Gall", "title": "SCT: Set Constrained Temporal Transformer for Set Supervised Action\n  Segmentation", "comments": "CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action segmentation is a topic of increasing interest, however,\nannotating each frame in a video is cumbersome and costly. Weakly supervised\napproaches therefore aim at learning temporal action segmentation from videos\nthat are only weakly labeled. In this work, we assume that for each training\nvideo only the list of actions is given that occur in the video, but not when,\nhow often, and in which order they occur. In order to address this task, we\npropose an approach that can be trained end-to-end on such data. The approach\ndivides the video into smaller temporal regions and predicts for each region\nthe action label and its length. In addition, the network estimates the action\nlabels for each frame. By measuring how consistent the frame-wise predictions\nare with respect to the temporal regions and the annotated action labels, the\nnetwork learns to divide a video into class-consistent regions. We evaluate our\napproach on three datasets where the approach achieves state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:51:41 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["Gall", "Juergen", ""]]}, {"id": "2003.14269", "submitter": "Felix Yu", "authors": "Felix Yu, Zhiwei Deng, Karthik Narasimhan, Olga Russakovsky", "title": "Take the Scenic Route: Improving Generalization in Vision-and-Language\n  Navigation", "comments": "4 page short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Vision-and-Language Navigation (VLN) task, an agent with egocentric\nvision navigates to a destination given natural language instructions. The act\nof manually annotating these instructions is timely and expensive, such that\nmany existing approaches automatically generate additional samples to improve\nagent performance. However, these approaches still have difficulty generalizing\ntheir performance to new environments. In this work, we investigate the popular\nRoom-to-Room (R2R) VLN benchmark and discover that what is important is not\nonly the amount of data you synthesize, but also how you do it. We find that\nshortest path sampling, which is used by both the R2R benchmark and existing\naugmentation methods, encode biases in the action space of the agent which we\ndub as action priors. We then show that these action priors offer one\nexplanation toward the poor generalization of existing works. To mitigate such\npriors, we propose a path sampling method based on random walks to augment the\ndata. By training with this augmentation strategy, our agent is able to\ngeneralize better to unknown environments compared to the baseline,\nsignificantly improving model performance in the process.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 14:52:42 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Yu", "Felix", ""], ["Deng", "Zhiwei", ""], ["Narasimhan", "Karthik", ""], ["Russakovsky", "Olga", ""]]}, {"id": "2003.14287", "submitter": "Vladimir Kokh", "authors": "Manvel Avetisian, Vladimir Kokh, Alex Tuzhilin, Dmitry Umerenkov", "title": "Radiologist-level stroke classification on non-contrast CT scans with\n  Deep U-Net", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-32248-9_91", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of ischemic stroke and intracranial hemorrhage on computed\ntomography is essential for investigation and treatment of stroke. In this\npaper, we modified the U-Net CNN architecture for the stroke identification\nproblem using non-contrast CT. We applied the proposed DL model to historical\npatient data and also conducted clinical experiments involving ten experienced\nradiologists. Our model achieved strong results on historical data, and\nsignificantly outperformed seven radiologist out of ten, while being on par\nwith the remaining three.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:21:11 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Avetisian", "Manvel", ""], ["Kokh", "Vladimir", ""], ["Tuzhilin", "Alex", ""], ["Umerenkov", "Dmitry", ""]]}, {"id": "2003.14297", "submitter": "Idan Azuri", "authors": "Idan Azuri, Daphna Weinshall", "title": "Generative Latent Implicit Conditional Optimization when Learning from\n  Small Sample", "comments": "Published at ICPR 2020", "journal-ref": "Proc. ICPR, January 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the long-standing problem of learning from a small sample, to\nwhich end we propose a novel method called GLICO (Generative Latent Implicit\nConditional Optimization). GLICO learns a mapping from the training examples to\na latent space and a generator that generates images from vectors in the latent\nspace. Unlike most recent works, which rely on access to large amounts of\nunlabeled data, GLICO does not require access to any additional data other than\nthe small set of labeled points. In fact, GLICO learns to synthesize completely\nnew samples for every class using as little as 5 or 10 examples per class, with\nas few as 10 such classes without imposing any prior. GLICO is then used to\naugment the small training set while training a classifier on the small sample.\nTo this end, our proposed method samples the learned latent space using\nspherical interpolation, and generates new examples using the trained\ngenerator. Empirical results show that the new sampled set is diverse enough,\nleading to improvement in image classification in comparison with the state of\nthe art, when trained on small samples obtained from CIFAR-10, CIFAR-100, and\nCUB-200.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:38:45 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:24:44 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 10:51:13 GMT"}, {"version": "v4", "created": "Sun, 18 Oct 2020 13:57:09 GMT"}, {"version": "v5", "created": "Tue, 15 Dec 2020 12:01:47 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Azuri", "Idan", ""], ["Weinshall", "Daphna", ""]]}, {"id": "2003.14299", "submitter": "Yinda Zhang", "authors": "Yinda Zhang, Neal Wadhwa, Sergio Orts-Escolano, Christian H\\\"ane, Sean\n  Fanello, and Rahul Garg", "title": "Du$^2$Net: Learning Depth Estimation from Dual-Cameras and Dual-Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational stereo has reached a high level of accuracy, but degrades in\nthe presence of occlusions, repeated textures, and correspondence errors along\nedges. We present a novel approach based on neural networks for depth\nestimation that combines stereo from dual cameras with stereo from a dual-pixel\nsensor, which is increasingly common on consumer cameras. Our network uses a\nnovel architecture to fuse these two sources of information and can overcome\nthe above-mentioned limitations of pure binocular stereo matching. Our method\nprovides a dense depth map with sharp edges, which is crucial for computational\nphotography applications like synthetic shallow-depth-of-field or 3D Photos.\nAdditionally, we avoid the inherent ambiguity due to the aperture problem in\nstereo cameras by designing the stereo baseline to be orthogonal to the\ndual-pixel baseline. We present experiments and comparisons with\nstate-of-the-art approaches to show that our method offers a substantial\nimprovement over previous works.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 15:39:43 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Zhang", "Yinda", ""], ["Wadhwa", "Neal", ""], ["Orts-Escolano", "Sergio", ""], ["H\u00e4ne", "Christian", ""], ["Fanello", "Sean", ""], ["Garg", "Rahul", ""]]}, {"id": "2003.14323", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Jia Deng", "title": "How Useful is Self-Supervised Pretraining for Visual Tasks?", "comments": "To appear in CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have spurred incredible progress in self-supervised\npretraining for vision. We investigate what factors may play a role in the\nutility of these pretraining methods for practitioners. To do this, we evaluate\nvarious self-supervised algorithms across a comprehensive array of synthetic\ndatasets and downstream tasks. We prepare a suite of synthetic data that\nenables an endless supply of annotated images as well as full control over\ndataset difficulty. Our experiments offer insights into how the utility of\nself-supervision changes as the number of available labels grows as well as how\nthe utility changes as a function of the downstream task and the properties of\nthe training data. We also find that linear evaluation does not correlate with\nfinetuning performance. Code and data is available at\n\\href{https://www.github.com/princeton-vl/selfstudy}{github.com/princeton-vl/selfstudy}.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:03:22 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Newell", "Alejandro", ""], ["Deng", "Jia", ""]]}, {"id": "2003.14348", "submitter": "Amirreza Lashkari", "authors": "Tom Ching LingChen, Ava Khonsari, Amirreza Lashkari, Mina Rafi Nazari,\n  Jaspreet Singh Sambee, Mario A. Nascimento", "title": "UniformAugment: A Search-free Probabilistic Data Augmentation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmenting training datasets has been shown to improve the learning\neffectiveness for several computer vision tasks. A good augmentation produces\nan augmented dataset that adds variability while retaining the statistical\nproperties of the original dataset. Some techniques, such as AutoAugment and\nFast AutoAugment, have introduced a search phase to find a set of suitable\naugmentation policies for a given model and dataset. This comes at the cost of\ngreat computational overhead, adding up to several thousand GPU hours. More\nrecently RandAugment was proposed to substantially speedup the search phase by\napproximating the search space by a couple of hyperparameters, but still\nincurring non-negligible cost for tuning those. In this paper we show that,\nunder the assumption that the augmentation space is approximately distribution\ninvariant, a uniform sampling over the continuous space of augmentation\ntransformations is sufficient to train highly effective models. Based on that\nresult we propose UniformAugment, an automated data augmentation approach that\ncompletely avoids a search phase. In addition to discussing the theoretical\nunderpinning supporting our approach, we also use the standard datasets, as\nwell as established models for image classification, to show that\nUniformAugment's effectiveness is comparable to the aforementioned methods,\nwhile still being highly efficient by virtue of not requiring any search.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:32:18 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["LingChen", "Tom Ching", ""], ["Khonsari", "Ava", ""], ["Lashkari", "Amirreza", ""], ["Nazari", "Mina Rafi", ""], ["Sambee", "Jaspreet Singh", ""], ["Nascimento", "Mario A.", ""]]}, {"id": "2003.14363", "submitter": "Khalid Elasnaoui", "authors": "Khalid El Asnaoui, Youness Chawki, Ali Idri", "title": "Automated Methods for Detection and Classification Pneumonia based on\n  X-Ray Images Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers, specialists, and companies around the world are\nrolling out deep learning and image processing-based systems that can fastly\nprocess hundreds of X-Ray and computed tomography (CT) images to accelerate the\ndiagnosis of pneumonia such as SARS, COVID-19, and aid in its containment.\nMedical images analysis is one of the most promising research areas, it\nprovides facilities for diagnosis and making decisions of a number of diseases\nsuch as MERS, COVID-19. In this paper, we present a comparison of recent Deep\nConvolutional Neural Network (DCNN) architectures for automatic binary\nclassification of pneumonia images based fined tuned versions of (VGG16, VGG19,\nDenseNet201, Inception_ResNet_V2, Inception_V3, Resnet50, MobileNet_V2 and\nXception). The proposed work has been tested using chest X-Ray & CT dataset\nwhich contains 5856 images (4273 pneumonia and 1583 normal). As result we can\nconclude that fine-tuned version of Resnet50, MobileNet_V2 and\nInception_Resnet_V2 show highly satisfactory performance with rate of increase\nin training and validation accuracy (more than 96% of accuracy). Unlike CNN,\nXception, VGG16, VGG19, Inception_V3 and DenseNet201 display low performance\n(more than 84% accuracy).\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 16:48:27 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Asnaoui", "Khalid El", ""], ["Chawki", "Youness", ""], ["Idri", "Ali", ""]]}, {"id": "2003.14395", "submitter": "Muhamma Farooq", "authors": "Muhammad Farooq, Abdul Hafeez", "title": "COVID-ResNet: A Deep Learning Framework for Screening of COVID19 from\n  Radiographs", "comments": "6 pages, 3 Figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few months, the novel COVID19 pandemic has spread all over the\nworld. Due to its easy transmission, developing techniques to accurately and\neasily identify the presence of COVID19 and distinguish it from other forms of\nflu and pneumonia is crucial. Recent research has shown that the chest Xrays of\npatients suffering from COVID19 depicts certain abnormalities in the\nradiography. However, those approaches are closed source and not made available\nto the research community for re-producibility and gaining deeper insight. The\ngoal of this work is to build open source and open access datasets and present\nan accurate Convolutional Neural Network framework for differentiating COVID19\ncases from other pneumonia cases. Our work utilizes state of the art training\ntechniques including progressive resizing, cyclical learning rate finding and\ndiscriminative learning rates to training fast and accurate residual neural\nnetworks. Using these techniques, we showed the state of the art results on the\nopen-access COVID-19 dataset. This work presents a 3-step technique to\nfine-tune a pre-trained ResNet-50 architecture to improve model performance and\nreduce training time. We call it COVIDResNet. This is achieved through\nprogressively re-sizing of input images to 128x128x3, 224x224x3, and 229x229x3\npixels and fine-tuning the network at each stage. This approach along with the\nautomatic learning rate selection enabled us to achieve the state of the art\naccuracy of 96.23% (on all the classes) on the COVIDx dataset with only 41\nepochs. This work presented a computationally efficient and highly accurate\nmodel for multi-class classification of three different infection types from\nalong with Normal individuals. This model can help in the early screening of\nCOVID19 cases and help reduce the burden on healthcare systems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:42:28 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Farooq", "Muhammad", ""], ["Hafeez", "Abdul", ""]]}, {"id": "2003.14401", "submitter": "Wentao Zhu", "authors": "Zhuoqian Yang, Wentao Zhu, Wayne Wu, Chen Qian, Qiang Zhou, Bolei\n  Zhou, Chen Change Loy", "title": "TransMoMo: Invariance-Driven Unsupervised Video Motion Retargeting", "comments": "Accepted by CVPR 2020. The first three authors contributed equally.\n  Project page: https://yzhq97.github.io/transmomo/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a lightweight video motion retargeting approach TransMoMo that is\ncapable of transferring motion of a person in a source video realistically to\nanother video of a target person. Without using any paired data for\nsupervision, the proposed method can be trained in an unsupervised manner by\nexploiting invariance properties of three orthogonal factors of variation\nincluding motion, structure, and view-angle. Specifically, with loss functions\ncarefully derived based on invariance, we train an auto-encoder to disentangle\nthe latent representations of such factors given the source and target video\nclips. This allows us to selectively transfer motion extracted from the source\nvideo seamlessly to the target video in spite of structural and view-angle\ndisparities between the source and the target. The relaxed assumption of paired\ndata allows our method to be trained on a vast amount of videos needless of\nmanual annotation of source-target pairing, leading to improved robustness\nagainst large structural variations and extreme motion in videos. We\ndemonstrate the effectiveness of our method over the state-of-the-art methods.\nCode, model and data are publicly available on our project page\n(https://yzhq97.github.io/transmomo).\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:49:53 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 02:49:21 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yang", "Zhuoqian", ""], ["Zhu", "Wentao", ""], ["Wu", "Wayne", ""], ["Qian", "Chen", ""], ["Zhou", "Qiang", ""], ["Zhou", "Bolei", ""], ["Loy", "Chen Change", ""]]}, {"id": "2003.14407", "submitter": "Anne S. Wannenwetsch", "authors": "Anne S. Wannenwetsch, Stefan Roth", "title": "Probabilistic Pixel-Adaptive Refinement Networks", "comments": "To appear at CVPR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoder-decoder networks have found widespread use in various dense\nprediction tasks. However, the strong reduction of spatial resolution in the\nencoder leads to a loss of location information as well as boundary artifacts.\nTo address this, image-adaptive post-processing methods have shown beneficial\nby leveraging the high-resolution input image(s) as guidance data. We extend\nsuch approaches by considering an important orthogonal source of information:\nthe network's confidence in its own predictions. We introduce probabilistic\npixel-adaptive convolutions (PPACs), which not only depend on image guidance\ndata for filtering, but also respect the reliability of per-pixel predictions.\nAs such, PPACs allow for image-adaptive smoothing and simultaneously\npropagating pixels of high confidence into less reliable regions, while\nrespecting object boundaries. We demonstrate their utility in refinement\nnetworks for optical flow and semantic segmentation, where PPACs lead to a\nclear reduction in boundary artifacts. Moreover, our proposed refinement step\nis able to substantially improve the accuracy on various widely used\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:53:21 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Wannenwetsch", "Anne S.", ""], ["Roth", "Stefan", ""]]}, {"id": "2003.14414", "submitter": "Ye Yuan", "authors": "Mariko Isogawa, Ye Yuan, Matthew O'Toole, Kris Kitani", "title": "Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation", "comments": "CVPR 2020. Video: https://youtu.be/4HFulrdmLE8. Project page:\n  https://marikoisogawa.github.io/project/nlos_pose", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for 3D human pose estimation from transient images\n(i.e., a 3D spatio-temporal histogram of photons) acquired by an optical\nnon-line-of-sight (NLOS) imaging system. Our method can perceive 3D human pose\nby `looking around corners' through the use of light indirectly reflected by\nthe environment. We bring together a diverse set of technologies from NLOS\nimaging, human pose estimation and deep reinforcement learning to construct an\nend-to-end data processing pipeline that converts a raw stream of photon\nmeasurements into a full 3D human pose sequence estimate. Our contributions are\nthe design of data representation process which includes (1) a learnable\ninverse point spread function (PSF) to convert raw transient images into a deep\nfeature vector; (2) a neural humanoid control policy conditioned on the\ntransient image feature and learned from interactions with a physics simulator;\nand (3) a data synthesis and augmentation strategy based on depth data that can\nbe transferred to a real-world NLOS imaging system. Our preliminary experiments\nsuggest that our method is able to generalize to real-world NLOS measurement to\nestimate physically-valid 3D human poses.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 17:57:16 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Isogawa", "Mariko", ""], ["Yuan", "Ye", ""], ["O'Toole", "Matthew", ""], ["Kitani", "Kris", ""]]}]