[{"id": "1611.00050", "submitter": "Eder Santana", "authors": "Eder Santana, Matthew Emigh, Pablo Zegers, Jose C Principe", "title": "Exploiting Spatio-Temporal Structure with Recurrent Winner-Take-All\n  Networks", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a convolutional recurrent neural network, with Winner-Take-All\ndropout for high dimensional unsupervised feature learning in multi-dimensional\ntime series. We apply the proposedmethod for object recognition with temporal\ncontext in videos and obtain better results than comparable methods in the\nliterature, including the Deep Predictive Coding Networks previously proposed\nby Chalasani and Principe.Our contributions can be summarized as a scalable\nreinterpretation of the Deep Predictive Coding Networks trained end-to-end with\nbackpropagation through time, an extension of the previously proposed\nWinner-Take-All Autoencoders to sequences in time, and a new technique for\ninitializing and regularizing convolutional-recurrent neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 21:16:46 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 16:01:43 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Santana", "Eder", ""], ["Emigh", "Matthew", ""], ["Zegers", "Pablo", ""], ["Principe", "Jose C", ""]]}, {"id": "1611.00094", "submitter": "Eyrun Eyjolfsdottir", "authors": "Eyrun Eyjolfsdottir, Kristin Branson, Yisong Yue and Pietro Perona", "title": "Learning recurrent representations for hierarchical behavior modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for detecting action patterns from motion sequences\nand modeling the sensory-motor relationship of animals, using a generative\nrecurrent neural network. The network has a discriminative part (classifying\nactions) and a generative part (predicting motion), whose recurrent cells are\nlaterally connected, allowing higher levels of the network to represent high\nlevel phenomena. We test our framework on two types of data, fruit fly behavior\nand online handwriting. Our results show that 1) taking advantage of unlabeled\nsequences, by predicting future motion, significantly improves action detection\nperformance when training labels are scarce, 2) the network learns to represent\nhigh level phenomena such as writer identity and fly gender, without\nsupervision, and 3) simulated motion trajectories, generated by treating motion\nprediction as input to the network, look realistic and may be used to\nqualitatively evaluate whether the model has learnt generative control rules.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 01:03:53 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 23:39:43 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 18:06:10 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Eyjolfsdottir", "Eyrun", ""], ["Branson", "Kristin", ""], ["Yue", "Yisong", ""], ["Perona", "Pietro", ""]]}, {"id": "1611.00135", "submitter": "Jia Li", "authors": "Jia Li, Changqun Xia and Xiaowu Chen", "title": "A Benchmark Dataset and Saliency-guided Stacked Autoencoders for\n  Video-based Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based salient object detection (SOD) has been extensively studied in\nthe past decades. However, video-based SOD is much less explored since there\nlack large-scale video datasets within which salient objects are unambiguously\ndefined and annotated. Toward this end, this paper proposes a video-based SOD\ndataset that consists of 200 videos (64 minutes). In constructing the dataset,\nwe manually annotate all objects and regions over 7,650 uniformly sampled\nkeyframes and collect the eye-tracking data of 23 subjects that free-view all\nvideos. From the user data, we find salient objects in video can be defined as\nobjects that consistently pop-out throughout the video, and objects with such\nattributes can be unambiguously annotated by combining manually annotated\nobject/region masks with eye-tracking data of multiple subjects. To the best of\nour knowledge, it is currently the largest dataset for video-based salient\nobject detection.\n  Based on this dataset, this paper proposes an unsupervised baseline approach\nfor video-based SOD by using saliency-guided stacked autoencoders. In the\nproposed approach, multiple spatiotemporal saliency cues are first extracted at\npixel, superpixel and object levels. With these saliency cues, stacked\nautoencoders are unsupervisedly constructed which automatically infer a\nsaliency score for each pixel by progressively encoding the high-dimensional\nsaliency cues gathered from the pixel and its spatiotemporal neighbors.\nExperimental results show that the proposed unsupervised approach outperforms\n30 state-of-the-art models on the proposed dataset, including 19 image-based &\nclassic (unsupervised or non-deep learning), 6 image-based & deep learning, and\n5 video-based & unsupervised. Moreover, benchmarking results show that the\nproposed dataset is very challenging and has the potential to boost the\ndevelopment of video-based SOD.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 05:48:05 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 07:38:17 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Li", "Jia", ""], ["Xia", "Changqun", ""], ["Chen", "Xiaowu", ""]]}, {"id": "1611.00137", "submitter": "Hailin Shi", "authors": "Hailin Shi, Yang Yang, Xiangyu Zhu, Shengcai Liao, Zhen Lei, Weishi\n  Zheng, Stan Z. Li", "title": "Embedding Deep Metric for Person Re-identication A Study Against Large\n  Variations", "comments": "Published in ECCV2016. arXiv admin note: substantial text overlap\n  with arXiv:1511.07545", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is challenging due to the large variations of pose,\nillumination, occlusion and camera view. Owing to these variations, the\npedestrian data is distributed as highly-curved manifolds in the feature space,\ndespite the current convolutional neural networks (CNN)'s capability of feature\nextraction. However, the distribution is unknown, so it is difficult to use the\ngeodesic distance when comparing two samples. In practice, the current deep\nembedding methods use the Euclidean distance for the training and test. On the\nother hand, the manifold learning methods suggest to use the Euclidean distance\nin the local range, combining with the graphical relationship between samples,\nfor approximating the geodesic distance. From this point of view, selecting\nsuitable positive i.e. intra-class) training samples within a local range is\ncritical for training the CNN embedding, especially when the data has large\nintra-class variations. In this paper, we propose a novel moderate positive\nsample mining method to train robust CNN for person re-identification, dealing\nwith the problem of large variation. In addition, we improve the learning by a\nmetric weight constraint, so that the learned metric has a better\ngeneralization ability. Experiments show that these two strategies are\neffective in learning robust deep metrics for person re-identification, and\naccordingly our deep model significantly outperforms the state-of-the-art\nmethods on several benchmarks of person re-identification. Therefore, the study\npresented in this paper may be useful in inspiring new designs of deep models\nfor person re-identification.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 06:03:48 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Shi", "Hailin", ""], ["Yang", "Yang", ""], ["Zhu", "Xiangyu", ""], ["Liao", "Shengcai", ""], ["Lei", "Zhen", ""], ["Zheng", "Weishi", ""], ["Li", "Stan Z.", ""]]}, {"id": "1611.00142", "submitter": "Binod Bhattarai", "authors": "Binod Bhattarai, Gaurav Sharma, Frederic Jurie", "title": "Deep fusion of visual signatures for client-server facial analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial analysis is a key technology for enabling human-machine interaction.\nIn this context, we present a client-server framework, where a client transmits\nthe signature of a face to be analyzed to the server, and, in return, the\nserver sends back various information describing the face e.g. is the person\nmale or female, is she/he bald, does he have a mustache, etc. We assume that a\nclient can compute one (or a combination) of visual features; from very simple\nand efficient features, like Local Binary Patterns, to more complex and\ncomputationally heavy, like Fisher Vectors and CNN based, depending on the\ncomputing resources available. The challenge addressed in this paper is to\ndesign a common universal representation such that a single merged signature is\ntransmitted to the server, whatever be the type and number of features computed\nby the client, ensuring nonetheless an optimal performance. Our solution is\nbased on learning of a common optimal subspace for aligning the different face\nfeatures and merging them into a universal signature. We have validated the\nproposed method on the challenging CelebA dataset, on which our method\noutperforms existing state-of-the-art methods when rich representation is\navailable at test time, while giving competitive performance when only simple\nsignatures (like LBP) are available at test time due to resource constraints on\nthe client.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 06:57:58 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 10:48:58 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Bhattarai", "Binod", ""], ["Sharma", "Gaurav", ""], ["Jurie", "Frederic", ""]]}, {"id": "1611.00148", "submitter": "Shaul Oron", "authors": "Shaul Oron and Denis Suhanov and Shai Avidan", "title": "Best-Buddies Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Best-Buddies Tracking (BBT) applies the Best-Buddies Similarity measure (BBS)\nto the problem of model-free online tracking. BBS was introduced as a\nsimilarity measure between two point sets and was shown to be very effective\nfor template matching. Originally, BBS was designed to work with point sets of\nequal size, and we propose a modification that lets it handle point sets of\ndifferent size. The modified BBS is better suited to handle scale changes in\nthe template size, as well as support a variable number of template images. We\nembed the modified BBS in a particle filter framework and obtain good results\non a number of standard benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 07:19:50 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Oron", "Shaul", ""], ["Suhanov", "Denis", ""], ["Avidan", "Shai", ""]]}, {"id": "1611.00218", "submitter": "Yashas Annadani", "authors": "Yashas Annadani, D L Rakshith, Soma Biswas", "title": "Sliding Dictionary Based Sparse Representation For Action Recognition", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of action recognition has been in the forefront of research, given\nits applications in gaming, surveillance and health care. In this work, we\npropose a simple, yet very effective approach which works seamlessly for both\noffline and online action recognition using the skeletal joints. We construct a\nsliding dictionary which has the training data along with their time stamps.\nThis is used to compute the sparse coefficients of the input action sequence\nwhich is divided into overlapping windows and each window gives a probability\nscore for each action class. In addition, we compute another simple feature,\nwhich calibrates each of the action sequences to the training sequences, and\nmodels the deviation of the action from the each of the training data. Finally,\na score level fusion of the two heterogeneous but complementary features for\neach window is obtained and the scores for the available windows are\nsuccessively combined to give the confidence scores of each action class. This\nway of combining the scores makes the approach suitable for scenarios where\nonly part of the sequence is available. Extensive experimental evaluation on\nthree publicly available datasets shows the effectiveness of the proposed\napproach for both offline and online action recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 13:29:38 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Annadani", "Yashas", ""], ["Rakshith", "D L", ""], ["Biswas", "Soma", ""]]}, {"id": "1611.00284", "submitter": "Zhenhua Feng", "authors": "Xiaoning Song, Zhen-Hua Feng, Guosheng Hu, Josef Kittler, William\n  Christmas and Xiao-Jun Wu", "title": "Dictionary Integration using 3D Morphable Face Models for Pose-invariant\n  Collaborative-representation-based Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a dictionary integration algorithm using 3D morphable face\nmodels (3DMM) for pose-invariant collaborative-representation-based face\nclassification. To this end, we first fit a 3DMM to the 2D face images of a\ndictionary to reconstruct the 3D shape and texture of each image. The 3D faces\nare used to render a number of virtual 2D face images with arbitrary pose\nvariations to augment the training data, by merging the original and rendered\nvirtual samples to create an extended dictionary. Second, to reduce the\ninformation redundancy of the extended dictionary and improve the sparsity of\nreconstruction coefficient vectors using collaborative-representation-based\nclassification (CRC), we exploit an on-line elimination scheme to optimise the\nextended dictionary by identifying the most representative training samples for\na given query. The final goal is to perform pose-invariant face classification\nusing the proposed dictionary integration method and the on-line pruning\nstrategy under the CRC framework. Experimental results obtained for a set of\nwell-known face datasets demonstrate the merits of the proposed method,\nespecially its robustness to pose variations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 16:06:07 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 18:22:31 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 16:27:37 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Song", "Xiaoning", ""], ["Feng", "Zhen-Hua", ""], ["Hu", "Guosheng", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1611.00287", "submitter": "Li-Hao Yeh", "authors": "Li-Hao Yeh, Lei Tian, and Laura Waller", "title": "Structured illumination microscopy with unknown patterns and a\n  statistical prior", "comments": null, "journal-ref": "Biomed. Opt. Express 8, 695-711 (2017)", "doi": "10.1364/BOE.8.000695", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured illumination microscopy (SIM) improves resolution by\ndown-modulating high-frequency information of an object to fit within the\npassband of the optical system. Generally, the reconstruction process requires\nprior knowledge of the illumination patterns, which implies a well-calibrated\nand aberration-free system. Here, we propose a new \\textit{algorithmic\nself-calibration} strategy for SIM that does not need to know the exact\npatterns {\\it a priori}, but only their covariance. The algorithm, termed\nPE-SIMS, includes a Pattern-Estimation (PE) step requiring the uniformity of\nthe sum of the illumination patterns and a SIM reconstruction procedure using a\nStatistical prior (SIMS). Additionally, we perform a pixel reassignment process\n(SIMS-PR) to enhance the reconstruction quality. We achieve 2$\\times$ better\nresolution than a conventional widefield microscope, while remaining\ninsensitive to aberration-induced pattern distortion and robust against\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 14:55:42 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 07:12:09 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Yeh", "Li-Hao", ""], ["Tian", "Lei", ""], ["Waller", "Laura", ""]]}, {"id": "1611.00393", "submitter": "Tatiana Tommasi", "authors": "Tatiana Tommasi, Arun Mallya, Bryan Plummer, Svetlana Lazebnik,\n  Alexander C. Berg, Tamara L. Berg", "title": "Combining Multiple Cues for Visual Madlibs Question Answering", "comments": "submitted to IJCV -- under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for answering fill-in-the-blank multiple\nchoice questions from the Visual Madlibs dataset. Instead of generic and\ncommonly used representations trained on the ImageNet classification task, our\napproach employs a combination of networks trained for specialized tasks such\nas scene recognition, person activity classification, and attribute prediction.\nWe also present a method for localizing phrases from candidate answers in order\nto provide spatial support for feature extraction. We map each of these\nfeatures, together with candidate answers, to a joint embedding space through\nnormalized canonical correlation analysis (nCCA). Finally, we solve an\noptimization problem to learn to combine scores from nCCA models trained on\nmultiple cues to select the best answer. Extensive experimental results show a\nsignificant improvement over the previous state of the art and confirm that\nanswering questions from a wide range of types benefits from examining a\nvariety of image cues and carefully choosing the spatial support for feature\nextraction.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 21:09:16 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 09:44:14 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 23:28:36 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Tommasi", "Tatiana", ""], ["Mallya", "Arun", ""], ["Plummer", "Bryan", ""], ["Lazebnik", "Svetlana", ""], ["Berg", "Alexander C.", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1611.00421", "submitter": "Michal Januszewski", "authors": "Micha{\\l} Januszewski, Jeremy Maitin-Shepard, Peter Li, J\\\"orgen\n  Kornfeld, Winfried Denk, Viren Jain", "title": "Flood-Filling Networks", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art image segmentation algorithms generally consist of at least\ntwo successive and distinct computations: a boundary detection process that\nuses local image information to classify image locations as boundaries between\nobjects, followed by a pixel grouping step such as watershed or connected\ncomponents that clusters pixels into segments. Prior work has varied the\ncomplexity and approach employed in these two steps, including the\nincorporation of multi-layer neural networks to perform boundary prediction,\nand the use of global optimizations during pixel clustering. We propose a\nunified and end-to-end trainable machine learning approach, flood-filling\nnetworks, in which a recurrent 3d convolutional network directly produces\nindividual segments from a raw image. The proposed approach robustly segments\nimages with an unknown and variable number of objects as well as highly\nvariable object sizes. We demonstrate the approach on a challenging 3d image\nsegmentation task, connectomic reconstruction from volume electron microscopy\ndata, on which flood-filling neural networks substantially improve accuracy\nover other state-of-the-art methods. The proposed approach can replace complex\nmulti-step segmentation pipelines with a single neural network that is learned\nend-to-end.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 23:17:55 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Januszewski", "Micha\u0142", ""], ["Maitin-Shepard", "Jeremy", ""], ["Li", "Peter", ""], ["Kornfeld", "J\u00f6rgen", ""], ["Denk", "Winfried", ""], ["Jain", "Viren", ""]]}, {"id": "1611.00448", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:32:05 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00454", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in\n  the Blanks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid methods that utilize both content and rating information are commonly\nused in many recommender systems. However, most of them use either handcrafted\nfeatures or the bag-of-words representation as a surrogate for the content\ninformation but they are neither effective nor natural enough. To address this\nproblem, we develop a collaborative recurrent autoencoder (CRAE) which is a\ndenoising recurrent autoencoder (DRAE) that models the generation of content\nsequences in the collaborative filtering (CF) setting. The model generalizes\nrecent advances in recurrent deep learning from i.i.d. input to non-i.i.d.\n(CF-based) input and provides a new denoising scheme along with a novel\nlearnable pooling scheme for the recurrent autoencoder. To do this, we first\ndevelop a hierarchical Bayesian model for the DRAE and then generalize it to\nthe CF setting. The synergy between denoising and CF enables CRAE to make\naccurate recommendations while learning to fill in the blanks in sequences.\nExperiments on real-world datasets from different domains (CiteULike and\nNetflix) show that, by jointly modeling the order-aware generation of sequences\nfor the content information and performing CF for the ratings, CRAE is able to\nsignificantly outperform the state of the art on both the recommendation task\nbased on ratings and the sequence generation task based on content information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:49:44 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00468", "submitter": "Xiao Chu", "authors": "Xiao Chu, Wanli Ouyang, Hongsheng Li and Xiaogang Wang", "title": "CRF-CNN: Modeling Structured Information in Human Pose Estimation", "comments": "NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNN) have achieved great success. On the\nother hand, modeling structural information has been proved critical in many\nvision problems. It is of great interest to integrate them effectively. In a\nclassical neural network, there is no message passing between neurons in the\nsame layer. In this paper, we propose a CRF-CNN framework which can\nsimultaneously model structural information in both output and hidden feature\nlayers in a probabilistic way, and it is applied to human pose estimation. A\nmessage passing scheme is proposed, so that in various layers each body joint\nreceives messages from all the others in an efficient way. Such message passing\ncan be implemented with convolution between features maps in the same layer,\nand it is also integrated with feedforward propagation in neural networks.\nFinally, a neural network implementation of end-to-end learning CRF-CNN is\nprovided. Its effectiveness is demonstrated through experiments on two\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 04:42:40 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Chu", "Xiao", ""], ["Ouyang", "Wanli", ""], ["Li", "Hongsheng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1611.00471", "submitter": "Hyeonseob Nam", "authors": "Hyeonseob Nam, Jung-Woo Ha, Jeonghee Kim", "title": "Dual Attention Networks for Multimodal Reasoning and Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Dual Attention Networks (DANs) which jointly leverage visual and\ntextual attention mechanisms to capture fine-grained interplay between vision\nand language. DANs attend to specific regions in images and words in text\nthrough multiple steps and gather essential information from both modalities.\nBased on this framework, we introduce two types of DANs for multimodal\nreasoning and matching, respectively. The reasoning model allows visual and\ntextual attentions to steer each other during collaborative inference, which is\nuseful for tasks such as Visual Question Answering (VQA). In addition, the\nmatching model exploits the two attention mechanisms to estimate the similarity\nbetween images and sentences by focusing on their shared semantics. Our\nextensive experiments validate the effectiveness of DANs in combining vision\nand language, achieving the state-of-the-art performance on public benchmarks\nfor VQA and image-text matching.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 05:17:03 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 05:12:02 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Nam", "Hyeonseob", ""], ["Ha", "Jung-Woo", ""], ["Kim", "Jeonghee", ""]]}, {"id": "1611.00591", "submitter": "Kshiteej Sheth Jitesh", "authors": "Kshiteej Sheth", "title": "Deep Neural Networks for HDR imaging", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose novel methods of solving two tasks using Convolutional Neural\nNetworks, firstly the task of generating HDR map of a static scene using\ndifferently exposed LDR images of the scene captured using conventional cameras\nand secondly the task of finding an optimal tone mapping operator that would\ngive a better score on the TMQI metric compared to the existing methods. We\nquantitatively show the performance of our networks and illustrate the cases\nwhere our networks performs good as well as bad.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 16:20:13 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Sheth", "Kshiteej", ""]]}, {"id": "1611.00684", "submitter": "Mina Nouredanesh", "authors": "Mina Nouredanesh, Andrew McCormick, Sunil L. Kukreja and James Tung", "title": "Wearable Vision Detection of Environmental Fall Risks using\n  Convolutional Neural Networks", "comments": "Accepted paper-The 38th Annual International Conference of the IEEE\n  Engineering in Medicine and Biology Society (EMBC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method to detect environmental hazards related to a fall\nrisk using a mobile vision system is proposed. First-person perspective videos\nare proposed to provide objective evidence on cause and circumstances of\nperturbed balance during activities of daily living, targeted to seniors. A\nclassification problem was defined with 12 total classes of potential fall\nrisks, including slope changes (e.g., stairs, curbs, ramps) and surfaces (e.g.,\ngravel, grass, concrete). Data was collected using a chest-mounted GoPro\ncamera. We developed a convolutional neural network for automatic feature\nextraction, reduction, and classification of frames. Initial results, with a\nmean square error of 8%, are promising.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:54:39 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Nouredanesh", "Mina", ""], ["McCormick", "Andrew", ""], ["Kukreja", "Sunil L.", ""], ["Tung", "James", ""]]}, {"id": "1611.00800", "submitter": "Andy Jinhua Ma", "authors": "Frodo Kin Sun Chan, Andy J Ma, Pong C Yuen, Terry Cheuk-Fung Yip,\n  Yee-Kit Tse, Vincent Wai-Sun Wong and Grace Lai-Hung Wong", "title": "Temporal Matrix Completion with Locally Linear Latent Factors for\n  Medical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular medical records are useful for medical practitioners to analyze and\nmonitor patient health status especially for those with chronic disease, but\nsuch records are usually incomplete due to unpunctuality and absence of\npatients. In order to resolve the missing data problem over time, tensor-based\nmodel is suggested for missing data imputation in recent papers because this\napproach makes use of low rank tensor assumption for highly correlated data.\nHowever, when the time intervals between records are long, the data correlation\nis not high along temporal direction and such assumption is not valid. To\naddress this problem, we propose to decompose a matrix with missing data into\nits latent factors. Then, the locally linear constraint is imposed on these\nfactors for matrix completion in this paper. By using a publicly available\ndataset and two medical datasets collected from hospital, experimental results\nshow that the proposed algorithm achieves the best performance by comparing\nwith the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 12:02:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Chan", "Frodo Kin Sun", ""], ["Ma", "Andy J", ""], ["Yuen", "Pong C", ""], ["Yip", "Terry Cheuk-Fung", ""], ["Tse", "Yee-Kit", ""], ["Wong", "Vincent Wai-Sun", ""], ["Wong", "Grace Lai-Hung", ""]]}, {"id": "1611.00822", "submitter": "Evgeniya Ustinova", "authors": "Evgeniya Ustinova, Victor Lempitsky", "title": "Learning Deep Embeddings with Histogram Loss", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest a loss for learning deep embeddings. The new loss does not\nintroduce parameters that need to be tuned and results in very good embeddings\nacross a range of datasets and problems. The loss is computed by estimating two\ndistribution of similarities for positive (matching) and negative\n(non-matching) sample pairs, and then computing the probability of a positive\npair to have a lower similarity score than a negative pair based on the\nestimated similarity distributions. We show that such operations can be\nperformed in a simple and piecewise-differentiable manner using 1D histograms\nwith soft assignment operations. This makes the proposed loss suitable for\nlearning deep embeddings using stochastic optimization. In the experiments, the\nnew loss performs favourably compared to recently proposed alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 21:48:32 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Ustinova", "Evgeniya", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1611.00838", "submitter": "Da Tang", "authors": "Da Tang and Tony Jebara", "title": "Initialization and Coordinate Optimization for Multi-way Matching", "comments": "Artificial Intelligence and Statistics (AISTATS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of consistently matching multiple sets of elements to\neach other, which is a common task in fields such as computer vision. To solve\nthe underlying NP-hard objective, existing methods often relax or approximate\nit, but end up with unsatisfying empirical performance due to a misaligned\nobjective. We propose a coordinate update algorithm that directly optimizes the\ntarget objective. By using pairwise alignment information to build an\nundirected graph and initializing the permutation matrices along the edges of\nits Maximum Spanning Tree, our algorithm successfully avoids bad local optima.\nTheoretically, with high probability our algorithm guarantees an optimal\nsolution under reasonable noise assumptions. Empirically, our algorithm\nconsistently and significantly outperforms existing methods on several\nbenchmark tasks on real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 23:12:05 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 20:10:26 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 07:32:24 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 14:25:37 GMT"}, {"version": "v5", "created": "Thu, 18 Jul 2019 05:34:20 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Tang", "Da", ""], ["Jebara", "Tony", ""]]}, {"id": "1611.00847", "submitter": "Leslie Smith", "authors": "Leslie N. Smith and Nicholay Topin", "title": "Deep Convolutional Neural Network Design Patterns", "comments": "Submitted as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in the deep learning field has produced a plethora of new\narchitectures. At the same time, a growing number of groups are applying deep\nlearning to new applications. Some of these groups are likely to be composed of\ninexperienced deep learning practitioners who are baffled by the dizzying array\nof architecture choices and therefore opt to use an older architecture (i.e.,\nAlexnet). Here we attempt to bridge this gap by mining the collective knowledge\ncontained in recent deep learning research to discover underlying principles\nfor designing neural network architectures. In addition, we describe several\narchitectural innovations, including Fractal of FractalNet network, Stagewise\nBoosting Networks, and Taylor Series Networks (our Caffe code and prototxt\nfiles is available at https://github.com/iPhysicist/CNNDesignPatterns). We hope\nothers are inspired to build on our preliminary work.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 23:48:04 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 00:49:46 GMT"}, {"version": "v3", "created": "Mon, 14 Nov 2016 14:10:41 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Smith", "Leslie N.", ""], ["Topin", "Nicholay", ""]]}, {"id": "1611.00850", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan, Michael J. Black", "title": "Optical Flow Estimation using a Spatial Pyramid Network", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We learn to compute optical flow by combining a classical spatial-pyramid\nformulation with deep learning. This estimates large motions in a\ncoarse-to-fine approach by warping one image of a pair at each pyramid level by\nthe current flow estimate and computing an update to the flow. Instead of the\nstandard minimization of an objective function at each pyramid level, we train\none deep network per level to compute the flow update. Unlike the recent\nFlowNet approach, the networks do not need to deal with large motions; these\nare dealt with by the pyramid. This has several advantages. First, our Spatial\nPyramid Network (SPyNet) is much simpler and 96% smaller than FlowNet in terms\nof model parameters. This makes it more efficient and appropriate for embedded\napplications. Second, since the flow at each pyramid level is small (< 1\npixel), a convolutional approach applied to pairs of warped images is\nappropriate. Third, unlike FlowNet, the learned convolution filters appear\nsimilar to classical spatio-temporal filters, giving insight into the method\nand how to improve it. Our results are more accurate than FlowNet on most\nstandard benchmarks, suggesting a new direction of combining classical flow\nmethods with deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 00:10:42 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 19:01:19 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Ranjan", "Anurag", ""], ["Black", "Michael J.", ""]]}, {"id": "1611.00851", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Swami Sankaranarayanan, Carlos D. Castillo and Rama\n  Chellappa", "title": "An All-In-One Convolutional Neural Network for Face Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multi-purpose algorithm for simultaneous face detection, face\nalignment, pose estimation, gender recognition, smile detection, age estimation\nand face recognition using a single deep convolutional neural network (CNN).\nThe proposed method employs a multi-task learning framework that regularizes\nthe shared parameters of CNN and builds a synergy among different domains and\ntasks. Extensive experiments show that the network has a better understanding\nof face and achieves state-of-the-art result for most of these tasks.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 00:17:48 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Sankaranarayanan", "Swami", ""], ["Castillo", "Carlos D.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1611.00931", "submitter": "Soumyabrata Dev", "authors": "Soumyabrata Dev, Florian M. Savoy, Yee Hui Lee, Stefan Winkler", "title": "Rough Set Based Color Channel Selection", "comments": "Accepted in IEEE Geoscience and Remote Sensing Letters, 2016", "journal-ref": null, "doi": "10.1109/LGRS.2016.2625303", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Color channel selection is essential for accurate segmentation of sky and\nclouds in images obtained from ground-based sky cameras. Most prior works in\ncloud segmentation use threshold based methods on color channels selected in an\nad-hoc manner. In this letter, we propose the use of rough sets for color\nchannel selection in visible-light images. Our proposed approach assesses color\nchannels with respect to their contribution for segmentation, and identifies\nthe most effective ones.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 09:41:38 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Dev", "Soumyabrata", ""], ["Savoy", "Florian M.", ""], ["Lee", "Yee Hui", ""], ["Winkler", "Stefan", ""]]}, {"id": "1611.00939", "submitter": "Adrian Jarabo", "authors": "Adrian Jarabo, Belen Masia, Julio Marco, Diego Gutierrez", "title": "Recent Advances in Transient Imaging: A Computer Graphics and Vision\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient imaging has recently made a huge impact in the computer graphics\nand computer vision fields. By capturing, reconstructing, or simulating light\ntransport at extreme temporal resolutions, researchers have proposed novel\ntechniques to show movies of light in motion, see around corners, detect\nobjects in highly-scattering media, or infer material properties from a\ndistance, to name a few. The key idea is to leverage the wealth of information\nin the temporal domain at the pico or nanosecond resolution, information\nusually lost during the capture-time temporal integration. This paper presents\nrecent advances in this field of transient imaging from a graphics and vision\nperspective, including capture techniques, analysis, applications and\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:11:10 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Jarabo", "Adrian", ""], ["Masia", "Belen", ""], ["Marco", "Julio", ""], ["Gutierrez", "Diego", ""]]}, {"id": "1611.00960", "submitter": "Vania Estrela Dr.", "authors": "Vania V. Estrela, Matthias O. Franz, Ricardo T. Lopes, G. P. De Araujo", "title": "Adaptive mixed norm optical flow estimation", "comments": "8 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1403.7365", "journal-ref": "Proc. SPIE 5960, Visual Communications and Image Processing 2005,\n  59603W, July 31, 2006, Beijing, China", "doi": "10.1117/12.632674", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pel-recursive computation of 2-D optical flow has been extensively\nstudied in computer vision to estimate motion from image sequences, but it\nstill raises a wealth of issues, such as the treatment of outliers, motion\ndiscontinuities and occlusion. It relies on spatio-temporal brightness\nvariations due to motion. Our proposed adaptive regularized approach deals with\nthese issues within a common framework. It relies on the use of a data-driven\ntechnique called Mixed Norm (MN) to estimate the best motion vector for a given\npixel. In our model, various types of noise can be handled, representing\ndifferent sources of error. The motion vector estimation takes into\nconsideration local image properties and it results from the minimization of a\nmixed norm functional with a regularization parameter depending on the\nkurtosis. This parameter determines the relative importance of the fourth norm\nand makes the functional convex. The main advantage of the developed procedure\nis that no knowledge of the noise distribution is necessary. Experiments\nindicate that this approach provides robust estimates of the optical flow.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 11:32:46 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Estrela", "Vania V.", ""], ["Franz", "Matthias O.", ""], ["Lopes", "Ricardo T.", ""], ["De Araujo", "G. P.", ""]]}, {"id": "1611.01195", "submitter": "Shusil Dangi", "authors": "Shusil Dangi, Nathan Cahill, Cristian A. Linte", "title": "Integrating Atlas and Graph Cut Methods for LV Segmentation from Cardiac\n  Cine MRI", "comments": "Statistical Atlases and Computational Modelling of Heart workshop\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) has evolved as a clinical standard-of-care\nimaging modality for cardiac morphology, function assessment, and guidance of\ncardiac interventions. All these applications rely on accurate extraction of\nthe myocardial tissue and blood pool from the imaging data. Here we propose a\nframework for left ventricle (LV) segmentation from cardiac cine-MRI. First, we\nsegment the LV blood pool using iterative graph cuts, and subsequently use this\ninformation to segment the myocardium. We formulate the segmentation procedure\nas an energy minimization problem in a graph subject to the shape prior\nobtained by label propagation from an average atlas using affine registration.\nThe proposed framework has been validated on 30 patient cardiac cine-MRI\ndatasets available through the STACOM LV segmentation challenge and yielded\nfast, robust, and accurate segmentation results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 21:12:55 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Dangi", "Shusil", ""], ["Cahill", "Nathan", ""], ["Linte", "Cristian A.", ""]]}, {"id": "1611.01230", "submitter": "Jie Sun", "authors": "Jie Sun, Fernando J. Quevedo, Erik Bollt", "title": "Bayesian Optical Flow with Uncertainty Quantification", "comments": "Published 20 August 2018", "journal-ref": "Inverse Problems 34, 105008 (2018)", "doi": "10.1088/1361-6420/aad7cc", "report-no": null, "categories": "stat.ML cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow refers to the visual motion observed between two consecutive\nimages. Since the degree of freedom is typically much larger than the\nconstraints imposed by the image observations, the straightforward formulation\nof optical flow as an inverse problem is ill-posed. Standard approaches to\ndetermine optical flow rely on formulating and solving an optimization problem\nthat contains both a data fidelity term and a regularization term, the latter\neffectively resolves the otherwise ill-posedness of the inverse problem. In\nthis work, we depart from the deterministic formalism, and instead treat\noptical flow as a statistical inverse problem. We discuss how a classical\noptical flow solution can be interpreted as a point estimate in this more\ngeneral framework. The statistical approach, whose \"solution\" is a distribution\nof flow fields, which we refer to as Bayesian optical flow, allows not only\n\"point\" estimates (e.g., the computation of average flow field), but also\nstatistical estimates (e.g., quantification of uncertainty) that are beyond any\nstandard method for optical flow. As application, we benchmark Bayesian optical\nflow together with uncertainty quantification using several types of prescribed\nground-truth flow fields and images.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 00:29:02 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 11:41:49 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Sun", "Jie", ""], ["Quevedo", "Fernando J.", ""], ["Bollt", "Erik", ""]]}, {"id": "1611.01236", "submitter": "Alexey Kurakin", "authors": "Alexey Kurakin, Ian Goodfellow, Samy Bengio", "title": "Adversarial Machine Learning at Scale", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are malicious inputs designed to fool machine learning\nmodels. They often transfer from one model to another, allowing attackers to\nmount black box attacks without knowledge of the target model's parameters.\nAdversarial training is the process of explicitly training a model on\nadversarial examples, in order to make it more robust to attack or to reduce\nits test error on clean inputs. So far, adversarial training has primarily been\napplied to small problems. In this research, we apply adversarial training to\nImageNet. Our contributions include: (1) recommendations for how to succesfully\nscale adversarial training to large models and datasets, (2) the observation\nthat adversarial training confers robustness to single-step attack methods, (3)\nthe finding that multi-step attack methods are somewhat less transferable than\nsingle-step attack methods, so single-step attacks are the best for mounting\nblack-box attacks, and (4) resolution of a \"label leaking\" effect that causes\nadversarially trained models to perform better on adversarial examples than on\nclean examples, because the adversarial example construction process uses the\ntrue label and the model can learn to exploit regularities in the construction\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:11:02 GMT"}, {"version": "v2", "created": "Sat, 11 Feb 2017 00:15:46 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Kurakin", "Alexey", ""], ["Goodfellow", "Ian", ""], ["Bengio", "Samy", ""]]}, {"id": "1611.01260", "submitter": "Pedro H. P. Savarese", "authors": "Pedro H. P. Savarese and Leonardo O. Mazza and Daniel R. Figueiredo", "title": "Learning Identity Mappings with Residual Gates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new layer design by adding a linear gating mechanism to shortcut\nconnections. By using a scalar parameter to control each gate, we provide a way\nto learn identity mappings by optimizing only one parameter. We build upon the\nmotivation behind Residual Networks, where a layer is reformulated in order to\nmake learning identity mappings less problematic to the optimizer. The\naugmentation introduces only one extra parameter per layer, and provides easier\noptimization by making degeneration into identity mappings simpler. We propose\na new model, the Gated Residual Network, which is the result when augmenting\nResidual Networks. Experimental results show that augmenting layers provides\nbetter optimization, increased performance, and more layer independence. We\nevaluate our method on MNIST using fully-connected networks, showing empirical\nindications that our augmentation facilitates the optimization of deep models,\nand that it provides high tolerance to full layer removal: the model retains\nover 90% of its performance even after half of its layers have been randomly\nremoved. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated\nResNets, achieving 3.65% and 18.27% error, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 04:34:38 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 01:36:47 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Savarese", "Pedro H. P.", ""], ["Mazza", "Leonardo O.", ""], ["Figueiredo", "Daniel R.", ""]]}, {"id": "1611.01298", "submitter": "Vania Estrela Dr.", "authors": "Vania V. Estrela, Luis A. Rivera, Paulo C. Beggio, Ricardo T. Lopes", "title": "Regularized Pel-Recursive Motion Estimation Using Generalized\n  Cross-Validation and Spatial Adaptation", "comments": "8 pages, 6 figures in Proceedings of the XVI Brazilian Symposium on\n  Computer Graphics and Image Processing, 2003. SIBGRAPI 2003. IEEE. arXiv\n  admin note: text overlap with arXiv:1403.7365, arXiv:1611.00960", "journal-ref": null, "doi": "10.1109/SIBGRA.2003.1241027", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The computation of 2-D optical flow by means of regularized pel-recursive\nalgorithms raises a host of issues, which include the treatment of outliers,\nmotion discontinuities and occlusion among other problems. We propose a new\napproach which allows us to deal with these issues within a common framework.\nOur approach is based on the use of a technique called Generalized\nCross-Validation to estimate the best regularization scheme for a given pixel.\nIn our model, the regularization parameter is a matrix whose entries can\naccount for diverse sources of error. The estimation of the motion vectors\ntakes into consideration local properties of the image following a spatially\nadaptive approach where each moving pixel is supposed to have its own\nregularization matrix. Preliminary experiments indicate that this approach\nprovides robust estimates of the optical flow.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 09:51:21 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Estrela", "Vania V.", ""], ["Rivera", "Luis A.", ""], ["Beggio", "Paulo C.", ""], ["Lopes", "Ricardo T.", ""]]}, {"id": "1611.01331", "submitter": "Leon Sixt", "authors": "Leon Sixt, Benjamin Wild, Tim Landgraf", "title": "RenderGAN: Generating Realistic Labeled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neuronal Networks (DCNNs) are showing remarkable\nperformance on many computer vision tasks. Due to their large parameter space,\nthey require many labeled samples when trained in a supervised setting. The\ncosts of annotating data manually can render the use of DCNNs infeasible. We\npresent a novel framework called RenderGAN that can generate large amounts of\nrealistic, labeled images by combining a 3D model and the Generative\nAdversarial Network framework. In our approach, image augmentations (e.g.\nlighting, background, and detail) are learned from unlabeled data such that the\ngenerated images are strikingly realistic while preserving the labels known\nfrom the 3D model. We apply the RenderGAN framework to generate images of\nbarcode-like markers that are attached to honeybees. Training a DCNN on data\ngenerated by the RenderGAN yields considerably better performance than training\nit on various baselines.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 11:20:38 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 06:28:15 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 17:23:28 GMT"}, {"version": "v4", "created": "Fri, 9 Dec 2016 13:18:13 GMT"}, {"version": "v5", "created": "Thu, 12 Jan 2017 13:37:26 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Sixt", "Leon", ""], ["Wild", "Benjamin", ""], ["Landgraf", "Tim", ""]]}, {"id": "1611.01390", "submitter": "Jonathan Vacher", "authors": "Jonathan Vacher, Andrew Isaac Meso, Laurent U. Perrinet and Gabriel\n  Peyr\\'e", "title": "Bayesian Modeling of Motion Perception using Dynamical Stochastic\n  Textures", "comments": "article+supplementary, 34+5 pages, 10+1 figures, accepted to Neural\n  Computation. arXiv admin note: text overlap with arXiv:1511.02705", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common practice to account for psychophysical biases in vision is to frame\nthem as consequences of a dynamic process relying on optimal inference with\nrespect to a generative model. The present study details the complete\nformulation of such a generative model intended to probe visual motion\nperception with a dynamic texture model. It is first derived in a set of\naxiomatic steps constrained by biological plausibility. We extend previous\ncontributions by detailing three equivalent formulations of this texture model.\nFirst, the composite dynamic textures are constructed by the random aggregation\nof warped patterns, which can be viewed as 3D Gaussian fields. Secondly, these\ntextures are cast as solutions to a stochastic partial differential equation\n(sPDE). This essential step enables real time, on-the-fly texture synthesis\nusing time-discretized auto-regressive processes. It also allows for the\nderivation of a local motion-energy model, which corresponds to the\nlog-likelihood of the probability density. The log-likelihoods are essential\nfor the construction of a Bayesian inference framework. We use the dynamic\ntexture model to psychophysically probe speed perception in humans using\nzoom-like changes in the spatial frequency content of the stimulus. The human\ndata replicates previous findings showing perceived speed to be positively\nbiased by spatial frequency increments. A Bayesian observer who combines a\nGaussian likelihood centered at the true speed and a spatial frequency\ndependent width with a \"slow speed prior\" successfully accounts for the\nperceptual bias. More precisely, the bias arises from a decrease in the\nobserver's likelihood width estimated from the experiments as the spatial\nfrequency increases. Such a trend is compatible with the trend of the dynamic\ntexture likelihood width.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 21:20:03 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 21:02:49 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Vacher", "Jonathan", ""], ["Meso", "Andrew Isaac", ""], ["Perrinet", "Laurent U.", ""], ["Peyr\u00e9", "Gabriel", ""]]}, {"id": "1611.01408", "submitter": "Mariano Tepper", "authors": "Mariano Tepper and Guillermo Sapiro", "title": "Nonnegative Matrix Underapproximation for Robust Multiple Model Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a highly efficient algorithm to address the\nnonnegative matrix underapproximation (NMU) problem, i.e., nonnegative matrix\nfactorization (NMF) with an additional underapproximation constraint. NMU\nresults are interesting as, compared to traditional NMF, they present\nadditional sparsity and part-based behavior, explaining unique data features.\nTo show these features in practice, we first present an application to the\nanalysis of climate data. We then present an NMU-based algorithm to robustly\nfit multiple parametric models to a dataset. The proposed approach delivers\nstate-of-the-art results for the estimation of multiple fundamental matrices\nand homographies, outperforming other alternatives in the literature and\nexemplifying the use of efficient NMU computations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 15:09:24 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 14:11:37 GMT"}, {"version": "v3", "created": "Thu, 24 Nov 2016 13:12:14 GMT"}, {"version": "v4", "created": "Thu, 6 Apr 2017 15:16:51 GMT"}, {"version": "v5", "created": "Mon, 10 Apr 2017 12:15:40 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Tepper", "Mariano", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1611.01421", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Mohammad Ganjtabesh, Simon J Thorpe,\n  Timoth\\'ee Masquelier", "title": "STDP-based spiking deep convolutional neural networks for object\n  recognition", "comments": null, "journal-ref": "Neural Networks 2018", "doi": "10.1016/j.neunet.2017.12.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have shown that spike-timing-dependent plasticity (STDP) can\nbe used in spiking neural networks (SNN) to extract visual features of low or\nintermediate complexity in an unsupervised manner. These studies, however, used\nrelatively shallow architectures, and only one layer was trainable. Another\nline of research has demonstrated - using rate-based neural networks trained\nwith back-propagation - that having many layers increases the recognition\nrobustness, an approach known as deep learning. We thus designed a deep SNN,\ncomprising several convolutional (trainable with STDP) and pooling layers. We\nused a temporal coding scheme where the most strongly activated neurons fire\nfirst, and less activated neurons fire later or not at all. The network was\nexposed to natural images. Thanks to STDP, neurons progressively learned\nfeatures corresponding to prototypical patterns that were both salient and\nfrequent. Only a few tens of examples per category were required and no label\nwas needed. After learning, the complexity of the extracted features increased\nalong the hierarchy, from edge detectors in the first layer to object\nprototypes in the last layer. Coding was very sparse, with only a few thousands\nspikes per image, and in some cases the object category could be reasonably\nwell inferred from the activity of a single higher-order neuron. More\ngenerally, the activity of a few hundreds of such neurons contained robust\ncategory information, as demonstrated using a classifier on Caltech 101,\nETH-80, and MNIST databases. We also demonstrate the superiority of STDP over\nother unsupervised techniques such as random crops (HMAX) or auto-encoders.\nTaken together, our results suggest that the combination of STDP with latency\ncoding may be a key to understanding the way that the primate visual system\nlearns, its remarkable processing speed and its low energy consumption.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 15:25:13 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 14:28:09 GMT"}, {"version": "v3", "created": "Mon, 25 Dec 2017 12:57:57 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ganjtabesh", "Mohammad", ""], ["Thorpe", "Simon J", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1611.01484", "submitter": "Ankan Bansal", "authors": "Ankan Bansal, Anirudh Nanduri, Carlos Castillo, Rajeev Ranjan, Rama\n  Chellappa", "title": "UMDFaces: An Annotated Face Dataset for Training Deep Networks", "comments": "Updates: Verified keypoints, removed duplicate subjects, released\n  test protocol", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in face detection (including keypoint detection), and\nrecognition is mainly being driven by (i) deeper convolutional neural network\narchitectures, and (ii) larger datasets. However, most of the large datasets\nare maintained by private companies and are not publicly available. The\nacademic computer vision community needs larger and more varied datasets to\nmake further progress.\n  In this paper we introduce a new face dataset, called UMDFaces, which has\n367,888 annotated faces of 8,277 subjects. We also introduce a new face\nrecognition evaluation protocol which will help advance the state-of-the-art in\nthis area. We discuss how a large dataset can be collected and annotated using\nhuman annotators and deep networks. We provide human curated bounding boxes for\nfaces. We also provide estimated pose (roll, pitch and yaw), locations of\ntwenty-one key-points and gender information generated by a pre-trained neural\nnetwork. In addition, the quality of keypoint annotations has been verified by\nhumans for about 115,000 images. Finally, we compare the quality of the dataset\nwith other publicly available face datasets at similar scales.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:37:41 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 08:00:42 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Bansal", "Ankan", ""], ["Nanduri", "Anirudh", ""], ["Castillo", "Carlos", ""], ["Ranjan", "Rajeev", ""], ["Chellappa", "Rama", ""]]}, {"id": "1611.01584", "submitter": "Brandon Smith", "authors": "Brandon M. Smith and Charles R. Dyer", "title": "Efficient Branching Cascaded Regression for Face Alignment under\n  Significant Head Rotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much interest in face alignment in recent years, the large majority\nof work has focused on near-frontal faces. Algorithms typically break down on\nprofile faces, or are too slow for real-time applications. In this work we\npropose an efficient approach to face alignment that can handle 180 degrees of\nhead rotation in a unified way (e.g., without resorting to view-based models)\nusing 2D training data. The foundation of our approach is cascaded shape\nregression (CSR), which has emerged recently as the leading strategy. We\npropose a generalization of conventional CSRs that we call branching cascaded\nregression (BCR). Conventional CSRs are single-track; that is, they progress\nfrom one cascade level to the next in a straight line, with each regressor\nattempting to fit the entire dataset. We instead split the regression problem\ninto two or more simpler ones after each cascade level. Intuitively, each\nregressor can then operate on a simpler objective function (i.e., with fewer\nconflicting gradient directions). Within the BCR framework, we model and infer\npose-related landmark visibility and face shape simultaneously using Structured\nPoint Distribution Models (SPDMs). We propose to learn task-specific feature\nmapping functions that are adaptive to landmark visibility, and that use SPDM\nparameters as regression targets instead of 2D landmark coordinates.\nAdditionally, we introduce a new in-the-wild dataset of profile faces to\nvalidate our approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 01:42:39 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 04:53:39 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Smith", "Brandon M.", ""], ["Dyer", "Charles R.", ""]]}, {"id": "1611.01599", "submitter": "Yannis Assael", "authors": "Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de\n  Freitas", "title": "LipNet: End-to-End Sentence-level Lipreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipreading is the task of decoding text from the movement of a speaker's\nmouth. Traditional approaches separated the problem into two stages: designing\nor learning visual features, and prediction. More recent deep lipreading\napproaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman,\n2016a). However, existing work on models trained end-to-end perform only word\nclassification, rather than sentence-level sequence prediction. Studies have\nshown that human lipreading performance increases for longer words (Easton &\nBasala, 1982), indicating the importance of features capturing temporal context\nin an ambiguous communication channel. Motivated by this observation, we\npresent LipNet, a model that maps a variable-length sequence of video frames to\ntext, making use of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. To the\nbest of our knowledge, LipNet is the first end-to-end sentence-level lipreading\nmodel that simultaneously learns spatiotemporal visual features and a sequence\nmodel. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level,\noverlapped speaker split task, outperforming experienced human lipreaders and\nthe previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 04:05:18 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 16:09:34 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Assael", "Yannis M.", ""], ["Shillingford", "Brendan", ""], ["Whiteson", "Shimon", ""], ["de Freitas", "Nando", ""]]}, {"id": "1611.01639", "submitter": "Patrick McClure", "authors": "Patrick McClure, Nikolaus Kriegeskorte", "title": "Robustly representing uncertainty in deep neural networks through\n  sampling", "comments": "Bayesian Deep Learning Workshop (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks (DNNs) are applied to increasingly challenging\nproblems, they will need to be able to represent their own uncertainty.\nModeling uncertainty is one of the key features of Bayesian methods. Using\nBernoulli dropout with sampling at prediction time has recently been proposed\nas an efficient and well performing variational inference method for DNNs.\nHowever, sampling from other multiplicative noise based variational\ndistributions has not been investigated in depth. We evaluated Bayesian DNNs\ntrained with Bernoulli or Gaussian multiplicative masking of either the units\n(dropout) or the weights (dropconnect). We tested the calibration of the\nprobabilistic predictions of Bayesian convolutional neural networks (CNNs) on\nMNIST and CIFAR-10. Sampling at prediction time increased the calibration of\nthe DNNs' probabalistic predictions. Sampling weights, whether Gaussian or\nBernoulli, led to more robust representation of uncertainty compared to\nsampling of units. However, using either Gaussian or Bernoulli dropout led to\nincreased test set classification accuracy. Based on these findings we used\nboth Bernoulli dropout and Gaussian dropconnect concurrently, which we show\napproximates the use of a spike-and-slab variational distribution without\nincreasing the number of learned parameters. We found that spike-and-slab\nsampling had higher test set performance than Gaussian dropconnect and more\nrobustly represented its uncertainty compared to Bernoulli dropout.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 12:32:16 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 09:27:46 GMT"}, {"version": "v3", "created": "Thu, 2 Feb 2017 10:21:33 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 02:50:59 GMT"}, {"version": "v5", "created": "Tue, 5 Dec 2017 16:11:17 GMT"}, {"version": "v6", "created": "Fri, 8 Dec 2017 17:36:22 GMT"}, {"version": "v7", "created": "Sat, 20 Jan 2018 13:44:32 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["McClure", "Patrick", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "1611.01640", "submitter": "Jiedong Hao", "authors": "Jiedong Hao, Jing Dong, Wei Wang, Tieniu Tan", "title": "What Is the Best Practice for CNNs Applied to Visual Instance Retrieval?", "comments": "The verison submitted to ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that feature maps of deep convolutional neural\nnetworks (CNNs) can be interpreted as feature representation of a particular\nimage region. Features aggregated from these feature maps have been exploited\nfor image retrieval tasks and achieved state-of-the-art performances in recent\nyears. The key to the success of such methods is the feature representation.\nHowever, the different factors that impact the effectiveness of features are\nstill not explored thoroughly. There are much less discussion about the best\ncombination of them.\n  The main contribution of our paper is the thorough evaluations of the various\nfactors that affect the discriminative ability of the features extracted from\nCNNs. Based on the evaluation results, we also identify the best choices for\ndifferent factors and propose a new multi-scale image feature representation\nmethod to encode the image effectively. Finally, we show that the proposed\nmethod generalises well and outperforms the state-of-the-art methods on four\ntypical datasets used for visual instance retrieval.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 12:44:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Hao", "Jiedong", ""], ["Dong", "Jing", ""], ["Wang", "Wei", ""], ["Tan", "Tieniu", ""]]}, {"id": "1611.01642", "submitter": "Victor Campmany", "authors": "Victor Campmany, Sergio Silva, Antonio Espinosa, Juan Carlos Moure,\n  David V\\'azquez, Antonio M. L\\'opez", "title": "GPU-based Pedestrian Detection for Autonomous Driving", "comments": "10 pages", "journal-ref": "International Conference on Computational Science 2016 Volume 80\n  Pages 2377 to 2381", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a real-time pedestrian detection system for the embedded Nvidia\nTegra X1 GPU-CPU hybrid platform. The pipeline is composed by the following\nstate-of-the-art algorithms: Histogram of Local Binary Patterns (LBP) and\nHistograms of Oriented Gradients (HOG) features extracted from the input image;\nPyramidal Sliding Window technique for candidate generation; and Support Vector\nMachine (SVM) for classification. Results show a 8x speedup in the target Tegra\nX1 platform and a better performance/watt ratio than desktop CUDA platforms in\nstudy.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 12:47:32 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Campmany", "Victor", ""], ["Silva", "Sergio", ""], ["Espinosa", "Antonio", ""], ["Moure", "Juan Carlos", ""], ["V\u00e1zquez", "David", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1611.01646", "submitter": "Ting Yao", "authors": "Ting Yao, Yingwei Pan, Yehao Li, Zhaofan Qiu, Tao Mei", "title": "Boosting Image Captioning with Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically describing an image with a natural language has been an\nemerging challenge in both fields of computer vision and natural language\nprocessing. In this paper, we present Long Short-Term Memory with Attributes\n(LSTM-A) - a novel architecture that integrates attributes into the successful\nConvolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs)\nimage captioning framework, by training them in an end-to-end manner. To\nincorporate attributes, we construct variants of architectures by feeding image\nrepresentations and attributes into RNNs in different ways to explore the\nmutual but also fuzzy relationship between them. Extensive experiments are\nconducted on COCO image captioning dataset and our framework achieves superior\nresults when compared to state-of-the-art deep models. Most remarkably, we\nobtain METEOR/CIDEr-D of 25.2%/98.6% on testing data of widely used and\npublicly available splits in (Karpathy & Fei-Fei, 2015) when extracting image\nrepresentations by GoogleNet and achieve to date top-1 performance on COCO\ncaptioning Leaderboard.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 13:12:29 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Yao", "Ting", ""], ["Pan", "Yingwei", ""], ["Li", "Yehao", ""], ["Qiu", "Zhaofan", ""], ["Mei", "Tao", ""]]}, {"id": "1611.01704", "submitter": "Johannes Ball\\'e", "authors": "Johannes Ball\\'e, Valero Laparra, Eero P. Simoncelli", "title": "End-to-end Optimized Image Compression", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": "Presented at: Int'l Conf on Learning Representations, Toulon,\n  France, April 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an image compression method, consisting of a nonlinear analysis\ntransformation, a uniform quantizer, and a nonlinear synthesis transformation.\nThe transforms are constructed in three successive stages of convolutional\nlinear filters and nonlinear activation functions. Unlike most convolutional\nneural networks, the joint nonlinearity is chosen to implement a form of local\ngain control, inspired by those used to model biological neurons. Using a\nvariant of stochastic gradient descent, we jointly optimize the entire model\nfor rate-distortion performance over a database of training images, introducing\na continuous proxy for the discontinuous loss function arising from the\nquantizer. Under certain conditions, the relaxed loss function may be\ninterpreted as the log likelihood of a generative model, as implemented by a\nvariational autoencoder. Unlike these models, however, the compression model\nmust operate at any given point along the rate-distortion curve, as specified\nby a trade-off parameter. Across an independent set of test images, we find\nthat the optimized method generally exhibits better rate-distortion performance\nthan the standard JPEG and JPEG 2000 compression methods. More importantly, we\nobserve a dramatic improvement in visual quality for all images at all bit\nrates, which is supported by objective quality estimates using MS-SSIM.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 21:39:53 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 01:44:59 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 14:53:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ball\u00e9", "Johannes", ""], ["Laparra", "Valero", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1611.01730", "submitter": "Surya Prasath", "authors": "Henrique Tomaz Amaral-Silva, Luiz Otavio Murta-Jr, Paulo Mazzoncini de\n  Azevedo-Marques, Lauro Wichert-Ana, V. B. Surya Prasath, Colin Studholme", "title": "Validation of Tsallis Entropy In Inter-Modality Neuroimage Registration", "comments": "15 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical image registration plays an important role in determining topographic\nand morphological changes for functional diagnostic and therapeutic purposes.\nManual alignment and semi-automated software still have been used; however they\nare subjective and make specialists spend precious time. Fully automated\nmethods are faster and user-independent, but the critical point is registration\nreliability. Similarity measurement using Mutual Information (MI) with Shannon\nentropy (MIS) is the most common automated method that is being currently\napplied in medical images, although more reliable algorithms have been proposed\nover the last decade, suggesting improvements and different entropies; such as\nStudholme et al, (1999), who demonstrated that the normalization of Mutual\nInformation (NMI) provides an invariant entropy measure for 3D medical image\nregistration. In this paper, we described a set of experiments to evaluate the\napplicability of Tsallis entropy in the Mutual Information (MIT) and in the\nNormalized Mutual Information (NMIT) as cost functions for Magnetic Resonance\nImaging (MRI), Positron Emission Tomography (PET) and Computed Tomography (CT)\nexams registration. The effect of changing overlap in a simple image model and\nclinical experiments on current entropies (Entropy Correlation Coefficient -\nECC, MIS and NMI) and the proposed ones (MIT and NMT) showed NMI and NMIT with\nTsallis parameter close to 1 as the best options (confidence and accuracy) for\nCT to MRI and PET to MRI automatic neuroimaging registration.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 05:56:12 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Amaral-Silva", "Henrique Tomaz", ""], ["Murta-Jr", "Luiz Otavio", ""], ["de Azevedo-Marques", "Paulo Mazzoncini", ""], ["Wichert-Ana", "Lauro", ""], ["Prasath", "V. B. Surya", ""], ["Studholme", "Colin", ""]]}, {"id": "1611.01731", "submitter": "Bin-Bin Gao", "authors": "Bin-Bin Gao, Chao Xing, Chen-Wei Xie, Jianxin Wu, Xin Geng", "title": "Deep Label Distribution Learning with Label Ambiguity", "comments": "Accepted by IEEE TIP 2017. Projects page, see\n  http://lamda.nju.edu.cn/gaobb/projects/DLDL.html", "journal-ref": "IEEE Transactions on Image Processing 26(6), 2017:2825-2838", "doi": "10.1109/TIP.2017.2689998", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (ConvNets) have achieved excellent recognition\nperformance in various visual recognition tasks. A large labeled training set\nis one of the most important factors for its success. However, it is difficult\nto collect sufficient training images with precise labels in some domains such\nas apparent age estimation, head pose estimation, multi-label classification\nand semantic segmentation. Fortunately, there is ambiguous information among\nlabels, which makes these tasks different from traditional classification.\nBased on this observation, we convert the label of each image into a discrete\nlabel distribution, and learn the label distribution by minimizing a\nKullback-Leibler divergence between the predicted and ground-truth label\ndistributions using deep ConvNets. The proposed DLDL (Deep Label Distribution\nLearning) method effectively utilizes the label ambiguity in both feature\nlearning and classifier learning, which help prevent the network from\nover-fitting even when the training set is small. Experimental results show\nthat the proposed approach produces significantly better results than\nstate-of-the-art methods for age estimation and head pose estimation. At the\nsame time, it also improves recognition performance for multi-label\nclassification and semantic segmentation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 06:46:50 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 13:47:26 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Gao", "Bin-Bin", ""], ["Xing", "Chao", ""], ["Xie", "Chen-Wei", ""], ["Wu", "Jianxin", ""], ["Geng", "Xin", ""]]}, {"id": "1611.01751", "submitter": "Connor Parde", "authors": "Connor J. Parde, Carlos Castillo, Matthew Q. Hill, Y. Ivette Colon,\n  Swami Sankaranarayanan, Jun-Cheng Chen, Alice J. O'Toole", "title": "Deep Convolutional Neural Network Features and the Original Image", "comments": "Submitted to Face and Gesture Conference, 2017", "journal-ref": null, "doi": "10.1109/FG.2017.85", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face recognition algorithms based on deep convolutional neural networks\n(DCNNs) have made progress on the task of recognizing faces in unconstrained\nviewing conditions. These networks operate with compact feature-based face\nrepresentations derived from learning a very large number of face images. While\nthe learned features produced by DCNNs can be highly robust to changes in\nviewpoint, illumination, and appearance, little is known about the nature of\nthe face code that emerges at the top level of such networks. We analyzed the\nDCNN features produced by two face recognition algorithms. In the first set of\nexperiments we used the top-level features from the DCNNs as input into linear\nclassifiers aimed at predicting metadata about the images. The results show\nthat the DCNN features contain surprisingly accurate information about the yaw\nand pitch of a face, and about whether the face came from a still image or a\nvideo frame. In the second set of experiments, we measured the extent to which\nindividual DCNN features operated in a view-dependent or view-invariant manner.\nWe found that view-dependent coding was a characteristic of the identities\nrather than the DCNN features - with some identities coded consistently in a\nview-dependent way and others in a view-independent way. In our third analysis,\nwe visualized the DCNN feature space for over 24,000 images of 500 identities.\nImages in the center of the space were uniformly of low quality (e.g., extreme\nviews, face occlusion, low resolution). Image quality increased monotonically\nas a function of distance from the origin. This result suggests that image\nquality information is available in the DCNN features, such that consistently\naverage feature values reflect coding failures that reliably indicate poor or\nunusable images. Combined, the results offer insight into the coding mechanisms\nthat support robust representation of faces in DCNNs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 10:32:37 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Parde", "Connor J.", ""], ["Castillo", "Carlos", ""], ["Hill", "Matthew Q.", ""], ["Colon", "Y. Ivette", ""], ["Sankaranarayanan", "Swami", ""], ["Chen", "Jun-Cheng", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "1611.01773", "submitter": "Mingkui Tan", "authors": "Yong Guo, Jian Chen, Qing Du, Anton Van Den Hengel, Qinfeng Shi,\n  Mingkui Tan", "title": "The Shallow End: Empowering Shallower Deep-Convolutional Networks\n  through Auxiliary Outputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth is one of the key factors behind the success of convolutional neural\nnetworks (CNNs). Since ResNet, we are able to train very deep CNNs as the\ngradient vanishing issue has been largely addressed by the introduction of skip\nconnections. However, we observe that, when the depth is very large, the\nintermediate layers (especially shallow layers) may fail to receive sufficient\nsupervision from the loss due to the severe transformation through a long\nbackpropagation path. As a result, the representation power of intermediate\nlayers can be very weak and the model becomes very redundant with limited\nperformance. In this paper, we first investigate the supervision vanishing\nissue in existing backpropagation (BP) methods. And then, we propose to address\nit via an effective method, called Multi-way BP (MW-BP), which relies on\nmultiple auxiliary losses added to the intermediate layers of the network. The\nproposed MW-BP method can be applied to most deep architectures with slight\nmodifications, such as ResNet and MobileNet. Our method often gives rise to\nmuch more compact models (denoted by \"Mw+Architecture\") than existing methods.\nFor example, MwResNet-44 with 44 layers performs better than ResNet-110 with\n110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models\neven outperform the light models obtained by state-of-the-art model compression\nmethods. Last, our method inherently produces multiple compact models with\ndifferent depths at the same time, which is helpful for model selection.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 13:20:06 GMT"}, {"version": "v2", "created": "Sat, 19 Nov 2016 04:57:40 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 14:59:13 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 12:01:57 GMT"}, {"version": "v5", "created": "Sat, 2 Nov 2019 07:23:25 GMT"}, {"version": "v6", "created": "Sun, 16 Feb 2020 04:18:25 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Guo", "Yong", ""], ["Chen", "Jian", ""], ["Du", "Qing", ""], ["Hengel", "Anton Van Den", ""], ["Shi", "Qinfeng", ""], ["Tan", "Mingkui", ""]]}, {"id": "1611.01779", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Vladlen Koltun", "title": "Learning to Act by Predicting the Future", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 13:45:00 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 19:47:46 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1611.01843", "submitter": "Misha Denil", "authors": "Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter\n  Battaglia, Nando de Freitas", "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:55:19 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:40:58 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 19:51:29 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Denil", "Misha", ""], ["Agrawal", "Pulkit", ""], ["Kulkarni", "Tejas D", ""], ["Erez", "Tom", ""], ["Battaglia", "Peter", ""], ["de Freitas", "Nando", ""]]}, {"id": "1611.01872", "submitter": "Ye Liu", "authors": "Ye Liu, Liqiang Nie, Lei Han, Luming Zhang, David S Rosenblum", "title": "Action2Activity: Recognizing Complex Activities from Sensor Data", "comments": "IJCAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As compared to simple actions, activities are much more complex, but\nsemantically consistent with a human's real life. Techniques for action\nrecognition from sensor generated data are mature. However, there has been\nrelatively little work on bridging the gap between actions and activities. To\nthis end, this paper presents a novel approach for complex activity recognition\ncomprising of two components. The first component is temporal pattern mining,\nwhich provides a mid-level feature representation for activities, encodes\ntemporal relatedness among actions, and captures the intrinsic properties of\nactivities. The second component is adaptive Multi-Task Learning, which\ncaptures relatedness among activities and selects discriminant features.\nExtensive experiments on a real-world dataset demonstrate the effectiveness of\nour work.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 02:01:29 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Liu", "Ye", ""], ["Nie", "Liqiang", ""], ["Han", "Lei", ""], ["Zhang", "Luming", ""], ["Rosenblum", "David S", ""]]}, {"id": "1611.01962", "submitter": "Emmanuel Maggiori", "authors": "Emmanuel Maggiori, Yuliya Tarabalka, Guillaume Charpiat and Pierre\n  Alliez", "title": "High-Resolution Semantic Labeling with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2740362", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have received increasing attention over\nthe last few years. They were initially conceived for image categorization,\ni.e., the problem of assigning a semantic label to an entire input image.\n  In this paper we address the problem of dense semantic labeling, which\nconsists in assigning a semantic label to every pixel in an image. Since this\nrequires a high spatial accuracy to determine where labels are assigned,\ncategorization CNNs, intended to be highly robust to local deformations, are\nnot directly applicable.\n  By adapting categorization networks, many semantic labeling CNNs have been\nrecently proposed. Our first contribution is an in-depth analysis of these\narchitectures. We establish the desired properties of an ideal semantic\nlabeling CNN, and assess how those methods stand with regard to these\nproperties. We observe that even though they provide competitive results, these\nCNNs often underexploit properties of semantic labeling that could lead to more\neffective and efficient architectures.\n  Out of these observations, we then derive a CNN framework specifically\nadapted to the semantic labeling problem. In addition to learning features at\ndifferent resolutions, it learns how to combine these features. By integrating\nlocal and global information in an efficient and flexible manner, it\noutperforms previous techniques. We evaluate the proposed framework and compare\nit with state-of-the-art architectures on public benchmarks of high-resolution\naerial image labeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:02:49 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Maggiori", "Emmanuel", ""], ["Tarabalka", "Yuliya", ""], ["Charpiat", "Guillaume", ""], ["Alliez", "Pierre", ""]]}, {"id": "1611.01972", "submitter": "Peisong Wang", "authors": "Peisong Wang and Jian Cheng", "title": "Fixed-point Factorized Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, Deep Neural Networks (DNN) based methods have achieved\nremarkable performance in a wide range of tasks and have been among the most\npowerful and widely used techniques in computer vision. However, DNN-based\nmethods are both computational-intensive and resource-consuming, which hinders\nthe application of these methods on embedded systems like smart phones. To\nalleviate this problem, we introduce a novel Fixed-point Factorized Networks\n(FFN) for pretrained models to reduce the computational complexity as well as\nthe storage requirement of networks. The resulting networks have only weights\nof -1, 0 and 1, which significantly eliminates the most resource-consuming\nmultiply-accumulate operations (MACs). Extensive experiments on large-scale\nImageNet classification task show the proposed FFN only requires one-thousandth\nof multiply operations with comparable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:26:41 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 09:46:41 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Wang", "Peisong", ""], ["Cheng", "Jian", ""]]}, {"id": "1611.01982", "submitter": "Huabin Zheng", "authors": "Huabin Zheng, Jingyu Wang, Zhengjie Huang, Yang Yang, Rong Pan", "title": "Chinese/English mixed Character Segmentation as Semantic Segmentation", "comments": "Submitted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OCR character segmentation for multilingual printed documents is difficult\ndue to the diversity of different linguistic characters. Previous approaches\nmainly focus on monolingual texts and are not suitable for multilingual-lingual\ncases. In this work, we particularly tackle the Chinese/English mixed case by\nreframing it as a semantic segmentation problem. We take advantage of the\nsuccessful architecture called fully convolutional networks (FCN) in the field\nof semantic segmentation. Given a wide enough receptive field, FCN can utilize\nthe necessary context around a horizontal position to determinate whether this\nis a splitting point or not. As a deep neural architecture, FCN can\nautomatically learn useful features from raw text line images. Although trained\non synthesized samples with simulated random disturbance, our FCN model\ngeneralizes well to real-world samples. The experimental results show that our\nmodel significantly outperforms the previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 10:53:29 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:46:11 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zheng", "Huabin", ""], ["Wang", "Jingyu", ""], ["Huang", "Zhengjie", ""], ["Yang", "Yang", ""], ["Pan", "Rong", ""]]}, {"id": "1611.01990", "submitter": "Yoni Choukroun", "authors": "Yoni Choukroun, Alon Shtern, Alex Bronstein and Ron Kimmel", "title": "Hamiltonian operator for spectral shape analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many shape analysis methods treat the geometry of an object as a metric space\nthat can be captured by the Laplace-Beltrami operator. In this paper, we\npropose to adapt the classical Hamiltonian operator from quantum mechanics to\nthe field of shape analysis. To this end we study the addition of a potential\nfunction to the Laplacian as a generator for dual spaces in which shape\nprocessing is performed. We present a general optimization approach for solving\nvariational problems involving the basis defined by the Hamiltonian using\nperturbation theory for its eigenvectors. The suggested operator is shown to\nproduce better functional spaces to operate with, as demonstrated on different\nshape analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 11:11:10 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 17:45:00 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Choukroun", "Yoni", ""], ["Shtern", "Alon", ""], ["Bronstein", "Alex", ""], ["Kimmel", "Ron", ""]]}, {"id": "1611.02061", "submitter": "Michal Nowicki", "authors": "Micha{\\l} Nowicki and Jan Wietrzykowski and Piotr Skrzypczy\\'nski", "title": "Real-Time Visual Place Recognition for Personal Localization on a Mobile\n  Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an approach to indoor personal localization on a mobile\ndevice based on visual place recognition. We implemented on a smartphone two\nstate-of-the-art algorithms that are representative to two different approaches\nto visual place recognition: FAB-MAP that recognizes places using individual\nimages, and ABLE-M that utilizes sequences of images. These algorithms are\nevaluated in environments of different structure, focusing on problems commonly\nencountered when a mobile device camera is used. The conclusions drawn from\nthis evaluation are guidelines to design the FastABLE system, which is based on\nthe ABLE-M algorithm, but introduces major modifications to the concept of\nimage matching. The improvements radically cut down the processing time and\nimprove scalability, making it possible to localize the user in long image\nsequences with the limited computing power of a mobile device. The resulting\nplace recognition system compares favorably to both the ABLE-M and the FAB-MAP\nsolutions in the context of real-time personal localization.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 14:11:12 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 09:50:30 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Nowicki", "Micha\u0142", ""], ["Wietrzykowski", "Jan", ""], ["Skrzypczy\u0144ski", "Piotr", ""]]}, {"id": "1611.02064", "submitter": "Avijit Dasgupta", "authors": "Avijit Dasgupta and Sonam Singh", "title": "A Fully Convolutional Neural Network based Structured Prediction\n  Approach Towards the Retinal Vessel Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of retinal blood vessels from fundus images plays an\nimportant role in the computer aided diagnosis of retinal diseases. The task of\nblood vessel segmentation is challenging due to the extreme variations in\nmorphology of the vessels against noisy background. In this paper, we formulate\nthe segmentation task as a multi-label inference task and utilize the implicit\nadvantages of the combination of convolutional neural networks and structured\nprediction. Our proposed convolutional neural network based model achieves\nstrong performance and significantly outperforms the state-of-the-art for\nautomatic retinal blood vessel segmentation on DRIVE dataset with 95.33%\naccuracy and 0.974 AUC score.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 14:16:18 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 09:21:40 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dasgupta", "Avijit", ""], ["Singh", "Sonam", ""]]}, {"id": "1611.02102", "submitter": "Minh-Tan Pham", "authors": "Minh-Tan Pham, Gr\\'egoire Mercier, Lionel Bombrun, Julien Michel", "title": "Texture and Color-based Image Retrieval Using the Local Extrema Features\n  and Riemannian Distance", "comments": "This paper has been withdrawn by the author due to a crucial equation\n  modification in part II.B", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel efficient method for content-based image retrieval (CBIR) is\ndeveloped in this paper using both texture and color features. Our motivation\nis to represent and characterize an input image by a set of local descriptors\nextracted at characteristic points (i.e. keypoints) within the image. Then,\ndissimilarity measure between images is calculated based on the geometric\ndistance between the topological feature spaces (i.e. manifolds) formed by the\nsets of local descriptors generated from these images. In this work, we propose\nto extract and use the local extrema pixels as our feature points. Then, the\nso-called local extrema-based descriptor (LED) is generated for each keypoint\nby integrating all color, spatial as well as gradient information captured by a\nset of its nearest local extrema. Hence, each image is encoded by a LED feature\npoint cloud and riemannian distances between these point clouds enable us to\ntackle CBIR. Experiments performed on Vistex, Stex and colored Brodatz texture\ndatabases using the proposed approach provide very efficient and competitive\nresults compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:20:59 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 09:25:17 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Pham", "Minh-Tan", ""], ["Mercier", "Gr\u00e9goire", ""], ["Bombrun", "Lionel", ""], ["Michel", "Julien", ""]]}, {"id": "1611.02145", "submitter": "Olga Russakovsky", "authors": "Adriana Kovashka, Olga Russakovsky, Li Fei-Fei, Kristen Grauman", "title": "Crowdsourcing in Computer Vision", "comments": "A 69-page meta review of the field, Foundations and Trends in\n  Computer Graphics and Vision, 2016", "journal-ref": null, "doi": "10.1561/0600000073", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision systems require large amounts of manually annotated data to\nproperly learn challenging visual concepts. Crowdsourcing platforms offer an\ninexpensive method to capture human knowledge and understanding, for a vast\nnumber of visual perception tasks. In this survey, we describe the types of\nannotations computer vision researchers have collected using crowdsourcing, and\nhow they have ensured that this data is of high quality while annotation effort\nis minimized. We begin by discussing data collection on both classic (e.g.,\nobject recognition) and recent (e.g., visual story-telling) vision tasks. We\nthen summarize key design decisions for creating effective data collection\ninterfaces and workflows, and present strategies for intelligently selecting\nthe most important data instances to annotate. Finally, we conclude with some\nthoughts on the future of crowdsourcing in computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 16:11:19 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Kovashka", "Adriana", ""], ["Russakovsky", "Olga", ""], ["Fei-Fei", "Li", ""], ["Grauman", "Kristen", ""]]}, {"id": "1611.02155", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Axel Pinz, Richard P. Wildes", "title": "Spatiotemporal Residual Networks for Video Action Recognition", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stream Convolutional Networks (ConvNets) have shown strong performance\nfor human action recognition in videos. Recently, Residual Networks (ResNets)\nhave arisen as a new technique to train extremely deep architectures. In this\npaper, we introduce spatiotemporal ResNets as a combination of these two\napproaches. Our novel architecture generalizes ResNets for the spatiotemporal\ndomain by introducing residual connections in two ways. First, we inject\nresidual connections between the appearance and motion pathways of a two-stream\narchitecture to allow spatiotemporal interaction between the two streams.\nSecond, we transform pretrained image ConvNets into spatiotemporal networks by\nequipping these with learnable convolutional filters that are initialized as\ntemporal residual connections and operate on adjacent feature maps in time.\nThis approach slowly increases the spatiotemporal receptive field as the depth\nof the model increases and naturally integrates image ConvNet design\nprinciples. The whole model is trained end-to-end to allow hierarchical\nlearning of complex spatiotemporal features. We evaluate our novel\nspatiotemporal ResNet using two widely used action recognition benchmarks where\nit exceeds the previous state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 16:17:16 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Pinz", "Axel", ""], ["Wildes", "Richard P.", ""]]}, {"id": "1611.02174", "submitter": "Yiyi Liao", "authors": "Yiyi Liao, Lichao Huang, Yue Wang, Sarath Kodagoda, Yinan Yu, Yong Liu", "title": "Parse Geometry from a Line: Monocular Depth Estimation with Partial\n  Laser Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many standard robotic platforms are equipped with at least a fixed 2D laser\nrange finder and a monocular camera. Although those platforms do not have\nsensors for 3D depth sensing capability, knowledge of depth is an essential\npart in many robotics activities. Therefore, recently, there is an increasing\ninterest in depth estimation using monocular images. As this task is inherently\nambiguous, the data-driven estimated depth might be unreliable in robotics\napplications. In this paper, we have attempted to improve the precision of\nmonocular depth estimation by introducing 2D planar observation from the\nremaining laser range finder without extra cost. Specifically, we construct a\ndense reference map from the sparse laser range data, redefining the depth\nestimation task as estimating the distance between the real and the reference\ndepth. To solve the problem, we construct a novel residual of residual neural\nnetwork, and tightly combine the classification and regression losses for\ncontinuous depth estimation. Experimental results suggest that our method\nachieves considerable promotion compared to the state-of-the-art methods on\nboth NYUD2 and KITTI, validating the effectiveness of our method on leveraging\nthe additional sensory information. We further demonstrate the potential usage\nof our method in obstacle avoidance where our methodology provides\ncomprehensive depth information compared to the solution using monocular camera\nor 2D laser range finder alone.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 21:12:07 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Liao", "Yiyi", ""], ["Huang", "Lichao", ""], ["Wang", "Yue", ""], ["Kodagoda", "Sarath", ""], ["Yu", "Yinan", ""], ["Liu", "Yong", ""]]}, {"id": "1611.02200", "submitter": "Yaniv Taigman", "authors": "Yaniv Taigman, Adam Polyak, Lior Wolf", "title": "Unsupervised Cross-Domain Image Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of transferring a sample in one domain to an analog\nsample in another domain. Given two related domains, S and T, we would like to\nlearn a generative function G that maps an input sample from S to the domain T,\nsuch that the output of a given function f, which accepts inputs in either\ndomains, would remain unchanged. Other than the function f, the training data\nis unsupervised and consist of a set of samples from each domain. The Domain\nTransfer Network (DTN) we present employs a compound loss function that\nincludes a multiclass GAN loss, an f-constancy component, and a regularizing\ncomponent that encourages G to map samples from T to themselves. We apply our\nmethod to visual domains including digits and face images and demonstrate its\nability to generate convincing novel images of previously unseen entities,\nwhile preserving their identity.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 18:14:57 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Taigman", "Yaniv", ""], ["Polyak", "Adam", ""], ["Wolf", "Lior", ""]]}, {"id": "1611.02260", "submitter": "Joao Macedo Neto", "authors": "Jo\\~ao J. de Macedo Neto, Jefersson A. dos Santos and William Robson\n  Schwartz", "title": "Meat adulteration detection through digital image analysis of\n  histological cuts using LBP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food fraud has been an area of great concern due to its risk to public\nhealth, reduction of food quality or nutritional value and for its economic\nconsequences. For this reason, it's been object of regulation in many countries\n(e.g. [1], [2]). One type of food that has been frequently object of fraud\nthrough the addition of water or an aqueous solution is bovine meat. The\ntraditional methods used to detect this kind of fraud are expensive,\ntime-consuming and depend on physicochemical analysis that require complex\nlaboratory techniques, specific for each added substance. In this paper, based\non digital images of histological cuts of adulterated and not-adulterated\n(normal) bovine meat, we evaluate the of digital image analysis methods to\nidentify the aforementioned kind of fraud, with focus on the Local Binary\nPattern (LBP) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:40:57 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Neto", "Jo\u00e3o J. de Macedo", ""], ["Santos", "Jefersson A. dos", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1611.02261", "submitter": "Rasool Fakoor", "authors": "Rasool Fakoor, Abdel-rahman Mohamed, Margaret Mitchell, Sing Bing\n  Kang, Pushmeet Kohli", "title": "Memory-augmented Attention Modelling for Videos", "comments": "Revised version, minor changes, add the link for the source codes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to improve video description generation by modeling\nhigher-order interactions between video frames and described concepts. By\nstoring past visual attention in the video associated to previously generated\nwords, the system is able to decide what to look at and describe in light of\nwhat it has already looked at and described. This enables not only more\neffective local attention, but tractable consideration of the video sequence\nwhile generating each word. Evaluation on the challenging and popular MSVD and\nCharades datasets demonstrates that the proposed architecture outperforms\nprevious video description approaches without requiring external temporal video\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:50:08 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 22:39:13 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 02:22:51 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 07:26:01 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Fakoor", "Rasool", ""], ["Mohamed", "Abdel-rahman", ""], ["Mitchell", "Margaret", ""], ["Kang", "Sing Bing", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1611.02302", "submitter": "Mario Mastriani", "authors": "Mario Mastriani", "title": "Quantum spectral analysis: frequency in time, with applications to\n  signal and image processing", "comments": "140 pages, 78 figures, 8 tables. arXiv admin note: text overlap with\n  arXiv:0803.2507, arXiv:quant-ph/0402085,arXiv:1611.07351 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum time-dependent spectrum analysis, or simply, quantum spectral\nanalysis (QSA) is presented in this work, and it is based on Schrodinger\nequation, which is a partial differential equation that describes how the\nquantum state of a non-relativistic physical system changes with time. In\nclassic world is named frequency in time (FIT), which is presented here in\nopposition and as a complement of traditional spectral analysis\nfrequency-dependent based on Fourier theory. Besides, FIT is a metric, which\nassesses the impact of the flanks of a signal on its frequency spectrum, which\nis not taken into account by Fourier theory and even less in real time. Even\nmore, and unlike all derived tools from Fourier Theory (i.e., continuous,\ndiscrete, fast, short-time, fractional and quantum Fourier Transform, as well\nas, Gabor) FIT has the following advantages: a) compact support with excellent\nenergy output treatment, b) low computational cost, O(N) for signals and O(N2)\nfor images, c) it does not have phase uncertainties (indeterminate phase for\nmagnitude = 0) as Discrete and Fast Fourier Transform (DFT, FFT, respectively),\nd) among others. In fact, FIT constitutes one side of a triangle (which from\nnow on is closed) and it consists of the original signal in time, spectral\nanalysis based on Fourier Theory and FIT. Thus a toolbox is completed, which it\nis essential for all applications of Digital Signal Processing (DSP) and\nDigital Image Processing (DIP); and, even, in the latter, FIT allows edge\ndetection (which is called flank detection in case of signals), denoising,\ndespeckling, compression, and superresolution of still images. Such\napplications include signals intelligence and imagery intelligence. On the\nother hand, we will present other DIP tools, which are also derived from the\nSchrodinger equation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 18:37:33 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2016 15:07:47 GMT"}, {"version": "v3", "created": "Thu, 9 Mar 2017 11:33:11 GMT"}, {"version": "v4", "created": "Fri, 17 Mar 2017 20:40:40 GMT"}, {"version": "v5", "created": "Fri, 5 May 2017 21:09:41 GMT"}, {"version": "v6", "created": "Fri, 16 Mar 2018 11:11:42 GMT"}, {"version": "v7", "created": "Tue, 4 Jun 2019 19:32:38 GMT"}, {"version": "v8", "created": "Thu, 18 Feb 2021 21:04:57 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Mastriani", "Mario", ""]]}, {"id": "1611.02364", "submitter": "Guillaume-Alexandre Bilodeau", "authors": "Yuebin Yang, Guillaume-Alexandre Bilodeau", "title": "Multiple Object Tracking with Kernelized Correlation Filters in Urban\n  Mixed Traffic", "comments": "Accepted for CRV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Kernelized Correlation Filters tracker (KCF) achieved\ncompetitive performance and robustness in visual object tracking. On the other\nhand, visual trackers are not typically used in multiple object tracking. In\nthis paper, we investigate how a robust visual tracker like KCF can improve\nmultiple object tracking. Since KCF is a fast tracker, many can be used in\nparallel and still result in fast tracking. We build a multiple object tracking\nsystem based on KCF and background subtraction. Background subtraction is\napplied to extract moving objects and get their scale and size in combination\nwith KCF outputs, while KCF is used for data association and to handle\nfragmentation and occlusion problems. As a result, KCF and background\nsubtraction help each other to take tracking decision at every frame. Sometimes\nKCF outputs are the most trustworthy (e.g. during occlusion), while in some\nother case, it is the background subtraction outputs. To validate the\neffectiveness of our system, the algorithm is demonstrated on four urban video\nrecordings from a standard dataset. Results show that our method is competitive\nwith state-of-the-art trackers even if we use a much simpler data association\nstep.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 02:20:09 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 14:36:59 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Yang", "Yuebin", ""], ["Bilodeau", "Guillaume-Alexandre", ""]]}, {"id": "1611.02443", "submitter": "Toru Tamaki", "authors": "Toru Tamaki, Shoji Sonoyama, Takio Kurita, Tsubasa Hirakawa, Bisser\n  Raytchev, Kazufumi Kaneda, Tetsushi Koide, Shigeto Yoshida, Hiroshi Mieno,\n  Shinji Tanaka, Kazuaki Chayama", "title": "Domain Adaptation with L2 constraints for classifying images from\n  different endoscope systems", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for domain adaptation that extends the maximum\nmargin domain transfer (MMDT) proposed by Hoffman et al., by introducing L2\ndistance constraints between samples of different domains; thus, our method is\ndenoted as MMDTL2. Motivated by the differences between the images taken by\nnarrow band imaging (NBI) endoscopic devices, we utilize different NBI devices\nas different domains and estimate the transformations between samples of\ndifferent domains, i.e., image samples taken by different NBI endoscope\nsystems. We first formulate the problem in the primal form, and then derive the\ndual form with much lesser computational costs as compared to the naive\napproach. From our experimental results using NBI image datasets from two\ndifferent NBI endoscopic devices, we find that MMDTL2 is better than MMDT and\nalso support vector machines without adaptation, especially when NBI image\nfeatures are high-dimensional and the per-class training samples are greater\nthan 20.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:29:17 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 09:01:20 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Tamaki", "Toru", ""], ["Sonoyama", "Shoji", ""], ["Kurita", "Takio", ""], ["Hirakawa", "Tsubasa", ""], ["Raytchev", "Bisser", ""], ["Kaneda", "Kazufumi", ""], ["Koide", "Tetsushi", ""], ["Yoshida", "Shigeto", ""], ["Mieno", "Hiroshi", ""], ["Tanaka", "Shinji", ""], ["Chayama", "Kazuaki", ""]]}, {"id": "1611.02447", "submitter": "Pichao Wang", "authors": "Pichao Wang and Zhaoyang Li and Yonghong Hou and Wanqing Li", "title": "Action Recognition Based on Joint Trajectory Maps Using Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Convolutional Neural Networks (ConvNets) have shown promising\nperformances in many computer vision tasks, especially image-based recognition.\nHow to effectively use ConvNets for video-based recognition is still an open\nproblem. In this paper, we propose a compact, effective yet simple method to\nencode spatio-temporal information carried in $3D$ skeleton sequences into\nmultiple $2D$ images, referred to as Joint Trajectory Maps (JTM), and ConvNets\nare adopted to exploit the discriminative features for real-time human action\nrecognition. The proposed method has been evaluated on three public benchmarks,\ni.e., MSRC-12 Kinect gesture dataset (MSRC-12), G3D dataset and UTD multimodal\nhuman action dataset (UTD-MHAD) and achieved the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:35:17 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2016 23:24:58 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Zhaoyang", ""], ["Hou", "Yonghong", ""], ["Li", "Wanqing", ""]]}, {"id": "1611.02525", "submitter": "Etai Littwin", "authors": "Etai Littwin, Lior Wolf", "title": "The Loss Surface of Residual Networks: Ensembles and the Role of Batch\n  Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Residual Networks present a premium in performance in comparison to\nconventional networks of the same depth and are trainable at extreme depths. It\nhas recently been shown that Residual Networks behave like ensembles of\nrelatively shallow networks. We show that these ensembles are dynamic: while\ninitially the virtual ensemble is mostly at depths lower than half the\nnetwork's depth, as training progresses, it becomes deeper and deeper. The main\nmechanism that controls the dynamic ensemble behavior is the scaling\nintroduced, e.g., by the Batch Normalization technique. We explain this\nbehavior and demonstrate the driving force behind it. As a main tool in our\nanalysis, we employ generalized spin glass models, which we also use in order\nto study the number of critical points in the optimization of Residual\nNetworks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 14:17:13 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Littwin", "Etai", ""], ["Wolf", "Lior", ""]]}, {"id": "1611.02637", "submitter": "Vania Estrela Dr.", "authors": "Felipe P. do Carmo, Vania Vieira Estrela, Joaquim Teixeira de Assis", "title": "Estimating motion with principal component regression strategies", "comments": "6 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1610.02923", "journal-ref": "Proceedings of the IEEE International Workshop on Multimedia\n  Signal Processing, 2009, MMSP '09, 2009", "doi": "10.1109/mmsp.2009.5293264", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, two simple principal component regression methods for\nestimating the optical flow between frames of video sequences according to a\npel-recursive manner are introduced. These are easy alternatives to dealing\nwith mixtures of motion vectors in addition to the lack of prior information on\nspatial-temporal statistics (although they are supposed to be normal in a local\nsense). The 2D motion vector estimation approaches take into consideration\nsimple image properties and are used to harmonize regularized least square\nestimates. Their main advantage is that no knowledge of the noise distribution\nis necessary, although there is an underlying assumption of localized\nsmoothness. Preliminary experiments indicate that this approach provides robust\nestimates of the optical flow.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 18:01:54 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Carmo", "Felipe P. do", ""], ["Estrela", "Vania Vieira", ""], ["de Assis", "Joaquim Teixeira", ""]]}, {"id": "1611.02639", "submitter": "Ankur Taly", "authors": "Mukund Sundararajan, Ankur Taly, Qiqi Yan", "title": "Gradients of Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradients have been used to quantify feature importance in machine learning\nmodels. Unfortunately, in nonlinear deep networks, not only individual neurons\nbut also the whole network can saturate, and as a result an important input\nfeature can have a tiny gradient. We study various networks, and observe that\nthis phenomena is indeed widespread, across many inputs.\n  We propose to examine interior gradients, which are gradients of\ncounterfactual inputs constructed by scaling down the original input. We apply\nour method to the GoogleNet architecture for object recognition in images, as\nwell as a ligand-based virtual screening network with categorical features and\nan LSTM based language model for the Penn Treebank dataset. We visualize how\ninterior gradients better capture feature importance. Furthermore, interior\ngradients are applicable to a wide variety of deep networks, and have the\nattribution property that the feature importance scores sum to the the\nprediction score.\n  Best of all, interior gradients can be computed just as easily as gradients.\nIn contrast, previous methods are complex to implement, which hinders practical\nadoption.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 18:10:44 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 19:55:26 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Sundararajan", "Mukund", ""], ["Taly", "Ankur", ""], ["Yan", "Qiqi", ""]]}, {"id": "1611.02644", "submitter": "Jingjing Liu", "authors": "Jingjing Liu, Shaoting Zhang, Shu Wang, Dimitris N. Metaxas", "title": "Multispectral Deep Neural Networks for Pedestrian Detection", "comments": "13 pages, 8 figures, BMVC 2016 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multispectral pedestrian detection is essential for around-the-clock\napplications, e.g., surveillance and autonomous driving. We deeply analyze\nFaster R-CNN for multispectral pedestrian detection task and then model it into\na convolutional network (ConvNet) fusion problem. Further, we discover that\nConvNet-based pedestrian detectors trained by color or thermal images\nseparately provide complementary information in discriminating human instances.\nThus there is a large potential to improve pedestrian detection by using color\nand thermal images in DNNs simultaneously. We carefully design four ConvNet\nfusion architectures that integrate two-branch ConvNets on different DNNs\nstages, all of which yield better performance compared with the baseline\ndetector. Our experimental results on KAIST pedestrian benchmark show that the\nHalfway Fusion model that performs fusion on the middle-level convolutional\nfeatures outperforms the baseline method by 11% and yields a missing rate 3.5%\nlower than the other proposed architectures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 18:22:31 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Liu", "Jingjing", ""], ["Zhang", "Shaoting", ""], ["Wang", "Shu", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1611.02730", "submitter": "Angelica I. Aviles", "authors": "Angelica I. Aviles, Thomas Widlak, Alicia Casals, Maartje M. Nillesen\n  and Habib Ammari", "title": "Robust Cardiac Motion Estimation using Ultrafast Ultrasound Data: A\n  Low-Rank-Topology-Preserving Approach", "comments": "15 pages, 10 figures, Physics in Medicine and Biology, 2017", "journal-ref": null, "doi": "10.1088/1361-6560/aa6914", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiac motion estimation is an important diagnostic tool to detect heart\ndiseases and it has been explored with modalities such as MRI and conventional\nultrasound (US) sequences. US cardiac motion estimation still presents\nchallenges because of the complex motion patterns and the presence of noise. In\nthis work, we propose a novel approach to estimate the cardiac motion using\nultrafast ultrasound data. -- Our solution is based on a variational\nformulation characterized by the L2-regularized class. The displacement is\nrepresented by a lattice of b-splines and we ensure robustness by applying a\nmaximum likelihood type estimator. While this is an important part of our\nsolution, the main highlight of this paper is to combine a low-rank data\nrepresentation with topology preservation. Low-rank data representation\n(achieved by finding the k-dominant singular values of a Casorati Matrix\narranged from the data sequence) speeds up the global solution and achieves\nnoise reduction. On the other hand, topology preservation (achieved by\nmonitoring the Jacobian determinant) allows to radically rule out distortions\nwhile carefully controlling the size of allowed expansions and contractions.\nOur variational approach is carried out on a realistic dataset as well as on a\nsimulated one. We demonstrate how our proposed variational solution deals with\ncomplex deformations through careful numerical experiments. While maintaining\nthe accuracy of the solution, the low-rank preprocessing is shown to speed up\nthe convergence of the variational problem. Beyond cardiac motion estimation,\nour approach is promising for the analysis of other organs that experience\nmotion.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 22:38:48 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 13:24:48 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Aviles", "Angelica I.", ""], ["Widlak", "Thomas", ""], ["Casals", "Alicia", ""], ["Nillesen", "Maartje M.", ""], ["Ammari", "Habib", ""]]}, {"id": "1611.02764", "submitter": "Kipton Barros", "authors": "Nicholas Lubbers, Turab Lookman, Kipton Barros", "title": "Inferring low-dimensional microstructure representations using\n  convolutional neural networks", "comments": "14 Pages, 12 Figures", "journal-ref": "Phys. Rev. E 96, 052111 (2017)", "doi": "10.1103/PhysRevE.96.052111", "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply recent advances in machine learning and computer vision to a central\nproblem in materials informatics: The statistical representation of\nmicrostructural images. We use activations in a pre-trained convolutional\nneural network to provide a high-dimensional characterization of a set of\nsynthetic microstructural images. Next, we use manifold learning to obtain a\nlow-dimensional embedding of this statistical characterization. We show that\nthe low-dimensional embedding extracts the parameters used to generate the\nimages. According to a variety of metrics, the convolutional neural network\nmethod yields dramatically better embeddings than the analogous method derived\nfrom two-point correlations alone.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 23:10:24 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 21:40:46 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lubbers", "Nicholas", ""], ["Lookman", "Turab", ""], ["Barros", "Kipton", ""]]}, {"id": "1611.02767", "submitter": "Huayan Wang", "authors": "Huayan Wang, Anna Chen, Yi Liu, Dileep George, D. Scott Phoenix", "title": "A backward pass through a CNN using a generative model of its\n  activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have shown to be a practical way of building a very complex\nmapping between a pre-specified input space and output space. For example, a\nconvolutional neural network (CNN) mapping an image into one of a thousand\nobject labels is approaching human performance in this particular task. However\nthe mapping (neural network) does not automatically lend itself to other forms\nof queries, for example, to detect/reconstruct object instances, to enforce\ntop-down signal on ambiguous inputs, or to recover object instances from\nocclusion. One way to address these queries is a backward pass through the\nnetwork that fuses top-down and bottom-up information. In this paper, we show a\nway of building such a backward pass by defining a generative model of the\nneural network's activations. Approximate inference of the model would\nnaturally take the form of a backward pass through the CNN layers, and it\naddresses the aforementioned queries in a unified framework.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 23:18:50 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Wang", "Huayan", ""], ["Chen", "Anna", ""], ["Liu", "Yi", ""], ["George", "Dileep", ""], ["Phoenix", "D. Scott", ""]]}, {"id": "1611.02776", "submitter": "Daoyuan Jia", "authors": "Daoyuan Jia, Yongchi Su, Chunping Li", "title": "Deep Convolutional Neural Network for 6-DOF Image Localization", "comments": "will update soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an accurate and robust method for six degree of freedom image\nlocalization. There are two key-points of our method, 1. automatic immense\nphoto synthesis and labeling from point cloud model and, 2. pose estimation\nwith deep convolutional neural networks regression. Our model can directly\nregresses 6-DOF camera poses from images, accurately describing where and how\nit was captured. We achieved an accuracy within 1 meters and 1 degree on our\nout-door dataset, which covers about 2 acres on our school campus.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 23:59:16 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Jia", "Daoyuan", ""], ["Su", "Yongchi", ""], ["Li", "Chunping", ""]]}, {"id": "1611.02788", "submitter": "Wolfgang Lehrach", "authors": "Xinghua Lou, Ken Kansky, Wolfgang Lehrach, CC Laan, Bhaskara Marthi,\n  D. Scott Phoenix, Dileep George", "title": "Generative Shape Models: Joint Text Recognition and Segmentation with\n  Very Little Training Data", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 2016", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that a generative model for object shapes can achieve state of\nthe art results on challenging scene text recognition tasks, and with orders of\nmagnitude fewer training images than required for competing discriminative\nmethods. In addition to transcribing text from challenging images, our method\nperforms fine-grained instance segmentation of characters. We show that our\nmodel is more robust to both affine transformations and non-affine deformations\ncompared to previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 01:12:38 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Lou", "Xinghua", ""], ["Kansky", "Ken", ""], ["Lehrach", "Wolfgang", ""], ["Laan", "CC", ""], ["Marthi", "Bhaskara", ""], ["Phoenix", "D. Scott", ""], ["George", "Dileep", ""]]}, {"id": "1611.02803", "submitter": "Jhony Heriberto Giraldo Zuluaga", "authors": "Jhony-Heriberto Giraldo-Zuluaga, Augusto Salazar, Juan M. Daza", "title": "Semi-Supervised Recognition of the Diploglossus Millepunctatus Lizard\n  Species using Artificial Vision Algorithms", "comments": "arXiv admin note: text overlap with arXiv:1603.00841", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animal biometrics is an important requirement for monitoring and conservation\ntasks. The classical animal biometrics risk the animals' integrity, are\nexpensive for numerous animals, and depend on expert criterion. The\nnon-invasive biometrics techniques offer alternatives to manage the\naforementioned problems. In this paper we propose an automatic segmentation and\nidentification algorithm based on artificial vision algorithms to recognize\nDiploglossus millepunctatus. Diploglossus millepunctatus is an endangered\nlizard species. The algorithm is based on two stages: automatic segmentation to\nremove the subjective evaluation, and one identification stage to reduce the\nanalysis time. A 82.87% of correct segmentation in average is reached.\nMeanwhile the identification algorithm is achieved with euclidean distance\npoint algorithms such as Iterative Closest Point and Procrustes Analysis. A\nperformance of 92.99% on the top 1, and a 96.82% on the top 5 is reached. The\ndeveloped software, and the database used in this paper are publicly available\nfor download from the web page of the project.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 02:54:59 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Giraldo-Zuluaga", "Jhony-Heriberto", ""], ["Salazar", "Augusto", ""], ["Daza", "Juan M.", ""]]}, {"id": "1611.02862", "submitter": "Yaniv Romano", "authors": "Yaniv Romano, Michael Elad, and Peyman Milanfar", "title": "The Little Engine that Could: Regularization by Denoising (RED)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removal of noise from an image is an extensively studied problem in image\nprocessing. Indeed, the recent advent of sophisticated and highly effective\ndenoising algorithms lead some to believe that existing methods are touching\nthe ceiling in terms of noise removal performance. Can we leverage this\nimpressive achievement to treat other tasks in image processing? Recent work\nhas answered this question positively, in the form of the Plug-and-Play Prior\n($P^3$) method, showing that any inverse problem can be handled by sequentially\napplying image denoising steps. This relies heavily on the ADMM optimization\ntechnique in order to obtain this chained denoising interpretation.\n  Is this the only way in which tasks in image processing can exploit the image\ndenoising engine? In this paper we provide an alternative, more powerful and\nmore flexible framework for achieving the same goal. As opposed to the $P^3$\nmethod, we offer Regularization by Denoising (RED): using the denoising engine\nin defining the regularization of the inverse problem. We propose an explicit\nimage-adaptive Laplacian-based regularization functional, making the overall\nobjective functional clearer and better defined. With a complete flexibility to\nchoose the iterative optimization procedure for minimizing the above\nfunctional, RED is capable of incorporating any image denoising algorithm,\ntreat general inverse problems very effectively, and is guaranteed to converge\nto the globally optimal result. We test this approach and demonstrate\nstate-of-the-art results in the image deblurring and super-resolution problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:32:29 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 06:58:07 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 10:36:54 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Romano", "Yaniv", ""], ["Elad", "Michael", ""], ["Milanfar", "Peyman", ""]]}, {"id": "1611.02869", "submitter": "Jens Sj\\\"olund", "authors": "Jens Sj\\\"olund, Anders Eklund, Evren \\\"Ozarslan and Hans Knutsson", "title": "Gaussian process regression can turn non-uniform and undersampled\n  diffusion MRI data into diffusion spectrum imaging", "comments": "5 pages", "journal-ref": "2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI\n  2017)", "doi": "10.1109/ISBI.2017.7950634", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use Gaussian process regression to accurately estimate the\ndiffusion MRI signal at arbitrary locations in q-space. By estimating the\nsignal on a grid, we can do synthetic diffusion spectrum imaging:\nreconstructing the ensemble averaged propagator (EAP) by an inverse Fourier\ntransform. We also propose an alternative reconstruction method guaranteeing a\nnonnegative EAP that integrates to unity. The reconstruction is validated on\ndata simulated from two Gaussians at various crossing angles. Moreover, we\ndemonstrate on non-uniformly sampled in vivo data that the method is far\nsuperior to linear interpolation, and allows a drastic undersampling of the\ndata with only a minor loss of accuracy. We envision the method as a potential\nreplacement for standard diffusion spectrum imaging, in particular when\nacquistion time is limited.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:54:47 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sj\u00f6lund", "Jens", ""], ["Eklund", "Anders", ""], ["\u00d6zarslan", "Evren", ""], ["Knutsson", "Hans", ""]]}, {"id": "1611.02879", "submitter": "Abhinav Thanda", "authors": "Abhinav Thanda, Shankar M Venkatesan", "title": "Audio Visual Speech Recognition using Deep Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a training algorithm for an audio-visual automatic\nspeech recognition (AV-ASR) system using deep recurrent neural network\n(RNN).First, we train a deep RNN acoustic model with a Connectionist Temporal\nClassification (CTC) objective function. The frame labels obtained from the\nacoustic model are then used to perform a non-linear dimensionality reduction\nof the visual features using a deep bottleneck network. Audio and visual\nfeatures are fused and used to train a fusion RNN. The use of bottleneck\nfeatures for visual modality helps the model to converge properly during\ntraining. Our system is evaluated on GRID corpus. Our results show that\npresence of visual modality gives significant improvement in character error\nrate (CER) at various levels of noise even when the model is trained without\nnoisy data. We also provide a comparison of two fusion methods: feature fusion\nand decision fusion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 10:24:52 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Thanda", "Abhinav", ""], ["Venkatesan", "Shankar M", ""]]}, {"id": "1611.02886", "submitter": "Azadeh Mozafari", "authors": "Azadeh S. Mozafari, David Vazquez, Mansour Jamzad and Antonio M. Lopez", "title": "Node-Adapt, Path-Adapt and Tree-Adapt:Model-Transfer Domain Adaptation\n  for Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest (RF) is a successful paradigm for learning classifiers due to\nits ability to learn from large feature spaces and seamlessly integrate\nmulti-class classification, as well as the achieved accuracy and processing\nefficiency. However, as many other classifiers, RF requires domain adaptation\n(DA) provided that there is a mismatch between the training (source) and\ntesting (target) domains which provokes classification degradation.\nConsequently, different RF-DA methods have been proposed, which not only\nrequire target-domain samples but revisiting the source-domain ones, too. As\nnovelty, we propose three inherently different methods (Node-Adapt, Path-Adapt\nand Tree-Adapt) that only require the learned source-domain RF and a relatively\nfew target-domain samples for DA, i.e. source-domain samples do not need to be\navailable. To assess the performance of our proposals we focus on image-based\nobject detection, using the pedestrian detection problem as challenging\nproof-of-concept. Moreover, we use the RF with expert nodes because it is a\ncompetitive patch-based pedestrian model. We test our Node-, Path- and\nTree-Adapt methods in standard benchmarks, showing that DA is largely achieved.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 10:59:58 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Mozafari", "Azadeh S.", ""], ["Vazquez", "David", ""], ["Jamzad", "Mansour", ""], ["Lopez", "Antonio M.", ""]]}, {"id": "1611.03059", "submitter": "Abhay Shah", "authors": "Abhay Shah, Michael D. Abramoff and Xiaodong Wu", "title": "Optimal Surface Segmentation with Convex Priors in Irregularly Sampled\n  Space", "comments": "20 pages, Medical Image Analysis (2019)", "journal-ref": null, "doi": "10.1016/j.media.2019.02.004", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal surface segmentation is a state-of-the-art method used for\nsegmentation of multiple globally optimal surfaces in volumetric datasets. The\nmethod is widely used in numerous medical image segmentation applications.\nHowever, nodes in the graph based optimal surface segmentation method typically\nencode uniformly distributed orthogonal voxels of the volume. Thus the\nsegmentation cannot attain an accuracy greater than a single unit voxel, i.e.\nthe distance between two adjoining nodes in graph space. Segmentation accuracy\nhigher than a unit voxel is achievable by exploiting partial volume information\nin the voxels which shall result in non-equidistant spacing between adjoining\ngraph nodes. This paper reports a generalized graph based multiple surface\nsegmentation method with convex priors which can optimally segment the target\nsurfaces in an irregularly sampled space. The proposed method allows\nnon-equidistant spacing between the adjoining graph nodes to achieve subvoxel\nsegmentation accuracy by utilizing the partial volume information in the\nvoxels. The partial volume information in the voxels is exploited by computing\na displacement field from the original volume data to identify the\nsubvoxel-accurate centers within each voxel resulting in non-equidistant\nspacing between the adjoining graph nodes. The smoothness of each surface\nmodeled as a convex constraint governs the connectivity and regularity of the\nsurface. We employ an edge-based graph representation to incorporate the\nnecessary constraints and the globally optimal solution is obtained by\ncomputing a minimum s-t cut. The proposed method was validated on 10\nintravascular multi-frame ultrasound image datasets for subvoxel segmentation\naccuracy. In all cases, the approach yielded highly accurate results. Our\napproach can be readily extended to higher-dimensional segmentations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 19:46:49 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 21:09:01 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2019 19:38:25 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Shah", "Abhay", ""], ["Abramoff", "Michael D.", ""], ["Wu", "Xiaodong", ""]]}, {"id": "1611.03130", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Dominic Bernath, Michele Magno, Luca Benini", "title": "Computationally Efficient Target Classification in Multispectral Image\n  Data with Deep Neural Networks", "comments": "Presented at SPIE Security + Defence 2016 Proc. SPIE 9997, Target and\n  Background Signatures II", "journal-ref": null, "doi": "10.1117/12.2241383", "report-no": null, "categories": "cs.CV cs.AI cs.NE eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and classifying targets in video streams from surveillance cameras\nis a cumbersome, error-prone and expensive task. Often, the incurred costs are\nprohibitive for real-time monitoring. This leads to data being stored locally\nor transmitted to a central storage site for post-incident examination. The\nrequired communication links and archiving of the video data are still\nexpensive and this setup excludes preemptive actions to respond to imminent\nthreats. An effective way to overcome these limitations is to build a smart\ncamera that transmits alerts when relevant video sequences are detected. Deep\nneural networks (DNNs) have come to outperform humans in visual classifications\ntasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be\nextended to make use of higher-dimensional input data such as multispectral\ndata. We explore this opportunity in terms of achievable accuracy and required\ncomputational effort. To analyze the precision of DNNs for scene labeling in an\nurban surveillance scenario we have created a dataset with 8 classes obtained\nin a field experiment. We combine an RGB camera with a 25-channel VIS-NIR\nsnapshot sensor to assess the potential of multispectral image data for target\nclassification. We evaluate several new DNNs, showing that the spectral\ninformation fused together with the RGB frames can be used to improve the\naccuracy of the system or to achieve similar accuracy with a 3x smaller\ncomputation effort. We achieve a very high per-pixel accuracy of 99.1%. Even\nfor scarcely occurring, but particularly interesting classes, such as cars, 75%\nof the pixels are labeled correctly with errors occurring only around the\nborder of the objects. This high accuracy was obtained with a training set of\nonly 30 labeled images, paving the way for fast adaptation to various\napplication scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 23:13:18 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Bernath", "Dominic", ""], ["Magno", "Michele", ""], ["Benini", "Luca", ""]]}, {"id": "1611.03193", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Zhizhen Zhao, Amit Singer", "title": "Mahalanobis Distance for Class Averaging of Cryo-EM Images", "comments": "Final version accepted to the 14th IEEE International Symposium on\n  Biomedical Imaging (ISBI 2017)", "journal-ref": null, "doi": "10.1109/ISBI.2017.7950605", "report-no": null, "categories": "stat.AP cs.CV q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle reconstruction (SPR) from cryo-electron microscopy (EM) is a\ntechnique in which the 3D structure of a molecule needs to be determined from\nits contrast transfer function (CTF) affected, noisy 2D projection images taken\nat unknown viewing directions. One of the main challenges in cryo-EM is the\ntypically low signal to noise ratio (SNR) of the acquired images. 2D\nclassification of images, followed by class averaging, improves the SNR of the\nresulting averages, and is used for selecting particles from micrographs and\nfor inspecting the particle images. We introduce a new affinity measure, akin\nto the Mahalanobis distance, to compare cryo-EM images belonging to different\ndefocus groups. The new similarity measure is employed to detect similar\nimages, thereby leading to an improved algorithm for class averaging. We\nevaluate the performance of the proposed class averaging procedure on synthetic\ndatasets, obtaining state of the art classification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 05:55:27 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 15:11:24 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 15:56:31 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 03:40:18 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhao", "Zhizhen", ""], ["Singer", "Amit", ""]]}, {"id": "1611.03217", "submitter": "Somnath Mukherjee", "authors": "Somnath Mukherjee, Soumyajit Ganguly", "title": "Real Time Video Analysis using Smart Phone Camera for Stroboscopic Image", "comments": "5 pages, 2 Figures in SPIE Electronic Imaging 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion capturing and there by segmentation of the motion of any moving object\nfrom a sequence of continuous images or a video is not an exceptional task in\ncomputer vision area. Smart-phone camera application is an added integration\nfor the development of such tasks and it also provides for a smooth testing. A\nnew approach has been proposed for segmenting out the foreground moving object\nfrom the background and then masking the sequential motion with the static\nbackground which is commonly known as stroboscopic image. In this paper the\nwhole process of the stroboscopic image construction technique has been clearly\ndescribed along with some necessary constraints which is due to the traditional\nproblem of estimating and modeling dynamic background changes. The background\nsubtraction technique has been properly estimated here and number of sequential\nmotion have also been calculated with the correlation between the motion of the\nobject and its time of occurrence. This can be a very effective application\nthat can replace the traditional stroboscopic system using high end SLR\ncameras, tripod stand, shutter speed control and position etc.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 08:37:05 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Mukherjee", "Somnath", ""], ["Ganguly", "Soumyajit", ""]]}, {"id": "1611.03268", "submitter": "Vania Estrela Dr.", "authors": "Alessandra M. Coelho, Vania V. Estrela, Felipe P. do Carmo, Sandro R.\n  Fernandes", "title": "Error concealment by means of motion refinement and regularized Bregman\n  divergence", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of error concealment in video transmission\nsystems over noisy channels employing Bregman divergences along with\nregularization. Error concealment intends to improve the effects of\ndisturbances at the reception due to bit-errors or cell loss in packet\nnetworks. Bregman regularization gives accurate answers after just some\niterations with fast convergence, better accuracy, and stability. This\ntechnique has an adaptive nature: the regularization functional is updated\naccording to Bregman functions that change from iteration to iteration\naccording to the nature of the neighborhood under study at iteration n.\nNumerical experiments show that high-quality regularization parameter estimates\ncan be obtained. The convergence is sped up while turning the regularization\nparameter estimation less empiric, and more automatic.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 11:39:34 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Coelho", "Alessandra M.", ""], ["Estrela", "Vania V.", ""], ["Carmo", "Felipe P. do", ""], ["Fernandes", "Sandro R.", ""]]}, {"id": "1611.03270", "submitter": "Adi Dafni", "authors": "Adi Dafni, Yael Moses and Shai Avidan", "title": "Detecting Moving Regions in CrowdCam Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the novel problem of detecting dynamic regions in CrowdCam images,\na set of still images captured by a group of people. These regions capture the\nmost interesting parts of the scene, and detecting them plays an important role\nin the analysis of visual data. Our method is based on the observation that\nmatching static points must satisfy the epipolar geometry constraints, but\ncomputing exact matches is challenging. Instead, we compute the probability\nthat a pixel has a match, not necessarily the correct one, along the\ncorresponding epipolar line. The complement of this probability is not\nnecessarily the probability of a dynamic point because of occlusions, noise,\nand matching errors. Therefore, information from all pairs of images is\naggregated to obtain a high quality dynamic probability map, per image.\nExperiments on challenging datasets demonstrate the effectiveness of the\nalgorithm on a broad range of settings; no prior knowledge about the scene, the\ncamera characteristics or the camera locations is required.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 11:58:52 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Dafni", "Adi", ""], ["Moses", "Yael", ""], ["Avidan", "Shai", ""]]}, {"id": "1611.03313", "submitter": "Boyu Wang", "authors": "Boyu Wang, Kevin Yager, Dantong Yu, Minh Hoai", "title": "X-ray Scattering Image Classification Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual inspection of x-ray scattering images is a powerful technique for\nprobing the physical structure of materials at the molecular scale. In this\npaper, we explore the use of deep learning to develop methods for automatically\nanalyzing x-ray scattering images. In particular, we apply Convolutional Neural\nNetworks and Convolutional Autoencoders for x-ray scattering image\nclassification. To acquire enough training data for deep learning, we use\nsimulation software to generate synthetic x-ray scattering images. Experiments\nshow that deep learning methods outperform previously published methods by 10\\%\non synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 14:32:24 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Boyu", ""], ["Yager", "Kevin", ""], ["Yu", "Dantong", ""], ["Hoai", "Minh", ""]]}, {"id": "1611.03318", "submitter": "Jonathan Byrne", "authors": "Jonathan Byrne, Debra Laefer", "title": "Variables effecting photomosaic reconstruction and ortho-rectification\n  from aerial survey datasets", "comments": "Presented at CERAI Conference 2016, Galway", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmanned aerial vehicles now make it possible to obtain high quality aerial\nimagery at a low cost, but processing those images into a single, useful entity\nis neither simple nor seamless. Specifically, there are factors that must be\naddressed when merging multiple images into a single coherent one. While\northo-rectification can be done, it tends to be expensive and time consuming.\nImage stitching offers a more economical, low-tech approach. However direct\napplication tends to fail for low-elevation imagery due to one or more factors\nincluding insufficient keypoints, parallax issues, and homogeneity of the\nsurveyed area. This paper discusses these problems and possible solutions when\nusing techniques such as image stitching and structure from motion for\ngenerating ortho-rectified imagery. These are presented in terms of actual\nIrish projects including the Boland's Mills building in Dublin's city centre,\nthe Kilmoon Cross Farm, and the Richview buildings on the University College\nDublin campus. Implications for various Irish industries are explained in terms\nof both urban and rural projects.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 14:42:28 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Byrne", "Jonathan", ""], ["Laefer", "Debra", ""]]}, {"id": "1611.03341", "submitter": "Lianlin Li Dr", "authors": "Long Gang Wang, Lianlin Li, Tie Jun Cui", "title": "Fast Algorithm of High-resolution Microwave Imaging Using the\n  Non-parametric Generalized Reflectivity Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient algorithm of high-resolution microwave\nimaging based on the concept of generalized reflectivity. The contribution made\nin this paper is two-fold. We introduce the concept of non-parametric\ngeneralized reflectivity (GR, for short) as a function of operational\nfrequencies and view angles, etc. The GR extends the conventional Born-based\nimaging model, i.e., single-scattering model, into that accounting for more\nrealistic interaction between the electromagnetic wavefield and imaged scene.\nAfterwards, the GR-based microwave imaging is formulated in the convex of\nsparsity-regularized optimization. Typically, the sparsity-regularized\noptimization requires the implementation of iterative strategy, which is\ncomputationally expensive, especially for large-scale problems. To break this\nbottleneck, we convert the imaging problem into the problem of physics-driven\nimage processing by introducing a dual transformation. Moreover, this image\nprocessing is performed over overlapping patches, which can be efficiently\nsolved in the parallel or distributed manner. In this way, the proposed\nhigh-resolution imaging methodology could be applicable to large-scale\nmicrowave imaging problems. Selected simulation results are provided to\ndemonstrate the state-of-art performance of proposed methodology.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 07:34:44 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Wang", "Long Gang", ""], ["Li", "Lianlin", ""], ["Cui", "Tie Jun", ""]]}, {"id": "1611.03469", "submitter": "Amelia Carolina Sparavigna", "authors": "Amelia Carolina Sparavigna", "title": "Evaluating Urbanization from Satellite and Aerial Images by means of a\n  statistical approach to the texture analysis", "comments": "Keywords: Image analysis, 2D textures; texture functions, satellite\n  images, aerial images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods are usually applied in the processing of digital images\nfor the analysis of the textures displayed by them. Aiming to evaluate the\nurbanization of a given location from satellite or aerial images, here we\nconsider a simple processing to distinguish in them the 'urban' from the\n'rural' texture. The method is based on the mean values and the standard\ndeviations of the colour tones of image pixels. The processing of the input\nimages allows to obtain some maps from which a quantitative evaluation of the\ntextures can be obtained.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 20:16:57 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Sparavigna", "Amelia Carolina", ""]]}, {"id": "1611.03566", "submitter": "Ahmad Hasan", "authors": "Ahmad Hasan, Ashraf Qadir, Ian Nordeng, Jeremiah Neubert", "title": "Construction Inspection through Spatial Database", "comments": "8 pages, 8 figues, 3 tables, 1 graph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel pipeline for development of an efficient set of\ntools for extracting information from the video of a structure, captured by an\nUnmanned Aircraft System (UAS) to produce as-built documentation to aid\ninspection of large multi-storied building during construction. Our system uses\nthe output from a Simultaneous Localization and Mapping system and a 3D CAD\nmodel of the structure in order to construct a spatial database to store images\ninto the 3D CAD model space. This allows the user to perform a spatial query\nfor images through spatial indexing into the 3D CAD model space. The image\nreturned by the spatial query is used to extract metric information. The\nspatial database is also used to generate a 3D textured model which provides a\nvisual as-built documentation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 02:23:32 GMT"}, {"version": "v2", "created": "Sat, 4 Feb 2017 21:00:33 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 21:29:24 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hasan", "Ahmad", ""], ["Qadir", "Ashraf", ""], ["Nordeng", "Ian", ""], ["Neubert", "Jeremiah", ""]]}, {"id": "1611.03589", "submitter": "Renlong Hang", "authors": "Qingshan Liu (Senior Member, IEEE), Renlong Hang, Huihui Song, Fuping\n  Zhu, Javier Plaza (Senior Member, IEEE) and Antonio Plaza (Fellow, IEEE)", "title": "Adaptive Deep Pyramid Matching for Remote Sensing Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have attracted increasing attention in\nthe remote sensing community. Most CNNs only take the last fully-connected\nlayers as features for the classification of remotely sensed images, discarding\nthe other convolutional layer features which may also be helpful for\nclassification purposes. In this paper, we propose a new adaptive deep pyramid\nmatching (ADPM) model that takes advantage of the features from all of the\nconvolutional layers for remote sensing image classification. To this end, the\noptimal fusing weights for different convolutional layers are learned from the\ndata itself. In remotely sensed scenes, the objects of interest exhibit\ndifferent scales in distinct scenes, and even a single scene may contain\nobjects with different sizes. To address this issue, we select the CNN with\nspatial pyramid pooling (SPP-net) as the basic deep network, and further\nconstruct a multi-scale ADPM model to learn complementary information from\nmulti-scale images. Our experiments have been conducted using two widely used\nremote sensing image databases, and the results show that the proposed method\nsignificantly improves the performance when compared to other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 05:17:56 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Liu", "Qingshan", "", "Senior Member, IEEE"], ["Hang", "Renlong", "", "Senior Member, IEEE"], ["Song", "Huihui", "", "Senior Member, IEEE"], ["Zhu", "Fuping", "", "Senior Member, IEEE"], ["Plaza", "Javier", "", "Senior Member, IEEE"], ["Plaza", "Antonio", "", "Fellow, IEEE"]]}, {"id": "1611.03591", "submitter": "Renlong Hang", "authors": "Qingshan Liu, Renlong Hang, Huihui Song, Zhi Li", "title": "Learning Multi-Scale Deep Features for High-Resolution Satellite Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-scale deep feature learning method for\nhigh-resolution satellite image classification. Specifically, we firstly warp\nthe original satellite image into multiple different scales. The images in each\nscale are employed to train a deep convolutional neural network (DCNN).\nHowever, simultaneously training multiple DCNNs is time-consuming. To address\nthis issue, we explore DCNN with spatial pyramid pooling (SPP-net). Since\ndifferent SPP-nets have the same number of parameters, which share the\nidentical initial values, and only fine-tuning the parameters in\nfully-connected layers ensures the effectiveness of each network, thereby\ngreatly accelerating the training process. Then, the multi-scale satellite\nimages are fed into their corresponding SPP-nets respectively to extract\nmulti-scale deep features. Finally, a multiple kernel learning method is\ndeveloped to automatically learn the optimal combination of such features.\nExperiments on two difficult datasets show that the proposed method achieves\nfavorable performance compared to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 05:31:42 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Liu", "Qingshan", ""], ["Hang", "Renlong", ""], ["Song", "Huihui", ""], ["Li", "Zhi", ""]]}, {"id": "1611.03607", "submitter": "Masaya Inoue", "authors": "Masaya Inoue, Sozo Inoue, Takeshi Nishida", "title": "Deep Recurrent Neural Network for Mobile Human Activity Recognition with\n  High Throughput", "comments": "10 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method of human activity recognition with high\nthroughput from raw accelerometer data applying a deep recurrent neural network\n(DRNN), and investigate various architectures and its combination to find the\nbest parameter values. The \"high throughput\" refers to short time at a time of\nrecognition. We investigated various parameters and architectures of the DRNN\nby using the training dataset of 432 trials with 6 activity classes from 7\npeople. The maximum recognition rate was 95.42% and 83.43% against the test\ndata of 108 segmented trials each of which has single activity class and 18\nmultiple sequential trials, respectively. Here, the maximum recognition rates\nby traditional methods were 71.65% and 54.97% for each. In addition, the\nefficiency of the found parameters was evaluated by using additional dataset.\nFurther, as for throughput of the recognition per unit time, the constructed\nDRNN was requiring only 1.347 [ms], while the best traditional method required\n11.031 [ms] which includes 11.027 [ms] for feature calculation. These\nadvantages are caused by the compact and small architecture of the constructed\nreal time oriented DRNN.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 08:21:09 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Inoue", "Masaya", ""], ["Inoue", "Sozo", ""], ["Nishida", "Takeshi", ""]]}, {"id": "1611.03666", "submitter": "Vania Estrela Dr.", "authors": "L. A. Rivera, Vania V. Estrela, P. C. P. Carvalho", "title": "Oriented bounding boxes using multiresolution contours for fast\n  interference detection of arbitrary geometry objects", "comments": "8 pages, 10 figures", "journal-ref": "The 12-th International Conference in Central Europe on Computer\n  Graphics, Visualization and Computer Vision'2004, WSCG 2004, University of\n  West Bohemia, Campus Bory, Plzen-Bory, Czech Republic, February 2-6, 2004", "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interference detection of arbitrary geometric objects is not a trivial task\ndue to the heavy computational load imposed by implementation issues. The\nhierarchically structured bounding boxes help us to quickly isolate the contour\nof segments in interference. In this paper, a new approach is introduced to\ntreat the interference detection problem involving the representation of\narbitrary shaped objects. Our proposed method relies upon searching for the\nbest possible way to represent contours by means of hierarchically structured\nrectangular oriented bounding boxes. This technique handles 2D objects\nboundaries defined by closed B-spline curves with roughness details. Each\noriented box is adapted and fitted to the segments of the contour using second\norder statistical indicators from some elements of the segments of the object\ncontour in a multiresolution framework. Our method is efficient and robust when\nit comes to 2D animations in real time. It can deal with smooth curves and\npolygonal approximations as well results are present to illustrate the\nperformance of the new method.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 11:50:59 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Rivera", "L. A.", ""], ["Estrela", "Vania V.", ""], ["Carvalho", "P. C. P.", ""]]}, {"id": "1611.03673", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J.\n  Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray\n  Kavukcuoglu, Dharshan Kumaran and Raia Hadsell", "title": "Learning to Navigate in Complex Environments", "comments": "11 pages, 5 appendix pages, 11 figures, 3 tables, under review as a\n  conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 12:14:45 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 18:02:53 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 11:15:22 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Mirowski", "Piotr", ""], ["Pascanu", "Razvan", ""], ["Viola", "Fabio", ""], ["Soyer", "Hubert", ""], ["Ballard", "Andrew J.", ""], ["Banino", "Andrea", ""], ["Denil", "Misha", ""], ["Goroshin", "Ross", ""], ["Sifre", "Laurent", ""], ["Kavukcuoglu", "Koray", ""], ["Kumaran", "Dharshan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1611.03679", "submitter": "Kyong Hwan Jin", "authors": "Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, Michael Unser", "title": "Deep Convolutional Neural Network for Inverse Problems in Imaging", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, vol. 26, no. 9, pp.\n  4509-4522, Sept. 2017", "doi": "10.1109/TIP.2017.2713099", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel deep convolutional neural network\n(CNN)-based algorithm for solving ill-posed inverse problems. Regularized\niterative algorithms have emerged as the standard approach to ill-posed inverse\nproblems in the past few decades. These methods produce excellent results, but\ncan be challenging to deploy in practice due to factors including the high\ncomputational cost of the forward and adjoint operators and the difficulty of\nhyper parameter selection. The starting point of our work is the observation\nthat unrolled iterative methods have the form of a CNN (filtering followed by\npoint-wise non-linearity) when the normal operator (H*H, the adjoint of H times\nH) of the forward model is a convolution. Based on this observation, we propose\nusing direct inversion followed by a CNN to solve normal-convolutional inverse\nproblems. The direct inversion encapsulates the physical model of the system,\nbut leads to artifacts when the problem is ill-posed; the CNN combines\nmultiresolution decomposition and residual learning in order to learn to remove\nthese artifacts while preserving image structure. We demonstrate the\nperformance of the proposed network in sparse-view reconstruction (down to 50\nviews) on parallel beam X-ray computed tomography in synthetic phantoms as well\nas in real experimental sinograms. The proposed network outperforms total\nvariation-regularized iterative reconstruction for the more realistic phantoms\nand requires less than a second to reconstruct a 512 x 512 image on GPU.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 12:35:08 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Jin", "Kyong Hwan", ""], ["McCann", "Michael T.", ""], ["Froustey", "Emmanuel", ""], ["Unser", "Michael", ""]]}, {"id": "1611.03718", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Miriam Bellver, Xavier Giro-i-Nieto, Ferran Marques and Jordi Torres", "title": "Hierarchical Object Detection with Deep Reinforcement Learning", "comments": "Deep Reinforcement Learning Workshop (NIPS 2016). Project page at\n  https://imatge-upc.github.io/detection-2016-nipsws/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for performing hierarchical object detection in images\nguided by a deep reinforcement learning agent. The key idea is to focus on\nthose parts of the image that contain richer information and zoom on them. We\ntrain an intelligent agent that, given an image window, is capable of deciding\nwhere to focus the attention among five different predefined region candidates\n(smaller windows). This procedure is iterated providing a hierarchical image\nanalysis.We compare two different candidate proposal strategies to guide the\nobject search: with and without overlap. Moreover, our work compares two\ndifferent strategies to extract features from a convolutional neural network\nfor each region proposal: a first one that computes new feature maps for each\nregion proposal, and a second one that computes the feature maps for the whole\nimage to later generate crops for each region proposal. Experiments indicate\nbetter results for the overlapping candidate proposal strategy and a loss of\nperformance for the cropped image features due to the loss of spatial\nresolution. We argue that, while this loss seems unavoidable when working with\nlarge amounts of object candidates, the much more reduced amount of region\nproposals generated by our reinforcement learning agent allows considering to\nextract features for each location without sharing convolutional computation\namong regions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 14:25:54 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 14:31:07 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bellver", "Miriam", ""], ["Giro-i-Nieto", "Xavier", ""], ["Marques", "Ferran", ""], ["Torres", "Jordi", ""]]}, {"id": "1611.03749", "submitter": "Ertunc Erdil", "authors": "Ertunc Erdil, Sinan Y{\\i}ld{\\i}r{\\i}m, M\\\"ujdat \\c{C}etin, Tolga\n  Ta\\c{s}dizen", "title": "MCMC Shape Sampling for Image Segmentation with Nonparametric Shape\n  Priors", "comments": "Computer Vision and Pattern Recognition conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmenting images of low quality or with missing data is a challenging\nproblem. Integrating statistical prior information about the shapes to be\nsegmented can improve the segmentation results significantly. Most shape-based\nsegmentation algorithms optimize an energy functional and find a point estimate\nfor the object to be segmented. This does not provide a measure of the degree\nof confidence in that result, neither does it provide a picture of other\nprobable solutions based on the data and the priors. With a statistical view,\naddressing these issues would involve the problem of characterizing the\nposterior densities of the shapes of the objects to be segmented. For such\ncharacterization, we propose a Markov chain Monte Carlo (MCMC) sampling-based\nimage segmentation algorithm that uses statistical shape priors. In addition to\nbetter characterization of the statistical structure of the problem, such an\napproach would also have the potential to address issues with getting stuck at\nlocal optima, suffered by existing shape-based segmentation methods. Our\napproach is able to characterize the posterior probability density in the space\nof shapes through its samples, and to return multiple solutions, potentially\nfrom different modes of a multimodal probability density, which would be\nencountered, e.g., in segmenting objects from multiple shape classes. We\npresent promising results on a variety of data sets. We also provide an\nextension for segmenting shapes of objects with parts that can go through\nindependent shape variations. This extension involves the use of local shape\npriors on object parts and provides robustness to limitations in shape training\ndata size.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:37:06 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Erdil", "Ertunc", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""], ["\u00c7etin", "M\u00fcjdat", ""], ["Ta\u015fdizen", "Tolga", ""]]}, {"id": "1611.03811", "submitter": "Margarita Osadchy", "authors": "Mor Ohana, Orr Dunkelman, Stuart Gibson, Margarita Osadchy", "title": "HoneyFaces: Increasing the Security and Privacy of Authentication Using\n  Synthetic Facial Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges faced by Biometric-based authentication systems is\nthe need to offer secure authentication while maintaining the privacy of the\nbiometric data. Previous solutions, such as Secure Sketch and Fuzzy Extractors,\nrely on assumptions that cannot be guaranteed in practice, and often affect the\nauthentication accuracy.\n  In this paper, we introduce HoneyFaces: the concept of adding a large set of\nsynthetic faces (indistinguishable from real) into the biometric \"password\nfile\". This password inflation protects the privacy of users and increases the\nsecurity of the system without affecting the accuracy of the authentication. In\nparticular, privacy for the real users is provided by \"hiding\" them among a\nlarge number of fake users (as the distributions of synthetic and real faces\nare equal). In addition to maintaining the authentication accuracy, and thus\nnot affecting the security of the authentication process, HoneyFaces offer\nseveral security improvements: increased exfiltration hardness, improved\nleakage detection, and the ability to use a Two-server setting like in\nHoneyWords. Finally, HoneyFaces can be combined with other security and privacy\nmechanisms for biometric data.\n  We implemented the HoneyFaces system and tested it with a password file\ncomposed of 270 real users. The \"password file\" was then inflated to\naccommodate up to $2^{36.5}$ users (resulting in a 56.6 TB \"password file\"). At\nthe same time, the inclusion of additional faces does not affect the true\nacceptance rate or false acceptance rate which were 93.33\\% and 0.01\\%,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 18:40:32 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Ohana", "Mor", ""], ["Dunkelman", "Orr", ""], ["Gibson", "Stuart", ""], ["Osadchy", "Margarita", ""]]}, {"id": "1611.03873", "submitter": "Laura Rebollo-Neira", "authors": "Laura Rebollo-Neira", "title": "Effective sparse representation of X-Ray medical images", "comments": "Routines for implementing the approach are available on\n  http://www.nonlinear-approx.info/examples/node06.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective sparse representation of X-Ray medical images within the context of\ndata reduction is considered. The proposed framework is shown to render an\nenormous reduction in the cardinality of the data set required to represent\nthis class of images at very good quality. The particularity of the approach is\nthat it can be implemented at very competitive processing time and low memory\nrequirements\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 21:00:58 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Rebollo-Neira", "Laura", ""]]}, {"id": "1611.03915", "submitter": "Kuan-Ting Chen", "authors": "Kuan-Ting Chen and Jiebo Luo", "title": "When Fashion Meets Big Data: Discriminative Mining of Best Selling\n  Clothing Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of e-commence websites and the ease of online shopping,\nconsumers are embracing huge amounts of various options in products.\nUndeniably, shopping is one of the most essential activities in our society and\nstudying consumer's shopping behavior is important for the industry as well as\nsociology and psychology. Indisputable, one of the most popular e-commerce\ncategories is clothing business. There arises the needs for analysis of popular\nand attractive clothing features which could further boost many emerging\napplications, such as clothing recommendation and advertising. In this work, we\ndesign a novel system that consists of three major components: 1) exploring and\norganizing a large-scale clothing dataset from a online shopping website, 2)\npruning and extracting images of best-selling products in clothing item data\nand user transaction history, and 3) utilizing a machine learning based\napproach to discovering fine-grained clothing attributes as the representative\nand discriminative characteristics of popular clothing style elements. Through\nthe experiments over a large-scale online clothing shopping dataset, we\ndemonstrate the effectiveness of our proposed system, and obtain useful\ninsights on clothing consumption trends and profitable clothing features.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 23:58:06 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 05:34:33 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Chen", "Kuan-Ting", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.03968", "submitter": "Zhipeng Zeng", "authors": "Dapeng Luo, Zhipeng Zeng, Nong Sang, Xiang Wu, Longsheng Wei,\n  Quanzheng Mou, Jun Cheng, Chen Luo", "title": "Learning Scene-specific Object Detectors Based on a\n  Generative-Discriminative Model with Minimal Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One object class may show large variations due to diverse illuminations,\nbackgrounds and camera viewpoints. Traditional object detection methods often\nperform worse under unconstrained video environments. To address this problem,\nmany modern approaches model deep hierarchical appearance representations for\nobject detection. Most of these methods require a timeconsuming training\nprocess on large manual labelling sample set. In this paper, the proposed\nframework takes a remarkably different direction to resolve the multi-scene\ndetection problem in a bottom-up fashion. First, a scene-specific objector is\nobtained from a fully autonomous learning process triggered by marking several\nbounding boxes around the object in the first video frame via a mouse. Here the\nhuman labeled training data or a generic detector are not needed. Second, this\nlearning process is conveniently replicated many times in different\nsurveillance scenes and results in particular detectors under various camera\nviewpoints. Thus, the proposed framework can be employed in multi-scene object\ndetection applications with minimal supervision. Obviously, the initial\nscene-specific detector, initialized by several bounding boxes, exhibits poor\ndetection performance and is difficult to improve with traditional online\nlearning algorithm. Consequently, we propose Generative-Discriminative model to\npartition detection response space and assign each partition an individual\ndescriptor that progressively achieves high classification accuracy. A novel\nonline gradual optimized process is proposed to optimize the\nGenerative-Discriminative model and focus on the hard samples.Experimental\nresults on six video datasets show our approach achieves comparable performance\nto robust supervised methods, and outperforms the state of the art\nself-learning methods under varying imaging conditions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 08:15:26 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 06:07:08 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 06:17:02 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 01:58:08 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Luo", "Dapeng", ""], ["Zeng", "Zhipeng", ""], ["Sang", "Nong", ""], ["Wu", "Xiang", ""], ["Wei", "Longsheng", ""], ["Mou", "Quanzheng", ""], ["Cheng", "Jun", ""], ["Luo", "Chen", ""]]}, {"id": "1611.03999", "submitter": "David Freire-Obreg\\'on", "authors": "D. Freire-Obreg\\'on and M. Castrill\\'on-Santana and J. Lorenzo-Navarro", "title": "Optimized clothes segmentation to boost gender classification in\n  unconstrained scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several applications require demographic information of ordinary people in\nunconstrained scenarios. This is not a trivial task due to significant human\nappearance variations. In this work, we introduce trixels for clustering image\nregions, enumerating their advantages compared to superpixels. The classical\nGrabCut algorithm is later modified to segment trixels instead of pixels in an\nunsupervised context. Combining with face detection lead us to a clothes\nsegmentation approach close to real time. The study uses the challenging Pascal\nVOC dataset for segmentation evaluation experiments. A final experiment\nanalyzes the fusion of clothes features with state-of-the-art gender\nclassifiers in ClothesDB, revealing a significant performance improvement in\ngender classification.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 13:39:55 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Freire-Obreg\u00f3n", "D.", ""], ["Castrill\u00f3n-Santana", "M.", ""], ["Lorenzo-Navarro", "J.", ""]]}, {"id": "1611.04021", "submitter": "Tseng-Hung Chen", "authors": "Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan\n  Carlos Niebles, Min Sun", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "comments": "7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable approach to learn video-based question answering (QA):\nanswer a \"free-form natural language question\" about a video content. Our\napproach automatically harvests a large number of videos and descriptions\nfreely available online. Then, a large number of candidate QA pairs are\nautomatically generated from descriptions rather than manually annotated. Next,\nwe use these candidate QA pairs to train a number of video-based QA methods\nextended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et\nal. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect\ncandidate QA pairs, we propose a self-paced learning procedure to iteratively\nidentify them and mitigate their effects in training. Finally, we evaluate\nperformance on manually generated video-based QA pairs. The results show that\nour self-paced learning procedure is effective, and the extended SS model\noutperforms various baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:15:57 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 16:07:33 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Chuang", "Ching-Yao", ""], ["Liao", "Yuan-Hong", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1611.04023", "submitter": "Gerard Rinkus", "authors": "Gerard J. Rinkus", "title": "Sparsey: Event Recognition via Deep Hierarchical Spare Distributed Codes", "comments": "This is a manuscript form of a paper published in Frontiers in\n  Computational Neuroscience in 2014\n  (http://dx.doi.org/10.3389/fncom.2014.00160). 65 pages, 28 figures, 8 tables", "journal-ref": "Frontiers in Computational Neuroscience, Vol. 8, Article 160\n  (2014)", "doi": "10.3389/fncom.2014.00160", "report-no": null, "categories": "q-bio.NC cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual cortex's hierarchical, multi-level organization is captured in many\nbiologically inspired computational vision models, the general idea being that\nprogressively larger scale, more complex spatiotemporal features are\nrepresented in progressively higher areas. However, most earlier models use\nlocalist representations (codes) in each representational field, which we\nequate with the cortical macrocolumn (mac), at each level. In localism, each\nrepresented feature/event (item) is coded by a single unit. Our model, Sparsey,\nis also hierarchical but crucially, uses sparse distributed coding (SDC) in\nevery mac in all levels. In SDC, each represented item is coded by a small\nsubset of the mac's units. SDCs of different items can overlap and the size of\noverlap between items can represent their similarity. The difference between\nlocalism and SDC is crucial because SDC allows the two essential operations of\nassociative memory, storing a new item and retrieving the best-matching stored\nitem, to be done in fixed time for the life of the model. Since the model's\ncore algorithm, which does both storage and retrieval (inference), makes a\nsingle pass over all macs on each time step, the overall model's\nstorage/retrieval operation is also fixed-time, a criterion we consider\nessential for scalability to huge datasets. A 2010 paper described a\nnonhierarchical version of this model in the context of purely spatial pattern\nprocessing. Here, we elaborate a fully hierarchical model (arbitrary numbers of\nlevels and macs per level), describing novel model principles like progressive\ncritical periods, dynamic modulation of principal cells' activation functions\nbased on a mac-level familiarity measure, representation of multiple\nsimultaneously active hypotheses, a novel method of time warp invariant\nrecognition, and we report results showing learning/recognition of\nspatiotemporal patterns.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:35:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Rinkus", "Gerard J.", ""]]}, {"id": "1611.04076", "submitter": "Xudong Mao", "authors": "Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang and\n  Stephen Paul Smolley", "title": "Least Squares Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning with generative adversarial networks (GANs) has proven\nhugely successful. Regular GANs hypothesize the discriminator as a classifier\nwith the sigmoid cross entropy loss function. However, we found that this loss\nfunction may lead to the vanishing gradients problem during the learning\nprocess. To overcome such a problem, we propose in this paper the Least Squares\nGenerative Adversarial Networks (LSGANs) which adopt the least squares loss\nfunction for the discriminator. We show that minimizing the objective function\nof LSGAN yields minimizing the Pearson $\\chi^2$ divergence. There are two\nbenefits of LSGANs over regular GANs. First, LSGANs are able to generate higher\nquality images than regular GANs. Second, LSGANs perform more stable during the\nlearning process. We evaluate LSGANs on five scene datasets and the\nexperimental results show that the images generated by LSGANs are of better\nquality than the ones generated by regular GANs. We also conduct two comparison\nexperiments between LSGANs and regular GANs to illustrate the stability of\nLSGANs.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 03:38:28 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 07:50:53 GMT"}, {"version": "v3", "created": "Wed, 5 Apr 2017 05:44:47 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Mao", "Xudong", ""], ["Li", "Qing", ""], ["Xie", "Haoran", ""], ["Lau", "Raymond Y. K.", ""], ["Wang", "Zhen", ""], ["Smolley", "Stephen Paul", ""]]}, {"id": "1611.04135", "submitter": "Xi Zhang", "authors": "Xiaolin Wu, Xi Zhang", "title": "Responses to Critiques on Machine Learning of Criminality Perceptions\n  (Addendum of arXiv:1611.04135)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In November 2016 we submitted to arXiv our paper \"Automated Inference on\nCriminality Using Face Images\". It generated a great deal of discussions in the\nInternet and some media outlets. Our work is only intended for pure academic\ndiscussions; how it has become a media consumption is a total surprise to us.\nAlthough in agreement with our critics on the need and importance of policing\nAI research for the general good of the society, we are deeply baffled by the\nways some of them mispresented our work, in particular the motive and objective\nof our research.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 13:32:11 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 03:57:48 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 07:48:10 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Wu", "Xiaolin", ""], ["Zhang", "Xi", ""]]}, {"id": "1611.04138", "submitter": "Mohammad Hossein Jafari", "authors": "Ebrahim Nasr-Esfahani, Nader Karimi, S.M. Reza Soroushmehr, M. Hossein\n  Jafari, M. Amin Khorsandi, Shadrokh Samavi, Kayvan Najarian", "title": "Hand Gesture Recognition for Contactless Device Control in Operating\n  Rooms", "comments": null, "journal-ref": null, "doi": "10.1007/s11548-017-1588-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gesture is one of the most important means of touchless communication\nbetween human and machines. There is a great interest for commanding electronic\nequipment in surgery rooms by hand gesture for reducing the time of surgery and\nthe potential for infection. There are challenges in implementation of a hand\ngesture recognition system. It has to fulfill requirements such as high\naccuracy and fast response. In this paper we introduce a system of hand gesture\nrecognition based on a deep learning approach. Deep learning is known as an\naccurate detection model, but its high complexity prevents it from being\nfabricated as an embedded system. To cope with this problem, we applied some\nchanges in the structure of our work to achieve low complexity. As a result,\nthe proposed method could be implemented on a naive embedded system. Our\nexperiments show that the proposed system results in higher accuracy while\nhaving less complexity in comparison with the existing comparable methods.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 14:02:54 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Nasr-Esfahani", "Ebrahim", ""], ["Karimi", "Nader", ""], ["Soroushmehr", "S. M. Reza", ""], ["Jafari", "M. Hossein", ""], ["Khorsandi", "M. Amin", ""], ["Samavi", "Shadrokh", ""], ["Najarian", "Kayvan", ""]]}, {"id": "1611.04144", "submitter": "Xuanpeng Li", "authors": "Xuanpeng Li and Rachid Belaroussi", "title": "Semi-Dense 3D Semantic Mapping from Monocular SLAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bundle of geometry and appearance in computer vision has proven to be a\npromising solution for robots across a wide variety of applications. Stereo\ncameras and RGB-D sensors are widely used to realise fast 3D reconstruction and\ntrajectory tracking in a dense way. However, they lack flexibility of seamless\nswitch between different scaled environments, i.e., indoor and outdoor scenes.\nIn addition, semantic information are still hard to acquire in a 3D mapping. We\naddress this challenge by combining the state-of-art deep learning method and\nsemi-dense Simultaneous Localisation and Mapping (SLAM) based on video stream\nfrom a monocular camera. In our approach, 2D semantic information are\ntransferred to 3D mapping via correspondence between connective Keyframes with\nspatial consistency. There is no need to obtain a semantic segmentation for\neach frame in a sequence, so that it could achieve a reasonable computation\ntime. We evaluate our method on indoor/outdoor datasets and lead to an\nimprovement in the 2D semantic labelling over baseline single frame\npredictions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 15:31:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Li", "Xuanpeng", ""], ["Belaroussi", "Rachid", ""]]}, {"id": "1611.04201", "submitter": "Fereshteh Sadeghi", "authors": "Fereshteh Sadeghi and Sergey Levine", "title": "CAD2RL: Real Single-Image Flight without a Single Real Image", "comments": "To appear at Robotics: Science and Systems Conference (R:SS), 2017.\n  Supplementary video: https://www.youtube.com/watch?v=nXBWmzFrj5s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has emerged as a promising and powerful technique\nfor automatically acquiring control policies that can process raw sensory\ninputs, such as images, and perform complex behaviors. However, extending deep\nRL to real-world robotic tasks has proven challenging, particularly in\nsafety-critical domains such as autonomous flight, where a trial-and-error\nlearning process is often impractical. In this paper, we explore the following\nquestion: can we train vision-based navigation policies entirely in simulation,\nand then transfer them into the real world to achieve real-world flight without\na single real training image? We propose a learning method that we call\nCAD$^2$RL, which can be used to perform collision-free indoor flight in the\nreal world while being trained entirely on 3D CAD models. Our method uses\nsingle RGB images from a monocular camera, without needing to explicitly\nreconstruct the 3D geometry of the environment or perform explicit motion\nplanning. Our learned collision avoidance policy is represented by a deep\nconvolutional neural network that directly processes raw monocular images and\noutputs velocity commands. This policy is trained entirely on simulated images,\nwith a Monte Carlo policy evaluation algorithm that directly optimizes the\nnetwork's ability to produce collision-free flight. By highly randomizing the\nrendering settings for our simulated training set, we show that we can train a\npolicy that generalizes to the real world, without requiring the simulator to\nbe particularly realistic or high-fidelity. We evaluate our method by flying a\nreal quadrotor through indoor environments, and further evaluate the design\nchoices in our simulator through a series of ablation studies on depth\nprediction. For supplementary video see: https://youtu.be/nXBWmzFrj5s\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 23:08:42 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 20:48:48 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 11:47:41 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 07:21:39 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Sadeghi", "Fereshteh", ""], ["Levine", "Sergey", ""]]}, {"id": "1611.04215", "submitter": "Kai Chen", "authors": "Kai Chen and Wenbing Tao", "title": "Convolutional Regression for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, discriminatively learned correlation filters (DCF) has drawn much\nattention in visual object tracking community. The success of DCF is\npotentially attributed to the fact that a large amount of samples are utilized\nto train the ridge regression model and predict the location of object. To\nsolve the regression problem in an efficient way, these samples are all\ngenerated by circularly shifting from a search patch. However, these synthetic\nsamples also induce some negative effects which weaken the robustness of DCF\nbased trackers.\n  In this paper, we propose a Convolutional Regression framework for visual\ntracking (CRT). Instead of learning the linear regression model in a closed\nform, we try to solve the regression problem by optimizing a one-channel-output\nconvolution layer with Gradient Descent (GD). In particular, the receptive\nfield size of the convolution layer is set to the size of object. Contrary to\nDCF, it is possible to incorporate all \"real\" samples clipped from the whole\nimage. A critical issue of the GD approach is that most of the convolutional\nsamples are negative and the contribution of positive samples will be\nsuppressed. To address this problem, we propose a novel Automatic Hard Negative\nMining method to eliminate easy negatives and enhance positives. Extensive\nexperiments are conducted on a widely-used benchmark with 100 sequences. The\nresults show that the proposed algorithm achieves outstanding performance and\noutperforms almost all the existing DCF based algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 01:10:21 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 07:53:11 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Chen", "Kai", ""], ["Tao", "Wenbing", ""]]}, {"id": "1611.04246", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun Zhu", "title": "Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning", "comments": "in the Thirty-First AAAI Conference on Artificial Intelligence\n  (AAAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a learning strategy that extracts object-part concepts\nfrom a pre-trained convolutional neural network (CNN), in an attempt to 1)\nexplore explicit semantics hidden in CNN units and 2) gradually grow a\nsemantically interpretable graphical model on the pre-trained CNN for\nhierarchical object understanding. Given part annotations on very few (e.g.,\n3-12) objects, our method mines certain latent patterns from the pre-trained\nCNN and associates them with different semantic parts. We use a four-layer\nAnd-Or graph to organize the mined latent patterns, so as to clarify their\ninternal semantic hierarchy. Our method is guided by a small number of part\nannotations, and it achieves superior performance (about 13%-107% improvement)\nin part center prediction on the PASCAL VOC and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 04:13:37 GMT"}, {"version": "v2", "created": "Mon, 13 Mar 2017 07:23:20 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Zhang", "Quanshi", ""], ["Cao", "Ruiming", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1611.04251", "submitter": "Minchul Shin", "authors": "Minchul Shin, Munsang Kim and Dong-Soo Kwon", "title": "Baseline CNN structure analysis for facial expression recognition", "comments": "6 pages, RO-MAN2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a baseline convolutional neural network (CNN) structure and image\npreprocessing methodology to improve facial expression recognition algorithm\nusing CNN. To analyze the most efficient network structure, we investigated\nfour network structures that are known to show good performance in facial\nexpression recognition. Moreover, we also investigated the effect of input\nimage preprocessing methods. Five types of data input (raw, histogram\nequalization, isotropic smoothing, diffusion-based normalization, difference of\nGaussian) were tested, and the accuracy was compared. We trained 20 different\nCNN models (4 networks x 5 data input types) and verified the performance of\neach network with test images from five different databases. The experiment\nresult showed that a three-layer structure consisting of a simple convolutional\nand a max pooling layer with histogram equalization image input was the most\nefficient. We describe the detailed training procedure and analyze the result\nof the test accuracy based on considerable observation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 04:57:18 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Shin", "Minchul", ""], ["Kim", "Munsang", ""], ["Kwon", "Dong-Soo", ""]]}, {"id": "1611.04298", "submitter": "Chengzhe Yan Mr", "authors": "Chengzhe Yan, Jie Hu and Changshui Zhang", "title": "A DNN Framework For Text Image Rectification From Planar Transformations", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel neural network architecture is proposed attempting to\nrectify text images with mild assumptions. A new dataset of text images is\ncollected to verify our model and open to public. We explored the capability of\ndeep neural network in learning geometric transformation and found the model\ncould segment the text image without explicit supervised segmentation\ninformation. Experiments show the architecture proposed can restore planar\ntransformations with wonderful robustness and effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 09:40:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Yan", "Chengzhe", ""], ["Hu", "Jie", ""], ["Zhang", "Changshui", ""]]}, {"id": "1611.04353", "submitter": "Ece Ozkan", "authors": "Ece Ozkan, Gemma Roig, Orcun Goksel and Xavier Boix", "title": "Herding Generalizes Diverse M -Best Solutions", "comments": "8 pages, 2 algorithms, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the algorithm to extract diverse M -solutions from a Conditional\nRandom Field (called divMbest [1]) takes exactly the form of a Herding\nprocedure [2], i.e. a deterministic dynamical system that produces a sequence\nof hypotheses that respect a set of observed moment constraints. This\ngeneralization enables us to invoke properties of Herding that show that\ndivMbest enforces implausible constraints which may yield wrong assumptions for\nsome problem settings. Our experiments in semantic segmentation demonstrate\nthat seeing divMbest as an instance of Herding leads to better alternatives for\nthe implausible constraints of divMbest.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:13:58 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 13:40:12 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ozkan", "Ece", ""], ["Roig", "Gemma", ""], ["Goksel", "Orcun", ""], ["Boix", "Xavier", ""]]}, {"id": "1611.04357", "submitter": "Yashas Annadani", "authors": "Yashas Annadani, Vijayakrishna Naganoor, Akshay Kumar Jagadish,\n  Krishnan Chemmangat", "title": "Selfie Detection by Synergy-Constraint Based Convolutional Neural\n  Network", "comments": "8 Pages, Accepted for Publication at IEEE SITIS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Categorisation of huge amount of data on the multimedia platform is a crucial\ntask. In this work, we propose a novel approach to address the subtle problem\nof selfie detection for image database segregation on the web, given rapid rise\nin number of selfies clicked. A Convolutional Neural Network (CNN) is modeled\nto learn a synergy feature in the common subspace of head and shoulder\norientation, derived from Local Binary Pattern (LBP) and Histogram of Oriented\nGradients (HOG) features respectively. This synergy was captured by projecting\nthe aforementioned features using Canonical Correlation Analysis (CCA). We show\nthat the resulting network's convolutional activations in the neighbourhood of\nspatial keypoints captured by SIFT are discriminative for selfie-detection. In\ngeneral, proposed approach aids in capturing intricacies present in the image\ndata and has the potential for usage in other subtle image analysis scenarios\napart from just selfie detection. We investigate and analyse the performance of\npopular CNN architectures (GoogleNet, AlexNet), used for other image\nclassification tasks, when subjected to the task of detecting the selfies on\nthe multimedia platform. The results of the proposed approach are compared with\nthese popular architectures on a dataset of ninety thousand images comprising\nof roughly equal number of selfies and non-selfies. Experimental results on\nthis dataset shows the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:22:34 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Annadani", "Yashas", ""], ["Naganoor", "Vijayakrishna", ""], ["Jagadish", "Akshay Kumar", ""], ["Chemmangat", "Krishnan", ""]]}, {"id": "1611.04399", "submitter": "Bjoern Andres", "authors": "Evgeny Levinkov, Jonas Uhrig, Siyu Tang, Mohamed Omran, Eldar\n  Insafutdinov, Alexander Kirillov, Carsten Rother, Thomas Brox, Bernt Schiele,\n  Bjoern Andres", "title": "Joint Graph Decomposition and Node Labeling: Problem, Algorithms,\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We state a combinatorial optimization problem whose feasible solutions define\nboth a decomposition and a node labeling of a given graph. This problem offers\na common mathematical abstraction of seemingly unrelated computer vision tasks,\nincluding instance-separating semantic segmentation, articulated human body\npose estimation and multiple object tracking. Conceptually, the problem we\nstate generalizes the unconstrained integer quadratic program and the minimum\ncost lifted multicut problem, both of which are NP-hard. In order to find\nfeasible solutions efficiently, we define two local search algorithms that\nconverge monotonously to a local optimum, offering a feasible solution at any\ntime. To demonstrate their effectiveness in tackling computer vision tasks, we\napply these algorithms to instances of the problem that we construct from\npublished data, using published algorithms. We report state-of-the-art\napplication-specific accuracy for the three above-mentioned applications.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 14:48:38 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 09:50:46 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Levinkov", "Evgeny", ""], ["Uhrig", "Jonas", ""], ["Tang", "Siyu", ""], ["Omran", "Mohamed", ""], ["Insafutdinov", "Eldar", ""], ["Kirillov", "Alexander", ""], ["Rother", "Carsten", ""], ["Brox", "Thomas", ""], ["Schiele", "Bernt", ""], ["Andres", "Bjoern", ""]]}, {"id": "1611.04413", "submitter": "Ronan Sicre", "authors": "Ronan Sicre, Julien Rabin, Yannis Avrithis, Teddy Furon, Frederic\n  Jurie", "title": "Automatic discovery of discriminative parts as a quadratic assignment\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part-based image classification consists in representing categories by small\nsets of discriminative parts upon which a representation of the images is\nbuilt. This paper addresses the question of how to automatically learn such\nparts from a set of labeled training images. The training of parts is cast as a\nquadratic assignment problem in which optimal correspondences between image\nregions and parts are automatically learned. The paper analyses different\nassignment strategies and thoroughly evaluates them on two public datasets:\nWillow actions and MIT 67 scenes. State-of-the art results are obtained on\nthese datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 15:17:48 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Sicre", "Ronan", ""], ["Rabin", "Julien", ""], ["Avrithis", "Yannis", ""], ["Furon", "Teddy", ""], ["Jurie", "Frederic", ""]]}, {"id": "1611.04481", "submitter": "Hiya Roy", "authors": "Subhajit Chaudhury, Hiya Roy", "title": "Can fully convolutional networks perform well for general image\n  restoration problems?", "comments": "Accepted at IAPR MVA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully convolutional network(FCN) based approach for color image\nrestoration. FCNs have recently shown remarkable performance for high-level\nvision problem like semantic segmentation. In this paper, we investigate if FCN\nmodels can show promising performance for low-level problems like image\nrestoration as well. We propose a fully convolutional model, that learns a\ndirect end-to-end mapping between the corrupted images as input and the desired\nclean images as output. Our proposed method takes inspiration from domain\ntransformation techniques but presents a data-driven task specific approach\nwhere filters for novel basis projection, task dependent coefficient\nalterations, and image reconstruction are represented as convolutional\nnetworks. Experimental results show that our FCN model outperforms traditional\nsparse coding based methods and demonstrates competitive performance compared\nto the state-of-the-art methods for image denoising. We further show that our\nproposed model can solve the difficult problem of blind image inpainting and\ncan produce reconstructed images of impressive visual quality.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:13:29 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 15:04:50 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Chaudhury", "Subhajit", ""], ["Roy", "Hiya", ""]]}, {"id": "1611.04503", "submitter": "Hideki Nakayama", "authors": "Hideki Nakayama and Noriki Nishida", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network\n  with Multimedia Pivot", "comments": "Some error corrections in Sect.2.2 and Table 5, Machine Translation,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to build a neural machine translation system with no\nsupervised resources (i.e., no parallel corpora) using multimodal embedded\nrepresentation over texts and images. Based on the assumption that text\ndocuments are often likely to be described with other multimedia information\n(e.g., images) somewhat related to the content, we try to indirectly estimate\nthe relevance between two languages. Using multimedia as the \"pivot\", we\nproject all modalities into one common hidden space where samples belonging to\nsimilar semantic concepts should come close to each other, whatever the\nobserved space of each sample is. This modality-agnostic representation is the\nkey to bridging the gap between different modalities. Putting a decoder on top\nof it, our network can flexibly draw the outputs from any input modality.\nNotably, in the testing phase, we need only source language texts as the input\nfor translation. In experiments, we tested our method on two benchmarks to show\nthat it can achieve reasonable translation performance. We compared and\ninvestigated several possible implementations and found that an end-to-end\nmodel that simultaneously optimized both rank loss in multimodal encoders and\ncross-entropy loss in decoders performed the best.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 18:07:54 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 17:36:30 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 15:52:08 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Nakayama", "Hideki", ""], ["Nishida", "Noriki", ""]]}, {"id": "1611.04519", "submitter": "Wentao Luan", "authors": "Went Luan, Yezhou Yang, Cornelia Fermuller, John S. Baras", "title": "Fast Task-Specific Target Detection via Graph Based Constraints\n  Representation and Checking", "comments": "The paper is withdrawn for another work's convenience. We will upload\n  it later", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a fast target detection framework for real-world\nrobotics applications. Considering that an intelligent agent attends to a\ntask-specific object target during execution, our goal is to detect the object\nefficiently. We propose the concept of early recognition, which influences the\ncandidate proposal process to achieve fast and reliable detection performance.\nTo check the target constraints efficiently, we put forward a novel policy to\ngenerate a sub-optimal checking order, and prove that it has bounded time cost\ncompared to the optimal checking sequence, which is not achievable in\npolynomial time. Experiments on two different scenarios: 1) rigid object and 2)\nnon-rigid body part detection validate our pipeline. To show that our method is\nwidely applicable, we further present a human-robot interaction system based on\nour non-rigid body part detection.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:01:06 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 04:57:36 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Luan", "Went", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Baras", "John S.", ""]]}, {"id": "1611.04534", "submitter": "Mu Zhou", "authors": "Darvin Yi and Mu Zhou and Zhao Chen and Olivier Gevaert", "title": "3-D Convolutional Neural Networks for Glioblastoma Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have emerged as powerful tools for\nlearning discriminative image features. In this paper, we propose a framework\nof 3-D fully CNN models for Glioblastoma segmentation from multi-modality MRI\ndata. By generalizing CNN models to true 3-D convolutions in learning 3-D tumor\nMRI data, the proposed approach utilizes a unique network architecture to\ndecouple image pixels. Specifically, we design a convolutional layer with\npre-defined Difference- of-Gaussian (DoG) filters to perform true 3-D\nconvolution incorporating local neighborhood information at each pixel. We then\nuse three trained convolutional layers that act to decouple voxels from the\ninitial 3-D convolution. The proposed framework allows identification of\nhigh-level tumor structures on MRI. We evaluate segmentation performance on the\nBRATS segmentation dataset with 274 tumor samples. Extensive experimental\nresults demonstrate encouraging performance of the proposed approach comparing\nto the state-of-the-art methods. Our data-driven approach achieves a median\nDice score accuracy of 89% in whole tumor glioblastoma segmentation, revealing\na generalized low-bias possibility to learn from medium-size MRI datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:21:33 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Yi", "Darvin", ""], ["Zhou", "Mu", ""], ["Chen", "Zhao", ""], ["Gevaert", "Olivier", ""]]}, {"id": "1611.04636", "submitter": "Honglin Zheng", "authors": "Honglin Zheng, Tianlang Chen, Jiebo Luo", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes\n  Emotion and Sentiment", "comments": "7 pages, 5 figures, submitted to AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is crucial for extracting social signals from social media\ncontent. Due to the prevalence of images in social media, image sentiment\nanalysis is receiving increasing attention in recent years. However, most\nexisting systems are black-boxes that do not provide insight on how image\ncontent invokes sentiment and emotion in the viewers. Psychological studies\nhave confirmed that salient objects in an image often invoke emotions. In this\nwork, we investigate more fine-grained and more comprehensive interaction\nbetween visual saliency and visual sentiment. In particular, we partition\nimages in several primary scene-type dimensions, including: open-closed,\nnatural-manmade, indoor-outdoor, and face-noface. Using state of the art\nsaliency detection algorithm and sentiment classification algorithm, we examine\nhow the sentiment of the salient region(s) in an image relates to the overall\nsentiment of the image. The experiments on a representative image emotion\ndataset have shown interesting correlation between saliency and sentiment in\ndifferent scene types and in turn shed light on the mechanism of visual\nsentiment evocation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 22:02:09 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Zheng", "Honglin", ""], ["Chen", "Tianlang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.04655", "submitter": "Aurelien Bustin", "authors": "Aurelien Bustin, Anne Menini, Martin A. Janich, Darius Burschka,\n  Jacques Felblinger, Anja C.S. Brau, and Freddy Odille", "title": "Motion Estimated-Compensated Reconstruction with Preserved-Features in\n  Free-Breathing Cardiac MRI", "comments": "12 pages, 6 figures, accepted at MICCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To develop an efficient motion-compensated reconstruction technique for\nfree-breathing cardiac magnetic resonance imaging (MRI) that allows\nhigh-quality images to be reconstructed from multiple undersampled single-shot\nacquisitions. The proposed method is a joint image reconstruction and motion\ncorrection method consisting of several steps, including a non-rigid motion\nextraction and a motion-compensated reconstruction. The reconstruction includes\na denoising with the Beltrami regularization, which offers an ideal compromise\nbetween feature preservation and staircasing reduction. Results were assessed\nin simulation, phantom and volunteer experiments. The proposed joint image\nreconstruction and motion correction method exhibits visible quality\nimprovement over previous methods while reconstructing sharper edges. Moreover,\nwhen the acceleration factor increases, standard methods show blurry results\nwhile the proposed method preserves image quality. The method was applied to\nfree-breathing single-shot cardiac MRI, successfully achieving high image\nquality and higher spatial resolution than conventional segmented methods, with\nthe potential to offer high-quality delayed enhancement scans in challenging\npatients.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 00:33:47 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Bustin", "Aurelien", ""], ["Menini", "Anne", ""], ["Janich", "Martin A.", ""], ["Burschka", "Darius", ""], ["Felblinger", "Jacques", ""], ["Brau", "Anja C. S.", ""], ["Odille", "Freddy", ""]]}, {"id": "1611.04782", "submitter": "Salvatore Rampone", "authors": "Gianni D'Angelo, Salvatore Rampone", "title": "Feature Extraction and Soft Computing Methods for Aerospace Structure\n  Defect Classification", "comments": null, "journal-ref": "Measurement Volume 85, May 2016, Pages 192-209", "doi": "10.1016/j.measurement.2016.02.027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study concerns the effectiveness of several techniques and methods of\nsignals processing and data interpretation for the diagnosis of aerospace\nstructure defects. This is done by applying different known feature extraction\nmethods, in addition to a new CBIR-based one; and some soft computing\ntechniques including a recent HPC parallel implementation of the U-BRAIN\nlearning algorithm on Non Destructive Testing data. The performance of the\nresulting detection systems are measured in terms of Accuracy, Sensitivity,\nSpecificity, and Precision. Their effectiveness is evaluated by the Matthews\ncorrelation, the Area Under Curve (AUC), and the F-Measure. Several experiments\nare performed on a standard dataset of eddy current signal samples for aircraft\nstructures. Our experimental results evidence that the key to a successful\ndefect classifier is the feature extraction method - namely the novel\nCBIR-based one outperforms all the competitors - and they illustrate the\ngreater effectiveness of the U-BRAIN algorithm and the MLP neural network among\nthe soft computing methods in this kind of application.\n  Keywords- Non-destructive testing (NDT); Soft Computing; Feature Extraction;\nClassification Algorithms; Content-Based Image Retrieval (CBIR); Eddy Currents\n(EC).\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 10:47:12 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["D'Angelo", "Gianni", ""], ["Rampone", "Salvatore", ""]]}, {"id": "1611.04835", "submitter": "Nauman Shahid", "authors": "Nauman Shahid, Francesco Grassi, Pierre Vandergheynst", "title": "Multilinear Low-Rank Tensors on Graphs & Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for the analysis of low-rank tensors which lies at\nthe intersection of spectral graph theory and signal processing. As a first\nstep, we present a new graph based low-rank decomposition which approximates\nthe classical low-rank SVD for matrices and multi-linear SVD for tensors. Then,\nbuilding on this novel decomposition we construct a general class of convex\noptimization problems for approximately solving low-rank tensor inverse\nproblems, such as tensor Robust PCA. The whole framework is named as\n'Multilinear Low-rank tensors on Graphs (MLRTG)'. Our theoretical analysis\nshows: 1) MLRTG stands on the notion of approximate stationarity of\nmulti-dimensional signals on graphs and 2) the approximation error depends on\nthe eigen gaps of the graphs. We demonstrate applications for a wide variety of\n4 artificial and 12 real tensor datasets, such as EEG, FMRI, BCI, surveillance\nvideos and hyperspectral images. Generalization of the tensor concepts to\nnon-euclidean domain, orders of magnitude speed-up, low-memory requirement and\nsignificantly enhanced performance at low SNR are the key aspects of our\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:05:43 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Shahid", "Nauman", ""], ["Grassi", "Francesco", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1611.04849", "submitter": "Ming-Ming Cheng Prof.", "authors": "Qibin Hou and Ming-Ming Cheng and Xiao-Wei Hu and Ali Borji and\n  Zhuowen Tu and Philip Torr", "title": "Deeply supervised salient object detection with short connections", "comments": "IEEE TPAMI 2018 (IEEE CVPR 2017)", "journal-ref": "Q. Hou, M. M. Cheng, X. Hu, A. Borji, Z. Tu and P. H. S. Torr,\n  \"Deeply Supervised Salient Object Detection with Short Connections,\" IEEE\n  Transactions on Pattern Analysis and Machine Intelligence. 2018", "doi": "10.1109/TPAMI.2018.2815688", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on saliency detection is substantial, benefiting mostly from\nthe explosive development of Convolutional Neural Networks (CNNs). Semantic\nsegmentation and saliency detection algorithms developed lately have been\nmostly based on Fully Convolutional Neural Networks (FCNs). There is still a\nlarge room for improvement over the generic FCN models that do not explicitly\ndeal with the scale-space problem. Holistically-Nested Edge Detector (HED)\nprovides a skip-layer structure with deep supervision for edge and boundary\ndetection, but the performance gain of HED on salience detection is not\nobvious. In this paper, we propose a new method for saliency detection by\nintroducing short connections to the skip-layer structures within the HED\narchitecture. Our framework provides rich multi-scale feature maps at each\nlayer, a property that is critically needed to perform segment detection. Our\nmethod produces state-of-the-art results on 5 widely tested salient object\ndetection benchmarks, with advantages in terms of efficiency (0.15 seconds per\nimage), effectiveness, and simplicity over the existing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:19:06 GMT"}, {"version": "v2", "created": "Sun, 9 Apr 2017 08:47:13 GMT"}, {"version": "v3", "created": "Wed, 24 Jan 2018 01:58:48 GMT"}, {"version": "v4", "created": "Fri, 16 Mar 2018 01:46:40 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Hou", "Qibin", ""], ["Cheng", "Ming-Ming", ""], ["Hu", "Xiao-Wei", ""], ["Borji", "Ali", ""], ["Tu", "Zhuowen", ""], ["Torr", "Philip", ""]]}, {"id": "1611.04850", "submitter": "Yuhang Lu", "authors": "Yuhang Lu, Youchuan Wan, Gang Li", "title": "Scale-constrained Unsupervised Evaluation Method for Multi-scale Image\n  Segmentation", "comments": "5 pages, 2016 IEEE International Conference on Image Processing", "journal-ref": null, "doi": "10.1109/ICIP.2016.7532821", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised evaluation of segmentation quality is a crucial step in image\nsegmentation applications. Previous unsupervised evaluation methods usually\nlacked the adaptability to multi-scale segmentation. A scale-constrained\nevaluation method that evaluates segmentation quality according to the\nspecified target scale is proposed in this paper. First, regional saliency and\nmerging cost are employed to describe intra-region homogeneity and inter-region\nheterogeneity, respectively. Subsequently, both of them are standardized into\nequivalent spectral distances of a predefined region. Finally, by analyzing the\nrelationship between image characteristics and segmentation quality, we\nestablish the evaluation model. Experimental results show that the proposed\nmethod outperforms four commonly used unsupervised methods in multi-scale\nevaluation tasks.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:19:42 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Lu", "Yuhang", ""], ["Wan", "Youchuan", ""], ["Li", "Gang", ""]]}, {"id": "1611.04870", "submitter": "Ping Li PhD", "authors": "Ping Li and Jun Yu and Meng Wang and Luming Zhang and Deng Cai and\n  Xuelong Li", "title": "Constrained Low-Rank Learning Using Least Squares-Based Regularization", "comments": "14 pages, 7 figures, accepted to appear in IEEE Transactions on\n  Cybernetics", "journal-ref": "IEEE Transactions on Cybernetics, 2016", "doi": "10.1109/TCYB.2016.2623638", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank learning has attracted much attention recently due to its efficacy\nin a rich variety of real-world tasks, e.g., subspace segmentation and image\ncategorization. Most low-rank methods are incapable of capturing\nlow-dimensional subspace for supervised learning tasks, e.g., classification\nand regression. This paper aims to learn both the discriminant low-rank\nrepresentation (LRR) and the robust projecting subspace in a supervised manner.\nTo achieve this goal, we cast the problem into a constrained rank minimization\nframework by adopting the least squares regularization. Naturally, the data\nlabel structure tends to resemble that of the corresponding low-dimensional\nrepresentation, which is derived from the robust subspace projection of clean\ndata by low-rank learning. Moreover, the low-dimensional representation of\noriginal data can be paired with some informative structure by imposing an\nappropriate constraint, e.g., Laplacian regularizer. Therefore, we propose a\nnovel constrained LRR method. The objective function is formulated as a\nconstrained nuclear norm minimization problem, which can be solved by the\ninexact augmented Lagrange multiplier algorithm. Extensive experiments on image\nclassification, human pose estimation, and robust face recovery have confirmed\nthe superiority of our method.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 14:50:31 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Li", "Ping", ""], ["Yu", "Jun", ""], ["Wang", "Meng", ""], ["Zhang", "Luming", ""], ["Cai", "Deng", ""], ["Li", "Xuelong", ""]]}, {"id": "1611.04871", "submitter": "Anurag Kumar", "authors": "Anurag Kumar, Bhiksha Raj", "title": "Audio Event and Scene Recognition: A Unified Approach using Strongly and\n  Weakly Labeled Data", "comments": "IJCNN 2017, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel learning framework called Supervised and\nWeakly Supervised Learning where the goal is to learn simultaneously from\nweakly and strongly labeled data. Strongly labeled data can be simply\nunderstood as fully supervised data where all labeled instances are available.\nIn weakly supervised learning only data is weakly labeled which prevents one\nfrom directly applying supervised learning methods. Our proposed framework is\nmotivated by the fact that a small amount of strongly labeled data can give\nconsiderable improvement over only weakly supervised learning. The primary\nproblem domain focus of this paper is acoustic event and scene detection in\naudio recordings. We first propose a naive formulation for leveraging labeled\ndata in both forms. We then propose a more general framework for Supervised and\nWeakly Supervised Learning (SWSL). Based on this general framework, we propose\na graph based approach for SWSL. Our main method is based on manifold\nregularization on graphs in which we show that the unified learning can be\nformulated as a constraint optimization problem which can be solved by\niterative concave-convex procedure (CCCP). Our experiments show that our\nproposed framework can address several concerns of audio content analysis using\nweakly labeled data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 07:39:50 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 23:17:46 GMT"}, {"version": "v3", "created": "Sat, 18 Feb 2017 07:18:32 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Kumar", "Anurag", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1611.04899", "submitter": "Yilin Song", "authors": "Yilin Song, Jonathan Viventi, Yao Wang", "title": "Diversity encouraged learning of unsupervised LSTM ensemble for neural\n  activity video prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict the neural signal in the near future from the current\nand previous observations has the potential to enable real-time responsive\nbrain stimulation to suppress seizures. We have investigated how to use an\nauto-encoder model consisting of LSTM cells for such prediction. Recog- nizing\nthat there exist multiple activity pattern clusters, we have further explored\nto train an ensemble of LSTM mod- els so that each model can specialize in\nmodeling certain neural activities, without explicitly clustering the training\ndata. We train the ensemble using an ensemble-awareness loss, which jointly\nsolves the model assignment problem and the error minimization problem. During\ntraining, for each training sequence, only the model that has the lowest recon-\nstruction and prediction error is updated. Intrinsically such a loss function\nenables each LTSM model to be adapted to a subset of the training sequences\nthat share similar dynamic behavior. We demonstrate this can be trained in an\nend- to-end manner and achieve significant accuracy in neural activity\nprediction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 15:56:08 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 02:39:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Song", "Yilin", ""], ["Viventi", "Jonathan", ""], ["Wang", "Yao", ""]]}, {"id": "1611.04905", "submitter": "Yehya Abouelnaga", "authors": "Yehya Abouelnaga, Ola S. Ali, Hager Rady, and Mohamed Moustafa", "title": "CIFAR-10: KNN-based Ensemble of Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of different classifiers on the\nCIFAR-10 dataset, and build an ensemble of classifiers to reach a better\nperformance. We show that, on CIFAR-10, K-Nearest Neighbors (KNN) and\nConvolutional Neural Network (CNN), on some classes, are mutually exclusive,\nthus yield in higher accuracy when combined. We reduce KNN overfitting using\nPrincipal Component Analysis (PCA), and ensemble it with a CNN to increase its\naccuracy. Our approach improves our best CNN model from 93.33% to 94.03%.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:02:58 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Abouelnaga", "Yehya", ""], ["Ali", "Ola S.", ""], ["Rady", "Hager", ""], ["Moustafa", "Mohamed", ""]]}, {"id": "1611.04994", "submitter": "Jun Guo", "authors": "Jun Guo, and Hongyang Chao", "title": "One-to-Many Network for Visually Pleasing Compression Artifacts\n  Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the compression artifacts reduction problem, where a compressed\nimage is transformed into an artifact-free image. Recent approaches for this\nproblem typically train a one-to-one mapping using a per-pixel $L_2$ loss\nbetween the outputs and the ground-truths. We point out that these approaches\nused to produce overly smooth results, and PSNR doesn't reflect their real\nperformance. In this paper, we propose a one-to-many network, which measures\noutput quality using a perceptual loss, a naturalness loss, and a JPEG loss. We\nalso avoid grid-like artifacts during deconvolution using a \"shift-and-average\"\nstrategy. Extensive experimental results demonstrate the dramatic visual\nimprovement of our approach over the state of the arts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 19:22:58 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 09:18:59 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Guo", "Jun", ""], ["Chao", "Hongyang", ""]]}, {"id": "1611.05003", "submitter": "Bahadir Gunturk", "authors": "M. Umair Mukati and Bahadir K. Gunturk", "title": "Light Field Stitching for Extended Synthetic Aperture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through capturing spatial and angular radiance distribution, light field\ncameras introduce new capabilities that are not possible with conventional\ncameras. So far in the light field imaging literature, the focus has been on\nthe theory and applications of single light field capture. By combining\nmultiple light fields, it is possible to obtain new capabilities and\nenhancements, and even exceed physical limitations, such as spatial resolution\nand aperture size of the imaging device. In this paper, we present an algorithm\nto register and stitch multiple light fields. We utilize the regularity of the\nspatial and angular sampling in light field data, and extend some techniques\ndeveloped for stereo vision systems to light field data. Such an extension is\nnot straightforward for a micro-lens array (MLA) based light field camera due\nto extremely small baseline and low spatial resolution. By merging multiple\nlight fields captured by an MLA based camera, we obtain larger synthetic\naperture, which results in improvements in light field capabilities, such as\nincreased depth estimation range/accuracy and wider perspective shift range.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 19:49:21 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Mukati", "M. Umair", ""], ["Gunturk", "Bahadir K.", ""]]}, {"id": "1611.05008", "submitter": "Bahadir Gunturk", "authors": "M. Zeshan Alam and Bahadir K. Gunturk", "title": "Hybrid Light Field Imaging for Improved Spatial Resolution and Depth\n  Range", "comments": "Machine Vision and Applications", "journal-ref": null, "doi": "10.1007/s00138-017-0862-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light field imaging involves capturing both angular and spatial distribution\nof light; it enables new capabilities, such as post-capture digital refocusing,\ncamera aperture adjustment, perspective shift, and depth estimation. Micro-lens\narray (MLA) based light field cameras provide a cost-effective approach to\nlight field imaging. There are two main limitations of MLA-based light field\ncameras: low spatial resolution and narrow baseline. While low spatial\nresolution limits the general purpose use and applicability of light field\ncameras, narrow baseline limits the depth estimation range and accuracy. In\nthis paper, we present a hybrid stereo imaging system that includes a light\nfield camera and a regular camera. The hybrid system addresses both spatial\nresolution and narrow baseline issues of the MLA-based light field cameras\nwhile preserving light field imaging capabilities.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 20:04:40 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 08:26:27 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Alam", "M. Zeshan", ""], ["Gunturk", "Bahadir K.", ""]]}, {"id": "1611.05009", "submitter": "Gernot Riegler", "authors": "Gernot Riegler and Ali Osman Ulusoy and Andreas Geiger", "title": "OctNet: Learning Deep 3D Representations at High Resolutions", "comments": "CVPR 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OctNet, a representation for deep learning with sparse 3D data. In\ncontrast to existing models, our representation enables 3D convolutional\nnetworks which are both deep and high resolution. Towards this goal, we exploit\nthe sparsity in the input data to hierarchically partition the space using a\nset of unbalanced octrees where each leaf node stores a pooled feature\nrepresentation. This allows to focus memory allocation and computation to the\nrelevant dense regions and enables deeper networks without compromising\nresolution. We demonstrate the utility of our OctNet representation by\nanalyzing the impact of resolution on several 3D tasks including 3D object\nclassification, orientation estimation and point cloud labeling.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 20:05:45 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 01:37:18 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 14:14:38 GMT"}, {"version": "v4", "created": "Mon, 10 Apr 2017 08:46:56 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Riegler", "Gernot", ""], ["Ulusoy", "Ali Osman", ""], ["Geiger", "Andreas", ""]]}, {"id": "1611.05053", "submitter": "Elad Richardson", "authors": "Elad Richardson, Matan Sela, Roy Or-El, Ron Kimmel", "title": "Learning Detailed Face Reconstruction from a Single Image", "comments": "15 pages, supplementary material included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing the detailed geometric structure of a face from a given image\nis a key to many computer vision and graphics applications, such as motion\ncapture and reenactment. The reconstruction task is challenging as human faces\nvary extensively when considering expressions, poses, textures, and intrinsic\ngeometries. While many approaches tackle this complexity by using additional\ndata to reconstruct the face of a single subject, extracting facial surface\nfrom a single image remains a difficult problem. As a result, single-image\nbased methods can usually provide only a rough estimate of the facial geometry.\nIn contrast, we propose to leverage the power of convolutional neural networks\nto produce a highly detailed face reconstruction from a single image. For this\npurpose, we introduce an end-to-end CNN framework which derives the shape in a\ncoarse-to-fine fashion. The proposed architecture is composed of two main\nblocks, a network that recovers the coarse facial geometry (CoarseNet),\nfollowed by a CNN that refines the facial features of that geometry (FineNet).\nThe proposed networks are connected by a novel layer which renders a depth\nimage given a mesh in 3D. Unlike object recognition and detection problems,\nthere are no suitable datasets for training CNNs to perform face geometry\nreconstruction. Therefore, our training regime begins with a supervised phase,\nbased on synthetic images, followed by an unsupervised phase that uses only\nunconstrained facial images. The accuracy and robustness of the proposed model\nis demonstrated by both qualitative and quantitative evaluation tests.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 21:08:15 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 15:05:16 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Richardson", "Elad", ""], ["Sela", "Matan", ""], ["Or-El", "Roy", ""], ["Kimmel", "Ron", ""]]}, {"id": "1611.05088", "submitter": "Li Zhang", "authors": "Li Zhang, Tao Xiang, Shaogang Gong", "title": "Learning a Deep Embedding Model for Zero-Shot Learning", "comments": "CVPR2017. Adding GZSL results. Code is available at\n  https://github.com/lzrobots/DeepEmbeddingModel_ZSL", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) models rely on learning a joint embedding space\nwhere both textual/semantic description of object classes and visual\nrepresentation of object images can be projected to for nearest neighbour\nsearch. Despite the success of deep neural networks that learn an end-to-end\nmodel between text and images in other vision problems such as image\ncaptioning, very few deep ZSL model exists and they show little advantage over\nZSL models that utilise deep feature representations but do not learn an\nend-to-end embedding. In this paper we argue that the key to make deep ZSL\nmodels succeed is to choose the right embedding space. Instead of embedding\ninto a semantic space or an intermediate space, we propose to use the visual\nspace as the embedding space. This is because that in this space, the\nsubsequent nearest neighbour search would suffer much less from the hubness\nproblem and thus become more effective. This model design also provides a\nnatural mechanism for multiple semantic modalities (e.g., attributes and\nsentence descriptions) to be fused and optimised jointly in an end-to-end\nmanner. Extensive experiments on four benchmarks show that our model\nsignificantly outperforms the existing models. Code is available at\nhttps://github.com/lzrobots/DeepEmbeddingModel_ZSL\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 22:51:43 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 14:55:46 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 10:30:48 GMT"}, {"version": "v4", "created": "Fri, 19 Jul 2019 12:05:03 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Zhang", "Li", ""], ["Xiang", "Tao", ""], ["Gong", "Shaogang", ""]]}, {"id": "1611.05109", "submitter": "Shu Kong", "authors": "Shu Kong, Charless Fowlkes", "title": "Low-rank Bilinear Pooling for Fine-Grained Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooling second-order local feature statistics to form a high-dimensional\nbilinear feature has been shown to achieve state-of-the-art performance on a\nvariety of fine-grained classification tasks. To address the computational\ndemands of high feature dimensionality, we propose to represent the covariance\nfeatures as a matrix and apply a low-rank bilinear classifier. The resulting\nclassifier can be evaluated without explicitly computing the bilinear feature\nmap which allows for a large reduction in the compute time as well as\ndecreasing the effective number of parameters to be learned.\n  To further compress the model, we propose classifier co-decomposition that\nfactorizes the collection of bilinear classifiers into a common factor and\ncompact per-class terms. The co-decomposition idea can be deployed through two\nconvolutional layers and trained in an end-to-end architecture. We suggest a\nsimple yet effective initialization that avoids explicitly first training and\nfactorizing the larger bilinear classifiers. Through extensive experiments, we\nshow that our model achieves state-of-the-art performance on several public\ndatasets for fine-grained classification trained with only category labels.\nImportantly, our final model is an order of magnitude smaller than the recently\nproposed compact bilinear model, and three orders smaller than the standard\nbilinear CNN model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 01:10:41 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 01:30:12 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Kong", "Shu", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1611.05113", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon, Ondrej Chum", "title": "Efficient Diffusion on Region Manifolds: Recovering Small Objects with\n  Compact CNN Representations", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Query expansion is a popular method to improve the quality of image retrieval\nwith both conventional and CNN representations. It has been so far limited to\nglobal image similarity. This work focuses on diffusion, a mechanism that\ncaptures the image manifold in the feature space. The diffusion is carried out\non descriptors of overlapping image regions rather than on a global image\ndescriptor like in previous approaches. An efficient off-line stage allows\noptional reduction in the number of stored regions. In the on-line stage, the\nproposed handling of unseen queries in the indexing stage removes additional\ncomputation to adjust the precomputed data. We perform diffusion through a\nsparse linear system solver, yielding practical query times well below one\nsecond. Experimentally, we observe a significant boost in performance of image\nretrieval with compact CNN descriptors on standard benchmarks, especially when\nthe query object covers only a small part of the image. Small objects have been\na common failure case of CNN-based retrieval.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 01:33:51 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 11:05:41 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 12:17:00 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Iscen", "Ahmet", ""], ["Tolias", "Giorgos", ""], ["Avrithis", "Yannis", ""], ["Furon", "Teddy", ""], ["Chum", "Ondrej", ""]]}, {"id": "1611.05118", "submitter": "Mohit Iyyer", "authors": "Mohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan\n  Boyd-Graber, Hal Daum\\'e III, Larry Davis", "title": "The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels\n  in Comic Book Narratives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual narrative is often a combination of explicit information and judicious\nomissions, relying on the viewer to supply missing details. In comics, most\nmovements in time and space are hidden in the \"gutters\" between panels. To\nfollow the story, readers logically connect panels together by inferring unseen\nactions through a process called \"closure\". While computers can now describe\nwhat is explicitly depicted in natural images, in this paper we examine whether\nthey can understand the closure-driven narratives conveyed by stylized artwork\nand dialogue in comic book panels. We construct a dataset, COMICS, that\nconsists of over 1.2 million panels (120 GB) paired with automatic textbox\ntranscriptions. An in-depth analysis of COMICS demonstrates that neither text\nnor image alone can tell a comic book story, so a computer must understand both\nmodalities to keep up with the plot. We introduce three cloze-style tasks that\nask models to predict narrative and character-centric aspects of a panel given\nn preceding panels as context. Various deep neural architectures underperform\nhuman baselines on these tasks, suggesting that COMICS contains fundamental\nchallenges for both vision and language.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 02:16:09 GMT"}, {"version": "v2", "created": "Sun, 7 May 2017 20:26:24 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Iyyer", "Mohit", ""], ["Manjunatha", "Varun", ""], ["Guha", "Anupam", ""], ["Vyas", "Yogarshi", ""], ["Boyd-Graber", "Jordan", ""], ["Daum\u00e9", "Hal", "III"], ["Davis", "Larry", ""]]}, {"id": "1611.05125", "submitter": "Paritosh Parmar", "authors": "Paritosh Parmar and Brendan Tran Morris", "title": "Learning To Score Olympic Events", "comments": "CVPR 2017 - CVSports Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating action quality, the process of assigning a \"score\" to the\nexecution of an action, is crucial in areas such as sports and health care.\nUnlike action recognition, which has millions of examples to learn from, the\naction quality datasets that are currently available are small -- typically\ncomprised of only a few hundred samples. This work presents three frameworks\nfor evaluating Olympic sports which utilize spatiotemporal features learned\nusing 3D convolutional neural networks (C3D) and perform score regression with\ni) SVR, ii) LSTM, and iii) LSTM followed by SVR. An efficient training\nmechanism for the limited data scenarios is presented for clip-based training\nwith LSTM. The proposed systems show significant improvement over existing\nquality assessment approaches on the task of predicting scores of Olympic\nevents {diving, vault, figure skating}. While the SVR-based frameworks yield\nbetter results, LSTM-based frameworks are more natural for describing an action\nand can be used for improvement feedback.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 02:56:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 00:47:29 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 05:55:24 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Parmar", "Paritosh", ""], ["Morris", "Brendan Tran", ""]]}, {"id": "1611.05128", "submitter": "Vivienne Sze", "authors": "Tien-Ju Yang, Yu-Hsin Chen, Vivienne Sze", "title": "Designing Energy-Efficient Convolutional Neural Networks using\n  Energy-Aware Pruning", "comments": "Published as a conference paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) are indispensable to\nstate-of-the-art computer vision algorithms. However, they are still rarely\ndeployed on battery-powered mobile devices, such as smartphones and wearable\ngadgets, where vision algorithms can enable many revolutionary real-world\napplications. The key limiting factor is the high energy consumption of CNN\nprocessing due to its high computational complexity. While there are many\nprevious efforts that try to reduce the CNN model size or amount of\ncomputation, we find that they do not necessarily result in lower energy\nconsumption, and therefore do not serve as a good metric for energy cost\nestimation.\n  To close the gap between CNN design and energy consumption optimization, we\npropose an energy-aware pruning algorithm for CNNs that directly uses energy\nconsumption estimation of a CNN to guide the pruning process. The energy\nestimation methodology uses parameters extrapolated from actual hardware\nmeasurements that target realistic battery-powered system setups. The proposed\nlayer-by-layer pruning algorithm also prunes more aggressively than previously\nproposed pruning methods by minimizing the error in output feature maps instead\nof filter weights. For each layer, the weights are first pruned and then\nlocally fine-tuned with a closed-form least-square solution to quickly restore\nthe accuracy. After all layers are pruned, the entire network is further\nglobally fine-tuned using back-propagation. With the proposed pruning method,\nthe energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x,\nrespectively, with less than 1% top-5 accuracy loss. Finally, we show that\npruning the AlexNet with a reduced number of target classes can greatly\ndecrease the number of weights but the energy reduction is limited.\n  Energy modeling tool and energy-aware pruned models available at\nhttp://eyeriss.mit.edu/energy.html\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 03:00:40 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 12:17:44 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 19:52:08 GMT"}, {"version": "v4", "created": "Tue, 18 Apr 2017 19:49:29 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Yang", "Tien-Ju", ""], ["Chen", "Yu-Hsin", ""], ["Sze", "Vivienne", ""]]}, {"id": "1611.05134", "submitter": "Yu-An Chung", "authors": "Yu-An Chung, Shao-Wen Yang, Hsuan-Tien Lin", "title": "Cost-Sensitive Deep Learning with Layer-Wise Cost Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep neural networks have succeeded in several visual applications,\nsuch as object recognition, detection, and localization, by reaching very high\nclassification accuracies, it is important to note that many real-world\napplications demand varying costs for different types of misclassification\nerrors, thus requiring cost-sensitive classification algorithms. Current models\nof deep neural networks for cost-sensitive classification are restricted to\nsome specific network structures and limited depth. In this paper, we propose a\nnovel framework that can be applied to deep neural networks with any structure\nto facilitate their learning of meaningful representations for cost-sensitive\nclassification problems. Furthermore, the framework allows end-to-end training\nof deeper networks directly. The framework is designed by augmenting auxiliary\nneurons to the output of each hidden layer for layer-wise cost estimation, and\nincluding the total estimation loss within the optimization objective.\nExperimental results on public benchmark visual data sets with two cost\ninformation settings demonstrate that the proposed framework outperforms\nstate-of-the-art cost-sensitive deep learning models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 03:31:25 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 03:10:16 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chung", "Yu-An", ""], ["Yang", "Shao-Wen", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1611.05138", "submitter": "Shuangfei Zhai", "authors": "Shuangfei Zhai, Hui Wu, Abhishek Kumar, Yu Cheng, Yongxi Lu, Zhongfei\n  Zhang, Rogerio Feris", "title": "S3Pool: Pooling with Stochastic Spatial Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature pooling layers (e.g., max pooling) in convolutional neural networks\n(CNNs) serve the dual purpose of providing increasingly abstract\nrepresentations as well as yielding computational savings in subsequent\nconvolutional layers. We view the pooling operation in CNNs as a two-step\nprocedure: first, a pooling window (e.g., $2\\times 2$) slides over the feature\nmap with stride one which leaves the spatial resolution intact, and second,\ndownsampling is performed by selecting one pixel from each non-overlapping\npooling window in an often uniform and deterministic (e.g., top-left) manner.\nOur starting point in this work is the observation that this regularly spaced\ndownsampling arising from non-overlapping windows, although intuitive from a\nsignal processing perspective (which has the goal of signal reconstruction), is\nnot necessarily optimal for \\emph{learning} (where the goal is to generalize).\nWe study this aspect and propose a novel pooling strategy with stochastic\nspatial sampling (S3Pool), where the regular downsampling is replaced by a more\ngeneral stochastic version. We observe that this general stochasticity acts as\na strong regularizer, and can also be seen as doing implicit data augmentation\nby introducing distortions in the feature maps. We further introduce a\nmechanism to control the amount of distortion to suit different datasets and\narchitectures. To demonstrate the effectiveness of the proposed approach, we\nperform extensive experiments on several popular image classification\nbenchmarks, observing excellent improvements over baseline models. Experimental\ncode is available at https://github.com/Shuangfei/s3pool.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 04:17:52 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhai", "Shuangfei", ""], ["Wu", "Hui", ""], ["Kumar", "Abhishek", ""], ["Cheng", "Yu", ""], ["Lu", "Yongxi", ""], ["Zhang", "Zhongfei", ""], ["Feris", "Rogerio", ""]]}, {"id": "1611.05148", "submitter": "Yin Zheng", "authors": "Zhuxi Jiang, Yin Zheng, Huachun Tan, Bangsheng Tang, Hanning Zhou", "title": "Variational Deep Embedding: An Unsupervised and Generative Approach to\n  Clustering", "comments": "8 pages, 6 figures, accepted by IJCAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is among the most fundamental tasks in computer vision and machine\nlearning. In this paper, we propose Variational Deep Embedding (VaDE), a novel\nunsupervised generative clustering approach within the framework of Variational\nAuto-Encoder (VAE). Specifically, VaDE models the data generative procedure\nwith a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM\npicks a cluster; 2) from which a latent embedding is generated; 3) then the DNN\ndecodes the latent embedding into observables. Inference in VaDE is done in a\nvariational way: a different DNN is used to encode observables to latent\nembeddings, so that the evidence lower bound (ELBO) can be optimized using\nStochastic Gradient Variational Bayes (SGVB) estimator and the\nreparameterization trick. Quantitative comparisons with strong baselines are\nincluded in this paper, and experimental results show that VaDE significantly\noutperforms the state-of-the-art clustering methods on 4 benchmarks from\nvarious modalities. Moreover, by VaDE's generative nature, we show its\ncapability of generating highly realistic samples for any specified cluster,\nwithout using supervised information during training. Lastly, VaDE is a\nflexible and extensible framework for unsupervised generative clustering, more\ngeneral mixture models than GMM can be easily plugged in.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 05:19:50 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 03:21:58 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 02:45:32 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Jiang", "Zhuxi", ""], ["Zheng", "Yin", ""], ["Tan", "Huachun", ""], ["Tang", "Bangsheng", ""], ["Zhou", "Hanning", ""]]}, {"id": "1611.05198", "submitter": "Kevis-Kokitsi Maninis", "authors": "Sergi Caelles and Kevis-Kokitsi Maninis and Jordi Pont-Tuset and Laura\n  Leal-Taix\\'e and Daniel Cremers and Luc Van Gool", "title": "One-Shot Video Object Segmentation", "comments": "CVPR 2017 camera ready. Code:\n  http://www.vision.ee.ethz.ch/~cvlsegmentation/osvos/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the task of semi-supervised video object segmentation,\ni.e., the separation of an object from the background in a video, given the\nmask of the first frame. We present One-Shot Video Object Segmentation (OSVOS),\nbased on a fully-convolutional neural network architecture that is able to\nsuccessively transfer generic semantic information, learned on ImageNet, to the\ntask of foreground segmentation, and finally to learning the appearance of a\nsingle annotated object of the test sequence (hence one-shot). Although all\nframes are processed independently, the results are temporally coherent and\nstable. We perform experiments on two annotated video segmentation databases,\nwhich show that OSVOS is fast and improves the state of the art by a\nsignificant margin (79.8% vs 68.0%).\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 09:58:37 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 13:51:01 GMT"}, {"version": "v3", "created": "Tue, 13 Dec 2016 16:01:05 GMT"}, {"version": "v4", "created": "Thu, 13 Apr 2017 08:08:55 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Caelles", "Sergi", ""], ["Maninis", "Kevis-Kokitsi", ""], ["Pont-Tuset", "Jordi", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Cremers", "Daniel", ""], ["Van Gool", "Luc", ""]]}, {"id": "1611.05203", "submitter": "Katharina Schwarz", "authors": "Katharina Schwarz, Patrick Wieschollek, Hendrik P. A. Lensch", "title": "Will People Like Your Image? Learning the Aesthetic Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rating how aesthetically pleasing an image appears is a highly complex matter\nand depends on a large number of different visual factors. Previous work has\ntackled the aesthetic rating problem by ranking on a 1-dimensional rating\nscale, e.g., incorporating handcrafted attributes. In this paper, we propose a\nrather general approach to automatically map aesthetic pleasingness with all\nits complexity into an \"aesthetic space\" to allow for a highly fine-grained\nresolution. In detail, making use of deep learning, our method directly learns\nan encoding of a given image into this high-dimensional feature space\nresembling visual aesthetics. Additionally to the mentioned visual factors,\ndifferences in personal judgments have a large impact on the likeableness of a\nphotograph. Nowadays, online platforms allow users to \"like\" or favor certain\ncontent with a single click. To incorporate a huge diversity of people, we make\nuse of such multi-user agreements and assemble a large data set of 380K images\n(AROD) with associated meta information and derive a score to rate how visually\npleasing a given photo is. We validate our derived model of aesthetics in a\nuser study. Further, without any extra data labeling or handcrafted features,\nwe achieve state-of-the art accuracy on the AVA benchmark data set. Finally, as\nour approach is able to predict the aesthetic quality of any arbitrary image or\nvideo, we demonstrate our results on applications for resorting photo\ncollections, capturing the best shot on mobile devices and aesthetic key-frame\nextraction from videos.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:13:06 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 08:36:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Schwarz", "Katharina", ""], ["Wieschollek", "Patrick", ""], ["Lensch", "Hendrik P. A.", ""]]}, {"id": "1611.05209", "submitter": "Siddharth Agrawal", "authors": "Siddharth Agrawal, Ambedkar Dukkipati", "title": "Deep Variational Inference Without Pixel-Wise Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs), that are built upon deep neural networks\nhave emerged as popular generative models in computer vision. Most of the work\ntowards improving variational autoencoders has focused mainly on making the\napproximations to the posterior flexible and accurate, leading to tremendous\nprogress. However, there have been limited efforts to replace pixel-wise\nreconstruction, which have known shortcomings. In this work, we use real-valued\nnon-volume preserving transformations (real NVP) to exactly compute the\nconditional likelihood of the data given the latent distribution. We show that\na simple VAE with this form of reconstruction is competitive with complicated\nVAE structures, on image modeling tasks. As part of our model, we develop\npowerful conditional coupling layers that enable real NVP to learn with fewer\nintermediate layers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:20:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Agrawal", "Siddharth", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1611.05215", "submitter": "Yemin Shi Shi", "authors": "Yemin Shi and Yonghong Tian and Yaowei Wang and Tiejun Huang", "title": "Joint Network based Attention for Action Recognition", "comments": "8 pages, 5 figures, JNA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By extracting spatial and temporal characteristics in one network, the\ntwo-stream ConvNets can achieve the state-of-the-art performance in action\nrecognition. However, such a framework typically suffers from the separately\nprocessing of spatial and temporal information between the two standalone\nstreams and is hard to capture long-term temporal dependence of an action. More\nimportantly, it is incapable of finding the salient portions of an action, say,\nthe frames that are the most discriminative to identify the action. To address\nthese problems, a \\textbf{j}oint \\textbf{n}etwork based \\textbf{a}ttention\n(JNA) is proposed in this study. We find that the fully-connected fusion,\nbranch selection and spatial attention mechanism are totally infeasible for\naction recognition. Thus in our joint network, the spatial and temporal\nbranches share some information during the training stage. We also introduce an\nattention mechanism on the temporal domain to capture the long-term dependence\nmeanwhile finding the salient portions. Extensive experiments are conducted on\ntwo benchmark datasets, UCF101 and HMDB51. Experimental results show that our\nmethod can improve the action recognition performance significantly and\nachieves the state-of-the-art results on both datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:40:30 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shi", "Yemin", ""], ["Tian", "Yonghong", ""], ["Wang", "Yaowei", ""], ["Huang", "Tiejun", ""]]}, {"id": "1611.05216", "submitter": "Yemin Shi Shi", "authors": "Yemin Shi and Yonghong Tian and Yaowei Wang and Tiejun Huang", "title": "Learning long-term dependencies for action recognition with a\n  biologically-inspired deep network", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a lot of research efforts devoted in recent years, how to efficiently\nlearn long-term dependencies from sequences still remains a pretty challenging\ntask. As one of the key models for sequence learning, recurrent neural network\n(RNN) and its variants such as long short term memory (LSTM) and gated\nrecurrent unit (GRU) are still not powerful enough in practice. One possible\nreason is that they have only feedforward connections, which is different from\nthe biological neural system that is typically composed of both feedforward and\nfeedback connections. To address this problem, this paper proposes a\nbiologically-inspired deep network, called shuttleNet\\footnote{Our code is\navailable at \\url{https://github.com/shiyemin/shuttlenet}}. Technologically,\nthe shuttleNet consists of several processors, each of which is a GRU while\nassociated with multiple groups of cells and states. Unlike traditional RNNs,\nall processors inside shuttleNet are loop connected to mimic the brain's\nfeedforward and feedback connections, in which they are shared across multiple\npathways in the loop connection. Attention mechanism is then employed to select\nthe best information flow pathway. Extensive experiments conducted on two\nbenchmark datasets (i.e UCF101 and HMDB51) show that we can beat\nstate-of-the-art methods by simply embedding shuttleNet into a CNN-RNN\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:49:43 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:55:14 GMT"}, {"version": "v3", "created": "Sun, 19 Mar 2017 08:27:24 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Shi", "Yemin", ""], ["Tian", "Yonghong", ""], ["Wang", "Yaowei", ""], ["Huang", "Tiejun", ""]]}, {"id": "1611.05241", "submitter": "Florian Bernard", "authors": "Florian Bernard, Frank R. Schmidt, Johan Thunberg, Daniel Cremers", "title": "A Combinatorial Solution to Non-Rigid 3D Shape-to-Image Matching", "comments": "10 pages, 7 figures", "journal-ref": "CVPR 2017", "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a combinatorial solution for the problem of non-rigidly matching a\n3D shape to 3D image data. To this end, we model the shape as a triangular mesh\nand allow each triangle of this mesh to be rigidly transformed to achieve a\nsuitable matching to the image. By penalising the distance and the relative\nrotation between neighbouring triangles our matching compromises between image\nand shape information. In this paper, we resolve two major challenges: Firstly,\nwe address the resulting large and NP-hard combinatorial problem with a\nsuitable graph-theoretic approach. Secondly, we propose an efficient\ndiscretisation of the unbounded 6-dimensional Lie group SE(3). To our knowledge\nthis is the first combinatorial formulation for non-rigid 3D shape-to-image\nmatching. In contrast to existing local (gradient descent) optimisation\nmethods, we obtain solutions that do not require a good initialisation and that\nare within a bound of the optimal solution. We evaluate the proposed method on\nthe two problems of non-rigid 3D shape-to-shape and non-rigid 3D shape-to-image\nregistration and demonstrate that it provides promising results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 12:08:13 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 14:59:07 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bernard", "Florian", ""], ["Schmidt", "Frank R.", ""], ["Thunberg", "Johan", ""], ["Cremers", "Daniel", ""]]}, {"id": "1611.05244", "submitter": "Mengyue Geng", "authors": "Mengyue Geng and Yaowei Wang and Tao Xiang and Yonghong Tian", "title": "Deep Transfer Learning for Person Re-identification", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) poses a unique challenge to deep learning:\nhow to learn a deep model with millions of parameters on a small training set\nof few or no labels. In this paper, a number of deep transfer learning models\nare proposed to address the data sparsity problem. First, a deep network\narchitecture is designed which differs from existing deep Re-ID models in that\n(a) it is more suitable for transferring representations learned from large\nimage classification datasets, and (b) classification loss and verification\nloss are combined, each of which adopts a different dropout strategy. Second, a\ntwo-stepped fine-tuning strategy is developed to transfer knowledge from\nauxiliary datasets. Third, given an unlabelled Re-ID dataset, a novel\nunsupervised deep transfer learning model is developed based on co-training.\nThe proposed models outperform the state-of-the-art deep Re-ID models by large\nmargins: we achieve Rank-1 accuracy of 85.4\\%, 83.7\\% and 56.3\\% on CUHK03,\nMarket1501, and VIPeR respectively, whilst on VIPeR, our unsupervised model\n(45.1\\%) beats most supervised models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 12:14:09 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 12:16:51 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Geng", "Mengyue", ""], ["Wang", "Yaowei", ""], ["Xiang", "Tao", ""], ["Tian", "Yonghong", ""]]}, {"id": "1611.05250", "submitter": "Jose Caballero", "authors": "Jose Caballero, Christian Ledig, Andrew Aitken, Alejandro Acosta,\n  Johannes Totz, Zehan Wang, Wenzhe Shi", "title": "Real-Time Video Super-Resolution with Spatio-Temporal Networks and\n  Motion Compensation", "comments": "Changes: * Uploaded Vid4 results (footnote 1). * Added references\n  [14, 29] as spatial-transformer prior art. * Fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have enabled accurate image super-resolution in\nreal-time. However, recent attempts to benefit from temporal correlations in\nvideo super-resolution have been limited to naive or inefficient architectures.\nIn this paper, we introduce spatio-temporal sub-pixel convolution networks that\neffectively exploit temporal redundancies and improve reconstruction accuracy\nwhile maintaining real-time speed. Specifically, we discuss the use of early\nfusion, slow fusion and 3D convolutions for the joint processing of multiple\nconsecutive video frames. We also propose a novel joint motion compensation and\nvideo super-resolution algorithm that is orders of magnitude more efficient\nthan competing methods, relying on a fast multi-resolution spatial transformer\nmodule that is end-to-end trainable. These contributions provide both higher\naccuracy and temporally more consistent videos, which we confirm qualitatively\nand quantitatively. Relative to single-frame models, spatio-temporal networks\ncan either reduce the computational cost by 30% whilst maintaining the same\nquality or provide a 0.2dB gain for a similar computational cost. Results on\npublicly available datasets demonstrate that the proposed algorithms surpass\ncurrent state-of-the-art performance in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 12:25:08 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 11:53:26 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Caballero", "Jose", ""], ["Ledig", "Christian", ""], ["Aitken", "Andrew", ""], ["Acosta", "Alejandro", ""], ["Totz", "Johannes", ""], ["Wang", "Zehan", ""], ["Shi", "Wenzhe", ""]]}, {"id": "1611.05267", "submitter": "Colin Lea", "authors": "Colin Lea, Michael D. Flynn, Rene Vidal, Austin Reiter, Gregory D.\n  Hager", "title": "Temporal Convolutional Networks for Action Segmentation and Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to identify and temporally segment fine-grained human actions\nthroughout a video is crucial for robotics, surveillance, education, and\nbeyond. Typical approaches decouple this problem by first extracting local\nspatiotemporal features from video frames and then feeding them into a temporal\nclassifier that captures high-level temporal patterns. We introduce a new class\nof temporal models, which we call Temporal Convolutional Networks (TCNs), that\nuse a hierarchy of temporal convolutions to perform fine-grained action\nsegmentation or detection. Our Encoder-Decoder TCN uses pooling and upsampling\nto efficiently capture long-range temporal patterns whereas our Dilated TCN\nuses dilated convolutions. We show that TCNs are capable of capturing action\ncompositions, segment durations, and long-range dependencies, and are over a\nmagnitude faster to train than competing LSTM-based Recurrent Neural Networks.\nWe apply these models to three challenging fine-grained datasets and show large\nimprovements over the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 13:19:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lea", "Colin", ""], ["Flynn", "Michael D.", ""], ["Vidal", "Rene", ""], ["Reiter", "Austin", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1611.05271", "submitter": "Shu Zhang", "authors": "Shu Zhang, Ran He, and Tieniu Tan", "title": "DeMeshNet: Blind Face Inpainting for Deep MeshFace Verification", "comments": "10pages, submitted to CVPR 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MeshFace photos have been widely used in many Chinese business organizations\nto protect ID face photos from being misused. The occlusions incurred by random\nmeshes severely degenerate the performance of face verification systems, which\nraises the MeshFace verification problem between MeshFace and daily photos.\nPrevious methods cast this problem as a typical low-level vision problem, i.e.\nblind inpainting. They recover perceptually pleasing clear ID photos from\nMeshFaces by enforcing pixel level similarity between the recovered ID images\nand the ground-truth clear ID images and then perform face verification on\nthem. Essentially, face verification is conducted on a compact feature space\nrather than the image pixel space. Therefore, this paper argues that pixel\nlevel similarity and feature level similarity jointly offer the key to improve\nthe verification performance. Based on this insight, we offer a novel feature\noriented blind face inpainting framework. Specifically, we implement this by\nestablishing a novel DeMeshNet, which consists of three parts. The first part\naddresses blind inpainting of the MeshFaces by implicitly exploiting extra\nsupervision from the occlusion position to enforce pixel level similarity. The\nsecond part explicitly enforces a feature level similarity in the compact\nfeature space, which can explore informative supervision from the feature space\nto produce better inpainting results for verification. The last part copes with\nface alignment within the net via a customized spatial transformer module when\nextracting deep facial features. All the three parts are implemented within an\nend-to-end network that facilitates efficient optimization. Extensive\nexperiments on two MeshFace datasets demonstrate the effectiveness of the\nproposed DeMeshNet as well as the insight of this paper.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 13:36:45 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Zhang", "Shu", ""], ["He", "Ran", ""], ["Tan", "Tieniu", ""]]}, {"id": "1611.05301", "submitter": "Tu Bui", "authors": "Tu Bui, Leonardo Ribeiro, Moacir Ponti, John Collomosse", "title": "Generalisation and Sharing in Triplet Convnets for Sketch based Visual\n  Search", "comments": "submitted to CVPR2017 on 15Nov16", "journal-ref": "Computers & Graphics, vol 71, pages 77-87 (2017)", "doi": "10.1016/j.cag.2017.12.006", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and evaluate several triplet CNN architectures for measuring the\nsimilarity between sketches and photographs, within the context of the sketch\nbased image retrieval (SBIR) task. In contrast to recent fine-grained SBIR\nwork, we study the ability of our networks to generalise across diverse object\ncategories from limited training data, and explore in detail strategies for\nweight sharing, pre-processing, data augmentation and dimensionality reduction.\nWe exceed the performance of pre-existing techniques on both the Flickr15k\ncategory level SBIR benchmark by $18\\%$, and the TU-Berlin SBIR benchmark by\n$\\sim10 \\mathcal{T}_b$, when trained on the 250 category TU-Berlin\nclassification dataset augmented with 25k corresponding photographs harvested\nfrom the Internet.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 14:57:19 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Bui", "Tu", ""], ["Ribeiro", "Leonardo", ""], ["Ponti", "Moacir", ""], ["Collomosse", "John", ""]]}, {"id": "1611.05319", "submitter": "Laird Robert Hocking", "authors": "L. Robert Hocking, Russell MacKenzie, and Carola-Bibiane Schoenlieb", "title": "Guidefill: GPU Accelerated, Artist Guided Geometric Inpainting for 3D\n  Conversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conversion of traditional film into stereo 3D has become an important\nproblem in the past decade. One of the main bottlenecks is a disocclusion step,\nwhich in commercial 3D conversion is usually done by teams of artists armed\nwith a toolbox of inpainting algorithms. A current difficulty in this is that\nmost available algorithms are either too slow for interactive use, or provide\nno intuitive means for users to tweak the output. In this paper we present a\nnew fast inpainting algorithm based on transporting along automatically\ndetected splines, which the user may edit. Our algorithm is implemented on the\nGPU and fills the inpainting domain in successive shells that adapt their shape\non the fly. In order to allocate GPU resources as efficiently as possible, we\npropose a parallel algorithm to track the inpainting interface as it evolves,\nensuring that no resources are wasted on pixels that are not currently being\nworked on. Theoretical analysis of the time and processor complexiy of our\nalgorithm without and with tracking (as well as numerous numerical experiments)\ndemonstrate the merits of the latter. Our transport mechanism is similar to the\none used in coherence transport, but improves upon it by corrected a \"kinking\"\nphenomena whereby extrapolated isophotes may bend at the boundary of the\ninpainting domain. Theoretical results explaining this phenomena and its\nresolution are presented. Although our method ignores texture, in many cases\nthis is not a problem due to the thin inpainting domains in 3D conversion.\nExperimental results show that our method can achieve a visual quality that is\ncompetitive with the state-of-the-art while maintaining interactive speeds and\nproviding the user with an intuitive interface to tweak the results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:30:58 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 18:26:27 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 19:40:09 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Hocking", "L. Robert", ""], ["MacKenzie", "Russell", ""], ["Schoenlieb", "Carola-Bibiane", ""]]}, {"id": "1611.05321", "submitter": "Aurelien Lucchi", "authors": "Wenhu Chen and Aurelien Lucchi and Thomas Hofmann", "title": "A Semi-supervised Framework for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches for image captioning require supervised training\ndata consisting of captions with paired image data. These methods are typically\nunable to use unsupervised data such as textual data with no corresponding\nimages, which is a much more abundant commodity. We here propose a novel way of\nusing such textual data by artificially generating missing visual information.\nWe evaluate this learning approach on a newly designed model that detects\nvisual concepts present in an image and feed them to a reviewer-decoder\narchitecture with an attention mechanism. Unlike previous approaches that\nencode visual concepts using word embeddings, we instead suggest using regional\nimage features which capture more intrinsic information. The main benefit of\nthis architecture is that it synthesizes meaningful thought vectors that\ncapture salient image properties and then applies a soft attentive decoder to\ndecode the thought vectors and generate image captions. We evaluate our model\non both Microsoft COCO and Flickr30K datasets and demonstrate that this model\ncombined with our semi-supervised learning method can largely improve\nperformance and help the model to generate more accurate and diverse captions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:33:12 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 13:51:31 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 08:24:44 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Chen", "Wenhu", ""], ["Lucchi", "Aurelien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "1611.05328", "submitter": "Zhiwei Jin", "authors": "Zhiwei Jin, Juan Cao, Jiebo Luo, and Yongdong Zhang", "title": "Image Credibility Analysis with Effective Domain Transferred Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous fake images spread on social media today and can severely jeopardize\nthe credibility of online content to public. In this paper, we employ deep\nnetworks to learn distinct fake image related features. In contrast to\nauthentic images, fake images tend to be eye-catching and visually striking.\nCompared with traditional visual recognition tasks, it is extremely challenging\nto understand these psychologically triggered visual patterns in fake images.\nTraditional general image classification datasets, such as ImageNet set, are\ndesigned for feature learning at the object level but are not suitable for\nlearning the hyper-features that would be required by image credibility\nanalysis. In order to overcome the scarcity of training samples of fake images,\nwe first construct a large-scale auxiliary dataset indirectly related to this\ntask. This auxiliary dataset contains 0.6 million weakly-labeled fake and real\nimages collected automatically from social media. Through an AdaBoost-like\ntransfer learning algorithm, we train a CNN model with a few instances in the\ntarget training set and 0.6 million images in the collected auxiliary set. This\nlearning algorithm is able to leverage knowledge from the auxiliary set and\ngradually transfer it to the target task. Experiments on a real-world testing\nset show that our proposed domain transferred CNN model outperforms several\ncompeting baselines. It obtains superiror results over transfer learning\nmethods based on the general ImageNet set. Moreover, case studies show that our\nproposed method reveals some interesting patterns for distinguishing fake and\nauthentic images.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:45:19 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jin", "Zhiwei", ""], ["Cao", "Juan", ""], ["Luo", "Jiebo", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1611.05335", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi", "title": "Unsupervised Learning of Important Objects from First-Person Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A first-person camera, placed at a person's head, captures, which objects are\nimportant to the camera wearer. Most prior methods for this task learn to\ndetect such important objects from the manually labeled first-person data in a\nsupervised fashion. However, important objects are strongly related to the\ncamera wearer's internal state such as his intentions and attention, and thus,\nonly the person wearing the camera can provide the importance labels. Such a\nconstraint makes the annotation process costly and limited in scalability.\n  In this work, we show that we can detect important objects in first-person\nimages without the supervision by the camera wearer or even third-person\nlabelers. We formulate an important detection problem as an interplay between\nthe 1) segmentation and 2) recognition agents. The segmentation agent first\nproposes a possible important object segmentation mask for each image, and then\nfeeds it to the recognition agent, which learns to predict an important object\nmask using visual semantics and spatial features.\n  We implement such an interplay between both agents via an alternating\ncross-pathway supervision scheme inside our proposed Visual-Spatial Network\n(VSN). Our VSN consists of spatial (\"where\") and visual (\"what\") pathways, one\nof which learns common visual semantics while the other focuses on the spatial\nlocation cues. Our unsupervised learning is accomplished via a cross-pathway\nsupervision, where one pathway feeds its predictions to a segmentation agent,\nwhich proposes a candidate important object segmentation mask that is then used\nby the other pathway as a supervisory signal. We show our method's success on\ntwo different important object datasets, where our method achieves similar or\nbetter results as the supervised methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:53:12 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 14:58:34 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 14:57:04 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Bertasius", "Gedas", ""], ["Park", "Hyun Soo", ""], ["Yu", "Stella X.", ""], ["Shi", "Jianbo", ""]]}, {"id": "1611.05345", "submitter": "Hisham Cholakkal", "authors": "Hisham Cholakkal, Jubin Johnson and Deepu Rajan", "title": "Backtracking Spatial Pyramid Pooling (SPP)-based Image Classifier for\n  Weakly Supervised Top-down Salient Object Detection", "comments": "14 pages, 7 figures", "journal-ref": "H. Cholakkal, J. Johnson, D. Rajan, \"Backtracking Spatial Pyramid\n  Pooling (SPP)-based Image Classifier for Weakly Supervised Top-down Salient\n  Object Detection\", in IEEE Transactions on Image processing, August 2018", "doi": "10.1109/TIP.2018.2864891", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down saliency models produce a probability map that peaks at target\nlocations specified by a task/goal such as object detection. They are usually\ntrained in a fully supervised setting involving pixel-level annotations of\nobjects. We propose a weakly supervised top-down saliency framework using only\nbinary labels that indicate the presence/absence of an object in an image.\nFirst, the probabilistic contribution of each image region to the confidence of\na CNN-based image classifier is computed through a backtracking strategy to\nproduce top-down saliency. From a set of saliency maps of an image produced by\nfast bottom-up saliency approaches, we select the best saliency map suitable\nfor the top-down task. The selected bottom-up saliency map is combined with the\ntop-down saliency map. Features having high combined saliency are used to train\na linear SVM classifier to estimate feature saliency. This is integrated with\ncombined saliency and further refined through a multi-scale\nsuperpixel-averaging of saliency map. We evaluate the performance of the\nproposed weakly supervised topdown saliency and achieve comparable performance\nwith fully supervised approaches. Experiments are carried out on seven\nchallenging datasets and quantitative results are compared with 40 closely\nrelated approaches across 4 different applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 16:23:51 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 02:24:23 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 17:39:11 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Cholakkal", "Hisham", ""], ["Johnson", "Jubin", ""], ["Rajan", "Deepu", ""]]}, {"id": "1611.05358", "submitter": "Joon Son Chung", "authors": "Joon Son Chung, Andrew Senior, Oriol Vinyals, Andrew Zisserman", "title": "Lip Reading Sentences in the Wild", "comments": null, "journal-ref": null, "doi": "10.1109/CVPR.2017.367", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to recognise phrases and sentences being spoken by a\ntalking face, with or without the audio. Unlike previous works that have\nfocussed on recognising a limited number of words or phrases, we tackle lip\nreading as an open-world problem - unconstrained natural language sentences,\nand in the wild videos.\n  Our key contributions are: (1) a 'Watch, Listen, Attend and Spell' (WLAS)\nnetwork that learns to transcribe videos of mouth motion to characters; (2) a\ncurriculum learning strategy to accelerate training and to reduce overfitting;\n(3) a 'Lip Reading Sentences' (LRS) dataset for visual speech recognition,\nconsisting of over 100,000 natural sentences from British television.\n  The WLAS model trained on the LRS dataset surpasses the performance of all\nprevious work on standard lip reading benchmark datasets, often by a\nsignificant margin. This lip reading performance beats a professional lip\nreader on videos from BBC television, and we also demonstrate that visual\ninformation helps to improve speech recognition performance even when the audio\nis available.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 16:53:46 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 22:46:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Chung", "Joon Son", ""], ["Senior", "Andrew", ""], ["Vinyals", "Oriol", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1611.05365", "submitter": "Gedas Bertasius", "authors": "Gedas Bertasius, Hyun Soo Park, Stella X. Yu, Jianbo Shi", "title": "Am I a Baller? Basketball Performance Assessment from First-Person\n  Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to assess a basketball player's performance from\nhis/her first-person video. A key challenge lies in the fact that the\nevaluation metric is highly subjective and specific to a particular evaluator.\nWe leverage the first-person camera to address this challenge. The\nspatiotemporal visual semantics provided by a first-person view allows us to\nreason about the camera wearer's actions while he/she is participating in an\nunscripted basketball game. Our method takes a player's first-person video and\nprovides a player's performance measure that is specific to an evaluator's\npreference.\n  To achieve this goal, we first use a convolutional LSTM network to detect\natomic basketball events from first-person videos. Our network's ability to\nzoom-in to the salient regions addresses the issue of a severe camera wearer's\nhead movement in first-person videos. The detected atomic events are then\npassed through the Gaussian mixtures to construct a highly non-linear visual\nspatiotemporal basketball assessment feature. Finally, we use this feature to\nlearn a basketball assessment model from pairs of labeled first-person\nbasketball videos, for which a basketball expert indicates, which of the two\nplayers is better.\n  We demonstrate that despite not knowing the basketball evaluator's criterion,\nour model learns to accurately assess the players in real-world games.\nFurthermore, our model can also discover basketball events that contribute\npositively and negatively to a player's performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:01:27 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 03:02:23 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 17:03:02 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 15:10:27 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Bertasius", "Gedas", ""], ["Park", "Hyun Soo", ""], ["Yu", "Stella X.", ""], ["Shi", "Jianbo", ""]]}, {"id": "1611.05368", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Neural Style Representations and the Large-Scale Classification of\n  Artistic Style", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Proceedings of the Future Technologies Conference, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1611.05369", "submitter": "Melanie Mitchell", "authors": "Anthony D. Rhodes, Max H. Quinn, and Melanie Mitchell", "title": "Fast On-Line Kernel Density Estimation for Active Object Localization", "comments": "arXiv admin note: text overlap with arXiv:1607.00548", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major goal of computer vision is to enable computers to interpret visual\nsituations---abstract concepts (e.g., \"a person walking a dog,\" \"a crowd\nwaiting for a bus,\" \"a picnic\") whose image instantiations are linked more by\ntheir common spatial and semantic structure than by low-level visual\nsimilarity. In this paper, we propose a novel method for prior learning and\nactive object localization for this kind of knowledge-driven search in static\nimages. In our system, prior situation knowledge is captured by a set of\nflexible, kernel-based density estimations---a situation model---that represent\nthe expected spatial structure of the given situation. These estimations are\nefficiently updated by information gained as the system searches for relevant\nobjects, allowing the system to use context as it is discovered to narrow the\nsearch.\n  More specifically, at any given time in a run on a test image, our system\nuses image features plus contextual information it has discovered to identify a\nsmall subset of training images---an importance cluster---that is deemed most\nsimilar to the given test image, given the context. This subset is used to\ngenerate an updated situation model in an on-line fashion, using an efficient\nmultipole expansion technique.\n  As a proof of concept, we apply our algorithm to a highly varied and\nchallenging dataset consisting of instances of a \"dog-walking\" situation. Our\nresults support the hypothesis that dynamically-rendered, context-based\nprobability models can support efficient object localization in visual\nsituations. Moreover, our approach is general enough to be applied to diverse\nmachine learning paradigms requiring interpretable, probabilistic\nrepresentations generated from partially observed data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:35 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Rhodes", "Anthony D.", ""], ["Quinn", "Max H.", ""], ["Mitchell", "Melanie", ""]]}, {"id": "1611.05377", "submitter": "Yongxi Lu", "authors": "Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi,\n  Rogerio Feris", "title": "Fully-adaptive Feature Sharing in Multi-Task Networks with Applications\n  in Person Attribute Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning aims to improve generalization performance of multiple\nprediction tasks by appropriately sharing relevant information across them. In\nthe context of deep neural networks, this idea is often realized by\nhand-designed network architectures with layers that are shared across tasks\nand branches that encode task-specific features. However, the space of possible\nmulti-task deep architectures is combinatorially large and often the final\narchitecture is arrived at by manual exploration of this space subject to\ndesigner's bias, which can be both error-prone and tedious. In this work, we\npropose a principled approach for designing compact multi-task deep learning\narchitectures. Our approach starts with a thin network and dynamically widens\nit in a greedy manner during training using a novel criterion that promotes\ngrouping of similar tasks together. Our Extensive evaluation on person\nattributes classification tasks involving facial and clothing attributes\nsuggests that the models produced by the proposed method are fast, compact and\ncan closely match or exceed the state-of-the-art accuracy from strong baselines\nby much more expensive models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:31:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lu", "Yongxi", ""], ["Kumar", "Abhishek", ""], ["Zhai", "Shuangfei", ""], ["Cheng", "Yu", ""], ["Javidi", "Tara", ""], ["Feris", "Rogerio", ""]]}, {"id": "1611.05396", "submitter": "Zhenhua Feng", "authors": "Zhen-Hua Feng, Josef Kittler, William Christmas, Patrik Huber and\n  Xiao-Jun Wu", "title": "Dynamic Attention-controlled Cascaded Shape Regression Exploiting\n  Training Data Augmentation and Fuzzy-set Sample Weighting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Cascaded Shape Regression (CSR) architecture, namely Dynamic\nAttention-Controlled CSR (DAC-CSR), for robust facial landmark detection on\nunconstrained faces. Our DAC-CSR divides facial landmark detection into three\ncascaded sub-tasks: face bounding box refinement, general CSR and\nattention-controlled CSR. The first two stages refine initial face bounding\nboxes and output intermediate facial landmarks. Then, an online dynamic model\nselection method is used to choose appropriate domain-specific CSRs for further\nlandmark refinement. The key innovation of our DAC-CSR is the fault-tolerant\nmechanism, using fuzzy set sample weighting for attention-controlled\ndomain-specific model training. Moreover, we advocate data augmentation with a\nsimple but effective 2D profile face generator, and context-aware feature\nextraction for better facial feature representation. Experimental results\nobtained on challenging datasets demonstrate the merits of our DAC-CSR over the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 18:18:07 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 17:45:43 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Feng", "Zhen-Hua", ""], ["Kittler", "Josef", ""], ["Christmas", "William", ""], ["Huber", "Patrik", ""], ["Wu", "Xiao-Jun", ""]]}, {"id": "1611.05418", "submitter": "Anna Choromanska", "authors": "Mariusz Bojarski, Anna Choromanska, Krzysztof Choromanski, Bernhard\n  Firner, Larry Jackel, Urs Muller, Karol Zieba", "title": "VisualBackProp: efficient visualization of CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method, that we call VisualBackProp, for\nvisualizing which sets of pixels of the input image contribute most to the\npredictions made by the convolutional neural network (CNN). The method heavily\nhinges on exploring the intuition that the feature maps contain less and less\nirrelevant information to the prediction decision when moving deeper into the\nnetwork. The technique we propose was developed as a debugging tool for\nCNN-based systems for steering self-driving cars and is therefore required to\nrun in real-time, i.e. it was designed to require less computations than a\nforward propagation. This makes the presented visualization method a valuable\ndebugging tool which can be easily used during both training and inference. We\nfurthermore justify our approach with theoretical arguments and theoretically\nconfirm that the proposed method identifies sets of input pixels, rather than\nindividual pixels, that collaboratively contribute to the prediction. Our\ntheoretical findings stand in agreement with the experimental results. The\nempirical evaluation shows the plausibility of the proposed approach on the\nroad video data as well as in other applications and reveals that it compares\nfavorably to the layer-wise relevance propagation approach, i.e. it obtains\nsimilar visualization results and simultaneously achieves order of magnitude\nspeed-ups.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:45:54 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 13:58:46 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 23:50:46 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Bojarski", "Mariusz", ""], ["Choromanska", "Anna", ""], ["Choromanski", "Krzysztof", ""], ["Firner", "Bernhard", ""], ["Jackel", "Larry", ""], ["Muller", "Urs", ""], ["Zieba", "Karol", ""]]}, {"id": "1611.05424", "submitter": "Alejandro Newell", "authors": "Alejandro Newell, Zhiao Huang, Jia Deng", "title": "Associative Embedding: End-to-End Learning for Joint Detection and\n  Grouping", "comments": "Added results on MS-COCO and updated results on MPII", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce associative embedding, a novel method for supervising\nconvolutional neural networks for the task of detection and grouping. A number\nof computer vision problems can be framed in this manner including multi-person\npose estimation, instance segmentation, and multi-object tracking. Usually the\ngrouping of detections is achieved with multi-stage pipelines, instead we\npropose an approach that teaches a network to simultaneously output detections\nand group assignments. This technique can be easily integrated into any\nstate-of-the-art network architecture that produces pixel-wise predictions. We\nshow how to apply this method to both multi-person pose estimation and instance\nsegmentation and report state-of-the-art performance for multi-person pose on\nthe MPII and MS-COCO datasets.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:04:28 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 16:13:48 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Newell", "Alejandro", ""], ["Huang", "Zhiao", ""], ["Deng", "Jia", ""]]}, {"id": "1611.05431", "submitter": "Saining Xie", "authors": "Saining Xie, Ross Girshick, Piotr Doll\\'ar, Zhuowen Tu, Kaiming He", "title": "Aggregated Residual Transformations for Deep Neural Networks", "comments": "Accepted to CVPR 2017. Code and models:\n  https://github.com/facebookresearch/ResNeXt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple, highly modularized network architecture for image\nclassification. Our network is constructed by repeating a building block that\naggregates a set of transformations with the same topology. Our simple design\nresults in a homogeneous, multi-branch architecture that has only a few\nhyper-parameters to set. This strategy exposes a new dimension, which we call\n\"cardinality\" (the size of the set of transformations), as an essential factor\nin addition to the dimensions of depth and width. On the ImageNet-1K dataset,\nwe empirically show that even under the restricted condition of maintaining\ncomplexity, increasing cardinality is able to improve classification accuracy.\nMoreover, increasing cardinality is more effective than going deeper or wider\nwhen we increase the capacity. Our models, named ResNeXt, are the foundations\nof our entry to the ILSVRC 2016 classification task in which we secured 2nd\nplace. We further investigate ResNeXt on an ImageNet-5K set and the COCO\ndetection set, also showing better results than its ResNet counterpart. The\ncode and models are publicly available online.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:34:42 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 01:53:41 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Xie", "Saining", ""], ["Girshick", "Ross", ""], ["Doll\u00e1r", "Piotr", ""], ["Tu", "Zhuowen", ""], ["He", "Kaiming", ""]]}, {"id": "1611.05435", "submitter": "Mennatullah Siam M.S.", "authors": "Mennatullah Siam, Sepehr Valipour, Martin Jagersand, Nilanjan Ray", "title": "Convolutional Gated Recurrent Networks for Video Segmentation", "comments": "arXiv admin note: text overlap with arXiv:1606.00487", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation has recently witnessed major progress, where fully\nconvolutional neural networks have shown to perform well. However, most of the\nprevious work focused on improving single image segmentation. To our knowledge,\nno prior work has made use of temporal video information in a recurrent\nnetwork. In this paper, we introduce a novel approach to implicitly utilize\ntemporal data in videos for online semantic segmentation. The method relies on\na fully convolutional network that is embedded into a gated recurrent\narchitecture. This design receives a sequence of consecutive video frames and\noutputs the segmentation of the last frame. Convolutional gated recurrent\nnetworks are used for the recurrent part to preserve spatial connectivities in\nthe image. Our proposed method can be applied in both online and batch\nsegmentation. This architecture is tested for both binary and semantic video\nsegmentation tasks. Experiments are conducted on the recent benchmarks in\nSegTrack V2, Davis, CityScapes, and Synthia. Using recurrent fully\nconvolutional networks improved the baseline network performance in all of our\nexperiments. Namely, 5% and 3% improvement of F-measure in SegTrack2 and Davis\nrespectively, 5.7% improvement in mean IoU in Synthia and 3.5% improvement in\ncategorical mean IoU in CityScapes. The performance of the RFCN network depends\non its baseline fully convolutional network. Thus RFCN architecture can be seen\nas a method to improve its baseline segmentation network by exploiting\nspatiotemporal information in videos.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:46:38 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 21:47:17 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Siam", "Mennatullah", ""], ["Valipour", "Sepehr", ""], ["Jagersand", "Martin", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1611.05476", "submitter": "Takayuki Okatani", "authors": "Eisuke Ito and Takayuki Okatani", "title": "Self-calibration-based Approach to Critical Motion Sequences of\n  Rolling-shutter Structure from Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider critical motion sequences (CMSs) of rolling-shutter\n(RS) SfM. Employing an RS camera model with linearized pure rotation, we show\nthat the RS distortion can be approximately expressed by two internal\nparameters of an \"imaginary\" camera plus one-parameter nonlinear transformation\nsimilar to lens distortion. We then reformulate the problem as self-calibration\nof the imaginary camera, in which its skew and aspect ratio are unknown and\nvarying in the image sequence. In the formulation, we derive a general\nrepresentation of CMSs. We also show that our method can explain the CMS that\nwas recently reported in the literature, and then present a new remedy to deal\nwith the degeneracy. Our theoretical results agree well with experimental\nresults; it explains degeneracies observed when we employ naive bundle\nadjustment, and how they are resolved by our method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 21:50:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ito", "Eisuke", ""], ["Okatani", "Takayuki", ""]]}, {"id": "1611.05479", "submitter": "Anish Simhal", "authors": "Anish K. Simhal, Cecilia Aguerrebere, Forrest Collman, Joshua T.\n  Vogelstein, Kristina D. Micheva, Richard J. Weinberg, Stephen J. Smith,\n  Guillermo Sapiro", "title": "Probabilistic Fluorescence-Based Synapse Detection", "comments": "Current awaiting peer review", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005493", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain function results from communication between neurons connected by\ncomplex synaptic networks. Synapses are themselves highly complex and diverse\nsignaling machines, containing protein products of hundreds of different genes,\nsome in hundreds of copies, arranged in precise lattice at each individual\nsynapse. Synapses are fundamental not only to synaptic network function but\nalso to network development, adaptation, and memory. In addition, abnormalities\nof synapse numbers or molecular components are implicated in most mental and\nneurological disorders. Despite their obvious importance, mammalian synapse\npopulations have so far resisted detailed quantitative study. In human brains\nand most animal nervous systems, synapses are very small and very densely\npacked: there are approximately 1 billion synapses per cubic millimeter of\nhuman cortex. This volumetric density poses very substantial challenges to\nproteometric analysis at the critical level of the individual synapse. The\npresent work describes new probabilistic image analysis methods for\nsingle-synapse analysis of synapse populations in both animal and human brains.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:01:31 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Simhal", "Anish K.", ""], ["Aguerrebere", "Cecilia", ""], ["Collman", "Forrest", ""], ["Vogelstein", "Joshua T.", ""], ["Micheva", "Kristina D.", ""], ["Weinberg", "Richard J.", ""], ["Smith", "Stephen J.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1611.05490", "submitter": "Feng Liu", "authors": "Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun", "title": "Semantic Regularisation for Recurrent Image Annotation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"CNN-RNN\" design pattern is increasingly widely applied in a variety of\nimage annotation tasks including multi-label classification and captioning.\nExisting models use the weakly semantic CNN hidden layer or its transform as\nthe image embedding that provides the interface between the CNN and RNN. This\nleaves the RNN overstretched with two jobs: predicting the visual concepts and\nmodelling their correlations for generating structured annotation output.\nImportantly this makes the end-to-end training of the CNN and RNN slow and\nineffective due to the difficulty of back propagating gradients through the RNN\nto train the CNN. We propose a simple modification to the design pattern that\nmakes learning more effective and efficient. Specifically, we propose to use a\nsemantically regularised embedding layer as the interface between the CNN and\nRNN. Regularising the interface can partially or completely decouple the\nlearning problems, allowing each to be more effectively trained and jointly\ntraining much more efficient. Extensive experiments show that state-of-the art\nperformance is achieved on multi-label classification as well as image\ncaptioning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 22:39:59 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Liu", "Feng", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""]]}, {"id": "1611.05503", "submitter": "Yu Liu", "authors": "Yu Liu, Yanming Guo, and Michael S. Lew", "title": "On the Exploration of Convolutional Fusion Networks for Visual\n  Recognition", "comments": "23rd International Conference on MultiMedia Modeling (MMM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent advances in multi-scale deep representations, their\nlimitations are attributed to expensive parameters and weak fusion modules.\nHence, we propose an efficient approach to fuse multi-scale deep\nrepresentations, called convolutional fusion networks (CFN). Owing to using\n1$\\times$1 convolution and global average pooling, CFN can efficiently generate\nthe side branches while adding few parameters. In addition, we present a\nlocally-connected fusion module, which can learn adaptive weights for the side\nbranches and form a discriminatively fused feature. CFN models trained on the\nCIFAR and ImageNet datasets demonstrate remarkable improvements over the plain\nCNNs. Furthermore, we generalize CFN to three new tasks, including scene\nrecognition, fine-grained recognition and image retrieval. Our experiments show\nthat it can obtain consistent improvements towards the transferring tasks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 23:33:38 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Liu", "Yu", ""], ["Guo", "Yanming", ""], ["Lew", "Michael S.", ""]]}, {"id": "1611.05507", "submitter": "Paul Upchurch", "authors": "Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah\n  Snavely, Kavita Bala, Kilian Weinberger", "title": "Deep Feature Interpolation for Image Content Changes", "comments": "First two authors contributed equally. Accepted by CVPR 2017. Code at\n  https://github.com/paulu/deepfeatinterp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Deep Feature Interpolation (DFI), a new data-driven baseline for\nautomatic high-resolution image transformation. As the name suggests, it relies\nonly on simple linear interpolation of deep convolutional features from\npre-trained convnets. We show that despite its simplicity, DFI can perform\nhigh-level semantic transformations like \"make older/younger\", \"make\nbespectacled\", \"add smile\", among others, surprisingly well - sometimes even\nmatching or outperforming the state-of-the-art. This is particularly unexpected\nas DFI requires no specialized network architecture or even any deep network to\nbe trained for these tasks. DFI therefore can be used as a new baseline to\nevaluate more complex algorithms and provides a practical answer to the\nquestion of which image transformation tasks are still challenging in the rise\nof deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 23:56:27 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 17:20:31 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Upchurch", "Paul", ""], ["Gardner", "Jacob", ""], ["Pleiss", "Geoff", ""], ["Pless", "Robert", ""], ["Snavely", "Noah", ""], ["Bala", "Kavita", ""], ["Weinberger", "Kilian", ""]]}, {"id": "1611.05520", "submitter": "Mohammad Sadegh Aliakbarian", "authors": "Mohammad Sadegh Aliakbarian, Fatemehsadat Saleh, Basura Fernando,\n  Mathieu Salzmann, Lars Petersson, Lars Andersson", "title": "Deep Action- and Context-Aware Sequence Learning for Activity\n  Recognition and Anticipation", "comments": "10 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition and anticipation are key to the success of many computer\nvision applications. Existing methods can roughly be grouped into those that\nextract global, context-aware representations of the entire image or sequence,\nand those that aim at focusing on the regions where the action occurs. While\nthe former may suffer from the fact that context is not always reliable, the\nlatter completely ignore this source of information, which can nonetheless be\nhelpful in many situations. In this paper, we aim at making the best of both\nworlds by developing an approach that leverages both context-aware and\naction-aware features. At the core of our method lies a novel multi-stage\nrecurrent architecture that allows us to effectively combine these two sources\nof information throughout a video. This architecture first exploits the global,\ncontext-aware features, and merges the resulting representation with the\nlocalized, action-aware ones. Our experiments on standard datasets evidence the\nbenefits of our approach over methods that use each information type\nseparately. We outperform the state-of-the-art methods that, as us, rely only\non RGB frames as input for both action recognition and anticipation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 01:08:56 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 01:41:40 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Aliakbarian", "Mohammad Sadegh", ""], ["Saleh", "Fatemehsadat", ""], ["Fernando", "Basura", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Andersson", "Lars", ""]]}, {"id": "1611.05546", "submitter": "Damien Teney", "authors": "Damien Teney, Anton van den Hengel", "title": "Zero-Shot Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:21:00 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 21:51:24 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Teney", "Damien", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1611.05552", "submitter": "Jason Kuen", "authors": "Jason Kuen, Xiangfei Kong, Gang Wang, Yap-Peng Tan", "title": "DelugeNets: Deep Networks with Efficient and Flexible Cross-layer\n  Information Inflows", "comments": "Code: https://github.com/xternalz/DelugeNets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deluge Networks (DelugeNets) are deep neural networks which efficiently\nfacilitate massive cross-layer information inflows from preceding layers to\nsucceeding layers. The connections between layers in DelugeNets are established\nthrough cross-layer depthwise convolutional layers with learnable filters,\nacting as a flexible yet efficient selection mechanism. DelugeNets can\npropagate information across many layers with greater flexibility and utilize\nnetwork parameters more effectively compared to ResNets, whilst being more\nefficient than DenseNets. Remarkably, a DelugeNet model with just model\ncomplexity of 4.31 GigaFLOPs and 20.2M network parameters, achieve\nclassification errors of 3.76% and 19.02% on CIFAR-10 and CIFAR-100 dataset\nrespectively. Moreover, DelugeNet-122 performs competitively to ResNet-200 on\nImageNet dataset, despite costing merely half of the computations needed by the\nlatter.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:45:48 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 07:41:33 GMT"}, {"version": "v3", "created": "Wed, 28 Dec 2016 02:08:45 GMT"}, {"version": "v4", "created": "Fri, 30 Dec 2016 04:56:02 GMT"}, {"version": "v5", "created": "Wed, 23 Aug 2017 14:09:55 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Kuen", "Jason", ""], ["Kong", "Xiangfei", ""], ["Wang", "Gang", ""], ["Tan", "Yap-Peng", ""]]}, {"id": "1611.05588", "submitter": "Yan Huang", "authors": "Yan Huang, Wei Wang, Liang Wang", "title": "Instance-aware Image and Sentence Matching with Selective Multimodal\n  LSTM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective image and sentence matching depends on how to well measure their\nglobal visual-semantic similarity. Based on the observation that such a global\nsimilarity arises from a complex aggregation of multiple local similarities\nbetween pairwise instances of image (objects) and sentence (words), we propose\na selective multimodal Long Short-Term Memory network (sm-LSTM) for\ninstance-aware image and sentence matching. The sm-LSTM includes a multimodal\ncontext-modulated attention scheme at each timestep that can selectively attend\nto a pair of instances of image and sentence, by predicting pairwise\ninstance-aware saliency maps for image and sentence. For selected pairwise\ninstances, their representations are obtained based on the predicted saliency\nmaps, and then compared to measure their local similarity. By similarly\nmeasuring multiple local similarities within a few timesteps, the sm-LSTM\nsequentially aggregates them with hidden states to obtain a final matching\nscore as the desired global similarity. Extensive experiments show that our\nmodel can well match image and sentence with complex content, and achieve the\nstate-of-the-art results on two public benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 06:57:01 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Huang", "Yan", ""], ["Wang", "Wei", ""], ["Wang", "Liang", ""]]}, {"id": "1611.05592", "submitter": "Junbo Wang", "authors": "Junbo Wang, Wei Wang, Yan Huang, Liang Wang, Tieniu Tan", "title": "Multimodal Memory Modelling for Video Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video captioning which automatically translates video clips into natural\nlanguage sentences is a very important task in computer vision. By virtue of\nrecent deep learning technologies, e.g., convolutional neural networks (CNNs)\nand recurrent neural networks (RNNs), video captioning has made great progress.\nHowever, learning an effective mapping from visual sequence space to language\nspace is still a challenging problem. In this paper, we propose a Multimodal\nMemory Model (M3) to describe videos, which builds a visual and textual shared\nmemory to model the long-term visual-textual dependency and further guide\nglobal visual attention on described targets. Specifically, the proposed M3\nattaches an external memory to store and retrieve both visual and textual\ncontents by interacting with video and sentence with multiple read and write\noperations. First, text representation in the Long Short-Term Memory (LSTM)\nbased text decoder is written into the memory, and the memory contents will be\nread out to guide an attention to select related visual targets. Then, the\nselected visual information is written into the memory, which will be further\nread out to the text decoder. To evaluate the proposed model, we perform\nexperiments on two publicly benchmark datasets: MSVD and MSR-VTT. The\nexperimental results demonstrate that our method outperforms the\nstate-of-theart methods in terms of BLEU and METEOR.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 07:24:03 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wang", "Junbo", ""], ["Wang", "Wei", ""], ["Huang", "Yan", ""], ["Wang", "Liang", ""], ["Tan", "Tieniu", ""]]}, {"id": "1611.05594", "submitter": "Long Chen", "authors": "Long Chen, Hanwang Zhang, Jun Xiao, Liqiang Nie, Jian Shao, Wei Liu,\n  Tat-Seng Chua", "title": "SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks\n  for Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention has been successfully applied in structural prediction tasks\nsuch as visual captioning and question answering. Existing visual attention\nmodels are generally spatial, i.e., the attention is modeled as spatial\nprobabilities that re-weight the last conv-layer feature map of a CNN encoding\nan input image. However, we argue that such spatial attention does not\nnecessarily conform to the attention mechanism --- a dynamic feature extractor\nthat combines contextual fixations over time, as CNN features are naturally\nspatial, channel-wise and multi-layer. In this paper, we introduce a novel\nconvolutional neural network dubbed SCA-CNN that incorporates Spatial and\nChannel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN\ndynamically modulates the sentence generation context in multi-layer feature\nmaps, encoding where (i.e., attentive spatial locations at multiple layers) and\nwhat (i.e., attentive channels) the visual attention is. We evaluate the\nproposed SCA-CNN architecture on three benchmark image captioning datasets:\nFlickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN\nsignificantly outperforms state-of-the-art visual attention-based image\ncaptioning methods.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 07:39:46 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 05:48:44 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Chen", "Long", ""], ["Zhang", "Hanwang", ""], ["Xiao", "Jun", ""], ["Nie", "Liqiang", ""], ["Shao", "Jian", ""], ["Liu", "Wei", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1611.05603", "submitter": "Kai Yu", "authors": "Kai Yu, Biao Leng, Zhang Zhang, Dangwei Li, Kaiqi Huang", "title": "Weakly-supervised Learning of Mid-level Features for Pedestrian\n  Attribute Recognition and Localization", "comments": "Containing 9 pages and 5 figures. Codes open-sourced on\n  https://github.com/kyu-sz/WPAL-network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods treat pedestrian attribute recognition as a\nmulti-label image classification problem. The location information of person\nattributes is usually eliminated or simply encoded in the rigid splitting of\nwhole body in previous work. In this paper, we formulate the task in a\nweakly-supervised attribute localization framework. Based on GoogLeNet,\nfirstly, a set of mid-level attribute features are discovered by novelly\ndesigned detection layers, where a max-pooling based weakly-supervised object\ndetection technique is used to train these layers with only image-level labels\nwithout the need of bounding box annotations of pedestrian attributes.\nSecondly, attribute labels are predicted by regression of the detection\nresponse magnitudes. Finally, the locations and rough shapes of pedestrian\nattributes can be inferred by performing clustering on a fusion of activation\nmaps of the detection layers, where the fusion weights are estimated as the\ncorrelation strengths between each attribute and its relevant mid-level\nfeatures. Extensive experiments are performed on the two currently largest\npedestrian attribute datasets, i.e. the PETA dataset and the RAP dataset.\nResults show that the proposed method has achieved competitive performance on\nattribute recognition, compared to other state-of-the-art methods. Moreover,\nthe results of attribute localization are visualized to understand the\ncharacteristics of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 08:20:23 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Yu", "Kai", ""], ["Leng", "Biao", ""], ["Zhang", "Zhang", ""], ["Li", "Dangwei", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1611.05607", "submitter": "Tal Schuster", "authors": "Tal Schuster, Lior Wolf and David Gadot", "title": "Optical Flow Requires Multiple Strategies (but only one network)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the matching problem that underlies optical flow requires\nmultiple strategies, depending on the amount of image motion and other factors.\nWe then study the implications of this observation on training a deep neural\nnetwork for representing image patches in the context of descriptor based\noptical flow. We propose a metric learning method, which selects suitable\nnegative samples based on the nature of the true match. This type of training\nproduces a network that displays multiple strategies depending on the input and\nleads to state of the art results on the KITTI 2012 and KITTI 2015 optical flow\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 08:31:56 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 22:37:31 GMT"}, {"version": "v3", "created": "Thu, 2 Feb 2017 10:52:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Schuster", "Tal", ""], ["Wolf", "Lior", ""], ["Gadot", "David", ""]]}, {"id": "1611.05644", "submitter": "Antonia Creswell", "authors": "Antonia Creswell and Anil Anthony Bharath", "title": "Inverting The Generator Of A Generative Adversarial Network", "comments": "Accepted at NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) learn to synthesise new samples from a\nhigh-dimensional distribution by passing samples drawn from a latent space\nthrough a generative network. When the high-dimensional distribution describes\nimages of a particular data set, the network should learn to generate visually\nsimilar image samples for latent variables that are close to each other in the\nlatent space. For tasks such as image retrieval and image classification, it\nmay be useful to exploit the arrangement of the latent space by projecting\nimages into it, and using this as a representation for discriminative tasks.\nGANs often consist of multiple layers of non-linear computations, making them\nvery difficult to invert. This paper introduces techniques for projecting image\nsamples into the latent space using any pre-trained GAN, provided that the\ncomputational graph is available. We evaluate these techniques on both MNIST\ndigits and Omniglot handwritten characters. In the case of MNIST digits, we\nshow that projections into the latent space maintain information about the\nstyle and the identity of the digit. In the case of Omniglot characters, we\nshow that even characters from alphabets that have not been seen during\ntraining may be projected well into the latent space; this suggests that this\napproach may have applications in one-shot learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 11:55:16 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Creswell", "Antonia", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1611.05664", "submitter": "Bastien Moysset", "authors": "Bastien Moysset, Christoper Kermorvant and Christian Wolf", "title": "Learning to detect and localize many objects from few examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend in object detection and localization is to learn\npredictions with high capacity deep neural networks trained on a very large\namount of annotated data and using a high amount of processing power. In this\nwork, we propose a new neural model which directly predicts bounding box\ncoordinates. The particularity of our contribution lies in the local\ncomputations of predictions with a new form of local parameter sharing which\nkeeps the overall amount of trainable parameters low. Key components of the\nmodel are spatial 2D-LSTM recurrent layers which convey contextual information\nbetween the regions of the image. We show that this model is more powerful than\nthe state of the art in applications where training data is not as abundant as\nin the classical configuration of natural images and Imagenet/Pascal VOC tasks.\nWe particularly target the detection of text in document images, but our method\nis not limited to this setting. The proposed model also facilitates the\ndetection of many objects in a single image and can deal with inputs of\nvariable sizes without resizing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 12:51:18 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Moysset", "Bastien", ""], ["Kermorvant", "Christoper", ""], ["Wolf", "Christian", ""]]}, {"id": "1611.05666", "submitter": "Zhedong Zheng", "authors": "Zhedong Zheng, Liang Zheng, Yi Yang", "title": "A Discriminatively Learned CNN Embedding for Person Re-identification", "comments": null, "journal-ref": null, "doi": "10.1145/3159171", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit two popular convolutional neural networks (CNN) in person\nre-identification (re-ID), i.e, verification and classification models. The two\nmodels have their respective advantages and limitations due to different loss\nfunctions. In this paper, we shed light on how to combine the two models to\nlearn more discriminative pedestrian descriptors. Specifically, we propose a\nnew siamese network that simultaneously computes identification loss and\nverification loss. Given a pair of training images, the network predicts the\nidentities of the two images and whether they belong to the same identity. Our\nnetwork learns a discriminative embedding and a similarity measurement at the\nsame time, thus making full usage of the annotations. Albeit simple, the\nlearned embedding improves the state-of-the-art performance on two public\nperson re-ID benchmarks. Further, we show our architecture can also be applied\nin image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 12:59:22 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 05:53:00 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Yang", "Yi", ""]]}, {"id": "1611.05689", "submitter": "Andrey Kuzmin Andrey Kuzmin", "authors": "Andrey Kuzmin, Dmitry Mikushin, Victor Lempitsky", "title": "End-to-end Learning of Cost-Volume Aggregation for Real-time Dense\n  Stereo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep learning-based approach for dense stereo matching.\nCompared to previous works, our approach does not use deep learning of pixel\nappearance descriptors, employing very fast classical matching scores instead.\nAt the same time, our approach uses a deep convolutional network to predict the\nlocal parameters of cost volume aggregation process, which in this paper we\nimplement using differentiable domain transform. By treating such transform as\na recurrent neural network, we are able to train our whole system that includes\ncost volume computation, cost-volume aggregation (smoothing), and\nwinner-takes-all disparity selection end-to-end. The resulting method is highly\nefficient at test time, while achieving good matching accuracy. On the KITTI\n2015 benchmark, it achieves a result of 6.34\\% error rate while running at 29\nframes per second rate on a modern GPU.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:14:02 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kuzmin", "Andrey", ""], ["Mikushin", "Dmitry", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1611.05705", "submitter": "Eric Brachmann", "authors": "Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton,\n  Frank Michel, Stefan Gumhold, Carsten Rother", "title": "DSAC - Differentiable RANSAC for Camera Localization", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RANSAC is an important algorithm in robust optimization and a central\nbuilding block for many computer vision applications. In recent years,\ntraditionally hand-crafted pipelines have been replaced by deep learning\npipelines, which can be trained in an end-to-end fashion. However, RANSAC has\nso far not been used as part of such deep learning pipelines, because its\nhypothesis selection procedure is non-differentiable. In this work, we present\ntwo different ways to overcome this limitation. The most promising approach is\ninspired by reinforcement learning, namely to replace the deterministic\nhypothesis selection by a probabilistic selection for which we can derive the\nexpected loss w.r.t. to all learnable parameters. We call this approach DSAC,\nthe differentiable counterpart of RANSAC. We apply DSAC to the problem of\ncamera localization, where deep learning has so far failed to improve on\ntraditional approaches. We demonstrate that by directly minimizing the expected\nloss of the output camera poses, robustly estimated by RANSAC, we achieve an\nincrease in accuracy. In the future, any deep learning pipeline can use DSAC as\na robust optimization component.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:39:53 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 16:28:45 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 16:14:45 GMT"}, {"version": "v4", "created": "Wed, 21 Mar 2018 13:16:09 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Brachmann", "Eric", ""], ["Krull", "Alexander", ""], ["Nowozin", "Sebastian", ""], ["Shotton", "Jamie", ""], ["Michel", "Frank", ""], ["Gumhold", "Stefan", ""], ["Rother", "Carsten", ""]]}, {"id": "1611.05708", "submitter": "Bugra Tekin", "authors": "Bugra Tekin, Pablo M\\'arquez-Neila, Mathieu Salzmann, Pascal Fua", "title": "Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most recent approaches to monocular 3D human pose estimation rely on Deep\nLearning. They typically involve regressing from an image to either 3D joint\ncoordinates directly or 2D joint locations from which 3D coordinates are\ninferred. Both approaches have their strengths and weaknesses and we therefore\npropose a novel architecture designed to deliver the best of both worlds by\nperforming both simultaneously and fusing the information along the way. At the\nheart of our framework is a trainable fusion scheme that learns how to fuse the\ninformation optimally instead of being hand-designed. This yields significant\nimprovements upon the state-of-the-art on standard 3D human pose estimation\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:40:53 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2016 16:25:27 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 10:49:00 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Tekin", "Bugra", ""], ["M\u00e1rquez-Neila", "Pablo", ""], ["Salzmann", "Mathieu", ""], ["Fua", "Pascal", ""]]}, {"id": "1611.05709", "submitter": "Yanghao Li", "authors": "Yanghao Li, Naiyan Wang, Jiaying Liu and Xiaodi Hou", "title": "Factorized Bilinear Models for Image Recognition", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Deep Convolutional Neural Networks (CNNs) have liberated their power\nin various computer vision tasks, the most important components of CNN,\nconvolutional layers and fully connected layers, are still limited to linear\ntransformations. In this paper, we propose a novel Factorized Bilinear (FB)\nlayer to model the pairwise feature interactions by considering the quadratic\nterms in the transformations. Compared with existing methods that tried to\nincorporate complex non-linearity structures into CNNs, the factorized\nparameterization makes our FB layer only require a linear increase of\nparameters and affordable computational cost. To further reduce the risk of\noverfitting of the FB layer, a specific remedy called DropFactor is devised\nduring the training process. We also analyze the connection between FB layer\nand some existing models, and show FB layer is a generalization to them.\nFinally, we validate the effectiveness of FB layer on several widely adopted\ndatasets including CIFAR-10, CIFAR-100 and ImageNet, and demonstrate superior\nresults compared with various state-of-the-art deep models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:40:57 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 08:14:01 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Yanghao", ""], ["Wang", "Naiyan", ""], ["Liu", "Jiaying", ""], ["Hou", "Xiaodi", ""]]}, {"id": "1611.05720", "submitter": "Yuhui Yuan", "authors": "Yuhui Yuan, Kuiyuan Yang, Chao Zhang", "title": "Hard-Aware Deeply Cascaded Embedding", "comments": "accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riding on the waves of deep neural networks, deep metric learning has also\nachieved promising results in various tasks using triplet network or Siamese\nnetwork. Though the basic goal of making images from the same category closer\nthan the ones from different categories is intuitive, it is hard to directly\noptimize due to the quadratic or cubic sample size. To solve the problem, hard\nexample mining which only focuses on a subset of samples that are considered\nhard is widely used. However, hard is defined relative to a model, where\ncomplex models treat most samples as easy ones and vice versa for simple\nmodels, and both are not good for training. Samples are also with different\nhard levels, it is hard to define a model with the just right complexity and\nchoose hard examples adequately. This motivates us to ensemble a set of models\nwith different complexities in cascaded manner and mine hard examples\nadaptively, a sample is judged by a series of models with increasing\ncomplexities and only updates models that consider the sample as a hard case.\nWe evaluate our method on CARS196, CUB-200-2011, Stanford Online Products,\nVehicleID and DeepFashion datasets. Our method outperforms state-of-the-art\nmethods by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:54:33 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 07:22:25 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Yuan", "Yuhui", ""], ["Yang", "Kuiyuan", ""], ["Zhang", "Chao", ""]]}, {"id": "1611.05725", "submitter": "Xingcheng Zhang", "authors": "Xingcheng Zhang, Zhizhong Li, Chen Change Loy, Dahua Lin", "title": "PolyNet: A Pursuit of Structural Diversity in Very Deep Networks", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of studies have shown that increasing the depth or width of\nconvolutional networks is a rewarding approach to improve the performance of\nimage recognition. In our study, however, we observed difficulties along both\ndirections. On one hand, the pursuit for very deep networks is met with a\ndiminishing return and increased training difficulty; on the other hand,\nwidening a network would result in a quadratic growth in both computational\ncost and memory demand. These difficulties motivate us to explore structural\ndiversity in designing deep networks, a new dimension beyond just depth and\nwidth. Specifically, we present a new family of modules, namely the\nPolyInception, which can be flexibly inserted in isolation or in a composition\nas replacements of different parts of a network. Choosing PolyInception modules\nwith the guidance of architectural efficiency can improve the expressive power\nwhile preserving comparable computational cost. The Very Deep PolyNet, designed\nfollowing this direction, demonstrates substantial improvements over the\nstate-of-the-art on the ILSVRC 2012 benchmark. Compared to Inception-ResNet-v2,\nit reduces the top-5 validation error on single crops from 4.9% to 4.25%, and\nthat on multi-crops from 3.7% to 3.45%.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 15:00:42 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 05:18:01 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zhang", "Xingcheng", ""], ["Li", "Zhizhong", ""], ["Loy", "Chen Change", ""], ["Lin", "Dahua", ""]]}, {"id": "1611.05742", "submitter": "Zhiwu Huang", "authors": "Zhiwu Huang, Jiqing Wu, Luc Van Gool", "title": "Building Deep Networks on Grassmann Manifolds", "comments": "AAAI'18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representations on Grassmann manifolds is popular in quite a few\nvisual recognition tasks. In order to enable deep learning on Grassmann\nmanifolds, this paper proposes a deep network architecture by generalizing the\nEuclidean network paradigm to Grassmann manifolds. In particular, we design\nfull rank mapping layers to transform input Grassmannian data to more desirable\nones, exploit re-orthonormalization layers to normalize the resulting matrices,\nstudy projection pooling layers to reduce the model complexity in the\nGrassmannian context, and devise projection mapping layers to respect\nGrassmannian geometry and meanwhile achieve Euclidean forms for regular output\nlayers. To train the Grassmann networks, we exploit a stochastic gradient\ndescent setting on manifolds of the connection weights, and study a matrix\ngeneralization of backpropagation to update the structured data. The\nevaluations on three visual recognition tasks show that our Grassmann networks\nhave clear advantages over existing Grassmann learning methods, and achieve\nresults comparable with state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 15:37:23 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 07:57:48 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 14:18:04 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Huang", "Zhiwu", ""], ["Wu", "Jiqing", ""], ["Van Gool", "Luc", ""]]}, {"id": "1611.05744", "submitter": "Lokesh Boominathan", "authors": "Lokesh Boominathan, Suraj Srinivas, R. Venkatesh Babu", "title": "Compensating for Large In-Plane Rotations in Natural Images", "comments": "Accepted at Indian Conference on Computer Vision, Graphics and Image\n  Processing (ICVGIP) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rotation invariance has been studied in the computer vision community\nprimarily in the context of small in-plane rotations. This is usually achieved\nby building invariant image features. However, the problem of achieving\ninvariance for large rotation angles remains largely unexplored. In this work,\nwe tackle this problem by directly compensating for large rotations, as opposed\nto building invariant features. This is inspired by the neuro-scientific\nconcept of mental rotation, which humans use to compare pairs of rotated\nobjects. Our contributions here are three-fold. First, we train a Convolutional\nNeural Network (CNN) to detect image rotations. We find that generic CNN\narchitectures are not suitable for this purpose. To this end, we introduce a\nconvolutional template layer, which learns representations for canonical\n'unrotated' images. Second, we use Bayesian Optimization to quickly sift\nthrough a large number of candidate images to find the canonical 'unrotated'\nimage. Third, we use this method to achieve robustness to large angles in an\nimage retrieval scenario. Our method is task-agnostic, and can be used as a\npre-processing step in any computer vision system.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 15:50:36 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Boominathan", "Lokesh", ""], ["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1611.05755", "submitter": "Alan Godoy", "authors": "Guilherme Folego, Marcus A. Angeloni, Jos\\'e Augusto Stuchi, Alan\n  Godoy, Anderson Rocha", "title": "Cross-Domain Face Verification: Matching ID Document and Self-Portrait\n  Photographs", "comments": "XII WORKSHOP DE VIS\\~AO COMPUTACIONAL (Campo Grande, Brazil). In XII\n  Workshop de Vis\\~ao Computacional (pp. 311-316) (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-domain biometrics has been emerging as a new necessity, which poses\nseveral additional challenges, including harsh illumination changes, noise,\npose variation, among others. In this paper, we explore approaches to\ncross-domain face verification, comparing self-portrait photographs (\"selfies\")\nto ID documents. We approach the problem with proper image photometric\nadjustment and data standardization techniques, along with deep learning\nmethods to extract the most prominent features from the data, reducing the\neffects of domain shift in this problem. We validate the methods using a novel\ndataset comprising 50 individuals. The obtained results are promising and\nindicate that the adopted path is worth further investigation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:05:11 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Folego", "Guilherme", ""], ["Angeloni", "Marcus A.", ""], ["Stuchi", "Jos\u00e9 Augusto", ""], ["Godoy", "Alan", ""], ["Rocha", "Anderson", ""]]}, {"id": "1611.05760", "submitter": "Ayan Chakrabarti", "authors": "Igor Vasiljevic, Ayan Chakrabarti, Gregory Shakhnarovich", "title": "Examining the Impact of Blur on Recognition by Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art algorithms for many semantic visual tasks are based on the\nuse of convolutional neural networks. These networks are commonly trained, and\nevaluated, on large annotated datasets of artifact-free high-quality images. In\nthis paper, we investigate the effect of one such artifact that is quite common\nin natural capture settings: optical blur. We show that standard network\nmodels, trained only on high-quality images, suffer a significant degradation\nin performance when applied to those degraded by blur due to defocus, or\nsubject or camera motion. We investigate the extent to which this degradation\nis due to the mismatch between training and input image statistics.\nSpecifically, we find that fine-tuning a pre-trained model with blurred images\nadded to the training set allows it to regain much of the lost accuracy. We\nalso show that there is a fair amount of generalization between different\ndegrees and types of blur, which implies that a single network model can be\nused robustly for recognition when the nature of the blur in the input is\nunknown. We find that this robustness arises as a result of these models\nlearning to generate blur invariant representations in their hidden layers. Our\nfindings provide useful insights towards developing vision systems that can\nperform reliably on real world images affected by blur.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:17:10 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 13:56:22 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Vasiljevic", "Igor", ""], ["Chakrabarti", "Ayan", ""], ["Shakhnarovich", "Gregory", ""]]}, {"id": "1611.05777", "submitter": "Hamid Reza Hassanzadeh", "authors": "Hamid Reza Hassanzadeh, May D. Wang", "title": "DeeperBind: Enhancing Prediction of Sequence Specificities of DNA\n  Binding Proteins", "comments": "in 2016 IEEE International Conference on Bioinformatics and\n  Biomedicine (BIBM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcription factors (TFs) are macromolecules that bind to\n\\textit{cis}-regulatory specific sub-regions of DNA promoters and initiate\ntranscription. Finding the exact location of these binding sites (aka motifs)\nis important in a variety of domains such as drug design and development. To\naddress this need, several \\textit{in vivo} and \\textit{in vitro} techniques\nhave been developed so far that try to characterize and predict the binding\nspecificity of a protein to different DNA loci. The major problem with these\ntechniques is that they are not accurate enough in prediction of the binding\naffinity and characterization of the corresponding motifs. As a result,\ndownstream analysis is required to uncover the locations where proteins of\ninterest bind. Here, we propose DeeperBind, a long short term recurrent\nconvolutional network for prediction of protein binding specificities with\nrespect to DNA probes. DeeperBind can model the positional dynamics of probe\nsequences and hence reckons with the contributions made by individual\nsub-regions in DNA sequences, in an effective way. Moreover, it can be trained\nand tested on datasets containing varying-length sequences. We apply our\npipeline to the datasets derived from protein binding microarrays (PBMs), an\nin-vitro high-throughput technology for quantification of protein-DNA binding\npreferences, and present promising results. To the best of our knowledge, this\nis the most accurate pipeline that can predict binding specificities of DNA\nsequences from the data produced by high-throughput technologies through\nutilization of the power of deep learning for feature generation and positional\ndynamics modeling.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:52:41 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Hassanzadeh", "Hamid Reza", ""], ["Wang", "May D.", ""]]}, {"id": "1611.05799", "submitter": "Nichola Abdo", "authors": "Philipp Jund, Nichola Abdo, Andreas Eitel, Wolfram Burgard", "title": "The Freiburg Groceries Dataset", "comments": "Link to dataset:\n  http://www2.informatik.uni-freiburg.de/~eitel/freiburg_groceries_dataset.html\n  Link to code: https://github.com/PhilJd/freiburg_groceries_dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing performance of machine learning techniques in the last\nfew years, the computer vision and robotics communities have created a large\nnumber of datasets for benchmarking object recognition tasks. These datasets\ncover a large spectrum of natural images and object categories, making them not\nonly useful as a testbed for comparing machine learning approaches, but also a\ngreat resource for bootstrapping different domain-specific perception and\nrobotic systems. One such domain is domestic environments, where an autonomous\nrobot has to recognize a large variety of everyday objects such as groceries.\nThis is a challenging task due to the large variety of objects and products,\nand where there is great need for real-world training data that goes beyond\nproduct images available online. In this paper, we address this issue and\npresent a dataset consisting of 5,000 images covering 25 different classes of\ngroceries, with at least 97 images per class. We collected all images from\nreal-world settings at different stores and apartments. In contrast to existing\ngroceries datasets, our dataset includes a large variety of perspectives,\nlighting conditions, and degrees of clutter. Overall, our images contain\nthousands of different object instances. It is our hope that machine learning\nand robotics researchers find this dataset of use for training, testing, and\nbootstrapping their approaches. As a baseline classifier to facilitate\ncomparison, we re-trained the CaffeNet architecture (an adaptation of the\nwell-known AlexNet) on our dataset and achieved a mean accuracy of 78.9%. We\nrelease this trained model along with the code and data splits we used in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 17:35:21 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Jund", "Philipp", ""], ["Abdo", "Nichola", ""], ["Eitel", "Andreas", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1611.05837", "submitter": "Shenlong Wang", "authors": "Shenlong Wang, Linjie Luo, Ning Zhang, Jia Li", "title": "AutoScaler: Scale-Attention Networks for Visual Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Finding visual correspondence between local features is key to many computer\nvision problems. While defining features with larger contextual scales usually\nimplies greater discriminativeness, it could also lead to less spatial accuracy\nof the features. We propose AutoScaler, a scale-attention network to explicitly\noptimize this trade-off in visual correspondence tasks. Our network consists of\na weight-sharing feature network to compute multi-scale feature maps and an\nattention network to combine them optimally in the scale space. This allows our\nnetwork to have adaptive receptive field sizes over different scales of the\ninput. The entire network is trained end-to-end in a siamese framework for\nvisual correspondence tasks. Our method achieves favorable results compared to\nstate-of-the-art methods on challenging optical flow and semantic matching\nbenchmarks, including Sintel, KITTI and CUB-2011. We also show that our method\ncan generalize to improve hand-crafted descriptors (e.g Daisy) on general\nvisual correspondence tasks. Finally, our attention network can generate\nvisually interpretable scale attention maps.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 20:01:05 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wang", "Shenlong", ""], ["Luo", "Linjie", ""], ["Zhang", "Ning", ""], ["Li", "Jia", ""]]}, {"id": "1611.05842", "submitter": "Dilip K. Prasad", "authors": "D. K. Prasad, D. Rajan, L. Rachmawati, E. Rajabaly, C. Quek", "title": "Video Processing from Electro-optical Sensors for Object Detection and\n  Tracking in Maritime Environment: A Survey", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a survey on maritime object detection and tracking approaches,\nwhich are essential for the development of a navigational system for autonomous\nships. The electro-optical (EO) sensor considered here is a video camera that\noperates in the visible or the infrared spectra, which conventionally\ncomplement radar and sonar and have demonstrated effectiveness for situational\nawareness at sea has demonstrated its effectiveness over the last few years.\nThis paper provides a comprehensive overview of various approaches of video\nprocessing for object detection and tracking in the maritime environment. We\nfollow an approach-based taxonomy wherein the advantages and limitations of\neach approach are compared. The object detection system consists of the\nfollowing modules: horizon detection, static background subtraction and\nforeground segmentation. Each of these has been studied extensively in maritime\nsituations and has been shown to be challenging due to the presence of\nbackground motion especially due to waves and wakes. The main processes\ninvolved in object tracking include video frame registration, dynamic\nbackground subtraction, and the object tracking algorithm itself. The\nchallenges for robust tracking arise due to camera motion, dynamic background\nand low contrast of tracked object, possibly due to environmental degradation.\nThe survey also discusses multisensor approaches and commercial maritime\nsystems that use EO sensors. The survey also highlights methods from computer\nvision research which hold promise to perform well in maritime EO data\nprocessing. Performance of several maritime and computer vision techniques is\nevaluated on newly proposed Singapore Maritime Dataset.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 20:11:51 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Prasad", "D. K.", ""], ["Rajan", "D.", ""], ["Rachmawati", "L.", ""], ["Rajabaly", "E.", ""], ["Quek", "C.", ""]]}, {"id": "1611.05896", "submitter": "Somak Aditya", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos", "title": "Answering Image Riddles using Vision and Reasoning through Probabilistic\n  Soft Logic", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore a genre of puzzles (\"image riddles\") which involves\na set of images and a question. Answering these puzzles require both\ncapabilities involving visual detection (including object, activity\nrecognition) and, knowledge-based or commonsense reasoning. We compile a\ndataset of over 3k riddles where each riddle consists of 4 images and a\ngroundtruth answer. The annotations are validated using crowd-sourced\nevaluation. We also define an automatic evaluation metric to track future\nprogress. Our task bears similarity with the commonly known IQ tasks such as\nanalogy solving, sequence filling that are often used to test intelligence.\n  We develop a Probabilistic Reasoning-based approach that utilizes\nprobabilistic commonsense knowledge to answer these riddles with a reasonable\naccuracy. We demonstrate the results of our approach using both automatic and\nhuman evaluations. Our approach achieves some promising results for these\nriddles and provides a strong baseline for future attempts. We make the entire\ndataset and related materials publicly available to the community in\nImageRiddle Website (http://bit.ly/22f9Ala).\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:10:33 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1611.05915", "submitter": "David Geronimo", "authors": "David Ger\\'onimo and Hedvig Kjellstr\\\"om", "title": "Generative One-Class Models for Text-based Person Retrieval in Forensic\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic forensic image analysis assists criminal investigation experts in\nthe search for suspicious persons, abnormal behaviors detection and identity\nmatching in images. In this paper we propose a person retrieval system that\nuses textual queries (e.g., \"black trousers and green shirt\") as descriptions\nand a one-class generative color model with outlier filtering to represent the\nimages both to train the models and to perform the search. The method is\nevaluated in terms of its efficiency in fulfilling the needs of a forensic\nretrieval system: limited annotation, robustness, extensibility, adaptability\nand computational cost. The proposed generative method is compared to a\ncorresponding discriminative approach. Experiments are carried out using a\nrange of queries in three different databases. The experiments show that the\ntwo evaluated algorithms provide average retrieval performance and adaptable to\nnew datasets. The proposed generative algorithm has some advantages over the\ndiscriminative one, specifically its capability to work with very few training\nsamples and its much lower computational requirements when the number of\ntraining examples increases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 22:02:06 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Ger\u00f3nimo", "David", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "1611.05916", "submitter": "Le Hou", "authors": "Le Hou, Chen-Ping Yu, Dimitris Samaras", "title": "Squared Earth Mover's Distance-based Loss for Training Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of single-label classification, despite the huge success of\ndeep learning, the commonly used cross-entropy loss function ignores the\nintricate inter-class relationships that often exist in real-life tasks such as\nage classification. In this work, we propose to leverage these relationships\nbetween classes by training deep nets with the exact squared Earth Mover's\nDistance (also known as Wasserstein distance) for single-label classification.\nThe squared EMD loss uses the predicted probabilities of all classes and\npenalizes the miss-predictions according to a ground distance matrix that\nquantifies the dissimilarities between classes. We demonstrate that on datasets\nwith strong inter-class relationships such as an ordering between classes, our\nexact squared EMD losses yield new state-of-the-art results. Furthermore, we\npropose a method to automatically learn this matrix using the CNN's own\nfeatures during training. We show that our method can learn a ground distance\nmatrix efficiently with no inter-class relationship priors and yield the same\nperformance gain. Finally, we show that our method can be generalized to\napplications that lack strong inter-class relationships and still maintain\nstate-of-the-art performance. Therefore, with limited computational overhead,\none can always deploy the proposed loss function on any dataset over the\nconventional cross-entropy.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 22:03:35 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 16:30:12 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 20:12:23 GMT"}, {"version": "v4", "created": "Mon, 3 Apr 2017 02:30:57 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Hou", "Le", ""], ["Yu", "Chen-Ping", ""], ["Samaras", "Dimitris", ""]]}, {"id": "1611.05927", "submitter": "Mehrtash Harandi", "authors": "Mehrtash Harandi and Basura Fernando", "title": "Generalized BackPropagation, \\'{E}tude De Cas: Orthogonality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extension of the backpropagation algorithm that\nenables us to have layers with constrained weights in a deep network. In\nparticular, we make use of the Riemannian geometry and optimization techniques\non matrix manifolds to step outside of normal practice in training deep\nnetworks, equipping the network with structures such as orthogonality or\npositive definiteness. Based on our development, we make another contribution\nby introducing the Stiefel layer, a layer with orthogonal weights. Among\nvarious applications, Stiefel layers can be used to design orthogonal filter\nbanks, perform dimensionality reduction and feature extraction. We demonstrate\nthe benefits of having orthogonality in deep networks through a broad set of\nexperiments, ranging from unsupervised feature learning to fine-grained image\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 22:55:09 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Harandi", "Mehrtash", ""], ["Fernando", "Basura", ""]]}, {"id": "1611.05939", "submitter": "Zhe Li", "authors": "Ao Ren, Ji Li, Zhe Li, Caiwen Ding, Xuehai Qian, Qinru Qiu, Bo Yuan,\n  Yanzhi Wang", "title": "SC-DCNN: Highly-Scalable Deep Convolutional Neural Network using\n  Stochastic Computing", "comments": "This paper is accepted by 22nd ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advancing of Internet of Things (IoTs), it becomes very\nattractive to implement the deep convolutional neural networks (DCNNs) onto\nembedded/portable systems. Presently, executing the software-based DCNNs\nrequires high-performance server clusters in practice, restricting their\nwidespread deployment on the mobile devices. To overcome this issue,\nconsiderable research efforts have been conducted in the context of developing\nhighly-parallel and specific DCNN hardware, utilizing GPGPUs, FPGAs, and ASICs.\nStochastic Computing (SC), which uses bit-stream to represent a number within\n[-1, 1] by counting the number of ones in the bit-stream, has a high potential\nfor implementing DCNNs with high scalability and ultra-low hardware footprint.\nSince multiplications and additions can be calculated using AND gates and\nmultiplexers in SC, significant reductions in power/energy and hardware\nfootprint can be achieved compared to the conventional binary arithmetic\nimplementations. The tremendous savings in power (energy) and hardware\nresources bring about immense design space for enhancing scalability and\nrobustness for hardware DCNNs. This paper presents the first comprehensive\ndesign and optimization framework of SC-based DCNNs (SC-DCNNs). We first\npresent the optimal designs of function blocks that perform the basic\noperations, i.e., inner product, pooling, and activation function. Then we\npropose the optimal design of four types of combinations of basic function\nblocks, named feature extraction blocks, which are in charge of extracting\nfeatures from input feature maps. Besides, weight storage methods are\ninvestigated to reduce the area and power/energy consumption for storing\nweights. Finally, the whole SC-DCNN implementation is optimized, with feature\nextraction blocks carefully selected, to minimize area and power/energy\nconsumption while maintaining a high network accuracy level.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 01:11:17 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 16:19:46 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Ren", "Ao", ""], ["Li", "Ji", ""], ["Li", "Zhe", ""], ["Ding", "Caiwen", ""], ["Qian", "Xuehai", ""], ["Qiu", "Qinru", ""], ["Yuan", "Bo", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1611.05947", "submitter": "Joe Kileel", "authors": "Joe Kileel", "title": "Minimal Problems for the Calibrated Trifocal Variety", "comments": "23 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the algebraic degree of minimal problems for the calibrated\ntrifocal variety in computer vision. We rely on numerical algebraic geometry\nand the homotopy continuation software Bertini.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 01:52:35 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Kileel", "Joe", ""]]}, {"id": "1611.05963", "submitter": "Sudhish N George", "authors": "M. Baburaj, Sudhish N. George", "title": "Reweighted Low-Rank Tensor Decomposition based on t-SVD and its\n  Applications in Video Denoising", "comments": "Algorithm 1 is inefficient since line 2 is processed n 3 times need\n  to be changed There are inconsistent notations throughout the manuscript\n  Unitary Tensor are not defined", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The t-SVD based Tensor Robust Principal Component Analysis (TRPCA) decomposes\nlow rank multi-linear signal corrupted by gross errors into low multi-rank and\nsparse component by simultaneously minimizing tensor nuclear norm and l 1 norm.\nBut if the multi-rank of the signal is considerably large and/or large amount\nof noise is present, the performance of TRPCA deteriorates. To overcome this\nproblem, this paper proposes a new efficient iterative reweighted tensor\ndecomposition scheme based on t-SVD which significantly improves tensor\nmulti-rank in TRPCA. Further, the sparse component of the tensor is also\nrecovered by reweighted l 1 norm which enhances the accuracy of decomposition.\nThe effectiveness of the proposed method is established by applying it to the\nvideo denoising problem and the experimental results reveal that the proposed\nalgorithm outperforms its counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 03:26:12 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2017 14:26:44 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 07:02:25 GMT"}, {"version": "v4", "created": "Sun, 9 Jul 2017 16:54:56 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Baburaj", "M.", ""], ["George", "Sudhish N.", ""]]}, {"id": "1611.05964", "submitter": "Sudhish N George", "authors": "Baburaj M., Sudhish N. George", "title": "Reweighted Low-Rank Tensor Completion and its Applications in Video\n  Recovery", "comments": "Algorithm 1 is inefficient since line 2 is processed n 3 times need\n  to be changed There are inconsistent notations throughout the manuscript\n  Unitary Tensor are not defined", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focus on recovering multi-dimensional data called tensor from\nrandomly corrupted incomplete observation. Inspired by reweighted $l_1$ norm\nminimization for sparsity enhancement, this paper proposes a reweighted\nsingular value enhancement scheme to improve tensor low tubular rank in the\ntensor completion process. An efficient iterative decomposition scheme based on\nt-SVD is proposed which improves low-rank signal recovery significantly. The\neffectiveness of the proposed method is established by applying to video\ncompletion problem, and experimental results reveal that the algorithm\noutperforms its counterparts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 03:28:11 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 14:14:08 GMT"}, {"version": "v3", "created": "Tue, 31 Jan 2017 15:20:10 GMT"}, {"version": "v4", "created": "Sun, 9 Jul 2017 16:54:03 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["M.", "Baburaj", ""], ["George", "Sudhish N.", ""]]}, {"id": "1611.05971", "submitter": "Marcelo Cicconet", "authors": "Marcelo Cicconet, David G. C. Hildebrand, and Hunter Elliott", "title": "Finding Mirror Symmetry via Registration", "comments": "Submitted to ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is prevalent in nature and a common theme in man-made designs. Both\nthe human visual system and computer vision algorithms can use symmetry to\nfacilitate object recognition and other tasks. Detecting mirror symmetry in\nimages and data is, therefore, useful for a number of applications. Here, we\ndemonstrate that the problem of fitting a plane of mirror symmetry to data in\nany Euclidian space can be reduced to the problem of registering two datasets.\nThe exactness of the resulting solution depends entirely on the registration\naccuracy. This new Mirror Symmetry via Registration (MSR) framework involves\n(1) data reflection with respect to an arbitrary plane, (2) registration of\noriginal and reflected datasets, and (3) calculation of the eigenvector of\neigenvalue -1 for the transformation matrix representing the reflection and\nregistration mappings. To support MSR, we also introduce a novel 2D\nregistration method based on random sample consensus of an ensemble of\nnormalized cross-correlation matches. With this as its registration back-end,\nMSR achieves state-of-the-art performance for symmetry line detection in two\nindependent 2D testing databases. We further demonstrate the generality of MSR\nby testing it on a database of 3D shapes with an iterative closest point\nregistration back-end. Finally, we explore its applicability to examining\nsymmetry in natural systems by assessing the degree of symmetry present in\nmyelinated axon reconstructions from a larval zebrafish.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 04:37:26 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 01:41:41 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Cicconet", "Marcelo", ""], ["Hildebrand", "David G. C.", ""], ["Elliott", "Hunter", ""]]}, {"id": "1611.06009", "submitter": "Guillaume Thibault", "authors": "Guillaume Thibault and Izhak Shafran", "title": "Fuzzy Statistical Matrices for Cell Classification", "comments": "21 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize image (texture) statistical descriptors and\npropose algorithms that improve their efficacy. Recently, a new method showed\nhow the popular Co-Occurrence Matrix (COM) can be modified into a fuzzy version\n(FCOM) which is more effective and robust to noise. Here, we introduce new\nfuzzy versions of two additional higher order statistical matrices: the Run\nLength Matrix (RLM) and the Size Zone Matrix (SZM). We define the fuzzy zones\nand propose an efficient algorithm to compute the descriptors. We demonstrate\nthe advantage of the proposed improvements over several state-of-the-art\nmethods on three tasks from quantitative cell biology: analyzing and\nclassifying Human Epithelial type 2 (HEp-2) cells using Indirect\nImmunofluorescence protocol (IFF).\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 08:49:28 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Thibault", "Guillaume", ""], ["Shafran", "Izhak", ""]]}, {"id": "1611.06011", "submitter": "Du Yong Kim", "authors": "Du Yong Kim, Ba-Ngu Vo, and Ba-Tuong Vo", "title": "Online Visual Multi-Object Tracking via Labeled Random Finite Set\n  Filtering", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an online visual multi-object tracking algorithm using a\ntop-down Bayesian formulation that seamlessly integrates state estimation,\ntrack management, clutter rejection, occlusion and mis-detection handling into\na single recursion. This is achieved by modeling the multi-object state as\nlabeled random finite set and using the Bayes recursion to propagate the\nmulti-object filtering density forward in time. The proposed filter updates\ntracks with detections but switches to image data when mis-detection occurs,\nthereby exploiting the efficiency of detection data and the accuracy of image\ndata. Furthermore the labeled random finite set framework enables the\nincorporation of prior knowledge that mis-detections of long tracks which occur\nin the middle of the scene are likely to be due to occlusions. Such prior\nknowledge can be exploited to improve occlusion handling, especially long\nocclusions that can lead to premature track termination in on-line multi-object\ntracking. Tracking performance are compared to state-of-the-art algorithms on\nwell-known benchmark video datasets.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 09:00:22 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 09:01:51 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Kim", "Du Yong", ""], ["Vo", "Ba-Ngu", ""], ["Vo", "Ba-Tuong", ""]]}, {"id": "1611.06013", "submitter": "Kui Jia", "authors": "Kui Jia", "title": "Improving training of deep neural networks via Singular Value Bounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods achieve great success recently on many computer vision\nproblems, with image classification and object detection as the prominent\nexamples. In spite of these practical successes, optimization of deep networks\nremains an active topic in deep learning research. In this work, we focus on\ninvestigation of the network solution properties that can potentially lead to\ngood performance. Our research is inspired by theoretical and empirical results\nthat use orthogonal matrices to initialize networks, but we are interested in\ninvestigating how orthogonal weight matrices perform when network training\nconverges. To this end, we propose to constrain the solutions of weight\nmatrices in the orthogonal feasible set during the whole process of network\ntraining, and achieve this by a simple yet effective method called Singular\nValue Bounding (SVB). In SVB, all singular values of each weight matrix are\nsimply bounded in a narrow band around the value of 1. Based on the same\nmotivation, we also propose Bounded Batch Normalization (BBN), which improves\nBatch Normalization by removing its potential risk of ill-conditioned layer\ntransform. We present both theoretical and empirical results to justify our\nproposed methods. Experiments on benchmark image classification datasets show\nthe efficacy of our proposed SVB and BBN. In particular, we achieve the\nstate-of-the-art results of 3.06% error rate on CIFAR10 and 16.90% on CIFAR100,\nusing off-the-shelf network architectures (Wide ResNets). Our preliminary\nresults on ImageNet also show the promise in large-scale learning.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 09:09:56 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 09:49:11 GMT"}, {"version": "v3", "created": "Sat, 18 Mar 2017 07:27:09 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Jia", "Kui", ""]]}, {"id": "1611.06026", "submitter": "Qiqi Xiao", "authors": "Qiqi Xiao, Kelei Cao, Haonan Chen, Fangyue Peng, Chi Zhang", "title": "Cross Domain Knowledge Transfer for Person Re-identification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (re-id) is a challenging task in computer vision,\nespecially when there are limited training data from multiple camera views. In\nthis paper, we pro- pose a deep learning based person re-identification method\nby transferring knowledge of mid-level attribute features and high-level\nclassification features. Building on the idea that identity classification,\nattribute recognition and re- identification share the same mid-level semantic\nrepresentations, they can be trained sequentially by fine-tuning one based on\nanother. In our framework, we train identity classification and attribute\nrecognition tasks from deep Convolutional Neural Network (dCNN) to learn person\ninformation. The information can be transferred to the person re-id task and\nimproves its accuracy by a large margin. Further- more, a Long Short Term\nMemory(LSTM) based Recurrent Neural Network (RNN) component is extended by a\nspacial gate. This component is used in the re-id model to pay attention to\ncertain spacial parts in each recurrent unit. Experimental results show that\nour method achieves 78.3% of rank-1 recognition accuracy on the CUHK03\nbenchmark.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 10:06:58 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Xiao", "Qiqi", ""], ["Cao", "Kelei", ""], ["Chen", "Haonan", ""], ["Peng", "Fangyue", ""], ["Zhang", "Chi", ""]]}, {"id": "1611.06067", "submitter": "Sijie Song", "authors": "Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jiaying Liu", "title": "An End-to-End Spatio-Temporal Attention Model for Human Action\n  Recognition from Skeleton Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition is an important task in computer vision. Extracting\ndiscriminative spatial and temporal features to model the spatial and temporal\nevolutions of different actions plays a key role in accomplishing this task. In\nthis work, we propose an end-to-end spatial and temporal attention model for\nhuman action recognition from skeleton data. We build our model on top of the\nRecurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM), which\nlearns to selectively focus on discriminative joints of skeleton within each\nframe of the inputs and pays different levels of attention to the outputs of\ndifferent frames. Furthermore, to ensure effective training of the network, we\npropose a regularized cross-entropy loss to drive the model learning process\nand develop a joint training strategy accordingly. Experimental results\ndemonstrate the effectiveness of the proposed model,both on the small human\naction recognition data set of SBU and the currently largest NTU dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 13:33:28 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Song", "Sijie", ""], ["Lan", "Cuiling", ""], ["Xing", "Junliang", ""], ["Zeng", "Wenjun", ""], ["Liu", "Jiaying", ""]]}, {"id": "1611.06069", "submitter": "Vikram Mohanty", "authors": "Vikram Mohanty, Shubh Agrawal, Shaswat Datta, Arna Ghosh, Vishnu Dutt\n  Sharma, Debashish Chakravarty", "title": "DeepVO: A Deep Learning approach for Monocular Visual Odometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning based techniques have been adopted with precision to solve a\nlot of standard computer vision problems, some of which are image\nclassification, object detection and segmentation. Despite the widespread\nsuccess of these approaches, they have not yet been exploited largely for\nsolving the standard perception related problems encountered in autonomous\nnavigation such as Visual Odometry (VO), Structure from Motion (SfM) and\nSimultaneous Localization and Mapping (SLAM). This paper analyzes the problem\nof Monocular Visual Odometry using a Deep Learning-based framework, instead of\nthe regular 'feature detection and tracking' pipeline approaches. Several\nexperiments were performed to understand the influence of a known/unknown\nenvironment, a conventional trackable feature and pre-trained activations tuned\nfor object classification on the network's ability to accurately estimate the\nmotion trajectory of the camera (or the vehicle). Based on these observations,\nwe propose a Convolutional Neural Network architecture, best suited for\nestimating the object's pose under known environment conditions, and displays\npromising results when it comes to inferring the actual scale using just a\nsingle camera in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 13:41:22 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Mohanty", "Vikram", ""], ["Agrawal", "Shubh", ""], ["Datta", "Shaswat", ""], ["Ghosh", "Arna", ""], ["Sharma", "Vishnu Dutt", ""], ["Chakravarty", "Debashish", ""]]}, {"id": "1611.06115", "submitter": "Ana Sovic Krzic", "authors": "Janja Paliska Soldo and Ana Sovic Krzic and and Damir Sersic", "title": "Fast low-level pattern matching algorithm", "comments": "14 pages, 7 tables This work has been fully supported by Croatian\n  Science Foundation under the project UIP-11-2013-7353 Algorithms for Genome\n  Sequence Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on pattern matching in the DNA sequence. It was inspired\nby a previously reported method that proposes encoding both pattern and\nsequence using prime numbers. Although fast, the method is limited to rather\nsmall pattern lengths, due to computing precision problem. Our approach\nsuccessfully deals with large patterns, due to our implementation that uses\nmodular arithmetic. In order to get the results very fast, the code was adapted\nfor multithreading and parallel implementations. The method is reduced to\nassembly language level instructions, thus the final result shows significant\ntime and memory savings compared to the reference algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:05:30 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Soldo", "Janja Paliska", ""], ["Krzic", "Ana Sovic", ""], ["Sersic", "and Damir", ""]]}, {"id": "1611.06158", "submitter": "Manuel G\\\"unther", "authors": "Manuel G\\\"unther and Andras Rozsa and Terrance E. Boult", "title": "AFFACT - Alignment-Free Facial Attribute Classification Technique", "comments": "This is a pre-print of the original paper accepted for oral\n  presentation at the International Joint Conference on Biometrics (IJCB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial attributes are soft-biometrics that allow limiting the search space,\ne.g., by rejecting identities with non-matching facial characteristics such as\nnose sizes or eyebrow shapes. In this paper, we investigate how the latest\nversions of deep convolutional neural networks, ResNets, perform on the facial\nattribute classification task. We test two loss functions: the sigmoid\ncross-entropy loss and the Euclidean loss, and find that for classification\nperformance there is little difference between these two. Using an ensemble of\nthree ResNets, we obtain the new state-of-the-art facial attribute\nclassification error of 8.00% on the aligned images of the CelebA dataset. More\nsignificantly, we introduce the Alignment-Free Facial Attribute Classification\nTechnique (AFFACT), a data augmentation technique that allows a network to\nclassify facial attributes without requiring alignment beyond detected face\nbounding boxes. To our best knowledge, we are the first to report similar\naccuracy when using only the detected bounding boxes -- rather than requiring\nalignment based on automatically detected facial landmarks -- and who can\nimprove classification accuracy with rotating and scaling test images. We show\nthat this approach outperforms the CelebA baseline on unaligned images with a\nrelative improvement of 36.8%.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:06:22 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 23:53:03 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["G\u00fcnther", "Manuel", ""], ["Rozsa", "Andras", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1611.06159", "submitter": "Yipei Wang", "authors": "Yan Xu, Siyuan Shan, Ziming Qiu, Zhipeng Jia, Zhengyang Shen, Yipei\n  Wang, Mengfei Shi, Eric I-Chao Chang", "title": "End-to-End Subtitle Detection and Recognition for Videos in East Asian\n  Languages via CNN Ensemble with Near-Human-Level Performance", "comments": "35 pages", "journal-ref": null, "doi": "10.1016/j.image.2017.09.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an innovative end-to-end subtitle detection and\nrecognition system for videos in East Asian languages. Our end-to-end system\nconsists of multiple stages. Subtitles are firstly detected by a novel image\noperator based on the sequence information of consecutive video frames. Then,\nan ensemble of Convolutional Neural Networks (CNNs) trained on synthetic data\nis adopted for detecting and recognizing East Asian characters. Finally, a\ndynamic programming approach leveraging language models is applied to\nconstitute results of the entire body of text lines. The proposed system\nachieves average end-to-end accuracies of 98.2% and 98.3% on 40 videos in\nSimplified Chinese and 40 videos in Traditional Chinese respectively, which is\na significant outperformance of other existing methods. The near-perfect\naccuracy of our system dramatically narrows the gap between human cognitive\nability and state-of-the-art algorithms used for such a task.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:09:14 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Xu", "Yan", ""], ["Shan", "Siyuan", ""], ["Qiu", "Ziming", ""], ["Jia", "Zhipeng", ""], ["Shen", "Zhengyang", ""], ["Wang", "Yipei", ""], ["Shi", "Mengfei", ""], ["Chang", "Eric I-Chao", ""]]}, {"id": "1611.06179", "submitter": "Andras Rozsa", "authors": "Andras Rozsa, Manuel G\\\"unther, and Terrance E. Boult", "title": "LOTS about Attacking Deep Features", "comments": "Accepted to the International Joint Conference on Biometrics (IJCB)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks provide state-of-the-art performance on various tasks\nand are, therefore, widely used in real world applications. DNNs are becoming\nfrequently utilized in biometrics for extracting deep features, which can be\nused in recognition systems for enrolling and recognizing new individuals. It\nwas revealed that deep neural networks suffer from a fundamental problem,\nnamely, they can unexpectedly misclassify examples formed by slightly\nperturbing correctly recognized inputs. Various approaches have been developed\nfor generating these so-called adversarial examples, but they aim at attacking\nend-to-end networks. For biometrics, it is natural to ask whether systems using\ndeep features are immune to or, at least, more resilient to attacks than\nend-to-end networks. In this paper, we introduce a general technique called the\nlayerwise origin-target synthesis (LOTS) that can be efficiently used to form\nadversarial examples that mimic the deep features of the target. We analyze and\ncompare the adversarial robustness of the end-to-end VGG Face network with\nsystems that use Euclidean or cosine distance between gallery templates and\nextracted deep features. We demonstrate that iterative LOTS is very effective\nand show that systems utilizing deep features are easier to attack than the\nend-to-end network.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:59:23 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 02:53:08 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 01:51:57 GMT"}, {"version": "v4", "created": "Sat, 5 Aug 2017 02:49:05 GMT"}, {"version": "v5", "created": "Thu, 31 May 2018 16:12:20 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Rozsa", "Andras", ""], ["G\u00fcnther", "Manuel", ""], ["Boult", "Terrance E.", ""]]}, {"id": "1611.06194", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Punarjay Chakravarty and Tinne Tuytelaars", "title": "Expert Gate: Lifelong Learning with a Network of Experts", "comments": "CVPR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a model of lifelong learning, based on a Network\nof Experts. New tasks / experts are learned and added to the model\nsequentially, building on what was learned before. To ensure scalability of\nthis process,data from previous tasks cannot be stored and hence is not\navailable when learning a new task. A critical issue in such context, not\naddressed in the literature so far, relates to the decision which expert to\ndeploy at test time. We introduce a set of gating autoencoders that learn a\nrepresentation for the task at hand, and, at test time, automatically forward\nthe test sample to the relevant expert. This also brings memory efficiency as\nonly one expert network has to be loaded into memory at any given time.\nFurther, the autoencoders inherently capture the relatedness of one task to\nanother, based on which the most relevant prior model to be used for training a\nnew expert, with finetuning or learning without-forgetting, can be selected. We\nevaluate our method on image classification and video prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:50:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 09:25:55 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Chakravarty", "Punarjay", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1611.06203", "submitter": "\\v{Z}iga Emer\\v{s}i\\v{c}", "authors": "\\v{Z}iga Emer\\v{s}i\\v{c}, Vitomir \\v{S}truc, Peter Peer", "title": "Ear Recognition: More Than a Survey", "comments": "17 pages, paper accepted to Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic identity recognition from ear images represents an active field of\nresearch within the biometric community. The ability to capture ear images from\na distance and in a covert manner makes the technology an appealing choice for\nsurveillance and security applications as well as other application domains.\nSignificant contributions have been made in the field over recent years, but\nopen research problems still remain and hinder a wider (commercial) deployment\nof the technology. This paper presents an overview of the field of automatic\near recognition (from 2D images) and focuses specifically on the most recent,\ndescriptor-based methods proposed in this area. Open challenges are discussed\nand potential research directions are outlined with the goal of providing the\nreader with a point of reference for issues worth examining in the future. In\naddition to a comprehensive review on ear recognition technology, the paper\nalso introduces a new, fully unconstrained dataset of ear images gathered from\nthe web and a toolbox implementing several state-of-the-art techniques for ear\nrecognition. The dataset and toolbox are meant to address some of the open\nissues in the field and are made publicly available to the research community.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 19:32:03 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 09:08:25 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Emer\u0161i\u010d", "\u017diga", ""], ["\u0160truc", "Vitomir", ""], ["Peer", "Peter", ""]]}, {"id": "1611.06211", "submitter": "Mohammad Babaeizadeh", "authors": "Mohammad Babaeizadeh, Paris Smaragdis, Roy H. Campbell", "title": "NoiseOut: A Simple Way to Prune Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are usually over-parameterized with significant redundancy in\nthe number of required neurons which results in unnecessary computation and\nmemory usage at inference time. One common approach to address this issue is to\nprune these big networks by removing extra neurons and parameters while\nmaintaining the accuracy. In this paper, we propose NoiseOut, a fully automated\npruning algorithm based on the correlation between activations of neurons in\nthe hidden layers. We prove that adding additional output neurons with entirely\nrandom targets results into a higher correlation between neurons which makes\npruning by NoiseOut even more efficient. Finally, we test our method on various\nnetworks and datasets. These experiments exhibit high pruning rates while\nmaintaining the accuracy of the original network.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 19:55:29 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Babaeizadeh", "Mohammad", ""], ["Smaragdis", "Paris", ""], ["Campbell", "Roy H.", ""]]}, {"id": "1611.06224", "submitter": "Hui Miao", "authors": "Hui Miao, Ang Li, Larry S. Davis, Amol Deshpande", "title": "ModelHub: Towards Unified Data and Lifecycle Management for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has improved state-of-the-art results in many important fields,\nand has been the subject of much research in recent years, leading to the\ndevelopment of several systems for facilitating deep learning. Current systems,\nhowever, mainly focus on model building and training phases, while the issues\nof data management, model sharing, and lifecycle management are largely\nignored. Deep learning modeling lifecycle generates a rich set of data\nartifacts, such as learned parameters and training logs, and comprises of\nseveral frequently conducted tasks, e.g., to understand the model behaviors and\nto try out new models. Dealing with such artifacts and tasks is cumbersome and\nlargely left to the users. This paper describes our vision and implementation\nof a data and lifecycle management system for deep learning. First, we\ngeneralize model exploration and model enumeration queries from commonly\nconducted tasks by deep learning modelers, and propose a high-level domain\nspecific language (DSL), inspired by SQL, to raise the abstraction level and\naccelerate the modeling process. To manage the data artifacts, especially the\nlarge amount of checkpointed float parameters, we design a novel model\nversioning system (dlv), and a read-optimized parameter archival storage system\n(PAS) that minimizes storage footprint and accelerates query workloads without\nlosing accuracy. PAS archives versioned models using deltas in a\nmulti-resolution fashion by separately storing the less significant bits, and\nfeatures a novel progressive query (inference) evaluation algorithm. Third, we\nshow that archiving versioned models using deltas poses a new dataset\nversioning problem and we develop efficient algorithms for solving it. We\nconduct extensive experiments over several real datasets from computer vision\ndomain to show the efficiency of the proposed techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:59:25 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Miao", "Hui", ""], ["Li", "Ang", ""], ["Davis", "Larry S.", ""], ["Deshpande", "Amol", ""]]}, {"id": "1611.06284", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Vlado Menkovski, Graham W. Taylor and Alexander Wong", "title": "Understanding Anatomy Classification Through Attentive Response Maps", "comments": "Accepted at ISBI, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main challenges for broad adoption of deep learning based models\nsuch as convolutional neural networks (CNN), is the lack of understanding of\ntheir decisions. In many applications, a simpler, less capable model that can\nbe easily understood is favorable to a black-box model that has superior\nperformance. In this paper, we present an approach for designing CNNs based on\nvisualization of the internal activations of the model. We visualize the\nmodel's response through attentive response maps obtained using a fractional\nstride convolution technique and compare the results with known imaging\nlandmarks from the medical literature. We show that sufficiently deep and\ncapable models can be successfully trained to use the same medical landmarks a\nhuman expert would use. Our approach allows for communicating the model\ndecision process well, but also offers insight towards detecting biases.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 00:20:38 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 18:35:02 GMT"}, {"version": "v3", "created": "Wed, 7 Feb 2018 15:58:59 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kumar", "Devinder", ""], ["Menkovski", "Vlado", ""], ["Taylor", "Graham W.", ""], ["Wong", "Alexander", ""]]}, {"id": "1611.06296", "submitter": "Matthew Collett", "authors": "Matthew Collett", "title": "A Bayesian approach to type-specific conic fitting", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A perturbative approach is used to quantify the effect of noise in data\npoints on fitted parameters in a general homogeneous linear model, and the\nresults applied to the case of conic sections. There is an optimal choice of\nnormalisation that minimises bias, and iteration with the correct reweighting\nsignificantly improves statistical reliability. By conditioning on an\nappropriate prior, an unbiased type-specific fit can be obtained. Error\nestimates for the conic coefficients may also be used to obtain both bias\ncorrections and confidence intervals for other curve parameters.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 02:51:48 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Collett", "Matthew", ""]]}, {"id": "1611.06301", "submitter": "Haofu Liao", "authors": "Haofu Liao, Yuncheng Li, Tianran Hu and Jiebo Luo", "title": "Inferring Restaurant Styles by Mining Crowd Sourced Photos from\n  User-Review Websites", "comments": "10 pages, Accepted by IEEE BigData 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When looking for a restaurant online, user uploaded photos often give people\nan immediate and tangible impression about a restaurant. Due to their\ninformativeness, such user contributed photos are leveraged by restaurant\nreview websites to provide their users an intuitive and effective search\nexperience. In this paper, we present a novel approach to inferring restaurant\ntypes or styles (ambiance, dish styles, suitability for different occasions)\nfrom user uploaded photos on user-review websites. To that end, we first\ncollect a novel restaurant photo dataset associating the user contributed\nphotos with the restaurant styles from TripAdvior. We then propose a deep\nmulti-instance multi-label learning (MIML) framework to deal with the unique\nproblem setting of the restaurant style classification task. We employ a\ntwo-step bootstrap strategy to train a multi-label convolutional neural network\n(CNN). The multi-label CNN is then used to compute the confidence scores of\nrestaurant styles for all the images associated with a restaurant. The computed\nconfidence scores are further used to train a final binary classifier for each\nrestaurant style tag. Upon training, the styles of a restaurant can be profiled\nby analyzing restaurant photos with the trained multi-label CNN and SVM models.\nExperimental evaluation has demonstrated that our crowd sourcing-based approach\ncan effectively infer the restaurant style when there are a sufficient number\nof user uploaded photos for a given restaurant.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 04:27:28 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 21:01:03 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Liao", "Haofu", ""], ["Li", "Yuncheng", ""], ["Hu", "Tianran", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.06307", "submitter": "Shubham Pachori", "authors": "Shubham Pachori", "title": "Multi-Scale Saliency Detection using Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency detection has drawn a lot of attention of researchers in various\nfields over the past several years. Saliency is the perceptual quality that\nmakes an object, person to draw the attention of humans at the very sight.\nSalient object detection in an image has been used centrally in many\ncomputational photography and computer vision applications like video\ncompression, object recognition and classification, object segmentation,\nadaptive content delivery, motion detection, content aware resizing, camouflage\nimages and change blindness images to name a few. We propose a method to detect\nsaliency in the objects using multimodal dictionary learning which has been\nrecently used in classification and image fusion. The multimodal dictionary\nthat we are learning is task driven which gives improved performance over its\ncounterpart (the one which is not task specific).\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 05:27:12 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 18:56:37 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 03:46:15 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Pachori", "Shubham", ""]]}, {"id": "1611.06321", "submitter": "Jose M. Alvarez", "authors": "Jose M Alvarez and Mathieu Salzmann", "title": "Learning the Number of Neurons in Deep Networks", "comments": "NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the number of layers and of neurons in each layer of a deep network\nare typically set manually. While very deep and wide networks have proven\neffective in general, they come at a high memory and computation cost, thus\nmaking them impractical for constrained platforms. These networks, however, are\nknown to have many redundant parameters, and could thus, in principle, be\nreplaced by more compact architectures. In this paper, we introduce an approach\nto automatically determining the number of neurons in each layer of a deep\nnetwork during learning. To this end, we propose to make use of structured\nsparsity during learning. More precisely, we use a group sparsity regularizer\non the parameters of the network, where each group is defined to act on a\nsingle neuron. Starting from an overcomplete network, we show that our approach\ncan reduce the number of parameters by up to 80\\% while retaining or even\nimproving the network accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 07:18:17 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 05:21:29 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 07:18:09 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Alvarez", "Jose M", ""], ["Salzmann", "Mathieu", ""]]}, {"id": "1611.06345", "submitter": "Jong Chul Ye", "authors": "Woong Bae, Jaejun Yoo, Jong Chul Ye", "title": "Beyond Deep Residual Learning for Image Restoration: Persistent\n  Homology-Guided Manifold Simplification", "comments": "Accepted at CVPRW 2017 source code :\n  https://github.com/iorism/CNN.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest deep learning approaches perform better than the state-of-the-art\nsignal processing approaches in various image restoration tasks. However, if an\nimage contains many patterns and structures, the performance of these CNNs is\nstill inferior. To address this issue, here we propose a novel feature space\ndeep residual learning algorithm that outperforms the existing residual\nlearning. The main idea is originated from the observation that the performance\nof a learning algorithm can be improved if the input and/or label manifolds can\nbe made topologically simpler by an analytic mapping to a feature space. Our\nextensive numerical studies using denoising experiments and NTIRE single-image\nsuper-resolution (SISR) competition demonstrate that the proposed feature space\nresidual learning outperforms the existing state-of-the-art approaches.\nMoreover, our algorithm was ranked third in NTIRE competition with 5-10 times\nfaster computational time compared to the top ranked teams. The source code is\navailable on page : https://github.com/iorism/CNN.git\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 11:43:43 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 08:49:47 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 12:58:48 GMT"}, {"version": "v4", "created": "Thu, 8 Jun 2017 16:52:33 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Bae", "Woong", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1611.06355", "submitter": "Guim Perarnau", "authors": "Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, Jose M. \\'Alvarez", "title": "Invertible Conditional GANs for image editing", "comments": "Accepted paper at NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently demonstrated to\nsuccessfully approximate complex data distributions. A relevant extension of\nthis model is conditional GANs (cGANs), where the introduction of external\ninformation allows to determine specific representations of the generated\nimages. In this work, we evaluate encoders to inverse the mapping of a cGAN,\ni.e., mapping a real image into a latent space and a conditional\nrepresentation. This allows, for example, to reconstruct and modify real images\nof faces conditioning on arbitrary attributes. Additionally, we evaluate the\ndesign of cGANs. The combination of an encoder with a cGAN, which we call\nInvertible cGAN (IcGAN), enables to re-generate real images with deterministic\ncomplex modifications.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 12:35:01 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Perarnau", "Guim", ""], ["van de Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""], ["\u00c1lvarez", "Jose M.", ""]]}, {"id": "1611.06362", "submitter": "Hong Liu", "authors": "Hong Liu, Rongrong Ji, Yongjian Wu, Feiyue Huang", "title": "Ordinal Constrained Binary Code Learning for Nearest Neighbor Search", "comments": "Accepted to AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed extensive attention in binary code learning,\na.k.a. hashing, for nearest neighbor search problems. It has been seen that\nhigh-dimensional data points can be quantized into binary codes to give an\nefficient similarity approximation via Hamming distance. Among existing\nschemes, ranking-based hashing is recent promising that targets at preserving\nordinal relations of ranking in the Hamming space to minimize retrieval loss.\nHowever, the size of the ranking tuples, which shows the ordinal relations, is\nquadratic or cubic to the size of training samples. By given a large-scale\ntraining data set, it is very expensive to embed such ranking tuples in binary\ncode learning. Besides, it remains a dificulty to build ranking tuples\nefficiently for most ranking-preserving hashing, which are deployed over an\nordinal graph-based setting. To handle these problems, we propose a novel\nranking-preserving hashing method, dubbed Ordinal Constraint Hashing (OCH),\nwhich efficiently learns the optimal hashing functions with a graph-based\napproximation to embed the ordinal relations. The core idea is to reduce the\nsize of ordinal graph with ordinal constraint projection, which preserves the\nordinal relations through a small data set (such as clusters or random\nsamples). In particular, to learn such hash functions effectively, we further\nrelax the discrete constraints and design a specific stochastic gradient decent\nalgorithm for optimization. Experimental results on three large-scale visual\nsearch benchmark datasets, i.e. LabelMe, Tiny100K and GIST1M, show that the\nproposed OCH method can achieve superior performance over the state-of-the-arts\napproaches.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 13:24:10 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Liu", "Hong", ""], ["Ji", "Rongrong", ""], ["Wu", "Yongjian", ""], ["Huang", "Feiyue", ""]]}, {"id": "1611.06391", "submitter": "Jong Chul Ye", "authors": "Yo Seob Han, Jaejun Yoo, Jong Chul Ye", "title": "Deep Residual Learning for Compressed Sensing CT Reconstruction via\n  Persistent Homology Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, compressed sensing (CS) computed tomography (CT) using sparse\nprojection views has been extensively investigated to reduce the potential risk\nof radiation to patient. However, due to the insufficient number of projection\nviews, an analytic reconstruction approach results in severe streaking\nartifacts and CS-based iterative approach is computationally very expensive. To\naddress this issue, here we propose a novel deep residual learning approach for\nsparse view CT reconstruction. Specifically, based on a novel persistent\nhomology analysis showing that the manifold of streaking artifacts is\ntopologically simpler than original ones, a deep residual learning architecture\nthat estimates the streaking artifacts is developed. Once a streaking artifact\nimage is estimated, an artifact-free image can be obtained by subtracting the\nstreaking artifacts from the input image. Using extensive experiments with real\npatient data set, we confirm that the proposed residual learning provides\nsignificantly better image reconstruction performance with several orders of\nmagnitude faster computational speed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 16:05:43 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 06:54:06 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Han", "Yo Seob", ""], ["Yoo", "Jaejun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1611.06395", "submitter": "Jingjing Xiao", "authors": "Jingjing Xiao, Qiang Lan, Linbo Qiao, Ales Leonardis", "title": "Semantic tracking: Single-target tracking with inter-supervised\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a semantic tracker which simultaneously tracks a single\ntarget and recognises its category. In general, it is hard to design a tracking\nmodel suitable for all object categories, e.g., a rigid tracker for a car is\nnot suitable for a deformable gymnast. Category-based trackers usually achieve\nsuperior tracking performance for the objects of that specific category, but\nhave difficulties being generalised. Therefore, we propose a novel unified\nrobust tracking framework which explicitly encodes both generic features and\ncategory-based features. The tracker consists of a shared convolutional network\n(NetS), which feeds into two parallel networks, NetC for classification and\nNetT for tracking. NetS is pre-trained on ImageNet to serve as a generic\nfeature extractor across the different object categories for NetC and NetT.\nNetC utilises those features within fully connected layers to classify the\nobject category. NetT has multiple branches, corresponding to multiple\ncategories, to distinguish the tracked object from the background. Since each\nbranch in NetT is trained by the videos of a specific category or groups of\nsimilar categories, NetT encodes category-based features for tracking. During\nonline tracking, NetC and NetT jointly determine the target regions with the\nright category and foreground labels for target estimation. To improve the\nrobustness and precision, NetC and NetT inter-supervise each other and trigger\nnetwork adaptation when their outputs are ambiguous for the same image regions\n(i.e., when the category label contradicts the foreground/background\nclassification). We have compared the performance of our tracker to other\nstate-of-the-art trackers on a large-scale tracking benchmark (100\nsequences)---the obtained results demonstrate the effectiveness of our proposed\ntracker as it outperformed other 38 state-of-the-art tracking algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 16:10:03 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Xiao", "Jingjing", ""], ["Lan", "Qiang", ""], ["Qiao", "Linbo", ""], ["Leonardis", "Ales", ""]]}, {"id": "1611.06403", "submitter": "Yannick Hold-Geoffroy", "authors": "Yannick Hold-Geoffroy, Kalyan Sunkavalli, Sunil Hadap, Emiliano\n  Gambaretto, Jean-Fran\\c{c}ois Lalonde", "title": "Deep Outdoor Illumination Estimation", "comments": "CVPR'17 preprint, 8 pages + 2 pages of citations, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a CNN-based technique to estimate high-dynamic range outdoor\nillumination from a single low dynamic range image. To train the CNN, we\nleverage a large dataset of outdoor panoramas. We fit a low-dimensional\nphysically-based outdoor illumination model to the skies in these panoramas\ngiving us a compact set of parameters (including sun position, atmospheric\nconditions, and camera parameters). We extract limited field-of-view images\nfrom the panoramas, and train a CNN with this large set of input image--output\nlighting parameter pairs. Given a test image, this network can be used to infer\nillumination parameters that can, in turn, be used to reconstruct an outdoor\nillumination environment map. We demonstrate that our approach allows the\nrecovery of plausible illumination conditions and enables photorealistic\nvirtual object insertion from a single image. An extensive evaluation on both\nthe panorama dataset and captured HDR environment maps shows that our technique\nsignificantly outperforms previous solutions to this problem.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 17:23:15 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 21:38:27 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 15:47:14 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Hold-Geoffroy", "Yannick", ""], ["Sunkavalli", "Kalyan", ""], ["Hadap", "Sunil", ""], ["Gambaretto", "Emiliano", ""], ["Lalonde", "Jean-Fran\u00e7ois", ""]]}, {"id": "1611.06430", "submitter": "Emily Denton", "authors": "Emily Denton, Sam Gross, Rob Fergus", "title": "Semi-Supervised Learning with Context-Conditional Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple semi-supervised learning approach for images based on\nin-painting using an adversarial loss. Images with random patches removed are\npresented to a generator whose task is to fill in the hole, based on the\nsurrounding pixels. The in-painted images are then presented to a discriminator\nnetwork that judges if they are real (unaltered training images) or not. This\ntask acts as a regularizer for standard supervised training of the\ndiscriminator. Using our approach we are able to directly train large VGG-style\nnetworks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL\ndatasets, where our approach obtains performance comparable or superior to\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 21:02:14 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Denton", "Emily", ""], ["Gross", "Sam", ""], ["Fergus", "Rob", ""]]}, {"id": "1611.06448", "submitter": "Brandon RichardWebster", "authors": "Brandon RichardWebster, Samuel E. Anthony, and Walter J. Scheirer", "title": "PsyPhy: A Psychophysics Driven Evaluation Framework for Visual\n  Recognition", "comments": "9 pages, 4 figures. Published at IEEE Transactions on Pattern\n  Analysis and Machine Intelligence. For supplemental material see\n  http://bjrichardwebster.com/papers/psyphy/supp", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2849989", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By providing substantial amounts of data and standardized evaluation\nprotocols, datasets in computer vision have helped fuel advances across all\nareas of visual recognition. But even in light of breakthrough results on\nrecent benchmarks, it is still fair to ask if our recognition algorithms are\ndoing as well as we think they are. The vision sciences at large make use of a\nvery different evaluation regime known as Visual Psychophysics to study visual\nperception. Psychophysics is the quantitative examination of the relationships\nbetween controlled stimuli and the behavioral responses they elicit in\nexperimental test subjects. Instead of using summary statistics to gauge\nperformance, psychophysics directs us to construct item-response curves made up\nof individual stimulus responses to find perceptual thresholds, thus allowing\none to identify the exact point at which a subject can no longer reliably\nrecognize the stimulus class. In this article, we introduce a comprehensive\nevaluation framework for visual recognition models that is underpinned by this\nmethodology. Over millions of procedurally rendered 3D scenes and 2D images, we\ncompare the performance of well-known convolutional neural networks. Our\nresults bring into question recent claims of human-like performance, and\nprovide a path forward for correcting newly surfaced algorithmic deficiencies.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 23:23:32 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 01:38:44 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 19:10:16 GMT"}, {"version": "v4", "created": "Sat, 9 Jun 2018 21:09:45 GMT"}, {"version": "v5", "created": "Thu, 14 Jun 2018 20:58:00 GMT"}, {"version": "v6", "created": "Wed, 4 Jul 2018 18:39:18 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["RichardWebster", "Brandon", ""], ["Anthony", "Samuel E.", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1611.06453", "submitter": "Haichen Shen", "authors": "Haichen Shen, Seungyeop Han, Matthai Philipose, Arvind Krishnamurthy", "title": "Fast Video Classification via Adaptive Cascading of Deep Models", "comments": "Accepted at IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances have enabled \"oracle\" classifiers that can classify across\nmany classes and input distributions with high accuracy without retraining.\nHowever, these classifiers are relatively heavyweight, so that applying them to\nclassify video is costly. We show that day-to-day video exhibits highly skewed\nclass distributions over the short term, and that these distributions can be\nclassified by much simpler models. We formulate the problem of detecting the\nshort-term skews online and exploiting models based on it as a new sequential\ndecision making problem dubbed the Online Bandit Problem, and present a new\nalgorithm to solve it. When applied to recognizing faces in TV shows and\nmovies, we realize end-to-end classification speedups of 2.4-7.8x/2.6-11.2x (on\nGPU/CPU) relative to a state-of-the-art convolutional neural network, at\ncompetitive accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 00:21:32 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 02:17:00 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Shen", "Haichen", ""], ["Han", "Seungyeop", ""], ["Philipose", "Matthai", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "1611.06467", "submitter": "Naiyan Wang", "authors": "Hong Zhang, Naiyan Wang", "title": "On The Stability of Video Detection and Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an important yet less explored aspect in video\ndetection and tracking -- stability. Surprisingly, there is no prior work that\ntried to study it. As a result, we start our work by proposing a novel\nevaluation metric for video detection which considers both stability and\naccuracy. For accuracy, we extend the existing accuracy metric mean Average\nPrecision (mAP). For stability, we decompose it into three terms: fragment\nerror, center position error, scale and ratio error. Each error represents one\naspect of stability. Furthermore, we demonstrate that the stability metric has\nlow correlation with accuracy metric. Thus, it indeed captures a different\nperspective of quality. Lastly, based on this metric, we evaluate several\nexisting methods for video detection and show how they affect accuracy and\nstability. We believe our work can provide guidance and solid baselines for\nfuture researches in the related areas.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 03:45:34 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 15:00:14 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Zhang", "Hong", ""], ["Wang", "Naiyan", ""]]}, {"id": "1611.06473", "submitter": "Hessam Bagherinezhad", "authors": "Hessam Bagherinezhad, Mohammad Rastegari and Ali Farhadi", "title": "LCNN: Lookup-based Convolutional Neural Network", "comments": "CVPR 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Porting state of the art deep learning algorithms to resource constrained\ncompute platforms (e.g. VR, AR, wearables) is extremely challenging. We propose\na fast, compact, and accurate model for convolutional neural networks that\nenables efficient learning and inference. We introduce LCNN, a lookup-based\nconvolutional neural network that encodes convolutions by few lookups to a\ndictionary that is trained to cover the space of weights in CNNs. Training LCNN\ninvolves jointly learning a dictionary and a small set of linear combinations.\nThe size of the dictionary naturally traces a spectrum of trade-offs between\nefficiency and accuracy. Our experimental results on ImageNet challenge show\nthat LCNN can offer 3.2x speedup while achieving 55.1% top-1 accuracy using\nAlexNet architecture. Our fastest LCNN offers 37.6x speed up over AlexNet while\nmaintaining 44.3% top-1 accuracy. LCNN not only offers dramatic speed ups at\ninference, but it also enables efficient training. In this paper, we show the\nbenefits of LCNN in few-shot learning and few-iteration learning, two crucial\naspects of on-device training of deep learning models.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 05:50:57 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 02:43:30 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Bagherinezhad", "Hessam", ""], ["Rastegari", "Mohammad", ""], ["Farhadi", "Ali", ""]]}, {"id": "1611.06474", "submitter": "Nazia Attari", "authors": "N. Attari and F. Ofli and M. Awad and J. Lucas and S. Chawla", "title": "Nazr-CNN: Fine-Grained Classification of UAV Imagery for Damage\n  Assessment", "comments": "Accepted for publication in the 4th IEEE International Conference on\n  Data Science and Advanced Analytics (DSAA) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Nazr-CNN1, a deep learning pipeline for object detection and\nfine-grained classification in images acquired from Unmanned Aerial Vehicles\n(UAVs) for damage assessment and monitoring. Nazr-CNN consists of two\ncomponents. The function of the first component is to localize objects (e.g.\nhouses or infrastructure) in an image by carrying out a pixel-level\nclassification. In the second component, a hidden layer of a Convolutional\nNeural Network (CNN) is used to encode Fisher Vectors (FV) of the segments\ngenerated from the first component in order to help discriminate between\ndifferent levels of damage. To showcase our approach we use data from UAVs that\nwere deployed to assess the level of damage in the aftermath of a devastating\ncyclone that hit the island of Vanuatu in 2015. The collected images were\nlabeled by a crowdsourcing effort and the labeling categories consisted of\nfine-grained levels of damage to built structures. Since our data set is\nrelatively small, a pre- trained network for pixel-level classification and FV\nencoding was used. Nazr-CNN attains promising results both for object detection\nand damage assessment suggesting that the integrated pipeline is robust in the\nface of small data sets and labeling errors by annotators. While the focus of\nNazr-CNN is on assessment of UAV images in a post-disaster scenario, our\nsolution is general and can be applied in many diverse settings. We show one\nsuch case of transfer learning to assess the level of damage in aerial images\ncollected after a typhoon in Philippines.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 05:54:06 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 08:27:52 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Attari", "N.", ""], ["Ofli", "F.", ""], ["Awad", "M.", ""], ["Lucas", "J.", ""], ["Chawla", "S.", ""]]}, {"id": "1611.06492", "submitter": "Abhinav Agarwalla", "authors": "Arnav Kumar Jain, Abhinav Agarwalla, Kumar Krishna Agrawal, Pabitra\n  Mitra", "title": "Recurrent Memory Addressing for describing videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Key-Value Memory Networks to a multimodal setting\nand a novel key-addressing mechanism to deal with sequence-to-sequence models.\nThe proposed model naturally decomposes the problem of video captioning into\nvision and language segments, dealing with them as key-value pairs. More\nspecifically, we learn a semantic embedding (v) corresponding to each frame (k)\nin the video, thereby creating (k, v) memory slots. We propose to find the next\nstep attention weights conditioned on the previous attention distributions for\nthe key-value memory slots in the memory addressing schema. Exploiting this\nflexibility of the framework, we additionally capture spatial dependencies\nwhile mapping from the visual to semantic embedding. Experiments done on the\nYoutube2Text dataset demonstrate usefulness of recurrent key-addressing, while\nachieving competitive scores on BLEU@4, METEOR metrics against state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 10:07:54 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 14:01:20 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Jain", "Arnav Kumar", ""], ["Agarwalla", "Abhinav", ""], ["Agrawal", "Kumar Krishna", ""], ["Mitra", "Pabitra", ""]]}, {"id": "1611.06495", "submitter": "Jiawei Zhang", "authors": "Jiawei Zhang, Jinshan Pan, Wei-Sheng Lai, Rynson Lau, Ming-Hsuan Yang", "title": "Learning Fully Convolutional Networks for Iterative Non-blind\n  Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fully convolutional networks for iterative\nnon-blind deconvolution We decompose the non-blind deconvolution problem into\nimage denoising and image deconvolution. We train a FCNN to remove noises in\nthe gradient domain and use the learned gradients to guide the image\ndeconvolution step. In contrast to the existing deep neural network based\nmethods, we iteratively deconvolve the blurred images in a multi-stage\nframework. The proposed method is able to learn an adaptive image prior, which\nkeeps both local (details) and global (structures) information. Both\nquantitative and qualitative evaluations on benchmark datasets demonstrate that\nthe proposed method performs favorably against state-of-the-art algorithms in\nterms of quality and speed.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 10:25:06 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Zhang", "Jiawei", ""], ["Pan", "Jinshan", ""], ["Lai", "Wei-Sheng", ""], ["Lau", "Rynson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1611.06565", "submitter": "David Budden", "authors": "David Budden, Alexander Matveev, Shibani Santurkar, Shraman Ray\n  Chaudhuri and Nir Shavit", "title": "Deep Tensor Convolution on Multicores", "comments": "11 pages, 4 figures, 1 supplementary doc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (ConvNets) of 3-dimensional kernels allow\njoint modeling of spatiotemporal features. These networks have improved\nperformance of video and volumetric image analysis, but have been limited in\nsize due to the low memory ceiling of GPU hardware. Existing CPU\nimplementations overcome this constraint but are impractically slow. Here we\nextend and optimize the faster Winograd-class of convolutional algorithms to\nthe $N$-dimensional case and specifically for CPU hardware. First, we remove\nthe need to manually hand-craft algorithms by exploiting the relaxed\nconstraints and cheap sparse access of CPU memory. Second, we maximize CPU\nutilization and multicore scalability by transforming data matrices to be\ncache-aware, integer multiples of AVX vector widths. Treating 2-dimensional\nConvNets as a special (and the least beneficial) case of our approach, we\ndemonstrate a 5 to 25-fold improvement in throughput compared to previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 18:41:48 GMT"}, {"version": "v2", "created": "Sat, 28 Jan 2017 15:01:13 GMT"}, {"version": "v3", "created": "Sun, 11 Jun 2017 15:29:16 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Budden", "David", ""], ["Matveev", "Alexander", ""], ["Santurkar", "Shibani", ""], ["Chaudhuri", "Shraman Ray", ""], ["Shavit", "Nir", ""]]}, {"id": "1611.06596", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Lingxi Xie, Alan L. Yuille", "title": "Object Recognition with and without Objects", "comments": "To Appear in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent deep neural networks have achieved a promising performance on\nobject recognition, they rely implicitly on the visual contents of the whole\nimage. In this paper, we train deep neural net- works on the foreground\n(object) and background (context) regions of images respectively. Consider- ing\nhuman recognition in the same situations, net- works trained on the pure\nbackground without ob- jects achieves highly reasonable recognition performance\nthat beats humans by a large margin if only given context. However, humans\nstill outperform networks with pure object available, which indicates networks\nand human beings have different mechanisms in understanding an image.\nFurthermore, we straightforwardly combine multiple trained networks to explore\ndifferent visual cues learned by different networks. Experiments show that\nuseful visual hints can be explicitly learned separately and then combined to\nachieve higher performance, which verifies the advantages of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 21:20:32 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 02:58:11 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 18:15:06 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Xie", "Lingxi", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1611.06607", "submitter": "Jonathan Krause", "authors": "Jonathan Krause, Justin Johnson, Ranjay Krishna, Li Fei-Fei", "title": "A Hierarchical Approach for Generating Descriptive Image Paragraphs", "comments": "CVPR 2017 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress on image captioning has made it possible to generate novel\nsentences describing images in natural language, but compressing an image into\na single sentence can describe visual content in only coarse detail. While one\nnew captioning approach, dense captioning, can potentially describe images in\nfiner levels of detail by captioning many regions within an image, it in turn\nis unable to produce a coherent story for an image. In this paper we overcome\nthese limitations by generating entire paragraphs for describing images, which\ncan tell detailed, unified stories. We develop a model that decomposes both\nimages and paragraphs into their constituent parts, detecting semantic regions\nin images and using a hierarchical recurrent neural network to reason about\nlanguage. Linguistic analysis confirms the complexity of the paragraph\ngeneration task, and thorough experiments on a new dataset of image and\nparagraph pairs demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 23:10:51 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 17:59:15 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Krause", "Jonathan", ""], ["Johnson", "Justin", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1611.06612", "submitter": "Guosheng Lin", "authors": "Guosheng Lin, Anton Milan, Chunhua Shen, Ian Reid", "title": "RefineNet: Multi-Path Refinement Networks for High-Resolution Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, very deep convolutional neural networks (CNNs) have shown\noutstanding performance in object recognition and have also been the first\nchoice for dense classification problems such as semantic segmentation.\nHowever, repeated subsampling operations like pooling or convolution striding\nin deep CNNs lead to a significant decrease in the initial image resolution.\nHere, we present RefineNet, a generic multi-path refinement network that\nexplicitly exploits all the information available along the down-sampling\nprocess to enable high-resolution prediction using long-range residual\nconnections. In this way, the deeper layers that capture high-level semantic\nfeatures can be directly refined using fine-grained features from earlier\nconvolutions. The individual components of RefineNet employ residual\nconnections following the identity mapping mindset, which allows for effective\nend-to-end training. Further, we introduce chained residual pooling, which\ncaptures rich background context in an efficient manner. We carry out\ncomprehensive experiments and set new state-of-the-art results on seven public\ndatasets. In particular, we achieve an intersection-over-union score of 83.4 on\nthe challenging PASCAL VOC 2012 dataset, which is the best reported result to\ndate.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 23:39:52 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 06:14:12 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 02:01:05 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Lin", "Guosheng", ""], ["Milan", "Anton", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1611.06624", "submitter": "Masaki Saito", "authors": "Masaki Saito, Eiichi Matsumoto, Shunta Saito", "title": "Temporal Generative Adversarial Nets with Singular Value Clipping", "comments": "to appear in ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generative model, Temporal Generative Adversarial\nNets (TGAN), which can learn a semantic representation of unlabeled videos, and\nis capable of generating videos. Unlike existing Generative Adversarial Nets\n(GAN)-based methods that generate videos with a single generator consisting of\n3D deconvolutional layers, our model exploits two different types of\ngenerators: a temporal generator and an image generator. The temporal generator\ntakes a single latent variable as input and outputs a set of latent variables,\neach of which corresponds to an image frame in a video. The image generator\ntransforms a set of such latent variables into a video. To deal with\ninstability in training of GAN with such advanced networks, we adopt a recently\nproposed model, Wasserstein GAN, and propose a novel method to train it stably\nin an end-to-end manner. The experimental results demonstrate the effectiveness\nof our methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 01:10:50 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 01:33:45 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 02:32:16 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Saito", "Masaki", ""], ["Matsumoto", "Eiichi", ""], ["Saito", "Shunta", ""]]}, {"id": "1611.06638", "submitter": "Qiang Qiu", "authors": "Jose Lezama, Qiang Qiu, Guillermo Sapiro", "title": "Not Afraid of the Dark: NIR-VIS Face Recognition via Cross-spectral\n  Hallucination and Low-rank Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance cameras today often capture NIR (near infrared) images in\nlow-light environments. However, most face datasets accessible for training and\nverification are only collected in the VIS (visible light) spectrum. It remains\na challenging problem to match NIR to VIS face images due to the different\nlight spectrum. Recently, breakthroughs have been made for VIS face recognition\nby applying deep learning on a huge amount of labeled VIS face samples. The\nsame deep learning approach cannot be simply applied to NIR face recognition\nfor two main reasons: First, much limited NIR face images are available for\ntraining compared to the VIS spectrum. Second, face galleries to be matched are\nmostly available only in the VIS spectrum. In this paper, we propose an\napproach to extend the deep learning breakthrough for VIS face recognition to\nthe NIR spectrum, without retraining the underlying deep models that see only\nVIS faces. Our approach consists of two core components, cross-spectral\nhallucination and low-rank embedding, to optimize respectively input and output\nof a VIS deep model for cross-spectral face recognition. Cross-spectral\nhallucination produces VIS faces from NIR images through a deep learning\napproach. Low-rank embedding restores a low-rank structure for faces deep\nfeatures across both NIR and VIS spectrum. We observe that it is often equally\neffective to perform hallucination to input NIR images or low-rank embedding to\noutput deep features for a VIS deep model for cross-spectral recognition. When\nhallucination and low-rank embedding are deployed together, we observe\nsignificant further improvement; we obtain state-of-the-art accuracy on the\nCASIA NIR-VIS v2.0 benchmark, without the need at all to re-train the\nrecognition system.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 03:22:23 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Lezama", "Jose", ""], ["Qiu", "Qiang", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1611.06641", "submitter": "Bryan Plummer", "authors": "Bryan A. Plummer, Arun Mallya, Christopher M. Cervantes, Julia\n  Hockenmaier, Svetlana Lazebnik", "title": "Phrase Localization and Visual Relationship Detection with Comprehensive\n  Image-Language Cues", "comments": "IEEE ICCV 2017 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for localization or grounding of phrases in\nimages using a large collection of linguistic and visual cues. We model the\nappearance, size, and position of entity bounding boxes, adjectives that\ncontain attribute information, and spatial relationships between pairs of\nentities connected by verbs or prepositions. Special attention is given to\nrelationships between people and clothing or body part mentions, as they are\nuseful for distinguishing individuals. We automatically learn weights for\ncombining these cues and at test time, perform joint inference over all phrases\nin a caption. The resulting system produces state of the art performance on\nphrase localization on the Flickr30k Entities dataset and visual relationship\ndetection on the Stanford VRD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 03:43:22 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 17:52:12 GMT"}, {"version": "v3", "created": "Tue, 8 Aug 2017 04:17:13 GMT"}, {"version": "v4", "created": "Wed, 9 Aug 2017 00:25:47 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Plummer", "Bryan A.", ""], ["Mallya", "Arun", ""], ["Cervantes", "Christopher M.", ""], ["Hockenmaier", "Julia", ""], ["Lazebnik", "Svetlana", ""]]}, {"id": "1611.06642", "submitter": "HaiLiang Li", "authors": "Hailiang Li, Kin-Man Lam, Edmond M. Y. Chiu, Kangheng Wu, Zhibin Lei", "title": "Cascaded Face Alignment via Intimacy Definition Feature", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.26.5.053024", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a random-forest based fast cascaded regression\nmodel for face alignment, via a novel local feature. Our proposed local\nlightweight feature, namely intimacy definition feature (IDF), is more\ndiscriminative than landmark pose-indexed feature, more efficient than\nhistogram of oriented gradients (HOG) feature and scale-invariant feature\ntransform (SIFT) feature, and more compact than the local binary feature (LBF).\nExperimental results show that our approach achieves state-of-the-art\nperformance when tested on the most challenging datasets. Compared with an\nLBF-based algorithm, our method can achieve about two times the speed-up and\nmore than 20% improvement, in terms of alignment accuracy measurement, and save\nan order of magnitude of memory requirement.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 03:53:30 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 04:24:41 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Li", "Hailiang", ""], ["Lam", "Kin-Man", ""], ["Chiu", "Edmond M. Y.", ""], ["Wu", "Kangheng", ""], ["Lei", "Zhibin", ""]]}, {"id": "1611.06646", "submitter": "Basura Fernando", "authors": "Basura Fernando, Hakan Bilen, Efstratios Gavves, Stephen Gould", "title": "Self-Supervised Video Representation Learning With Odd-One-Out Networks", "comments": "Accepted in In IEEE International Conference on Computer Vision and\n  Pattern Recognition CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new self-supervised CNN pre-training technique based on a novel\nauxiliary task called \"odd-one-out learning\". In this task, the machine is\nasked to identify the unrelated or odd element from a set of otherwise related\nelements. We apply this technique to self-supervised video representation\nlearning where we sample subsequences from videos and ask the network to learn\nto predict the odd video subsequence. The odd video subsequence is sampled such\nthat it has wrong temporal order of frames while the even ones have the correct\ntemporal order. Therefore, to generate a odd-one-out question no manual\nannotation is required. Our learning machine is implemented as multi-stream\nconvolutional neural network, which is learned end-to-end. Using odd-one-out\nnetworks, we learn temporal representations for videos that generalizes to\nother related tasks such as action recognition.\n  On action classification, our method obtains 60.3\\% on the UCF101 dataset\nusing only UCF101 data for training which is approximately 10% better than\ncurrent state-of-the-art self-supervised learning methods. Similarly, on HMDB51\ndataset we outperform self-supervised state-of-the art methods by 12.7% on\naction classification task.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 04:35:45 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 00:05:09 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 03:51:05 GMT"}, {"version": "v4", "created": "Wed, 5 Apr 2017 05:52:00 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Fernando", "Basura", ""], ["Bilen", "Hakan", ""], ["Gavves", "Efstratios", ""], ["Gould", "Stephen", ""]]}, {"id": "1611.06651", "submitter": "He Yang", "authors": "He Yang, Hengyong Yu and Ge Wang", "title": "Deep Learning for the Classification of Lung Nodules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning, as a promising new area of machine learning, has attracted a\nrapidly increasing attention in the field of medical imaging. Compared to the\nconventional machine learning methods, deep learning requires no hand-tuned\nfeature extractor, and has shown a superior performance in many visual object\nrecognition applications. In this study, we develop a deep convolutional neural\nnetwork (CNN) and apply it to thoracic CT images for the classification of lung\nnodules. We present the CNN architecture and classification accuracy for the\noriginal images of lung nodules. In order to understand the features of lung\nnodules, we further construct new datasets, based on the combination of\nartificial geometric nodules and some transformations of the original images,\nas well as a stochastic nodule shape model. It is found that simplistic\ngeometric nodules cannot capture the important features of lung nodules.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:12:44 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 21:43:48 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Yang", "He", ""], ["Yu", "Hengyong", ""], ["Wang", "Ge", ""]]}, {"id": "1611.06656", "submitter": "Ammar Mahmood Mr.", "authors": "Ammar Mahmood, Mohammed Bennamoun, Senjian An, Ferdous Sohel", "title": "ResFeats: Residual Network Based Features for Image Classification", "comments": null, "journal-ref": "A. Mahmood, M. Bennamoun, S. An and F. Sohel, \"Resfeats: Residual\n  network based features for image classification,\" 2017 IEEE International\n  Conference on Image Processing (ICIP), Beijing, 2017, pp. 1597-1601", "doi": "10.1109/ICIP.2017.8296551", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks have recently emerged as the state-of-the-art\narchitecture in image segmentation and object detection. In this paper, we\npropose new image features (called ResFeats) extracted from the last\nconvolutional layer of deep residual networks pre-trained on ImageNet. We\npropose to use ResFeats for diverse image classification tasks namely, object\nclassification, scene classification and coral classification and show that\nResFeats consistently perform better than their CNN counterparts on these\nclassification tasks. Since the ResFeats are large feature vectors, we propose\nto use PCA for dimensionality reduction. Experimental results are provided to\nshow the effectiveness of ResFeats with state-of-the-art classification\naccuracies on Caltech-101, Caltech-256 and MLC datasets and a significant\nperformance improvement on MIT-67 dataset compared to the widely used CNN\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:42:13 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Mahmood", "Ammar", ""], ["Bennamoun", "Mohammed", ""], ["An", "Senjian", ""], ["Sohel", "Ferdous", ""]]}, {"id": "1611.06661", "submitter": "Yipei Wang", "authors": "Yan Xu, Yang Li, Yipei Wang, Mingyuan Liu, Yubo Fan, Maode Lai, Eric\n  I-Chao Chang", "title": "Gland Instance Segmentation Using Deep Multichannel Neural Networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1607.04889", "journal-ref": "IEEE Transactions on Biomedical Engineering, Volume: 64, Issue:\n  12, Dec. 2017, Pages: 2901 - 2912", "doi": "10.1109/TBME.2017.2686418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A new image instance segmentation method is proposed to segment\nindividual glands (instances) in colon histology images. This process is\nchallenging since the glands not only need to be segmented from a complex\nbackground, they must also be individually identified. Methods: We leverage the\nidea of image-to-image prediction in recent deep learning by designing an\nalgorithm that automatically exploits and fuses complex multichannel\ninformation - regional, location, and boundary cues - in gland histology\nimages. Our proposed algorithm, a deep multichannel framework, alleviates heavy\nfeature design due to the use of convolutional neural networks and is able to\nmeet multifarious requirements by altering channels. Results: Compared with\nmethods reported in the 2015 MICCAI Gland Segmentation Challenge and other\ncurrently prevalent instance segmentation methods, we observe state-of-the-art\nresults based on the evaluation metrics. Conclusion: The proposed deep\nmultichannel algorithm is an effective method for gland instance segmentation.\nSignificance: The generalization ability of our model not only enable the\nalgorithm to solve gland instance segmentation problems, but the channel is\nalso alternative that can be replaced for a specific task.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 06:13:20 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 10:29:43 GMT"}, {"version": "v3", "created": "Thu, 23 Nov 2017 10:57:50 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Xu", "Yan", ""], ["Li", "Yang", ""], ["Wang", "Yipei", ""], ["Liu", "Mingyuan", ""], ["Fan", "Yubo", ""], ["Lai", "Maode", ""], ["Chang", "Eric I-Chao", ""]]}, {"id": "1611.06674", "submitter": "Pragathi Praveena", "authors": "A. P. Prathosh, Pragathi Praveena, Lalit K. Mestha, Sanjay Bharadwaj", "title": "Estimation of respiratory pattern from video using selective ensemble\n  aggregation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2664048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-contact estimation of respiratory pattern (RP) and respiration rate (RR)\nhas multiple applications. Existing methods for RP and RR measurement fall into\none of the three categories - (i) estimation through nasal air flow\nmeasurement, (ii) estimation from video-based remote photoplethysmography, and\n(iii) estimation by measurement of motion induced by respiration using motion\ndetectors. These methods, however, require specialized sensors, are\ncomputationally expensive and/or critically depend on selection of a region of\ninterest (ROI) for processing. In this paper a general framework is described\nfor estimating a periodic signal driving noisy LTI channels connected in\nparallel with unknown dynamics. The method is then applied to derive a\ncomputationally inexpensive method for estimating RP using 2D cameras that does\nnot critically depend on ROI. Specifically, RP is estimated by imaging the\nchanges in the reflected light caused by respiration-induced motion. Each\nspatial location in the field of view of the camera is modeled as a\nnoise-corrupted linear time-invariant (LTI) measurement channel with unknown\nsystem dynamics, driven by a single generating respiratory signal. Estimation\nof RP is cast as a blind deconvolution problem and is solved through a method\ncomprising subspace projection and statistical aggregation. Experiments are\ncarried out on 31 healthy human subjects by generating multiple RPs and\ncomparing the proposed estimates with simultaneously acquired ground truth from\nan impedance pneumograph device. The proposed estimator agrees well with the\nground truth device in terms of correlation measures, despite variability in\nclothing pattern, angle of view and ROI.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 08:02:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Prathosh", "A. P.", ""], ["Praveena", "Pragathi", ""], ["Mestha", "Lalit K.", ""], ["Bharadwaj", "Sanjay", ""]]}, {"id": "1611.06678", "submitter": "Ali Diba", "authors": "Ali Diba, Vivek Sharma, Luc Van Gool", "title": "Deep Temporal Linear Encoding Networks", "comments": "Ali Diba and Vivek Sharma contributed equally to this work and listed\n  in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The CNN-encoding of features from entire videos for the representation of\nhuman actions has rarely been addressed. Instead, CNN work has focused on\napproaches to fuse spatial and temporal networks, but these were typically\nlimited to processing shorter sequences. We present a new video representation,\ncalled temporal linear encoding (TLE) and embedded inside of CNNs as a new\nlayer, which captures the appearance and motion throughout entire videos. It\nencodes this aggregated information into a robust video feature representation,\nvia end-to-end learning. Advantages of TLEs are: (a) they encode the entire\nvideo into a compact feature representation, learning the semantics and a\ndiscriminative feature space; (b) they are applicable to all kinds of networks\nlike 2D and 3D CNNs for video classification; and (c) they model feature\ninteractions in a more expressive way and without loss of information. We\nconduct experiments on two challenging human action datasets: HMDB51 and\nUCF101. The experiments show that TLE outperforms current state-of-the-art\nmethods on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 08:27:31 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Diba", "Ali", ""], ["Sharma", "Vivek", ""], ["Van Gool", "Luc", ""]]}, {"id": "1611.06683", "submitter": "HImanshu Aggarwal", "authors": "Himanshu Aggarwal, Dinesh K. Vishwakarma", "title": "Covariate conscious approach for Gait recognition based upon Zernike\n  moment invariants", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait recognition i.e. identification of an individual from his/her walking\npattern is an emerging field. While existing gait recognition techniques\nperform satisfactorily in normal walking conditions, there performance tend to\nsuffer drastically with variations in clothing and carrying conditions. In this\nwork, we propose a novel covariate cognizant framework to deal with the\npresence of such covariates. We describe gait motion by forming a single 2D\nspatio-temporal template from video sequence, called Average Energy Silhouette\nimage (AESI). Zernike moment invariants (ZMIs) are then computed to screen the\nparts of AESI infected with covariates. Following this, features are extracted\nfrom Spatial Distribution of Oriented Gradients (SDOGs) and novel Mean of\nDirectional Pixels (MDPs) methods. The obtained features are fused together to\nform the final well-endowed feature set. Experimental evaluation of the\nproposed framework on three publicly available datasets i.e. CASIA dataset B,\nOU-ISIR Treadmill dataset B and USF Human-ID challenge dataset with recently\npublished gait recognition approaches, prove its superior performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 08:48:47 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Aggarwal", "Himanshu", ""], ["Vishwakarma", "Dinesh K.", ""]]}, {"id": "1611.06689", "submitter": "Jiali Duan", "authors": "Jiali Duan, Shuai Zhou, Jun Wan, Xiaoyuan Guo, and Stan Z. Li", "title": "Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for\n  Isolated Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the popularity of depth-sensors such as Kinect has made depth\nvideos easily available while its advantages have not been fully exploited.\nThis paper investigates, for gesture recognition, to explore the spatial and\ntemporal information complementarily embedded in RGB and depth sequences. We\npropose a convolutional twostream consensus voting network (2SCVN) which\nexplicitly models both the short-term and long-term structure of the RGB\nsequences. To alleviate distractions from background, a 3d depth-saliency\nConvNet stream (3DDSN) is aggregated in parallel to identify subtle motion\ncharacteristics. These two components in an unified framework significantly\nimprove the recognition accuracy. On the challenging Chalearn IsoGD benchmark,\nour proposed method outperforms the first place on the leader-board by a large\nmargin (10.29%) while also achieving the best result on RGBD-HuDaAct dataset\n(96.74%). Both quantitative experiments and qualitative analysis shows the\neffectiveness of our proposed framework and codes will be released to\nfacilitate future research.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 09:16:21 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 08:16:27 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Duan", "Jiali", ""], ["Zhou", "Shuai", ""], ["Wan", "Jun", ""], ["Guo", "Xiaoyuan", ""], ["Li", "Stan Z.", ""]]}, {"id": "1611.06694", "submitter": "Akshayvarun Subramanya", "authors": "Suraj Srinivas, Akshayvarun Subramanya, R. Venkatesh Babu", "title": "Training Sparse Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks with lots of parameters are typically used for\nlarge-scale computer vision tasks such as image classification. This is a\nresult of using dense matrix multiplications and convolutions. However, sparse\ncomputations are known to be much more efficient. In this work, we train and\nbuild neural networks which implicitly use sparse computations. We introduce\nadditional gate variables to perform parameter selection and show that this is\nequivalent to using a spike-and-slab prior. We experimentally validate our\nmethod on both small and large networks and achieve state-of-the-art\ncompression results for sparse neural network models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 09:24:24 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Srinivas", "Suraj", ""], ["Subramanya", "Akshayvarun", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1611.06748", "submitter": "Di Kang", "authors": "Di Kang, Debarun Dhar, Antoni B. Chan", "title": "Crowd Counting by Adapting Convolutional Neural Networks with Side\n  Information", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision tasks often have side information available that is helpful\nto solve the task. For example, for crowd counting, the camera perspective\n(e.g., camera angle and height) gives a clue about the appearance and scale of\npeople in the scene. While side information has been shown to be useful for\ncounting systems using traditional hand-crafted features, it has not been fully\nutilized in counting systems based on deep learning. In order to incorporate\nthe available side information, we propose an adaptive convolutional neural\nnetwork (ACNN), where the convolutional filter weights adapt to the current\nscene context via the side information. In particular, we model the filter\nweights as a low-dimensional manifold, parametrized by the side information,\nwithin the high-dimensional space of filter weights. With the help of side\ninformation and adaptive weights, the ACNN can disentangle the variations\nrelated to the side information, and extract discriminative features related to\nthe current context. Since existing crowd counting datasets do not contain\nground-truth side information, we collect a new dataset with the ground-truth\ncamera angle and height as the side information. On experiments in crowd\ncounting, the ACNN improves counting accuracy compared to a plain CNN with a\nsimilar number of parameters. We also apply ACNN to image deconvolution to show\nits potential effectiveness on other computer vision applications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:09:06 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Kang", "Di", ""], ["Dhar", "Debarun", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1611.06757", "submitter": "Stamatios Lefkimmiatis", "authors": "Stamatios Lefkimmiatis", "title": "Non-Local Color Image Denoising with Convolutional Neural Networks", "comments": "15 pages, accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep network architecture for grayscale and color image\ndenoising that is based on a non-local image model. Our motivation for the\noverall design of the proposed network stems from variational methods that\nexploit the inherent non-local self-similarity property of natural images. We\nbuild on this concept and introduce deep networks that perform non-local\nprocessing and at the same time they significantly benefit from discriminative\nlearning. Experiments on the Berkeley segmentation dataset, comparing several\nstate-of-the-art methods, show that the proposed non-local models achieve the\nbest reported denoising performance both for grayscale and color images for all\nthe tested noise levels. It is also worth noting that this increase in\nperformance comes at no extra cost on the capacity of the network compared to\nexisting alternative deep network architectures. In addition, we highlight a\ndirect link of the proposed non-local models to convolutional neural networks.\nThis connection is of significant importance since it allows our models to take\nfull advantage of the latest advances on GPU computing in deep learning and\nmakes them amenable to efficient implementations through their inherent\nparallelism.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:36:10 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 20:06:26 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1611.06764", "submitter": "Mahdyar Ravanbakhsh", "authors": "Mahdyar Ravanbakhsh, Hossein Mousavi, Moin Nabi, Lucio Marcenaro,\n  Carlo Regazzoni", "title": "Efficient Convolutional Neural Network with Binary Quantization Layer", "comments": "Workshop on Efficient Methods for Deep Neural Networks (EMDNN), NIPS\n  2016, Barcelona, Spain. arXiv admin note: substantial text overlap with\n  arXiv:1609.09220", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method for segmentation that can benefit\nfrom general semantics of Convolutional Neural Network (CNN). Our segmentation\nproposes visually and semantically coherent image segments. We use binary\nencoding of CNN features to overcome the difficulty of the clustering on the\nhigh-dimensional CNN feature space. These binary encoding can be embedded into\nthe CNN as an extra layer at the end of the network. This results in real-time\nsegmentation. To the best of our knowledge our method is the first attempt on\ngeneral semantic image segmentation using CNN. All the previous papers were\nlimited to few number of category of the images (e.g. PASCAL VOC). Experiments\nshow that our segmentation algorithm outperform the state-of-the-art\nnon-semantic segmentation methods by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:55:30 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Ravanbakhsh", "Mahdyar", ""], ["Mousavi", "Hossein", ""], ["Nabi", "Moin", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1611.06777", "submitter": "Fengfu Li", "authors": "Fengfu Li, Hong Qiao, and Bo Zhang", "title": "Effective Deterministic Initialization for $k$-Means-Like Methods via\n  Local Density Peaks Searching", "comments": "16 pages, 9 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means clustering algorithm is popular but has the following main\ndrawbacks: 1) the number of clusters, $k$, needs to be provided by the user in\nadvance, 2) it can easily reach local minima with randomly selected initial\ncenters, 3) it is sensitive to outliers, and 4) it can only deal with well\nseparated hyperspherical clusters. In this paper, we propose a Local Density\nPeaks Searching (LDPS) initialization framework to address these issues. The\nLDPS framework includes two basic components: one of them is the local density\nthat characterizes the density distribution of a data set, and the other is the\nlocal distinctiveness index (LDI) which we introduce to characterize how\ndistinctive a data point is compared with its neighbors. Based on these two\ncomponents, we search for the local density peaks which are characterized with\nhigh local densities and high LDIs to deal with 1) and 2). Moreover, we detect\noutliers characterized with low local densities but high LDIs, and exclude them\nout before clustering begins. Finally, we apply the LDPS initialization\nframework to $k$-medoids, which is a variant of $k$-means and chooses data\nsamples as centers, with diverse similarity measures other than the Euclidean\ndistance to fix the last drawback of $k$-means. Combining the LDPS\ninitialization framework with $k$-means and $k$-medoids, we obtain two novel\nclustering methods called LDPS-means and LDPS-medoids, respectively.\nExperiments on synthetic data sets verify the effectiveness of the proposed\nmethods, especially when the ground truth of the cluster number $k$ is large.\nFurther, experiments on several real world data sets, Handwritten Pendigits,\nCoil-20, Coil-100 and Olivetti Face Database, illustrate that our methods give\na superior performance than the analogous approaches on both estimating $k$ and\nunsupervised object categorization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 13:26:37 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Li", "Fengfu", ""], ["Qiao", "Hong", ""], ["Zhang", "Bo", ""]]}, {"id": "1611.06779", "submitter": "Minghui Liao", "authors": "Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, Wenyu Liu", "title": "TextBoxes: A Fast Text Detector with a Single Deep Neural Network", "comments": "Accepted by AAAI2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an end-to-end trainable fast scene text detector, named\nTextBoxes, which detects scene text with both high accuracy and efficiency in a\nsingle network forward pass, involving no post-process except for a standard\nnon-maximum suppression. TextBoxes outperforms competing methods in terms of\ntext localization accuracy and is much faster, taking only 0.09s per image in a\nfast implementation. Furthermore, combined with a text recognizer, TextBoxes\nsignificantly outperforms state-of-the-art approaches on word spotting and\nend-to-end text recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 13:35:15 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Liao", "Minghui", ""], ["Shi", "Baoguang", ""], ["Bai", "Xiang", ""], ["Wang", "Xinggang", ""], ["Liu", "Wenyu", ""]]}, {"id": "1611.06791", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, R. Venkatesh Babu", "title": "Generalized Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks often require good regularizers to generalize well.\nDropout is one such regularizer that is widely used among Deep Learning\npractitioners. Recent work has shown that Dropout can also be viewed as\nperforming Approximate Bayesian Inference over the network parameters. In this\nwork, we generalize this notion and introduce a rich family of regularizers\nwhich we call Generalized Dropout. One set of methods in this family, called\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\nemerges as a special case of this method. Another member of this family selects\nthe width of neural network layers. Experiments show that these methods help in\nimproving generalization performance over Dropout.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:06:48 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1611.06878", "submitter": "Heng Fan", "authors": "Heng Fan and Haibin Ling", "title": "SANet: Structure-Aware Network for Visual Tracking", "comments": "In CVPR Deep Vision Workshop, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) has drawn increasing interest in visual\ntracking owing to its powerfulness in feature extraction. Most existing\nCNN-based trackers treat tracking as a classification problem. However, these\ntrackers are sensitive to similar distractors because their CNN models mainly\nfocus on inter-class classification. To address this problem, we use\nself-structure information of object to distinguish it from distractors.\nSpecifically, we utilize recurrent neural network (RNN) to model object\nstructure, and incorporate it into CNN to improve its robustness to similar\ndistractors. Considering that convolutional layers in different levels\ncharacterize the object from different perspectives, we use multiple RNNs to\nmodel object structure in different levels respectively. Extensive experiments\non three benchmarks, OTB100, TC-128 and VOT2015, show that the proposed\nalgorithm outperforms other methods. Code is released at\nhttp://www.dabi.temple.edu/~hbling/code/SANet/SANet.html.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:19:30 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 13:40:32 GMT"}, {"version": "v3", "created": "Mon, 1 May 2017 17:43:07 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Fan", "Heng", ""], ["Ling", "Haibin", ""]]}, {"id": "1611.06880", "submitter": "Hannah Dee", "authors": "Jonathan Bell and Hannah M. Dee", "title": "The subset-matched Jaccard index for evaluation of Segmentation for\n  Plant Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new measure for the evaluation of region level segmentation of\nobjects, as applied to evaluating the accuracy of leaf-level segmentation of\nplant images. The proposed approach enforces the rule that a region (e.g. a\nleaf) in either the image being evaluated or the ground truth image evaluated\nagainst can be mapped to no more than one region in the other image. We call\nthis measure the subset-matched Jaccard index.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:21:20 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Bell", "Jonathan", ""], ["Dee", "Hannah M.", ""]]}, {"id": "1611.06906", "submitter": "Shekoufeh Gorgi Zadeh", "authors": "Shekoufeh Gorgi Zadeh, Stephan Didas, Maximilian W. M. Wintergerst,\n  Thomas Schultz", "title": "Multi-Scale Anisotropic Fourth-Order Diffusion Improves Ridge and Valley\n  Localization", "comments": "12 pages, 8 figures, 1 table", "journal-ref": "Journal of Mathematical Imaging and Vision, 1-13, 2017", "doi": "10.1007/s10851-017-0729-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge and valley enhancing filters are widely used in applications such as\nvessel detection in medical image computing. When images are degraded by noise\nor include vessels at different scales, such filters are an essential step for\nmeaningful and stable vessel localization. In this work, we propose a novel\nmulti-scale anisotropic fourth-order diffusion equation that allows us to\nsmooth along vessels, while sharpening them in the orthogonal direction. The\nproposed filter uses a fourth order diffusion tensor whose eigentensors and\neigenvalues are determined from the local Hessian matrix, at a scale that is\nautomatically selected for each pixel. We discuss efficient implementation\nusing a Fast Explicit Diffusion scheme and demonstrate results on synthetic\nimages and vessels in fundus images. Compared to previous isotropic and\nanisotropic fourth-order filters, as well as established second-order vessel\nenhancing filters, our newly proposed one better restores the centerlines in\nall cases.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 17:08:44 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 10:31:11 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zadeh", "Shekoufeh Gorgi", ""], ["Didas", "Stephan", ""], ["Wintergerst", "Maximilian W. M.", ""], ["Schultz", "Thomas", ""]]}, {"id": "1611.06939", "submitter": "Zeynettin Akkus", "authors": "Zeynettin Akkus, Issa Ali, Jiri Sedlar, Timothy L. Kline, Jay P.\n  Agrawal, Ian F. Parney, Caterina Giannini, Bradley J. Erickson", "title": "Predicting 1p19q Chromosomal Deletion of Low-Grade Gliomas from MR\n  Images using Deep Learning", "comments": "This work has been presented in Conference on Machine Intelligence in\n  Medical Imaging 2016 and RSNA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Several studies have associated codeletion of chromosome arms\n1p/19q in low-grade gliomas (LGG) with positive response to treatment and\nlonger progression free survival. Therefore, predicting 1p/19q status is\ncrucial for effective treatment planning of LGG. In this study, we predict the\n1p/19q status from MR images using convolutional neural networks (CNN), which\ncould be a noninvasive alternative to surgical biopsy and histopathological\nanalysis. Method: Our method consists of three main steps: image registration,\ntumor segmentation, and classification of 1p/19q status using CNN. We included\na total of 159 LGG with 3 image slices each who had biopsy-proven 1p/19q status\n(57 nondeleted and 102 codeleted) and preoperative postcontrast-T1 (T1C) and T2\nimages. We divided our data into training, validation, and test sets. The\ntraining data was balanced for equal class probability and then augmented with\niterations of random translational shift, rotation, and horizontal and vertical\nflips to increase the size of the training set. We shuffled and augmented the\ntraining data to counter overfitting in each epoch. Finally, we evaluated\nseveral configurations of a multi-scale CNN architecture until training and\nvalidation accuracies became consistent. Results: The results of the best\nperforming configuration on the unseen test set were 93.3% (sensitivity),\n82.22% (specificity), and 87.7% (accuracy). Conclusion: Multi-scale CNN with\ntheir self-learning capability provides promising results for predicting 1p/19q\nstatus noninvasively based on T1C and T2 images. Significance: Predicting\n1p/19q status noninvasively from MR images would allow selecting effective\ntreatment strategies for LGG patients without the need for surgical biopsy.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:43:20 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Akkus", "Zeynettin", ""], ["Ali", "Issa", ""], ["Sedlar", "Jiri", ""], ["Kline", "Timothy L.", ""], ["Agrawal", "Jay P.", ""], ["Parney", "Ian F.", ""], ["Giannini", "Caterina", ""], ["Erickson", "Bradley J.", ""]]}, {"id": "1611.06949", "submitter": "Linjie Yang", "authors": "Linjie Yang, Kevin Tang, Jianchao Yang, Li-Jia Li", "title": "Dense Captioning with Joint Inference and Visual Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense captioning is a newly emerging computer vision topic for understanding\nimages with dense language descriptions. The goal is to densely detect visual\nconcepts (e.g., objects, object parts, and interactions between them) from\nimages, labeling each with a short descriptive phrase. We identify two key\nchallenges of dense captioning that need to be properly addressed when tackling\nthe problem. First, dense visual concept annotations in each image are\nassociated with highly overlapping target regions, making accurate localization\nof each visual concept challenging. Second, the large amount of visual concepts\nmakes it hard to recognize each of them by appearance alone. We propose a new\nmodel pipeline based on two novel ideas, joint inference and context fusion, to\nalleviate these two challenges. We design our model architecture in a\nmethodical manner and thoroughly evaluate the variations in architecture. Our\nfinal model, compact and efficient, achieves state-of-the-art accuracy on\nVisual Genome for dense captioning with a relative gain of 73\\% compared to the\nprevious best algorithm. Qualitative experiments also reveal the semantic\ncapabilities of our model in dense captioning.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:58:58 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 23:17:34 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Yang", "Linjie", ""], ["Tang", "Kevin", ""], ["Yang", "Jianchao", ""], ["Li", "Li-Jia", ""]]}, {"id": "1611.06950", "submitter": "Jie Mei", "authors": "Jie Mei, Aminul Islam, Yajing Wu, Abidalrahman Moh'd, Evangelos E.\n  Milios", "title": "Statistical Learning for OCR Text Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of Optical Character Recognition (OCR) is crucial to the success\nof subsequent applications used in text analyzing pipeline. Recent models of\nOCR post-processing significantly improve the quality of OCR-generated text,\nbut are still prone to suggest correction candidates from limited observations\nwhile insufficiently accounting for the characteristics of OCR errors. In this\npaper, we show how to enlarge candidate suggestion space by using external\ncorpus and integrating OCR-specific features in a regression approach to\ncorrect OCR-generated errors. The evaluation results show that our model can\ncorrect 61.5% of the OCR-errors (considering the top 1 suggestion) and 71.5% of\nthe OCR-errors (considering the top 3 suggestions), for cases where the\ntheoretical correction upper-bound is 78%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:00:32 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Mei", "Jie", ""], ["Islam", "Aminul", ""], ["Wu", "Yajing", ""], ["Moh'd", "Abidalrahman", ""], ["Milios", "Evangelos E.", ""]]}, {"id": "1611.06962", "submitter": "Karl Ni", "authors": "Karl Ni, Kyle Zaragoza, Charles Foster, Carmen Carrano, Barry Chen,\n  Yonas Tesfaye, Alex Gude", "title": "Sampled Image Tagging and Retrieval Methods on User Generated Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional image tagging and retrieval algorithms have limited value as a\nresult of being trained with heavily curated datasets. These limitations are\nmost evident when arbitrary search words are used that do not intersect with\ntraining set labels. Weak labels from user generated content (UGC) found in the\nwild (e.g., Google Photos, FlickR, etc.) have an almost unlimited number of\nunique words in the metadata tags. Prior work on word embeddings successfully\nleveraged unstructured text with large vocabularies, and our proposed method\nseeks to apply similar cost functions to open source imagery. Specifically, we\ntrain a deep learning image tagging and retrieval system on large scale, user\ngenerated content (UGC) using sampling methods and joint optimization of word\nembeddings. By using the Yahoo! FlickR Creative Commons (YFCC100M) dataset,\nsuch an approach builds robustness to common unstructured data issues that\ninclude but are not limited to irrelevant tags, misspellings, multiple\nlanguages, polysemy, and tag imbalance. As a result, the final proposed\nalgorithm will not only yield comparable results to state of the art in\nconventional image tagging, but will enable new capability to train algorithms\non large, scale unstructured text in the YFCC100M dataset and outperform cited\nwork in zero-shot capability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:24:58 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 01:32:21 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2016 20:52:40 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Ni", "Karl", ""], ["Zaragoza", "Kyle", ""], ["Foster", "Charles", ""], ["Carrano", "Carmen", ""], ["Chen", "Barry", ""], ["Tesfaye", "Yonas", ""], ["Gude", "Alex", ""]]}, {"id": "1611.06969", "submitter": "Raphael Felipe Prates", "authors": "Raphael Prates and William Robson Schwartz", "title": "Kernel Cross-View Collaborative Representation based Classification for\n  Person Re-Identification", "comments": "Paper submitted to CVPR 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims at the maintenance of a global identity as a\nperson moves among non-overlapping surveillance cameras. It is a hard task due\nto different illumination conditions, viewpoints and the small number of\nannotated individuals from each pair of cameras (small-sample-size problem).\nCollaborative Representation based Classification (CRC) has been employed\nsuccessfully to address the small-sample-size problem in computer vision.\nHowever, the original CRC formulation is not well-suited for person\nre-identification since it does not consider that probe and gallery samples are\nfrom different cameras. Furthermore, it is a linear model, while appearance\nchanges caused by different camera conditions indicate a strong nonlinear\ntransition between cameras. To overcome such limitations, we propose the Kernel\nCross-View Collaborative Representation based Classification (Kernel X-CRC)\nthat represents probe and gallery images by balancing representativeness and\nsimilarity nonlinearly. It assumes that a probe and its corresponding gallery\nimage are represented with similar coding vectors using individuals from the\ntraining set. Experimental results demonstrate that our assumption is true when\nusing a high-dimensional feature vector and becomes more compelling when\ndealing with a low-dimensional and discriminative representation computed using\na common subspace learning method. We achieve state-of-the-art for rank-1\nmatching rates in two person re-identification datasets (PRID450S and GRID) and\nthe second best results on VIPeR and CUHK01 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:44:50 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Prates", "Raphael", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1611.06973", "submitter": "Seymour Knowles-Barley", "authors": "Seymour Knowles-Barley, Verena Kaynig, Thouis Ray Jones, Alyssa\n  Wilson, Joshua Morgan, Dongil Lee, Daniel Berger, Narayanan Kasthuri, Jeff W.\n  Lichtman, Hanspeter Pfister", "title": "RhoanaNet Pipeline: Dense Automatic Neural Annotation", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstructing a synaptic wiring diagram, or connectome, from electron\nmicroscopy (EM) images of brain tissue currently requires many hours of manual\nannotation or proofreading (Kasthuri and Lichtman, 2010; Lichtman and Sanes,\n2008; Seung, 2009). The desire to reconstruct ever larger and more complex\nnetworks has pushed the collection of ever larger EM datasets. A cubic\nmillimeter of raw imaging data would take up 1 PB of storage and present an\nannotation project that would be impractical without relying heavily on\nautomatic segmentation methods. The RhoanaNet image processing pipeline was\ndeveloped to automatically segment large volumes of EM data and ease the burden\nof manual proofreading and annotation. Based on (Kaynig et al., 2015), we\nupdated every stage of the software pipeline to provide better throughput\nperformance and higher quality segmentation results. We used state of the art\ndeep learning techniques to generate improved membrane probability maps, and\nGala (Nunez-Iglesias et al., 2014) was used to agglomerate 2D segments into 3D\nobjects.\n  We applied the RhoanaNet pipeline to four densely annotated EM datasets, two\nfrom mouse cortex, one from cerebellum and one from mouse lateral geniculate\nnucleus (LGN). All training and test data is made available for benchmark\ncomparisons. The best segmentation results obtained gave\n$V^\\text{Info}_\\text{F-score}$ scores of 0.9054 and 09182 for the cortex\ndatasets, 0.9438 for LGN, and 0.9150 for Cerebellum.\n  The RhoanaNet pipeline is open source software. All source code, training\ndata, test data, and annotations for all four benchmark datasets are available\nat www.rhoana.org.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:48:29 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Knowles-Barley", "Seymour", ""], ["Kaynig", "Verena", ""], ["Jones", "Thouis Ray", ""], ["Wilson", "Alyssa", ""], ["Morgan", "Joshua", ""], ["Lee", "Dongil", ""], ["Berger", "Daniel", ""], ["Kasthuri", "Narayanan", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1611.06981", "submitter": "Nathan Cahill", "authors": "Nathan D. Cahill, Harmeet Singh, Chao Zhang, Daryl A. Corcoran, Alison\n  M. Prengaman, Paul S. Wenger, John F. Hamilton, Peter Bajorski, and Andrew M.\n  Michael", "title": "Multiple-View Spectral Clustering for Group-wise Functional Community\n  Detection", "comments": "Presented at The MICCAI-BACON 16 Workshop\n  (https://arxiv.org/abs/1611.03363)", "journal-ref": null, "doi": null, "report-no": "BACON/2016/01", "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity analysis yields powerful insights into our\nunderstanding of the human brain. Group-wise functional community detection\naims to partition the brain into clusters, or communities, in which functional\nactivity is inter-regionally correlated in a common manner across a group of\nsubjects. In this article, we show how to use multiple-view spectral clustering\nto perform group-wise functional community detection. In a series of\nexperiments on 291 subjects from the Human Connectome Project, we compare three\nversions of multiple-view spectral clustering: MVSC (uniform weights), MVSCW\n(weights based on subject-specific embedding quality), and AASC (weights\noptimized along with the embedding) with the competing technique of Joint\nDiagonalization of Laplacians (JDL). Results show that multiple-view spectral\nclustering not only yields group-wise functional communities that are more\nconsistent than JDL when using randomly selected subsets of individual brains,\nbut it is several orders of magnitude faster than JDL.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:57:48 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Cahill", "Nathan D.", ""], ["Singh", "Harmeet", ""], ["Zhang", "Chao", ""], ["Corcoran", "Daryl A.", ""], ["Prengaman", "Alison M.", ""], ["Wenger", "Paul S.", ""], ["Hamilton", "John F.", ""], ["Bajorski", "Peter", ""], ["Michael", "Andrew M.", ""]]}, {"id": "1611.06987", "submitter": "Thomas M\\\"ollenhoff", "authors": "Thomas M\\\"ollenhoff, Daniel Cremers", "title": "Sublabel-Accurate Discretization of Nonconvex Free-Discontinuity\n  Problems", "comments": "ICCV 2017 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we show how sublabel-accurate multilabeling approaches can be\nderived by approximating a classical label-continuous convex relaxation of\nnonconvex free-discontinuity problems. This insight allows to extend these\nsublabel-accurate approaches from total variation to general convex and\nnonconvex regularizations. Furthermore, it leads to a systematic approach to\nthe discretization of continuous convex relaxations. We study the relationship\nto existing discretizations and to discrete-continuous MRFs. Finally, we apply\nthe proposed approach to obtain a sublabel-accurate and convex solution to the\nvectorial Mumford-Shah functional and show in several experiments that it leads\nto more precise solutions using fewer labels.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:09:18 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 07:46:22 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["M\u00f6llenhoff", "Thomas", ""], ["Cremers", "Daniel", ""]]}, {"id": "1611.07004", "submitter": "Jun-Yan Zhu", "authors": "Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros", "title": "Image-to-Image Translation with Conditional Adversarial Networks", "comments": "Website: https://phillipi.github.io/pix2pix/, CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate conditional adversarial networks as a general-purpose solution\nto image-to-image translation problems. These networks not only learn the\nmapping from input image to output image, but also learn a loss function to\ntrain this mapping. This makes it possible to apply the same generic approach\nto problems that traditionally would require very different loss formulations.\nWe demonstrate that this approach is effective at synthesizing photos from\nlabel maps, reconstructing objects from edge maps, and colorizing images, among\nother tasks. Indeed, since the release of the pix2pix software associated with\nthis paper, a large number of internet users (many of them artists) have posted\ntheir own experiments with our system, further demonstrating its wide\napplicability and ease of adoption without the need for parameter tweaking. As\na community, we no longer hand-engineer our mapping functions, and this work\nsuggests we can achieve reasonable results without hand-engineering our loss\nfunctions either.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:48:16 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:48:39 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 13:54:12 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Isola", "Phillip", ""], ["Zhu", "Jun-Yan", ""], ["Zhou", "Tinghui", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1611.07119", "submitter": "Chongxuan Li", "authors": "Chongxuan Li and Jun Zhu and Bo Zhang", "title": "Max-Margin Deep Generative Models for (Semi-)Supervised Learning", "comments": "arXiv admin note: substantial text overlap with arXiv:1504.06787", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models (DGMs) are effective on learning multilayered\nrepresentations of complex data and performing inference of input data by\nexploring the generative ability. However, it is relatively insufficient to\nempower the discriminative ability of DGMs on making accurate predictions. This\npaper presents max-margin deep generative models (mmDGMs) and a\nclass-conditional variant (mmDCGMs), which explore the strongly discriminative\nprinciple of max-margin learning to improve the predictive performance of DGMs\nin both supervised and semi-supervised learning, while retaining the generative\ncapability. In semi-supervised learning, we use the predictions of a max-margin\nclassifier as the missing labels instead of performing full posterior inference\nfor efficiency; we also introduce additional max-margin and label-balance\nregularization terms of unlabeled data for effectiveness. We develop an\nefficient doubly stochastic subgradient algorithm for the piecewise linear\nobjectives in different settings. Empirical results on various datasets\ndemonstrate that: (1) max-margin learning can significantly improve the\nprediction performance of DGMs and meanwhile retain the generative ability; (2)\nin supervised learning, mmDGMs are competitive to the best fully discriminative\nnetworks when employing convolutional neural networks as the generative and\nrecognition models; and (3) in semi-supervised learning, mmDCGMs can perform\nefficient inference and achieve state-of-the-art classification results on\nseveral benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 01:36:29 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1611.07136", "submitter": "Masaharu Sakamoto", "authors": "Masaharu Sakamoto, Hiroki Nakano", "title": "Cascaded Neural Networks with Selective Classifiers and its evaluation\n  using Lung X-ray CT Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodule detection is a class imbalanced problem because nodules are found\nwith much lower frequency than non-nodules. In the class imbalanced problem,\nconventional classifiers tend to be overwhelmed by the majority class and\nignore the minority class. We therefore propose cascaded convolutional neural\nnetworks to cope with the class imbalanced problem. In the proposed approach,\ncascaded convolutional neural networks that perform as selective classifiers\nfilter out obvious non-nodules. Successively, a convolutional neural network\ntrained with a balanced data set calculates nodule probabilities. The proposed\nmethod achieved the detection sensitivity of 85.3% and 90.7% at 1 and 4 false\npositives per scan in FROC curve, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 03:21:05 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Sakamoto", "Masaharu", ""], ["Nakano", "Hiroki", ""]]}, {"id": "1611.07143", "submitter": "Yipei Wang", "authors": "Yan Xu, Zhengyang Shen, Xin Zhang, Yifan Gao, Shujian Deng, Yipei\n  Wang, Yubo Fan, Eric I-Chao Chang", "title": "Learning Multi-level Features For Sensor-based Human Action Recognition", "comments": "26 pages, 23 figures", "journal-ref": "Pervasive and Mobile Computing, Volume 40, September 2017, Pages\n  324-338", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multi-level feature learning framework for human action\nrecognition using a single body-worn inertial sensor. The framework consists of\nthree phases, respectively designed to analyze signal-based (low-level),\ncomponents (mid-level) and semantic (high-level) information. Low-level\nfeatures capture the time and frequency domain property while mid-level\nrepresentations learn the composition of the action. The Max-margin Latent\nPattern Learning (MLPL) method is proposed to learn high-level semantic\ndescriptions of latent action patterns as the output of our framework. The\nproposed method achieves the state-of-the-art performances, 88.7%, 98.8% and\n72.6% (weighted F1 score) respectively, on Skoda, WISDM and OPP datasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 04:24:00 GMT"}, {"version": "v2", "created": "Sat, 2 Sep 2017 17:57:21 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Xu", "Yan", ""], ["Shen", "Zhengyang", ""], ["Zhang", "Xin", ""], ["Gao", "Yifan", ""], ["Deng", "Shujian", ""], ["Wang", "Yipei", ""], ["Fan", "Yubo", ""], ["Chang", "Eric I-Chao", ""]]}, {"id": "1611.07145", "submitter": "Tianrong Rao", "authors": "Tianrong Rao, Min Xu, Dong Xu", "title": "Learning Multi-level Deep Representations for Image Emotion\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new deep network that learns multi-level deep\nrepresentations for image emotion classification (MldrNet). Image emotion can\nbe recognized through image semantics, image aesthetics and low-level visual\nfeatures from both global and local views. Existing image emotion\nclassification works using hand-crafted features or deep features mainly focus\non either low-level visual features or semantic-level image representations\nwithout taking all factors into consideration. The proposed MldrNet combines\ndeep representations of different levels, i.e. image semantics, image\naesthetics, and low-level visual features to effectively classify the emotion\ntypes of different kinds of images, such as abstract paintings and web images.\nExtensive experiments on both Internet images and abstract paintings\ndemonstrate the proposed method outperforms the state-of-the-art methods using\ndeep features or hand-crafted features. The proposed approach also outperforms\nthe state-of-the-art methods with at least 6% performance improvement in terms\nof overall classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 05:12:19 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 10:18:23 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Rao", "Tianrong", ""], ["Xu", "Min", ""], ["Xu", "Dong", ""]]}, {"id": "1611.07156", "submitter": "Yazhou Yao", "authors": "Yazhou Yao, Jian Zhang, Fumin Shen, Xiansheng Hua, Jingsong Xu and\n  Zhenmin Tang", "title": "Exploiting Web Images for Dataset Construction: A Domain Robust Approach", "comments": "Journal", "journal-ref": null, "doi": "10.1109/TMM.2017.2684626", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelled image datasets have played a critical role in high-level image\nunderstanding. However, the process of manual labelling is both time-consuming\nand labor intensive. To reduce the cost of manual labelling, there has been\nincreased research interest in automatically constructing image datasets by\nexploiting web images. Datasets constructed by existing methods tend to have a\nweak domain adaptation ability, which is known as the \"dataset bias problem\".\nTo address this issue, we present a novel image dataset construction framework\nthat can be generalized well to unseen target domains. Specifically, the given\nqueries are first expanded by searching the Google Books Ngrams Corpus to\nobtain a rich semantic description, from which the visually non-salient and\nless relevant expansions are filtered out. By treating each selected expansion\nas a \"bag\" and the retrieved images as \"instances\", image selection can be\nformulated as a multi-instance learning problem with constrained positive bags.\nWe propose to solve the employed problems by the cutting-plane and\nconcave-convex procedure (CCCP) algorithm. By using this approach, images from\ndifferent distributions can be kept while noisy images are filtered out. To\nverify the effectiveness of our proposed approach, we build an image dataset\nwith 20 categories. Extensive experiments on image classification,\ncross-dataset generalization, diversity comparison and object detection\ndemonstrate the domain robustness of our dataset.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 06:22:19 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 23:53:20 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 22:54:15 GMT"}, {"version": "v4", "created": "Tue, 28 Mar 2017 06:30:41 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Yao", "Yazhou", ""], ["Zhang", "Jian", ""], ["Shen", "Fumin", ""], ["Hua", "Xiansheng", ""], ["Xu", "Jingsong", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1611.07191", "submitter": "Nan Hu", "authors": "Nan Hu, Qixing Huang, Boris Thibert, Leonidas Guibas", "title": "Distributable Consistent Multi-Object Matching", "comments": "Final version for CVPR2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an optimization-based framework to multiple object\nmatching. The framework takes maps computed between pairs of objects as input,\nand outputs maps that are consistent among all pairs of objects. The central\nidea of our approach is to divide the input object collection into overlapping\nsub-collections and enforce map consistency among each sub-collection. This\nleads to a distributed formulation, which is scalable to large-scale datasets.\nWe also present an equivalence condition between this decoupled scheme and the\noriginal scheme. Experiments on both synthetic and real-world datasets show\nthat our framework is competitive against state-of-the-art multi-object\nmatching techniques.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 08:21:38 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 01:41:47 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 18:57:46 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Hu", "Nan", ""], ["Huang", "Qixing", ""], ["Thibert", "Boris", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1611.07212", "submitter": "Albert Haque", "authors": "Albert Haque, Alexandre Alahi, Li Fei-Fei", "title": "Recurrent Attention Models for Depth-Based Person Identification", "comments": "Computer Vision and Pattern Recognition (CVPR) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an attention-based model that reasons on human body shape and\nmotion dynamics to identify individuals in the absence of RGB information,\nhence in the dark. Our approach leverages unique 4D spatio-temporal signatures\nto address the identification problem across days. Formulated as a\nreinforcement learning task, our model is based on a combination of\nconvolutional and recurrent neural networks with the goal of identifying small,\ndiscriminative regions indicative of human identity. We demonstrate that our\nmodel produces state-of-the-art results on several published datasets given\nonly depth images. We further study the robustness of our model towards\nviewpoint, appearance, and volumetric changes. Finally, we share insights\ngleaned from interpretable 2D, 3D, and 4D visualizations of our model's\nspatio-temporal attention.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 09:27:30 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Haque", "Albert", ""], ["Alahi", "Alexandre", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1611.07218", "submitter": "Harish Katti", "authors": "Harish Katti, Marius V. Peelen, S. P. Arun", "title": "Deep neural networks can be improved using human-derived contextual\n  expectations", "comments": "30 pages, 5 figures, 3 tables, 2 supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world objects occur in specific contexts. Such context has been shown to\nfacilitate detection by constraining the locations to search. But can context\ndirectly benefit object detection? To do so, context needs to be learned\nindependently from target features. This is impossible in traditional object\ndetection where classifiers are trained on images containing both target\nfeatures and surrounding context. In contrast, humans can learn context and\ntarget features separately, such as when we see highways without cars. Here we\nshow for the first time that human-derived scene expectations can be used to\nimprove object detection performance in machines. To measure contextual\nexpectations, we asked human subjects to indicate the scale, location and\nlikelihood at which cars or people might occur in scenes without these objects.\nHumans showed highly systematic expectations that we could accurately predict\nusing scene features. This allowed us to predict human expectations on novel\nscenes without requiring manual annotation. On augmenting deep neural networks\nwith predicted human expectations, we obtained substantial gains in accuracy\nfor detecting cars and people (1-3%) as well as on detecting associated objects\n(3-20%). In contrast, augmenting deep networks with other conventional features\nyielded far smaller gains. This improvement was due to relatively poor matches\nat highly likely locations being correctly labelled as target and conversely\nstrong matches at unlikely locations being correctly rejected as false alarms.\nTaken together, our results show that augmenting deep neural networks with\nhuman-derived context features improves their performance, suggesting that\nhumans learn scene context separately unlike deep networks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 09:45:01 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 09:17:23 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 06:33:50 GMT"}, {"version": "v4", "created": "Thu, 29 Mar 2018 00:59:25 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Katti", "Harish", ""], ["Peelen", "Marius V.", ""], ["Arun", "S. P.", ""]]}, {"id": "1611.07231", "submitter": "Qing Cheng", "authors": "Qing Cheng, Huiqing Liu, Huanfeng Shen, Penghai Wu, Liangpei Zhang", "title": "A Spatial and Temporal Non-Local Filter Based Data Fusion", "comments": null, "journal-ref": null, "doi": "10.1109/TGRS.2017.2692802", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trade-off in remote sensing instruments that balances the spatial\nresolution and temporal frequency limits our capacity to monitor spatial and\ntemporal dynamics effectively. The spatiotemporal data fusion technique is\nconsidered as a cost-effective way to obtain remote sensing data with both high\nspatial resolution and high temporal frequency, by blending observations from\nmultiple sensors with different advantages or characteristics. In this paper,\nwe develop the spatial and temporal non-local filter based fusion model\n(STNLFFM) to enhance the prediction capacity and accuracy, especially for\ncomplex changed landscapes. The STNLFFM method provides a new transformation\nrelationship between the fine-resolution reflectance images acquired from the\nsame sensor at different dates with the help of coarse-resolution reflectance\ndata, and makes full use of the high degree of spatiotemporal redundancy in the\nremote sensing image sequence to produce the final prediction. The proposed\nmethod was tested over both the Coleambally Irrigation Area study site and the\nLower Gwydir Catchment study site. The results show that the proposed method\ncan provide a more accurate and robust prediction, especially for heterogeneous\nlandscapes and temporally dynamic areas.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:08:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Cheng", "Qing", ""], ["Liu", "Huiqing", ""], ["Shen", "Huanfeng", ""], ["Wu", "Penghai", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1611.07233", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Pascal Hager, Luca Benini", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression\n  Artifact Suppression", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965927", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression algorithms are pervasively used to reduce the size of\nimages transmitted over the web and recorded on data storage media. However, we\npay for their high compression rate with visual artifacts degrading the user\nexperience. Deep convolutional neural networks have become a widespread tool to\naddress high-level computer vision tasks very successfully. Recently, they have\nfound their way into the areas of low-level computer vision and image\nprocessing to solve regression problems mostly with relatively shallow\nnetworks.\n  We present a novel 12-layer deep convolutional network for image compression\nartifact suppression with hierarchical skip connections and a multi-scale loss\nfunction. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an\nimprovement of up to 0.36 dB over the best previous ConvNet result. We show\nthat a network trained for a specific quality factor (QF) is resilient to the\nQF used to compress the input image - a single network trained for QF 60\nprovides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:11:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Hager", "Pascal", ""], ["Benini", "Luca", ""]]}, {"id": "1611.07245", "submitter": "Jos\\'e M. F\\'acil", "authors": "Jos\\'e M. F\\'acil, Alejo Concha, Luis Montesano and Javier Civera", "title": "Single-View and Multi-View Depth Fusion", "comments": "Accepted for publication in IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2017.2715400", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dense and accurate 3D mapping from a monocular sequence is a key technology\nfor several applications and still an open research area. This paper leverages\nrecent results on single-view CNN-based depth estimation and fuses them with\nmulti-view depth estimation. Both approaches present complementary strengths.\nMulti-view depth is highly accurate but only in high-texture areas and\nhigh-parallax cases. Single-view depth captures the local structure of\nmid-level regions, including texture-less areas, but the estimated depth lacks\nglobal coherence. The single and multi-view fusion we propose is challenging in\nseveral aspects. First, both depths are related by a deformation that depends\non the image content. Second, the selection of multi-view points of high\naccuracy might be difficult for low-parallax configurations. We present\ncontributions for both problems. Our results in the public datasets of NYUv2\nand TUM shows that our algorithm outperforms the individual single and\nmulti-view approaches. A video showing the key aspects of mapping in our Single\nand Multi-view depth proposal is available at https://youtu.be/ipc5HukTb4k\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:51:43 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 09:37:04 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["F\u00e1cil", "Jos\u00e9 M.", ""], ["Concha", "Alejo", ""], ["Montesano", "Luis", ""], ["Civera", "Javier", ""]]}, {"id": "1611.07285", "submitter": "Soumya Roy", "authors": "Soumya Roy, Vinay P. Namboodiri, Arijit Biswas", "title": "Active learning with version spaces for object detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an image, we would like to learn to detect objects belonging to\nparticular object categories. Common object detection methods train on large\nannotated datasets which are annotated in terms of bounding boxes that contain\nthe object of interest. Previous works on object detection model the problem as\na structured regression problem which ranks the correct bounding boxes more\nthan the background ones. In this paper we develop algorithms which actively\nobtain annotations from human annotators for a small set of images, instead of\nall images, thereby reducing the annotation effort. Towards this goal, we make\nthe following contributions: 1. We develop a principled version space based\nactive learning method that solves for object detection as a structured\nprediction problem in a weakly supervised setting 2. We also propose two\nvariants of the margin sampling strategy 3. We analyse the results on standard\nobject detection benchmarks that show that with only 20% of the data we can\nobtain more than 95% of the localization accuracy of full supervision. Our\nmethods outperform random sampling and the classical uncertainty-based active\nlearning algorithms like entropy\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 12:58:24 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 06:47:29 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Roy", "Soumya", ""], ["Namboodiri", "Vinay P.", ""], ["Biswas", "Arijit", ""]]}, {"id": "1611.07289", "submitter": "Bernhard Kainz", "authors": "Amir Alansary, Bernhard Kainz, Martin Rajchl, Maria Murgasova, Mellisa\n  Damodaram, David F.A. Lloyd, Alice Davidson, Steven G. McDonagh, Mary\n  Rutherford, Joseph V. Hajnal and Daniel Rueckert", "title": "PVR: Patch-to-Volume Reconstruction for Large Area Motion Correction of\n  Fetal MRI", "comments": "10 pages, 13 figures, submitted to IEEE Transactions on Medical\n  Imaging. v2: wadded funders acknowledgements to preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel method for the correction of motion\nartifacts that are present in fetal Magnetic Resonance Imaging (MRI) scans of\nthe whole uterus. Contrary to current slice-to-volume registration (SVR)\nmethods, requiring an inflexible anatomical enclosure of a single investigated\norgan, the proposed patch-to-volume reconstruction (PVR) approach is able to\nreconstruct a large field of view of non-rigidly deforming structures. It\nrelaxes rigid motion assumptions by introducing a specific amount of redundant\ninformation that is exploited with parallelized patch-wise optimization,\nsuper-resolution, and automatic outlier rejection. We further describe and\nprovide an efficient parallel implementation of PVR allowing its execution\nwithin reasonable time on commercially available graphics processing units\n(GPU), enabling its use in the clinical practice. We evaluate PVR's\ncomputational overhead compared to standard methods and observe improved\nreconstruction accuracy in presence of affine motion artifacts of approximately\n30% compared to conventional SVR in synthetic experiments. Furthermore, we have\nevaluated our method qualitatively and quantitatively on real fetal MRI data\nsubject to maternal breathing and sudden fetal movements. We evaluate\npeak-signal-to-noise ratio (PSNR), structural similarity index (SSIM), and\ncross correlation (CC) with respect to the originally acquired data and provide\na method for visual inspection of reconstruction uncertainty. With these\nexperiments we demonstrate successful application of PVR motion compensation to\nthe whole uterus, the human fetus, and the human placenta.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 13:21:14 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 17:54:39 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Alansary", "Amir", ""], ["Kainz", "Bernhard", ""], ["Rajchl", "Martin", ""], ["Murgasova", "Maria", ""], ["Damodaram", "Mellisa", ""], ["Lloyd", "David F. A.", ""], ["Davidson", "Alice", ""], ["McDonagh", "Steven G.", ""], ["Rutherford", "Mary", ""], ["Hajnal", "Joseph V.", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1611.07369", "submitter": "Georgina Hall", "authors": "Amir Ali Ahmadi, Georgina Hall, Ameesh Makadia, Vikas Sindhwani", "title": "Geometry of 3D Environments and Sum of Squares Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in robotics and computer vision, we study problems\nrelated to spatial reasoning of a 3D environment using sublevel sets of\npolynomials. These include: tightly containing a cloud of points (e.g.,\nrepresenting an obstacle) with convex or nearly-convex basic semialgebraic\nsets, computation of Euclidean distances between two such sets, separation of\ntwo convex basic semalgebraic sets that overlap, and tight containment of the\nunion of several basic semialgebraic sets with a single convex one. We use\nalgebraic techniques from sum of squares optimization that reduce all these\ntasks to semidefinite programs of small size and present numerical experiments\nin realistic scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:40:14 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 05:13:55 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 22:46:15 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Ahmadi", "Amir Ali", ""], ["Hall", "Georgina", ""], ["Makadia", "Ameesh", ""], ["Sindhwani", "Vikas", ""]]}, {"id": "1611.07385", "submitter": "Xiao Yang", "authors": "Xiao Yang, Dafang He, Wenyi Huang, Zihan Zhou, Alex Ororbia, Dan\n  Kifer, C. Lee Giles", "title": "Smart Library: Identifying Books in a Library using Richly Supervised\n  Deep Scene Text Reading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical library collections are valuable and long standing resources for\nknowledge and learning. However, managing books in a large bookshelf and\nfinding books on it often leads to tedious manual work, especially for large\nbook collections where books might be missing or misplaced. Recently, deep\nneural models, such as Convolutional Neural Networks (CNN) and Recurrent Neural\nNetworks (RNN) have achieved great success for scene text detection and\nrecognition. Motivated by these recent successes, we aim to investigate their\nviability in facilitating book management, a task that introduces further\nchallenges including large amounts of cluttered scene text, distortion, and\nvaried lighting conditions. In this paper, we present a library inventory\nbuilding and retrieval system based on scene text reading methods. We\nspecifically design our scene text recognition model using rich supervision to\naccelerate training and achieve state-of-the-art performance on several\nbenchmark datasets. Our proposed system has the potential to greatly reduce the\namount of human labor required in managing book inventories as well as the\nspace needed to store book information.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:12:03 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Yang", "Xiao", ""], ["He", "Dafang", ""], ["Huang", "Wenyi", ""], ["Zhou", "Zihan", ""], ["Ororbia", "Alex", ""], ["Kifer", "Dan", ""], ["Giles", "C. Lee", ""]]}, {"id": "1611.07390", "submitter": "M Klodt", "authors": "Maria Klodt and Raphael Hauser", "title": "3D Image Reconstruction from X-Ray Measurements with Overlap", "comments": "Published in Computer Vision - ECCV 2016. The final publication is\n  available at link.springer.com/chapter/10.1007/978-3-319-46466-4_2", "journal-ref": null, "doi": "10.1007/978-3-319-46466-4_2", "report-no": null, "categories": "physics.med-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D image reconstruction from a set of X-ray projections is an important image\nreconstruction problem, with applications in medical imaging, industrial\ninspection and airport security. The innovation of X-ray emitter arrays allows\nfor a novel type of X-ray scanners with multiple simultaneously emitting\nsources. However, two or more sources emitting at the same time can yield\nmeasurements from overlapping rays, imposing a new type of image reconstruction\nproblem based on nonlinear constraints. Using traditional linear reconstruction\nmethods, respective scanner geometries have to be implemented such that no rays\noverlap, which severely restricts the scanner design. We derive a new type of\n3D image reconstruction model with nonlinear constraints, based on measurements\nwith overlapping X-rays. Further, we show that the arising optimization problem\nis partially convex, and present an algorithm to solve it. Experiments show\nhighly improved image reconstruction results from both simulated and real-world\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:22:20 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Klodt", "Maria", ""], ["Hauser", "Raphael", ""]]}, {"id": "1611.07450", "submitter": "Michael Cogswell", "authors": "Ramprasaath R Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael\n  Cogswell, Devi Parikh, Dhruv Batra", "title": "Grad-CAM: Why did you say that?", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems. This is an extended abstract version of arXiv:1610.02391\n  (CVPR format)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique for making Convolutional Neural Network (CNN)-based\nmodels more transparent by visualizing input regions that are 'important' for\npredictions -- or visual explanations. Our approach, called Gradient-weighted\nClass Activation Mapping (Grad-CAM), uses class-specific gradient information\nto localize important regions. These localizations are combined with existing\npixel-space visualizations to create a novel high-resolution and\nclass-discriminative visualization called Guided Grad-CAM. These methods help\nbetter understand CNN-based models, including image captioning and visual\nquestion answering (VQA) models. We evaluate our visual explanations by\nmeasuring their ability to discriminate between classes, to inspire trust in\nhumans, and their correlation with occlusion maps. Grad-CAM provides a new way\nto understand CNN-based models.\n  We have released code, an online demo hosted on CloudCV, and a full version\nof this extended abstract.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 18:34:36 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 16:33:29 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Selvaraju", "Ramprasaath R", ""], ["Das", "Abhishek", ""], ["Vedantam", "Ramakrishna", ""], ["Cogswell", "Michael", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1611.07485", "submitter": "Qiangui Huang", "authors": "Qiangui Huang, Weiyue Wang, Kevin Zhou, Suya You, Ulrich Neumann", "title": "Scene Labeling using Gated Recurrent Units with Explicit Long Range\n  Conditioning", "comments": "updated version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN), as a powerful contextual dependency modeling\nframework, has been widely applied to scene labeling problems. However, this\nwork shows that directly applying traditional RNN architectures, which unfolds\na 2D lattice grid into a sequence, is not sufficient to model structure\ndependencies in images due to the \"impact vanishing\" problem. First, we give an\nempirical analysis about the \"impact vanishing\" problem. Then, a new RNN unit\nnamed Recurrent Neural Network with explicit long range conditioning (RNN-ELC)\nis designed to alleviate this problem. A novel neural network architecture is\nbuilt for scene labeling tasks where one of the variants of the new RNN unit,\nGated Recurrent Unit with Explicit Long-range Conditioning (GRU-ELC), is used\nto model multi scale contextual dependencies in images. We validate the use of\nGRU-ELC units with state-of-the-art performance on three standard scene\nlabeling datasets. Comprehensive experiments demonstrate that the new GRU-ELC\nunit benefits scene labeling problem a lot as it can encode longer contextual\ndependencies in images more effectively than traditional RNN units.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 19:43:24 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 05:12:44 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Huang", "Qiangui", ""], ["Wang", "Weiyue", ""], ["Zhou", "Kevin", ""], ["You", "Suya", ""], ["Neumann", "Ulrich", ""]]}, {"id": "1611.07492", "submitter": "Siddharth Narayanaswamy", "authors": "N. Siddharth and Brooks Paige and Alban Desmaison and Jan-Willem Van\n  de Meent and Frank Wood and Noah D. Goodman and Pushmeet Kohli and Philip\n  H.S. Torr", "title": "Inducing Interpretable Representations with Variational Autoencoders", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for incorporating structured graphical models in the\n\\emph{encoders} of variational autoencoders (VAEs) that allows us to induce\ninterpretable representations through approximate variational inference. This\nallows us to both perform reasoning (e.g. classification) under the structural\nconstraints of a given graphical model, and use deep generative models to deal\nwith messy, high-dimensional domains where it is often difficult to model all\nthe variation. Learning in this framework is carried out end-to-end with a\nvariational objective, applying to both unsupervised and semi-supervised\nschemes.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 20:04:59 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Siddharth", "N.", ""], ["Paige", "Brooks", ""], ["Desmaison", "Alban", ""], ["Van de Meent", "Jan-Willem", ""], ["Wood", "Frank", ""], ["Goodman", "Noah D.", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1611.07544", "submitter": "Qixiang Ye", "authors": "Qixiang Ye, Tianliang Zhang, Qiang Qiu, Baochang Zhang, Jie Chen,\n  Guillermo Sapiro", "title": "Self-learning Scene-specific Pedestrian Detectors using a Progressive\n  Latent Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a self-learning approach is proposed towards solving\nscene-specific pedestrian detection problem without any human' annotation\ninvolved. The self-learning approach is deployed as progressive steps of object\ndiscovery, object enforcement, and label propagation. In the learning\nprocedure, object locations in each frame are treated as latent variables that\nare solved with a progressive latent model (PLM). Compared with conventional\nlatent models, the proposed PLM incorporates a spatial regularization term to\nreduce ambiguities in object proposals and to enforce object localization, and\nalso a graph-based label propagation to discover harder instances in adjacent\nframes. With the difference of convex (DC) objective functions, PLM can be\nefficiently optimized with a concave-convex programming and thus guaranteeing\nthe stability of self-learning. Extensive experiments demonstrate that even\nwithout annotation the proposed self-learning approach outperforms weakly\nsupervised learning approaches, while achieving comparable performance with\ntransfer learning and fully supervised approaches.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 21:33:07 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Ye", "Qixiang", ""], ["Zhang", "Tianliang", ""], ["Qiu", "Qiang", ""], ["Zhang", "Baochang", ""], ["Chen", "Jie", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1611.07559", "submitter": "Chengwei Sang", "authors": "Chengwei Sang, Hong Sun, Quisong Xia", "title": "Sar image despeckling based on nonlocal similarity sparse decomposition", "comments": "5pages,5 figures,20 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter presents a method of synthetic aperture radar (SAR) image\ndespeckling aimed to preserve the detail information while suppressing speckle\nnoise. This method combines the nonlocal self-similarity partition and a\nproposed modified sparse decomposition. The nonlocal partition method groups a\nseries of structure-similarity data sets. Each data set has a good sparsity for\nlearning an over-complete dictionary in sparse representation. In the sparse\ndecomposition, we propose a novel method to identify principal atoms from\nover-complete dictionary to form a principal dictionary. Despeckling is\nperformed on each data set over the principal dictionary with principal atoms.\nExperimental results demonstrate that the proposed method can achieve high\nperformances in terms of both speckle noise reduction and structure details\npreservation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:28:37 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Sang", "Chengwei", ""], ["Sun", "Hong", ""], ["Xia", "Quisong", ""]]}, {"id": "1611.07571", "submitter": "Nikolay Savinov", "authors": "Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler and Marc\n  Pollefeys", "title": "Quad-networks: unsupervised learning to rank for interest point\n  detection", "comments": "Accepted at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several machine learning tasks require to represent the data using only a\nsparse set of interest points. An ideal detector is able to find the\ncorresponding interest points even if the data undergo a transformation typical\nfor a given domain. Since the task is of high practical interest in computer\nvision, many hand-crafted solutions were proposed. In this paper, we ask a\nfundamental question: can we learn such detectors from scratch? Since it is\noften unclear what points are \"interesting\", human labelling cannot be used to\nfind a truly unbiased solution. Therefore, the task requires an unsupervised\nformulation. We are the first to propose such a formulation: training a neural\nnetwork to rank points in a transformation-invariant manner. Interest points\nare then extracted from the top/bottom quantiles of this ranking. We validate\nour approach on two tasks: standard RGB image interest point detection and\nchallenging cross-modal interest point detection between RGB and depth images.\nWe quantitatively show that our unsupervised method performs better or on-par\nwith baselines.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:46:17 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 21:15:18 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Savinov", "Nikolay", ""], ["Seki", "Akihito", ""], ["Ladicky", "Lubor", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1611.07573", "submitter": "Manuel Isaac Martinez Torres", "authors": "Manuel Martinez, Monica Haurilet, Ziad Al-Halah, Makarand Tapaswi,\n  Rainer Stiefelhagen", "title": "Relaxed Earth Mover's Distances for Chain- and Tree-connected Spaces and\n  their use as a Loss Function in Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Earth Mover's Distance (EMD) computes the optimal cost of transforming\none distribution into another, given a known transport metric between them. In\ndeep learning, the EMD loss allows us to embed information during training\nabout the output space structure like hierarchical or semantic relations. This\nhelps in achieving better output smoothness and generalization. However EMD is\ncomputationally expensive.Moreover, solving EMD optimization problems usually\nrequire complex techniques like lasso. These properties limit the applicability\nof EMD-based approaches in large scale machine learning.\n  We address in this work the difficulties facing incorporation of EMD-based\nloss in deep learning frameworks. Additionally, we provide insight and novel\nsolutions on how to integrate such loss function in training deep neural\nnetworks. Specifically, we make three main contributions: (i) we provide an\nin-depth analysis of the fastest state-of-the-art EMD algorithm (Sinkhorn\nDistance) and discuss its limitations in deep learning scenarios. (ii) we\nderive fast and numerically stable closed-form solutions for the EMD gradient\nin output spaces with chain- and tree- connectivity; and (iii) we propose a\nrelaxed form of the EMD gradient with equivalent computational complexity but\nfaster convergence rate. We support our claims with experiments on real\ndatasets. In a restricted data setting on the ImageNet dataset, we train a\nmodel to classify 1000 categories using 50K images, and demonstrate that our\nrelaxed EMD loss achieves better Top-1 accuracy than the cross entropy loss.\nOverall, we show that our relaxed EMD loss criterion is a powerful asset for\ndeep learning in the small data regime.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:54:05 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Martinez", "Manuel", ""], ["Haurilet", "Monica", ""], ["Al-Halah", "Ziad", ""], ["Tapaswi", "Makarand", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1611.07583", "submitter": "D. Khu\\^e L\\^e-Huu", "authors": "D. Khu\\^e L\\^e-Huu and Nikos Paragios", "title": "Alternating Direction Graph Matching", "comments": "Accepted for publication at the 2017 IEEE Conference on Computer\n  Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a graph matching method that can account for\nconstraints of arbitrary order, with arbitrary potential functions. Unlike\nprevious decomposition approaches that rely on the graph structures, we\nintroduce a decomposition of the matching constraints. Graph matching is then\nreformulated as a non-convex non-separable optimization problem that can be\nsplit into smaller and much-easier-to-solve subproblems, by means of the\nalternating direction method of multipliers. The proposed framework is modular,\nscalable, and can be instantiated into different variants. Two instantiations\nare studied exploring pairwise and higher-order constraints. Experimental\nresults on widely adopted benchmarks involving synthetic and real examples\ndemonstrate that the proposed solutions outperform existing pairwise graph\nmatching methods, and competitive with the state of the art in higher-order\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 23:51:03 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 11:41:27 GMT"}, {"version": "v3", "created": "Wed, 21 Feb 2018 22:13:42 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 12:10:18 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["L\u00ea-Huu", "D. Khu\u00ea", ""], ["Paragios", "Nikos", ""]]}, {"id": "1611.07593", "submitter": "Ziming Zhang", "authors": "Ziming Zhang and Venkatesh Saligrama", "title": "Learning Joint Feature Adaptation for Zero-Shot Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot recognition (ZSR) aims to recognize target-domain data instances of\nunseen classes based on the models learned from associated pairs of seen-class\nsource and target domain data. One of the key challenges in ZSR is the relative\nscarcity of source-domain features (e.g. one feature vector per class), which\ndo not fully account for wide variability in target-domain instances. In this\npaper we propose a novel framework of learning data-dependent feature\ntransforms for scoring similarity between an arbitrary pair of source and\ntarget data instances to account for the wide variability in target domain. Our\nproposed approach is based on optimizing over a parameterized family of local\nfeature displacements that maximize the source-target adaptive similarity\nfunctions. Accordingly we propose formulating zero-shot learning (ZSL) using\nlatent structural SVMs to learn our similarity functions from training data. As\ndemonstration we design a specific algorithm under the proposed framework\ninvolving bilinear similarity functions and regularized least squares as\npenalties for feature displacement. We test our approach on several benchmark\ndatasets for ZSR and show significant improvement over the state-of-the-art.\nFor instance, on aP&Y dataset we can achieve 80.89% in terms of recognition\naccuracy, outperforming the state-of-the-art by 11.15%.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 01:13:37 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2016 03:17:02 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Zhang", "Ziming", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1611.07596", "submitter": "Jonathan Barron", "authors": "Jonathan T. Barron, Yun-Ta Tsai", "title": "Fast Fourier Color Constancy", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm\nwhich solves illuminant estimation by reducing it to a spatial localization\ntask on a torus. By operating in the frequency domain, FFCC produces lower\nerror rates than the previous state-of-the-art by 13-20% while being 250-3000\ntimes faster. This unconventional approach introduces challenges regarding\naliasing, directional statistics, and preconditioning, which we address. By\nproducing a complete posterior distribution over illuminants instead of a\nsingle illuminant estimate, FFCC enables better training techniques, an\neffective temporal smoothing technique, and richer methods for error analysis.\nOur implementation of FFCC runs at ~700 frames per second on a mobile device,\nallowing it to be used as an accurate, real-time, temporally-coherent automatic\nwhite balance algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 01:28:43 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 22:38:28 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 17:14:28 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Barron", "Jonathan T.", ""], ["Tsai", "Yun-Ta", ""]]}, {"id": "1611.07635", "submitter": "Jianming Lv", "authors": "Jianming Lv, Qing Li, Xintong Wang", "title": "T-CONV: A Convolutional Neural Network For Multi-scale Taxi Trajectory\n  Prediction", "comments": null, "journal-ref": "BigComp 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise destination prediction of taxi trajectories can benefit many\nintelligent location based services such as accurate ad for passengers.\nTraditional prediction approaches, which treat trajectories as one-dimensional\nsequences and process them in single scale, fail to capture the diverse\ntwo-dimensional patterns of trajectories in different spatial scales. In this\npaper, we propose T-CONV which models trajectories as two-dimensional images,\nand adopts multi-layer convolutional neural networks to combine multi-scale\ntrajectory patterns to achieve precise prediction. Furthermore, we conduct\ngradient analysis to visualize the multi-scale spatial patterns captured by\nT-CONV and extract the areas with distinct influence on the ultimate\nprediction. Finally, we integrate multiple local enhancement convolutional\nfields to explore these important areas deeply for better prediction.\nComprehensive experiments based on real trajectory data show that T-CONV can\nachieve higher accuracy than the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 04:23:49 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 07:27:21 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 10:26:29 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Lv", "Jianming", ""], ["Li", "Qing", ""], ["Wang", "Xintong", ""]]}, {"id": "1611.07661", "submitter": "Michael Maire", "authors": "Tsung-Wei Ke, Michael Maire, Stella X. Yu", "title": "Multigrid Neural Architectures", "comments": "updated with ImageNet results; to appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multigrid extension of convolutional neural networks (CNNs).\nRather than manipulating representations living on a single spatial grid, our\nnetwork layers operate across scale space, on a pyramid of grids. They consume\nmultigrid inputs and produce multigrid outputs; convolutional filters\nthemselves have both within-scale and cross-scale extent. This aspect is\ndistinct from simple multiscale designs, which only process the input at\ndifferent scales. Viewed in terms of information flow, a multigrid network\npasses messages across a spatial pyramid. As a consequence, receptive field\nsize grows exponentially with depth, facilitating rapid integration of context.\nMost critically, multigrid structure enables networks to learn internal\nattention and dynamic routing mechanisms, and use them to accomplish tasks on\nwhich modern CNNs fail.\n  Experiments demonstrate wide-ranging performance advantages of multigrid. On\nCIFAR and ImageNet classification tasks, flipping from a single grid to\nmultigrid within the standard CNN paradigm improves accuracy, while being\ncompute and parameter efficient. Multigrid is independent of other\narchitectural choices; we show synergy in combination with residual\nconnections. Multigrid yields dramatic improvement on a synthetic semantic\nsegmentation dataset. Most strikingly, relatively shallow multigrid networks\ncan learn to directly perform spatial transformation tasks, where, in contrast,\ncurrent CNNs fail. Together, our results suggest that continuous evolution of\nfeatures on a multigrid pyramid is a more powerful alternative to existing CNN\ndesigns on a flat grid.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 06:55:53 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 19:24:33 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Ke", "Tsung-Wei", ""], ["Maire", "Michael", ""], ["Yu", "Stella X.", ""]]}, {"id": "1611.07675", "submitter": "Ting Yao", "authors": "Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei", "title": "Video Captioning with Transferred Semantic Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically generating natural language descriptions of videos plays a\nfundamental challenge for computer vision community. Most recent progress in\nthis problem has been achieved through employing 2-D and/or 3-D Convolutional\nNeural Networks (CNN) to encode video content and Recurrent Neural Networks\n(RNN) to decode a sentence. In this paper, we present Long Short-Term Memory\nwith Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture\nthat incorporates the transferred semantic attributes learnt from images and\nvideos into the CNN plus RNN framework, by training them in an end-to-end\nmanner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic\nattributes play a significant contribution to captioning, and 2) images and\nvideos carry complementary semantics and thus can reinforce each other for\ncaptioning. To boost video captioning, we propose a novel transfer unit to\nmodel the mutually correlated attributes learnt from images and videos.\nExtensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD\nand MPII-MD. Our proposed LSTM-TSA achieves to-date the best published\nperformance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4\nand CIDEr-D. Superior results when compared to state-of-the-art methods are\nalso reported on M-VAD and MPII-MD.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 07:59:59 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Pan", "Yingwei", ""], ["Yao", "Ting", ""], ["Li", "Houqiang", ""], ["Mei", "Tao", ""]]}, {"id": "1611.07688", "submitter": "Paolo Napoletano", "authors": "Daniela Micucci, Marco Mobilio, Paolo Napoletano", "title": "UniMiB SHAR: a new dataset for human activity recognition using\n  acceleration data from smartphones", "comments": "submitted to MDPI Sensors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones, smartwatches, fitness trackers, and ad-hoc wearable devices are\nbeing increasingly used to monitor human activities. Data acquired by the\nhosted sensors are usually processed by machine-learning-based algorithms to\nclassify human activities. The success of those algorithms mostly depends on\nthe availability of training (labeled) data that, if made publicly available,\nwould allow researchers to make objective comparisons between techniques.\nNowadays, publicly available data sets are few, often contain samples from\nsubjects with too similar characteristics, and very often lack of specific\ninformation so that is not possible to select subsets of samples according to\nspecific criteria. In this article, we present a new dataset of acceleration\nsamples acquired with an Android smartphone designed for human activity\nrecognition and fall detection. The dataset includes 11,771 samples of both\nhuman activities and falls performed by 30 subjects of ages ranging from 18 to\n60 years. Samples are divided in 17 fine grained classes grouped in two coarse\ngrained classes: one containing samples of 9 types of activities of daily\nliving (ADL) and the other containing samples of 8 types of falls. The dataset\nhas been stored to include all the information useful to select samples\naccording to different criteria, such as the type of ADL, the age, the gender,\nand so on. Finally, the dataset has been benchmarked with four different\nclassifiers and with two different feature vectors. We evaluated four different\nclassification tasks: fall vs no fall, 9 activities, 8 falls, 17 activities and\nfalls. For each classification task we performed a subject-dependent and\nindependent evaluation. The major findings of the evaluation are the following:\ni) it is more difficult to distinguish between types of falls than types of\nactivities; ii) subject-dependent evaluation outperforms the\nsubject-independent one\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 08:45:49 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 09:53:23 GMT"}, {"version": "v3", "created": "Sun, 4 Jun 2017 17:40:49 GMT"}, {"version": "v4", "created": "Fri, 14 Jul 2017 14:19:03 GMT"}, {"version": "v5", "created": "Tue, 8 Aug 2017 09:55:41 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Micucci", "Daniela", ""], ["Mobilio", "Marco", ""], ["Napoletano", "Paolo", ""]]}, {"id": "1611.07700", "submitter": "Silvia Zuffi", "authors": "Silvia Zuffi, Angjoo Kanazawa, David Jacobs, Michael J. Black", "title": "3D Menagerie: Modeling the 3D shape and pose of animals", "comments": "Accepted at CVPR 2017 (camera ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant work on learning realistic, articulated, 3D models\nof the human body. In contrast, there are few such models of animals, despite\nmany applications. The main challenge is that animals are much less cooperative\nthan humans. The best human body models are learned from thousands of 3D scans\nof people in specific poses, which is infeasible with live animals.\nConsequently, we learn our model from a small set of 3D scans of toy figurines\nin arbitrary poses. We employ a novel part-based shape model to compute an\ninitial registration to the scans. We then normalize their pose, learn a\nstatistical shape model, and refine the registrations and the model together.\nIn this way, we accurately align animal scans from different quadruped families\nwith very different shapes and poses. With the registration to a common\ntemplate we learn a shape space representing animals including lions, cats,\ndogs, horses, cows and hippos. Animal shapes can be sampled from the model,\nposed, animated, and fit to data. We demonstrate generalization by fitting it\nto images of real animals including species not seen in training.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 09:30:50 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 10:39:46 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zuffi", "Silvia", ""], ["Kanazawa", "Angjoo", ""], ["Jacobs", "David", ""], ["Black", "Michael J.", ""]]}, {"id": "1611.07703", "submitter": "RaviKiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Shanthakumar Venkatraman, R. Venkatesh\n  Babu", "title": "'Part'ly first among equals: Semantic part-based benchmarking for\n  state-of-the-art object recognition systems", "comments": "Extended version of our ACCV-2016 paper. Author formatting modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An examination of object recognition challenge leaderboards (ILSVRC,\nPASCAL-VOC) reveals that the top-performing classifiers typically exhibit small\ndifferences amongst themselves in terms of error rate/mAP. To better\ndifferentiate the top performers, additional criteria are required. Moreover,\nthe (test) images, on which the performance scores are based, predominantly\ncontain fully visible objects. Therefore, `harder' test images, mimicking the\nchallenging conditions (e.g. occlusion) in which humans routinely recognize\nobjects, need to be utilized for benchmarking. To address the concerns\nmentioned above, we make two contributions. First, we systematically vary the\nlevel of local object-part content, global detail and spatial context in images\nfrom PASCAL VOC 2010 to create a new benchmarking dataset dubbed PPSS-12.\nSecond, we propose an object-part based benchmarking procedure which quantifies\nclassifiers' robustness to a range of visibility and contextual settings. The\nbenchmarking procedure relies on a semantic similarity measure that naturally\naddresses potential semantic granularity differences between the category\nlabels in training and test datasets, thus eliminating manual mapping. We use\nour procedure on the PPSS-12 dataset to benchmark top-performing classifiers\ntrained on the ILSVRC-2012 dataset. Our results show that the proposed\nbenchmarking procedure enables additional differentiation among\nstate-of-the-art object classifiers in terms of their ability to handle missing\ncontent and insufficient object detail. Given this capability for additional\ndifferentiation, our approach can potentially supplement existing benchmarking\nprocedures used in object recognition challenge leaderboards.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 09:38:09 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 14:06:06 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Venkatraman", "Shanthakumar", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1611.07709", "submitter": "Jifeng Dai", "authors": "Yi Li, Haozhi Qi, Jifeng Dai, Xiangyang Ji, Yichen Wei", "title": "Fully Convolutional Instance-aware Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first fully convolutional end-to-end solution for\ninstance-aware semantic segmentation task. It inherits all the merits of FCNs\nfor semantic segmentation and instance mask proposal. It performs instance mask\nprediction and classification jointly. The underlying convolutional\nrepresentation is fully shared between the two sub-tasks, as well as between\nall regions of interest. The proposed network is highly integrated and achieves\nstate-of-the-art performance in both accuracy and efficiency. It wins the COCO\n2016 segmentation competition by a large margin. Code would be released at\n\\url{https://github.com/daijifeng001/TA-FCN}.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 09:53:57 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 09:00:54 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Li", "Yi", ""], ["Qi", "Haozhi", ""], ["Dai", "Jifeng", ""], ["Ji", "Xiangyang", ""], ["Wei", "Yichen", ""]]}, {"id": "1611.07715", "submitter": "Jifeng Dai", "authors": "Xizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, Yichen Wei", "title": "Deep Feature Flow for Video Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neutral networks have achieved great success on image\nrecognition tasks. Yet, it is non-trivial to transfer the state-of-the-art\nimage recognition networks to videos as per-frame evaluation is too slow and\nunaffordable. We present deep feature flow, a fast and accurate framework for\nvideo recognition. It runs the expensive convolutional sub-network only on\nsparse key frames and propagates their deep feature maps to other frames via a\nflow field. It achieves significant speedup as flow computation is relatively\nfast. The end-to-end training of the whole architecture significantly boosts\nthe recognition accuracy. Deep feature flow is flexible and general. It is\nvalidated on two recent large scale video datasets. It makes a large step\ntowards practical video recognition.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:06:30 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 11:41:51 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhu", "Xizhou", ""], ["Xiong", "Yuwen", ""], ["Dai", "Jifeng", ""], ["Yuan", "Lu", ""], ["Wei", "Yichen", ""]]}, {"id": "1611.07718", "submitter": "Jingdong Wang", "authors": "Liming Zhao, Jingdong Wang, Xi Li, Zhuowen Tu, Wenjun Zeng", "title": "Deep Convolutional Neural Networks with Merge-and-Run Mappings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep residual network, built by stacking a sequence of residual blocks, is\neasy to train, because identity mappings skip residual branches and thus\nimprove information flow. To further reduce the training difficulty, we present\na simple network architecture, deep merge-and-run neural networks. The novelty\nlies in a modularized building block, merge-and-run block, which assembles\nresidual branches in parallel through a merge-and-run mapping: Average the\ninputs of these residual branches (Merge), and add the average to the output of\neach residual branch as the input of the subsequent residual branch (Run),\nrespectively. We show that the merge-and-run mapping is a linear idempotent\nfunction in which the transformation matrix is idempotent, and thus improves\ninformation flow, making training easy. In comparison to residual networks, our\nnetworks enjoy compelling advantages: they contain much shorter paths, and the\nwidth, i.e., the number of channels, is increased. We evaluate the performance\non the standard recognition tasks. Our approach demonstrates consistent\nimprovements over ResNets with the comparable setup, and achieves competitive\nresults (e.g., $3.57\\%$ testing error on CIFAR-$10$, $19.00\\%$ on CIFAR-$100$,\n$1.51\\%$ on SVHN).\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:08:40 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 07:27:21 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Zhao", "Liming", ""], ["Wang", "Jingdong", ""], ["Li", "Xi", ""], ["Tu", "Zhuowen", ""], ["Zeng", "Wenjun", ""]]}, {"id": "1611.07725", "submitter": "Christoph H. Lampert", "authors": "Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, Christoph\n  H. Lampert", "title": "iCaRL: Incremental Classifier and Representation Learning", "comments": "Accepted paper at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major open problem on the road to artificial intelligence is the\ndevelopment of incrementally learning systems that learn about more and more\nconcepts over time from a stream of data. In this work, we introduce a new\ntraining strategy, iCaRL, that allows learning in such a class-incremental way:\nonly the training data for a small number of classes has to be present at the\nsame time and new classes can be added progressively. iCaRL learns strong\nclassifiers and a data representation simultaneously. This distinguishes it\nfrom earlier works that were fundamentally limited to fixed data\nrepresentations and therefore incompatible with deep learning architectures. We\nshow by experiments on CIFAR-100 and ImageNet ILSVRC 2012 data that iCaRL can\nlearn many classes incrementally over a long period of time where other\nstrategies quickly fail.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:24:11 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 16:41:02 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Rebuffi", "Sylvestre-Alvise", ""], ["Kolesnikov", "Alexander", ""], ["Sperl", "Georg", ""], ["Lampert", "Christoph H.", ""]]}, {"id": "1611.07727", "submitter": "Umar Iqbal", "authors": "Umar Iqbal, Anton Milan, Juergen Gall", "title": "PoseTrack: Joint Multi-Person Pose Estimation and Tracking", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce the challenging problem of joint multi-person pose\nestimation and tracking of an unknown number of persons in unconstrained\nvideos. Existing methods for multi-person pose estimation in images cannot be\napplied directly to this problem, since it also requires to solve the problem\nof person association over time in addition to the pose estimation for each\nperson. We therefore propose a novel method that jointly models multi-person\npose estimation and tracking in a single formulation. To this end, we represent\nbody joint detections in a video by a spatio-temporal graph and solve an\ninteger linear program to partition the graph into sub-graphs that correspond\nto plausible body pose trajectories for each person. The proposed approach\nimplicitly handles occlusion and truncation of persons. Since the problem has\nnot been addressed quantitatively in the literature, we introduce a challenging\n\"Multi-Person PoseTrack\" dataset, and also propose a completely unconstrained\nevaluation protocol that does not make any assumptions about the scale, size,\nlocation or the number of persons. Finally, we evaluate the proposed approach\nand several baseline methods on our new dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:30:06 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 12:56:22 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 14:16:38 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Iqbal", "Umar", ""], ["Milan", "Anton", ""], ["Gall", "Juergen", ""]]}, {"id": "1611.07752", "submitter": "Sunghyun Cho", "authors": "Sunghyun Cho and Seungyong Lee", "title": "Convergence Analysis of MAP based Blur Kernel Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular approach for blind deconvolution is to formulate a maximum a\nposteriori (MAP) problem with sparsity priors on the gradients of the latent\nimage, and then alternatingly estimate the blur kernel and the latent image.\nWhile several successful MAP based methods have been proposed, there has been\nmuch controversy and confusion about their convergence, because sparsity priors\nhave been shown to prefer blurry images to sharp natural images. In this paper,\nwe revisit this problem and provide an analysis on the convergence of MAP based\napproaches. We first introduce a slight modification to a conventional joint\nenergy function for blind deconvolution. The reformulated energy function\nyields the same alternating estimation process, but more clearly reveals how\nblind deconvolution works. We then show the energy function can actually favor\nthe right solution instead of the no-blur solution under certain conditions,\nwhich explains the success of previous MAP based approaches. The reformulated\nenergy function and our conditions for the convergence also provide a way to\ncompare the qualities of different blur kernels, and we demonstrate its\napplicability to automatic blur kernel size selection, blur kernel estimation\nusing light streaks, and defocus estimation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 11:53:15 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:05:20 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Cho", "Sunghyun", ""], ["Lee", "Seungyong", ""]]}, {"id": "1611.07759", "submitter": "Xiaozhi Chen", "authors": "Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, Tian Xia", "title": "Multi-View 3D Object Detection Network for Autonomous Driving", "comments": "To appear in IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at high-accuracy 3D object detection in autonomous driving\nscenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework\nthat takes both LIDAR point cloud and RGB images as input and predicts oriented\n3D bounding boxes. We encode the sparse 3D point cloud with a compact\nmulti-view representation. The network is composed of two subnetworks: one for\n3D object proposal generation and another for multi-view feature fusion. The\nproposal network generates 3D candidate boxes efficiently from the bird's eye\nview representation of 3D point cloud. We design a deep fusion scheme to\ncombine region-wise features from multiple views and enable interactions\nbetween intermediate layers of different paths. Experiments on the challenging\nKITTI benchmark show that our approach outperforms the state-of-the-art by\naround 25% and 30% AP on the tasks of 3D localization and 3D detection. In\naddition, for 2D detection, our approach obtains 10.3% higher AP than the\nstate-of-the-art on the hard data among the LIDAR-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 12:08:38 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 11:34:01 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 03:23:51 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Chen", "Xiaozhi", ""], ["Ma", "Huimin", ""], ["Wan", "Ji", ""], ["Li", "Bo", ""], ["Xia", "Tian", ""]]}, {"id": "1611.07767", "submitter": "Jonas Geiping", "authors": "Jonas Geiping, Hendrik Dirks, Daniel Cremers, Michael Moeller", "title": "Multiframe Motion Coupling for Video Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of video super resolution is to use different view points of a\nsingle scene to enhance the overall resolution and quality. Classical energy\nminimization approaches first establish a correspondence of the current frame\nto all its neighbors in some radius and then use this temporal information for\nenhancement. In this paper, we propose the first variational super resolution\napproach that computes several super resolved frames in one batch optimization\nprocedure by incorporating motion information between the high-resolution image\nframes themselves. As a consequence, the number of motion estimation problems\ngrows linearly in the number of frames, opposed to a quadratic growth of\nclassical methods and temporal consistency is enforced naturally. We use\ninfimal convolution regularization as well as an automatic parameter balancing\nscheme to automatically determine the reliability of the motion information and\nreweight the regularization locally. We demonstrate that our approach yields\nstate-of-the-art results and even is competitive with machine learning\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 12:36:45 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 10:06:43 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Geiping", "Jonas", ""], ["Dirks", "Hendrik", ""], ["Cremers", "Daniel", ""], ["Moeller", "Michael", ""]]}, {"id": "1611.07781", "submitter": "Pierre-Francois Marteau", "authors": "Pierre-Fran\\c{c}ois Marteau (EXPRESSION), Sylvie Gibet (EXPRESSION),\n  Cl\\'ement Reverdy (EXPRESSION)", "title": "Adaptive Down-Sampling and Dimension Reduction in Time Elastic Kernel\n  Machines for Efficient Recognition of Isolated Gestures", "comments": null, "journal-ref": "Guillet, Fabrice and Pinaud, Bruno and Venturini, Gilles. Advances\n  in Knowledge Discovery and Management: volume 6, Volume (665), Springer\n  International Publishing, pp.39 - 59, 2016, Studies in Computational\n  Intelligence, 978-3-319-45763-5", "doi": "10.1007/978-3-319-45763-5_3", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scope of gestural action recognition, the size of the feature vector\nrepresenting movements is in general quite large especially when full body\nmovements are considered. Furthermore, this feature vector evolves during the\nmovement performance so that a complete movement is fully represented by a\nmatrix M of size DxT , whose element M i, j represents the value of feature i\nat timestamps j. Many studies have addressed dimensionality reduction\nconsidering only the size of the feature vector lying in R D to reduce both the\nvariability of gestural sequences expressed in the reduced space, and the\ncomputational complexity of their processing. In return, very few of these\nmethods have explicitly addressed the dimensionality reduction along the time\naxis. Yet this is a major issue when considering the use of elastic distances\nwhich are characterized by a quadratic complexity along the time axis. We\npresent in this paper an evaluation of straightforward approaches aiming at\nreducing the dimensionality of the matrix M for each movement, leading to\nconsider both the dimensionality reduction of the feature vector as well as its\nreduction along the time axis. The dimensionality reduction of the feature\nvector is achieved by selecting remarkable joints in the skeleton performing\nthe movement, basically the extremities of the articulatory chains composing\nthe skeleton. The temporal dimen-sionality reduction is achieved using either a\nregular or adaptive down-sampling that seeks to minimize the reconstruction\nerror of the movements. Elastic and Euclidean kernels are then compared through\nsupport vector machine learning. Two data sets 1 that are widely referenced in\nthe domain of human gesture recognition, and quite distinctive in terms of\nquality of motion capture, are used for the experimental assessment of the\nproposed approaches. On these data sets we experimentally show that it is\nfeasible, and possibly desirable, to significantly reduce simultaneously the\nsize of the feature vector and the number of skeleton frames to represent body\nmovements while maintaining a very good recognition rate. The method proves to\ngive satisfactory results at a level currently reached by state-of-the-art\nmethods on these data sets. We experimentally show that the computational\ncomplexity reduction that is obtained makes this approach eligible for\nreal-time applications.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 13:18:17 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Marteau", "Pierre-Fran\u00e7ois", "", "EXPRESSION"], ["Gibet", "Sylvie", "", "EXPRESSION"], ["Reverdy", "Cl\u00e9ment", "", "EXPRESSION"]]}, {"id": "1611.07791", "submitter": "Fares Jalled", "authors": "Fares Jalled, Ilia Voronkov", "title": "Object Detection using Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Unmanned Ariel vehicle (UAV) has greater importance in the army for border\nsecurity. The main objective of this article is to develop an OpenCV-Python\ncode using Haar Cascade algorithm for object and face detection. Currently,\nUAVs are used for detecting and attacking the infiltrated ground targets. The\nmain drawback for this type of UAVs is that sometimes the object are not\nproperly detected, which thereby causes the object to hit the UAV. This project\naims to avoid such unwanted collisions and damages of UAV. UAV is also used for\nsurveillance that uses Voila-jones algorithm to detect and track humans. This\nalgorithm uses cascade object detector function and vision. train function to\ntrain the algorithm. The main advantage of this code is the reduced processing\ntime. The Python code was tested with the help of available database of video\nand image, the output was verified.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 13:48:07 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Jalled", "Fares", ""], ["Voronkov", "Ilia", ""]]}, {"id": "1611.07807", "submitter": "Gautam Pai", "authors": "Gautam Pai, Aaron Wetzler, Ron Kimmel", "title": "Learning Invariant Representations Of Planar Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a metric learning framework for the construction of invariant\ngeometric functions of planar curves for the Eucledian and Similarity group of\ntransformations. We leverage on the representational power of convolutional\nneural networks to compute these geometric quantities. In comparison with\naxiomatic constructions, we show that the invariants approximated by the\nlearning architectures have better numerical qualities such as robustness to\nnoise, resiliency to sampling, as well as the ability to adapt to occlusion and\npartiality. Finally, we develop a novel multi-scale representation in a\nsimilarity metric learning paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 14:20:17 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 21:44:00 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Pai", "Gautam", ""], ["Wetzler", "Aaron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1611.07810", "submitter": "Tegan Maharaj", "authors": "Tegan Maharaj and Nicolas Ballas and Anna Rohrbach and Aaron Courville\n  and Christopher Pal", "title": "A dataset and exploration of models for understanding video data through\n  fill-in-the-blank question-answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks frequently approach or exceed\nhuman-level performance at benchmark tasks involving static images, extending\nthis success to moving images is not straightforward. Having models which can\nlearn to understand video is of interest for many applications, including\ncontent recommendation, prediction, summarization, event/object detection and\nunderstanding human visual perception, but many domains lack sufficient data to\nexplore and perfect video models. In order to address the need for a simple,\nquantitative benchmark for developing and understanding video, we present\nMovieFIB, a fill-in-the-blank question-answering dataset with over 300,000\nexamples, based on descriptive video annotations for the visually impaired. In\naddition to presenting statistics and a description of the dataset, we perform\na detailed analysis of 5 different models' predictions, and compare these with\nhuman performance. We investigate the relative importance of language, static\n(2D) visual features, and moving (3D) visual features; the effects of\nincreasing dataset size, the number of frames sampled; and of vocabulary size.\nWe illustrate that: this task is not solvable by a language model alone; our\nmodel combining 2D and 3D visual information indeed provides the best result;\nall models perform significantly worse than human-level. We provide human\nevaluations for responses given by different models and find that accuracy on\nthe MovieFIB evaluation corresponds well with human judgement. We suggest\navenues for improving video models, and hope that the proposed dataset can be\nuseful for measuring and encouraging progress in this very interesting field.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 14:22:51 GMT"}, {"version": "v2", "created": "Sun, 5 Feb 2017 17:51:19 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Maharaj", "Tegan", ""], ["Ballas", "Nicolas", ""], ["Rohrbach", "Anna", ""], ["Courville", "Aaron", ""], ["Pal", "Christopher", ""]]}, {"id": "1611.07828", "submitter": "Georgios Pavlakos", "authors": "Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Derpanis, Kostas\n  Daniilidis", "title": "Coarse-to-Fine Volumetric Prediction for Single-Image 3D Human Pose", "comments": "CVPR 2017 Camera Ready. Project Page:\n  https://www.seas.upenn.edu/~pavlakos/projects/volumetric/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of 3D human pose estimation from a single\ncolor image. Despite the general success of the end-to-end learning paradigm,\ntop performing approaches employ a two-step solution consisting of a\nConvolutional Network (ConvNet) for 2D joint localization and a subsequent\noptimization step to recover 3D pose. In this paper, we identify the\nrepresentation of 3D pose as a critical issue with current ConvNet approaches\nand make two important contributions towards validating the value of end-to-end\nlearning for this task. First, we propose a fine discretization of the 3D space\naround the subject and train a ConvNet to predict per voxel likelihoods for\neach joint. This creates a natural representation for 3D pose and greatly\nimproves performance over the direct regression of joint coordinates. Second,\nto further improve upon initial estimates, we employ a coarse-to-fine\nprediction scheme. This step addresses the large dimensionality increase and\nenables iterative refinement and repeated processing of the image features. The\nproposed approach outperforms all state-of-the-art methods on standard\nbenchmarks achieving a relative error reduction greater than 30% on average.\nAdditionally, we investigate using our volumetric representation in a related\narchitecture which is suboptimal compared to our end-to-end approach, but is of\npractical interest, since it enables training when no image with corresponding\n3D groundtruth is available, and allows us to present compelling results for\nin-the-wild images.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 15:06:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 12:10:16 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Pavlakos", "Georgios", ""], ["Zhou", "Xiaowei", ""], ["Derpanis", "Konstantinos G.", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1611.07837", "submitter": "Zhe Gan", "authors": "Yunchen Pu, Martin Renqiang Min, Zhe Gan, Lawrence Carin", "title": "Adaptive Feature Abstraction for Translating Video to Text", "comments": "Accepted to AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous models for video captioning often use the output from a specific\nlayer of a Convolutional Neural Network (CNN) as video features. However, the\nvariable context-dependent semantics in the video may make it more appropriate\nto adaptively select features from the multiple CNN layers. We propose a new\napproach for generating adaptive spatiotemporal representations of videos for\nthe captioning task. A novel attention mechanism is developed, that adaptively\nand sequentially focuses on different layers of CNN features (levels of feature\n\"abstraction\"), as well as local spatiotemporal regions of the feature maps at\neach layer. The proposed approach is evaluated on three benchmark datasets:\nYouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the\nmodel works, these experiments quantitatively demonstrate the effectiveness of\nthe proposed adaptive spatiotemporal feature abstraction for translating videos\nto sentences with rich semantics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 15:21:48 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:40:47 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 05:13:16 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Pu", "Yunchen", ""], ["Min", "Martin Renqiang", ""], ["Gan", "Zhe", ""], ["Carin", "Lawrence", ""]]}, {"id": "1611.07865", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker, Matthias Bethge, Aaron Hertzmann\n  and Eli Shechtman", "title": "Controlling Perceptual Factors in Neural Style Transfer", "comments": "Accepted at CVPR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Style Transfer has shown very exciting results enabling new forms of\nimage manipulation. Here we extend the existing method to introduce control\nover spatial location, colour information and across spatial scale. We\ndemonstrate how this enhances the method by allowing high-resolution controlled\nstylisation and helps to alleviate common failure cases such as applying ground\ntextures to sky regions. Furthermore, by decomposing style into these\nperceptual factors we enable the combination of style information from multiple\nsources to generate new, perceptually appealing styles from existing ones. We\nalso describe how these methods can be used to more efficiently produce large\nsize, high-quality stylisation. Finally we show how the introduced control\nmeasures can be applied in recent methods for Fast Neural Style Transfer.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:24:08 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 16:55:01 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""], ["Hertzmann", "Aaron", ""], ["Shechtman", "Eli", ""]]}, {"id": "1611.07889", "submitter": "Denys Rozumnyi", "authors": "Denys Rozumnyi, Jan Kotera, Filip Sroubek, Lukas Novotny, Jiri Matas", "title": "The World of Fast Moving Objects", "comments": null, "journal-ref": "2017 IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR)", "doi": "10.1109/CVPR.2017.514", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of a Fast Moving Object (FMO), i.e. an object that moves over a\ndistance exceeding its size within the exposure time, is introduced. FMOs may,\nand typically do, rotate with high angular speed. FMOs are very common in\nsports videos, but are not rare elsewhere. In a single frame, such objects are\noften barely visible and appear as semi-transparent streaks.\n  A method for the detection and tracking of FMOs is proposed. The method\nconsists of three distinct algorithms, which form an efficient localization\npipeline that operates successfully in a broad range of conditions. We show\nthat it is possible to recover the appearance of the object and its axis of\nrotation, despite its blurred appearance. The proposed method is evaluated on a\nnew annotated dataset. The results show that existing trackers are inadequate\nfor the problem of FMO localization and a new approach is required. Two\napplications of localization, temporal super-resolution and highlighting, are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 17:20:04 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Rozumnyi", "Denys", ""], ["Kotera", "Jan", ""], ["Sroubek", "Filip", ""], ["Novotny", "Lukas", ""], ["Matas", "Jiri", ""]]}, {"id": "1611.07890", "submitter": "Caner Hazirbas", "authors": "Florian Walch, Caner Hazirbas, Laura Leal-Taix\\'e, Torsten Sattler,\n  Sebastian Hilsenbeck, Daniel Cremers", "title": "Image-based localization using LSTMs for structured feature correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a new CNN+LSTM architecture for camera pose\nregression for indoor and outdoor scenes. CNNs allow us to learn suitable\nfeature representations for localization that are robust against motion blur\nand illumination changes. We make use of LSTM units on the CNN output, which\nplay the role of a structured dimensionality reduction on the feature vector,\nleading to drastic improvements in localization performance. We provide\nextensive quantitative comparison of CNN-based and SIFT-based localization\nmethods, showing the weaknesses and strengths of each. Furthermore, we present\na new large-scale indoor dataset with accurate ground truth from a laser\nscanner. Experimental results on both indoor and outdoor public datasets show\nour method outperforms existing deep architectures, and can localize images in\nhard conditions, e.g., in the presence of mostly textureless surfaces, where\nclassic SIFT-based methods fail.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 17:22:27 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 17:31:08 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 10:24:41 GMT"}, {"version": "v4", "created": "Sun, 20 Aug 2017 22:07:43 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Walch", "Florian", ""], ["Hazirbas", "Caner", ""], ["Leal-Taix\u00e9", "Laura", ""], ["Sattler", "Torsten", ""], ["Hilsenbeck", "Sebastian", ""], ["Cremers", "Daniel", ""]]}, {"id": "1611.07909", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Yao Wang", "title": "Image Segmentation Using Overlapping Group Sparsity", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.02434.\n  appears in IEEE Signal Processing in Medicine and Biology Symposium, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse decomposition has been widely used for different applications, such as\nsource separation, image classification and image denoising. This paper\npresents a new algorithm for segmentation of an image into background and\nforeground text and graphics using sparse decomposition. First, the background\nis represented using a suitable smooth model, which is a linear combination of\na few smoothly varying basis functions, and the foreground text and graphics\nare modeled as a sparse component overlaid on the smooth background. Then the\nbackground and foreground are separated using a sparse decomposition framework\nand imposing some prior information, which promote the smoothness of\nbackground, and the sparsity and connectivity of foreground pixels. This\nalgorithm has been tested on a dataset of images extracted from HEVC standard\ntest sequences for screen content coding, and is shown to outperform prior\nmethods, including least absolute deviation fitting, k-means clustering based\nsegmentation in DjVu, and shape primitive extraction and coding algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 18:08:33 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 03:42:36 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 15:38:42 GMT"}, {"version": "v4", "created": "Wed, 21 Dec 2016 15:36:41 GMT"}], "update_date": "2016-12-22", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1611.07932", "submitter": "Saumya Jetley", "authors": "Saumya Jetley, Michael Sapienza, Stuart Golodetz and Philip H.S. Torr", "title": "Straight to Shapes: Real-time Detection of Encoded Shapes", "comments": "16 pages including appendix; Published at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current object detection approaches predict bounding boxes, but these provide\nlittle instance-specific information beyond location, scale and aspect ratio.\nIn this work, we propose to directly regress to objects' shapes in addition to\ntheir bounding boxes and categories. It is crucial to find an appropriate shape\nrepresentation that is compact and decodable, and in which objects can be\ncompared for higher-order concepts such as view similarity, pose variation and\nocclusion. To achieve this, we use a denoising convolutional auto-encoder to\nestablish an embedding space, and place the decoder after a fast end-to-end\nnetwork trained to regress directly to the encoded shape vectors. This yields\nwhat to the best of our knowledge is the first real-time shape prediction\nnetwork, running at ~35 FPS on a high-end desktop. With higher-order shape\nreasoning well-integrated into the network pipeline, the network shows the\nuseful practical quality of generalising to unseen categories similar to the\nones in the training set, something that most existing approaches fail to\nhandle.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 19:04:43 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 17:25:25 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Jetley", "Saumya", ""], ["Sapienza", "Michael", ""], ["Golodetz", "Stuart", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1611.07941", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Fran\\c{c}ois Fleuret and Pascal Fua", "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping", "comments": "Submitted for review to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field inference is central to statistical physics. It has attracted much\ninterest in the Computer Vision community to efficiently solve problems\nexpressible in terms of large Conditional Random Fields. However, since it\nmodels the posterior probability distribution as a product of marginal\nprobabilities, it may fail to properly account for important dependencies\nbetween variables. We therefore replace the fully factorized distribution of\nMean Field by a weighted mixture of such distributions, that similarly\nminimizes the KL-Divergence to the true posterior. By introducing two new\nideas, namely, conditioning on groups of variables instead of single ones and\nusing a parameter of the conditional random field potentials, that we identify\nto the temperature in the sense of statistical physics to select such groups,\nwe can perform this minimization efficiently. Our extension of the clamping\nmethod proposed in previous works allows us to both produce a more descriptive\napproximation of the true posterior and, inspired by the diverse MAP paradigms,\nfit a mixture of Mean Field approximations. We demonstrate that this positively\nimpacts real-world algorithms that initially relied on mean fields.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 19:14:25 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1611.08002", "submitter": "Zhe Gan", "authors": "Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng\n  Gao, Lawrence Carin, Li Deng", "title": "Semantic Compositional Networks for Visual Captioning", "comments": "Accepted in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Semantic Compositional Network (SCN) is developed for image captioning, in\nwhich semantic concepts (i.e., tags) are detected from the image, and the\nprobability of each tag is used to compose the parameters in a long short-term\nmemory (LSTM) network. The SCN extends each weight matrix of the LSTM to an\nensemble of tag-dependent weight matrices. The degree to which each member of\nthe ensemble is used to generate an image caption is tied to the\nimage-dependent probability of the corresponding tag. In addition to captioning\nimages, we also extend the SCN to generate captions for video clips. We\nqualitatively analyze semantic composition in SCNs, and quantitatively evaluate\nthe algorithm on three benchmark datasets: COCO, Flickr30k, and Youtube2Text.\nExperimental results show that the proposed method significantly outperforms\nprior state-of-the-art approaches, across multiple evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 21:22:22 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 18:33:51 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Gan", "Zhe", ""], ["Gan", "Chuang", ""], ["He", "Xiaodong", ""], ["Pu", "Yunchen", ""], ["Tran", "Kenneth", ""], ["Gao", "Jianfeng", ""], ["Carin", "Lawrence", ""], ["Deng", "Li", ""]]}, {"id": "1611.08036", "submitter": "Sulabh Kumra", "authors": "Sulabh Kumra and Christopher Kanan", "title": "Robotic Grasp Detection using Deep Convolutional Neural Networks", "comments": "8 pages, 9 figures, 2017 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has significantly advanced computer vision and natural language\nprocessing. While there have been some successes in robotics using deep\nlearning, it has not been widely adopted. In this paper, we present a novel\nrobotic grasp detection system that predicts the best grasping pose of a\nparallel-plate robotic gripper for novel objects using the RGB-D image of the\nscene. The proposed model uses a deep convolutional neural network to extract\nfeatures from the scene and then uses a shallow convolutional neural network to\npredict the grasp configuration for the object of interest. Our multi-modal\nmodel achieved an accuracy of 89.21% on the standard Cornell Grasp Dataset and\nruns at real-time speeds. This redefines the state-of-the-art for robotic grasp\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 00:07:39 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 22:53:53 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 17:31:55 GMT"}, {"version": "v4", "created": "Fri, 21 Jul 2017 22:09:02 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Kumra", "Sulabh", ""], ["Kanan", "Christopher", ""]]}, {"id": "1611.08050", "submitter": "Zhe Cao", "authors": "Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh", "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields", "comments": "Accepted as CVPR 2017 Oral. Video result:\n  https://youtu.be/pW6nZXeWlGM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to efficiently detect the 2D pose of multiple people\nin an image. The approach uses a nonparametric representation, which we refer\nto as Part Affinity Fields (PAFs), to learn to associate body parts with\nindividuals in the image. The architecture encodes global context, allowing a\ngreedy bottom-up parsing step that maintains high accuracy while achieving\nrealtime performance, irrespective of the number of people in the image. The\narchitecture is designed to jointly learn part locations and their association\nvia two branches of the same sequential prediction process. Our method placed\nfirst in the inaugural COCO 2016 keypoints challenge, and significantly exceeds\nthe previous state-of-the-art result on the MPII Multi-Person benchmark, both\nin performance and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 01:58:16 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 00:19:18 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Cao", "Zhe", ""], ["Simon", "Tomas", ""], ["Wei", "Shih-En", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1611.08061", "submitter": "Hexiang Hu", "authors": "Hexiang Hu, Zhiwei Deng, Guang-tong Zhou, Fei Sha, Greg Mori", "title": "Recalling Holistic Information for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation requires a detailed labeling of image pixels by object\ncategory. Information derived from local image patches is necessary to describe\nthe detailed shape of individual objects. However, this information is\nambiguous and can result in noisy labels. Global inference of image content can\ninstead capture the general semantic concepts present. We advocate that\nhigh-recall holistic inference of image concepts provides valuable information\nfor detailed pixel labeling. We build a two-stream neural network architecture\nthat facilitates information flow from holistic information to local pixels,\nwhile keeping common image features shared among the low-level layers of both\nthe holistic analysis and segmentation branches. We empirically evaluate our\nnetwork on four standard semantic segmentation datasets. Our network obtains\nstate-of-the-art performance on PASCAL-Context and NYUDv2, and ablation studies\nverify its effectiveness on ADE20K and SIFT-Flow.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 03:46:37 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hu", "Hexiang", ""], ["Deng", "Zhiwei", ""], ["Zhou", "Guang-tong", ""], ["Sha", "Fei", ""], ["Mori", "Greg", ""]]}, {"id": "1611.08069", "submitter": "Bo Li", "authors": "Bo Li", "title": "3D Fully Convolutional Network for Vehicle Detection in Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D fully convolutional network has been recently successfully applied to\nobject detection from images. In this paper, we extend the fully convolutional\nnetwork based detection techniques to 3D and apply it to point cloud data. The\nproposed approach is verified on the task of vehicle detection from lidar point\ncloud for autonomous driving. Experiments on the KITTI dataset shows a\nsignificant performance improvement over the previous point cloud based\ndetection approaches.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 05:06:05 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 05:56:01 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Li", "Bo", ""]]}, {"id": "1611.08091", "submitter": "Junyu Wu", "authors": "Junyu Wu and Shengyong Ding and Wei Xu and Hongyang Chao", "title": "Deep Joint Face Hallucination and Recognition", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep models have achieved impressive performance for face hallucination\ntasks. However, we observe that directly feeding the hallucinated facial images\ninto recog- nition models can even degrade the recognition performance despite\nthe much better visualization quality. In this paper, we address this problem\nby jointly learning a deep model for two tasks, i.e. face hallucination and\nrecognition. In particular, we design an end-to-end deep convolution network\nwith hallucination sub-network cascaded by recognition sub-network. The\nrecognition sub- network are responsible for producing discriminative feature\nrepresentations using the hallucinated images as inputs generated by\nhallucination sub-network. During training, we feed LR facial images into the\nnetwork and optimize the parameters by minimizing two loss items, i.e. 1) face\nhallucination loss measured by the pixel wise difference between the ground\ntruth HR images and network-generated images; and 2) verification loss which is\nmeasured by the classification error and intra-class distance. We extensively\nevaluate our method on LFW and YTF datasets. The experimental results show that\nour method can achieve recognition accuracy 97.95% on 4x down-sampled LFW\ntesting set, outperforming the accuracy 96.35% of conventional face recognition\nmodel. And on the more challenging YTF dataset, we achieve recognition accuracy\n90.65%, a margin over the recognition accuracy 89.45% obtained by conventional\nface recognition model on the 4x down-sampled version.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 08:19:49 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Wu", "Junyu", ""], ["Ding", "Shengyong", ""], ["Xu", "Wei", ""], ["Chao", "Hongyang", ""]]}, {"id": "1611.08097", "submitter": "Michael Bronstein", "authors": "Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, Pierre\n  Vandergheynst", "title": "Geometric deep learning: going beyond Euclidean data", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2017.2693418", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific fields study data with an underlying structure that is a\nnon-Euclidean space. Some examples include social networks in computational\nsocial sciences, sensor networks in communications, functional networks in\nbrain imaging, regulatory networks in genetics, and meshed surfaces in computer\ngraphics. In many applications, such geometric data are large and complex (in\nthe case of social networks, on the scale of billions), and are natural targets\nfor machine learning techniques. In particular, we would like to use deep\nneural networks, which have recently proven to be powerful tools for a broad\nrange of problems from computer vision, natural language processing, and audio\nanalysis. However, these tools have been most successful on data with an\nunderlying Euclidean or grid-like structure, and in cases where the invariances\nof these structures are built into networks used to model them. Geometric deep\nlearning is an umbrella term for emerging techniques attempting to generalize\n(structured) deep neural models to non-Euclidean domains such as graphs and\nmanifolds. The purpose of this paper is to overview different examples of\ngeometric deep learning problems and present available solutions, key\ndifficulties, applications, and future research directions in this nascent\nfield.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 08:45:01 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 12:37:19 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Bruna", "Joan", ""], ["LeCun", "Yann", ""], ["Szlam", "Arthur", ""], ["Vandergheynst", "Pierre", ""]]}, {"id": "1611.08107", "submitter": "Junyu Wu", "authors": "Shengyong Ding and Junyu Wu and Wei Xu and Hongyang Chao", "title": "Automatically Building Face Datasets of New Domains from Weakly Labeled\n  Data with Pretrained Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data are critical in face recognition systems. However, labeling a\nlarge scale face data for a particular domain is very tedious. In this paper,\nwe propose a method to automatically and incrementally construct datasets from\nmassive weakly labeled data of the target domain which are readily available on\nthe Internet under the help of a pretrained face model. More specifically,\ngiven a large scale weakly labeled dataset in which each face image is\nassociated with a label, i.e. the name of an identity, we create a graph for\neach identity with edges linking matched faces verified by the existing model\nunder a tight threshold. Then we use the maximal subgraph as the cleaned data\nfor that identity. With the cleaned dataset, we update the existing face model\nand use the new model to filter the original dataset to get a larger cleaned\ndataset. We collect a large weakly labeled dataset containing 530,560 Asian\nface images of 7,962 identities from the Internet, which will be published for\nthe study of face recognition. By running the filtering process, we obtain a\ncleaned datasets (99.7+% purity) of size 223,767 (recall 70.9%). On our testing\ndataset of Asian faces, the model trained by the cleaned dataset achieves\nrecognition rate 93.1%, which obviously outperforms the model trained by the\npublic dataset CASIA whose recognition rate is 85.9%.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:11:21 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Ding", "Shengyong", ""], ["Wu", "Junyu", ""], ["Xu", "Wei", ""], ["Chao", "Hongyang", ""]]}, {"id": "1611.08131", "submitter": "Raghavendra Selvan", "authors": "Raghavendra Selvan, Jens Petersen, Jesper H. Pedersen, Marleen de\n  Bruijne", "title": "Extraction of airway trees using multiple hypothesis tracking and\n  template matching", "comments": "12 pages. Presented at the MICCAI Pulmonary Image Analysis Workshop,\n  Athens, Greece, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of airway tree morphology has important clinical applications in\ndiagnosis of chronic obstructive pulmonary disease. We present an automatic\ntree extraction method based on multiple hypothesis tracking and template\nmatching for this purpose and evaluate its performance on chest CT images. The\nmethod is adapted from a semi-automatic method devised for vessel segmentation.\nIdealized tubular templates are constructed that match airway probability\nobtained from a trained classifier and ranked based on their relative\nsignificance. Several such regularly spaced templates form the local hypotheses\nused in constructing a multiple hypothesis tree, which is then traversed to\nreach decisions. The proposed modifications remove the need for local\nthresholding of hypotheses as decisions are made entirely based on statistical\ncomparisons involving the hypothesis tree. The results show improvements in\nperformance when compared to the original method and region growing on\nintensity images. We also compare the method with region growing on the\nprobability images, where the presented method does not show substantial\nimprovement, but we expect it to be less sensitive to local anomalies in the\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 10:42:07 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Selvan", "Raghavendra", ""], ["Petersen", "Jens", ""], ["Pedersen", "Jesper H.", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1611.08134", "submitter": "Pedro A. Mar\\'in-Reyes", "authors": "Pedro A. Mar\\'in-Reyes, Javier Lorenzo-Navarro, Modesto\n  Castrill\\'on-Santana", "title": "Comparative study of histogram distance measures for re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Color based re-identification methods usually rely on a distance function to\nmeasure the similarity between individuals. In this paper we study the behavior\nof several histogram distance measures in different color spaces. We wonder\nwhether there is a particular histogram distance measure better than others,\nlikewise also, if there is a color space that present better discrimination\nfeatures. Several experiments are designed and evaluated in several images to\nobtain measures against various color spaces. We test in several image\ndatabases. A measure ranking is generated to calculate the area under the CMC,\nthis area is the indicator used to evaluate which distance measure and color\nspace present the best performance for the considered databases. Also, other\nparameters such as the image division in horizontal stripes and number of\nhistogram bins, have been studied.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 10:59:33 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Mar\u00edn-Reyes", "Pedro A.", ""], ["Lorenzo-Navarro", "Javier", ""], ["Castrill\u00f3n-Santana", "Modesto", ""]]}, {"id": "1611.08194", "submitter": "Naila Murray", "authors": "Naila Murray, Herv\\'e J\\'egou, Florent Perronnin and Andrew Zisserman", "title": "Interferences in match kernels", "comments": "Accepted as regular paper in IEEE Transactions on Pattern Analysis\n  and Machine Intelligence (TPAMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of an image representation that embeds and aggregates\na set of local descriptors into a single vector. Popular representations of\nthis kind include the bag-of-visual-words, the Fisher vector and the VLAD. When\ntwo such image representations are compared with the dot-product, the\nimage-to-image similarity can be interpreted as a match kernel. In match\nkernels, one has to deal with interference, i.e. with the fact that even if two\ndescriptors are unrelated, their matching score may contribute to the overall\nsimilarity.\n  We formalise this problem and propose two related solutions, both aimed at\nequalising the individual contributions of the local descriptors in the final\nrepresentation. These methods modify the aggregation stage by including a set\nof per-descriptor weights. They differ by the objective function that is\noptimised to compute those weights. The first is a \"democratisation\" strategy\nthat aims at equalising the relative importance of each descriptor in the set\ncomparison metric. The second one involves equalising the match of a single\ndescriptor to the aggregated vector.\n  These concurrent methods give a substantial performance boost over the state\nof the art in image search with short or mid-size vectors, as demonstrated by\nour experiments on standard public image retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 14:25:43 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Murray", "Naila", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Perronnin", "Florent", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1611.08195", "submitter": "Piotr Koniusz", "authors": "Piotr Koniusz and Yusuf Tas and Fatih Porikli", "title": "Domain Adaptation by Mixture of Alignments of Second- or Higher-Order\n  Scatter Tensors", "comments": "CVPR'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to the domain adaptation, dubbed\nSecond- or Higher-order Transfer of Knowledge (So-HoT), based on the mixture of\nalignments of second- or higher-order scatter statistics between the source and\ntarget domains. The human ability to learn from few labeled samples is a\nrecurring motivation in the literature for domain adaptation. Towards this end,\nwe investigate the supervised target scenario for which few labeled target\ntraining samples per category exist. Specifically, we utilize two CNN streams:\nthe source and target networks fused at the classifier level. Features from the\nfully connected layers fc7 of each network are used to compute second- or even\nhigher-order scatter tensors; one per network stream per class. As the source\nand target distributions are somewhat different despite being related, we align\nthe scatters of the two network streams of the same class (within-class\nscatters) to a desired degree with our bespoke loss while maintaining good\nseparation of the between-class scatters. We train the entire network in\nend-to-end fashion. We provide evaluations on the standard Office benchmark\n(visual domains), RGB-D combined with Caltech256 (depth-to-rgb transfer) and\nPascal VOC2007 combined with the TU Berlin dataset (image-to-sketch transfer).\nWe attain state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 14:27:08 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 13:43:24 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Koniusz", "Piotr", ""], ["Tas", "Yusuf", ""], ["Porikli", "Fatih", ""]]}, {"id": "1611.08207", "submitter": "Nikolay Jetchev", "authors": "Nikolay Jetchev, Urs Bergmann, Roland Vollgraf", "title": "Texture Synthesis with Spatial Generative Adversarial Networks", "comments": "presented at the NIPS 2016 adversarial learning workshop, Barcelona,\n  Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a recent approach to train\ngenerative models of data, which have been shown to work particularly well on\nimage data. In the current paper we introduce a new model for texture synthesis\nbased on GAN learning. By extending the input noise distribution space from a\nsingle vector to a whole spatial tensor, we create an architecture with\nproperties well suited to the task of texture synthesis, which we call spatial\nGAN (SGAN). To our knowledge, this is the first successful completely\ndata-driven texture synthesis method based on GANs.\n  Our method has the following features which make it a state of the art\nalgorithm for texture synthesis: high image quality of the generated textures,\nvery high scalability w.r.t. the output texture size, fast real-time forward\ngeneration, the ability to fuse multiple diverse source images in complex\ntextures. To illustrate these capabilities we present multiple experiments with\ndifferent classes of texture images and use cases. We also discuss some\nlimitations of our method with respect to the types of texture images it can\nsynthesize, and compare it to other neural techniques for texture generation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:01:42 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 11:32:11 GMT"}, {"version": "v3", "created": "Thu, 18 May 2017 13:42:04 GMT"}, {"version": "v4", "created": "Fri, 8 Sep 2017 15:09:52 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Jetchev", "Nikolay", ""], ["Bergmann", "Urs", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1611.08215", "submitter": "Andrea Palazzi", "authors": "Andrea Palazzi, Francesco Solera, Simone Calderara, Stefano Alletto,\n  Rita Cucchiara", "title": "Learning Where to Attend Like a Human Driver", "comments": "To appear in IEEE Intelligent Vehicles Symposium 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the advent of autonomous cars, it's likely - at least in the near\nfuture - that human attention will still maintain a central role as a guarantee\nin terms of legal responsibility during the driving task. In this paper we\nstudy the dynamics of the driver's gaze and use it as a proxy to understand\nrelated attentional mechanisms. First, we build our analysis upon two\nquestions: where and what the driver is looking at? Second, we model the\ndriver's gaze by training a coarse-to-fine convolutional network on short\nsequences extracted from the DR(eye)VE dataset. Experimental comparison against\ndifferent baselines reveal that the driver's gaze can indeed be learnt to some\nextent, despite i) being highly subjective and ii) having only one driver's\ngaze available for each sequence due to the irreproducibility of the scene.\nEventually, we advocate for a new assisted driving paradigm which suggests to\nthe driver, with no intervention, where she should focus her attention.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:14:23 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:24:16 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Palazzi", "Andrea", ""], ["Solera", "Francesco", ""], ["Calderara", "Simone", ""], ["Alletto", "Stefano", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1611.08240", "submitter": "Nishant Rai", "authors": "Amlan Kar, Nishant Rai, Karan Sikka, Gaurav Sharma", "title": "AdaScan: Adaptive Scan Pooling in Deep Convolutional Neural Networks for\n  Human Action Recognition in Videos", "comments": "CVPR 2017 Camera Ready Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for temporally pooling frames in a video for the\ntask of human action recognition. The method is motivated by the observation\nthat there are only a small number of frames which, together, contain\nsufficient information to discriminate an action class present in a video, from\nthe rest. The proposed method learns to pool such discriminative and\ninformative frames, while discarding a majority of the non-informative frames\nin a single temporal scan of the video. Our algorithm does so by continuously\npredicting the discriminative importance of each video frame and subsequently\npooling them in a deep learning framework. We show the effectiveness of our\nproposed pooling method on standard benchmarks where it consistently improves\non baseline pooling methods, with both RGB and optical flow based Convolutional\nnetworks. Further, in combination with complementary video representations, we\nshow results that are competitive with respect to the state-of-the-art results\non two challenging and publicly available benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 16:26:11 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 18:04:51 GMT"}, {"version": "v3", "created": "Fri, 9 Jun 2017 16:20:12 GMT"}, {"version": "v4", "created": "Sun, 25 Jun 2017 08:55:48 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Kar", "Amlan", ""], ["Rai", "Nishant", ""], ["Sikka", "Karan", ""], ["Sharma", "Gaurav", ""]]}, {"id": "1611.08258", "submitter": "Ali Diba", "authors": "Ali Diba, Vivek Sharma, Ali Pazandeh, Hamed Pirsiavash, Luc Van Gool", "title": "Weakly Supervised Cascaded Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection is a challenging task in visual understanding domain, and\neven more so if the supervision is to be weak. Recently, few efforts to handle\nthe task without expensive human annotations is established by promising deep\nneural network. A new architecture of cascaded networks is proposed to learn a\nconvolutional neural network (CNN) under such conditions. We introduce two such\narchitectures, with either two cascade stages or three which are trained in an\nend-to-end pipeline. The first stage of both architectures extracts best\ncandidate of class specific region proposals by training a fully convolutional\nnetwork. In the case of the three stage architecture, the middle stage provides\nobject segmentation, using the output of the activation maps of first stage.\nThe final stage of both architectures is a part of a convolutional neural\nnetwork that performs multiple instance learning on proposals extracted in the\nprevious stage(s). Our experiments on the PASCAL VOC 2007, 2010, 2012 and large\nscale object datasets, ILSVRC 2013, 2014 datasets show improvements in the\nareas of weakly-supervised object detection, classification and localization.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:07:48 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Diba", "Ali", ""], ["Sharma", "Vivek", ""], ["Pazandeh", "Ali", ""], ["Pirsiavash", "Hamed", ""], ["Van Gool", "Luc", ""]]}, {"id": "1611.08272", "submitter": "Alexander Kirillov", "authors": "Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan\n  Savchynskyy, Carsten Rother", "title": "InstanceCut: from Edges to Instances with MultiCut", "comments": "The code would be released at\n  https://github.com/alexander-kirillov/InstanceCut", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the task of instance-aware semantic segmentation. Our key\nmotivation is to design a simple method with a new modelling-paradigm, which\ntherefore has a different trade-off between advantages and disadvantages\ncompared to known approaches. Our approach, we term InstanceCut, represents the\nproblem by two output modalities: (i) an instance-agnostic semantic\nsegmentation and (ii) all instance-boundaries. The former is computed from a\nstandard convolutional neural network for semantic segmentation, and the latter\nis derived from a new instance-aware edge detection model. To reason globally\nabout the optimal partitioning of an image into instances, we combine these two\nmodalities into a novel MultiCut formulation. We evaluate our approach on the\nchallenging CityScapes dataset. Despite the conceptual simplicity of our\napproach, we achieve the best result among all published methods, and perform\nparticularly well for rare object classes.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:54:32 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Kirillov", "Alexander", ""], ["Levinkov", "Evgeny", ""], ["Andres", "Bjoern", ""], ["Savchynskyy", "Bogdan", ""], ["Rother", "Carsten", ""]]}, {"id": "1611.08280", "submitter": "Chiwoo Park", "authors": "Xin Li, Alex Belianinov, Ondrej Dyck, Stephen Jesse, and Chiwoo Park", "title": "Two-Level Structural Sparsity Regularization for Identifying Lattices\n  and Defects in Noisy Images", "comments": "Accepted to Annals of Applied Statistics", "journal-ref": "Annals of Applied Statistics 2018, Vol. 12, No. 1, 348-377", "doi": "10.1214/17-AOAS1096", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a regularized regression model with a two-level\nstructural sparsity penalty applied to locate individual atoms in a noisy\nscanning transmission electron microscopy image (STEM). In crystals, the\nlocations of atoms is symmetric, condensed into a few lattice groups.\nTherefore, by identifying the underlying lattice in a given image, individual\natoms can be accurately located. We propose to formulate the identification of\nthe lattice groups as a sparse group selection problem. Furthermore, real\natomic scale images contain defects and vacancies, so atomic identification\nbased solely on a lattice group may result in false positives and false\nnegatives. To minimize error, model includes an individual sparsity\nregularization in addition to the group sparsity for a within-group selection,\nwhich results in a regression model with a two-level sparsity regularization.\nWe propose a modification of the group orthogonal matching pursuit (gOMP)\nalgorithm with a thresholding step to solve the atom finding problem. The\nconvergence and statistical analyses of the proposed algorithm are presented.\nThe proposed algorithm is also evaluated through numerical experiments with\nsimulated images. The applicability of the algorithm on determination of atom\nstructures and identification of imaging distortions and atomic defects was\ndemonstrated using three real STEM images. We believe this is an important step\ntoward automatic phase identification and assignment with the advent of genomic\ndatabases for materials.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 18:24:58 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 02:22:11 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 18:41:02 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 19:22:15 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "Xin", ""], ["Belianinov", "Alex", ""], ["Dyck", "Ondrej", ""], ["Jesse", "Stephen", ""], ["Park", "Chiwoo", ""]]}, {"id": "1611.08303", "submitter": "Min Bai", "authors": "Min Bai, Raquel Urtasun", "title": "Deep Watershed Transform for Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most contemporary approaches to instance segmentation use complex pipelines\ninvolving conditional random fields, recurrent neural networks, object\nproposals, or template matching schemes. In our paper, we present a simple yet\npowerful end-to-end convolutional neural network to tackle this task. Our\napproach combines intuitions from the classical watershed transform and modern\ndeep learning to produce an energy map of the image where object instances are\nunambiguously represented as basins in the energy map. We then perform a cut at\na single energy level to directly yield connected components corresponding to\nobject instances. Our model more than doubles the performance of the\nstate-of-the-art on the challenging Cityscapes Instance Level Segmentation\ntask.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 20:46:33 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 21:13:12 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Bai", "Min", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1611.08321", "submitter": "Junhua Mao", "authors": "Junhua Mao, Jiajing Xu, Yushi Jing, Alan Yuille", "title": "Training and Evaluating Multimodal Word Embeddings with Large-scale Web\n  Annotated Images", "comments": "Appears in NIPS 2016. The datasets introduced in this work will be\n  gradually released on the project page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on training and evaluating effective word embeddings\nwith both text and visual information. More specifically, we introduce a\nlarge-scale dataset with 300 million sentences describing over 40 million\nimages crawled and downloaded from publicly available Pins (i.e. an image with\nsentence descriptions uploaded by users) on Pinterest. This dataset is more\nthan 200 times larger than MS COCO, the standard large-scale image dataset with\nsentence descriptions. In addition, we construct an evaluation dataset to\ndirectly assess the effectiveness of word embeddings in terms of finding\nsemantically similar or related words and phrases. The word/phrase pairs in\nthis evaluation dataset are collected from the click data with millions of\nusers in an image search system, thus contain rich semantic relationships.\nBased on these datasets, we propose and compare several Recurrent Neural\nNetworks (RNNs) based multimodal (text and image) models. Experiments show that\nour model benefits from incorporating the visual information into the word\nembeddings, and a weight sharing strategy is crucial for learning such\nmultimodal embeddings. The project page is:\nhttp://www.stat.ucla.edu/~junhua.mao/multimodal_embedding.html\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 23:15:56 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Mao", "Junhua", ""], ["Xu", "Jiajing", ""], ["Jing", "Yushi", ""], ["Yuille", "Alan", ""]]}, {"id": "1611.08323", "submitter": "Tobias Pohlen", "authors": "Tobias Pohlen, Alexander Hermans, Markus Mathias, Bastian Leibe", "title": "Full-Resolution Residual Networks for Semantic Segmentation in Street\n  Scenes", "comments": "Changes in v2: Fixed equation (10), fixed legend of Figure 6, fixed\n  legend of Figure 9, added page numbers, fixed minor spelling mistakes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is an essential component of modern autonomous\ndriving systems, as an accurate understanding of the surrounding scene is\ncrucial to navigation and action planning. Current state-of-the-art approaches\nin semantic image segmentation rely on pre-trained networks that were initially\ndeveloped for classifying images as a whole. While these networks exhibit\noutstanding recognition performance (i.e., what is visible?), they lack\nlocalization accuracy (i.e., where precisely is something located?). Therefore,\nadditional processing steps have to be performed in order to obtain\npixel-accurate segmentation masks at the full image resolution. To alleviate\nthis problem we propose a novel ResNet-like architecture that exhibits strong\nlocalization and recognition performance. We combine multi-scale context with\npixel-level accuracy by using two processing streams within our network: One\nstream carries information at the full image resolution, enabling precise\nadherence to segment boundaries. The other stream undergoes a sequence of\npooling operations to obtain robust features for recognition. The two streams\nare coupled at the full image resolution using residuals. Without additional\nprocessing steps and without pre-training, our approach achieves an\nintersection-over-union score of 71.8% on the Cityscapes dataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 23:55:28 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 19:36:19 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Pohlen", "Tobias", ""], ["Hermans", "Alexander", ""], ["Mathias", "Markus", ""], ["Leibe", "Bastian", ""]]}, {"id": "1611.08350", "submitter": "Samitha Herath", "authors": "Samitha Herath, Mehrtash Harandi, and Fatih Porikli", "title": "Learning an Invariant Hilbert Space for Domain Adaptation", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a learning scheme to construct a Hilbert space (i.e., a\nvector space along its inner product) to address both unsupervised and\nsemi-supervised domain adaptation problems. This is achieved by learning\nprojections from each domain to a latent space along the Mahalanobis metric of\nthe latent space to simultaneously minimizing a notion of domain variance while\nmaximizing a measure of discriminatory power. In particular, we make use of the\nRiemannian optimization techniques to match statistical properties (e.g., first\nand second order statistics) between samples projected into the latent space\nfrom different domains. Upon availability of class labels, we further deem\nsamples sharing the same label to form more compact clusters while pulling away\nsamples coming from different classes.We extensively evaluate and contrast our\nproposal against state-of-the-art methods for the task of visual domain\nadaptation using both handcrafted and deep-net features. Our experiments show\nthat even with a simple nearest neighbor classifier, the proposed method can\noutperform several state-of-the-art methods benefitting from more involved\nclassification schemes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 04:26:35 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 09:12:00 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Herath", "Samitha", ""], ["Harandi", "Mehrtash", ""], ["Porikli", "Fatih", ""]]}, {"id": "1611.08387", "submitter": "Shuochen Su", "authors": "Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang\n  Heidrich, Oliver Wang", "title": "Deep Video Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motion blur from camera shake is a major problem in videos captured by\nhand-held devices. Unlike single-image deblurring, video-based approaches can\ntake advantage of the abundant information that exists across neighboring\nframes. As a result the best performing methods rely on aligning nearby frames.\nHowever, aligning images is a computationally expensive and fragile procedure,\nand methods that aggregate information must therefore be able to identify which\nregions have been accurately aligned and which have not, a task which requires\nhigh level scene understanding. In this work, we introduce a deep learning\nsolution to video deblurring, where a CNN is trained end-to-end to learn how to\naccumulate information across frames. To train this network, we collected a\ndataset of real videos recorded with a high framerate camera, which we use to\ngenerate synthetic motion blur for supervision. We show that the features\nlearned from this dataset extend to deblurring motion blur that arises due to\ncamera shake in a wide range of videos, and compare the quality of results to a\nnumber of other baselines.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 08:51:51 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Su", "Shuochen", ""], ["Delbracio", "Mauricio", ""], ["Wang", "Jue", ""], ["Sapiro", "Guillermo", ""], ["Heidrich", "Wolfgang", ""], ["Wang", "Oliver", ""]]}, {"id": "1611.08389", "submitter": "Guang Jiang", "authors": "Huan Lei, Guang Jiang, Long Quan", "title": "Color Constancy with Derivative Colors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the illuminant color is well contained in both achromatic\nregions and the specular components of highlight regions. In this paper, we\npropose a novel way to achieve color constancy by exploiting such clues. The\nkey to our approach lies in the use of suitably extracted derivative colors,\nwhich are able to compute the illuminant color robustly with kernel density\nestimation. While extracting derivative colors from achromatic regions to\napproximate the illuminant color well is basically straightforward, the success\nof our extraction in highlight regions is attributed to the different rates of\nvariation of the diffuse and specular magnitudes in the dichromatic reflection\nmodel. The proposed approach requires no training phase and is simple to\nimplement. More significantly, it performs quite satisfactorily under\ninter-database parameter settings. Our experiments on three standard databases\ndemonstrate its effectiveness and fine performance in comparison to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 09:06:18 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Lei", "Huan", ""], ["Jiang", "Guang", ""], ["Quan", "Long", ""]]}, {"id": "1611.08402", "submitter": "Davide Boscaini", "authors": "Federico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol\\`a,\n  Jan Svoboda, Michael M. Bronstein", "title": "Geometric deep learning on graphs and manifolds using mixture model CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved a remarkable performance breakthrough in several\nfields, most notably in speech recognition, natural language processing, and\ncomputer vision. In particular, convolutional neural network (CNN)\narchitectures currently produce state-of-the-art performance on a variety of\nimage analysis tasks such as object detection and recognition. Most of deep\nlearning research has so far focused on dealing with 1D, 2D, or 3D\nEuclidean-structured data such as acoustic signals, images, or videos.\nRecently, there has been an increasing interest in geometric deep learning,\nattempting to generalize deep learning methods to non-Euclidean structured data\nsuch as graphs and manifolds, with a variety of applications from the domains\nof network analysis, computational social science, or computer graphics. In\nthis paper, we propose a unified framework allowing to generalize CNN\narchitectures to non-Euclidean domains (graphs and manifolds) and learn local,\nstationary, and compositional task-specific features. We show that various\nnon-Euclidean CNN methods previously proposed in the literature can be\nconsidered as particular instances of our framework. We test the proposed\nmethod on standard tasks from the realms of image-, graph- and 3D shape\nanalysis and show that it consistently outperforms previous approaches.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 10:05:03 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 10:06:39 GMT"}, {"version": "v3", "created": "Tue, 6 Dec 2016 21:38:12 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Monti", "Federico", ""], ["Boscaini", "Davide", ""], ["Masci", "Jonathan", ""], ["Rodol\u00e0", "Emanuele", ""], ["Svoboda", "Jan", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1611.08408", "submitter": "Pauline Luc", "authors": "Pauline Luc and Camille Couprie and Soumith Chintala and Jakob Verbeek", "title": "Semantic Segmentation using Adversarial Networks", "comments": null, "journal-ref": "NIPS Workshop on Adversarial Training, Dec 2016, Barcelona, Spain", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training has been shown to produce state of the art results for\ngenerative image modeling. In this paper we propose an adversarial training\napproach to train semantic segmentation models. We train a convolutional\nsemantic segmentation network along with an adversarial network that\ndiscriminates segmentation maps coming either from the ground truth or from the\nsegmentation network. The motivation for our approach is that it can detect and\ncorrect higher-order inconsistencies between ground truth segmentation maps and\nthe ones produced by the segmentation net. Our experiments show that our\nadversarial training approach leads to improved accuracy on the Stanford\nBackground and PASCAL VOC 2012 datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 10:36:30 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Luc", "Pauline", ""], ["Couprie", "Camille", ""], ["Chintala", "Soumith", ""], ["Verbeek", "Jakob", ""]]}, {"id": "1611.08461", "submitter": "Alan Lukezic", "authors": "Alan Luke\\v{z}i\\v{c}, Tom\\'a\\v{s} Voj\\'i\\v{r}, Luka \\v{C}ehovin,\n  Ji\\v{r}\\'i Matas and Matej Kristan", "title": "Discriminative Correlation Filter with Channel and Spatial Reliability", "comments": "Accepted to: International Journal of Computer Vision:\n  https://link.springer.com/article/10.1007/s11263-017-1061-3", "journal-ref": null, "doi": "10.1007/s11263-017-1061-3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term tracking is an open and challenging problem for which\ndiscriminative correlation filters (DCF) have shown excellent performance. We\nintroduce the channel and spatial reliability concepts to DCF tracking and\nprovide a novel learning algorithm for its efficient and seamless integration\nin the filter update and the tracking process. The spatial reliability map\nadjusts the filter support to the part of the object suitable for tracking.\nThis both allows to enlarge the search region and improves tracking of\nnon-rectangular objects. Reliability scores reflect channel-wise quality of the\nlearned filters and are used as feature weighting coefficients in localization.\nExperimentally, with only two simple standard features, HoGs and Colornames,\nthe novel CSR-DCF method -- DCF with Channel and Spatial Reliability --\nachieves state-of-the-art results on VOT 2016, VOT 2015 and OTB100. The CSR-DCF\nruns in real-time on a CPU.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 14:18:52 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 11:31:48 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 08:05:50 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Luke\u017ei\u010d", "Alan", ""], ["Voj\u00ed\u0159", "Tom\u00e1\u0161", ""], ["\u010cehovin", "Luka", ""], ["Matas", "Ji\u0159\u00ed", ""], ["Kristan", "Matej", ""]]}, {"id": "1611.08472", "submitter": "Vardan Papyan", "authors": "Vardan Papyan and Ronen Talmon", "title": "Multimodal Latent Variable Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of multiple, multimodal sensors capturing a complex system or\na physical phenomenon of interest. Our primary goal is to distinguish the\nunderlying sources of variability manifested in the measured data. The first\nstep in our analysis is to find the common source of variability present in all\nsensor measurements. We base our work on a recent paper, which tackles this\nproblem with alternating diffusion (AD). In this work, we suggest to further\nthe analysis by extracting the sensor-specific variables in addition to the\ncommon source. We propose an algorithm, which we analyze theoretically, and\nthen demonstrate on three different applications: a synthetic example, a toy\nproblem, and the task of fetal ECG extraction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 14:38:50 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Papyan", "Vardan", ""], ["Talmon", "Ronen", ""]]}, {"id": "1611.08481", "submitter": "Harm de Vries", "authors": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo\n  Larochelle, Aaron Courville", "title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "comments": "23 pages; CVPR 2017 submission; see https://guesswhat.ai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GuessWhat?!, a two-player guessing game as a testbed for\nresearch on the interplay of computer vision and dialogue systems. The goal of\nthe game is to locate an unknown object in a rich image scene by asking a\nsequence of questions. Higher-level image understanding, like spatial reasoning\nand language grounding, is required to solve the proposed task. Our key\ncontribution is the collection of a large-scale dataset consisting of 150K\nhuman-played games with a total of 800K visual question-answer pairs on 66K\nimages. We explain our design decisions in collecting the dataset and introduce\nthe oracle and questioner tasks that are associated with the two players of the\ngame. We prototyped deep learning models to establish initial baselines of the\nintroduced tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 20:56:13 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 12:52:53 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Chandar", "Sarath", ""], ["Pietquin", "Olivier", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1611.08512", "submitter": "Xiatian Zhu", "authors": "Xiaolong Ma, Xiatian Zhu, Shaogang Gong, Xudong Xie, Jianming Hu,\n  Kin-Man Lam, Yisheng Zhong", "title": "Person Re-Identification by Unsupervised Video Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (ReID) methods rely only on the\nspatial appearance information from either one or multiple person images,\nwhilst ignore the space-time cues readily available in video or image-sequence\ndata. Moreover, they often assume the availability of exhaustively labelled\ncross-view pairwise data for every camera pair, making them non-scalable to\nReID applications in real-world large scale camera networks. In this work, we\nintroduce a novel video based person ReID method capable of accurately matching\npeople across views from arbitrary unaligned image-sequences without any\nlabelled pairwise data. Specifically, we introduce a new space-time person\nrepresentation by encoding multiple granularities of spatio-temporal dynamics\nin form of time series. Moreover, a Time Shift Dynamic Time Warping (TS-DTW)\nmodel is derived for performing automatically alignment whilst achieving data\nselection and matching between inherently inaccurate and incomplete sequences\nin a unified way. We further extend the TS-DTW model for accommodating multiple\nfeature-sequences of an image-sequence in order to fuse information from\ndifferent descriptions. Crucially, this model does not require pairwise\nlabelled training data (i.e. unsupervised) therefore readily scalable to large\nscale camera networks of arbitrary camera pairs without the need for exhaustive\ndata annotation for every camera pair. We show the effectiveness and advantages\nof the proposed method by extensive comparisons with related state-of-the-art\napproaches using two benchmarking ReID datasets, PRID2011 and iLIDS-VID.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 16:47:39 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 09:20:29 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ma", "Xiaolong", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""], ["Xie", "Xudong", ""], ["Hu", "Jianming", ""], ["Lam", "Kin-Man", ""], ["Zhong", "Yisheng", ""]]}, {"id": "1611.08527", "submitter": "Eric Heim", "authors": "Eric Heim, Alexander Seitel, Jonas Andrulis, Fabian Isensee, Christian\n  Stock, Tobias Ross and Lena Maier-Hein", "title": "Clickstream analysis for crowd-based object segmentation with confidence", "comments": "to appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2017.2777967", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly increasing interest in machine learning based solutions for\nautomatic image annotation, the availability of reference annotations for\nalgorithm training is one of the major bottlenecks in the field. Crowdsourcing\nhas evolved as a valuable option for low-cost and large-scale data annotation;\nhowever, quality control remains a major issue which needs to be addressed. To\nour knowledge, we are the first to analyze the annotation process to improve\ncrowd-sourced image segmentation. Our method involves training a regressor to\nestimate the quality of a segmentation from the annotator's clickstream data.\nThe quality estimation can be used to identify spam and weight individual\nannotations by their (estimated) quality when merging multiple segmentations of\none image. Using a total of 29,000 crowd annotations performed on publicly\navailable data of different object classes, we show that (1) our method is\nhighly accurate in estimating the segmentation quality based on clickstream\ndata, (2) outperforms state-of-the-art methods for merging multiple\nannotations. As the regressor does not need to be trained on the object class\nthat it is applied to it can be regarded as a low-cost option for quality\ncontrol and confidence analysis in the context of crowd-based image annotation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 17:29:58 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 14:55:33 GMT"}, {"version": "v3", "created": "Fri, 20 Oct 2017 13:45:50 GMT"}, {"version": "v4", "created": "Wed, 29 Nov 2017 13:15:26 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Heim", "Eric", ""], ["Seitel", "Alexander", ""], ["Andrulis", "Jonas", ""], ["Isensee", "Fabian", ""], ["Stock", "Christian", ""], ["Ross", "Tobias", ""], ["Maier-Hein", "Lena", ""]]}, {"id": "1611.08563", "submitter": "Gurkirt Singh", "authors": "Gurkirt Singh and Suman Saha and Michael Sapienza and Philip Torr and\n  Fabio Cuzzolin", "title": "Online Real-time Multiple Spatiotemporal Action Localisation and\n  Prediction", "comments": "10 pages 3 figures, ICCV 2017, Added link to new annotations of\n  ucf101-24", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep-learning framework for real-time multiple spatio-temporal\n(S/T) action localisation, classification and early prediction. Current\nstate-of-the-art approaches work offline and are too slow to be useful in real-\nworld settings. To overcome their limitations we introduce two major\ndevelopments. Firstly, we adopt real-time SSD (Single Shot MultiBox Detector)\nconvolutional neural networks to regress and classify detection boxes in each\nvideo frame potentially containing an action of interest. Secondly, we design\nan original and efficient online algorithm to incrementally construct and label\n`action tubes' from the SSD frame level detections. As a result, our system is\nnot only capable of performing S/T detection in real time, but can also perform\nearly action prediction in an online fashion. We achieve new state-of-the-art\nresults in both S/T action localisation and early action prediction on the\nchallenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top\noffline competitors. To the best of our knowledge, ours is the first real-time\n(up to 40fps) system able to perform online S/T action localisation and early\naction prediction on the untrimmed videos of UCF101-24.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 19:21:36 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 15:48:36 GMT"}, {"version": "v3", "created": "Fri, 17 Feb 2017 14:53:24 GMT"}, {"version": "v4", "created": "Fri, 7 Apr 2017 13:04:31 GMT"}, {"version": "v5", "created": "Sun, 13 Aug 2017 20:51:04 GMT"}, {"version": "v6", "created": "Thu, 24 Aug 2017 09:15:13 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Singh", "Gurkirt", ""], ["Saha", "Suman", ""], ["Sapienza", "Michael", ""], ["Torr", "Philip", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1611.08583", "submitter": "Ari Seff", "authors": "Ari Seff and Jianxiong Xiao", "title": "Learning from Maps: Visual Common Sense for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's autonomous vehicles rely extensively on high-definition 3D maps to\nnavigate the environment. While this approach works well when these maps are\ncompletely up-to-date, safe autonomous vehicles must be able to corroborate the\nmap's information via a real time sensor-based system. Our goal in this work is\nto develop a model for road layout inference given imagery from on-board\ncameras, without any reliance on high-definition maps. However, no sufficient\ndataset for training such a model exists. Here, we leverage the availability of\nstandard navigation maps and corresponding street view images to construct an\nautomatically labeled, large-scale dataset for this complex scene understanding\nproblem. By matching road vectors and metadata from navigation maps with Google\nStreet View images, we can assign ground truth road layout attributes (e.g.,\ndistance to an intersection, one-way vs. two-way street) to the images. We then\ntrain deep convolutional networks to predict these road layout attributes given\na single monocular RGB image. Experimental evaluation demonstrates that our\nmodel learns to correctly infer the road attributes using only panoramas\ncaptured by car-mounted cameras as input. Additionally, our results indicate\nthat this method may be suitable to the novel application of recommending\nsafety improvements to infrastructure (e.g., suggesting an alternative speed\nlimit for a street).\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 20:56:55 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 22:24:52 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Seff", "Ari", ""], ["Xiao", "Jianxiong", ""]]}, {"id": "1611.08588", "submitter": "Sanghoon Hong", "authors": "Sanghoon Hong, Byungseok Roh, Kye-Hyeon Kim, Yeongjae Cheon, Minje\n  Park", "title": "PVANet: Lightweight Deep Neural Networks for Real-time Object Detection", "comments": "Presented at NIPS 2016 Workshop on Efficient Methods for Deep Neural\n  Networks (EMDNN). Continuation of arXiv:1608.08021. The affiliation has been\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In object detection, reducing computational cost is as important as improving\naccuracy for most practical usages. This paper proposes a novel network\nstructure, which is an order of magnitude lighter than other state-of-the-art\nnetworks while maintaining the accuracy. Based on the basic principle of more\nlayers with less channels, this new deep neural network minimizes its\nredundancy by adopting recent innovations including C.ReLU and Inception\nstructure. We also show that this network can be trained efficiently to achieve\nsolid results on well-known object detection benchmarks: 84.9% and 84.2% mAP on\nVOC2007 and VOC2012 while the required compute is less than 10% of the recent\nResNet-101.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 17:43:28 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 22:30:17 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Hong", "Sanghoon", ""], ["Roh", "Byungseok", ""], ["Kim", "Kye-Hyeon", ""], ["Cheon", "Yeongjae", ""], ["Park", "Minje", ""]]}, {"id": "1611.08624", "submitter": "Odemir Bruno PhD", "authors": "Lucas Correia Ribas, Odemir Martinez Bruno", "title": "Fast deterministic tourist walk for texture analysis", "comments": "7 page, 7 figure", "journal-ref": "WVC 2016 proceedings p45-50", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic tourist walk (DTW) has attracted increasing interest in\ncomputer vision. In the last years, different methods for analysis of dynamic\nand static textures were proposed. So far, all works based on the DTW for\ntexture analysis use all image pixels as initial point of a walk. However, this\nrequires much runtime. In this paper, we conducted a study to verify the\nperformance of the DTW method according to the number of initial points to\nstart a walk. The proposed method assigns a unique code to each image pixel,\nthen, the pixels whose code is not divisible by a given $k$ value are ignored\nas initial points of walks. Feature vectors were extracted and a classification\nprocess was performed for different percentages of initial points. Experimental\nresults on the Brodatz and Vistex datasets indicate that to use fewer pixels as\ninitial points significantly improves the runtime compared to use all image\npixels. In addition, the correct classification rate decreases very little.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 22:21:05 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ribas", "Lucas Correia", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1611.08625", "submitter": "Duy Thai Hoang", "authors": "Duy Hoang Thai and David Banks", "title": "Directional Mean Curvature for Textured Image Demixing", "comments": "72 pages, including main manuscript (36 pages) and Appendix (36\n  pages); 14 figures; journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation theory plays an important role in image processing, especially\nimage deconvolution and decomposition. For piecewise smooth images, there are\nmany methods that have been developed over the past thirty years. The goal of\nthis study is to devise similar and practical methodology for handling textured\nimages. This problem is motivated by forensic imaging, since fingerprints,\nshoeprints and bullet ballistic evidence are textured images. In particular, it\nis known that texture information is almost destroyed by a blur operator, such\nas a blurred ballistic image captured from a low-cost microscope. The\ncontribution of this work is twofold: first, we propose a mathematical model\nfor textured image deconvolution and decomposition into four meaningful\ncomponents, using a high-order partial differential equation approach based on\nthe directional mean curvature. Second, we uncover a link between functional\nanalysis and multiscale sampling theory, e.g., harmonic analysis and filter\nbanks. Both theoretical results and examples with natural images are provided\nto illustrate the performance of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 22:21:06 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Thai", "Duy Hoang", ""], ["Banks", "David", ""]]}, {"id": "1611.08629", "submitter": "Lucas Ribas", "authors": "Lucas Correia Ribas, Wesley Nunes Gon\\c{c}alves and Odemir Martinez\n  Bruno", "title": "Texture analysis using deterministic partially self-avoiding walk with\n  thresholds", "comments": "8 pages, 3 figures", "journal-ref": "WVC proceedings 2016, pages 39-44", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new texture analysis method using the\ndeterministic partially self-avoiding walk performed on maps modified with\nthresholds. In this method, two pixels of the map are neighbors if the\nEuclidean distance between them is less than $\\sqrt{2}$ and the weight\n(difference between its intensities) is less than a given threshold. The maps\nobtained by using different thresholds highlight several properties of the\nimage that are extracted by the deterministic walk. To compose the feature\nvector, deterministic walks are performed with different thresholds and its\nstatistics are concatenated. Thus, this approach can be considered as a\nmulti-scale analysis. We validate our method on the Brodatz database, which is\nvery well known public image database and widely used by texture analysis\nmethods. Experimental results indicate that the proposed method presents a good\ntexture discrimination, overcoming traditional texture methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 22:51:07 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ribas", "Lucas Correia", ""], ["Gon\u00e7alves", "Wesley Nunes", ""], ["Bruno", "Odemir Martinez", ""]]}, {"id": "1611.08657", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Tadas Baltru\\v{s}aitis, Louis-Philippe Morency", "title": "Convolutional Experts Constrained Local Model for Facial Landmark\n  Detection", "comments": "Accepted at CVPR-W 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Local Models (CLMs) are a well-established family of methods for\nfacial landmark detection. However, they have recently fallen out of favor to\ncascaded regression-based approaches. This is in part due to the inability of\nexisting CLM local detectors to model the very complex individual landmark\nappearance that is affected by expression, illumination, facial hair, makeup,\nand accessories. In our work, we present a novel local detector --\nConvolutional Experts Network (CEN) -- that brings together the advantages of\nneural architectures and mixtures of experts in an end-to-end framework. We\nfurther propose a Convolutional Experts Constrained Local Model (CE-CLM)\nalgorithm that uses CEN as local detectors. We demonstrate that our proposed\nCE-CLM algorithm outperforms competitive state-of-the-art baselines for facial\nlandmark detection by a large margin on four publicly-available datasets. Our\napproach is especially accurate and robust on challenging profile images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 04:47:34 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 16:00:45 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 18:03:56 GMT"}, {"version": "v4", "created": "Sun, 23 Jul 2017 10:15:06 GMT"}, {"version": "v5", "created": "Wed, 26 Jul 2017 19:46:15 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Zadeh", "Amir", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1611.08663", "submitter": "Xun Xu", "authors": "Xun Xu and Timothy M. Hospedales and Shaogang Gong", "title": "Multi-Task Zero-Shot Action Recognition with Prioritised Data\n  Augmentation", "comments": "Published in ECCV 2016", "journal-ref": null, "doi": "10.1007/978-3-319-46475-6_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-Shot Learning (ZSL) promises to scale visual recognition by bypassing\nthe conventional model training requirement of annotated examples for every\ncategory. This is achieved by establishing a mapping connecting low-level\nfeatures and a semantic description of the label space, referred as\nvisual-semantic mapping, on auxiliary data. Reusing the learned mapping to\nproject target videos into an embedding space thus allows novel-classes to be\nrecognised by nearest neighbour inference. However, existing ZSL methods suffer\nfrom auxiliary-target domain shift intrinsically induced by assuming the same\nmapping for the disjoint auxiliary and target classes. This compromises the\ngeneralisation accuracy of ZSL recognition on the target data. In this work, we\nimprove the ability of ZSL to generalise across this domain shift in both\nmodel- and data-centric ways by formulating a visual-semantic mapping with\nbetter generalisation properties and a dynamic data re-weighting method to\nprioritise auxiliary data that are relevant to the target classes.\nSpecifically: (1) We introduce a multi-task visual-semantic mapping to improve\ngeneralisation by constraining the semantic mapping parameters to lie on a\nlow-dimensional manifold, (2) We explore prioritised data augmentation by\nexpanding the pool of auxiliary data with additional instances weighted by\nrelevance to the target domain. The proposed new model is applied to the\nchallenging zero-shot action recognition problem to demonstrate its advantages\nover existing ZSL models.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 05:51:11 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Xu", "Xun", ""], ["Hospedales", "Timothy M.", ""], ["Gong", "Shaogang", ""]]}, {"id": "1611.08664", "submitter": "Varghese Alex Kollerathu Mr.", "authors": "Varghese Alex, Kiran Vaidhya, Subramaniam Thirunavukkarasu,\n  Chandrasekharan Kesavdas and Ganapathy Krishnamurthi", "title": "Semi-supervised Learning using Denoising Autoencoders for Brain Lesion\n  Detection and Segmentation", "comments": "11 Pages, 42 Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented explores the use of denoising autoencoders (DAE) for brain\nlesion detection, segmentation and false positive reduction. Stacked denoising\nautoencoders (SDAE) were pre-trained using a large number of unlabeled patient\nvolumes and fine tuned with patches drawn from a limited number of patients\n(n=20, 40, 65). The results show negligible loss in performance even when SDAE\nwas fine tuned using 20 patients. Low grade glioma (LGG) segmentation was\nachieved using a transfer learning approach wherein a network pre-trained with\nHigh Grade Glioma (HGG) data was fine tuned using LGG image patches. The weakly\nsupervised SDAE (for HGG) and transfer learning based LGG network were also\nshown to generalize well and provide good segmentation on unseen BraTS 2013 &\nBraTS 2015 test data. An unique contribution includes a single layer DAE,\nreferred to as novelty detector(ND). ND was trained to accurately reconstruct\nnon-lesion patches using a mean squared error loss function. The reconstruction\nerror maps of test data were used to identify regions containing lesions. The\nerror maps were shown to assign unique error distributions to various\nconstituents of the glioma, enabling localization. The ND learns the non-lesion\nbrain accurately as it was also shown to provide good segmentation performance\non ischemic brain lesions in images from a different database.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 06:19:09 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 09:24:59 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 04:19:01 GMT"}, {"version": "v4", "created": "Tue, 10 Jan 2017 04:59:00 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Alex", "Varghese", ""], ["Vaidhya", "Kiran", ""], ["Thirunavukkarasu", "Subramaniam", ""], ["Kesavdas", "Chandrasekharan", ""], ["Krishnamurthi", "Ganapathy", ""]]}, {"id": "1611.08669", "submitter": "Satwik Kottur", "authors": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,\n  Jos\\'e M. F. Moura, Devi Parikh, Dhruv Batra", "title": "Visual Dialog", "comments": "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9\n  dataset, Webpage: http://visualdialog.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 06:39:28 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 02:00:49 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 16:29:55 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 02:10:49 GMT"}, {"version": "v5", "created": "Tue, 1 Aug 2017 22:04:37 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Das", "Abhishek", ""], ["Kottur", "Satwik", ""], ["Gupta", "Khushi", ""], ["Singh", "Avi", ""], ["Yadav", "Deshraj", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1611.08754", "submitter": "Lex Fridman", "authors": "Lex Fridman, Heishiro Toyoda, Sean Seaman, Bobbie Seppelt, Linda\n  Angell, Joonbum Lee, Bruce Mehler, Bryan Reimer", "title": "What Can Be Predicted from Six Seconds of Driver Glances?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a large dataset of real-world, on-road driving from a 100-car\nnaturalistic study to explore the predictive power of driver glances and,\nspecifically, to answer the following question: what can be predicted about the\nstate of the driver and the state of the driving environment from a 6-second\nsequence of macro-glances? The context-based nature of such glances allows for\napplication of supervised learning to the problem of vision-based gaze\nestimation, making it robust, accurate, and reliable in messy, real-world\nconditions. So, it's valuable to ask whether such macro-glances can be used to\ninfer behavioral, environmental, and demographic variables? We analyze 27\nbinary classification problems based on these variables. The takeaway is that\nglance can be used as part of a multi-sensor real-time system to predict\nradio-tuning, fatigue state, failure to signal, talking, and several\nenvironment variables.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:41:51 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Fridman", "Lex", ""], ["Toyoda", "Heishiro", ""], ["Seaman", "Sean", ""], ["Seppelt", "Bobbie", ""], ["Angell", "Linda", ""], ["Lee", "Joonbum", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""]]}, {"id": "1611.08780", "submitter": "Yale Song", "authors": "Yale Song", "title": "Real-Time Video Highlights for Yahoo Esports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Esports has gained global popularity in recent years and several companies\nhave started offering live streaming videos of esports games and events. This\ncreates opportunities to develop large scale video understanding systems for\nnew product features and services. We present a technique for detecting\nhighlights from live streaming videos of esports game matches. Most video games\nuse pronounced visual effects to emphasize highlight moments; we use CNNs to\nlearn convolution filters of those visual effects for detecting highlights. We\npropose a cascaded prediction approach that allows us to deal with several\nchallenges arise in a production environment. We demonstrate our technique on\nour new dataset of three popular game titles, Heroes of the Storm, League of\nLegends, and Dota 2. Our technique achieves 18 FPS on a single CPU with an\naverage precision of up to 83.18%. Part of our technique is currently deployed\nin production on Yahoo Esports.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 03:58:41 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Song", "Yale", ""]]}, {"id": "1611.08788", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial\n  Networks", "comments": "5 pages; 4 figures; Accepted at the Deep Learning for Action and\n  Interaction Workshop, 30th Conference on Neural Information Processing\n  Systems (NIPS 2016), Barcelona, Spain; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is one of the most recent topics of interest which is\naimed at replicating human driving behavior keeping in mind the safety issues.\nWe approach the problem of learning synthetic driving using generative neural\nnetworks. The main idea is to make a controller trainer network using images\nplus key press data to mimic human learning. We used the architecture of a\nstable GAN to make predictions between driving scenes using key presses. We\ntrain our model on one video game (Road Rash) and tested the accuracy and\ncompared it by running the model on other maps in Road Rash to determine the\nextent of learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:01:39 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1611.08789", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "Handwriting Profiling using Generative Adversarial Networks", "comments": "2 pages; 2 figures; Accepted at The Thirty-First AAAI Conference on\n  Artificial Intelligence (AAAI-17 Student Abstract and Poster Program), San\n  Francisco, USA; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting is a skill learned by humans from a very early age. The ability\nto develop one's own unique handwriting as well as mimic another person's\nhandwriting is a task learned by the brain with practice. This paper deals with\nthis very problem where an intelligent system tries to learn the handwriting of\nan entity using Generative Adversarial Networks (GANs). We propose a modified\narchitecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We\nalso discuss about applying reinforcement learning techniques to achieve faster\nlearning. Our algorithm hopes to give new insights in this area and its uses\ninclude identification of forged documents, signature verification, computer\ngenerated art, digitization of documents among others. Our early implementation\nof the algorithm illustrates a good performance with MNIST datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:02:47 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1611.08796", "submitter": "Sayan Ghosal", "authors": "Sayan Ghosal and Nilanjan Ray", "title": "Deep Deformable Registration: Enhancing Accuracy by Fully Convolutional\n  Neural Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable registration is ubiquitous in medical image analysis. Many\ndeformable registration methods minimize sum of squared difference (SSD) as the\nregistration cost with respect to deformable model parameters. In this work, we\nconstruct a tight upper bound of the SSD registration cost by using a fully\nconvolutional neural network (FCNN) in the registration pipeline. The upper\nbound SSD (UB-SSD) enhances the original deformable model parameter space by\nadding a heatmap output from FCNN. Next, we minimize this UB-SSD by adjusting\nboth the parameters of the FCNN and the parameters of the deformable model in\ncoordinate descent. Our coordinate descent framework is end-to-end and can work\nwith any deformable registration method that uses SSD. We demonstrate\nexperimentally that our method enhances the accuracy of deformable registration\nalgorithms significantly on two publicly available 3D brain MRI data sets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 06:41:49 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ghosal", "Sayan", ""], ["Ray", "Nilanjan", ""]]}, {"id": "1611.08812", "submitter": "Yulia Dodonova", "authors": "Yulia Dodonova, Mikhail Belyaev, Anna Tkachev, Dmitry Petrov, and\n  Leonid Zhukov", "title": "Kernel classification of connectomes based on earth mover's distance\n  between graph spectra", "comments": "Presented at The MICCAI-BACON 16 Workshop (arXiv:1611.03363)", "journal-ref": null, "doi": null, "report-no": "BACON/2016/05", "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle a problem of predicting phenotypes from structural\nconnectomes. We propose that normalized Laplacian spectra can capture\nstructural properties of brain networks, and hence graph spectral distributions\nare useful for a task of connectome-based classification. We introduce a kernel\nthat is based on earth mover's distance (EMD) between spectral distributions of\nbrain networks. We access performance of an SVM classifier with the proposed\nkernel for a task of classification of autism spectrum disorder versus typical\ndevelopment based on a publicly available dataset. Classification quality (area\nunder the ROC-curve) obtained with the EMD-based kernel on spectral\ndistributions is 0.71, which is higher than that based on simpler graph\nembedding methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 09:35:04 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Dodonova", "Yulia", ""], ["Belyaev", "Mikhail", ""], ["Tkachev", "Anna", ""], ["Petrov", "Dmitry", ""], ["Zhukov", "Leonid", ""]]}, {"id": "1611.08815", "submitter": "J. Rafid Siddiqui", "authors": "J. Rafid Siddiqui", "title": "Did Evolution get it right? An evaluation of Near-Infrared imaging in\n  semantic scene segmentation using deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals have evolved to restrict their sensing capabilities to certain region\nof electromagnetic spectrum. This is surprisingly a very narrow band on a vast\nscale which makes one think if there is a systematic bias underlying such\nselective filtration. The situation becomes even more intriguing when we find a\nsharp cutoff point at Near-infrared point whereby almost all animal vision\nsystems seem to have a lower bound. This brings us to an interesting question:\ndid evolution \"intentionally\" performed such a restriction in order to evolve\nhigher visual cognition? In this work this question is addressed by\nexperimenting with Near-infrared images for their potential applicability in\nhigher visual processing such as semantic segmentation. A modified version of\nFully Convolutional Networks are trained on NIR images and RGB images\nrespectively and compared for their respective effectiveness in the wake of\nsemantic segmentation. The results from the experiments show that visible part\nof the spectrum alone is sufficient for the robust semantic segmentation of the\nindoor as well as outdoor scenes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 09:59:10 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Siddiqui", "J. Rafid", ""]]}, {"id": "1611.08841", "submitter": "Apratim Bhattacharyya", "authors": "Apratim Bhattacharyya, Mateusz Malinowski, Bernt Schiele, Mario Fritz", "title": "Long-Term Image Boundary Prediction", "comments": "Accepted in the AAAI Conference for Artificial Intelligence, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boundary estimation in images and videos has been a very active topic of\nresearch, and organizing visual information into boundaries and segments is\nbelieved to be a corner stone of visual perception. While prior work has\nfocused on estimating boundaries for observed frames, our work aims at\npredicting boundaries of future unobserved frames. This requires our model to\nlearn about the fate of boundaries and corresponding motion patterns --\nincluding a notion of \"intuitive physics\". We experiment on natural video\nsequences along with synthetic sequences with deterministic physics-based and\nagent-based motions. While not being our primary goal, we also show that fusion\nof RGB and boundary prediction leads to improved RGB predictions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 13:45:14 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 15:23:40 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Bhattacharyya", "Apratim", ""], ["Malinowski", "Mateusz", ""], ["Schiele", "Bernt", ""], ["Fritz", "Mario", ""]]}, {"id": "1611.08844", "submitter": "Benedetta Franceschiello Dr.", "authors": "B. Franceschiello, A. Sarti, G. Citti", "title": "A neuro-mathematical model for geometrical optical illusions", "comments": "13 pages, 38 figures divided in 15 groups", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometrical optical illusions have been object of many studies due to the\npossibility they offer to understand the behaviour of low-level visual\nprocessing. They consist in situations in which the perceived geometrical\nproperties of an object differ from those of the object in the visual stimulus.\nStarting from the geometrical model introduced by Citti and Sarti in [3], we\nprovide a mathematical model and a computational algorithm which allows to\ninterpret these phenomena and to qualitatively reproduce the perceived\nmisperception.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 13:52:24 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Franceschiello", "B.", ""], ["Sarti", "A.", ""], ["Citti", "G.", ""]]}, {"id": "1611.08860", "submitter": "Xucong Zhang", "authors": "Xucong Zhang, Yusuke Sugano, Mario Fritz, Andreas Bulling", "title": "It's Written All Over Your Face: Full-Face Appearance-Based Gaze\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye gaze is an important non-verbal cue for human affect analysis. Recent\ngaze estimation work indicated that information from the full face region can\nbenefit performance. Pushing this idea further, we propose an appearance-based\nmethod that, in contrast to a long-standing line of work in computer vision,\nonly takes the full face image as input. Our method encodes the face image\nusing a convolutional neural network with spatial weights applied on the\nfeature maps to flexibly suppress or enhance information in different facial\nregions. Through extensive evaluation, we show that our full-face method\nsignificantly outperforms the state of the art for both 2D and 3D gaze\nestimation, achieving improvements of up to 14.3% on MPIIGaze and 27.7% on\nEYEDIAP for person-independent 3D gaze estimation. We further show that this\nimprovement is consistent across different illumination conditions and gaze\ndirections and particularly pronounced for the most challenging extreme head\nposes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 15:00:10 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 13:54:13 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1611.08896", "submitter": "Radhakrishna Achanta", "authors": "Radhakrishna Achanta, Pablo M\\'arquez-Neila, Pascal Fua, and Sabine\n  S\\\"usstrunk", "title": "Uniform Information Segmentation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Size uniformity is one of the main criteria of superpixel methods. But size\nuniformity rarely conforms to the varying content of an image. The chosen size\nof the superpixels therefore represents a compromise - how to obtain the fewest\nsuperpixels without losing too much important detail. We propose that a more\nappropriate criterion for creating image segments is information uniformity. We\nintroduce a novel method for segmenting an image based on this criterion. Since\ninformation is a natural way of measuring image complexity, our proposed\nalgorithm leads to image segments that are smaller and denser in areas of high\ncomplexity and larger in homogeneous regions, thus simplifying the image while\npreserving its details. Our algorithm is simple and requires just one input\nparameter - a threshold on the information content. On segmentation comparison\nbenchmarks it proves to be superior to the state-of-the-art. In addition, our\nmethod is computationally very efficient, approaching real-time performance,\nand is easily extensible to three-dimensional image stacks and video volumes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 19:31:03 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Achanta", "Radhakrishna", ""], ["M\u00e1rquez-Neila", "Pablo", ""], ["Fua", "Pascal", ""], ["S\u00fcsstrunk", "Sabine", ""]]}, {"id": "1611.08906", "submitter": "Yiannis Andreopoulos", "authors": "Aaron Chadha and Yiannis Andreopoulos", "title": "Voronoi-based compact image descriptors: Efficient Region-of-Interest\n  retrieval with VLAD and deep-learning-based descriptors", "comments": "IEEE Transaction on Multimedia, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of image retrieval based on visual queries when\nthe latter comprise arbitrary regions-of-interest (ROI) rather than entire\nimages. Our proposal is a compact image descriptor that combines the\nstate-of-the-art in content-based descriptor extraction with a multi-level,\nVoronoi-based spatial partitioning of each dataset image. The proposed\nmulti-level Voronoi-based encoding uses a spatial hierarchical K-means over\ninterest-point locations, and computes a content-based descriptor over each\ncell. In order to reduce the matching complexity with minimal or no sacrifice\nin retrieval performance: (i) we utilize the tree structure of the spatial\nhierarchical K-means to perform a top-to-bottom pruning for local similarity\nmaxima; (ii) we propose a new image similarity score that combines relevant\ninformation from all partition levels into a single measure for similarity;\n(iii) we combine our proposal with a novel and efficient approach for optimal\nbit allocation within quantized descriptor representations. By deriving both a\nVoronoi-based VLAD descriptor (termed as Fast-VVLAD) and a Voronoi-based deep\nconvolutional neural network (CNN) descriptor (termed as Fast-VDCNN), we\ndemonstrate that our Voronoi-based framework is agnostic to the descriptor\nbasis, and can easily be slotted into existing frameworks. Via a range of ROI\nqueries in two standard datasets, it is shown that the Voronoi-based\ndescriptors achieve comparable or higher mean Average Precision against\nconventional grid-based spatial search, while offering more than two-fold\nreduction in complexity. Finally, beyond ROI queries, we show that Voronoi\npartitioning improves the geometric invariance of compact CNN descriptors,\nthereby resulting in competitive performance to the current state-of-the-art on\nwhole image retrieval.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 20:35:48 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 18:37:56 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Chadha", "Aaron", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1611.08974", "submitter": "Shuran Song", "authors": "Shuran Song, Fisher Yu, Andy Zeng, Angel X. Chang, Manolis Savva,\n  Thomas Funkhouser", "title": "Semantic Scene Completion from a Single Depth Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on semantic scene completion, a task for producing a\ncomplete 3D voxel representation of volumetric occupancy and semantic labels\nfor a scene from a single-view depth map observation. Previous work has\nconsidered scene completion and semantic labeling of depth maps separately.\nHowever, we observe that these two problems are tightly intertwined. To\nleverage the coupled nature of these two tasks, we introduce the semantic scene\ncompletion network (SSCNet), an end-to-end 3D convolutional network that takes\na single depth image as input and simultaneously outputs occupancy and semantic\nlabels for all voxels in the camera view frustum. Our network uses a\ndilation-based 3D context module to efficiently expand the receptive field and\nenable 3D context learning. To train our network, we construct SUNCG - a\nmanually created large-scale dataset of synthetic 3D scenes with dense\nvolumetric annotations. Our experiments demonstrate that the joint model\noutperforms methods addressing each task in isolation and outperforms\nalternative approaches on the semantic scene completion task.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 03:38:42 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Song", "Shuran", ""], ["Yu", "Fisher", ""], ["Zeng", "Andy", ""], ["Chang", "Angel X.", ""], ["Savva", "Manolis", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "1611.08976", "submitter": "Zhiyuan Fang", "authors": "Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, Yu Qiao", "title": "Range Loss for Deep Face Recognition with Long-tail", "comments": "9 pages, 5 figures, Submitted to CVPR, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have achieved great improvement on face\nrecognition in recent years because of its extraordinary ability in learning\ndiscriminative features of people with different identities. To train such a\nwell-designed deep network, tremendous amounts of data is indispensable. Long\ntail distribution specifically refers to the fact that a small number of\ngeneric entities appear frequently while other objects far less existing.\nConsidering the existence of long tail distribution of the real world data,\nlarge but uniform distributed data are usually hard to retrieve. Empirical\nexperiences and analysis show that classes with more samples will pose greater\nimpact on the feature learning process and inversely cripple the whole models\nfeature extracting ability on tail part data. Contrary to most of the existing\nworks that alleviate this problem by simply cutting the tailed data for uniform\ndistributions across the classes, this paper proposes a new loss function\ncalled range loss to effectively utilize the whole long tailed data in training\nprocess. More specifically, range loss is designed to reduce overall\nintra-personal variations while enlarging inter-personal differences within one\nmini-batch simultaneously when facing even extremely unbalanced data. The\noptimization objective of range loss is the $k$ greatest range's harmonic mean\nvalues in one class and the shortest inter-class distance within one batch.\nExtensive experiments on two famous and challenging face recognition benchmarks\n(Labeled Faces in the Wild (LFW) and YouTube Faces (YTF) not only demonstrate\nthe effectiveness of the proposed approach in overcoming the long tail effect\nbut also show the good generalization ability of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 03:41:46 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Zhang", "Xiao", ""], ["Fang", "Zhiyuan", ""], ["Wen", "Yandong", ""], ["Li", "Zhifeng", ""], ["Qiao", "Yu", ""]]}, {"id": "1611.08983", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Liu, Xiaohua Huang, Henglin Shi, Yingyue Xu, Qiong\n  Wang, Lan Tang, Xinggan Zhang", "title": "Analyzing the group sparsity based on the rank minimization methods", "comments": "arXiv admin note: text overlap with arXiv:1702.04463", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse coding has achieved a great success in various image processing\nstudies. However, there is not any benchmark to measure the sparsity of image\npatch/group because sparse discriminant conditions cannot keep unchanged. This\npaper analyzes the sparsity of group based on the strategy of the rank\nminimization. Firstly, an adaptive dictionary for each group is designed. Then,\nwe prove that group-based sparse coding is equivalent to the rank minimization\nproblem, and thus the sparse coefficient of each group is measured by\nestimating the singular values of each group. Based on that measurement, the\nweighted Schatten $p$-norm minimization (WSNM) has been found to be the closest\nsolution to the real singular values of each group. Thus, WSNM can be\nequivalently transformed into a non-convex $\\ell_p$-norm minimization problem\nin group-based sparse coding. To make the proposed scheme tractable and robust,\nthe alternating direction method of multipliers (ADMM) is used to solve the\n$\\ell_p$-norm minimization problem. Experimental results on two applications:\nimage inpainting and image compressive sensing (CS) recovery have shown that\nthe proposed scheme outperforms many state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 05:11:52 GMT"}, {"version": "v10", "created": "Wed, 19 Apr 2017 01:02:25 GMT"}, {"version": "v11", "created": "Mon, 24 Apr 2017 12:36:41 GMT"}, {"version": "v12", "created": "Mon, 12 Jun 2017 12:18:36 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 06:52:59 GMT"}, {"version": "v3", "created": "Sun, 15 Jan 2017 09:38:05 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 11:09:31 GMT"}, {"version": "v5", "created": "Mon, 27 Feb 2017 03:15:24 GMT"}, {"version": "v6", "created": "Fri, 24 Mar 2017 14:34:56 GMT"}, {"version": "v7", "created": "Thu, 30 Mar 2017 06:51:22 GMT"}, {"version": "v8", "created": "Wed, 5 Apr 2017 07:57:08 GMT"}, {"version": "v9", "created": "Sat, 8 Apr 2017 14:26:03 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Liu", "Xin", ""], ["Huang", "Xiaohua", ""], ["Shi", "Henglin", ""], ["Xu", "Yingyue", ""], ["Wang", "Qiong", ""], ["Tang", "Lan", ""], ["Zhang", "Xinggan", ""]]}, {"id": "1611.08986", "submitter": "Bing Shuai", "authors": "Bing Shuai, Ting Liu and Gang Wang", "title": "Improving Fully Convolution Network for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Convolution Networks (FCN) have achieved great success in dense\nprediction tasks including semantic segmentation. In this paper, we start from\ndiscussing FCN by understanding its architecture limitations in building a\nstrong segmentation network. Next, we present our Improved Fully Convolution\nNetwork (IFCN). In contrast to FCN, IFCN introduces a context network that\nprogressively expands the receptive fields of feature maps. In addition, dense\nskip connections are added so that the context network can be effectively\noptimized. More importantly, these dense skip connections enable IFCN to fuse\nrich-scale context to make reliable predictions. Empirically, those\narchitecture modifications are proven to be significant to enhance the\nsegmentation performance. Without engaging any contextual post-processing, IFCN\nsignificantly advances the state-of-the-arts on ADE20K (ImageNet scene\nparsing), Pascal Context, Pascal VOC 2012 and SUN-RGBD segmentation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 05:31:10 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Shuai", "Bing", ""], ["Liu", "Ting", ""], ["Wang", "Gang", ""]]}, {"id": "1611.08991", "submitter": "Long Jin", "authors": "Long Jin, Zeyu Chen, Zhuowen Tu", "title": "Object Detection Free Instance Segmentation With Labeling\n  Transformations", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation has attracted recent attention in computer vision and\nexisting methods in this domain mostly have an object detection stage. In this\npaper, we study the intrinsic challenge of the instance segmentation problem,\nthe presence of a quotient space (swapping the labels of different instances\nleads to the same result), and propose new methods that are object proposal-\nand object detection- free. We propose three alternative methods, namely\npixel-based affinity mapping, superpixel-based affinity learning, and\nboundary-based component segmentation, all focusing on performing labeling\ntransformations to cope with the quotient space problem. By adopting fully\nconvolutional neural networks (FCN) like models, our framework attains\ncompetitive results on both the PASCAL dataset (object-centric) and the Gland\ndataset (texture-centric), which the existing methods are not able to do. Our\nwork also has the advantages in its transparency, simplicity, and being all\nsegmentation based.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 05:52:37 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 05:42:11 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Jin", "Long", ""], ["Chen", "Zeyu", ""], ["Tu", "Zhuowen", ""]]}, {"id": "1611.08998", "submitter": "Seyed Hamid Rezatofighi", "authors": "S. Hamid Rezatofighi, Vijay Kumar B G, Anton Milan, Ehsan Abbasnejad,\n  Anthony Dick, Ian Reid", "title": "DeepSetNet: Predicting Sets with Deep Neural Networks", "comments": "Accepted in IEEE International Conference on Computer Vision (ICCV),\n  Venice, 2017, (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of set prediction using deep learning. This is\nimportant because the output of many computer vision tasks, including image\ntagging and object detection, are naturally expressed as sets of entities\nrather than vectors. As opposed to a vector, the size of a set is not fixed in\nadvance, and it is invariant to the ordering of entities within it. We define a\nlikelihood for a set distribution and learn its parameters using a deep neural\nnetwork. We also derive a loss for predicting a discrete distribution\ncorresponding to set cardinality. Set prediction is demonstrated on the problem\nof multi-class image classification. Moreover, we show that the proposed\ncardinality loss can also trivially be applied to the tasks of object counting\nand pedestrian detection. Our approach outperforms existing methods in all\nthree cases on standard datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 06:42:56 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 06:18:14 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 01:10:13 GMT"}, {"version": "v4", "created": "Fri, 31 Mar 2017 06:45:52 GMT"}, {"version": "v5", "created": "Fri, 11 Aug 2017 02:52:36 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Rezatofighi", "S. Hamid", ""], ["G", "Vijay Kumar B", ""], ["Milan", "Anton", ""], ["Abbasnejad", "Ehsan", ""], ["Dick", "Anthony", ""], ["Reid", "Ian", ""]]}, {"id": "1611.09007", "submitter": "Lloyd Windrim Mr", "authors": "Lloyd Windrim, Rishi Ramakrishnan, Arman Melkumyan, Richard Murphy", "title": "Hyperspectral CNN Classification with Limited Training Samples", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging sensors are becoming increasingly popular in robotics\napplications such as agriculture and mining, and allow per-pixel thematic\nclassification of materials in a scene based on their unique spectral\nsignatures. Recently, convolutional neural networks have shown remarkable\nperformance for classification tasks, but require substantial amounts of\nlabelled training data. This data must sufficiently cover the variability\nexpected to be encountered in the environment. For hyperspectral data, one of\nthe main variations encountered outdoors is due to incident illumination, which\ncan change in spectral shape and intensity depending on the scene geometry. For\nexample, regions occluded from the sun have a lower intensity and their\nincident irradiance skewed towards shorter wavelengths.\n  In this work, a data augmentation strategy based on relighting is used during\ntraining of a hyperspectral convolutional neural network. It allows training to\noccur in the outdoor environment given only a small labelled region, which does\nnot need to sufficiently represent the geometric variability of the entire\nscene. This is important for applications where obtaining large amounts of\ntraining data is labourious, hazardous or difficult, such as labelling pixels\nwithin shadows. Radiometric normalisation approaches for pre-processing the\nhyperspectral data are analysed and it is shown that methods based on the raw\npixel data are sufficient to be used as input for the classifier. This removes\nthe need for external hardware such as calibration boards, which can restrict\nthe application of hyperspectral sensors in robotics applications. Experiments\nto evaluate the classification system are carried out on two datasets captured\nfrom a field-based platform.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:29:29 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Windrim", "Lloyd", ""], ["Ramakrishnan", "Rishi", ""], ["Melkumyan", "Arman", ""], ["Murphy", "Richard", ""]]}, {"id": "1611.09010", "submitter": "Francesc Moreno-Noguer", "authors": "Francesc Moreno-Noguer", "title": "3D Human Pose Estimation from a Single Image via Distance Matrix\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of 3D human pose estimation from a single\nimage. We follow a standard two-step pipeline by first detecting the 2D\nposition of the $N$ body joints, and then using these observations to infer 3D\npose. For the first step, we use a recent CNN-based detector. For the second\nstep, most existing approaches perform 2$N$-to-3$N$ regression of the Cartesian\njoint coordinates. We show that more precise pose estimates can be obtained by\nrepresenting both the 2D and 3D human poses using $N\\times N$ distance\nmatrices, and formulating the problem as a 2D-to-3D distance matrix regression.\nFor learning such a regressor we leverage on simple Neural Network\narchitectures, which by construction, enforce positivity and symmetry of the\npredicted matrices. The approach has also the advantage to naturally handle\nmissing observations and allowing to hypothesize the position of non-observed\njoints. Quantitative results on Humaneva and Human3.6M datasets demonstrate\nconsistent performance gains over state-of-the-art. Qualitative evaluation on\nthe images in-the-wild of the LSP dataset, using the regressor learned on\nHuman3.6M, reveals very promising generalization results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:36:31 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Moreno-Noguer", "Francesc", ""]]}, {"id": "1611.09026", "submitter": "Shuai Yang", "authors": "Shuai Yang, Jiaying Liu, Zhouhui Lian and Zongming Guo", "title": "Awesome Typography: Statistics-Based Text Effects Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the problem of generating fantastic special-effects\nfor the typography. It is quite challenging due to the model diversities to\nillustrate varied text effects for different characters. To address this issue,\nour key idea is to exploit the analytics on the high regularity of the spatial\ndistribution for text effects to guide the synthesis process. Specifically, we\ncharacterize the stylized patches by their normalized positions and the optimal\nscales to depict their style elements. Our method first estimates these two\nfeatures and derives their correlation statistically. They are then converted\ninto soft constraints for texture transfer to accomplish adaptive multi-scale\ntexture synthesis and to make style element distribution uniform. It allows our\nalgorithm to produce artistic typography that fits for both local texture\npatterns and the global spatial distribution in the example. Experimental\nresults demonstrate the superiority of our method for various text effects over\nconventional style transfer methods. In addition, we validate the effectiveness\nof our algorithm with extensive artistic typography library generation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 08:48:28 GMT"}, {"version": "v2", "created": "Tue, 6 Dec 2016 04:49:14 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Yang", "Shuai", ""], ["Liu", "Jiaying", ""], ["Lian", "Zhouhui", ""], ["Guo", "Zongming", ""]]}, {"id": "1611.09051", "submitter": "Siddhartha Chandra", "authors": "Siddhartha Chandra and Iasonas Kokkinos", "title": "Deep, Dense, and Low-Rank Gaussian Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a fully-connected graph structure in the Deep\nGaussian Conditional Random Field (G-CRF) model. For this we express the\npairwise interactions between pixels as the inner-products of low-dimensional\nembeddings, delivered by a new subnetwork of a deep architecture. We\nefficiently minimize the resulting energy by solving the resulting low-rank\nlinear system with conjugate gradients, and derive an analytic expression for\nthe gradient of our embeddings which allows us to train them end-to-end with\nbackpropagation.\n  We demonstrate the merit of our approach by achieving state of the art\nresults on three challenging Computer Vision benchmarks, namely semantic\nsegmentation, human parts segmentation, and saliency estimation. Our\nimplementation is fully GPU based, built on top of the Caffe library, and will\nbe made publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 10:29:53 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Chandra", "Siddhartha", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1611.09053", "submitter": "Zhongwen Xu", "authors": "Linchao Zhu, Zhongwen Xu, Yi Yang", "title": "Bidirectional Multirate Reconstruction for Temporal Modeling in Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent success of neural networks in image feature learning, a\nmajor problem in the video domain is the lack of sufficient labeled data for\nlearning to model temporal information. In this paper, we propose an\nunsupervised temporal modeling method that learns from untrimmed videos. The\nspeed of motion varies constantly, e.g., a man may run quickly or slowly. We\ntherefore train a Multirate Visual Recurrent Model (MVRM) by encoding frames of\na clip with different intervals. This learning process makes the learned model\nmore capable of dealing with motion speed variance. Given a clip sampled from a\nvideo, we use its past and future neighboring clips as the temporal context,\nand reconstruct the two temporal transitions, i.e., present$\\rightarrow$past\ntransition and present$\\rightarrow$future transition, reflecting the temporal\ninformation in different views. The proposed method exploits the two\ntransitions simultaneously by incorporating a bidirectional reconstruction\nwhich consists of a backward reconstruction and a forward reconstruction. We\napply the proposed method to two challenging video tasks, i.e., complex event\ndetection and video captioning, in which it achieves state-of-the-art\nperformance. Notably, our method generates the best single feature for event\ndetection with a relative improvement of 10.4% on the MEDTest-13 dataset and\nachieves the best performance in video captioning across all evaluation metrics\non the YouTube2Text dataset.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 10:32:03 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Zhu", "Linchao", ""], ["Xu", "Zhongwen", ""], ["Yang", "Yi", ""]]}, {"id": "1611.09078", "submitter": "Timur Bagautdinov", "authors": "Timur Bagautdinov, Alexandre Alahi, Fran\\c{c}ois Fleuret, Pascal Fua,\n  Silvio Savarese", "title": "Social Scene Understanding: End-to-End Multi-Person Action Localization\n  and Collective Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework for understanding human social behaviors in\nraw image sequences. Our model jointly detects multiple individuals, infers\ntheir social actions, and estimates the collective actions with a single\nfeed-forward pass through a neural network. We propose a single architecture\nthat does not rely on external detection algorithms but rather is trained\nend-to-end to generate dense proposal maps that are refined via a novel\ninference scheme. The temporal consistency is handled via a person-level\nmatching Recurrent Neural Network. The complete model takes as input a sequence\nof frames and outputs detections along with the estimates of individual actions\nand collective activities. We demonstrate state-of-the-art performance of our\nalgorithm on multiple publicly available benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 11:34:20 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bagautdinov", "Timur", ""], ["Alahi", "Alexandre", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""], ["Savarese", "Silvio", ""]]}, {"id": "1611.09099", "submitter": "Thierry Bouwmans", "authors": "Thierry Bouwmans and Caroline Silva and Cristina Marghes and Mohammed\n  Sami Zitouni and Harish Bhaskar and Carl Frelicot", "title": "On the Role and the Importance of Features for Background Modeling and\n  Foreground Detection", "comments": "To be submitted to Computer Science Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background modeling has emerged as a popular foreground detection technique\nfor various applications in video surveillance. Background modeling methods\nhave become increasing efficient in robustly modeling the background and hence\ndetecting moving objects in any visual scene. Although several background\nsubtraction and foreground detection have been proposed recently, no\ntraditional algorithm today still seem to be able to simultaneously address all\nthe key challenges of illumination variation, dynamic camera motion, cluttered\nbackground and occlusion. This limitation can be attributed to the lack of\nsystematic investigation concerning the role and importance of features within\nbackground modeling and foreground detection. With the availability of a rather\nlarge set of invariant features, the challenge is in determining the best\ncombination of features that would improve accuracy and robustness in\ndetection. The purpose of this study is to initiate a rigorous and\ncomprehensive survey of features used within background modeling and foreground\ndetection. Further, this paper presents a systematic experimental and\nstatistical analysis of techniques that provide valuable insight on the trends\nin background modeling and use it to draw meaningful recommendations for\npractitioners. In this paper, a preliminary review of the key characteristics\nof features based on the types and sizes is provided in addition to\ninvestigating their intrinsic spectral, spatial and temporal properties.\nFurthermore, improvements using statistical and fuzzy tools are examined and\ntechniques based on multiple features are benchmarked against reliability and\nselection criterion. Finally, a description of the different resources\navailable such as datasets and codes is provided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 12:55:16 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Bouwmans", "Thierry", ""], ["Silva", "Caroline", ""], ["Marghes", "Cristina", ""], ["Zitouni", "Mohammed Sami", ""], ["Bhaskar", "Harish", ""], ["Frelicot", "Carl", ""]]}, {"id": "1611.09119", "submitter": "Jianfeng Dong", "authors": "Jianfeng Dong, Xiao-Jiao Mao, Chunhua Shen, Yu-Bin Yang", "title": "Learning Deep Representations Using Convolutional Auto-encoders with\n  Symmetric Skip Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised pre-training was a critical technique for training deep neural\nnetworks years ago. With sufficient labeled data and modern training\ntechniques, it is possible to train very deep neural networks from scratch in a\npurely supervised manner nowadays. However, unlabeled data is easier to obtain\nand usually of very large scale. How to make use of them better to help\nsupervised learning is still a well-valued topic. In this paper, we investigate\nconvolutional denoising auto-encoders to show that unsupervised pre-training\ncan still improve the performance of high-level image related tasks such as\nimage classification and semantic segmentation. The architecture we use is a\nconvolutional auto-encoder network with symmetric shortcut connections. We\nempirically show that symmetric shortcut connections are very important for\nlearning abstract representations via image reconstruction. When no extra\nunlabeled data are available, unsupervised pre-training with our network can\nregularize the supervised training and therefore lead to better generalization\nperformance. With the help of unsupervised pre-training, our method achieves\nvery competitive results in image classification using very simple\nall-convolution networks. When labeled data are limited but extra unlabeled\ndata are available, our method achieves good results in several semi-supervised\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 13:56:20 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 14:49:42 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Dong", "Jianfeng", ""], ["Mao", "Xiao-Jiao", ""], ["Shen", "Chunhua", ""], ["Yang", "Yu-Bin", ""]]}, {"id": "1611.09159", "submitter": "Evgeny Burnaev", "authors": "Alexandr Notchenko, Ermek Kapushev, Evgeny Burnaev", "title": "Large-Scale Shape Retrieval with Sparse 3D Convolutional Neural Networks", "comments": "8 pages, 3 figures, 2 tables, accepted to 3D Deep Learning Workshop\n  at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present results of performance evaluation of S3DCNN - a\nSparse 3D Convolutional Neural Network - on a large-scale 3D Shape benchmark\nModelNet40, and measure how it is impacted by voxel resolution of input shape.\nWe demonstrate comparable classification and retrieval performance to\nstate-of-the-art models, but with much less computational costs in training and\ninference phases. We also notice that benefits of higher input resolution can\nbe limited by an ability of a neural network to generalize high level features.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:09:10 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 15:50:22 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Notchenko", "Alexandr", ""], ["Kapushev", "Ermek", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1611.09162", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Punarjay Chakravarty and Tinne Tuytelaars", "title": "Who's that Actor? Automatic Labelling of Actors in TV series starting\n  from IMDB Images", "comments": "ACCV 2016 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we aim at automatically labeling actors in a TV series. Rather\nthan relying on transcripts and subtitles, as has been demonstrated in the\npast, we show how to achieve this goal starting from a set of example images of\neach of the main actors involved, collected from the Internet Movie Database\n(IMDB). The problem then becomes one of domain adaptation: actors' IMDB photos\nare typically taken at awards ceremonies and are quite different from their\nappearances in TV series. In each series as well, there is considerable change\nin actor appearance due to makeup, lighting, ageing, etc. To bridge this gap,\nwe propose a graph-matching based self-labelling algorithm, which we coin HSL\n(Hungarian Self Labeling). Further, we propose a new edge cost to be used in\nthis context, as well as an extension that is more robust to outliers, where\nprototypical faces for each of the actors are selected based on a hierarchical\nclustering procedure. We conduct experiments with 15 episodes from 3 different\nTV series and demonstrate automatic annotation with an accuracy of 90% and up.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:09:26 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Chakravarty", "Punarjay", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1611.09180", "submitter": "Quanzeng You", "authors": "Quanzeng You, Ran Pang, Liangliang Cao, Jiebo Luo", "title": "Image Based Appraisal of Real Estate Properties", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TMM.2017.2710804", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real estate appraisal, which is the process of estimating the price for real\nestate properties, is crucial for both buys and sellers as the basis for\nnegotiation and transaction. Traditionally, the repeat sales model has been\nwidely adopted to estimate real estate price. However, it depends the design\nand calculation of a complex economic related index, which is challenging to\nestimate accurately. Today, real estate brokers provide easy access to detailed\nonline information on real estate properties to their clients. We are\ninterested in estimating the real estate price from these large amounts of\neasily accessed data. In particular, we analyze the prediction power of online\nhouse pictures, which is one of the key factors for online users to make a\npotential visiting decision. The development of robust computer vision\nalgorithms makes the analysis of visual content possible. In this work, we\nemploy a Recurrent Neural Network (RNN) to predict real estate price using the\nstate-of-the-art visual features. The experimental results indicate that our\nmodel outperforms several of other state-of-the-art baseline algorithms in\nterms of both mean absolute error (MAE) and mean absolute percentage error\n(MAPE).\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:23:14 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 19:18:27 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["You", "Quanzeng", ""], ["Pang", "Ran", ""], ["Cao", "Liangliang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.09203", "submitter": "Juan Castorena", "authors": "Juan Castorena", "title": "Computational Mapping of the Ground Reflectivity with Laser Scanners", "comments": "Submitted to TIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this investigation we focus on the problem of mapping the ground\nreflectivity with multiple laser scanners mounted on mobile robots/vehicles.\nThe problem originates because regions of the ground become populated with a\nvarying number of reflectivity measurements whose value depends on the observer\nand its corresponding perspective. Here, we propose a novel automatic,\ndata-driven computational mapping framework specifically aimed at preserving\nedge sharpness in the map reconstruction process and that considers the sources\nof measurement variation. Our new formulation generates map-perspective\ngradients and applies sub-set selection fusion and de-noising operators to\nthese through iterative algorithms that minimize an $\\ell_1$ sparse regularized\nleast squares formulation. Reconstruction of the ground reflectivity is then\ncarried out based on Poisson's formulation posed as an $\\ell_2$ term promoting\nconsistency with the fused gradient of map-perspectives and a term that ensures\nequality constraints with reference measurement data. We demonstrate our new\nframework outperforms the capabilities of existing ones with experiments\nrealized on Ford's fleet of autonomous vehicles. For example, we show we can\nachieve map enhancement (i.e., contrast enhancement), artifact removal,\nde-noising and map-stitching without requiring an additional reflectivity\nadjustment to calibrate sensors to the specific mounting and robot/vehicle\nmotion.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 15:45:57 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 17:34:26 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Castorena", "Juan", ""]]}, {"id": "1611.09224", "submitter": "Martin Danelljan", "authors": "Martin Danelljan, Goutam Bhat, Fahad Shahbaz Khan, Michael Felsberg", "title": "ECO: Efficient Convolution Operators for Tracking", "comments": "Accepted at CVPR 2017. Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Discriminative Correlation Filter (DCF) based methods have\nsignificantly advanced the state-of-the-art in tracking. However, in the\npursuit of ever increasing tracking performance, their characteristic speed and\nreal-time capability have gradually faded. Further, the increasingly complex\nmodels, with massive number of trainable parameters, have introduced the risk\nof severe over-fitting. In this work, we tackle the key causes behind the\nproblems of computational complexity and over-fitting, with the aim of\nsimultaneously improving both speed and performance.\n  We revisit the core DCF formulation and introduce: (i) a factorized\nconvolution operator, which drastically reduces the number of parameters in the\nmodel; (ii) a compact generative model of the training sample distribution,\nthat significantly reduces memory and time complexity, while providing better\ndiversity of samples; (iii) a conservative model update strategy with improved\nrobustness and reduced complexity. We perform comprehensive experiments on four\nbenchmarks: VOT2016, UAV123, OTB-2015, and TempleColor. When using expensive\ndeep features, our tracker provides a 20-fold speedup and achieves a 13.0%\nrelative gain in Expected Average Overlap compared to the top ranked method in\nthe VOT2016 challenge. Moreover, our fast variant, using hand-crafted features,\noperates at 60 Hz on a single CPU, while obtaining 65.0% AUC on OTB-2015.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:26:27 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 18:13:05 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Danelljan", "Martin", ""], ["Bhat", "Goutam", ""], ["Khan", "Fahad Shahbaz", ""], ["Felsberg", "Michael", ""]]}, {"id": "1611.09309", "submitter": "Zeynep Akata PhD", "authors": "Nour Karessli, Zeynep Akata, Bernt Schiele, Andreas Bulling", "title": "Gaze Embeddings for Zero-Shot Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot image classification using auxiliary information, such as\nattributes describing discriminative object properties, requires time-consuming\nannotation by domain experts. We instead propose a method that relies on human\ngaze as auxiliary information, exploiting that even non-expert users have a\nnatural ability to judge class membership. We present a data collection\nparadigm that involves a discrimination task to increase the information\ncontent obtained from gaze data. Our method extracts discriminative descriptors\nfrom the data and learns a compatibility function between image and gaze using\nthree novel gaze embeddings: Gaze Histograms (GH), Gaze Features with Grid\n(GFG) and Gaze Features with Sequence (GFS). We introduce two new\ngaze-annotated datasets for fine-grained image classification and show that\nhuman gaze data is indeed class discriminative, provides a competitive\nalternative to expert-annotated attributes, and outperforms other baselines for\nzero-shot image classification.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:00:16 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 18:19:41 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Karessli", "Nour", ""], ["Akata", "Zeynep", ""], ["Schiele", "Bernt", ""], ["Bulling", "Andreas", ""]]}, {"id": "1611.09312", "submitter": "Lorenzo Baraldi", "authors": "Lorenzo Baraldi, Costantino Grana, Rita Cucchiara", "title": "Hierarchical Boundary-Aware Neural Encoder for Video Captioning", "comments": "CVPR 2017", "journal-ref": null, "doi": "10.1109/CVPR.2017.339", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Recurrent Neural Networks for video captioning has recently gained\na lot of attention, since they can be used both to encode the input video and\nto generate the corresponding description. In this paper, we present a\nrecurrent video encoding scheme which can discover and leverage the\nhierarchical structure of the video. Unlike the classical encoder-decoder\napproach, in which a video is encoded continuously by a recurrent layer, we\npropose a novel LSTM cell, which can identify discontinuity points between\nframes or segments and modify the temporal connections of the encoding layer\naccordingly. We evaluate our approach on three large-scale datasets: the\nMontreal Video Annotation dataset, the MPII Movie Description dataset and the\nMicrosoft Video Description Corpus. Experiments show that our approach can\ndiscover appropriate hierarchical representations of input videos and improve\nthe state of the art results on movie description datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:02:28 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 12:50:20 GMT"}, {"version": "v3", "created": "Mon, 10 Apr 2017 13:36:08 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Baraldi", "Lorenzo", ""], ["Grana", "Costantino", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1611.09325", "submitter": "Konstantinos Rematas", "authors": "Stamatios Georgoulis, Konstantinos Rematas, Tobias Ritschel, Mario\n  Fritz, Tinne Tuytelaars, Luc Van Gool", "title": "What Is Around The Camera?", "comments": "Accepted to ICCV. Project:\n  http://homes.esat.kuleuven.be/~sgeorgou/multinatillum/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much does a single image reveal about the environment it was taken in? In\nthis paper, we investigate how much of that information can be retrieved from a\nforeground object, combined with the background (i.e. the visible part of the\nenvironment). Assuming it is not perfectly diffuse, the foreground object acts\nas a complexly shaped and far-from-perfect mirror. An additional challenge is\nthat its appearance confounds the light coming from the environment with the\nunknown materials it is made of. We propose a learning-based approach to\npredict the environment from multiple reflectance maps that are computed from\napproximate surface normals. The proposed method allows us to jointly model the\nstatistics of environments and material properties. We train our system from\nsynthesized training data, but demonstrate its applicability to real-world\ndata. Interestingly, our analysis shows that the information obtained from\nobjects made out of multiple materials often is complementary and leads to\nbetter performance.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:27:53 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 00:55:25 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Georgoulis", "Stamatios", ""], ["Rematas", "Konstantinos", ""], ["Ritschel", "Tobias", ""], ["Fritz", "Mario", ""], ["Tuytelaars", "Tinne", ""], ["Van Gool", "Luc", ""]]}, {"id": "1611.09326", "submitter": "Simon Jegou", "authors": "Simon J\\'egou, Michal Drozdzal, David Vazquez, Adriana Romero and\n  Yoshua Bengio", "title": "The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for\n  Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches for semantic image segmentation are built on\nConvolutional Neural Networks (CNNs). The typical segmentation architecture is\ncomposed of (a) a downsampling path responsible for extracting coarse semantic\nfeatures, followed by (b) an upsampling path trained to recover the input image\nresolution at the output of the model and, optionally, (c) a post-processing\nmodule (e.g. Conditional Random Fields) to refine the model predictions.\n  Recently, a new CNN architecture, Densely Connected Convolutional Networks\n(DenseNets), has shown excellent results on image classification tasks. The\nidea of DenseNets is based on the observation that if each layer is directly\nconnected to every other layer in a feed-forward fashion then the network will\nbe more accurate and easier to train.\n  In this paper, we extend DenseNets to deal with the problem of semantic\nsegmentation. We achieve state-of-the-art results on urban scene benchmark\ndatasets such as CamVid and Gatech, without any further post-processing module\nnor pretraining. Moreover, due to smart construction of the model, our approach\nhas much less parameters than currently published best entries for these\ndatasets.\n  Code to reproduce the experiments is available here :\nhttps://github.com/SimJeg/FC-DenseNet/blob/master/train.py\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:27:54 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 22:24:04 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 13:10:48 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["J\u00e9gou", "Simon", ""], ["Drozdzal", "Michal", ""], ["Vazquez", "David", ""], ["Romero", "Adriana", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1611.09392", "submitter": "Ang Li", "authors": "Ang Li, Jin Sun, Joe Yue-Hei Ng, Ruichi Yu, Vlad I. Morariu, Larry S.\n  Davis", "title": "Generating Holistic 3D Scene Abstractions for Text-based Image Retrieval", "comments": "CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial relationships between objects provide important information for\ntext-based image retrieval. As users are more likely to describe a scene from a\nreal world perspective, using 3D spatial relationships rather than 2D\nrelationships that assume a particular viewing direction, one of the main\nchallenges is to infer the 3D structure that bridges images with users' text\ndescriptions. However, direct inference of 3D structure from images requires\nlearning from large scale annotated data. Since interactions between objects\ncan be reduced to a limited set of atomic spatial relations in 3D, we study the\npossibility of inferring 3D structure from a text description rather than an\nimage, applying physical relation models to synthesize holistic 3D abstract\nobject layouts satisfying the spatial constraints present in a textual\ndescription. We present a generic framework for retrieving images from a\ntextual description of a scene by matching images with these generated abstract\nobject layouts. Images are ranked by matching object detection outputs\n(bounding boxes) to 2D layout candidates (also represented by bounding boxes)\nwhich are obtained by projecting the 3D scenes with sampled camera directions.\nWe validate our approach using public indoor scene datasets and show that our\nmethod outperforms baselines built upon object occurrence histograms and\nlearned 2D pairwise relations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:29:07 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 20:37:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Li", "Ang", ""], ["Sun", "Jin", ""], ["Ng", "Joe Yue-Hei", ""], ["Yu", "Ruichi", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1611.09394", "submitter": "Gabriel Schwartz", "authors": "Gabriel Schwartz and Ko Nishino", "title": "Material Recognition from Local Appearance in Global Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of materials has proven to be a challenging problem due to the\nwide variation in appearance within and between categories. Global image\ncontext, such as where the material is or what object it makes up, can be\ncrucial to recognizing the material. Existing methods, however, operate on an\nimplicit fusion of materials and context by using large receptive fields as\ninput (i.e., large image patches). Many recent material recognition methods\ntreat materials as yet another set of labels like objects. Materials are,\nhowever, fundamentally different from objects as they have no inherent shape or\ndefined spatial extent. Approaches that ignore this can only take advantage of\nlimited implicit context as it appears during training. We instead show that\nrecognizing materials purely from their local appearance and integrating\nseparately recognized global contextual cues including objects and places leads\nto superior dense, per-pixel, material recognition. We achieve this by training\na fully-convolutional material recognition network end-to-end with only\nmaterial category supervision. We integrate object and place estimates to this\nnetwork from independent CNNs. This approach avoids the necessity of preparing\nan impractically-large amount of training data to cover the product space of\nmaterials, objects, and scenes, while fully leveraging contextual cues for\ndense material recognition. Furthermore, we perform a detailed analysis of the\neffects of context granularity, spatial resolution, and the network level at\nwhich we introduce context. On a recently introduced comprehensive and diverse\nmaterial database \\cite{Schwartz2016}, we confirm that our method achieves\nstate-of-the-art accuracy with significantly less training data compared to\npast methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:36:31 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 06:37:49 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 04:05:10 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Schwartz", "Gabriel", ""], ["Nishino", "Ko", ""]]}, {"id": "1611.09427", "submitter": "Phung Manh Duong", "authors": "Manh Duong Phung, Quang Vinh Tran, Kenji Hara, Hirohito Inagaki,\n  Masanobu Abe", "title": "Easy-setup eye movement recording system for human-computer interaction", "comments": "In IEEE International Conference on Research, Innovation and Vision\n  for the Future (RIVF), 2008", "journal-ref": null, "doi": "10.1109/RIVF.2008.4586369", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking the movement of human eyes is expected to yield natural and\nconvenient applications based on human-computer interaction (HCI). To implement\nan effective eye-tracking system, eye movements must be recorded without\nplacing any restriction on the user's behavior or user discomfort. This paper\ndescribes an eye movement recording system that offers free-head, simple\nconfiguration. It does not require the user to wear anything on her head, and\nshe can move her head freely. Instead of using a computer, the system uses a\nvisual digital signal processor (DSP) camera to detect the position of eye\ncorner, the center of pupil and then calculate the eye movement. Evaluation\ntests show that the sampling rate of the system can be 300 Hz and the accuracy\nis about 1.8 degree/s.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:35:01 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Phung", "Manh Duong", ""], ["Tran", "Quang Vinh", ""], ["Hara", "Kenji", ""], ["Inagaki", "Hirohito", ""], ["Abe", "Masanobu", ""]]}, {"id": "1611.09464", "submitter": "Shan Su", "authors": "Shan Su, Jung Pyo Hong, Jianbo Shi, Hyun Soo Park", "title": "Social Behavior Prediction from First Person Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to predict the future movements (location and\ngaze direction) of basketball players as a whole from their first person\nvideos. The predicted behaviors reflect an individual physical space that\naffords to take the next actions while conforming to social behaviors by\nengaging to joint attention. Our key innovation is to use the 3D reconstruction\nof multiple first person cameras to automatically annotate each other's the\nvisual semantics of social configurations.\n  We leverage two learning signals uniquely embedded in first person videos.\nIndividually, a first person video records the visual semantics of a spatial\nand social layout around a person that allows associating with past similar\nsituations. Collectively, first person videos follow joint attention that can\nlink the individuals to a group. We learn the egocentric visual semantics of\ngroup movements using a Siamese neural network to retrieve future trajectories.\nWe consolidate the retrieved trajectories from all players by maximizing a\nmeasure of social compatibility---the gaze alignment towards joint attention\npredicted by their social formation, where the dynamics of joint attention is\nlearned by a long-term recurrent convolutional network. This allows us to\ncharacterize which social configuration is more plausible and predict future\ngroup trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 03:04:41 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Su", "Shan", ""], ["Hong", "Jung Pyo", ""], ["Shi", "Jianbo", ""], ["Park", "Hyun Soo", ""]]}, {"id": "1611.09498", "submitter": "Janne Mustaniemi", "authors": "Janne Mustaniemi, Juho Kannala, Simo S\\\"arkk\\\"a, Jiri Matas, Janne\n  Heikkil\\\"a", "title": "Inertial-Based Scale Estimation for Structure from Motion on Mobile\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure from motion algorithms have an inherent limitation that the\nreconstruction can only be determined up to the unknown scale factor. Modern\nmobile devices are equipped with an inertial measurement unit (IMU), which can\nbe used for estimating the scale of the reconstruction. We propose a method\nthat recovers the metric scale given inertial measurements and camera poses. In\nthe process, we also perform a temporal and spatial alignment of the camera and\nthe IMU. Therefore, our solution can be easily combined with any existing\nvisual reconstruction software. The method can cope with noisy camera pose\nestimates, typically caused by motion blur or rolling shutter artifacts, via\nutilizing a Rauch-Tung-Striebel (RTS) smoother. Furthermore, the scale\nestimation is performed in the frequency domain, which provides more robustness\nto inaccurate sensor time stamps and noisy IMU samples than the previously used\ntime domain representation. In contrast to previous methods, our approach has\nno parameters that need to be tuned for achieving a good performance. In the\nexperiments, we show that the algorithm outperforms the state-of-the-art in\nboth accuracy and convergence speed of the scale estimate. The accuracy of the\nscale is around $1\\%$ from the ground truth depending on the recording. We also\ndemonstrate that our method can improve the scale accuracy of the Project\nTango's build-in motion tracking.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 05:56:25 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 09:44:42 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Mustaniemi", "Janne", ""], ["Kannala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Matas", "Jiri", ""], ["Heikkil\u00e4", "Janne", ""]]}, {"id": "1611.09502", "submitter": "Ting Yao", "authors": "Zhaofan Qiu, Ting Yao, Tao Mei", "title": "Deep Quantization: Encoding Convolutional Activations with Deep\n  Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have proven highly effective for\nvisual recognition, where learning a universal representation from activations\nof convolutional layer plays a fundamental problem. In this paper, we present\nFisher Vector encoding with Variational Auto-Encoder (FV-VAE), a novel deep\narchitecture that quantizes the local activations of convolutional layer in a\ndeep generative model, by training them in an end-to-end manner. To incorporate\nFV encoding strategy into deep generative models, we introduce Variational\nAuto-Encoder model, which steers a variational inference and learning in a\nneural network which can be straightforwardly optimized using standard\nstochastic gradient method. Different from the FV characterized by conventional\ngenerative models (e.g., Gaussian Mixture Model) which parsimoniously fit a\ndiscrete mixture model to data distribution, the proposed FV-VAE is more\nflexible to represent the natural property of data for better generalization.\nExtensive experiments are conducted on three public datasets, i.e., UCF101,\nActivityNet, and CUB-200-2011 in the context of video action recognition and\nfine-grained image classification, respectively. Superior results are reported\nwhen compared to state-of-the-art representations. Most remarkably, our\nproposed FV-VAE achieves to-date the best published accuracy of 94.2% on\nUCF101.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 06:07:28 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Qiu", "Zhaofan", ""], ["Yao", "Ting", ""], ["Mei", "Tao", ""]]}, {"id": "1611.09534", "submitter": "Tom Zahavy", "authors": "Tom Zahavy and Alessandro Magnani and Abhinandan Krishnan and Shie\n  Mannor", "title": "Is a picture worth a thousand words? A Deep Multi-Modal Fusion\n  Architecture for Product Classification in e-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classifying products into categories precisely and efficiently is a major\nchallenge in modern e-commerce. The high traffic of new products uploaded daily\nand the dynamic nature of the categories raise the need for machine learning\nmodels that can reduce the cost and time of human editors. In this paper, we\npropose a decision level fusion approach for multi-modal product classification\nusing text and image inputs. We train input specific state-of-the-art deep\nneural networks for each input source, show the potential of forging them\ntogether into a multi-modal architecture and train a novel policy network that\nlearns to choose between them. Finally, we demonstrate that our multi-modal\nnetwork improves the top-1 accuracy % over both networks on a real-world\nlarge-scale product classification dataset that we collected fromWalmart.com.\nWhile we focus on image-text fusion that characterizes e-commerce domains, our\nalgorithms can be easily applied to other modalities such as audio, video,\nphysical sensors, etc.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 09:05:11 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Zahavy", "Tom", ""], ["Magnani", "Alessandro", ""], ["Krishnan", "Abhinandan", ""], ["Mannor", "Shie", ""]]}, {"id": "1611.09559", "submitter": "Burak Benligiray", "authors": "Burak Benligiray and Cihan Topal", "title": "Lens Distortion Rectification using Triangulation based Interpolation", "comments": "International Symposium on Visual Computing, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-27863-6_4", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear lens distortion rectification is a common first step in image\nprocessing applications where the assumption of a linear camera model is\nessential. For rectifying the lens distortion, forward distortion model needs\nto be known. However, many self-calibration methods estimate the inverse\ndistortion model. In the literature, the inverse of the estimated model is\napproximated for image rectification, which introduces additional error to the\nsystem. We propose a novel distortion rectification method that uses the\ninverse distortion model directly. The method starts by mapping the distorted\npixels to the rectified image using the inverse distortion model. The resulting\nset of points with subpixel locations are triangulated. The pixel values of the\nrectified image are linearly interpolated based on this triangulation. The\nmethod is applicable to all camera calibration methods that estimate the\ninverse distortion model and performs well across a large range of parameters.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 10:39:12 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 20:06:04 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Benligiray", "Burak", ""], ["Topal", "Cihan", ""]]}, {"id": "1611.09571", "submitter": "Marcella Cornia", "authors": "Marcella Cornia, Lorenzo Baraldi, Giuseppe Serra, Rita Cucchiara", "title": "Predicting Human Eye Fixations via an LSTM-based Saliency Attentive\n  Model", "comments": "IEEE Transactions on Image Processing 2018", "journal-ref": null, "doi": "10.1109/TIP.2018.2851672", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven saliency has recently gained a lot of attention thanks to the use\nof Convolutional Neural Networks for predicting gaze fixations. In this paper\nwe go beyond standard approaches to saliency prediction, in which gaze maps are\ncomputed with a feed-forward network, and present a novel model which can\npredict accurate saliency maps by incorporating neural attentive mechanisms.\nThe core of our solution is a Convolutional LSTM that focuses on the most\nsalient regions of the input image to iteratively refine the predicted saliency\nmap. Additionally, to tackle the center bias typical of human eye fixations,\nour model can learn a set of prior maps generated with Gaussian functions. We\nshow, through an extensive evaluation, that the proposed architecture\noutperforms the current state of the art on public saliency prediction\ndatasets. We further study the contribution of each key component to\ndemonstrate their robustness on different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:27:19 GMT"}, {"version": "v2", "created": "Fri, 17 Mar 2017 13:58:27 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 10:34:02 GMT"}, {"version": "v4", "created": "Mon, 9 Jul 2018 10:18:43 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Cornia", "Marcella", ""], ["Baraldi", "Lorenzo", ""], ["Serra", "Giuseppe", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1611.09572", "submitter": "Byeongjoo Ahn", "authors": "Byeongjoo Ahn, Tae Hyun Kim, Wonsik Kim, Kyoung Mu Lee", "title": "Occlusion-Aware Video Deblurring with a New Layered Blur Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deblurring method for scenes with occluding objects using a\ncarefully designed layered blur model. Layered blur model is frequently used in\nthe motion deblurring problem to handle locally varying blurs, which is caused\nby object motions or depth variations in a scene. However, conventional models\nhave a limitation in representing the layer interactions occurring at occlusion\nboundaries. In this paper, we address this limitation in both theoretical and\nexperimental ways, and propose a new layered blur model reflecting actual blur\ngeneration process. Based on this model, we develop an occlusion-aware\ndeblurring method that can estimate not only the clear foreground and\nbackground, but also the object motion more accurately. We also provide a novel\nanalysis on the blur kernel at object boundaries, which shows the distinctive\ncharacteristics of the blur kernel that cannot be captured by conventional blur\nmodels. Experimental results on synthetic and real blurred videos demonstrate\nthat the proposed method yields superior results, especially at object\nboundaries.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:27:38 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Ahn", "Byeongjoo", ""], ["Kim", "Tae Hyun", ""], ["Kim", "Wonsik", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1611.09577", "submitter": "Iryna Korshunova", "authors": "Iryna Korshunova, Wenzhe Shi, Joni Dambre, Lucas Theis", "title": "Fast Face-swap Using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of face swapping in images, where an input identity\nis transformed into a target identity while preserving pose, facial expression,\nand lighting. To perform this mapping, we use convolutional neural networks\ntrained to capture the appearance of the target identity from an unstructured\ncollection of his/her photographs.This approach is enabled by framing the face\nswapping problem in terms of style transfer, where the goal is to render an\nimage in the style of another one. Building on recent advances in this area, we\ndevise a new loss function that enables the network to produce highly\nphotorealistic results. By combining neural networks with simple pre- and\npost-processing steps, we aim at making face swap work in real-time with no\ninput from the user.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:44:20 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 13:31:39 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Korshunova", "Iryna", ""], ["Shi", "Wenzhe", ""], ["Dambre", "Joni", ""], ["Theis", "Lucas", ""]]}, {"id": "1611.09580", "submitter": "Kai Yu", "authors": "Kai Yu, Yang Zhou, Da Li, Zhang Zhang, Kaiqi Huang", "title": "A Large-scale Distributed Video Parsing and Evaluation Platform", "comments": "Accepted by Chinese Conference on Intelligent Visual Surveillance\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual surveillance systems have become one of the largest data sources of\nBig Visual Data in real world. However, existing systems for video analysis\nstill lack the ability to handle the problems of scalability, expansibility and\nerror-prone, though great advances have been achieved in a number of visual\nrecognition tasks and surveillance applications, e.g., pedestrian/vehicle\ndetection, people/vehicle counting. Moreover, few algorithms explore the\nspecific values/characteristics in large-scale surveillance videos. To address\nthese problems in large-scale video analysis, we develop a scalable video\nparsing and evaluation platform through combining some advanced techniques for\nBig Data processing, including Spark Streaming, Kafka and Hadoop Distributed\nFilesystem (HDFS). Also, a Web User Interface is designed in the system, to\ncollect users' degrees of satisfaction on the recognition tasks so as to\nevaluate the performance of the whole system. Furthermore, the highly\nextensible platform running on the long-term surveillance videos makes it\npossible to develop more intelligent incremental algorithms to enhance the\nperformance of various visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 12:07:37 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Yu", "Kai", ""], ["Zhou", "Yang", ""], ["Li", "Da", ""], ["Zhang", "Zhang", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1611.09587", "submitter": "Si Liu", "authors": "Si Liu, Changhu Wang, Ruihe Qian, Han Yu, Renda Bao", "title": "Surveillance Video Parsing with Single Frame Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance video parsing, which segments the video frames into several\nlabels, e.g., face, pants, left-leg, has wide applications.\nHowever,pixel-wisely annotating all frames is tedious and inefficient. In this\npaper, we develop a Single frame Video Parsing (SVP) method which requires only\none labeled frame per video in training stage. To parse one particular frame,\nthe video segment preceding the frame is jointly considered. SVP (1) roughly\nparses the frames within the video segment, (2) estimates the optical flow\nbetween frames and (3) fuses the rough parsing results warped by optical flow\nto produce the refined parsing result. The three components of SVP, namely\nframe parsing, optical flow estimation and temporal fusion are integrated in an\nend-to-end manner. Experimental results on two surveillance video datasets show\nthe superiority of SVP over state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 12:22:46 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Liu", "Si", ""], ["Wang", "Changhu", ""], ["Qian", "Ruihe", ""], ["Yu", "Han", ""], ["Bao", "Renda", ""]]}, {"id": "1611.09718", "submitter": "Thalaiyasingam Ajanthan", "authors": "Thalaiyasingam Ajanthan, Alban Desmaison, Rudy Bunel, Mathieu\n  Salzmann, Philip H.S. Torr, M. Pawan Kumar", "title": "Efficient Linear Programming for Dense CRFs", "comments": "24 pages, 10 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fully connected conditional random field (CRF) with Gaussian pairwise\npotentials has proven popular and effective for multi-class semantic\nsegmentation. While the energy of a dense CRF can be minimized accurately using\na linear programming (LP) relaxation, the state-of-the-art algorithm is too\nslow to be useful in practice. To alleviate this deficiency, we introduce an\nefficient LP minimization algorithm for dense CRFs. To this end, we develop a\nproximal minimization framework, where the dual of each proximal problem is\noptimized via block coordinate descent. We show that each block of variables\ncan be efficiently optimized. Specifically, for one block, the problem\ndecomposes into significantly smaller subproblems, each of which is defined\nover a single pixel. For the other block, the problem is optimized via\nconditional gradient descent. This has two advantages: 1) the conditional\ngradient can be computed in a time linear in the number of pixels and labels;\nand 2) the optimal step size can be computed analytically. Our experiments on\nstandard datasets provide compelling evidence that our approach outperforms all\nexisting baselines including the previous LP based approach for dense CRFs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 16:46:54 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 07:34:13 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Ajanthan", "Thalaiyasingam", ""], ["Desmaison", "Alban", ""], ["Bunel", "Rudy", ""], ["Salzmann", "Mathieu", ""], ["Torr", "Philip H. S.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1611.09726", "submitter": "Michael Blot", "authors": "Michael Blot, David Picard, Matthieu Cord, Nicolas Thome", "title": "Gossip training for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of speeding up the training of convolutional networks.\nHere we study a distributed method adapted to stochastic gradient descent\n(SGD). The parallel optimization setup uses several threads, each applying\nindividual gradient descents on a local variable. We propose a new way to share\ninformation between different threads inspired by gossip algorithms and showing\ngood consensus convergence properties. Our method called GoSGD has the\nadvantage to be fully asynchronous and decentralized. We compared our method to\nthe recent EASGD in \\cite{elastic} on CIFAR-10 show encouraging results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:01:31 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Blot", "Michael", ""], ["Picard", "David", ""], ["Cord", "Matthieu", ""], ["Thome", "Nicolas", ""]]}, {"id": "1611.09769", "submitter": "Shaikat Galib", "authors": "Shaikat Galib, Fahima Islam, Muhammad Abir, and Hyoung-Koo Lee", "title": "Computer Aided Detection of Oral Lesions on CT Images", "comments": null, "journal-ref": null, "doi": "10.1088/1748-0221-10-12-C12030", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oral lesions are important findings on computed tomography (CT) images. In\nthis study, a fully automatic method to detect oral lesions in mandibular\nregion from dental CT images is proposed. Two methods were developed to\nrecognize two types of lesions namely (1) Close border (CB) lesions and (2)\nOpen border (OB) lesions, which cover most of the lesion types that can be\nfound on CT images. For the detection of CB lesions, fifteen features were\nextracted from each initial lesion candidates and multi layer perceptron (MLP)\nneural network was used to classify suspicious regions. Moreover, OB lesions\nwere detected using a rule based image processing method, where no feature\nextraction or classification algorithm were used. The results were validated\nusing a CT dataset of 52 patients, where 22 patients had abnormalities and 30\npatients were normal. Using non-training dataset, CB detection algorithm\nyielded 71% sensitivity with 0.31 false positives per patient. Furthermore, OB\ndetection algorithm achieved 100% sensitivity with 0.13 false positives per\npatient. Results suggest that, the proposed framework, which consists of two\nmethods, has the potential to be used in clinical context, and assist\nradiologists for better diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 18:24:23 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Galib", "Shaikat", ""], ["Islam", "Fahima", ""], ["Abir", "Muhammad", ""], ["Lee", "Hyoung-Koo", ""]]}, {"id": "1611.09791", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Koushik Maharatna, Fabio Apicella,\n  Georgia Chronaki, Federico Sicca, David Cohen, Filippo Muratori", "title": "On the Existence of Synchrostates in Multichannel EEG Signals during\n  Face-perception Tasks", "comments": "30 pages, 22 figures, 2 tables", "journal-ref": "Biomedical Physics & Engineering Express, vol. 1, no. 1, pp.\n  015002, 2015", "doi": "10.1088/2057-1976/1/1/015002", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase synchronisation in multichannel EEG is known as the manifestation of\nfunctional brain connectivity. Traditional phase synchronisation studies are\nmostly based on time average synchrony measures hence do not preserve the\ntemporal evolution of the phase difference. Here we propose a new method to\nshow the existence of a small set of unique phase synchronised patterns or\n\"states\" in multi-channel EEG recordings, each \"state\" being stable of the\norder of ms, from typical and pathological subjects during face perception\ntasks. The proposed methodology bridges the concepts of EEG microstates and\nphase synchronisation in time and frequency domain respectively. The analysis\nis reported for four groups of children including typical, Autism Spectrum\nDisorder (ASD), low and high anxiety subjects - a total of 44 subjects. In all\ncases, we observe consistent existence of these states - termed as\nsynchrostates - within specific cognition related frequency bands (beta and\ngamma bands), though the topographies of these synchrostates differ for\ndifferent subject groups with different pathological conditions. The\ninter-synchrostate switching follows a well-defined sequence capturing the\nunderlying inter-electrode phase relation dynamics in stimulus- and\nperson-centric manner. Our study is motivated from the well-known EEG\nmicrostate exhibiting stable potential maps over the scalp. However, here we\nreport a similar observation of quasi-stable phase synchronised states in\nmultichannel EEG. The existence of the synchrostates coupled with their unique\nswitching sequence characteristics could be considered as a potentially new\nfield over contemporary EEG phase synchronisation studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:03:29 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Apicella", "Fabio", ""], ["Chronaki", "Georgia", ""], ["Sicca", "Federico", ""], ["Cohen", "David", ""], ["Muratori", "Filippo", ""]]}, {"id": "1611.09803", "submitter": "Shay Zweig", "authors": "Shay Zweig, Lior Wolf", "title": "InterpoNet, A brain inspired neural network for optical flow dense\n  interpolation", "comments": "16 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse-to-dense interpolation for optical flow is a fundamental phase in the\npipeline of most of the leading optical flow estimation algorithms. The current\nstate-of-the-art method for interpolation, EpicFlow, is a local average method\nbased on an edge aware geodesic distance. We propose a new data-driven\nsparse-to-dense interpolation algorithm based on a fully convolutional network.\nWe draw inspiration from the filling-in process in the visual cortex and\nintroduce lateral dependencies between neurons and multi-layer supervision into\nour learning process. We also show the importance of the image contour to the\nlearning process. Our method is robust and outperforms EpicFlow on competitive\noptical flow benchmarks with several underlying matching algorithms. This leads\nto state-of-the-art performance on the Sintel and KITTI 2012 benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:37:57 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 11:08:13 GMT"}, {"version": "v3", "created": "Fri, 24 Feb 2017 13:48:29 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Zweig", "Shay", ""], ["Wolf", "Lior", ""]]}, {"id": "1611.09811", "submitter": "Mohammad Hamed Mozaffari", "authors": "Mohammad Hamed Mozaffari and WonSook Lee", "title": "3D Ultrasound image segmentation: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional Ultrasound image segmentation methods are surveyed in this\npaper. The focus of this report is to investigate applications of these\ntechniques and a review of the original ideas and concepts. Although many\ntwo-dimensional image segmentation in the literature have been considered as a\nthree-dimensional approach by mistake but we review them as a three-dimensional\ntechnique. We select the studies that have addressed the problem of medical\nthree-dimensional Ultrasound image segmentation utilizing their proposed\ntechniques. The evaluation methods and comparison between them are presented\nand tabulated in terms of evaluation techniques, interactivity, and robustness.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:57:28 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Mozaffari", "Mohammad Hamed", ""], ["Lee", "WonSook", ""]]}, {"id": "1611.09813", "submitter": "Dushyant Mehta", "authors": "Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal Fua, Oleksandr\n  Sotnychenko, Weipeng Xu, Christian Theobalt", "title": "Monocular 3D Human Pose Estimation In The Wild Using Improved CNN\n  Supervision", "comments": "Accepted at the International Conference on 3D Vision (3DV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a CNN-based approach for 3D human body pose estimation from single\nRGB images that addresses the issue of limited generalizability of models\ntrained solely on the starkly limited publicly available 3D pose data. Using\nonly the existing 3D pose data and 2D pose data, we show state-of-the-art\nperformance on established benchmarks through transfer of learned features,\nwhile also generalizing to in-the-wild scenes. We further introduce a new\ntraining set for human body pose estimation from monocular images of real\nhumans that has the ground truth captured with a multi-camera marker-less\nmotion capture system. It complements existing corpora with greater diversity\nin pose, human appearance, clothing, occlusion, and viewpoints, and enables an\nincreased scope of augmentation. We also contribute a new benchmark that covers\noutdoor and indoor scenes, and demonstrate that our 3D pose dataset shows\nbetter in-the-wild performance than existing annotated data, which is further\nimproved in conjunction with transfer learning from 2D pose data. All in all,\nwe argue that the use of transfer learning of representations in tandem with\nalgorithmic and data contributions is crucial for general 3D body pose\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:03:19 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 18:20:02 GMT"}, {"version": "v3", "created": "Mon, 20 Mar 2017 18:28:41 GMT"}, {"version": "v4", "created": "Mon, 8 May 2017 09:25:53 GMT"}, {"version": "v5", "created": "Wed, 4 Oct 2017 15:21:46 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Mehta", "Dushyant", ""], ["Rhodin", "Helge", ""], ["Casas", "Dan", ""], ["Fua", "Pascal", ""], ["Sotnychenko", "Oleksandr", ""], ["Xu", "Weipeng", ""], ["Theobalt", "Christian", ""]]}, {"id": "1611.09819", "submitter": "Daniel Harari", "authors": "Daniel Harari, Tao Gao, Nancy Kanwisher, Joshua Tenenbaum, Shimon\n  Ullman", "title": "Measuring and modeling the perception of natural and unconstrained gaze\n  in humans and machines", "comments": "Daniel Harari and Tao Gao contributed equally to this work", "journal-ref": null, "doi": null, "report-no": "Center for Brains, Minds and Machines Memo No. 059", "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are remarkably adept at interpreting the gaze direction of other\nindividuals in their surroundings. This skill is at the core of the ability to\nengage in joint visual attention, which is essential for establishing social\ninteractions. How accurate are humans in determining the gaze direction of\nothers in lifelike scenes, when they can move their heads and eyes freely, and\nwhat are the sources of information for the underlying perceptual processes?\nThese questions pose a challenge from both empirical and computational\nperspectives, due to the complexity of the visual input in real-life\nsituations. Here we measure empirically human accuracy in perceiving the gaze\ndirection of others in lifelike scenes, and study computationally the sources\nof information and representations underlying this cognitive capacity. We show\nthat humans perform better in face-to-face conditions compared with recorded\nconditions, and that this advantage is not due to the availability of input\ndynamics. We further show that humans are still performing well when only the\neyes-region is visible, rather than the whole face. We develop a computational\nmodel, which replicates the pattern of human performance, including the finding\nthat the eyes-region contains on its own, the required information for\nestimating both head orientation and direction of gaze. Consistent with\nneurophysiological findings on task-specific face regions in the brain, the\nlearned computational representations reproduce perceptual effects such as the\nWollaston illusion, when trained to estimate direction of gaze, but not when\ntrained to recognize objects or faces.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:11:09 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Harari", "Daniel", ""], ["Gao", "Tao", ""], ["Kanwisher", "Nancy", ""], ["Tenenbaum", "Joshua", ""], ["Ullman", "Shimon", ""]]}, {"id": "1611.09842", "submitter": "Richard Zhang", "authors": "Richard Zhang, Phillip Isola, Alexei A. Efros", "title": "Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel\n  Prediction", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose split-brain autoencoders, a straightforward modification of the\ntraditional autoencoder architecture, for unsupervised representation learning.\nThe method adds a split to the network, resulting in two disjoint sub-networks.\nEach sub-network is trained to perform a difficult task -- predicting one\nsubset of the data channels from another. Together, the sub-networks extract\nfeatures from the entire input signal. By forcing the network to solve\ncross-channel prediction tasks, we induce a representation within the network\nwhich transfers well to other, unseen tasks. This method achieves\nstate-of-the-art performance on several large-scale transfer learning\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:55:42 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 01:03:04 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 08:12:21 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Zhang", "Richard", ""], ["Isola", "Phillip", ""], ["Efros", "Alexei A.", ""]]}, {"id": "1611.09932", "submitter": "Yaming Wang", "authors": "Yaming Wang, Vlad I. Morariu, Larry S. Davis", "title": "Learning a Discriminative Filter Bank within a CNN for Fine-grained\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to earlier multistage frameworks using CNN features, recent\nend-to-end deep approaches for fine-grained recognition essentially enhance the\nmid-level learning capability of CNNs. Previous approaches achieve this by\nintroducing an auxiliary network to infuse localization information into the\nmain classification network, or a sophisticated feature encoding method to\ncapture higher order feature statistics. We show that mid-level representation\nlearning can be enhanced within the CNN framework, by learning a bank of\nconvolutional filters that capture class-specific discriminative patches\nwithout extra part or bounding box annotations. Such a filter bank is well\nstructured, properly initialized and discriminatively learned through a novel\nasymmetric multi-stream architecture with convolutional filter supervision and\na non-random layer initialization. Experimental results show that our approach\nachieves state-of-the-art on three publicly available fine-grained recognition\ndatasets (CUB-200-2011, Stanford Cars and FGVC-Aircraft). Ablation studies and\nvisualizations are provided to understand our approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:01:59 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 00:15:47 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 01:48:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wang", "Yaming", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1611.09942", "submitter": "L. Jason Anastasopoulos", "authors": "L. Jason Anastasopoulos, Dhruvil Badani, Crystal Lee, Shiry Ginosar\n  and Jake Williams", "title": "Photographic home styles in Congress: a computer vision approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While members of Congress now routinely communicate with constituents using\nimages on a variety of internet platforms, little is known about how images are\nused as a means of strategic political communication. This is due primarily to\ncomputational limitations which have prevented large-scale, systematic analyses\nof image features. New developments in computer vision, however, are bringing\nthe systematic study of images within reach. Here, we develop a framework for\nunderstanding visual political communication by extending Fenno's analysis of\nhome style (Fenno 1978) to images and introduce \"photographic\" home styles.\nUsing approximately 192,000 photographs collected from MCs Facebook profiles,\nwe build machine learning software with convolutional neural networks and\nconduct an image manipulation experiment to explore how the race of people that\nMCs pose with shape photographic home styles. We find evidence that electoral\npressures shape photographic home styles and demonstrate that Democratic and\nRepublican members of Congress use images in very different ways.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:41:00 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 23:34:49 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Anastasopoulos", "L. Jason", ""], ["Badani", "Dhruvil", ""], ["Lee", "Crystal", ""], ["Ginosar", "Shiry", ""], ["Williams", "Jake", ""]]}, {"id": "1611.09956", "submitter": "HaiLiang Li", "authors": "Hailiang Li, Kin-Man Lam, Man-Yau Chiu, Kangheng Wu, Zhibin Lei", "title": "Efficient Likelihood Bayesian Constrained Local Model", "comments": "6 pages, for submitting to ICME-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The constrained local model (CLM) proposes a paradigm that the locations of a\nset of local landmark detectors are constrained to lie in a subspace, spanned\nby a shape point distribution model (PDM). Fitting the model to an object\ninvolves two steps. A response map, which represents the likelihood of the\nlocation of a landmark, is first computed for each landmark using local-texture\ndetectors. Then, an optimal PDM is determined by jointly maximizing all the\nresponse maps simultaneously, with a global shape constraint. This global\noptimization can be considered as a Bayesian inference problem, where the\nposterior distribution of the shape parameters, as well as the pose parameters,\ncan be inferred using maximum a posteriori (MAP). In this paper, we present a\ncascaded face-alignment approach, which employs random-forest regressors to\nestimate the positions of each landmark, as a likelihood term, efficiently in\nthe CLM model. Interpretation from CLM framework, this algorithm is named as an\nefficient likelihood Bayesian constrained local model (elBCLM). Furthermore, in\neach stage of the regressors, the PDM non-rigid parameters of previous stage\ncan work as shape clues for training each stage regressors. Experimental\nresults on benchmarks show our approach achieve about 3 to 5 times speed-up\ncompared with CLM models and improve around 10% on fitting quality compare with\nthe same setting regression models.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 00:41:32 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Li", "Hailiang", ""], ["Lam", "Kin-Man", ""], ["Chiu", "Man-Yau", ""], ["Wu", "Kangheng", ""], ["Lei", "Zhibin", ""]]}, {"id": "1611.09958", "submitter": "Young-Jun Yu", "authors": "Young-jun Yu", "title": "Machine Learning for Dental Image Analysis", "comments": "This study was reviewed and approved by the institutional review\n  board of the Pusan National University Dental Hospital (PNUPH-2015-034)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to study the application of artificial intelligence (AI) to dental\nimaging, we applied AI technology to classify a set of panoramic radiographs\nusing (a) a convolutional neural network (CNN) which is a form of an artificial\nneural network (ANN), (b) representative image cognition algorithms that\nimplement scale-invariant feature transform (SIFT), and (c) histogram of\noriented gradients (HOG).\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:17:34 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 11:27:37 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yu", "Young-jun", ""]]}, {"id": "1611.09960", "submitter": "Chunhua Shen", "authors": "Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, Ian Reid", "title": "Attend in groups: a weakly-supervised deep learning framework for\n  learning from web data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale datasets have driven the rapid development of deep neural\nnetworks for visual recognition. However, annotating a massive dataset is\nexpensive and time-consuming. Web images and their labels are, in comparison,\nmuch easier to obtain, but direct training on such automatically harvested\nimages can lead to unsatisfactory performance, because the noisy labels of Web\nimages adversely affect the learned recognition models. To address this\ndrawback we propose an end-to-end weakly-supervised deep learning framework\nwhich is robust to the label noise in Web images. The proposed framework relies\non two unified strategies -- random grouping and attention -- to effectively\nreduce the negative impact of noisy web image annotations. Specifically, random\ngrouping stacks multiple images into a single training instance and thus\nincreases the labeling accuracy at the instance level. Attention, on the other\nhand, suppresses the noisy signals from both incorrectly labeled images and\nless discriminative image regions. By conducting intensive experiments on two\nchallenging datasets, including a newly collected fine-grained dataset with Web\nimages of different car models, the superior performance of the proposed\nmethods over competitive baselines is clearly demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:23:43 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Zhuang", "Bohan", ""], ["Liu", "Lingqiao", ""], ["Li", "Yao", ""], ["Shen", "Chunhua", ""], ["Reid", "Ian", ""]]}, {"id": "1611.09961", "submitter": "Raymond Yeh", "authors": "Raymond Yeh, Ziwei Liu, Dan B Goldman, Aseem Agarwala", "title": "Semantic Facial Expression Editing using Autoencoded Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level manipulation of facial expressions in images --- such as changing\na smile to a neutral expression --- is challenging because facial expression\nchanges are highly non-linear, and vary depending on the appearance of the\nface. We present a fully automatic approach to editing faces that combines the\nadvantages of flow-based face manipulation with the more recent generative\ncapabilities of Variational Autoencoders (VAEs). During training, our model\nlearns to encode the flow from one expression to another over a low-dimensional\nlatent space. At test time, expression editing can be done simply using latent\nvector arithmetic. We evaluate our methods on two applications: 1) single-image\nfacial expression editing, and 2) facial expression interpolation between two\nimages. We demonstrate that our method generates images of higher perceptual\nquality than previous VAE and flow-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:24:22 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Yeh", "Raymond", ""], ["Liu", "Ziwei", ""], ["Goldman", "Dan B", ""], ["Agarwala", "Aseem", ""]]}, {"id": "1611.09967", "submitter": "Chunhua Shen", "authors": "Yao Li, Guosheng Lin, Bohan Zhuang, Lingqiao Liu, Chunhua Shen, Anton\n  van den Hengel", "title": "Sequential Person Recognition in Photo Albums with a Recurrent Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the identities of people in everyday photos is still a very\nchallenging problem for machine vision, due to non-frontal faces, changes in\nclothing, location, lighting and similar. Recent studies have shown that rich\nrelational information between people in the same photo can help in recognizing\ntheir identities. In this work, we propose to model the relational information\nbetween people as a sequence prediction task. At the core of our work is a\nnovel recurrent network architecture, in which relational information between\ninstances' labels and appearance are modeled jointly. In addition to relational\ncues, scene context is incorporated in our sequence prediction model with no\nadditional cost. In this sense, our approach is a unified framework for\nmodeling both contextual cues and visual appearance of person instances. Our\nmodel is trained end-to-end with a sequence of annotated instances in a photo\nas inputs, and a sequence of corresponding labels as targets. We demonstrate\nthat this simple but elegant formulation achieves state-of-the-art performance\non the newly released People In Photo Albums (PIPA) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:45:23 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Li", "Yao", ""], ["Lin", "Guosheng", ""], ["Zhuang", "Bohan", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1611.09969", "submitter": "Chao Yang Mr.", "authors": "Chao Yang, Xin Lu, Zhe Lin, Eli Shechtman, Oliver Wang and Hao Li", "title": "High-Resolution Image Inpainting using Multi-Scale Neural Patch\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning have shown exciting promise in filling large\nholes in natural images with semantically plausible and context aware details,\nimpacting fundamental image manipulation tasks such as object removal. While\nthese learning-based methods are significantly more effective in capturing\nhigh-level features than prior techniques, they can only handle very\nlow-resolution inputs due to memory limitations and difficulty in training.\nEven for slightly larger images, the inpainted regions would appear blurry and\nunpleasant boundaries become visible. We propose a multi-scale neural patch\nsynthesis approach based on joint optimization of image content and texture\nconstraints, which not only preserves contextual structures but also produces\nhigh-frequency details by matching and adapting patches with the most similar\nmid-layer feature correlations of a deep classification network. We evaluate\nour method on the ImageNet and Paris Streetview datasets and achieved\nstate-of-the-art inpainting accuracy. We show our approach produces sharper and\nmore coherent results than prior methods, especially for high-resolution\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:58:54 GMT"}, {"version": "v2", "created": "Thu, 13 Apr 2017 06:56:06 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Yang", "Chao", ""], ["Lu", "Xin", ""], ["Lin", "Zhe", ""], ["Shechtman", "Eli", ""], ["Wang", "Oliver", ""], ["Li", "Hao", ""]]}, {"id": "1611.09978", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, Kate\n  Saenko", "title": "Modeling Relationships in Referential Expressions with Compositional\n  Modular Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often refer to entities in an image in terms of their relationships\nwith other entities. For example, \"the black cat sitting under the table\"\nrefers to both a \"black cat\" entity and its relationship with another \"table\"\nentity. Understanding these relationships is essential for interpreting and\ngrounding such natural language expressions. Most prior work focuses on either\ngrounding entire referential expressions holistically to one region, or\nlocalizing relationships based on a fixed set of categories. In this paper we\ninstead present a modular deep architecture capable of analyzing referential\nexpressions into their component parts, identifying entities and relationships\nmentioned in the input expression and grounding them all in the scene. We call\nthis approach Compositional Modular Networks (CMNs): a novel architecture that\nlearns linguistic analysis and visual inference end-to-end. Our approach is\nbuilt around two types of neural modules that inspect local regions and\npairwise interactions between regions. We evaluate CMNs on multiple referential\nexpression datasets, outperforming state-of-the-art approaches on all tasks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 02:52:09 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Hu", "Ronghang", ""], ["Rohrbach", "Marcus", ""], ["Andreas", "Jacob", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1611.10010", "submitter": "Debidatta Dwibedi", "authors": "Debidatta Dwibedi, Tomasz Malisiewicz, Vijay Badrinarayanan, Andrew\n  Rabinovich", "title": "Deep Cuboid Detection: Beyond 2D Bounding Boxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Cuboid Detector which takes a consumer-quality RGB image of\na cluttered scene and localizes all 3D cuboids (box-like objects). Contrary to\nclassical approaches which fit a 3D model from low-level cues like corners,\nedges, and vanishing points, we propose an end-to-end deep learning system to\ndetect cuboids across many semantic categories (e.g., ovens, shipping boxes,\nand furniture). We localize cuboids with a 2D bounding box, and simultaneously\nlocalize the cuboid's corners, effectively producing a 3D interpretation of\nbox-like objects. We refine keypoints by pooling convolutional features\niteratively, improving the baseline method significantly. Our deep learning\ncuboid detector is trained in an end-to-end fashion and is suitable for\nreal-time applications in augmented reality (AR) and robotics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 06:00:47 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Dwibedi", "Debidatta", ""], ["Malisiewicz", "Tomasz", ""], ["Badrinarayanan", "Vijay", ""], ["Rabinovich", "Andrew", ""]]}, {"id": "1611.10012", "submitter": "Jonathan Huang", "authors": "Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu, Anoop\n  Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wojna, Yang Song, Sergio\n  Guadarrama, Kevin Murphy", "title": "Speed/accuracy trade-offs for modern convolutional object detectors", "comments": "Accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to serve as a guide for selecting a detection\narchitecture that achieves the right speed/memory/accuracy balance for a given\napplication and platform. To this end, we investigate various ways to trade\naccuracy for speed and memory usage in modern convolutional object detection\nsystems. A number of successful systems have been proposed in recent years, but\napples-to-apples comparisons are difficult due to different base feature\nextractors (e.g., VGG, Residual Networks), different default image resolutions,\nas well as different hardware and software platforms. We present a unified\nimplementation of the Faster R-CNN [Ren et al., 2015], R-FCN [Dai et al., 2016]\nand SSD [Liu et al., 2015] systems, which we view as \"meta-architectures\" and\ntrace out the speed/accuracy trade-off curve created by using alternative\nfeature extractors and varying other critical parameters such as image size\nwithin each of these meta-architectures. On one extreme end of this spectrum\nwhere speed and memory are critical, we present a detector that achieves real\ntime speeds and can be deployed on a mobile device. On the opposite end in\nwhich accuracy is critical, we present a detector that achieves\nstate-of-the-art performance measured on the COCO detection task.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 06:06:15 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 03:52:10 GMT"}, {"version": "v3", "created": "Tue, 25 Apr 2017 03:42:55 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Huang", "Jonathan", ""], ["Rathod", "Vivek", ""], ["Sun", "Chen", ""], ["Zhu", "Menglong", ""], ["Korattikara", "Anoop", ""], ["Fathi", "Alireza", ""], ["Fischer", "Ian", ""], ["Wojna", "Zbigniew", ""], ["Song", "Yang", ""], ["Guadarrama", "Sergio", ""], ["Murphy", "Kevin", ""]]}, {"id": "1611.10017", "submitter": "Gou Koutaki", "authors": "Gou Koutaki, Keiichiro Shirai, Mitsuru Ambai", "title": "Fast Supervised Discrete Hashing and its Analysis", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a learning-based supervised discrete hashing\nmethod. Binary hashing is widely used for large-scale image retrieval as well\nas video and document searches because the compact representation of binary\ncode is essential for data storage and reasonable for query searches using\nbit-operations. The recently proposed Supervised Discrete Hashing (SDH)\nefficiently solves mixed-integer programming problems by alternating\noptimization and the Discrete Cyclic Coordinate descent (DCC) method. We show\nthat the SDH model can be simplified without performance degradation based on\nsome preliminary experiments; we call the approximate model for this the \"Fast\nSDH\" (FSDH) model. We analyze the FSDH model and provide a mathematically exact\nsolution for it. In contrast to SDH, our model does not require an alternating\noptimization algorithm and does not depend on initial values. FSDH is also\neasier to implement than Iterative Quantization (ITQ). Experimental results\ninvolving a large-scale database showed that FSDH outperforms conventional SDH\nin terms of precision, recall, and computation time.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 06:35:39 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Koutaki", "Gou", ""], ["Shirai", "Keiichiro", ""], ["Ambai", "Mitsuru", ""]]}, {"id": "1611.10031", "submitter": "Peng Liu", "authors": "Peng Liu, Hui Zhang, and Kie B. Eom", "title": "Active Deep Learning for Classification of Hyperspectral Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active deep learning classification of hyperspectral images is considered in\nthis paper. Deep learning has achieved success in many applications, but\ngood-quality labeled samples are needed to construct a deep learning network.\nIt is expensive getting good labeled samples in hyperspectral images for remote\nsensing applications. An active learning algorithm based on a weighted\nincremental dictionary learning is proposed for such applications. The proposed\nalgorithm selects training samples that maximize two selection criteria, namely\nrepresentative and uncertainty. This algorithm trains a deep network\nefficiently by actively selecting training samples at each iteration. The\nproposed algorithm is applied for the classification of hyperspectral images,\nand compared with other classification algorithms employing active learning. It\nis shown that the proposed algorithm is efficient and effective in classifying\nhyperspectral images.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 07:34:46 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Liu", "Peng", ""], ["Zhang", "Hui", ""], ["Eom", "Kie B.", ""]]}, {"id": "1611.10080", "submitter": "Chunhua Shen", "authors": "Zifeng Wu, Chunhua Shen, and Anton van den Hengel", "title": "Wider or Deeper: Revisiting the ResNet Model for Visual Recognition", "comments": "Code available at: https://github.com/itijyou/ademxapp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend towards increasingly deep neural networks has been driven by a\ngeneral observation that increasing depth increases the performance of a\nnetwork. Recently, however, evidence has been amassing that simply increasing\ndepth may not be the best way to increase performance, particularly given other\nlimitations. Investigations into deep residual networks have also suggested\nthat they may not in fact be operating as a single deep network, but rather as\nan ensemble of many relatively shallow networks. We examine these issues, and\nin doing so arrive at a new interpretation of the unravelled view of deep\nresidual networks which explains some of the behaviours that have been observed\nexperimentally. As a result, we are able to derive a new, shallower,\narchitecture of residual networks which significantly outperforms much deeper\nmodels such as ResNet-200 on the ImageNet classification dataset. We also show\nthat this performance is transferable to other problem domains by developing a\nsemantic segmentation approach which outperforms the state-of-the-art by a\nremarkable margin on datasets including PASCAL VOC, PASCAL Context, and\nCityscapes. The architecture that we propose thus outperforms its comparators,\nincluding very deep ResNets, and yet is more efficient in memory use and\nsometimes also in training time. The code and models are available at\nhttps://github.com/itijyou/ademxapp\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 10:24:32 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Wu", "Zifeng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1611.10104", "submitter": "K S Manjunatha", "authors": "D. S. Guru, K. S. Manjunatha and S. Manjunath", "title": "User Dependent Features in Online Signature Verification", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel approach for verification of on-line\nsignatures based on user dependent feature selection and symbolic\nrepresentation. Unlike other signature verification methods, which work with\nsame features for all users, the proposed approach introduces the concept of\nuser dependent features. It exploits the typicality of each and every user to\nselect different features for different users. Initially all possible features\nare extracted for all users and a method of feature selection is employed for\nselecting user dependent features. The selected features are clustered using\nFuzzy C means algorithm. In order to preserve the intra-class variation within\neach user, we recommend to represent each cluster in the form of an interval\nvalued symbolic feature vector. A method of signature verification based on the\nproposed cluster based symbolic representation is also presented. Extensive\nexperimentations are conducted on MCYT-100 User (DB1) and MCYT-330 User (DB2)\nonline signature data sets to demonstrate the effectiveness of the proposed\nnovel approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 11:51:50 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Guru", "D. S.", ""], ["Manjunatha", "K. S.", ""], ["Manjunath", "S.", ""]]}, {"id": "1611.10152", "submitter": "Hongwen Zhang", "authors": "Hongwen Zhang, Qi Li, Zhenan Sun, Yunfan Liu", "title": "Combining Data-driven and Model-driven Methods for Robust Facial\n  Landmark Detection", "comments": "Journal article accepted for publication in IEEE TIFS", "journal-ref": null, "doi": "10.1109/TIFS.2018.2800901", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmark detection is an important yet challenging task for real-world\ncomputer vision applications. This paper proposes an effective and robust\napproach for facial landmark detection by combining data- and model-driven\nmethods. Firstly, a Fully Convolutional Network (FCN) is trained to compute\nresponse maps of all facial landmark points. Such a data-driven method could\nmake full use of holistic information in a facial image for global estimation\nof facial landmarks. After that, the maximum points in the response maps are\nfitted with a pre-trained Point Distribution Model (PDM) to generate the\ninitial facial shape. This model-driven method is able to correct the\ninaccurate locations of outliers by considering the shape prior information.\nFinally, a weighted version of Regularized Landmark Mean-Shift (RLMS) is\nemployed to fine-tune the facial shape iteratively. This\nEstimation-Correction-Tuning process perfectly combines the advantages of the\nglobal robustness of data-driven method (FCN), outlier correction capability of\nmodel-driven method (PDM) and non-parametric optimization of RLMS. Results of\nextensive experiments demonstrate that our approach achieves state-of-the-art\nperformances on challenging datasets including 300W, AFLW, AFW and COFW. The\nproposed method is able to produce satisfying detection results on face images\nwith exaggerated expressions, large head poses, and partial occlusions.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:01:45 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 03:03:59 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Zhang", "Hongwen", ""], ["Li", "Qi", ""], ["Sun", "Zhenan", ""], ["Liu", "Yunfan", ""]]}, {"id": "1611.10162", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar and Andreas Bulling and Mario Fritz", "title": "Predicting the Category and Attributes of Visual Search Targets Using\n  Deep Gaze Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the target of visual search from eye fixation (gaze) data is a\nchallenging problem with many applications in human-computer interaction. In\ncontrast to previous work that has focused on individual instances as a search\ntarget, we propose the first approach to predict categories and attributes of\nsearch targets based on gaze data. However, state of the art models for\ncategorical recognition, in general, require large amounts of training data,\nwhich is prohibitive for gaze data. To address this challenge, we propose a\nnovel Gaze Pooling Layer that integrates gaze information into CNN-based\narchitectures as an attention mechanism - incorporating both spatial and\ntemporal aspects of human gaze behavior. We show that our approach is effective\neven when the gaze pooling layer is added to an already trained CNN, thus\neliminating the need for expensive joint data collection of visual and gaze\ndata. We propose an experimental setup and data set and demonstrate the\neffectiveness of our method for search target prediction based on gaze\nbehavior. We further study how to integrate temporal and spatial gaze\ninformation most effectively, and indicate directions for future research in\nthe gaze-based prediction of mental states.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 07:44:49 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 11:52:29 GMT"}, {"version": "v3", "created": "Mon, 3 Apr 2017 11:05:07 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1611.10176", "submitter": "Shuchang Zhou", "authors": "Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou,\n  Yuheng Zou", "title": "Effective Quantization Methods for Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reducing bit-widths of weights, activations, and gradients of a Neural\nNetwork can shrink its storage size and memory usage, and also allow for faster\ntraining and inference by exploiting bitwise operations. However, previous\nattempts for quantization of RNNs show considerable performance degradation\nwhen using low bit-width weights and activations. In this paper, we propose\nmethods to quantize the structure of gates and interlinks in LSTM and GRU\ncells. In addition, we propose balanced quantization methods for weights to\nfurther reduce performance degradation. Experiments on PTB and IMDB datasets\nconfirm effectiveness of our methods as performances of our models match or\nsurpass the previous state-of-the-art of quantized RNN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:33:08 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["He", "Qinyao", ""], ["Wen", "He", ""], ["Zhou", "Shuchang", ""], ["Wu", "Yuxin", ""], ["Yao", "Cong", ""], ["Zhou", "Xinyu", ""], ["Zou", "Yuheng", ""]]}, {"id": "1611.10195", "submitter": "Guido Borghi", "authors": "Guido Borghi, Marco Venturelli, Roberto Vezzani, Rita Cucchiara", "title": "POSEidon: Face-from-Depth for Driver Pose Estimation", "comments": "Accepted in Computer Vision and Pattern Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and accurate upper-body and head pose estimation is a key task for\nautomatic monitoring of driver attention, a challenging context characterized\nby severe illumination changes, occlusions and extreme poses. In this work, we\npresent a new deep learning framework for head localization and pose estimation\non depth images. The core of the proposal is a regression neural network,\ncalled POSEidon, which is composed of three independent convolutional nets\nfollowed by a fusion layer, specially conceived for understanding the pose by\ndepth. In addition, to recover the intrinsic value of face appearance for\nunderstanding head position and orientation, we propose a new Face-from-Depth\napproach for learning image faces from depth. Results in face reconstruction\nare qualitatively impressive. We test the proposed framework on two public\ndatasets, namely Biwi Kinect Head Pose and ICT-3DHP, and on Pandora, a new\nchallenging dataset mainly inspired by the automotive setup. Results show that\nour method overcomes all recent state-of-art works, running in real time at\nmore than 30 frames per second.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:57:06 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 12:08:21 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 22:08:30 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Borghi", "Guido", ""], ["Venturelli", "Marco", ""], ["Vezzani", "Roberto", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1611.10229", "submitter": "Patrick Kn\\\"obelreiter", "authors": "Patrick Kn\\\"obelreiter, Christian Reinbacher, Alexander Shekhovtsov,\n  Thomas Pock", "title": "End-to-End Training of Hybrid CNN-CRF Models for Stereo", "comments": "To appear at CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and principled hybrid CNN+CRF model for stereo estimation.\nOur model allows to exploit the advantages of both, convolutional neural\nnetworks (CNNs) and conditional random fields (CRFs) in an unified approach.\nThe CNNs compute expressive features for matching and distinctive color edges,\nwhich in turn are used to compute the unary and binary costs of the CRF. For\ninference, we apply a recently proposed highly parallel dual block descent\nalgorithm which only needs a small fixed number of iterations to compute a\nhigh-quality approximate minimizer. As the main contribution of the paper, we\npropose a theoretically sound method based on the structured output support\nvector machine (SSVM) to train the hybrid CNN+CRF model on large-scale data\nend-to-end. Our trained models perform very well despite the fact that we are\nusing shallow CNNs and do not apply any kind of post-processing to the final\noutput of the CRF. We evaluate our combined models on challenging stereo\nbenchmarks such as Middlebury 2014 and Kitti 2015 and also investigate the\nperformance of each individual component.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 15:45:02 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 09:33:20 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Kn\u00f6belreiter", "Patrick", ""], ["Reinbacher", "Christian", ""], ["Shekhovtsov", "Alexander", ""], ["Pock", "Thomas", ""]]}, {"id": "1611.10314", "submitter": "Gaurav Mittal", "authors": "Gaurav Mittal, Tanya Marwah, Vineeth N. Balasubramanian", "title": "Sync-DRAW: Automatic Video Generation using Deep Recurrent Attentive\n  Architectures", "comments": null, "journal-ref": null, "doi": "10.1145/3123266.3123309", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for generating videos called\nSynchronized Deep Recurrent Attentive Writer (Sync-DRAW). Sync-DRAW can also\nperform text-to-video generation which, to the best of our knowledge, makes it\nthe first approach of its kind. It combines a Variational Autoencoder~(VAE)\nwith a Recurrent Attention Mechanism in a novel manner to create a temporally\ndependent sequence of frames that are gradually formed over time. The recurrent\nattention mechanism in Sync-DRAW attends to each individual frame of the video\nin sychronization, while the VAE learns a latent distribution for the entire\nvideo at the global level. Our experiments with Bouncing MNIST, KTH and UCF-101\nsuggest that Sync-DRAW is efficient in learning the spatial and temporal\ninformation of the videos and generates frames with high structural integrity,\nand can generate videos from simple captions on these datasets. (Accepted as\noral paper in ACM-Multimedia 2017)\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:07:07 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 15:03:52 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 02:12:28 GMT"}, {"version": "v4", "created": "Sat, 21 Oct 2017 21:02:46 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Mittal", "Gaurav", ""], ["Marwah", "Tanya", ""], ["Balasubramanian", "Vineeth N.", ""]]}, {"id": "1611.10336", "submitter": "Shun Miao", "authors": "Rui Liao, Shun Miao, Pierre de Tournemire, Sasa Grbic, Ali Kamen,\n  Tommaso Mansi, Dorin Comaniciu", "title": "An Artificial Agent for Robust Image Registration", "comments": "To appear in AAAI Conference 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3-D image registration, which involves aligning two or more images, is a\ncritical step in a variety of medical applications from diagnosis to therapy.\nImage registration is commonly performed by optimizing an image matching metric\nas a cost function. However, this task is challenging due to the non-convex\nnature of the matching metric over the plausible registration parameter space\nand insufficient approaches for a robust optimization. As a result, current\napproaches are often customized to a specific problem and sensitive to image\nquality and artifacts. In this paper, we propose a completely different\napproach to image registration, inspired by how experts perform the task. We\nfirst cast the image registration problem as a \"strategy learning\" process,\nwhere the goal is to find the best sequence of motion actions (e.g. up, down,\netc.) that yields image alignment. Within this approach, an artificial agent is\nlearned, modeled using deep convolutional neural networks, with 3D raw image\ndata as the input, and the next optimal action as the output. To cope with the\ndimensionality of the problem, we propose a greedy supervised approach for an\nend-to-end training, coupled with attention-driven hierarchical strategy. The\nresulting registration approach inherently encodes both a data-driven matching\nmetric and an optimal registration strategy (policy). We demonstrate, on two\n3-D/3-D medical image registration examples with drastically different nature\nof challenges, that the artificial agent outperforms several state-of-art\nregistration methods by a large margin in terms of both accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:05:55 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Liao", "Rui", ""], ["Miao", "Shun", ""], ["de Tournemire", "Pierre", ""], ["Grbic", "Sasa", ""], ["Kamen", "Ali", ""], ["Mansi", "Tommaso", ""], ["Comaniciu", "Dorin", ""]]}]