[{"id": "1710.00002", "submitter": "Andrew Wagenmaker", "authors": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "title": "Robust Photometric Stereo Using Learned Image and Gradient Dictionaries", "comments": "ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric stereo is a method for estimating the normal vectors of an object\nfrom images of the object under varying lighting conditions. Motivated by\nseveral recent works that extend photometric stereo to more general objects and\nlighting conditions, we study a new robust approach to photometric stereo that\nutilizes dictionary learning. Specifically, we propose and analyze two\napproaches to adaptive dictionary regularization for the photometric stereo\nproblem. First, we propose an image preprocessing step that utilizes an\nadaptive dictionary learning model to remove noise and other non-idealities\nfrom the image dataset before estimating the normal vectors. We also propose an\nalternative model where we directly apply the adaptive dictionary\nregularization to the normal vectors themselves during estimation. We study the\npractical performance of both methods through extensive simulations, which\ndemonstrate the state-of-the-art performance of both methods in the presence of\nnoise.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 17:22:55 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Wagenmaker", "Andrew J.", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1710.00018", "submitter": "Vladimir Pavlovic", "authors": "Cuong D. Tran and Ognjen Rudovic and Vladimir Pavlovic", "title": "Unsupervised Domain Adaptation with Copula Models", "comments": "IEEE International Workshop On Machine Learning for Signal Processing\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of unsupervised domain adaptation, where no labeled data\nfrom the target domain is provided during training time. To deal with the\npotential discrepancy between the source and target distributions, both in\nfeatures and labels, we exploit a copula-based regression framework. The\nbenefits of this approach are two-fold: (a) it allows us to model a broader\nrange of conditional predictive densities beyond the common exponential family,\n(b) we show how to leverage Sklar's theorem, the essence of the copula\nformulation relating the joint density to the copula dependency functions, to\nfind effective feature mappings that mitigate the domain mismatch. By\ntransforming the data to a copula domain, we show on a number of benchmark\ndatasets (including human emotion estimation), and using different regression\nmodels for prediction, that we can achieve a more robust and accurate\nestimation of target labels, compared to recently proposed feature\ntransformation (adaptation) methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 18:14:55 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Tran", "Cuong D.", ""], ["Rudovic", "Ognjen", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1710.00075", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Anand Rangarajan and Paul D. Gader", "title": "A Gaussian mixture model representation of endmember variability in\n  hyperspectral unmixing", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2018.2795744", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral unmixing while considering endmember variability is usually\nperformed by the normal compositional model (NCM), where the endmembers for\neach pixel are assumed to be sampled from unimodal Gaussian distributions.\nHowever, in real applications, the distribution of a material is often not\nGaussian. In this paper, we use Gaussian mixture models (GMM) to represent the\nendmember variability. We show, given the GMM starting premise, that the\ndistribution of the mixed pixel (under the linear mixing model) is also a GMM\n(and this is shown from two perspectives). The first perspective originates\nfrom the random variable transformation and gives a conditional density\nfunction of the pixels given the abundances and GMM parameters. With proper\nsmoothness and sparsity prior constraints on the abundances, the conditional\ndensity function leads to a standard maximum a posteriori (MAP) problem which\ncan be solved using generalized expectation maximization. The second\nperspective originates from marginalizing over the endmembers in the GMM, which\nprovides us with a foundation to solve for the endmembers at each pixel. Hence,\nour model can not only estimate the abundances and distribution parameters, but\nalso the distinct endmember set for each pixel. We tested the proposed GMM on\nseveral synthetic and real datasets, and showed its potential by comparing it\nto current popular methods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 20:10:00 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 21:01:34 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhou", "Yuan", ""], ["Rangarajan", "Anand", ""], ["Gader", "Paul D.", ""]]}, {"id": "1710.00126", "submitter": "Li Sun Dr", "authors": "Li Sun and Zhi Yan and Sergi Molina Mellado and Marc Hanheide and Tom\n  Duckett", "title": "3DOF Pedestrian Trajectory Prediction Learned from Long-Term Autonomous\n  Mobile Robot Deployment Data", "comments": "7 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel 3DOF pedestrian trajectory prediction approach\nfor autonomous mobile service robots. While most previously reported methods\nare based on learning of 2D positions in monocular camera images, our approach\nuses range-finder sensors to learn and predict 3DOF pose trajectories (i.e. 2D\nposition plus 1D rotation within the world coordinate system). Our approach,\nT-Pose-LSTM (Temporal 3DOF-Pose Long-Short-Term Memory), is trained using\nlong-term data from real-world robot deployments and aims to learn\ncontext-dependent (environment- and time-specific) human activities. Our\napproach incorporates long-term temporal information (i.e. date and time) with\nshort-term pose observations as input. A sequence-to-sequence LSTM\nencoder-decoder is trained, which encodes observations into LSTM and then\ndecodes as predictions. For deployment, it can perform on-the-fly prediction in\nreal-time. Instead of using manually annotated data, we rely on a robust human\ndetection, tracking and SLAM system, providing us with examples in a global\ncoordinate system. We validate the approach using more than 15K pedestrian\ntrajectories recorded in a care home environment over a period of three months.\nThe experiment shows that the proposed T-Pose-LSTM model advances the\nstate-of-the-art 2D-based method for human trajectory prediction in long-term\nmobile robot deployments.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 00:40:54 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Sun", "Li", ""], ["Yan", "Zhi", ""], ["Mellado", "Sergi Molina", ""], ["Hanheide", "Marc", ""], ["Duckett", "Tom", ""]]}, {"id": "1710.00132", "submitter": "Cheng Zhao", "authors": "Cheng Zhao, Li Sun, Pulak Purkait and Rustam Stolkin", "title": "Dense RGB-D semantic mapping with Pixel-Voxel neural network", "comments": "8 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For intelligent robotics applications, extending 3D mapping to 3D semantic\nmapping enables robots to, not only localize themselves with respect to the\nscene's geometrical features but also simultaneously understand the higher\nlevel meaning of the scene contexts. Most previous methods focus on geometric\n3D reconstruction and scene understanding independently notwithstanding the\nfact that joint estimation can boost the accuracy of the semantic mapping. In\nthis paper, a dense RGB-D semantic mapping system with a Pixel-Voxel network is\nproposed, which can perform dense 3D mapping while simultaneously recognizing\nand semantically labelling each point in the 3D map. The proposed Pixel-Voxel\nnetwork obtains global context information by using PixelNet to exploit the RGB\nimage and meanwhile, preserves accurate local shape information by using\nVoxelNet to exploit the corresponding 3D point cloud. Unlike the existing\narchitecture that fuses score maps from different models with equal weights, we\nproposed a Softmax weighted fusion stack that adaptively learns the varying\ncontributions of PixelNet and VoxelNet, and fuses the score maps of the two\nmodels according to their respective confidence levels. The proposed\nPixel-Voxel network achieves the state-of-the-art semantic segmentation\nperformance on the SUN RGB-D benchmark dataset. The runtime of the proposed\nsystem can be boosted to 11-12Hz, enabling near to real-time performance using\nan i7 8-cores PC with Titan X GPU.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 01:10:53 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 04:25:21 GMT"}, {"version": "v3", "created": "Wed, 4 Oct 2017 21:21:05 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhao", "Cheng", ""], ["Sun", "Li", ""], ["Purkait", "Pulak", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1710.00166", "submitter": "Tian Lei", "authors": "Lei Tian, Xiaopeng Hong, Guoying Zhao, Chunxiao Fan, Yue Ming, and\n  Matti Pietik\\\"ainen", "title": "PCANet-II: When PCANet Meets the Second Order Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  PCANet, as one noticeable shallow network, employs the histogram\nrepresentation for feature pooling. However, there are three main problems\nabout this kind of pooling method. First, the histogram-based pooling method\nbinarizes the feature maps and leads to inevitable discriminative information\nloss. Second, it is difficult to effectively combine other visual cues into a\ncompact representation, because the simple concatenation of various visual cues\nleads to feature representation inefficiency. Third, the dimensionality of\nhistogram-based output grows exponentially with the number of feature maps\nused. In order to overcome these problems, we propose a novel shallow network\nmodel, named as PCANet-II. Compared with the histogram-based output, the second\norder pooling not only provides more discriminative information by preserving\nboth the magnitude and sign of convolutional responses, but also dramatically\nreduces the size of output features. Thus we combine the second order\nstatistical pooling method with the shallow network, i.e., PCANet. Moreover, it\nis easy to combine other discriminative and robust cues by using the second\norder pooling. So we introduce the binary feature difference encoding scheme\ninto our PCANet-II to further improve robustness. Experiments demonstrate the\neffectiveness and robustness of our proposed PCANet-II method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 09:11:38 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Tian", "Lei", ""], ["Hong", "Xiaopeng", ""], ["Zhao", "Guoying", ""], ["Fan", "Chunxiao", ""], ["Ming", "Yue", ""], ["Pietik\u00e4inen", "Matti", ""]]}, {"id": "1710.00187", "submitter": "Irwandi Hipiny", "authors": "I. Hipiny, H. Ujir, J.L. Minoi, S.F. Samson Juan, M.A. Khairuddin,\n  M.S. Sunar", "title": "Unsupervised Segmentation of Action Segments in Egocentric Videos using\n  Gaze", "comments": "Published in 2017 IEEE International Conference On Signal and Image\n  Processing Applications", "journal-ref": null, "doi": "10.1109/ICSIPA.2017.8120635", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised segmentation of action segments in egocentric videos is a\ndesirable feature in tasks such as activity recognition and content-based video\nretrieval. Reducing the search space into a finite set of action segments\nfacilitates a faster and less noisy matching. However, there exist a\nsubstantial gap in machine understanding of natural temporal cuts during a\ncontinuous human activity. This work reports on a novel gaze-based approach for\nsegmenting action segments in videos captured using an egocentric camera. Gaze\nis used to locate the region-of-interest inside a frame. By tracking two simple\nmotion-based parameters inside successive regions-of-interest, we discover a\nfinite set of temporal cuts. We present several results using combinations (of\nthe two parameters) on a dataset, i.e., BRISGAZE-ACTIONS. The dataset contains\negocentric videos depicting several daily-living activities. The quality of the\ntemporal cuts is further improved by implementing two entropy measures.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 12:19:41 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 10:46:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Hipiny", "I.", ""], ["Ujir", "H.", ""], ["Minoi", "J. L.", ""], ["Juan", "S. F. Samson", ""], ["Khairuddin", "M. A.", ""], ["Sunar", "M. S.", ""]]}, {"id": "1710.00189", "submitter": "Irwandi Hipiny", "authors": "S. Joseph, H. Ujir and I. Hipiny", "title": "Unsupervised Classification of Intrusive Igneous Rock Thin Section\n  Images using Edge Detection and Colour Analysis", "comments": "Published in 2017 IEEE International Conference On Signal and Image\n  Processing Applications", "journal-ref": null, "doi": "10.1109/ICSIPA.2017.8120669", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of rocks is one of the fundamental tasks in a geological\nstudy. The process requires a human expert to examine sampled thin section\nimages under a microscope. In this study, we propose a method that uses\nmicroscope automation, digital image acquisition, edge detection and colour\nanalysis (histogram). We collected 60 digital images from 20 standard thin\nsections using a digital camera mounted on a conventional microscope. Each\nimage is partitioned into a finite number of cells that form a grid structure.\nEdge and colour profile of pixels inside each cell determine its\nclassification. The individual cells then determine the thin section image\nclassification via a majority voting scheme. Our method yielded successful\nresults as high as 90% to 100% precision.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 12:22:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 10:39:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Joseph", "S.", ""], ["Ujir", "H.", ""], ["Hipiny", "I.", ""]]}, {"id": "1710.00194", "submitter": "Sergiy Zhuk", "authors": "Sergiy Zhuk, Tigran Tchrakian, Albert Akhriev, Siyuan Lu, Hendrik\n  Hamann", "title": "Where computer vision can aid physics: dynamic cloud motion forecasting\n  from satellite images", "comments": "published in the proceedings of 2017 IEEE 56th Conference on Decision\n  and Control (CDC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV physics.flu-dyn physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new algorithm for solar energy forecasting from a\nsequence of Cloud Optical Depth (COD) images. The algorithm is based on the\nfollowing simple observation: the dynamics of clouds represented by COD images\nresembles the motion (transport) of a density in a fluid flow. This suggests\nthat, to forecast the motion of COD images, it is sufficient to forecast the\nflow. The latter, in turn, can be accomplished by fitting a parametric model of\nthe fluid flow to the COD images observed in the past. Namely, the learning\nphase of the algorithm is composed of the following steps: (i) given a sequence\nof COD images, the snapshots of the optical flow are estimated from two\nconsecutive COD images; (ii) these snapshots are then assimilated into a\nNavier-Stokes Equation (NSE), i.e. an initial velocity field for NSE is\nselected so that the corresponding NSE' solution is as close as possible to the\noptical flow snapshots. The prediction phase consists of utilizing a linear\ntransport equation, which describes the propagation of COD images in the fluid\nflow predicted by NSE, to estimate the future motion of the COD images. The\nalgorithm has been tested on COD images provided by two geostationary\noperational environmental satellites from NOAA serving the west-hemisphere.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 12:55:13 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Zhuk", "Sergiy", ""], ["Tchrakian", "Tigran", ""], ["Akhriev", "Albert", ""], ["Lu", "Siyuan", ""], ["Hamann", "Hendrik", ""]]}, {"id": "1710.00230", "submitter": "Andrew Wagenmaker", "authors": "Andrew J. Wagenmaker, Brian E. Moore, Raj Rao Nadakuditi", "title": "Robust Surface Reconstruction from Gradients via Adaptive Dictionary\n  Regularization", "comments": "ICIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach to robust surface reconstruction from\nphotometric stereo normal vector maps that is particularly well-suited for\nreconstructing surfaces from noisy gradients. Specifically, we propose an\nadaptive dictionary learning based approach that attempts to simultaneously\nintegrate the gradient fields while sparsely representing the spatial patches\nof the reconstructed surface in an adaptive dictionary domain. We show that our\nformulation learns the underlying structure of the surface, effectively acting\nas an adaptive regularizer that enforces a smoothness constraint on the\nreconstructed surface. Our method is general and may be coupled with many\nexisting approaches in the literature to improve the integrity of the\nreconstructed surfaces. We demonstrate the performance of our method on\nsynthetic data as well as real photometric stereo data and evaluate its\nrobustness to noise.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 17:23:10 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Wagenmaker", "Andrew J.", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1710.00241", "submitter": "Shubhra Aich", "authors": "Shubhra Aich, Anique Josuttes, Ilya Ovsyannikov, Keegan Strueby, Imran\n  Ahmed, Hema Sudhakar Duddu, Curtis Pozniak, Steve Shirtliffe, and Ian\n  Stavness", "title": "DeepWheat: Estimating Phenotypic Traits from Crop Images with Deep\n  Learning", "comments": "WACV 2018 (Code repository:\n  https://github.com/p2irc/deepwheat_WACV-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate estimating emergence and biomass traits from\ncolor images and elevation maps of wheat field plots. We employ a\nstate-of-the-art deconvolutional network for segmentation and convolutional\narchitectures, with residual and Inception-like layers, to estimate traits via\nhigh dimensional nonlinear regression. Evaluation was performed on two\ndifferent species of wheat, grown in field plots for an experimental plant\nbreeding study. Our framework achieves satisfactory performance with mean and\nstandard deviation of absolute difference of 1.05 and 1.40 counts for emergence\nand 1.45 and 2.05 for biomass estimation. Our results for counting wheat plants\nfrom field images are better than the accuracy reported for the similar, but\narguably less difficult, task of counting leaves from indoor images of rosette\nplants. Our results for biomass estimation, even with a very small dataset,\nimprove upon all previously proposed approaches in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 18:31:53 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 19:14:00 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Aich", "Shubhra", ""], ["Josuttes", "Anique", ""], ["Ovsyannikov", "Ilya", ""], ["Strueby", "Keegan", ""], ["Ahmed", "Imran", ""], ["Duddu", "Hema Sudhakar", ""], ["Pozniak", "Curtis", ""], ["Shirtliffe", "Steve", ""], ["Stavness", "Ian", ""]]}, {"id": "1710.00262", "submitter": "Tuan Do", "authors": "Tuan Do and James Pustejovsky", "title": "Fine-grained Event Learning of Human-Object Interaction with LSTM-CRF", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event learning is one of the most important problems in AI. However,\nnotwithstanding significant research efforts, it is still a very complex task,\nespecially when the events involve the interaction of humans or agents with\nother objects, as it requires modeling human kinematics and object movements.\nThis study proposes a methodology for learning complex human-object interaction\n(HOI) events, involving the recording, annotation and classification of event\ninteractions. For annotation, we allow multiple interpretations of a motion\ncapture by slicing over its temporal span, for classification, we use\nLong-Short Term Memory (LSTM) sequential models with Conditional Randon Field\n(CRF) for constraints of outputs. Using a setup involving captures of\nhuman-object interaction as three dimensional inputs, we argue that this\napproach could be used for event types involving complex spatio-temporal\ndynamics.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 21:04:25 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Do", "Tuan", ""], ["Pustejovsky", "James", ""]]}, {"id": "1710.00279", "submitter": "Hui Yang", "authors": "Hui Yang, Jinshan Pan, Qiong Yan, Wenxiu Sun, Jimmy Ren, Yu-Wing Tai", "title": "Image Dehazing using Bilinear Composition Loss Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a bilinear composition loss function to address\nthe problem of image dehazing. Previous methods in image dehazing use a\ntwo-stage approach which first estimate the transmission map followed by clear\nimage estimation. The drawback of a two-stage method is that it tends to boost\nlocal image artifacts such as noise, aliasing and blocking. This is especially\nthe case for heavy haze images captured with a low quality device. Our method\nis based on convolutional neural networks. Unique in our method is the bilinear\ncomposition loss function which directly model the correlations between\ntransmission map, clear image, and atmospheric light. This allows errors to be\nback-propagated to each sub-network concurrently, while maintaining the\ncomposition constraint to avoid overfitting of each sub-network. We evaluate\nthe effectiveness of our proposed method using both synthetic and real world\nexamples. Extensive experiments show that our method outperfoms\nstate-of-the-art methods especially for haze images with severe noise level and\ncompressions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 02:39:11 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Yang", "Hui", ""], ["Pan", "Jinshan", ""], ["Yan", "Qiong", ""], ["Sun", "Wenxiu", ""], ["Ren", "Jimmy", ""], ["Tai", "Yu-Wing", ""]]}, {"id": "1710.00290", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Dimitrios Kanoulas, Luca Muratore, Darwin G. Caldwell,\n  Nikos G. Tsagarakis", "title": "Translating Videos to Commands for Robotic Manipulation with Deep\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method to translate videos to commands for robotic\nmanipulation using Deep Recurrent Neural Networks (RNN). Our framework first\nextracts deep features from the input video frames with a deep Convolutional\nNeural Networks (CNN). Two RNN layers with an encoder-decoder architecture are\nthen used to encode the visual features and sequentially generate the output\nwords as the command. We demonstrate that the translation accuracy can be\nimproved by allowing a smooth transaction between two RNN layers and using the\nstate-of-the-art feature extractor. The experimental results on our new\nchallenging dataset show that our approach outperforms recent methods by a fair\nmargin. Furthermore, we combine the proposed translation module with the vision\nand planning system to let a robot perform various manipulation tasks. Finally,\nwe demonstrate the effectiveness of our framework on a full-size humanoid robot\nWALK-MAN.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 04:19:52 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Nguyen", "Anh", ""], ["Kanoulas", "Dimitrios", ""], ["Muratore", "Luca", ""], ["Caldwell", "Darwin G.", ""], ["Tsagarakis", "Nikos G.", ""]]}, {"id": "1710.00307", "submitter": "Ke Zhang", "authors": "Ke Zhang, Liru Guo, Ce Gao and Zhenbing Zhao", "title": "Pyramidal RoR for Image Classification", "comments": "submit to Cluster Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Residual Networks of Residual Networks (RoR) exhibits excellent\nperformance in the image classification task, but sharply increasing the number\nof feature map channels makes the characteristic information transmission\nincoherent, which losses a certain of information related to classification\nprediction, limiting the classification performance. In this paper, a Pyramidal\nRoR network model is proposed by analysing the performance characteristics of\nRoR and combining with the PyramidNet. Firstly, based on RoR, the Pyramidal RoR\nnetwork model with channels gradually increasing is designed. Secondly, we\nanalysed the effect of different residual block structures on performance, and\nchosen the residual block structure which best favoured the classification\nperformance. Finally, we add an important principle to further optimize\nPyramidal RoR networks, drop-path is used to avoid over-fitting and save\ntraining time. In this paper, image classification experiments were performed\non CIFAR-10/100 and SVHN datasets, and we achieved the current lowest\nclassification error rates were 2.96%, 16.40% and 1.59%, respectively.\nExperiments show that the Pyramidal RoR network optimization method can improve\nthe network performance for different data sets and effectively suppress the\ngradient disappearance problem in DCNN training.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 07:34:17 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Zhang", "Ke", ""], ["Guo", "Liru", ""], ["Gao", "Ce", ""], ["Zhao", "Zhenbing", ""]]}, {"id": "1710.00448", "submitter": "Tuan Do", "authors": "Tuan Do and James Pustejovsky", "title": "Learning event representation: As sparse as possible, but not sparser", "comments": "Qualitative reasoning Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an optimal event representation is essential for event\nclassification in real world contexts. In this paper, we investigate the\napplication of qualitative spatial reasoning (QSR) frameworks for\nclassification of human-object interaction in three dimensional space, in\ncomparison with the use of quantitative feature extraction approaches for the\nsame purpose. In particular, we modify QSRLib, a library that allows\ncomputation of Qualitative Spatial Relations and Calculi, and employ it for\nfeature extraction, before inputting features into our neural network models.\nUsing an experimental setup involving motion captures of human-object\ninteraction as three dimensional inputs, we observe that the use of qualitative\nspatial features significantly improves the performance of our machine learning\nalgorithm against our baseline, while quantitative features of similar kinds\nfail to deliver similar improvement. We also observe that sequential\nrepresentations of QSR features yield the best classification performance. A\nresult of our learning method is a simple approach to the qualitative\nrepresentation of 3D activities as compositions of 2D actions that can be\nvisualized and learned using 2-dimensional QSR.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 01:18:16 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Do", "Tuan", ""], ["Pustejovsky", "James", ""]]}, {"id": "1710.00478", "submitter": "Qiqi Xiao", "authors": "Qiqi Xiao, Hao Luo, Chi Zhang", "title": "Margin Sample Mining Loss: A Deep Learning Based Method for Person\n  Re-identification", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (ReID) is an important task in computer vision.\nRecently, deep learning with a metric learning loss has become a common\nframework for ReID. In this paper, we also propose a new metric learning loss\nwith hard sample mining called margin smaple mining loss (MSML) which can\nachieve better accuracy compared with other metric learning losses, such as\ntriplet loss. In experi- ments, our proposed methods outperforms most of the\nstate-of-the-art algorithms on Market1501, MARS, CUHK03 and CUHK-SYSU.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 04:27:07 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 03:36:22 GMT"}, {"version": "v3", "created": "Sat, 7 Oct 2017 03:21:22 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Xiao", "Qiqi", ""], ["Luo", "Hao", ""], ["Zhang", "Chi", ""]]}, {"id": "1710.00489", "submitter": "Arunkumar Byravan", "authors": "Arunkumar Byravan, Felix Leeb, Franziska Meier and Dieter Fox", "title": "SE3-Pose-Nets: Structured Deep Dynamics Models for Visuomotor Planning\n  and Control", "comments": "8 pages, Initial submission to IEEE International Conference on\n  Robotics and Automation (ICRA) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an approach to deep visuomotor control using\nstructured deep dynamics models. Our deep dynamics model, a variant of\nSE3-Nets, learns a low-dimensional pose embedding for visuomotor control via an\nencoder-decoder structure. Unlike prior work, our dynamics model is structured:\ngiven an input scene, our network explicitly learns to segment salient parts\nand predict their pose-embedding along with their motion modeled as a change in\nthe pose space due to the applied actions. We train our model using a pair of\npoint clouds separated by an action and show that given supervision only in the\nform of point-wise data associations between the frames our network is able to\nlearn a meaningful segmentation of the scene along with consistent poses. We\nfurther show that our model can be used for closed-loop control directly in the\nlearned low-dimensional pose space, where the actions are computed by\nminimizing error in the pose space using gradient-based methods, similar to\ntraditional model-based control. We present results on controlling a Baxter\nrobot from raw depth data in simulation and in the real world and compare\nagainst two baseline deep networks. Our method runs in real-time, achieves good\nprediction of scene dynamics and outperforms the baseline methods on multiple\ncontrol runs. Video results can be found at:\nhttps://rse-lab.cs.washington.edu/se3-structured-deep-ctrl/\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 05:18:12 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Byravan", "Arunkumar", ""], ["Leeb", "Felix", ""], ["Meier", "Franziska", ""], ["Fox", "Dieter", ""]]}, {"id": "1710.00513", "submitter": "Hiroshi Kawasaki", "authors": "Ryo Furukawa, Ryusuke Sagawa, Hiroshi Kawasaki", "title": "Depth estimation using structured light flow -- analysis of projected\n  pattern flow on an object's surface --", "comments": "9 pages, Published at the International Conference on Computer Vision\n  (ICCV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape reconstruction techniques using structured light have been widely\nresearched and developed due to their robustness, high precision, and density.\nBecause the techniques are based on decoding a pattern to find correspondences,\nit implicitly requires that the projected patterns be clearly captured by an\nimage sensor, i.e., to avoid defocus and motion blur of the projected pattern.\nAlthough intensive researches have been conducted for solving defocus blur, few\nresearches for motion blur and only solution is to capture with extremely fast\nshutter speed. In this paper, unlike the previous approaches, we actively\nutilize motion blur, which we refer to as a light flow, to estimate depth.\nAnalysis reveals that minimum two light flows, which are retrieved from two\nprojected patterns on the object, are required for depth estimation. To\nretrieve two light flows at the same time, two sets of parallel line patterns\nare illuminated from two video projectors and the size of motion blur of each\nline is precisely measured. By analyzing the light flows, i.e. lengths of the\nblurs, scene depth information is estimated. In the experiments, 3D shapes of\nfast moving objects, which are inevitably captured with motion blur, are\nsuccessfully reconstructed by our technique.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 07:27:22 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Furukawa", "Ryo", ""], ["Sagawa", "Ryusuke", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1710.00517", "submitter": "Hiroshi Kawasaki", "authors": "Yuki Shiba, Satoshi Ono, Ryo Furukawa, Shinsaku Hiura, Hiroshi\n  Kawasaki", "title": "Temporal shape super-resolution by intra-frame motion encoding using\n  high-fps structured light", "comments": "9 pages, Published at the International Conference on Computer Vision\n  (ICCV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the solutions of depth imaging of moving scene is to project a static\npattern on the object and use just a single image for reconstruction. However,\nif the motion of the object is too fast with respect to the exposure time of\nthe image sensor, patterns on the captured image are blurred and reconstruction\nfails. In this paper, we impose multiple projection patterns into each single\ncaptured image to realize temporal super resolution of the depth image\nsequences. With our method, multiple patterns are projected onto the object\nwith higher fps than possible with a camera. In this case, the observed pattern\nvaries depending on the depth and motion of the object, so we can extract\ntemporal information of the scene from each single image. The decoding process\nis realized using a learning-based approach where no geometric calibration is\nneeded. Experiments confirm the effectiveness of our method where sequential\nshapes are reconstructed from a single image. Both quantitative evaluations and\ncomparisons with recent techniques were also conducted.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 07:52:04 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Shiba", "Yuki", ""], ["Ono", "Satoshi", ""], ["Furukawa", "Ryo", ""], ["Hiura", "Shinsaku", ""], ["Kawasaki", "Hiroshi", ""]]}, {"id": "1710.00568", "submitter": "Francesco Setti Ph.D.", "authors": "Marco Godi, Paolo Rota, Francesco Setti", "title": "Indirect Match Highlights Detection with Deep Convolutional Neural\n  Networks", "comments": "\"Social Signal Processing and Beyond\" workshop, in conjunction with\n  ICIAP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highlights in a sport video are usually referred as actions that stimulate\nexcitement or attract attention of the audience. A big effort is spent in\ndesigning techniques which find automatically highlights, in order to\nautomatize the otherwise manual editing process. Most of the state-of-the-art\napproaches try to solve the problem by training a classifier using the\ninformation extracted on the tv-like framing of players playing on the game\npitch, learning to detect game actions which are labeled by human observers\naccording to their perception of highlight. Obviously, this is a long and\nexpensive work. In this paper, we reverse the paradigm: instead of looking at\nthe gameplay, inferring what could be exciting for the audience, we directly\nanalyze the audience behavior, which we assume is triggered by events happening\nduring the game. We apply deep 3D Convolutional Neural Network (3D-CNN) to\nextract visual features from cropped video recordings of the supporters that\nare attending the event. Outputs of the crops belonging to the same frame are\nthen accumulated to produce a value indicating the Highlight Likelihood (HL)\nwhich is then used to discriminate between positive (i.e. when a highlight\noccurs) and negative samples (i.e. standard play or time-outs). Experimental\nresults on a public dataset of ice-hockey matches demonstrate the effectiveness\nof our method and promote further research in this new exciting direction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 10:14:41 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Godi", "Marco", ""], ["Rota", "Paolo", ""], ["Setti", "Francesco", ""]]}, {"id": "1710.00620", "submitter": "Yuzhen Lu", "authors": "Yuzhen Lu", "title": "Out-of-focus Blur: Image De-blurring", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image de-blurring is important in many cases of imaging a real scene or\nobject by a camera. This project focuses on de-blurring an image distorted by\nan out-of-focus blur through a simulation study. A pseudo-inverse filter is\nfirst explored but it fails because of severe noise amplification. Then\nTikhonov regularization methods are employed, which produce greatly improved\nresults compared to the pseudo-inverse filter. In Tikhonov regularization, the\nchoice of the regularization parameter plays a critical rule in obtaining a\nhigh-quality image, and the regularized solutions possess a semi-convergence\nproperty. The best result, with the relative restoration error of 8.49%, is\nachieved when the prescribed discrepancy principle is used to decide an optimal\nvalue. Furthermore, an iterative method, Conjugated Gradient, is employed for\nimage de-blurring, which is fast in computation and leads to an even better\nresult with the relative restoration error of 8.22%. The number of iteration in\nCG acts as a regularization parameter, and the iterates have a semi-convergence\nproperty as well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 13:08:12 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 03:11:32 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Lu", "Yuzhen", ""]]}, {"id": "1710.00629", "submitter": "Albert Vilamala", "authors": "Albert Vilamala, Kristoffer Hougaard Madsen and Lars Kai Hansen", "title": "Adaptive Smoothing in fMRI Data Processing Neural Networks", "comments": "4 pages, 3 figures, 1 table, IEEE 2017 International Workshop on\n  Pattern Recognition in Neuroimaging (PRNI)", "journal-ref": null, "doi": "10.1109/PRNI.2017.7981499", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging (fMRI) relies on multi-step data\nprocessing pipelines to accurately determine brain activity; among them, the\ncrucial step of spatial smoothing. These pipelines are commonly suboptimal,\ngiven the local optimisation strategy they use, treating each step in\nisolation. With the advent of new tools for deep learning, recent work has\nproposed to turn these pipelines into end-to-end learning networks. This change\nof paradigm offers new avenues to improvement as it allows for a global\noptimisation. The current work aims at benefitting from this paradigm shift by\ndefining a smoothing step as a layer in these networks able to adaptively\nmodulate the degree of smoothing required by each brain volume to better\naccomplish a given data analysis task. The viability is evaluated on real fMRI\ndata where subjects did alternate between left and right finger tapping tasks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 13:29:27 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Vilamala", "Albert", ""], ["Madsen", "Kristoffer Hougaard", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1710.00633", "submitter": "Albert Vilamala", "authors": "Albert Vilamala, Kristoffer H. Madsen and Lars K. Hansen", "title": "Deep Convolutional Neural Networks for Interpretable Analysis of EEG\n  Sleep Stage Scoring", "comments": "8 pages, 1 figure, 2 tables, IEEE 2017 International Workshop on\n  Machine Learning for Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sleep studies are important for diagnosing sleep disorders such as insomnia,\nnarcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw\npolisomnography signals, which is a tedious visual task requiring the workload\nof highly trained professionals. Consequently, research efforts to purse for an\nautomatic stage scoring based on machine learning techniques have been carried\nout over the last years. In this work, we resort to multitaper spectral\nanalysis to create visually interpretable images of sleep patterns from EEG\nsignals as inputs to a deep convolutional network trained to solve visual\nrecognition tasks. As a working example of transfer learning, a system able to\naccurately classify sleep stages in new unseen patients is presented.\nEvaluations in a widely-used publicly available dataset favourably compare to\nstate-of-the-art results, while providing a framework for visual interpretation\nof outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 13:36:29 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Vilamala", "Albert", ""], ["Madsen", "Kristoffer H.", ""], ["Hansen", "Lars K.", ""]]}, {"id": "1710.00672", "submitter": "Joan Duran", "authors": "Joan Duran and Antoni Buades", "title": "Restoration of Pansharpened Images by Conditional Filtering in the PCA\n  Domain", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Letters, vol. 16(3), pp.\n  442-446, 2019", "doi": "10.1109/LGRS.2018.2873654", "report-no": null, "categories": "cs.CV math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pansharpening techniques aim at fusing low-resolution multispectral (MS)\nimages and high-resolution panchromatic (PAN) images to produce high-resolution\nMS images. Despite significant progress in the field, spectral and spatial\ndistortions might still compromise the quality of the results. We introduce a\nrestoration strategy to mitigate artifacts of fused products. After applying\nthe Principal Component Analysis (PCA) transform to a pansharpened image, the\nchromatic components are filtered conditionally to the geometry of PAN. The\nstructural component is then replaced by the locally histogram-matched PAN for\nspatial enhancement. Experimental results illustrate the efficiency of the\nproposed restoration chain.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 14:13:58 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2018 23:22:12 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2018 12:05:02 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Duran", "Joan", ""], ["Buades", "Antoni", ""]]}, {"id": "1710.00755", "submitter": "Eman Hassan", "authors": "Eman T. Hassan, David J. Crandall", "title": "A Study of Cross-domain Generative Models applied to Cartoon Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Generative Adversarial Networks (GANs) to model one particular\nkind of image: frames from TV cartoons. Cartoons are particularly interesting\nbecause their visual appearance emphasizes the important semantic information\nabout a scene while abstracting out the less important details, but each\ncartoon series has a distinctive artistic style that performs this abstraction\nin different ways. We consider a dataset consisting of images from two popular\ntelevision cartoon series, Family Guy and The Simpsons. We examine the ability\nof GANs to generate images from each of these two domains, when trained\nindependently as well as on both domains jointly. We find that generative\nmodels may be capable of finding semantic-level correspondences between these\ntwo image domains despite the unsupervised setting, even when the training data\ndoes not give labeled alignments between them.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 13:59:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Hassan", "Eman T.", ""], ["Crandall", "David J.", ""]]}, {"id": "1710.00756", "submitter": "Jing Liao", "authors": "Mingming He, Jing Liao, Dongdong Chen, Lu Yuan and Pedro V. Sander", "title": "Progressive Color Transfer with Dense Semantic Correspondences", "comments": "Accepted by TOG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for color transfer between images that have\nperceptually similar semantic structures. We aim to achieve a more accurate\ncolor transfer that leverages semantically-meaningful dense correspondence\nbetween images. To accomplish this, our algorithm uses neural representations\nfor matching. Additionally, the color transfer should be spatially variant and\nglobally coherent. Therefore, our algorithm optimizes a local linear model for\ncolor transfer satisfying both local and global constraints. Our proposed\napproach jointly optimizes matching and color transfer, adopting a\ncoarse-to-fine strategy. The proposed method can be successfully extended from\none-to-one to one-to-many color transfer. The latter further addresses the\nproblem of mismatching elements of the input image. We validate our proposed\nmethod by testing it on a large variety of image content.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:25:41 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 18:47:07 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["He", "Mingming", ""], ["Liao", "Jing", ""], ["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Sander", "Pedro V.", ""]]}, {"id": "1710.00814", "submitter": "Jia-Bin Huang", "authors": "Yen-Chen Lin, Ming-Yu Liu, Min Sun, Jia-Bin Huang", "title": "Detecting Adversarial Attacks on Neural Network Policies with Visual\n  Foresight", "comments": "Project page: http://yclin.me/RL_attack_detection/ Code:\n  https://github.com/yenchenlin/rl-attack-detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has shown promising results in learning control\npolicies for complex sequential decision-making tasks. However, these neural\nnetwork-based policies are known to be vulnerable to adversarial examples. This\nvulnerability poses a potentially serious threat to safety-critical systems\nsuch as autonomous vehicles. In this paper, we propose a defense mechanism to\ndefend reinforcement learning agents from adversarial attacks by leveraging an\naction-conditioned frame prediction module. Our core idea is that the\nadversarial examples targeting at a neural network-based policy are not\neffective for the frame prediction model. By comparing the action distribution\nproduced by a policy from processing the current observed frame to the action\ndistribution produced by the same policy from processing the predicted frame\nfrom the action-conditioned frame prediction module, we can detect the presence\nof adversarial examples. Beyond detecting the presence of adversarial examples,\nour method allows the agent to continue performing the task using the predicted\nframe when the agent is under attack. We evaluate the performance of our\nalgorithm using five games in Atari 2600. Our results demonstrate that the\nproposed defense mechanism achieves favorable performance against baseline\nalgorithms in detecting adversarial examples and in earning rewards when the\nagents are under attack.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 17:56:26 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Lin", "Yen-Chen", ""], ["Liu", "Ming-Yu", ""], ["Sun", "Min", ""], ["Huang", "Jia-Bin", ""]]}, {"id": "1710.00870", "submitter": "Hongyang Li", "authors": "Yu Liu and Hongyang Li and Xiaogang Wang", "title": "Rethinking Feature Discrimination and Polymerization for Large-scale\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature matters. How to train a deep network to acquire discriminative\nfeatures across categories and polymerized features within classes has always\nbeen at the core of many computer vision tasks, specially for large-scale\nrecognition systems where test identities are unseen during training and the\nnumber of classes could be at million scale. In this paper, we address this\nproblem based on the simple intuition that the cosine distance of features in\nhigh-dimensional space should be close enough within one class and far away\nacross categories. To this end, we proposed the congenerous cosine (COCO)\nalgorithm to simultaneously optimize the cosine similarity among data. It\ninherits the softmax property to make inter-class features discriminative as\nwell as shares the idea of class centroid in metric learning. Unlike previous\nwork where the center is a temporal, statistical variable within one mini-batch\nduring training, the formulated centroid is responsible for clustering\ninner-class features to enforce them polymerized around the network truncus.\nCOCO is bundled with discriminative training and learned end-to-end with stable\nconvergence. Experiments on five benchmarks have been extensively conducted to\nverify the effectiveness of our approach on both small-scale classification\ntask and large-scale human recognition problem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 19:11:58 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 12:37:40 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Liu", "Yu", ""], ["Li", "Hongyang", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1710.00886", "submitter": "Nima Hatami", "authors": "Nima Hatami, Yann Gavet and Johan Debayle", "title": "Classification of Time-Series Images Using Deep Convolutional Neural\n  Networks", "comments": "The 10th International Conference on Machine Vision (ICMV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) has achieved a great success in image\nrecognition task by automatically learning a hierarchical feature\nrepresentation from raw data. While the majority of Time-Series Classification\n(TSC) literature is focused on 1D signals, this paper uses Recurrence Plots\n(RP) to transform time-series into 2D texture images and then take advantage of\nthe deep CNN classifier. Image representation of time-series introduces\ndifferent feature types that are not available for 1D signals, and therefore\nTSC can be treated as texture image recognition task. CNN model also allows\nlearning different levels of representations together with a classifier,\njointly and automatically. Therefore, using RP and CNN in a unified framework\nis expected to boost the recognition rate of TSC. Experimental results on the\nUCR time-series classification archive demonstrate competitive accuracy of the\nproposed approach, compared not only to the existing deep architectures, but\nalso to the state-of-the art TSC algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 19:59:24 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 13:39:44 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Hatami", "Nima", ""], ["Gavet", "Yann", ""], ["Debayle", "Johan", ""]]}, {"id": "1710.00893", "submitter": "Zhe Zhang", "authors": "Zhe Zhang, Shaoshan Liu, Grace Tsai, Hongbing Hu, Chen-Chi Chu, and\n  Feng Zheng", "title": "PIRVS: An Advanced Visual-Inertial SLAM System with Flexible Sensor\n  Fusion and Hardware Co-Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the PerceptIn Robotics Vision System (PIRVS)\nsystem, a visual-inertial computing hardware with embedded simultaneous\nlocalization and mapping (SLAM) algorithm. The PIRVS hardware is equipped with\na multi-core processor, a global-shutter stereo camera, and an IMU with precise\nhardware synchronization. The PIRVS software features a novel and flexible\nsensor fusion approach to not only tightly integrate visual measurements with\ninertial measurements and also to loosely couple with additional sensor\nmodalities. It runs in real-time on both PC and the PIRVS hardware. We perform\na thorough evaluation of the proposed system using multiple public\nvisual-inertial datasets. Experimental results demonstrate that our system\nreaches comparable accuracy of state-of-the-art visual-inertial algorithms on\nPC, while being more efficient on the PIRVS hardware.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 20:17:54 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Zhang", "Zhe", ""], ["Liu", "Shaoshan", ""], ["Tsai", "Grace", ""], ["Hu", "Hongbing", ""], ["Chu", "Chen-Chi", ""], ["Zheng", "Feng", ""]]}, {"id": "1710.00920", "submitter": "Hai Pham", "authors": "Hai X. Pham, Yuting Wang, Vladimir Pavlovic", "title": "End-to-end Learning for 3D Facial Animation from Raw Waveforms of Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep learning framework for real-time speech-driven 3D facial\nanimation from just raw waveforms. Our deep neural network directly maps an\ninput sequence of speech audio to a series of micro facial action unit\nactivations and head rotations to drive a 3D blendshape face model. In\nparticular, our deep model is able to learn the latent representations of\ntime-varying contextual information and affective states within the speech.\nHence, our model not only activates appropriate facial action units at\ninference to depict different utterance generating actions, in the form of lip\nmovements, but also, without any assumption, automatically estimates emotional\nintensity of the speaker and reproduces her ever-changing affective states by\nadjusting strength of facial unit activations. For example, in a happy speech,\nthe mouth opens wider than normal, while other facial units are relaxed; or in\na surprised state, both eyebrows raise higher. Experiments on a diverse\naudiovisual corpus of different actors across a wide range of emotional states\nshow interesting and promising results of our approach. Being\nspeaker-independent, our generalized model is readily applicable to various\ntasks in human-machine interaction and animation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 21:44:32 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 21:13:47 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Pham", "Hai X.", ""], ["Wang", "Yuting", ""], ["Pavlovic", "Vladimir", ""]]}, {"id": "1710.00925", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Eunji Chong, James M. Rehg", "title": "Fine-Grained Head Pose Estimation Without Keypoints", "comments": "Accepted to Computer Vision and Pattern Recognition Workshops\n  (CVPRW), 2018 IEEE Conference on. IEEE, 2018", "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR) Workshops, 2018, pp. 2074-2083", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the head pose of a person is a crucial problem that has a large\namount of applications such as aiding in gaze estimation, modeling attention,\nfitting 3D models to video and performing face alignment. Traditionally head\npose is computed by estimating some keypoints from the target face and solving\nthe 2D to 3D correspondence problem with a mean human head model. We argue that\nthis is a fragile method because it relies entirely on landmark detection\nperformance, the extraneous head model and an ad-hoc fitting step. We present\nan elegant and robust way to determine pose by training a multi-loss\nconvolutional neural network on 300W-LP, a large synthetically expanded\ndataset, to predict intrinsic Euler angles (yaw, pitch and roll) directly from\nimage intensities through joint binned pose classification and regression. We\npresent empirical tests on common in-the-wild pose benchmark datasets which\nshow state-of-the-art results. Additionally we test our method on a dataset\nusually used for pose estimation using depth and start to close the gap with\nstate-of-the-art depth pose methods. We open-source our training and testing\ncode as well as release our pre-trained models.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 22:01:20 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 23:00:15 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 22:38:07 GMT"}, {"version": "v4", "created": "Fri, 1 Dec 2017 17:25:56 GMT"}, {"version": "v5", "created": "Fri, 13 Apr 2018 18:10:29 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Chong", "Eunji", ""], ["Rehg", "James M.", ""]]}, {"id": "1710.00935", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Ying Nian Wu, Song-Chun Zhu", "title": "Interpretable Convolutional Neural Networks", "comments": "In this version, we release the website of the code. Compared to the\n  previous version, we have corrected all values of location instability in\n  Table 3--6 by dividing the values by sqrt(2), i.e., a=a/sqrt(2). Such\n  revisions do NOT decrease the significance of the superior performance of our\n  method, because we make the same correction to location-instability values of\n  all baselines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to modify traditional convolutional neural\nnetworks (CNNs) into interpretable CNNs, in order to clarify knowledge\nrepresentations in high conv-layers of CNNs. In an interpretable CNN, each\nfilter in a high conv-layer represents a certain object part. We do not need\nany annotations of object parts or textures to supervise the learning process.\nInstead, the interpretable CNN automatically assigns each filter in a high\nconv-layer with an object part during the learning process. Our method can be\napplied to different types of CNNs with different structures. The clear\nknowledge representation in an interpretable CNN can help people understand the\nlogics inside a CNN, i.e., based on which patterns the CNN makes the decision.\nExperiments showed that filters in an interpretable CNN were more semantically\nmeaningful than those in traditional CNNs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 23:00:42 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 15:18:05 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 23:28:23 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 10:25:43 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Zhang", "Quanshi", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1710.00947", "submitter": "Bihan Wen Mr", "authors": "Bihan Wen, Saiprasad Ravishankar, Yoram Bresler", "title": "VIDOSAT: High-dimensional Sparsifying Transform Learning for Online\n  Video Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Techniques exploiting the sparsity of images in a transform domain have been\neffective for various applications in image and video processing. Transform\nlearning methods involve cheap computations and have been demonstrated to\nperform well in applications such as image denoising and medical image\nreconstruction. Recently, we proposed methods for online learning of\nsparsifying transforms from streaming signals, which enjoy good convergence\nguarantees, and involve lower computational costs than online synthesis\ndictionary learning. In this work, we apply online transform learning to video\ndenoising. We present a novel framework for online video denoising based on\nhigh-dimensional sparsifying transform learning for spatio-temporal patches.\nThe patches are constructed either from corresponding 2D patches in successive\nframes or using an online block matching technique. The proposed online video\ndenoising requires little memory, and offers efficient processing. Numerical\nexperiments compare the performance to the proposed video denoising scheme but\nfixing the transform to be 3D DCT, as well as prior schemes such as dictionary\nlearning-based schemes, and the state-of-the-art VBM3D and VBM4D on several\nvideo data sets, demonstrating the promising performance of the proposed\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 00:47:00 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Wen", "Bihan", ""], ["Ravishankar", "Saiprasad", ""], ["Bresler", "Yoram", ""]]}, {"id": "1710.00962", "submitter": "Xing Di", "authors": "Xing Di, Vishwanath A. Sindagi, Vishal M. Patel", "title": "GP-GAN: Gender Preserving GAN for Synthesizing Faces from Landmarks", "comments": "6 pages, 5 figures, this paper is accepted as 2018 24th International\n  Conference on Pattern Recognition (ICPR2018)", "journal-ref": null, "doi": "10.1109/ICPR.2018.8545081", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial landmarks constitute the most compressed representation of faces and\nare known to preserve information such as pose, gender and facial structure\npresent in the faces. Several works exist that attempt to perform high-level\nface-related analysis tasks based on landmarks. In contrast, in this work, an\nattempt is made to tackle the inverse problem of synthesizing faces from their\nrespective landmarks. The primary aim of this work is to demonstrate that\ninformation preserved by landmarks (gender in particular) can be further\naccentuated by leveraging generative models to synthesize corresponding faces.\nThough the problem is particularly challenging due to its ill-posed nature, we\nbelieve that successful synthesis will enable several applications such as\nboosting performance of high-level face related tasks using landmark points and\nperforming dataset augmentation. To this end, a novel face-synthesis method\nknown as Gender Preserving Generative Adversarial Network (GP-GAN) that is\nguided by adversarial loss, perceptual loss and a gender preserving loss is\npresented. Further, we propose a novel generator sub-network UDeNet for GP-GAN\nthat leverages advantages of U-Net and DenseNet architectures. Extensive\nexperiments and comparison with recent methods are performed to verify the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 02:22:21 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 05:21:35 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Di", "Xing", ""], ["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1710.00974", "submitter": "Yujian Li", "authors": "Yujian Li, Ting Zhang, Zhaoying Liu, Haihe Hu", "title": "A concatenating framework of shortcut convolutional neural networks", "comments": "17 pages, 5 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well accepted that convolutional neural networks play an important role\nin learning excellent features for image classification and recognition.\nHowever, in tradition they only allow adjacent layers connected, limiting\nintegration of multi-scale information. To further improve their performance,\nwe present a concatenating framework of shortcut convolutional neural networks.\nThis framework can concatenate multi-scale features by shortcut connections to\nthe fully-connected layer that is directly fed to the output layer. We do a\nlarge number of experiments to investigate performance of the shortcut\nconvolutional neural networks on many benchmark visual datasets for different\ntasks. The datasets include AR, FERET, FaceScrub, CelebA for gender\nclassification, CUReT for texture classification, MNIST for digit recognition,\nand CIFAR-10 for object recognition. Experimental results show that the\nshortcut convolutional neural networks can achieve better results than the\ntraditional ones on these tasks, with more stability in different settings of\npooling schemes, activation functions, optimizations, initializations, kernel\nnumbers and kernel sizes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 03:56:33 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Li", "Yujian", ""], ["Zhang", "Ting", ""], ["Liu", "Zhaoying", ""], ["Hu", "Haihe", ""]]}, {"id": "1710.00977", "submitter": "Naimish Agarwal", "authors": "Naimish Agarwal, Artus Krohn-Grimberghe, Ranjana Vyas", "title": "Facial Key Points Detection using Deep Convolutional Neural Network -\n  NaimishNet", "comments": "7 pages, 21 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial Key Points (FKPs) Detection is an important and challenging problem in\nthe fields of computer vision and machine learning. It involves predicting the\nco-ordinates of the FKPs, e.g. nose tip, center of eyes, etc, for a given face.\nIn this paper, we propose a LeNet adapted Deep CNN model - NaimishNet, to\noperate on facial key points data and compare our model's performance against\nexisting state of the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 04:23:08 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Agarwal", "Naimish", ""], ["Krohn-Grimberghe", "Artus", ""], ["Vyas", "Ranjana", ""]]}, {"id": "1710.00983", "submitter": "Yeong-Jun Cho", "authors": "Yeong-Jun Cho, Su-A Kim, Jae-Han Park, Kyuewang Lee, Kuk-Jin Yoon", "title": "Joint Person Re-identification and Camera Network Topology Inference in\n  Multiple Cameras", "comments": "14 pages, 14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is the task of recognizing or identifying a person\nacross multiple views in multi-camera networks. Although there has been much\nprogress in person re-identification, person re-identification in large-scale\nmulti-camera networks still remains a challenging task because of the large\nspatio-temporal uncertainty and high complexity due to a large number of\ncameras and people. To handle these difficulties, additional information such\nas camera network topology should be provided, which is also difficult to\nautomatically estimate, unfortunately. In this study, we propose a unified\nframework which jointly solves both person re-identification and camera network\ntopology inference problems with minimal prior knowledge about the\nenvironments. The proposed framework takes general multi-camera network\nenvironments into account and can be applied to online person re-identification\nin large-scale multi-camera networks. In addition, to effectively show the\nsuperiority of the proposed framework, we provide a new person\nre-identification dataset with full annotations, named SLP, captured in the\nmulti-camera network consisting of nine non-overlapping cameras. Experimental\nresults using our person re-identification and public datasets show that the\nproposed methods are promising for both person re-identification and camera\ntopology inference tasks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 05:07:11 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Cho", "Yeong-Jun", ""], ["Kim", "Su-A", ""], ["Park", "Jae-Han", ""], ["Lee", "Kyuewang", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1710.01020", "submitter": "Sifei Liu", "authors": "Sifei Liu, Shalini De Mello, Jinwei Gu, Guangyu Zhong, Ming-Hsuan\n  Yang, Jan Kautz", "title": "Learning Affinity via Spatial Propagation Networks", "comments": "A long version of NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose spatial propagation networks for learning the\naffinity matrix for vision tasks. We show that by constructing a row/column\nlinear propagation model, the spatially varying transformation matrix exactly\nconstitutes an affinity matrix that models dense, global pairwise relationships\nof an image. Specifically, we develop a three-way connection for the linear\npropagation model, which (a) formulates a sparse transformation matrix, where\nall elements can be the output from a deep CNN, but (b) results in a dense\naffinity matrix that effectively models any task-specific pairwise similarity\nmatrix. Instead of designing the similarity kernels according to image features\nof two points, we can directly output all the similarities in a purely\ndata-driven manner. The spatial propagation network is a generic framework that\ncan be applied to many affinity-related tasks, including but not limited to\nimage matting, segmentation and colorization, to name a few. Essentially, the\nmodel can learn semantically-aware affinity values for high-level vision tasks\ndue to the powerful learning capability of the deep neural network classifier.\nWe validate the framework on the task of refinement for image segmentation\nboundaries. Experiments on the HELEN face parsing and PASCAL VOC-2012 semantic\nsegmentation tasks show that the spatial propagation network provides a\ngeneral, effective and efficient solution for generating high-quality\nsegmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 08:00:15 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Liu", "Sifei", ""], ["De Mello", "Shalini", ""], ["Gu", "Jinwei", ""], ["Zhong", "Guangyu", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1710.01052", "submitter": "Martin Hahner", "authors": "Martin Hahner and Orestis Varesis and Panagiotis Bountouris", "title": "Simulating Structure-from-Motion", "comments": "This paper was written as part of a group project in the \"3D Vision\"\n  course at ETH Z\\\"urich in Spring semester 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of a Structure-from-Motion (SfM) pipeline from a\nsynthetically generated scene as well as the investigation of the faithfulness\nof diverse reconstructions is the subject of this project. A series of\ndifferent SfM reconstructions are implemented and their camera pose estimations\nare being contrasted with their respective ground truth locations. Finally,\ninjection of ground truth location data into the rendered images in order to\nreduce the estimation error of the camera poses is studied as well.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 09:34:38 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hahner", "Martin", ""], ["Varesis", "Orestis", ""], ["Bountouris", "Panagiotis", ""]]}, {"id": "1710.01073", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Richard Harvey, Barry-John Theobald, and Yuxuan Lan", "title": "Resolution limits on visual speech recognition", "comments": null, "journal-ref": "Helen L. Bear, Richard Harvey, Barry-John Theobald, Yuxuan Lan.\n  Resolution limits on visual speech recognition. International Conference on\n  Image Processing (ICIP). 2014. p1371-1375", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-only speech recognition is dependent upon a number of factors that can\nbe difficult to control, such as: lighting; identity; motion; emotion and\nexpression. But some factors, such as video resolution are controllable, so it\nis surprising that there is not yet a systematic study of the effect of\nresolution on lip-reading. Here we use a new data set, the Rosetta Raven data,\nto train and test recognizers so we can measure the affect of video resolution\non recognition accuracy. We conclude that, contrary to common practice,\nresolution need not be that great for automatic lip-reading. However it is\nhighly unlikely that automatic lip-reading can work reliably when the distance\nbetween the bottom of the lower lip and the top of the upper lip is less than\nfour pixels at rest.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:07:06 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Harvey", "Richard", ""], ["Theobald", "Barry-John", ""], ["Lan", "Yuxuan", ""]]}, {"id": "1710.01079", "submitter": "Andrew Anderson", "authors": "Andrew Anderson and David Gregg", "title": "Optimal DNN Primitive Selection with Partitioned Boolean Quadratic\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) require very large amounts of computation both\nfor training and for inference when deployed in the field. Many different\nalgorithms have been proposed to implement the most computationally expensive\nlayers of DNNs. Further, each of these algorithms has a large number of\nvariants, which offer different trade-offs of parallelism, data locality,\nmemory footprint, and execution time. In addition, specific algorithms operate\nmuch more efficiently on specialized data layouts and formats.\n  We state the problem of optimal primitive selection in the presence of data\nformat transformations, and show that it is NP-hard by demonstrating an\nembedding in the Partitioned Boolean Quadratic Assignment problem (PBQP).\n  We propose an analytic solution via a PBQP solver, and evaluate our approach\nexperimentally by optimizing several popular DNNs using a library of more than\n70 DNN primitives, on an embedded platform and a general purpose platform. We\nshow experimentally that significant gains are possible versus the state of the\nart vendor libraries by using a principled analytic solution to the problem of\nlayout selection in the presence of data format transformations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:25:24 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 10:56:03 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Anderson", "Andrew", ""], ["Gregg", "David", ""]]}, {"id": "1710.01084", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Gari Owen, Richard Harvey, and Barry-John Theobald", "title": "Some observations on computer lip-reading: moving from the dream to the\n  reality", "comments": null, "journal-ref": "Helen L. Bear, Gari Owen, Richard Harvey, and Barry-John Theobald.\n  Some observations on computer lip-reading: moving from the dream to the\n  reality. International Society for Optics and Photonics- Security and\n  defence. 2014. p92530G--92530G", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest for greater computer lip-reading performance there are a number\nof tacit assumptions which are either present in the datasets (high resolution\nfor example) or in the methods (recognition of spoken visual units called\nvisemes for example). Here we review these and other assumptions and show the\nsurprising result that computer lip-reading is not heavily constrained by video\nresolution, pose, lighting and other practical factors. However, the working\nassumption that visemes, which are the visual equivalent of phonemes, are the\nbest unit for recognition does need further examination. We conclude that\nvisemes, which were defined over a century ago, are unlikely to be optimal for\na modern computer lip-reading system.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:33:50 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Owen", "Gari", ""], ["Harvey", "Richard", ""], ["Theobald", "Barry-John", ""]]}, {"id": "1710.01093", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Richard W. Harvey, Barry-John Theobald, and Yuxuan Lan", "title": "Which phoneme-to-viseme maps best improve visual-only computer\n  lip-reading?", "comments": null, "journal-ref": "Helen L. Bear, Richard W. Harvey, Barry-John Theobald, and Yuxuan\n  Lan. Which phoneme-to-viseme maps best improve visual-only computer\n  lip-reading? Advances in Visual Computing 2014. p230-239", "doi": null, "report-no": null, "categories": "cs.CV cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical assumption of all current visual speech recognition systems is\nthat there are visual speech units called visemes which can be mapped to units\nof acoustic speech, the phonemes. Despite there being a number of published\nmaps it is infrequent to see the effectiveness of these tested, particularly on\nvisual-only lip-reading (many works use audio-visual speech). Here we examine\n120 mappings and consider if any are stable across talkers. We show a method\nfor devising maps based on phoneme confusions from an automated lip-reading\nsystem, and we present new mappings that show improvements for individual\ntalkers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 11:44:40 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Harvey", "Richard W.", ""], ["Theobald", "Barry-John", ""], ["Lan", "Yuxuan", ""]]}, {"id": "1710.01103", "submitter": "Pablo Hernandez-Cerdan", "authors": "Pablo Hernandez-Cerdan", "title": "Isotropic and Steerable Wavelets in N Dimensions. A multiresolution\n  analysis framework for ITK", "comments": "Manuscript submitted to InsightJournal (ITK) in 2017:\n  http://hdl.handle.net/10380/3558", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the implementation of the external module\nITKIsotropicWavelets, a multiresolution (MRA) analysis framework using\nisotropic and steerable wavelets in the frequency domain. This framework\nprovides the backbone for state of the art filters for denoising, feature\ndetection or phase analysis in N-dimensions. It focus on reusability, and\nhighly decoupled modules for easy extension and implementation of new filters,\nand it contains a filter for multiresolution phase analysis,\n  The backbone of the multi-scale analysis is provided by an isotropic\nband-limited wavelet pyramid, and the detection of directional features is\nprovided by coupling the pyramid with a generalized Riesz transform. The\ngeneralized Riesz transform of order N behaves like a smoothed version of the\nNth order derivatives of the signal. Also, it is steerable: its components\nimpulse responses can be rotated to any spatial orientation, reducing\ncomputation time when detecting directional features.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 12:11:10 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hernandez-Cerdan", "Pablo", ""]]}, {"id": "1710.01115", "submitter": "Tahsin Reasat", "authors": "Tahsin Reasat, Celia Shahnaz", "title": "Detection of Inferior Myocardial Infarction using Shallow Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/R10-HTC.2017.8289058", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Myocardial Infarction is one of the leading causes of death worldwide. This\npaper presents a Convolutional Neural Network (CNN) architecture which takes\nraw Electrocardiography (ECG) signal from lead II, III and AVF and\ndifferentiates between inferior myocardial infarction (IMI) and healthy\nsignals. The performance of the model is evaluated on IMI and healthy signals\nobtained from Physikalisch-Technische Bundesanstalt (PTB) database. A\nsubject-oriented approach is taken to comprehend the generalization capability\nof the model and compared with the current state of the art. In a\nsubject-oriented approach, the network is tested on one patient and trained on\nrest of the patients. Our model achieved a superior metrics scores (accuracy=\n84.54%, sensitivity= 85.33% and specificity= 84.09%) when compared to the\nbenchmark. We also analyzed the discriminating strength of the features\nextracted by the convolutional layers by means of geometric separability index\nand euclidean distance and compared it with the benchmark model.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 12:56:49 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 13:30:22 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 08:51:11 GMT"}, {"version": "v4", "created": "Sat, 18 Aug 2018 04:59:13 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Reasat", "Tahsin", ""], ["Shahnaz", "Celia", ""]]}, {"id": "1710.01122", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Stephen J. Cox, Richard W. Harvey", "title": "Speaker-independent machine lip-reading with speaker-dependent viseme\n  classifiers", "comments": null, "journal-ref": "Helen L. Bear, Stephen J. Cox, Richard W. Harvey,\n  Speaker-independent machine lip-reading with speaker-dependent viseme\n  classifiers. Audio-Visual Speech Processing (AVSP) 2015, p190-195", "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine lip-reading, which is identification of speech from visual-only\ninformation, there is evidence to show that visual speech is highly dependent\nupon the speaker [1]. Here, we use a phoneme-clustering method to form new\nphoneme-to-viseme maps for both individual and multiple speakers. We use these\nmaps to examine how similarly speakers talk visually. We conclude that broadly\nspeaking, speakers have the same repertoire of mouth gestures, where they\ndiffer is in the use of the gestures.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:02:41 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Cox", "Stephen J.", ""], ["Harvey", "Richard W.", ""]]}, {"id": "1710.01142", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Richard W. Harvey, Yuxuan Lan", "title": "Finding phonemes: improving machine lip-reading", "comments": null, "journal-ref": "Helen L. Bear, Richard W. Harvey, Yuxuan Lan. Finding phonemes:\n  improving machine lip-reading. Audio-Visual Speech Processing (AVSP), 2015\n  p115-120", "doi": null, "report-no": null, "categories": "cs.CV cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine lip-reading there is continued debate and research around the\ncorrect classes to be used for recognition. In this paper we use a structured\napproach for devising speaker-dependent viseme classes, which enables the\ncreation of a set of phoneme-to-viseme maps where each has a different quantity\nof visemes ranging from two to 45. Viseme classes are based upon the mapping of\narticulated phonemes, which have been confused during phoneme recognition, into\nviseme groups. Using these maps, with the LiLIR dataset, we show the effect of\nchanging the viseme map size in speaker-dependent machine lip-reading, measured\nby word recognition correctness and so demonstrate that word recognition with\nphoneme classifiers is not just possible, but often better than word\nrecognition with viseme classifiers. Furthermore, there are intermediate units\nbetween visemes and phonemes which are better still.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 13:32:40 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Harvey", "Richard W.", ""], ["Lan", "Yuxuan", ""]]}, {"id": "1710.01168", "submitter": "Yuxin Peng", "authors": "Xiangteng He, Yuxin Peng and Junjie Zhao", "title": "Fast Fine-grained Image Classification via Weakly Supervised\n  Discriminative Localization", "comments": "13pages, submitted to IEEE Transactions on Circuits and Systems for\n  Video Technology. arXiv admin note: text overlap with arXiv:1709.08295", "journal-ref": null, "doi": "10.1109/TCSVT.2018.2834480", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image classification is to recognize hundreds of subcategories\nin each basic-level category. Existing methods employ discriminative\nlocalization to find the key distinctions among subcategories. However, they\ngenerally have two limitations: (1) Discriminative localization relies on\nregion proposal methods to hypothesize the locations of discriminative regions,\nwhich are time-consuming. (2) The training of discriminative localization\ndepends on object or part annotations, which are heavily labor-consuming. It is\nhighly challenging to address the two key limitations simultaneously, and\nexisting methods only focus on one of them. Therefore, we propose a weakly\nsupervised discriminative localization approach (WSDL) for fast fine-grained\nimage classification to address the two limitations at the same time, and its\nmain advantages are: (1) n-pathway end-to-end discriminative localization\nnetwork is designed to improve classification speed, which simultaneously\nlocalizes multiple different discriminative regions for one image to boost\nclassification accuracy, and shares full-image convolutional features generated\nby region proposal network to accelerate the process of generating region\nproposals as well as reduce the computation of convolutional operation. (2)\nMulti-level attention guided localization learning is proposed to localize\ndiscriminative regions with different focuses automatically, without using\nobject and part annotations, avoiding the labor consumption. Different level\nattentions focus on different characteristics of the image, which are\ncomplementary and boost the classification accuracy. Both are jointly employed\nto simultaneously improve classification speed and eliminate dependence on\nobject and part annotations. Compared with state-of-the-art methods on 2\nwidely-used fine-grained image classification datasets, our WSDL approach\nachieves the best performance.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 17:24:21 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["He", "Xiangteng", ""], ["Peng", "Yuxin", ""], ["Zhao", "Junjie", ""]]}, {"id": "1710.01169", "submitter": "Helen L Bear", "authors": "Helen L. Bear, Richard Harvey", "title": "Decoding visemes: improving machine lipreading", "comments": null, "journal-ref": "Helen L Bear and Richard Harvey. Decoding visemes: improving\n  machine lipreading. IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP), 2016. p2009-2013", "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To undertake machine lip-reading, we try to recognise speech from a visual\nsignal. Current work often uses viseme classification supported by language\nmodels with varying degrees of success. A few recent works suggest phoneme\nclassification, in the right circumstances, can outperform viseme\nclassification. In this work we present a novel two-pass method of training\nphoneme classifiers which uses previously trained visemes in the first pass.\nWith our new training algorithm, we show classification performance which\nsignificantly improves on previous lip-reading results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 14:01:32 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L.", ""], ["Harvey", "Richard", ""]]}, {"id": "1710.01202", "submitter": "F Yan", "authors": "Fei Yan, Krystian Mikolajczyk, Josef Kittler", "title": "Person Re-Identification with Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach to person re-identification using\nimages and natural language descriptions. We propose a joint vision and\nlanguage model based on CCA and CNN architectures to match across the two\nmodalities as well as to enrich visual examples for which there are no language\ndescriptions. We also introduce new annotations in the form of natural language\ndescriptions for two standard Re-ID benchmarks, namely CUHK03 and VIPeR. We\nperform experiments on these two datasets with techniques based on CNN,\nhand-crafted features as well as LSTM for analysing visual and natural\ndescription data. We investigate and demonstrate the advantages of using\nnatural language descriptions compared to attributes as well as CNN compared to\nLSTM in the context of Re-ID. We show that the joint use of language and vision\ncan significantly improve the state-of-the-art performance on standard Re-ID\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 15:05:31 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Yan", "Fei", ""], ["Mikolajczyk", "Krystian", ""], ["Kittler", "Josef", ""]]}, {"id": "1710.01214", "submitter": "Daniel Berio", "authors": "Daniel Berio, Memo Akten, Frederic Fol Leymarie, Mick Grierson,\n  R\\'ejean Plamondon", "title": "Calligraphic Stylisation Learning with a Physiologically Plausible Model\n  of Movement and Recurrent Neural Networks", "comments": "8 Pages, Accepted for publication at MOCO '17, 4th International\n  Conference on Movement Computing 28-30 June 2017, London, United Kingdom", "journal-ref": null, "doi": "10.1145/3077981.3078049", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computational framework to learn stylisation patterns from\nexample drawings or writings, and then generate new trajectories that possess\nsimilar stylistic qualities. We particularly focus on the generation and\nstylisation of trajectories that are similar to the ones that can be seen in\ncalligraphy and graffiti art. Our system is able to extract and learn dynamic\nand visual qualities from a small number of user defined examples which can be\nrecorded with a digitiser device, such as a tablet, mouse or motion capture\nsensors. Our system is then able to transform new user drawn traces to be\nkinematically and stylistically similar to the training examples. We implement\nthe system using a Recurrent Mixture Density Network (RMDN) combined with a\nrepresentation given by the parameters of the Sigma Lognormal model, a\nphysiologically plausible model of movement that has been shown to closely\nreproduce the velocity and trace of human handwriting gestures.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 19:53:40 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Berio", "Daniel", ""], ["Akten", "Memo", ""], ["Leymarie", "Frederic Fol", ""], ["Grierson", "Mick", ""], ["Plamondon", "R\u00e9jean", ""]]}, {"id": "1710.01215", "submitter": "Nasim Nematzadeh", "authors": "Nasim Nematzadeh, David M.W. Powers", "title": "The Cafe Wall Illusion: Local and Global Perception from multiple scale\n  to multiscale", "comments": "Under revision by Applied Computational Intelligence and Soft\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Geometrical illusions are a subclass of optical illusions in which the\ngeometrical characteristics of patterns such as orientations and angles are\ndistorted and misperceived as the result of low- to high-level retinal/cortical\nprocessing. Modelling the detection of tilt in these illusions and their\nstrengths as they are perceived is a challenging task computationally and leads\nto development of techniques that match with human performance. In this study,\nwe present a predictive and quantitative approach for modeling foveal and\nperipheral vision in the induced tilt in Caf\\'e Wall illusion in which parallel\nmortar lines between shifted rows of black and white tiles appear to converge\nand diverge. A bioderived filtering model for the responses of retinal/cortical\nsimple cells to the stimulus using Difference of Gaussians is utilized with an\nanalytic processing pipeline introduced in our previous studies to quantify the\nangle of tilt in the model. Here we have considered visual characteristics of\nfoveal and peripheral vision in the perceived tilt in the pattern to predict\ndifferent degrees of tilt in different areas of the fovea and periphery as the\neye saccades to different parts of the image. The tilt analysis results from\nseveral sampling sizes and aspect ratios, modelling variant foveal views are\nused from our previous investigations on the local tilt, and we specifically\ninvestigate in this work, different configurations of the whole pattern\nmodelling variant Gestalt views across multiple scales in order to provide\nconfidence intervals around the predicted tilts. The foveal sample sets are\nverified and quantified using two different sampling methods. We present here a\nprecise and quantified comparison contrasting local tilt detection in the\nfoveal sets with a global average across all of the Caf\\'e Wall configurations\ntested in this work.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 10:12:06 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Nematzadeh", "Nasim", ""], ["Powers", "David M. W.", ""]]}, {"id": "1710.01216", "submitter": "Bhanu Pratap Singh Rawat", "authors": "Saqib Shamsi, Bhanu Pratap Singh Rawat, Manya Wadhwa", "title": "Group Affect Prediction Using Multimodal Distributions", "comments": "This research paper has been accepted at Workshop on Computer Vision\n  for Active and Assisted Living, WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our approach towards building an efficient predictive model to\ndetect emotions for a group of people in an image. We have proposed that\ntraining a Convolutional Neural Network (CNN) model on the emotion heatmaps\nextracted from the image, outperforms a CNN model trained entirely on the raw\nimages. The comparison of the models have been done on a recently published\ndataset of Emotion Recognition in the Wild (EmotiW) challenge, 2017. The\nproposed method achieved validation accuracy of 55.23% which is 2.44% above the\nbaseline accuracy, provided by the EmotiW organizers.\n", "versions": [{"version": "v1", "created": "Sun, 17 Sep 2017 19:04:39 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 16:14:36 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Shamsi", "Saqib", ""], ["Rawat", "Bhanu Pratap Singh", ""], ["Wadhwa", "Manya", ""]]}, {"id": "1710.01217", "submitter": "Varun Arvind", "authors": "Varun Arvind, Anthony Costa, Marcus Badgeley, Samuel Cho, Eric Oermann", "title": "Wide and deep volumetric residual networks for volumetric image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  3D shape models that directly classify objects from 3D information have\nbecome more widely implementable. Current state of the art models rely on deep\nconvolutional and inception models that are resource intensive. Residual neural\nnetworks have been demonstrated to be easier to optimize and do not suffer from\nvanishing/exploding gradients observed in deep networks. Here we implement a\nresidual neural network for 3D object classification of the 3D Princeton\nModelNet dataset. Further, we show that widening network layers dramatically\nimproves accuracy in shallow residual nets, and residual neural networks\nperform comparable to state-of-the-art 3D shape net models, and we show that\nwidening network layers improves classification accuracy. We provide extensive\ntraining and architecture parameters providing a better understanding of\navailable network architectures for use in 3D object classification.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 04:30:13 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Arvind", "Varun", ""], ["Costa", "Anthony", ""], ["Badgeley", "Marcus", ""], ["Cho", "Samuel", ""], ["Oermann", "Eric", ""]]}, {"id": "1710.01218", "submitter": "Tianyi Li", "authors": "Mai Xu, Tianyi Li, Zulin Wang, Xin Deng, Ren Yang and Zhenyu Guan", "title": "Reducing Complexity of HEVC: A Deep Learning Approach", "comments": "17 pages, with 12 figures and 7 tables", "journal-ref": "Published in IEEE Transactions on Image Processing, Oct. 2018", "doi": "10.1109/TIP.2018.2847035", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High Efficiency Video Coding (HEVC) significantly reduces bit-rates over the\nproceeding H.264 standard but at the expense of extremely high encoding\ncomplexity. In HEVC, the quad-tree partition of coding unit (CU) consumes a\nlarge proportion of the HEVC encoding complexity, due to the bruteforce search\nfor rate-distortion optimization (RDO). Therefore, this paper proposes a deep\nlearning approach to predict the CU partition for reducing the HEVC complexity\nat both intra- and inter-modes, which is based on convolutional neural network\n(CNN) and long- and short-term memory (LSTM) network. First, we establish a\nlarge-scale database including substantial CU partition data for HEVC intra-\nand inter-modes. This enables deep learning on the CU partition. Second, we\nrepresent the CU partition of an entire coding tree unit (CTU) in the form of a\nhierarchical CU partition map (HCPM). Then, we propose an early-terminated\nhierarchical CNN (ETH-CNN) for learning to predict the HCPM. Consequently, the\nencoding complexity of intra-mode HEVC can be drastically reduced by replacing\nthe brute-force search with ETH-CNN to decide the CU partition. Third, an\nearly-terminated hierarchical LSTM (ETH-LSTM) is proposed to learn the temporal\ncorrelation of the CU partition. Then, we combine ETH-LSTM and ETH-CNN to\npredict the CU partition for reducing the HEVC complexity for inter-mode.\nFinally, experimental results show that our approach outperforms other\nstate-of-the-art approaches in reducing the HEVC complexity at both intra- and\ninter-modes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 02:02:00 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 07:48:00 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 11:13:05 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Xu", "Mai", ""], ["Li", "Tianyi", ""], ["Wang", "Zulin", ""], ["Deng", "Xin", ""], ["Yang", "Ren", ""], ["Guan", "Zhenyu", ""]]}, {"id": "1710.01244", "submitter": "Xuemei Xie", "authors": "Xuemei Xie, Yuxiang Wang, Guangming Shi, Chenye Wang, Jiang Du, and\n  Zhifu Zhao", "title": "Adaptive Measurement Network for CS Image Reconstruction", "comments": "11pages,8figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional compressive sensing (CS) reconstruction is very slow for its\ncharacteristic of solving an optimization problem. Convolu- tional neural\nnetwork can realize fast processing while achieving compa- rable results. While\nCS image recovery with high quality not only de- pends on good reconstruction\nalgorithms, but also good measurements. In this paper, we propose an adaptive\nmeasurement network in which measurement is obtained by learning. The new\nnetwork consists of a fully-connected layer and ReconNet. The fully-connected\nlayer which has low-dimension output acts as measurement. We train the\nfully-connected layer and ReconNet simultaneously and obtain adaptive\nmeasurement. Because the adaptive measurement fits dataset better, in contrast\nwith random Gaussian measurement matrix, under the same measuremen- t rate, it\ncan extract the information of scene more efficiently and get better\nreconstruction results. Experiments show that the new network outperforms the\noriginal one.\n", "versions": [{"version": "v1", "created": "Sat, 23 Sep 2017 15:11:19 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Xie", "Xuemei", ""], ["Wang", "Yuxiang", ""], ["Shi", "Guangming", ""], ["Wang", "Chenye", ""], ["Du", "Jiang", ""], ["Zhao", "Zhifu", ""]]}, {"id": "1710.01245", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Robust non-local means filter for ultrasound image denoising", "comments": "6 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to non-local means image denoising.\nInstead of using all pixels located in the search window for estimating the\nvalue of a pixel, we identify the highly corrupted pixels and assign less\nweight to these pixels. This method is called robust non-local means. Numerical\nand subjective evaluations using ultrasound images show good performances of\nthe proposed denoising method in recovering the shape of edges and important\ndetailed components, in comparison to traditional ultrasound image denoising\nmethods\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:36:10 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1710.01247", "submitter": "Hamid Tizhoosh", "authors": "Aditya Sriram, Shivam Kalra, H.R. Tizhoosh, Shahryar Rahnamayan", "title": "Learning Autoencoded Radon Projections", "comments": "To appear in proceedings of The IEEE Symposium Series on\n  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27\n  -- Dec 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoencoders have been recently used for encoding medical images. In this\nstudy, we design and validate a new framework for retrieving medical images by\nclassifying Radon projections, compressed in the deepest layer of an\nautoencoder. As the autoencoder reduces the dimensionality, a multilayer\nperceptron (MLP) can be employed to classify the images. The integration of MLP\npromotes a rather shallow learning architecture which makes the training\nfaster. We conducted a comparative study to examine the capabilities of\nautoencoders for different inputs such as raw images, Histogram of Oriented\nGradients (HOG) and normalized Radon projections. Our framework is benchmarked\non IRMA dataset containing $14,410$ x-ray images distributed across $57$\ndifferent classes. Experiments show an IRMA error of $313$ (equivalent to\n$\\approx 82\\%$ accuracy) outperforming state-of-the-art works on retrieval from\nIRMA dataset using autoencoders.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 22:35:59 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Sriram", "Aditya", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""], ["Rahnamayan", "Shahryar", ""]]}, {"id": "1710.01248", "submitter": "Hamid Tizhoosh", "authors": "Bill S. Lin, Kevin Michael, Shivam Kalra, H.R. Tizhoosh", "title": "Skin Lesion Segmentation: U-Nets versus Clustering", "comments": "To appear in proceedings of The IEEE Symposium Series on\n  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27\n  -- Dec 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automatic skin lesion diagnosis systems use segmentation as a\npreprocessing step to diagnose skin conditions because skin lesion shape,\nborder irregularity, and size can influence the likelihood of malignancy. This\npaper presents, examines and compares two different approaches to skin lesion\nsegmentation. The first approach uses U-Nets and introduces a histogram\nequalization based preprocessing step. The second approach is a C-Means\nclustering based approach that is much simpler to implement and faster to\nexecute. The Jaccard Index between the algorithm output and hand segmented\nimages by dermatologists is used to evaluate the proposed algorithms. While\nmany recently proposed deep neural networks to segment skin lesions require a\nsignificant amount of computational power for training (i.e., computer with\nGPUs), the main objective of this paper is to present methods that can be used\nwith only a CPU. This severely limits, for example, the number of training\ninstances that can be presented to the U-Net. Comparing the two proposed\nalgorithms, U-Nets achieved a significantly higher Jaccard Index compared to\nthe clustering approach. Moreover, using the histogram equalization for\npreprocessing step significantly improved the U-Net segmentation results.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 22:46:29 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Lin", "Bill S.", ""], ["Michael", "Kevin", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1710.01249", "submitter": "Hamid Tizhoosh", "authors": "Meghana Dinesh Kumar, Morteza Babaie, Shujin Zhu, Shivam Kalra, and\n  H.R.Tizhoosh", "title": "A Comparative Study of CNN, BoVW and LBP for Classification of\n  Histopathological Images", "comments": "To appear in proceedings of The IEEE Symposium Series on\n  Computational Intelligence (IEEE SSCI 2017), Honolulu, Hawaii, USA, Nov. 27\n  -- Dec 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the progress made in the field of medical imaging, it remains a large\narea of open research, especially due to the variety of imaging modalities and\ndisease-specific characteristics. This paper is a comparative study describing\nthe potential of using local binary patterns (LBP), deep features and the\nbag-of-visual words (BoVW) scheme for the classification of histopathological\nimages. We introduce a new dataset, \\emph{KIMIA Path960}, that contains 960\nhistopathology images belonging to 20 different classes (different tissue\ntypes). We make this dataset publicly available. The small size of the dataset\nand its inter- and intra-class variability makes it ideal for initial\ninvestigations when comparing image descriptors for search and classification\nin complex medical imaging cases like histopathology. We investigate deep\nfeatures, LBP histograms and BoVW to classify the images via leave-one-out\nvalidation. The accuracy of image classification obtained using LBP was 90.62\\%\nwhile the highest accuracy using deep features reached 94.72\\%. The dictionary\napproach (BoVW) achieved 96.50\\%. Deep solutions may be able to deliver higher\naccuracies but they need extensive training with a large number of (balanced)\nimage datasets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 23:02:52 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Kumar", "Meghana Dinesh", ""], ["Babaie", "Morteza", ""], ["Zhu", "Shujin", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1710.01255", "submitter": "Zi-Yu Huang", "authors": "Yu-Neng Chuang and Zi-Yu Huang and Yen-Lung Tsai", "title": "Variational Grid Setting Network", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new neural network architecture for automatic generation of\nmissing characters in a Chinese font set. We call the neural network\narchitecture the Variational Grid Setting Network which is based on the\nvariational autoencoder (VAE) with some tweaks. The neural network model is\nable to generate missing characters relatively large in size ($256 \\times 256$\npixels). Moreover, we show that one can use very few samples for training data\nset, and get a satisfied result.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 11:50:02 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 05:00:26 GMT"}, {"version": "v3", "created": "Thu, 26 Oct 2017 15:36:53 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Chuang", "Yu-Neng", ""], ["Huang", "Zi-Yu", ""], ["Tsai", "Yen-Lung", ""]]}, {"id": "1710.01257", "submitter": "David Freire-Obreg\\'on", "authors": "David Freire-Obreg\\'on, Fabio Narducci, Silvio Barra and Modesto\n  Castrill\\'on-Santana", "title": "Deep learning for source camera identification on mobile devices", "comments": "15 pages single column, 9 figures", "journal-ref": null, "doi": "10.1016/j.patrec.2018.01.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we propose a source camera identification method for\nmobile devices based on deep learning. Recently, convolutional neural networks\n(CNNs) have shown a remarkable performance on several tasks such as image\nrecognition, video analysis or natural language processing. A CNN consists on a\nset of layers where each layer is composed by a set of high pass filters which\nare applied all over the input image. This convolution process provides the\nunique ability to extract features automatically from data and to learn from\nthose features. Our proposal describes a CNN architecture which is able to\ninfer the noise pattern of mobile camera sensors (also known as camera\nfingerprint) with the aim at detecting and identifying not only the mobile\ndevice used to capture an image (with a 98\\% of accuracy), but also from which\nembedded camera the image was captured. More specifically, we provide an\nextensive analysis on the proposed architecture considering different\nconfigurations. The experiment has been carried out using the images captured\nfrom different mobile devices cameras (MICHE-I Dataset was used) and the\nobtained results have proved the robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 11:34:10 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 17:03:52 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Freire-Obreg\u00f3n", "David", ""], ["Narducci", "Fabio", ""], ["Barra", "Silvio", ""], ["Castrill\u00f3n-Santana", "Modesto", ""]]}, {"id": "1710.01260", "submitter": "Aydin Ayanzadeh", "authors": "Safar Irandoust-Pakchin, Aydin Ayanzadeh, Siamak Beikzadeh", "title": "Gaussian Three-Dimensional kernel SVM for Edge Detection Applications", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": "COMCONF01_732", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel and uniform algorithm for edge detection based on\nSVM (support vector machine) with Three-dimensional Gaussian radial basis\nfunction with kernel. Because of disadvantages in traditional edge detection\nsuch as inaccurate edge location, rough edge and careless on detect soft edge.\nThe experimental results indicate how the SVM can detect edge in efficient way.\nThe performance of the proposed algorithm is compared with existing methods,\nincluding Sobel and canny detectors. The results show that this method is\nbetter than classical algorithm such as canny and Sobel detector.\n", "versions": [{"version": "v1", "created": "Sat, 30 Sep 2017 18:50:10 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Irandoust-Pakchin", "Safar", ""], ["Ayanzadeh", "Aydin", ""], ["Beikzadeh", "Siamak", ""]]}, {"id": "1710.01269", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone, Evan Calabrese, Julien Cohen-Adad", "title": "Spinal cord gray matter segmentation using deep dilated convolutions", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": "10.1038/s41598-018-24304-3", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gray matter (GM) tissue changes have been associated with a wide range of\nneurological disorders and was also recently found relevant as a biomarker for\ndisability in amyotrophic lateral sclerosis. The ability to automatically\nsegment the GM is, therefore, an important task for modern studies of the\nspinal cord. In this work, we devise a modern, simple and end-to-end fully\nautomated human spinal cord gray matter segmentation method using Deep\nLearning, that works both on in vivo and ex vivo MRI acquisitions. We evaluate\nour method against six independently developed methods on a GM segmentation\nchallenge and report state-of-the-art results in 8 out of 10 different\nevaluation metrics as well as major network parameter reduction when compared\nto the traditional medical imaging architectures such as U-Nets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Oct 2017 16:25:14 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Perone", "Christian S.", ""], ["Calabrese", "Evan", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1710.01288", "submitter": "Helen L Bear", "authors": "Helen L Bear", "title": "Decoding visemes: improving machine lipreading", "comments": "PhD thesis. Computer Vision and Pattern Recognition (CVPR), Women in\n  Computer Vision (WiCV) Workshop 2017", "journal-ref": "Helen L Bear. Decoding visemes: improving lipreading (PhD thesis).\n  University of East Anglia. July 2016", "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine lipreading (MLR) is speech recognition from visual cues and a niche\nresearch problem in speech processing & computer vision. Current challenges\nfall into two groups: the content of the video, such as rate of speech or; the\nparameters of the video recording e.g, video resolution. We show that HD video\nis not needed to successfully lipread with a computer. The term \"viseme\" is\nused in machine lipreading to represent a visual cue or gesture which\ncorresponds to a subgroup of phonemes where the phonemes are visually\nindistinguishable. A phoneme is the smallest sound one can utter, because there\nare more phonemes per viseme, maps between units show a many-to-one\nrelationship. Many maps have been presented, we compare these and our results\nshow Lee's is best. We propose a new method of speaker-dependent\nphoneme-to-viseme maps and compare these to Lee's. Our results show the\nsensitivity of phoneme clustering and we use our new knowledge to augment a\nconventional MLR system. It has been observed in MLR, that classifiers need\ntraining on test subjects to achieve accuracy. Thus machine lipreading is\nhighly speaker-dependent. Conversely speaker independence is robust\nclassification of non-training speakers. We investigate the dependence of\nphoneme-to-viseme maps between speakers and show there is not a high\nvariability of visemes, but there is high variability in trajectory between\nvisemes of individual speakers with the same ground truth. This implies a\ndependency upon the number of visemes within each set for each individual. We\nshow that prior phoneme-to-viseme maps rarely have enough visemes and the\noptimal size, which varies by speaker, ranges from 11-35. Finally we decode\nfrom visemes back to phonemes and into words. Our novel approach uses the\noptimum range visemes within hierarchical training of phoneme classifiers and\ndemonstrates a significant increase in classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:29:24 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bear", "Helen L", ""]]}, {"id": "1710.01292", "submitter": "Helen L Bear", "authors": "Helen L Bear, Sarah Taylor", "title": "Visual speech recognition: aligning terminologies for better\n  understanding", "comments": null, "journal-ref": "Helen L Bear and Sarah Taylor. Visual speech recognition: aligning\n  terminologies for better understanding. British Machine Vision Conference\n  (BMVC) Deep learning for machine lip reading workshop. 2017", "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are at an exciting time for machine lipreading. Traditional research\nstemmed from the adaptation of audio recognition systems. But now, the computer\nvision community is also participating. This joining of two previously\ndisparate areas with different perspectives on computer lipreading is creating\nopportunities for collaborations, but in doing so the literature is\nexperiencing challenges in knowledge sharing due to multiple uses of terms and\nphrases and the range of methods for scoring results.\n  In particular we highlight three areas with the intention to improve\ncommunication between those researching lipreading; the effects of\ninterchanging between speech reading and lipreading; speaker dependence across\ntrain, validation, and test splits; and the use of accuracy, correctness,\nerrors, and varying units (phonemes, visemes, words, and sentences) to measure\nsystem performance. We make recommendations as to how we can be more\nconsistent.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:45:32 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L", ""], ["Taylor", "Sarah", ""]]}, {"id": "1710.01297", "submitter": "Helen L Bear", "authors": "Helen L Bear", "title": "Visual gesture variability between talkers in continuous visual speech", "comments": null, "journal-ref": "Helen L Bear. Visual gesture variability between talkers in\n  continuous visual speech. British Machine Vision Conference (BMVC) Deep\n  learning for machine lip reading workshop. 2017", "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent adoption of deep learning methods to the field of machine lipreading\nresearch gives us two options to pursue to improve system performance. Either,\nwe develop end-to-end systems holistically or, we experiment to further our\nunderstanding of the visual speech signal. The latter option is more difficult\nbut this knowledge would enable researchers to both improve systems and apply\nthe new knowledge to other domains such as speech therapy. One challenge in\nlipreading systems is the correct labeling of the classifiers. These labels map\nan estimated function between visemes on the lips and the phonemes uttered.\nHere we ask if such maps are speaker-dependent? Prior work investigated\nisolated word recognition from speaker-dependent (SD) visemes, we extend this\nto continuous speech. Benchmarked against SD results, and the isolated words\nperformance, we test with RMAV dataset speakers and observe that with\ncontinuous speech, the trajectory between visemes has a greater negative effect\non the speaker differentiation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:59:43 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L", ""]]}, {"id": "1710.01330", "submitter": "Andy Zeng", "authors": "Andy Zeng, Shuran Song, Kuan-Ting Yu, Elliott Donlon, Francois R.\n  Hogan, Maria Bauza, Daolin Ma, Orion Taylor, Melody Liu, Eudald Romo, Nima\n  Fazeli, Ferran Alet, Nikhil Chavan Dafle, Rachel Holladay, Isabella Morona,\n  Prem Qu Nair, Druck Green, Ian Taylor, Weber Liu, Thomas Funkhouser, Alberto\n  Rodriguez", "title": "Robotic Pick-and-Place of Novel Objects in Clutter with Multi-Affordance\n  Grasping and Cross-Domain Image Matching", "comments": "Project webpage: http://arc.cs.princeton.edu Summary video:\n  https://youtu.be/6fG7zwGfIkI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robotic pick-and-place system that is capable of\ngrasping and recognizing both known and novel objects in cluttered\nenvironments. The key new feature of the system is that it handles a wide range\nof object categories without needing any task-specific training data for novel\nobjects. To achieve this, it first uses a category-agnostic affordance\nprediction algorithm to select and execute among four different grasping\nprimitive behaviors. It then recognizes picked objects with a cross-domain\nimage classification framework that matches observed images to product images.\nSince product images are readily available for a wide range of objects (e.g.,\nfrom the web), the system works out-of-the-box for novel objects without\nrequiring any additional training data. Exhaustive experimental results\ndemonstrate that our multi-affordance grasping achieves high success rates for\na wide variety of objects in clutter, and our recognition algorithm achieves\nhigh accuracy for both known and novel grasped objects. The approach was part\nof the MIT-Princeton Team system that took 1st place in the stowing task at the\n2017 Amazon Robotics Challenge. All code, datasets, and pre-trained models are\navailable online at http://arc.cs.princeton.edu\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 18:16:09 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 18:56:06 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 06:07:22 GMT"}, {"version": "v4", "created": "Sun, 1 Apr 2018 04:19:40 GMT"}, {"version": "v5", "created": "Sat, 30 May 2020 19:54:31 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Yu", "Kuan-Ting", ""], ["Donlon", "Elliott", ""], ["Hogan", "Francois R.", ""], ["Bauza", "Maria", ""], ["Ma", "Daolin", ""], ["Taylor", "Orion", ""], ["Liu", "Melody", ""], ["Romo", "Eudald", ""], ["Fazeli", "Nima", ""], ["Alet", "Ferran", ""], ["Dafle", "Nikhil Chavan", ""], ["Holladay", "Rachel", ""], ["Morona", "Isabella", ""], ["Nair", "Prem Qu", ""], ["Green", "Druck", ""], ["Taylor", "Ian", ""], ["Liu", "Weber", ""], ["Funkhouser", "Thomas", ""], ["Rodriguez", "Alberto", ""]]}, {"id": "1710.01351", "submitter": "Helen L Bear", "authors": "Helen L Bear", "title": "Understanding the visual speech signal", "comments": "Computer Vision and Pattern Recognition (CVPR) Women in Computer\n  Vision (WiCV) workshop. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For machines to lipread, or understand speech from lip movement, they decode\nlip-motions (known as visemes) into the spoken sounds. We investigate the\nvisual speech channel to further our understanding of visemes. This has\napplications beyond machine lipreading; speech therapists, animators, and\npsychologists can benefit from this work. We explain the influence of speaker\nindividuality, and demonstrate how one can use visemes to boost lipreading.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 19:10:46 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Bear", "Helen L", ""]]}, {"id": "1710.01370", "submitter": "Jens Grubert", "authors": "Travis Gesslein, Daniel Scherer and Jens Grubert", "title": "BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner", "comments": "changed template, minor modifications for camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising popularity of Augmented and Virtual Reality, there is a need\nfor representing humans as virtual avatars in various application domains\nranging from remote telepresence, games to medical applications. Besides\nexplicitly modelling 3D avatars, sensing approaches that create person-specific\navatars are becoming popular. However, affordable solutions typically suffer\nfrom a low visual quality and professional solution are often too expensive to\nbe deployed in nonprofit projects.\n  We present an open-source project, BodyDigitizer, which aims at providing\nboth build instructions and configuration software for a high-resolution\nphotogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry\nPI cameras, active LED lighting, a sturdy frame construction and open-source\nconfiguration software. %We demonstrate the applicability of the body scanner\nin a nonprofit Mixed Reality health project. The detailed build instruction and\nsoftware are available at http://www.bodydigitizer.org.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 20:10:10 GMT"}, {"version": "v2", "created": "Sat, 28 Oct 2017 19:28:06 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Gesslein", "Travis", ""], ["Scherer", "Daniel", ""], ["Grubert", "Jens", ""]]}, {"id": "1710.01408", "submitter": "Mohammed Yousefhussien", "authors": "Mohammed Yousefhussien, David J. Kelbe, Emmett J. Ientilucci and Carl\n  Salvaggio", "title": "A Fully Convolutional Network for Semantic Labeling of 3D Point Clouds", "comments": null, "journal-ref": "ISPRS Journal of Photogrammetry and Remote Sensing, 2018", "doi": "10.1016/j.isprsjprs.2018.03.018", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When classifying point clouds, a large amount of time is devoted to the\nprocess of engineering a reliable set of features which are then passed to a\nclassifier of choice. Generally, such features - usually derived from the\n3D-covariance matrix - are computed using the surrounding neighborhood of\npoints. While these features capture local information, the process is usually\ntime-consuming, and requires the application at multiple scales combined with\ncontextual methods in order to adequately describe the diversity of objects\nwithin a scene. In this paper we present a 1D-fully convolutional network that\nconsumes terrain-normalized points directly with the corresponding spectral\ndata,if available, to generate point-wise labeling while implicitly learning\ncontextual features in an end-to-end fashion. Our method uses only the\n3D-coordinates and three corresponding spectral features for each point.\nSpectral features may either be extracted from 2D-georeferenced images, as\nshown here for Light Detection and Ranging (LiDAR) point clouds, or extracted\ndirectly for passive-derived point clouds,i.e. from muliple-view imagery. We\ntrain our network by splitting the data into square regions, and use a pooling\nlayer that respects the permutation-invariance of the input points. Evaluated\nusing the ISPRS 3D Semantic Labeling Contest, our method scored second place\nwith an overall accuracy of 81.6%. We ranked third place with a mean F1-score\nof 63.32%, surpassing the F1-score of the method with highest accuracy by\n1.69%. In addition to labeling 3D-point clouds, we also show that our method\ncan be easily extended to 2D-semantic segmentation tasks, with promising\ninitial results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 22:35:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Yousefhussien", "Mohammed", ""], ["Kelbe", "David J.", ""], ["Ientilucci", "Emmett J.", ""], ["Salvaggio", "Carl", ""]]}, {"id": "1710.01416", "submitter": "Saed Khawaldeh", "authors": "Vu Hoang Minh, Tajwar Abrar Aleef, Usama Pervaiz, Yeman Brhane Hagos,\n  Saed Khawaldeh", "title": "Smoothness-based Edge Detection using Low-SNR Camera for Robot\n  Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the emerging advancement in the branch of autonomous robotics, the ability\nof a robot to efficiently localize and construct maps of its surrounding is\ncrucial. This paper deals with utilizing thermal-infrared cameras, as opposed\nto conventional cameras as the primary sensor to capture images of the robot's\nsurroundings. For localization, the images need to be further processed before\nfeeding them to a navigational system. The main motivation of this paper was to\ndevelop an edge detection methodology capable of utilizing the low-SNR poor\noutput from such a thermal camera and effectively detect smooth edges of the\nsurrounding environment. The enhanced edge detector proposed in this paper\ntakes the raw image from the thermal sensor, denoises the images, applies Canny\nedge detection followed by CSS method. The edges are ranked to remove any noise\nand only edges of the highest rank are kept. Then, the broken edges are linked\nby computing edge metrics and a smooth edge of the surrounding is displayed in\na binary image. Several comparisons are also made in the paper between the\nproposed technique and the existing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 22:48:41 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Minh", "Vu Hoang", ""], ["Aleef", "Tajwar Abrar", ""], ["Pervaiz", "Usama", ""], ["Hagos", "Yeman Brhane", ""], ["Khawaldeh", "Saed", ""]]}, {"id": "1710.01422", "submitter": "Nima Sedaghat Alvar", "authors": "Nima Sedaghat, Ashish Mahabal", "title": "Effective Image Differencing with ConvNets for Real-time Transient\n  Hunting", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/sty613", "report-no": null, "categories": "astro-ph.IM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large sky surveys are increasingly relying on image subtraction pipelines for\nreal-time (and archival) transient detection. In this process one has to\ncontend with varying PSF, small brightness variations in many sources, as well\nas artifacts resulting from saturated stars, and, in general, matching errors.\nVery often the differencing is done with a reference image that is deeper than\nindividual images and the attendant difference in noise characteristics can\nalso lead to artifacts. We present here a deep-learning approach to transient\ndetection that encapsulates all the steps of a traditional image subtraction\npipeline -- image registration, background subtraction, noise removal, psf\nmatching, and subtraction -- into a single real-time convolutional network.\nOnce trained the method works lighteningly fast, and given that it does\nmultiple steps at one go, the advantages for multi-CCD, fast surveys like ZTF\nand LSST are obvious.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 00:02:02 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Sedaghat", "Nima", ""], ["Mahabal", "Ashish", ""]]}, {"id": "1710.01444", "submitter": "Liang Lin", "authors": "Chenglong Li, Liang Lin, Wangmeng Zuo, Jin Tang and Ming-Hsuan Yang", "title": "Visual Tracking via Dynamic Graph Learning", "comments": "Submitted to TPAMI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing visual tracking methods usually localize a target object with a\nbounding box, in which the performance of the foreground object trackers or\ndetectors is often affected by the inclusion of background clutter. To handle\nthis problem, we learn a patch-based graph representation for visual tracking.\nThe tracked object is modeled by with a graph by taking a set of\nnon-overlapping image patches as nodes, in which the weight of each node\nindicates how likely it belongs to the foreground and edges are weighted for\nindicating the appearance compatibility of two neighboring nodes. This graph is\ndynamically learned and applied in object tracking and model updating. During\nthe tracking process, the proposed algorithm performs three main steps in each\nframe. First, the graph is initialized by assigning binary weights of some\nimage patches to indicate the object and background patches according to the\npredicted bounding box. Second, the graph is optimized to refine the patch\nweights by using a novel alternating direction method of multipliers. Third,\nthe object feature representation is updated by imposing the weights of patches\non the extracted image features. The object location is predicted by maximizing\nthe classification score in the structured support vector machine. Extensive\nexperiments show that the proposed tracking algorithm performs well against the\nstate-of-the-art methods on large-scale benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 03:00:49 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 16:42:50 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Li", "Chenglong", ""], ["Lin", "Liang", ""], ["Zuo", "Wangmeng", ""], ["Tang", "Jin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.01453", "submitter": "Liang Lin", "authors": "Dongyu Zhang, Liang Lin, Tianshui Chen, Xian Wu, Wenwei Tan and Ebroul\n  Izquierdo", "title": "Content-Adaptive Sketch Portrait Generation by Decompositional\n  Representation Learning", "comments": "Published in TIP 2017", "journal-ref": null, "doi": "10.1109/TIP.2016.2623485", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch portrait generation benefits a wide range of applications such as\ndigital entertainment and law enforcement. Although plenty of efforts have been\ndedicated to this task, several issues still remain unsolved for generating\nvivid and detail-preserving personal sketch portraits. For example, quite a few\nartifacts may exist in synthesizing hairpins and glasses, and textural details\nmay be lost in the regions of hair or mustache. Moreover, the generalization\nability of current systems is somewhat limited since they usually require\nelaborately collecting a dictionary of examples or carefully tuning\nfeatures/components. In this paper, we present a novel representation learning\nframework that generates an end-to-end photo-sketch mapping through structure\nand texture decomposition. In the training stage, we first decompose the input\nface photo into different components according to their representational\ncontents (i.e., structural and textural parts) by using a pre-trained\nConvolutional Neural Network (CNN). Then, we utilize a Branched Fully\nConvolutional Neural Network (BFCN) for learning structural and textural\nrepresentations, respectively. In addition, we design a Sorted Matching Mean\nSquare Error (SM-MSE) metric to measure texture patterns in the loss function.\nIn the stage of sketch rendering, our approach automatically generates\nstructural and textural representations for the input photo and produces the\nfinal result via a probabilistic fusion scheme. Extensive experiments on\nseveral challenging benchmarks suggest that our approach outperforms\nexample-based synthesis algorithms in terms of both perceptual and objective\nmetrics. In addition, the proposed method also has better generalization\nability across dataset without additional training.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 03:37:39 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Zhang", "Dongyu", ""], ["Lin", "Liang", ""], ["Chen", "Tianshui", ""], ["Wu", "Xian", ""], ["Tan", "Wenwei", ""], ["Izquierdo", "Ebroul", ""]]}, {"id": "1710.01457", "submitter": "Liang Lin", "authors": "Xiaodan Liang and Yunchao Wei and Liang Lin and Yunpeng Chen and\n  Xiaohui Shen and Jianchao Yang and Shuicheng Yan", "title": "Learning to Segment Human by Watching YouTube", "comments": "Very-weakly supervised learning framework. New state-of-the-art\n  performance on the human segmentation task! (Published in T-PAMI 2017)", "journal-ref": "10.1109/TPAMI.2016.2598340", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intuition on human segmentation is that when a human is moving in a video,\nthe video-context (e.g., appearance and motion clues) may potentially infer\nreasonable mask information for the whole human body. Inspired by this, based\non popular deep convolutional neural networks (CNN), we explore a very-weakly\nsupervised learning framework for human segmentation task, where only an\nimperfect human detector is available along with massive weakly-labeled YouTube\nvideos. In our solution, the video-context guided human mask inference and CNN\nbased segmentation network learning iterate to mutually enhance each other\nuntil no further improvement gains. In the first step, each video is decomposed\ninto supervoxels by the unsupervised video segmentation. The superpixels within\nthe supervoxels are then classified as human or non-human by graph optimization\nwith unary energies from the imperfect human detection results and the\npredicted confidence maps by the CNN trained in the previous iteration. In the\nsecond step, the video-context derived human masks are used as direct labels to\ntrain CNN. Extensive experiments on the challenging PASCAL VOC 2012 semantic\nsegmentation benchmark demonstrate that the proposed framework has already\nachieved superior results than all previous weakly-supervised methods with\nobject class or bounding box annotations. In addition, by augmenting with the\nannotated masks from PASCAL VOC 2012, our method reaches a new state-of-the-art\nperformance on the human segmentation task.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 04:16:23 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:07:00 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Liang", "Xiaodan", ""], ["Wei", "Yunchao", ""], ["Lin", "Liang", ""], ["Chen", "Yunpeng", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jianchao", ""], ["Yan", "Shuicheng", ""]]}, {"id": "1710.01462", "submitter": "Junxuan Li", "authors": "Junxuan Li", "title": "Secrets in Computing Optical Flow by Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been widely used over many areas in\ncompute vision. Especially in classification. Recently, FlowNet and several\nworks on opti- cal estimation using CNNs shows the potential ability of CNNs in\ndoing per-pixel regression. We proposed several CNNs network architectures that\ncan estimate optical flow, and fully unveiled the intrinsic different between\nthese structures.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 05:20:41 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Li", "Junxuan", ""]]}, {"id": "1710.01493", "submitter": "Ruben H\\\"uhnerbein", "authors": "Ruben H\\\"uhnerbein, Fabrizio Savarino, Freddie \\r{A}str\\\"om, Christoph\n  Schn\\\"orr", "title": "Image Labeling Based on Graphical Models Using Wasserstein Messages and\n  Geometric Assignment", "comments": null, "journal-ref": null, "doi": "10.1137/17M1150669", "report-no": null, "categories": "cs.LG cs.CV cs.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to Maximum A Posteriori inference based on\ndiscrete graphical models. By utilizing local Wasserstein distances for\ncoupling assignment measures across edges of the underlying graph, a given\ndiscrete objective function is smoothly approximated and restricted to the\nassignment manifold. A corresponding multiplicative update scheme combines in a\nsingle process (i) geometric integration of the resulting Riemannian gradient\nflow and (ii) rounding to integral solutions that represent valid labelings.\nThroughout this process, local marginalization constraints known from the\nestablished LP relaxation are satisfied, whereas the smooth geometric setting\nresults in rapidly converging iterations that can be carried out in parallel\nfor every edge.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 08:00:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 08:54:37 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["H\u00fchnerbein", "Ruben", ""], ["Savarino", "Fabrizio", ""], ["\u00c5str\u00f6m", "Freddie", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1710.01559", "submitter": "Gwenole Quellec", "authors": "Hassan Al Hajj, Mathieu Lamard, Pierre-Henri Conze, B\\'eatrice\n  Cochener, Gwenol\\'e Quellec", "title": "Monitoring tool usage in surgery videos using boosted convolutional and\n  recurrent neural networks", "comments": "Accepted for publication in Medical Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the automatic monitoring of tool usage during a\nsurgery, with potential applications in report generation, surgical training\nand real-time decision support. Two surgeries are considered: cataract surgery,\nthe most common surgical procedure, and cholecystectomy, one of the most common\ndigestive surgeries. Tool usage is monitored in videos recorded either through\na microscope (cataract surgery) or an endoscope (cholecystectomy). Following\nstate-of-the-art video analysis solutions, each frame of the video is analyzed\nby convolutional neural networks (CNNs) whose outputs are fed to recurrent\nneural networks (RNNs) in order to take temporal relationships between events\ninto account. Novelty lies in the way those CNNs and RNNs are trained.\nComputational complexity prevents the end-to-end training of \"CNN+RNN\" systems.\nTherefore, CNNs are usually trained first, independently from the RNNs. This\napproach is clearly suboptimal for surgical tool analysis: many tools are very\nsimilar to one another, but they can generally be differentiated based on past\nevents. CNNs should be trained to extract the most useful visual features in\ncombination with the temporal context. A novel boosting strategy is proposed to\nachieve this goal: the CNN and RNN parts of the system are simultaneously\nenriched by progressively adding weak classifiers (either CNNs or RNNs) trained\nto improve the overall classification accuracy. Experiments were performed in a\ndataset of 50 cataract surgery videos and a dataset of 80 cholecystectomy\nvideos. Very good classification performance are achieved in both datasets:\ntool usage could be labeled with an average area under the ROC curve of $A_z =\n0.9961$ and $A_z = 0.9939$, respectively, in offline mode (using past, present\nand future information), and $A_z = 0.9957$ and $A_z = 0.9936$, respectively,\nin online mode (using past and present information only).\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 11:48:34 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 08:21:51 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Hajj", "Hassan Al", ""], ["Lamard", "Mathieu", ""], ["Conze", "Pierre-Henri", ""], ["Cochener", "B\u00e9atrice", ""], ["Quellec", "Gwenol\u00e9", ""]]}, {"id": "1710.01602", "submitter": "Victor Fragoso", "authors": "Qiaodong Cui, Victor Fragoso, Chris Sweeney and Pradeep Sen", "title": "GraphMatch: Efficient Large-Scale Graph Construction for Structure from\n  Motion", "comments": "Published at IEEE 3DV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GraphMatch, an approximate yet efficient method for building the\nmatching graph for large-scale structure-from-motion (SfM) pipelines. Unlike\nmodern SfM pipelines that use vocabulary (Voc.) trees to quickly build the\nmatching graph and avoid a costly brute-force search of matching image pairs,\nGraphMatch does not require an expensive offline pre-processing phase to\nconstruct a Voc. tree. Instead, GraphMatch leverages two priors that can\npredict which image pairs are likely to match, thereby making the matching\nprocess for SfM much more efficient. The first is a score computed from the\ndistance between the Fisher vectors of any two images. The second prior is\nbased on the graph distance between vertices in the underlying matching graph.\nGraphMatch combines these two priors into an iterative \"sample-and-propagate\"\nscheme similar to the PatchMatch algorithm. Its sampling stage uses Fisher\nsimilarity priors to guide the search for matching image pairs, while its\npropagation stage explores neighbors of matched pairs to find new ones with a\nhigh image similarity score. Our experiments show that GraphMatch finds the\nmost image pairs as compared to competing, approximate methods while at the\nsame time being the most efficient.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 13:45:00 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Cui", "Qiaodong", ""], ["Fragoso", "Victor", ""], ["Sweeney", "Chris", ""], ["Sen", "Pradeep", ""]]}, {"id": "1710.01691", "submitter": "Kun Ho Kim", "authors": "Kun Ho Kim, Oisin Mac Aodha, Pietro Perona", "title": "Context Embedding Networks", "comments": "CVPR 2018 spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low dimensional embeddings that capture the main variations of interest in\ncollections of data are important for many applications. One way to construct\nthese embeddings is to acquire estimates of similarity from the crowd. However,\nsimilarity is a multi-dimensional concept that varies from individual to\nindividual. Existing models for learning embeddings from the crowd typically\nmake simplifying assumptions such as all individuals estimate similarity using\nthe same criteria, the list of criteria is known in advance, or that the crowd\nworkers are not influenced by the data that they see. To overcome these\nlimitations we introduce Context Embedding Networks (CENs). In addition to\nlearning interpretable embeddings from images, CENs also model worker biases\nfor different attributes along with the visual context i.e. the visual\nattributes highlighted by a set of images. Experiments on two noisy crowd\nannotated datasets show that modeling both worker bias and visual context\nresults in more interpretable embeddings compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 18:46:40 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 22:47:50 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 16:32:35 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Kim", "Kun Ho", ""], ["Mac Aodha", "Oisin", ""], ["Perona", "Pietro", ""]]}, {"id": "1710.01692", "submitter": "Dokhyam Hoshen", "authors": "Dokhyam Hoshen, Michael Werman", "title": "IQ of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IQ tests are an accepted method for assessing human intelligence. The tests\nconsist of several parts that must be solved under a time constraint. Of all\nthe tested abilities, pattern recognition has been found to have the highest\ncorrelation with general intelligence. This is primarily because pattern\nrecognition is the ability to find order in a noisy environment, a necessary\nskill for intelligent agents. In this paper, we propose a convolutional neural\nnetwork (CNN) model for solving geometric pattern recognition problems. The CNN\nreceives as input multiple ordered input images and outputs the next image\naccording to the pattern. Our CNN is able to solve problems involving rotation,\nreflection, color, size and shape patterns and score within the top 5% of human\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 11:48:58 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Hoshen", "Dokhyam", ""], ["Werman", "Michael", ""]]}, {"id": "1710.01711", "submitter": "Jonathan Krause", "authors": "Jonathan Krause, Varun Gulshan, Ehsan Rahimy, Peter Karth, Kasumi\n  Widner, Greg S. Corrado, Lily Peng, Dale R. Webster", "title": "Grader variability and the importance of reference standards for\n  evaluating machine learning models for diabetic retinopathy", "comments": null, "journal-ref": "Ophthalmology (2018)", "doi": "10.1016/j.ophtha.2018.01.034", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic retinopathy (DR) and diabetic macular edema are common complications\nof diabetes which can lead to vision loss. The grading of DR is a fairly\ncomplex process that requires the detection of fine features such as\nmicroaneurysms, intraretinal hemorrhages, and intraretinal microvascular\nabnormalities. Because of this, there can be a fair amount of grader\nvariability. There are different methods of obtaining the reference standard\nand resolving disagreements between graders, and while it is usually accepted\nthat adjudication until full consensus will yield the best reference standard,\nthe difference between various methods of resolving disagreements has not been\nexamined extensively. In this study, we examine the variability in different\nmethods of grading, definitions of reference standards, and their effects on\nbuilding deep learning models for the detection of diabetic eye disease. We\nfind that a small set of adjudicated DR grades allows substantial improvements\nin algorithm performance. The resulting algorithm's performance was on par with\nthat of individual U.S. board-certified ophthalmologists and retinal\nspecialists.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 17:29:06 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 23:33:08 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 18:02:16 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Krause", "Jonathan", ""], ["Gulshan", "Varun", ""], ["Rahimy", "Ehsan", ""], ["Karth", "Peter", ""], ["Widner", "Kasumi", ""], ["Corrado", "Greg S.", ""], ["Peng", "Lily", ""], ["Webster", "Dale R.", ""]]}, {"id": "1710.01727", "submitter": "Hamed Haddadi", "authors": "Seyed Ali Osia, Ali Shahin Shamsabadi, Ali Taheri, Kleomenis Katevas,\n  Hamid R. Rabiee, Nicholas D. Lane, Hamed Haddadi", "title": "Privacy-Preserving Deep Inference for Rich User Data on The Cloud", "comments": "arXiv admin note: substantial text overlap with arXiv:1703.02952", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are increasingly being used in a variety of machine\nlearning applications applied to rich user data on the cloud. However, this\napproach introduces a number of privacy and efficiency challenges, as the cloud\noperator can perform secondary inferences on the available data. Recently,\nadvances in edge processing have paved the way for more efficient, and private,\ndata processing at the source for simple tasks and lighter models, though they\nremain a challenge for larger, and more complicated models. In this paper, we\npresent a hybrid approach for breaking down large, complex deep models for\ncooperative, privacy-preserving analytics. We do this by breaking down the\npopular deep architectures and fine-tune them in a particular way. We then\nevaluate the privacy benefits of this approach based on the information exposed\nto the cloud service. We also asses the local inference cost of different\nlayers on a modern handset for mobile applications. Our evaluations show that\nby using certain kind of fine-tuning and embedding techniques and at a small\nprocessing costs, we can greatly reduce the level of information available to\nunintended tasks applied to the data feature on the cloud, and hence achieving\nthe desired tradeoff between privacy and performance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:15:32 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 09:58:59 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 20:26:15 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Osia", "Seyed Ali", ""], ["Shamsabadi", "Ali Shahin", ""], ["Taheri", "Ali", ""], ["Katevas", "Kleomenis", ""], ["Rabiee", "Hamid R.", ""], ["Lane", "Nicholas D.", ""], ["Haddadi", "Hamed", ""]]}, {"id": "1710.01749", "submitter": "Christoph Vogel", "authors": "Audrey Richard, Christoph Vogel, Maros Blaha, Thomas Pock, Konrad\n  Schindler", "title": "Semantic 3D Reconstruction with Finite Element Bases", "comments": null, "journal-ref": "BMVC 2017, 28th British Machine Vision Conference", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for the discretisation of multi-label problems\non arbitrary, continuous domains. Our work bridges the gap between general FEM\ndiscretisations, and labeling problems that arise in a variety of computer\nvision tasks, including for instance those derived from the generalised Potts\nmodel. Starting from the popular formulation of labeling as a convex relaxation\nby functional lifting, we show that FEM discretisation is valid for the most\ngeneral case, where the regulariser is anisotropic and non-metric. While our\nfindings are generic and applicable to different vision problems, we\ndemonstrate their practical implementation in the context of semantic 3D\nreconstruction, where such regularisers have proved particularly beneficial.\nThe proposed FEM approach leads to a smaller memory footprint as well as faster\ncomputation, and it constitutes a very simple way to enable variable, adaptive\nresolution within the same model.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 18:08:01 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Richard", "Audrey", ""], ["Vogel", "Christoph", ""], ["Blaha", "Maros", ""], ["Pock", "Thomas", ""], ["Schindler", "Konrad", ""]]}, {"id": "1710.01758", "submitter": "Jeroen Van Gemert MSc", "authors": "Kirsten Koolstra (1), Jeroen van Gemert (2), Peter B\\\"ornert (1 and\n  3), Andrew Webb (1), and Rob Remis (2) ((1) C. J. Gorter Center for High\n  Field MRI, Department of Radiology, Leiden University Medical Center, The\n  Netherlands. (2) Circuits and Systems Group of the Electrical Engineering,\n  Mathematics and Computer Science faculty of the Delft University of\n  Technology, The Netherlands. (3) Philips Research Hamburg, Germany.)", "title": "Accelerating CS in Parallel Imaging Reconstructions Using an Efficient\n  and Effective Circulant Preconditioner", "comments": "27 pages, 8 figures, 2 tables", "journal-ref": null, "doi": "10.1002/mrm.27371", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Design of a preconditioner for fast and efficient parallel imaging\nand compressed sensing reconstructions. Theory: Parallel imaging and compressed\nsensing reconstructions become time consuming when the problem size or the\nnumber of coils is large, due to the large linear system of equations that has\nto be solved in l_1 and l_2-norm based reconstruction algorithms. Such linear\nsystems can be solved efficiently using effective preconditioning techniques.\nMethods: In this paper we construct such a preconditioner by approximating the\nsystem matrix of the linear system, which comprises the data fidelity and\nincludes total variation and wavelet regularization, by a matrix with the\nassumption that is a block circulant matrix with circulant blocks. Due to its\ncirculant structure, the preconditioner can be constructed quickly and its\ninverse can be evaluated fast using only two fast Fourier transformations. We\ntest the performance of the preconditioner for the conjugate gradient method as\nthe linear solver, integrated into the Split Bregman algorithm. Results: The\ndesigned circulant preconditioner reduces the number of iterations required in\nthe conjugate gradient method by almost a factor of~5. The speed up results in\na total acceleration factor of approximately 2.5 for the entire reconstruction\nalgorithm when implemented in MATLAB, while the initialization time of the\npreconditioner is negligible. Conclusion: The proposed preconditioner reduces\nthe reconstruction time for parallel imaging and compressed sensing in a Split\nBregman implementation and can easily handle large systems since it is\nFourier-based, allowing for efficient computations.\n  Key words: preconditioning; compressed sensing; Split Bregman; parallel\nimaging\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 18:48:20 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Koolstra", "Kirsten", "", "1 and\n  3"], ["van Gemert", "Jeroen", "", "1 and\n  3"], ["B\u00f6rnert", "Peter", "", "1 and\n  3"], ["Webb", "Andrew", ""], ["Remis", "Rob", ""]]}, {"id": "1710.01766", "submitter": "Ke Yan", "authors": "Ke Yan, Xiaosong Wang, Le Lu, and Ronald M. Summers", "title": "DeepLesion: Automated Deep Mining, Categorization and Detection of\n  Significant Radiology Image Findings using Large-Scale Clinical Lesion\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting, harvesting and building large-scale annotated radiological image\ndatasets is a greatly important yet challenging problem. It is also the\nbottleneck to designing more effective data-hungry computing paradigms (e.g.,\ndeep learning) for medical image analysis. Yet, vast amounts of clinical\nannotations (usually associated with disease image findings and marked using\narrows, lines, lesion diameters, segmentation, etc.) have been collected over\nseveral decades and stored in hospitals' Picture Archiving and Communication\nSystems. In this paper, we mine and harvest one major type of clinical\nannotation data - lesion diameters annotated on bookmarked images - to learn an\neffective multi-class lesion detector via unsupervised and supervised deep\nConvolutional Neural Networks (CNN). Our dataset is composed of 33,688\nbookmarked radiology images from 10,825 studies of 4,477 unique patients. For\nevery bookmarked image, a bounding box is created to cover the target lesion\nbased on its measured diameters. We categorize the collection of lesions using\nan unsupervised deep mining scheme to generate clustered pseudo lesion labels.\nNext, we adopt a regional-CNN method to detect lesions of multiple categories,\nregardless of missing annotations (normally only one lesion is annotated,\ndespite the presence of multiple co-existing findings). Our integrated mining,\ncategorization and detection framework is validated with promising empirical\nresults, as a scalable, universal or multi-purpose CAD paradigm built upon\nabundant retrospective medical data. Furthermore, we demonstrate that detection\naccuracy can be significantly improved by incorporating pseudo lesion labels\n(e.g., Liver lesion/tumor, Lung nodule/tumor, Abdomen lesions, Chest lymph node\nand others). This dataset will be made publicly available (under the open\nscience initiative).\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 19:10:38 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 17:49:41 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yan", "Ke", ""], ["Wang", "Xiaosong", ""], ["Lu", "Le", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1710.01820", "submitter": "Bailey Kong", "authors": "Bailey Kong, Charless C. Fowlkes", "title": "Energy-Based Spherical Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore an efficient variant of convolutional sparse coding\nwith unit norm code vectors where reconstruction quality is evaluated using an\ninner product (cosine distance). To use these codes for discriminative\nclassification, we describe a model we term Energy-Based Spherical Sparse\nCoding (EB-SSC) in which the hypothesized class label introduces a learned\nlinear bias into the coding step. We evaluate and visualize performance of\nstacking this encoder to make a deep layered model for image classification.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 22:22:50 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kong", "Bailey", ""], ["Fowlkes", "Charless C.", ""]]}, {"id": "1710.01916", "submitter": "Luiza Mici", "authors": "Luiza Mici, German I. Parisi, Stefan Wermter", "title": "A self-organizing neural network architecture for learning human-object\n  interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The visual recognition of transitive actions comprising human-object\ninteractions is a key component for artificial systems operating in natural\nenvironments. This challenging task requires jointly the recognition of\narticulated body actions as well as the extraction of semantic elements from\nthe scene such as the identity of the manipulated objects. In this paper, we\npresent a self-organizing neural network for the recognition of human-object\ninteractions from RGB-D videos. Our model consists of a hierarchy of\nGrow-When-Required (GWR) networks that learn prototypical representations of\nbody motion patterns and objects, accounting for the development of\naction-object mappings in an unsupervised fashion. We report experimental\nresults on a dataset of daily activities collected for the purpose of this\nstudy as well as on a publicly available benchmark dataset. In line with\nneurophysiological studies, our self-organizing architecture exhibits higher\nneural activation for congruent action-object pairs learned during training\nsessions with respect to synthetically created incongruent ones. We show that\nour unsupervised model shows competitive classification results on the\nbenchmark dataset with respect to strictly supervised approaches.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 08:40:24 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 09:00:41 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Mici", "Luiza", ""], ["Parisi", "German I.", ""], ["Wermter", "Stefan", ""]]}, {"id": "1710.01925", "submitter": "Radu Horaud P", "authors": "Richard T. Marriott, Alexander Paschevich and Radu Horaud", "title": "Plane-extraction from depth-data using a Gaussian mixture regression\n  model", "comments": "11 pages, 2 figures, 1 table", "journal-ref": "Pattern Recognition Letters, 2018, 110, pp 44-50", "doi": "10.1016/j.patrec.2018.03.024", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm for unsupervised extraction of piecewise planar\nmodels from depth-data. Among other applications, such models are a good way of\nenabling autonomous agents (robots, cars, drones, etc.) to effectively perceive\ntheir surroundings and to navigate in three dimensions. We propose to do this\nby fitting the data with a piecewise-linear Gaussian mixture regression model\nwhose components are skewed over planes, making them flat in appearance rather\nthan being ellipsoidal, by embedding an outlier-trimming process that is\nformally incorporated into the proposed expectation-maximization algorithm, and\nby selectively fusing contiguous, coplanar components. Part of our motivation\nis an attempt to estimate more accurate plane-extraction by allowing each model\ncomponent to make use of all available data through probabilistic clustering.\nThe algorithm is thoroughly evaluated against a standard benchmark and is shown\nto rank among the best of the existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 09:09:18 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 09:23:35 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 10:59:34 GMT"}, {"version": "v4", "created": "Fri, 30 Mar 2018 08:42:12 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Marriott", "Richard T.", ""], ["Paschevich", "Alexander", ""], ["Horaud", "Radu", ""]]}, {"id": "1710.01949", "submitter": "Herman Kamper", "authors": "Herman Kamper, Gregory Shakhnarovich, Karen Livescu", "title": "Semantic speech retrieval with a visually grounded model of\n  untranscribed speech", "comments": "10 pages, 3 figures, 5 tables; accepted to the IEEE/ACM Transactions\n  on Audio, Speech and Language Processing", "journal-ref": "IEEE/ACM Transactions on Audio, Speech and Language Processing 27\n  (2019) 89-98", "doi": "10.1109/TASLP.2018.2872106", "report-no": null, "categories": "cs.CL cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in models that can learn from unlabelled speech\npaired with visual context. This setting is relevant for low-resource speech\nprocessing, robotics, and human language acquisition research. Here we study\nhow a visually grounded speech model, trained on images of scenes paired with\nspoken captions, captures aspects of semantics. We use an external image tagger\nto generate soft text labels from images, which serve as targets for a neural\nmodel that maps untranscribed speech to (semantic) keyword labels. We introduce\na newly collected data set of human semantic relevance judgements and an\nassociated task, semantic speech retrieval, where the goal is to search for\nspoken utterances that are semantically relevant to a given text query. Without\nseeing any text, the model trained on parallel speech and images achieves a\nprecision of almost 60% on its top ten semantic retrievals. Compared to a\nsupervised model trained on transcriptions, our model matches human judgements\nbetter by some measures, especially in retrieving non-verbatim semantic\nmatches. We perform an extensive analysis of the model and its resulting\nrepresentations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 10:24:46 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 13:58:31 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Kamper", "Herman", ""], ["Shakhnarovich", "Gregory", ""], ["Livescu", "Karen", ""]]}, {"id": "1710.01992", "submitter": "Wei-Sheng Lai", "authors": "Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang", "title": "Fast and Accurate Image Super-Resolution with Deep Laplacian Pyramid\n  Networks", "comments": "The code and datasets are available at\n  http://vllab.ucmerced.edu/wlai24/LapSRN/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently demonstrated high-quality\nreconstruction for single image super-resolution. However, existing methods\noften require a large number of network parameters and entail heavy\ncomputational loads at runtime for generating high-accuracy super-resolution\nresults. In this paper, we propose the deep Laplacian Pyramid Super-Resolution\nNetwork for fast and accurate image super-resolution. The proposed network\nprogressively reconstructs the sub-band residuals of high-resolution images at\nmultiple pyramid levels. In contrast to existing methods that involve the\nbicubic interpolation for pre-processing (which results in large feature maps),\nthe proposed method directly extracts features from the low-resolution input\nspace and thereby entails low computational loads. We train the proposed\nnetwork with deep supervision using the robust Charbonnier loss functions and\nachieve high-quality image reconstruction. Furthermore, we utilize the\nrecursive layers to share parameters across as well as within pyramid levels,\nand thus drastically reduce the number of parameters. Extensive quantitative\nand qualitative evaluations on benchmark datasets show that the proposed\nalgorithm performs favorably against the state-of-the-art methods in terms of\nrun-time and image quality.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 17:58:55 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 22:40:42 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 19:31:38 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Lai", "Wei-Sheng", ""], ["Huang", "Jia-Bin", ""], ["Ahuja", "Narendra", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.02039", "submitter": "Feng Li", "authors": "Feng Li, Yingjie Yao, Peihua Li, David Zhang, Wangmeng Zuo and\n  Ming-Hsuan Yang", "title": "Integrating Boundary and Center Correlation Filters for Visual Tracking\n  with Aspect Ratio Variation", "comments": "Accepted by ICCV 2017 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aspect ratio variation frequently appears in visual tracking and has a\nsevere influence on performance. Although many correlation filter (CF)-based\ntrackers have also been suggested for scale adaptive tracking, few studies have\nbeen given to handle the aspect ratio variation for CF trackers. In this paper,\nwe make the first attempt to address this issue by introducing a family of 1D\nboundary CFs to localize the left, right, top, and bottom boundaries in videos.\nThis allows us cope with the aspect ratio variation flexibly during tracking.\nSpecifically, we present a novel tracking model to integrate 1D Boundary and 2D\nCenter CFs (IBCCF) where boundary and center filters are enforced by a\nnear-orthogonality regularization term. To optimize our IBCCF model, we develop\nan alternating direction method of multipliers. Experiments on several datasets\nshow that IBCCF can effectively handle aspect ratio variation, and achieves\nstate-of-the-art performance in terms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 14:19:50 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Li", "Feng", ""], ["Yao", "Yingjie", ""], ["Li", "Peihua", ""], ["Zhang", "David", ""], ["Zuo", "Wangmeng", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.02081", "submitter": "Paul Bergmann", "authors": "Paul Bergmann, Rui Wang and Daniel Cremers", "title": "Online Photometric Calibration for Auto Exposure Video for Realtime\n  Visual Odometry and SLAM", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent direct visual odometry and SLAM algorithms have demonstrated\nimpressive levels of precision. However, they require a photometric camera\ncalibration in order to achieve competitive results. Hence, the respective\nalgorithm cannot be directly applied to an off-the-shelf-camera or to a video\nsequence acquired with an unknown camera. In this work we propose a method for\nonline photometric calibration which enables to process auto exposure videos\nwith visual odometry precisions that are on par with those of photometrically\ncalibrated videos. Our algorithm recovers the exposure times of consecutive\nframes, the camera response function, and the attenuation factors of the sensor\nirradiance due to vignetting. Gain robust KLT feature tracks are used to obtain\nscene point correspondences as input to a nonlinear optimization framework. We\nshow that our approach can reliably calibrate arbitrary video sequences by\nevaluating it on datasets for which full photometric ground truth is available.\nWe further show that our calibration can improve the performance of a\nstate-of-the-art direct visual odometry method that works solely on pixel\nintensities, calibrating for photometric parameters in an online fashion in\nrealtime.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 15:49:13 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Bergmann", "Paul", ""], ["Wang", "Rui", ""], ["Cremers", "Daniel", ""]]}, {"id": "1710.02113", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Anatomical Pattern Analysis for decoding visual stimuli in human brains", "comments": "Published in Cognitive Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A universal unanswered question in neuroscience and machine\nlearning is whether computers can decode the patterns of the human brain.\nMulti-Voxels Pattern Analysis (MVPA) is a critical tool for addressing this\nquestion. However, there are two challenges in the previous MVPA methods, which\ninclude decreasing sparsity and noise in the extracted features and increasing\nthe performance of prediction.\n  Methods: In overcoming mentioned challenges, this paper proposes Anatomical\nPattern Analysis (APA) for decoding visual stimuli in the human brain. This\nframework develops a novel anatomical feature extraction method and a new\nimbalance AdaBoost algorithm for binary classification. Further, it utilizes an\nError-Correcting Output Codes (ECOC) method for multiclass prediction. APA can\nautomatically detect active regions for each category of the visual stimuli.\nMoreover, it enables us to combine homogeneous datasets for applying advanced\nclassification.\n  Results and Conclusions: Experimental studies on 4 visual categories (words,\nconsonants, objects and scrambled photos) demonstrate that the proposed\napproach achieves superior performance to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 16:57:21 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1710.02124", "submitter": "Robert Maier", "authors": "Vladislav Golyanik, Kihwan Kim, Robert Maier, Matthias Nie{\\ss}ner,\n  Didier Stricker, Jan Kautz", "title": "Multiframe Scene Flow with Piecewise Rigid Motion", "comments": "International Conference on 3D Vision (3DV), Qingdao, China, October\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel multiframe scene flow approach that jointly optimizes\nthe consistency of the patch appearances and their local rigid motions from\nRGB-D image sequences. In contrast to the competing methods, we take advantage\nof an oversegmentation of the reference frame and robust optimization\ntechniques. We formulate scene flow recovery as a global non-linear least\nsquares problem which is iteratively solved by a damped Gauss-Newton approach.\nAs a result, we obtain a qualitatively new level of accuracy in RGB-D based\nscene flow estimation which can potentially run in real-time. Our method can\nhandle challenging cases with rigid, piecewise rigid, articulated and moderate\nnon-rigid motion, and does not rely on prior knowledge about the types of\nmotions and deformations. Extensive experiments on synthetic and real data show\nthat our method outperforms state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 17:28:44 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Golyanik", "Vladislav", ""], ["Kim", "Kihwan", ""], ["Maier", "Robert", ""], ["Nie\u00dfner", "Matthias", ""], ["Stricker", "Didier", ""], ["Kautz", "Jan", ""]]}, {"id": "1710.02134", "submitter": "Grace Kuo", "authors": "Nick Antipa, Grace Kuo, Reinhard Heckel, Ben Mildenhall, Emrah Bostan,\n  Ren Ng, Laura Waller", "title": "DiffuserCam: Lensless Single-exposure 3D Imaging", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a compact and easy-to-build computational camera for\nsingle-shot 3D imaging. Our lensless system consists solely of a diffuser\nplaced in front of a standard image sensor. Every point within the volumetric\nfield-of-view projects a unique pseudorandom pattern of caustics on the sensor.\nBy using a physical approximation and simple calibration scheme, we solve the\nlarge-scale inverse problem in a computationally efficient way. The caustic\npatterns enable compressed sensing, which exploits sparsity in the sample to\nsolve for more 3D voxels than pixels on the 2D sensor. Our 3D voxel grid is\nchosen to match the experimentally measured two-point optical resolution across\nthe field-of-view, resulting in 100 million voxels being reconstructed from a\nsingle 1.3 megapixel image. However, the effective resolution varies\nsignificantly with scene content. Because this effect is common to a wide range\nof computational cameras, we provide new theory for analyzing resolution in\nsuch systems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 17:48:57 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Antipa", "Nick", ""], ["Kuo", "Grace", ""], ["Heckel", "Reinhard", ""], ["Mildenhall", "Ben", ""], ["Bostan", "Emrah", ""], ["Ng", "Ren", ""], ["Waller", "Laura", ""]]}, {"id": "1710.02139", "submitter": "Jia-Bin Huang", "authors": "Shun Zhang, Jia-Bin Huang, Jongwoo Lim, Yihong Gong, Jinjun Wang,\n  Narendra Ahuja, Ming-Hsuan Yang", "title": "Tracking Persons-of-Interest via Unsupervised Representation Adaptation", "comments": "Project page: http://vllab1.ucmerced.edu/~szhang/FaceTracking/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-face tracking in unconstrained videos is a challenging problem as faces\nof one person often appear drastically different in multiple shots due to\nsignificant variations in scale, pose, expression, illumination, and make-up.\nExisting multi-target tracking methods often use low-level features which are\nnot sufficiently discriminative for identifying faces with such large\nappearance variations. In this paper, we tackle this problem by learning\ndiscriminative, video-specific face representations using convolutional neural\nnetworks (CNNs). Unlike existing CNN-based approaches which are only trained on\nlarge-scale face image datasets offline, we use the contextual constraints to\ngenerate a large number of training samples for a given video, and further\nadapt the pre-trained face CNN to specific videos using discovered training\nsamples. Using these training samples, we optimize the embedding space so that\nthe Euclidean distances correspond to a measure of semantic face similarity via\nminimizing a triplet loss function. With the learned discriminative features,\nwe apply the hierarchical clustering algorithm to link tracklets across\nmultiple shots to generate trajectories. We extensively evaluate the proposed\nalgorithm on two sets of TV sitcoms and YouTube music videos, analyze the\ncontribution of each component, and demonstrate significant performance\nimprovement over existing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 17:58:28 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Zhang", "Shun", ""], ["Huang", "Jia-Bin", ""], ["Lim", "Jongwoo", ""], ["Gong", "Yihong", ""], ["Wang", "Jinjun", ""], ["Ahuja", "Narendra", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.02213", "submitter": "Han Guo", "authors": "Han Guo, Namrata Vaswani", "title": "Video Denoising and Enhancement via Dynamic Video Layering", "comments": "Shorter version with title \"Video Denoising via Online Sparse and\n  Low-rank Matrix Decomposition\" appeared in Statistical Signal Processing\n  Workshop (SSP) 2016", "journal-ref": null, "doi": "10.1109/LSP.2018.2833429", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video denoising refers to the problem of removing \"noise\" from a video\nsequence. Here the term \"noise\" is used in a broad sense to refer to any\ncorruption or outlier or interference that is not the quantity of interest. In\nthis work, we develop a novel approach to video denoising that is based on the\nidea that many noisy or corrupted videos can be split into three parts - the\n\"low-rank layer\", the \"sparse layer\", and a small residual (which is small and\nbounded). We show, using extensive experiments, that our denoising approach\noutperforms the state-of-the-art denoising algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 20:54:56 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Guo", "Han", ""], ["Vaswani", "Namrata", ""]]}, {"id": "1710.02238", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas,\n  Nathan Baker", "title": "How Much Chemistry Does a Deep Neural Network Need to Know to Make\n  Accurate Predictions?", "comments": "In Proceedings of 2018 IEEE Winter Conference on Applications of\n  Computer Vision (WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The meteoric rise of deep learning models in computer vision research, having\nachieved human-level accuracy in image recognition tasks is firm evidence of\nthe impact of representation learning of deep neural networks. In the chemistry\ndomain, recent advances have also led to the development of similar CNN models,\nsuch as Chemception, that is trained to predict chemical properties using\nimages of molecular drawings. In this work, we investigate the effects of\nsystematically removing and adding localized domain-specific information to the\nimage channels of the training data. By augmenting images with only 3\nadditional basic information, and without introducing any architectural\nchanges, we demonstrate that an augmented Chemception (AugChemception)\noutperforms the original model in the prediction of toxicity, activity, and\nsolvation free energy. Then, by altering the information content in the images,\nand examining the resulting model's performance, we also identify two distinct\nlearning patterns in predicting toxicity/activity as compared to solvation free\nenergy. These patterns suggest that Chemception is learning about its tasks in\nthe manner that is consistent with established knowledge. Thus, our work\ndemonstrates that advanced chemical knowledge is not a pre-requisite for deep\nlearning models to accurately predict complex chemical properties.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 23:53:59 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 14:03:12 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""], ["Baker", "Nathan", ""]]}, {"id": "1710.02260", "submitter": "Roopal Nahar", "authors": "Roopal Nahar, Akanksha Baranwal, K.Madhava Krishna", "title": "FPGA based Parallelized Architecture of Efficient Graph based Image\n  Segmentation Algorithm", "comments": "6 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and real time segmentation of color images has a variety of\nimportance in many fields of computer vision such as image compression, medical\nimaging, mapping and autonomous navigation. Being one of the most\ncomputationally expensive operation, it is usually done through software imple-\nmentation using high-performance processors. In robotic systems, however, with\nthe constrained platform dimensions and the need for portability, low power\nconsumption and simultaneously the need for real time image segmentation, we\nenvision hardware parallelism as the way forward to achieve higher\nacceleration. Field-programmable gate arrays (FPGAs) are among the best suited\nfor this task as they provide high computing power in a small physical area.\nThey exceed the computing speed of software based implementations by breaking\nthe paradigm of sequential execution and accomplishing more per clock cycle\noperations by enabling hardware level parallelization at an architectural\nlevel. In this paper, we propose three novel architectures of a well known\nEfficient Graph based Image Segmentation algorithm. These proposed\nimplementations optimizes time and power consumption when compared to software\nimplementations. The hybrid design proposed, has notable furtherance of\nacceleration capabilities delivering atleast 2X speed gain over other implemen-\ntations, which henceforth allows real time image segmentation that can be\ndeployed on Mobile Robotic systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 02:48:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Nahar", "Roopal", ""], ["Baranwal", "Akanksha", ""], ["Krishna", "K. Madhava", ""]]}, {"id": "1710.02266", "submitter": "Alexander Berardino", "authors": "Alexander Berardino, Johannes Ball\\'e, Valero Laparra and Eero P.\n  Simoncelli", "title": "Eigen-Distortions of Hierarchical Representations", "comments": "Selected for oral presentation at NIPS 2017", "journal-ref": "Adv. Neural Information Processing Systems (NIPS), Dec 2017, vol\n  30, pp 3530-3539", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for comparing hierarchical image representations in terms\nof their ability to explain perceptual sensitivity in humans. Specifically, we\nutilize Fisher information to establish a model-derived prediction of\nsensitivity to local perturbations of an image. For a given image, we compute\nthe eigenvectors of the Fisher information matrix with largest and smallest\neigenvalues, corresponding to the model-predicted most- and least-noticeable\nimage distortions, respectively. For human subjects, we then measure the amount\nof each distortion that can be reliably detected when added to the image. We\nuse this method to test the ability of a variety of representations to mimic\nhuman perceptual sensitivity. We find that the early layers of VGG16, a deep\nneural network optimized for object recognition, provide a better match to\nhuman perception than later layers, and a better match than a 4-stage\nconvolutional neural network (CNN) trained on a database of human ratings of\ndistorted image quality. On the other hand, we find that simple models of early\nvisual processing, incorporating one or more stages of local gain control,\ntrained on the same database of distortion ratings, provide substantially\nbetter predictions of human sensitivity than either the CNN, or any combination\nof layers of VGG16.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 03:22:20 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 12:12:25 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 16:45:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Berardino", "Alexander", ""], ["Ball\u00e9", "Johannes", ""], ["Laparra", "Valero", ""], ["Simoncelli", "Eero P.", ""]]}, {"id": "1710.02277", "submitter": "Donghyun Yoo", "authors": "Donghyun Yoo, Haoqi Fan, Vishnu Naresh Boddeti, Kris M. Kitani", "title": "Efficient K-Shot Learning with Regularized Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature representations from pre-trained deep neural networks have been known\nto exhibit excellent generalization and utility across a variety of related\ntasks. Fine-tuning is by far the simplest and most widely used approach that\nseeks to exploit and adapt these feature representations to novel tasks with\nlimited data. Despite the effectiveness of fine-tuning, itis often sub-optimal\nand requires very careful optimization to prevent severe over-fitting to small\ndatasets. The problem of sub-optimality and over-fitting, is due in part to the\nlarge number of parameters used in a typical deep convolutional neural network.\nTo address these problems, we propose a simple yet effective regularization\nmethod for fine-tuning pre-trained deep networks for the task of k-shot\nlearning. To prevent overfitting, our key strategy is to cluster the model\nparameters while ensuring intra-cluster similarity and inter-cluster diversity\nof the parameters, effectively regularizing the dimensionality of the parameter\nsearch space. In particular, we identify groups of neurons within each layer of\na deep network that shares similar activation patterns. When the network is to\nbe fine-tuned for a classification task using only k examples, we propagate a\nsingle gradient to all of the neuron parameters that belong to the same group.\nThe grouping of neurons is non-trivial as neuron activations depend on the\ndistribution of the input data. To efficiently search for optimal groupings\nconditioned on the input data, we propose a reinforcement learning search\nstrategy using recurrent networks to learn the optimal group assignments for\neach network layer. Experimental results show that our method can be easily\napplied to several popular convolutional neural networks and improve upon other\nstate-of-the-art fine-tuning based k-shot learning strategies by more than10%\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 05:07:28 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Yoo", "Donghyun", ""], ["Fan", "Haoqi", ""], ["Boddeti", "Vishnu Naresh", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1710.02286", "submitter": "Lars Hertel", "authors": "Lars Hertel, Erhardt Barth, Thomas K\\\"aster, Thomas Martinetz", "title": "Deep Convolutional Neural Networks as Generic Feature Extractors", "comments": "4 pages, accepted version for publication in Proceedings of the IEEE\n  International Joint Conference on Neural Networks (IJCNN), July 2015,\n  Killarney, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing objects in natural images is an intricate problem involving\nmultiple conflicting objectives. Deep convolutional neural networks, trained on\nlarge datasets, achieve convincing results and are currently the\nstate-of-the-art approach for this task. However, the long time needed to train\nsuch deep networks is a major drawback. We tackled this problem by reusing a\npreviously trained network. For this purpose, we first trained a deep\nconvolutional network on the ILSVRC2012 dataset. We then maintained the learned\nconvolution kernels and only retrained the classification part on different\ndatasets. Using this approach, we achieved an accuracy of 67.68 % on CIFAR-100,\ncompared to the previous state-of-the-art result of 65.43 %. Furthermore, our\nfindings indicate that convolutional networks are able to learn generic feature\nextractors that can be used for different tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 06:42:11 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Hertel", "Lars", ""], ["Barth", "Erhardt", ""], ["K\u00e4ster", "Thomas", ""], ["Martinetz", "Thomas", ""]]}, {"id": "1710.02310", "submitter": "Farnoosh Heidarivincheh", "authors": "Farnoosh Heidarivincheh, Majid Mirmehdi, Dima Damen", "title": "Detecting the Moment of Completion: Temporal Models for Localising\n  Action Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action completion detection is the problem of modelling the action's\nprogression towards localising the moment of completion - when the action's\ngoal is confidently considered achieved. In this work, we assess the ability of\ntwo temporal models, namely Hidden Markov Models (HMM) and Long-Short Term\nMemory (LSTM), to localise completion for six object interactions: switch,\nplug, open, pull, pick and drink. We use a supervised approach, where\nannotations of pre-completion and post-completion frames are available per\naction, and fine-tuned CNN features are used to train temporal models. Tested\non the Action-Completion-2016 dataset, we detect completion within 10 frames of\nannotations for ~75% of completed action sequences using both temporal models.\nResults show that fine-tuned CNN features outperform hand-crafted features for\nlocalisation, and that observing incomplete instances is necessary when\nincomplete sequences are also present in the test set.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 08:30:05 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Heidarivincheh", "Farnoosh", ""], ["Mirmehdi", "Majid", ""], ["Damen", "Dima", ""]]}, {"id": "1710.02316", "submitter": "Jean Stawiaski", "authors": "Jean Stawiaski", "title": "A Multiscale Patch Based Convolutional Network for Brain Tumor\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a multiscale patch based convolutional neural network\nfor the automatic segmentation of brain tumors in multi-modality 3D MR images.\nWe use multiscale deep supervision and inputs to train a convolutional network.\nWe evaluate the effectiveness of the proposed approach on the BRATS 2017\nsegmentation challenge where we obtained dice scores of 0.755, 0.900, 0.782 and\n95% Hausdorff distance of 3.63mm, 4.10mm, and 6.81mm for enhanced tumor core,\nwhole tumor and tumor core respectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:04:28 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Stawiaski", "Jean", ""]]}, {"id": "1710.02322", "submitter": "Diogo Luvizon", "authors": "Diogo C. Luvizon, Hedi Tabia, David Picard", "title": "Human Pose Regression by Combining Indirect Part Detection and\n  Contextual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an end-to-end trainable regression approach for\nhuman pose estimation from still images. We use the proposed Soft-argmax\nfunction to convert feature maps directly to joint coordinates, resulting in a\nfully differentiable framework. Our method is able to learn heat maps\nrepresentations indirectly, without additional steps of artificial ground truth\ngeneration. Consequently, contextual information can be included to the pose\npredictions in a seamless way. We evaluated our method on two very challenging\ndatasets, the Leeds Sports Poses (LSP) and the MPII Human Pose datasets,\nreaching the best performance among all the existing regression methods and\ncomparable results to the state-of-the-art detection based approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:27:44 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Luvizon", "Diogo C.", ""], ["Tabia", "Hedi", ""], ["Picard", "David", ""]]}, {"id": "1710.02338", "submitter": "Lei Huang", "authors": "Lei Huang, Xianglong Liu, Bo Lang and Bo Li", "title": "Projection Based Weight Normalization for Deep Neural Networks", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned\nproblem. We observe that the scaling-based weight space symmetry property in\nrectified nonlinear network will cause this negative effect. Therefore, we\npropose to constrain the incoming weights of each neuron to be unit-norm, which\nis formulated as an optimization problem over Oblique manifold. A simple yet\nefficient method referred to as projection based weight normalization (PBWN) is\nalso developed to solve this problem. PBWN executes standard gradient updates,\nfollowed by projecting the updated weight back to Oblique manifold. This\nproposed method has the property of regularization and collaborates well with\nthe commonly used batch normalization technique. We conduct comprehensive\nexperiments on several widely-used image datasets including CIFAR-10,\nCIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art\nconvolutional neural networks, such as Inception, VGG and residual networks.\nThe results show that our method is able to improve the performance of DNNs\nwith different architectures consistently. We also apply our method to Ladder\nnetwork for semi-supervised learning on permutation invariant MNIST dataset,\nand our method outperforms the state-of-the-art methods: we obtain test errors\nas 2.52%, 1.06%, and 0.91% with only 20, 50, and 100 labeled samples,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 10:24:38 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Huang", "Lei", ""], ["Liu", "Xianglong", ""], ["Lang", "Bo", ""], ["Li", "Bo", ""]]}, {"id": "1710.02410", "submitter": "Matthias M\\\"uller", "authors": "Felipe Codevilla, Matthias M\\\"uller, Antonio L\\'opez, Vladlen Koltun,\n  Alexey Dosovitskiy", "title": "End-to-end Driving via Conditional Imitation Learning", "comments": "Published at the International Conference on Robotics and Automation\n  (ICRA), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks trained on demonstrations of human driving have learned to\nfollow roads and avoid obstacles. However, driving policies trained via\nimitation learning cannot be controlled at test time. A vehicle trained\nend-to-end to imitate an expert cannot be guided to take a specific turn at an\nupcoming intersection. This limits the utility of such systems. We propose to\ncondition imitation learning on high-level command input. At test time, the\nlearned driving policy functions as a chauffeur that handles sensorimotor\ncoordination but continues to respond to navigational commands. We evaluate\ndifferent architectures for conditional imitation learning in vision-based\ndriving. We conduct experiments in realistic three-dimensional simulations of\nurban driving and on a 1/5 scale robotic truck that is trained to drive in a\nresidential area. Both systems drive based on visual input yet remain\nresponsive to high-level navigational commands. The supplementary video can be\nviewed at https://youtu.be/cFtnflNe5fM\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 14:00:31 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 16:43:34 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Codevilla", "Felipe", ""], ["M\u00fcller", "Matthias", ""], ["L\u00f3pez", "Antonio", ""], ["Koltun", "Vladlen", ""], ["Dosovitskiy", "Alexey", ""]]}, {"id": "1710.02534", "submitter": "Bo Dai", "authors": "Bo Dai, Dahua Lin", "title": "Contrastive Learning for Image Captioning", "comments": "accepted to 31st Conference on Neural Information Processing Systems\n  (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning, a popular topic in computer vision, has achieved\nsubstantial progress in recent years. However, the distinctiveness of natural\ndescriptions is often overlooked in previous work. It is closely related to the\nquality of captions, as distinctive captions are more likely to describe images\nwith their unique aspects. In this work, we propose a new learning method,\nContrastive Learning (CL), for image captioning. Specifically, via two\nconstraints formulated on top of a reference model, the proposed method can\nencourage distinctiveness, while maintaining the overall quality of the\ngenerated captions. We tested our method on two challenging datasets, where it\nimproves the baseline model by significant margins. We also showed in our\nstudies that the proposed method is generic and can be used for models with\nvarious structures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 18:00:48 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Dai", "Bo", ""], ["Lin", "Dahua", ""]]}, {"id": "1710.02546", "submitter": "Xuemei Xie", "authors": "Xuemei Xie, Chenye Wang, Shu Chen, Guangming Shi, Zhifu Zhao", "title": "Real-Time Illegal Parking Detection System Based on Deep Learning", "comments": "5pages,6figures", "journal-ref": null, "doi": "10.1145/3094243.3094261", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing illegal parking has become more and more serious. Nowadays the\nmethods of detecting illegally parked vehicles are based on background\nsegmentation. However, this method is weakly robust and sensitive to\nenvironment. Benefitting from deep learning, this paper proposes a novel\nillegal vehicle parking detection system. Illegal vehicles captured by camera\nare firstly located and classified by the famous Single Shot MultiBox Detector\n(SSD) algorithm. To improve the performance, we propose to optimize SSD by\nadjusting the aspect ratio of default box to accommodate with our dataset\nbetter. After that, a tracking and analysis of movement is adopted to judge the\nillegal vehicles in the region of interest (ROI). Experiments show that the\nsystem can achieve a 99% accuracy and real-time (25FPS) detection with strong\nrobustness in complex environments.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 07:57:29 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Xie", "Xuemei", ""], ["Wang", "Chenye", ""], ["Chen", "Shu", ""], ["Shi", "Guangming", ""], ["Zhao", "Zhifu", ""]]}, {"id": "1710.02566", "submitter": "Kaustubha Mendhurwar", "authors": "Kaustubha Mendhurwar, Qing Gu, Vladimir de la Cruz, Sudhir Mudur, and\n  Tiberiu Popa", "title": "CAMREP- Concordia Action and Motion Repository", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition, motion classification, gait analysis and synthesis are\nfundamental problems in a number of fields such as computer graphics,\nbio-mechanics and human computer interaction that generate a large body of\nresearch. This type of data is complex because it is inherently\nmultidimensional and has multiple modalities such as video, motion capture\ndata, accelerometer data, etc. While some of this data, such as monocular video\nare easy to acquire, others are much more difficult and expensive such as\nmotion capture data or multi-view video. This creates a large barrier of entry\nin the research community for data driven research. We have embarked on\ncreating a new large repository of motion and action data (CAMREP) consisting\nof several motion and action databases. What makes this database unique is that\nwe use a variety of modalities, enabling multi-modal analysis. Presently, the\nsize of datasets varies with some having a large number of subjects while\nothers having smaller numbers. We have also acquired long capture sequences in\na number of cases, making some datasets rather large.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 19:42:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Mendhurwar", "Kaustubha", ""], ["Gu", "Qing", ""], ["de la Cruz", "Vladimir", ""], ["Mudur", "Sudhir", ""], ["Popa", "Tiberiu", ""]]}, {"id": "1710.02584", "submitter": "Marc-Andr\\'e Carbonneau", "authors": "Marc-Andr\\'e Carbonneau, Eric Granger, Ghyslain Gagnon", "title": "Bag-Level Aggregation for Multiple Instance Active Learning in Instance\n  Classification Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of applications, e.g. video surveillance and medical image\nanalysis, require training recognition systems from large amounts of weakly\nannotated data while some targeted interactions with a domain expert are\nallowed to improve the training process. In such cases, active learning (AL)\ncan reduce labeling costs for training a classifier by querying the expert to\nprovide the labels of most informative instances. This paper focuses on AL\nmethods for instance classification problems in multiple instance learning\n(MIL), where data is arranged into sets, called bags, that are weakly labeled.\nMost AL methods focus on single instance learning problems. These methods are\nnot suitable for MIL problems because they cannot account for the bag structure\nof data. In this paper, new methods for bag-level aggregation of instance\ninformativeness are proposed for multiple instance active learning (MIAL). The\n\\textit{aggregated informativeness} method identifies the most informative\ninstances based on classifier uncertainty, and queries bags incorporating the\nmost information. The other proposed method, called \\textit{cluster-based\naggregative sampling}, clusters data hierarchically in the instance space. The\ninformativeness of instances is assessed by considering bag labels, inferred\ninstance labels, and the proportion of labels that remain to be discovered in\nclusters. Both proposed methods significantly outperform reference methods in\nextensive experiments using benchmark data from several application domains.\nResults indicate that using an appropriate strategy to address MIAL problems\nyields a significant reduction in the number of queries needed to achieve the\nsame level of performance as single instance AL methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 20:58:15 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Carbonneau", "Marc-Andr\u00e9", ""], ["Granger", "Eric", ""], ["Gagnon", "Ghyslain", ""]]}, {"id": "1710.02615", "submitter": "Salman Ul Hassan Dar", "authors": "Salman Ul Hassan Dar, Muzaffer \\\"Ozbey, Ahmet Burak \\c{C}atl{\\i},\n  Tolga \\c{C}ukur", "title": "A Transfer-Learning Approach for Accelerated MRI using Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Neural networks have received recent interest for reconstruction of\nundersampled MR acquisitions. Ideally network performance should be optimized\nby drawing the training and testing data from the same domain. In practice,\nhowever, large datasets comprising hundreds of subjects scanned under a common\nprotocol are rare. The goal of this study is to introduce a transfer-learning\napproach to address the problem of data scarcity in training deep networks for\naccelerated MRI.\n  Methods: Neural networks were trained on thousands of samples from public\ndatasets of either natural images or brain MR images. The networks were then\nfine-tuned using only few tens of brain MR images in a distinct testing domain.\nDomain-transferred networks were compared to networks trained directly in the\ntesting domain. Network performance was evaluated for varying acceleration\nfactors (2-10), number of training samples (0.5-4k) and number of fine-tuning\nsamples (0-100).\n  Results: The proposed approach achieves successful domain transfer between MR\nimages acquired with different contrasts (T1- and T2-weighted images), and\nbetween natural and MR images (ImageNet and T1- or T2-weighted images).\nNetworks obtained via transfer-learning using only tens of images in the\ntesting domain achieve nearly identical performance to networks trained\ndirectly in the testing domain using thousands of images.\n  Conclusion: The proposed approach might facilitate the use of neural networks\nfor MRI reconstruction without the need for collection of extensive imaging\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 01:22:24 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 17:34:35 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 09:43:36 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Dar", "Salman Ul Hassan", ""], ["\u00d6zbey", "Muzaffer", ""], ["\u00c7atl\u0131", "Ahmet Burak", ""], ["\u00c7ukur", "Tolga", ""]]}, {"id": "1710.02726", "submitter": "Ebrahim Karami", "authors": "Ebrahim Karami, Siva Prasad, Mohamed Shehata", "title": "Image Matching Using SIFT, SURF, BRIEF and ORB: Performance Comparison\n  for Distorted Images", "comments": "5 pages, 6 figures, In Proceedings of the 2015 Newfoundland\n  Electrical and Computer Engineering Conference,St. johns, Canada, November,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and robust image matching is a very important task with various\napplications in computer vision and robotics. In this paper, we compare the\nperformance of three different image matching techniques, i.e., SIFT, SURF, and\nORB, against different kinds of transformations and deformations such as\nscaling, rotation, noise, fish eye distortion, and shearing. For this purpose,\nwe manually apply different types of transformations on original images and\ncompute the matching evaluation parameters such as the number of key points in\nimages, the matching rate, and the execution time required for each algorithm\nand we will show that which algorithm is the best more robust against each kind\nof distortion. Index Terms-Image matching, scale invariant feature transform\n(SIFT), speed up robust feature (SURF), robust independent elementary features\n(BRIEF), oriented FAST, rotated BRIEF (ORB).\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 19:22:59 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Karami", "Ebrahim", ""], ["Prasad", "Siva", ""], ["Shehata", "Mohamed", ""]]}, {"id": "1710.02728", "submitter": "Ebrahim Karami", "authors": "Ebrahim Karami, Mohamed Shehata, and Andrew Smith", "title": "Image Identification Using SIFT Algorithm: Performance Analysis against\n  Different Image Deformations", "comments": "4 pages, 11 figures, In Proceedings of the 2015 Newfoundland\n  Electrical and Computer Engineering Conference,St. johns, Canada, November,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image identification is one of the most challenging tasks in different areas\nof computer vision. Scale-invariant feature transform is an algorithm to detect\nand describe local features in images to further use them as an image matching\ncriteria. In this paper, the performance of the SIFT matching algorithm against\nvarious image distortions such as rotation, scaling, fisheye and motion\ndistortion are evaluated and false and true positive rates for a large number\nof image pairs are calculated and presented. We also evaluate the distribution\nof the matched keypoint orientation difference for each image deformation.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 19:27:33 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 22:26:39 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Karami", "Ebrahim", ""], ["Shehata", "Mohamed", ""], ["Smith", "Andrew", ""]]}, {"id": "1710.02754", "submitter": "Jos\\'e Silva Neto", "authors": "Jos\\'e F. S. Neto, Waldson P. N. Leandro, Matheus A. Gadelha, Tiago S.\n  Santos, Bruno M. Carvalho, Edgar Gardu\\~no", "title": "Texture Fuzzy Segmentation using Skew Divergence Adaptive Affinity\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image segmentation is the process of assigning distinct labels to\ndifferent objects in a digital image, and the fuzzy segmentation algorithm has\nbeen successfully used in the segmentation of images from a wide variety of\nsources. However, the traditional fuzzy segmentation algorithm fails to segment\nobjects that are characterized by textures whose patterns cannot be\nsuccessfully described by simple statistics computed over a very restricted\narea. In this paper, we propose an extension of the fuzzy segmentation\nalgorithm that uses adaptive textural affinity functions to perform the\nsegmentation of such objects on bidimensional images. The adaptive affinity\nfunctions compute their appropriate neighborhood size as they compute the\ntexture descriptors surrounding the seed spels (spatial elements), according to\nthe characteristics of the texture being processed. The algorithm then segments\nthe image with an appropriate neighborhood for each object. We performed\nexperiments on mosaic images that were composed using images from the Brodatz\ndatabase, and compared our results with the ones produced by a recently\npublished texture segmentation algorithm, showing the applicability of our\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 22:10:08 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Neto", "Jos\u00e9 F. S.", ""], ["Leandro", "Waldson P. N.", ""], ["Gadelha", "Matheus A.", ""], ["Santos", "Tiago S.", ""], ["Carvalho", "Bruno M.", ""], ["Gardu\u00f1o", "Edgar", ""]]}, {"id": "1710.02756", "submitter": "William Casper", "authors": "W.R. Casper and Balu Nadiga", "title": "A New Spectral Clustering Algorithm", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new clustering algorithm that is based on searching for natural\ngaps in the components of the lowest energy eigenvectors of the Laplacian of a\ngraph. In comparing the performance of the proposed method with a set of other\npopular methods (KMEANS, spectral-KMEANS, and an agglomerative method) in the\ncontext of the Lancichinetti-Fortunato-Radicchi (LFR) Benchmark for undirected\nweighted overlapping networks, we find that the new method outperforms the\nother spectral methods considered in certain parameter regimes. Finally, in an\napplication to climate data involving one of the most important modes of\ninterannual climate variability, the El Nino Southern Oscillation phenomenon,\nwe demonstrate the ability of the new algorithm to readily identify different\nflavors of the phenomenon.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 22:59:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Casper", "W. R.", ""], ["Nadiga", "Balu", ""]]}, {"id": "1710.02759", "submitter": "Forrest Iandola", "authors": "Forrest Iandola and Kurt Keutzer", "title": "Keynote: Small Neural Nets Are Beautiful: Enabling Embedded Systems with\n  Small Deep-Neural-Network Architectures", "comments": "Keynote at Embedded Systems Week (ESWEEK) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last five years Deep Neural Nets have offered more accurate\nsolutions to many problems in speech recognition, and computer vision, and\nthese solutions have surpassed a threshold of acceptability for many\napplications. As a result, Deep Neural Networks have supplanted other\napproaches to solving problems in these areas, and enabled many new\napplications. While the design of Deep Neural Nets is still something of an art\nform, in our work we have found basic principles of design space exploration\nused to develop embedded microprocessor architectures to be highly applicable\nto the design of Deep Neural Net architectures. In particular, we have used\nthese design principles to create a novel Deep Neural Net called SqueezeNet\nthat requires as little as 480KB of storage for its model parameters. We have\nfurther integrated all these experiences to develop something of a playbook for\ncreating small Deep Neural Nets for embedded systems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 23:33:31 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Iandola", "Forrest", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1710.02820", "submitter": "Xiaopeng Hong", "authors": "Xiaopeng Hong, Thuong-Khanh Tran, Guoying Zhao", "title": "Micro-Expression Spotting: A Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Micro-expressions are rapid and involuntary facial expressions, which\nindicate the suppressed or concealed emotions. Recently, the research on\nautomatic micro-expression (ME) spotting obtains increasing attention. ME\nspotting is a crucial step prior to further ME analysis tasks. The spotting\nresults can be used as important cues to assist many other human-oriented tasks\nand thus have many potential applications. In this paper, by investigating\nexisting ME spotting methods, we recognize the immediacy of standardizing the\nperformance evaluation of micro-expression spotting methods. To this end, we\nconstruct a micro-expression spotting benchmark (MESB). Firstly, we set up a\nsliding window based multi-scale evaluation framework. Secondly, we introduce a\nseries of protocols. Thirdly, we also provide baseline results of popular\nmethods. The MESB facilitates the research on ME spotting with fairer and more\ncomprehensive evaluation and also enables to leverage the cutting-edge machine\nlearning tools widely.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 11:21:29 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Hong", "Xiaopeng", ""], ["Tran", "Thuong-Khanh", ""], ["Zhao", "Guoying", ""]]}, {"id": "1710.02844", "submitter": "Zeng Yu", "authors": "Zeng Yu, Tianrui Li, Ning Yu, Yi Pan, Hongmei Chen, Bing Liu", "title": "Reconstruction of Hidden Representation for Robust Feature Extraction", "comments": "This article has been accepted for publication in a future issue of\n  ACM Transactions on Intelligent Systems and Technology", "journal-ref": null, "doi": "10.1145/3284174", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to develop a new and robust approach to feature\nrepresentation. Motivated by the success of Auto-Encoders, we first theoretical\nsummarize the general properties of all algorithms that are based on\ntraditional Auto-Encoders: 1) The reconstruction error of the input can not be\nlower than a lower bound, which can be viewed as a guiding principle for\nreconstructing the input. Additionally, when the input is corrupted with\nnoises, the reconstruction error of the corrupted input also can not be lower\nthan a lower bound. 2) The reconstruction of a hidden representation achieving\nits ideal situation is the necessary condition for the reconstruction of the\ninput to reach the ideal state. 3) Minimizing the Frobenius norm of the\nJacobian matrix of the hidden representation has a deficiency and may result in\na much worse local optimum value. We believe that minimizing the reconstruction\nerror of the hidden representation is more robust than minimizing the Frobenius\nnorm of the Jacobian matrix of the hidden representation. Based on the above\nanalysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs),\nwhich uses corruption and reconstruction on both the input and the hidden\nrepresentation. We demonstrate that the proposed model is highly flexible and\nextensible and has a potentially better capability to learn invariant and\nrobust feature representations. We also show that our model is more robust than\nDenoising Auto-Encoders (DAEs) for dealing with noises or inessential features.\nFurthermore, we detail how to train DDAEs with two different pre-training\nmethods by optimizing the objective function in a combined and separate manner,\nrespectively. Comparative experiments illustrate that the proposed model is\nsignificantly better for representation learning than the state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 15:48:37 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 15:51:57 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Yu", "Zeng", ""], ["Li", "Tianrui", ""], ["Yu", "Ning", ""], ["Pan", "Yi", ""], ["Chen", "Hongmei", ""], ["Liu", "Bing", ""]]}, {"id": "1710.02856", "submitter": "Maneet Singh", "authors": "Maneet Singh, Shruti Nagpal, Mayank Vatsa, Richa Singh, Afzel Noore,\n  Angshul Majumdar", "title": "Gender and Ethnicity Classification of Iris Images using Deep\n  Class-Encoder", "comments": "International Joint Conference on Biometrics, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soft biometric modalities have shown their utility in different applications\nincluding reducing the search space significantly. This leads to improved\nrecognition performance, reduced computation time, and faster processing of\ntest samples. Some common soft biometric modalities are ethnicity, gender, age,\nhair color, iris color, presence of facial hair or moles, and markers. This\nresearch focuses on performing ethnicity and gender classification on iris\nimages. We present a novel supervised autoencoder based approach, Deep\nClass-Encoder, which uses class labels to learn discriminative representation\nfor the given sample by mapping the learned feature vector to its label. The\nproposed model is evaluated on two datasets each for ethnicity and gender\nclassification. The results obtained using the proposed Deep Class-Encoder\ndemonstrate its effectiveness in comparison to existing approaches and\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 17:01:37 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Singh", "Maneet", ""], ["Nagpal", "Shruti", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1710.02866", "submitter": "Maneet Singh", "authors": "Shruti Nagpal, Maneet Singh, Arushi Jain, Richa Singh, Mayank Vatsa,\n  Afzel Noore", "title": "On Matching Skulls to Digital Face Images: A Preliminary Approach", "comments": "International Joint Conference on Biometrics, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic application of automatically matching skull with face images is an\nimportant research area linking biometrics with practical applications in\nforensics. It is an opportunity for biometrics and face recognition researchers\nto help the law enforcement and forensic experts in giving an identity to\nunidentified human skulls. It is an extremely challenging problem which is\nfurther exacerbated due to lack of any publicly available database related to\nthis problem. This is the first research in this direction with a two-fold\ncontribution: (i) introducing the first of its kind skull-face image pair\ndatabase, IdentifyMe, and (ii) presenting a preliminary approach using the\nproposed semi-supervised formulation of transform learning. The experimental\nresults and comparison with existing algorithms showcase the challenging nature\nof the problem. We assert that the availability of the database will inspire\nresearchers to build sophisticated skull-to-face matching algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 18:05:10 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Nagpal", "Shruti", ""], ["Singh", "Maneet", ""], ["Jain", "Arushi", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Noore", "Afzel", ""]]}, {"id": "1710.02909", "submitter": "Rosaura Vidal Mata", "authors": "Rosaura G. Vidal, Sreya Banerjee, Klemen Grm, Vitomir Struc and Walter\n  J. Scheirer", "title": "UG^2: a Video Benchmark for Assessing the Impact of Image Restoration\n  and Enhancement on Automatic Visual Recognition", "comments": "Supplemental material: https://goo.gl/vVM1xe, Dataset:\n  https://goo.gl/AjA6En, CVPR 2018 Prize Challenge: ug2challenge.org", "journal-ref": null, "doi": "10.1109/WACV.2018.00177", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in image restoration and enhancement techniques have led to\ndiscussion about how such algorithmscan be applied as a pre-processing step to\nimprove automatic visual recognition. In principle, techniques like deblurring\nand super-resolution should yield improvements by de-emphasizing noise and\nincreasing signal in an input image. But the historically divergent goals of\nthe computational photography and visual recognition communities have created a\nsignificant need for more work in this direction. To facilitate new research,\nwe introduce a new benchmark dataset called UG^2, which contains three\ndifficult real-world scenarios: uncontrolled videos taken by UAVs and manned\ngliders, as well as controlled videos taken on the ground. Over 160,000\nannotated frames forhundreds of ImageNet classes are available, which are used\nfor baseline experiments that assess the impact of known and unknown image\nartifacts and other conditions on common deep learning-based object\nclassification approaches. Further, current image restoration and enhancement\ntechniques are evaluated by determining whether or not theyimprove baseline\nclassification performance. Results showthat there is plenty of room for\nalgorithmic innovation, making this dataset a useful tool going forward.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 02:01:58 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:12:50 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Vidal", "Rosaura G.", ""], ["Banerjee", "Sreya", ""], ["Grm", "Klemen", ""], ["Struc", "Vitomir", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1710.02914", "submitter": "Maneet Singh", "authors": "Shruti Nagpal, Maneet Singh, Richa Singh, Mayank Vatsa, Afzel Noore,\n  Angshul Majumdar", "title": "Face Sketch Matching via Coupled Deep Transform Learning", "comments": "International Conference on Computer Vision, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face sketch to digital image matching is an important challenge of face\nrecognition that involves matching across different domains. Current research\nefforts have primarily focused on extracting domain invariant representations\nor learning a mapping from one domain to the other. In this research, we\npropose a novel transform learning based approach termed as DeepTransformer,\nwhich learns a transformation and mapping function between the features of two\ndomains. The proposed formulation is independent of the input information and\ncan be applied with any existing learned or hand-crafted feature. Since the\nmapping function is directional in nature, we propose two variants of\nDeepTransformer: (i) semi-coupled and (ii) symmetrically-coupled deep transform\nlearning. This research also uses a novel IIIT-D Composite Sketch with Age\n(CSA) variations database which contains sketch images of 150 subjects along\nwith age-separated digital photos. The performance of the proposed models is\nevaluated on a novel application of sketch-to-sketch matching, along with\nsketch-to-digital photo matching. Experimental results demonstrate the\nrobustness of the proposed models in comparison to existing state-of-the-art\nsketch matching algorithms and a commercial face recognition system.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 02:42:01 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Nagpal", "Shruti", ""], ["Singh", "Maneet", ""], ["Singh", "Richa", ""], ["Vatsa", "Mayank", ""], ["Noore", "Afzel", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1710.02932", "submitter": "Haresh Karnan", "authors": "Haresh Karnan, Aritra Biswas, Pranav Vaidik Dhulipala, Jan Dufek and\n  Robin Murphy", "title": "Visual Servoing of Unmanned Surface Vehicle from Small Tethered Unmanned\n  Aerial Vehicle", "comments": "6 pages, 13 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm and the implementation of a motor schema to\naid the visual localization subsystem of the ongoing EMILY project at Texas A\nand M University. The EMILY project aims to team an Unmanned Surface Vehicle\n(USV) with an Unmanned Aerial Vehicle (UAV) to augment the search and rescue of\nmarine casualties during an emergency response phase. The USV is designed to\nserve as a flotation device once it reaches the victims. A live video feed from\nthe UAV is provided to the casuality responders giving them a visual estimate\nof the USVs orientation and position to help with its navigation. One of the\nchallenges involved with casualty response using a USV UAV team is to\nsimultaneously control the USV and track it. In this paper, we present an\nimplemented solution to automate the UAV camera movements to keep the USV in\nview at all times. The motor schema proposed, uses the USVs coordinates from\nthe visual localization subsystem to control the UAVs camera movements and\ntrack the USV with minimal camera movements such that the USV is always in the\ncameras field of view.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 04:14:05 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Karnan", "Haresh", ""], ["Biswas", "Aritra", ""], ["Dhulipala", "Pranav Vaidik", ""], ["Dufek", "Jan", ""], ["Murphy", "Robin", ""]]}, {"id": "1710.02939", "submitter": "Faxian Cao", "authors": "Faxian Cao, Zhijing Yang, Jinchang Ren, Mengying Jiang, Wing-Kuen Ling", "title": "Does Normalization Methods Play a Role for Hyperspectral Image\n  Classification?", "comments": "6 pages. 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For Hyperspectral image (HSI) datasets, each class have their salient feature\nand classifiers classify HSI datasets according to the class's saliency\nfeatures, however, there will be different salient features when use different\nnormalization method. In this letter, we report the effect on classifiers by\ndifferent normalization methods and recommend the best normalization methods\nfor classifier after analyzing the impact of different normalization methods on\nclassifiers. Pavia University datasets, Indian Pines datasets and Kennedy Space\nCenter datasets will apply to several typical classifiers in order to evaluate\nand analysis the impact of different normalization methods on typical\nclassifiers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 05:08:32 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Cao", "Faxian", ""], ["Yang", "Zhijing", ""], ["Ren", "Jinchang", ""], ["Jiang", "Mengying", ""], ["Ling", "Wing-Kuen", ""]]}, {"id": "1710.02984", "submitter": "Jan Egger", "authors": "Alexander Hann, Lucas Bettac, Mark M. Haenle, Tilmann Graeter, Andreas\n  W. Berger, Jens Dreyhaupt, Dieter Schmalstieg, Wolfram G. Zoller, Jan Egger", "title": "Algorithm guided outlining of 105 pancreatic cancer liver metastases in\n  Ultrasound", "comments": "7 pages, 3 Figures, 3 Tables, 46 References", "journal-ref": "Sci Rep. 2017 Oct 6;7(1):12779", "doi": "10.1038/s41598-017-12925-z", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual segmentation of hepatic metastases in ultrasound images acquired from\npatients suffering from pancreatic cancer is common practice. Semiautomatic\nmeasurements promising assistance in this process are often assessed using a\nsmall number of lesions performed by examiners who already know the algorithm.\nIn this work, we present the application of an algorithm for the segmentation\nof liver metastases due to pancreatic cancer using a set of 105 different\nimages of metastases. The algorithm and the two examiners had never assessed\nthe images before. The examiners first performed a manual segmentation and,\nafter five weeks, a semiautomatic segmentation using the algorithm. They were\nsatisfied in up to 90% of the cases with the semiautomatic segmentation\nresults. Using the algorithm was significantly faster and resulted in a median\nDice similarity score of over 80%. Estimation of the inter-operator variability\nby using the intra class correlation coefficient was good with 0.8. In\nconclusion, the algorithm facilitates fast and accurate segmentation of liver\nmetastases, comparable to the current gold standard of manual segmentation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 08:22:28 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Hann", "Alexander", ""], ["Bettac", "Lucas", ""], ["Haenle", "Mark M.", ""], ["Graeter", "Tilmann", ""], ["Berger", "Andreas W.", ""], ["Dreyhaupt", "Jens", ""], ["Schmalstieg", "Dieter", ""], ["Zoller", "Wolfram G.", ""], ["Egger", "Jan", ""]]}, {"id": "1710.02985", "submitter": "Ke Zhang", "authors": "Ke Zhang, Ce Gao, Liru Guo, Miao Sun, Xingfang Yuan, Tony X. Han,\n  Zhenbing Zhao and Baogang Li", "title": "Age Group and Gender Estimation in the Wild with Deep RoR Architecture", "comments": "accepted by IEEE ACCESS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically predicting age group and gender from face images acquired in\nunconstrained conditions is an important and challenging task in many\nreal-world applications. Nevertheless, the conventional methods with\nmanually-designed features on in-the-wild benchmarks are unsatisfactory because\nof incompetency to tackle large variations in unconstrained images. This\ndifficulty is alleviated to some degree through Convolutional Neural Networks\n(CNN) for its powerful feature representation. In this paper, we propose a new\nCNN based method for age group and gender estimation leveraging Residual\nNetworks of Residual Networks (RoR), which exhibits better optimization ability\nfor age group and gender classification than other CNN architectures.Moreover,\ntwo modest mechanisms based on observation of the characteristics of age group\nare presented to further improve the performance of age estimation.In order to\nfurther improve the performance and alleviate over-fitting problem, RoR model\nis pre-trained on ImageNet firstly, and then it is fune-tuned on the\nIMDB-WIKI-101 data set for further learning the features of face images,\nfinally, it is used to fine-tune on Adience data set. Our experiments\nillustrate the effectiveness of RoR method for age and gender estimation in the\nwild, where it achieves better performance than other CNN methods. Finally, the\nRoR-152+IMDB-WIKI-101 with two mechanisms achieves new state-of-the-art results\non Adience benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 08:27:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhang", "Ke", ""], ["Gao", "Ce", ""], ["Guo", "Liru", ""], ["Sun", "Miao", ""], ["Yuan", "Xingfang", ""], ["Han", "Tony X.", ""], ["Zhao", "Zhenbing", ""], ["Li", "Baogang", ""]]}, {"id": "1710.03011", "submitter": "Yanyu Xu", "authors": "Yanyu Xu, Shenghua Gao, Junru Wu, Nianyi Li, and Jingyi Yu", "title": "Personalized Saliency and its Prediction", "comments": "15 pages, 10 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all existing visual saliency models by far have focused on predicting\na universal saliency map across all observers. Yet psychology studies suggest\nthat visual attention of different observers can vary significantly under\nspecific circumstances, especially a scene is composed of multiple salient\nobjects. To study such heterogenous visual attention pattern across observers,\nwe first construct a personalized saliency dataset and explore correlations\nbetween visual attention, personal preferences, and image contents.\nSpecifically, we propose to decompose a personalized saliency map (referred to\nas PSM) into a universal saliency map (referred to as USM) predictable by\nexisting saliency detection models and a new discrepancy map across users that\ncharacterizes personalized saliency. We then present two solutions towards\npredicting such discrepancy maps, i.e., a multi-task convolutional neural\nnetwork (CNN) framework and an extended CNN with Person-specific Information\nEncoded Filters (CNN-PIEF). Extensive experimental results demonstrate the\neffectiveness of our models for PSM prediction as well their generalization\ncapability for unseen observers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 09:43:48 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 23:18:38 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Xu", "Yanyu", ""], ["Gao", "Shenghua", ""], ["Wu", "Junru", ""], ["Li", "Nianyi", ""], ["Yu", "Jingyi", ""]]}, {"id": "1710.03023", "submitter": "Gianmarco Santini", "authors": "G. Santini, D. Della Latta, N. Martini, G. Valvano, A. Gori, A.\n  Ripoli, C.L. Susini, L. Landini and D. Chiappino", "title": "An automatic deep learning approach for coronary artery calcium\n  segmentation", "comments": null, "journal-ref": "EMBEC & NBC 2017. IFMBE, volume 65, pp. 374-377", "doi": "10.1007/978-981-10-5122-7_94", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronary artery calcium (CAC) is a significant marker of atherosclerosis and\ncardiovascular events. In this work we present a system for the automatic\nquantification of calcium score in ECG-triggered non-contrast enhanced cardiac\ncomputed tomography (CT) images. The proposed system uses a supervised deep\nlearning algorithm, i.e. convolutional neural network (CNN) for the\nsegmentation and classification of candidate lesions as coronary or not,\npreviously extracted in the region of the heart using a cardiac atlas. We\ntrained our network with 45 CT volumes; 18 volumes were used to validate the\nmodel and 56 to test it. Individual lesions were detected with a sensitivity of\n91.24%, a specificity of 95.37% and a positive predicted value (PPV) of 90.5%;\ncomparing calcium score obtained by the system and calcium score manually\nevaluated by an expert operator, a Pearson coefficient of 0.983 was obtained. A\nhigh agreement (Cohen's k = 0.879) between manual and automatic risk prediction\nwas also observed. These results demonstrated that convolutional neural\nnetworks can be effectively applied for the automatic segmentation and\nclassification of coronary calcifications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 10:48:13 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Santini", "G.", ""], ["Della Latta", "D.", ""], ["Martini", "N.", ""], ["Valvano", "G.", ""], ["Gori", "A.", ""], ["Ripoli", "A.", ""], ["Susini", "C. L.", ""], ["Landini", "L.", ""], ["Chiappino", "D.", ""]]}, {"id": "1710.03025", "submitter": "Himanshu Jain", "authors": "Himanshu Jain, Archana Praveen Kumar", "title": "A Sequential Thinning Algorithm For Multi-Dimensional Binary Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thinning is the removal of contour pixels/points of connected components in\nan image to produce their skeleton with retained connectivity and structural\nproperties. The output requirements of a thinning procedure often vary with\napplication. This paper proposes a sequential algorithm that is very easy to\nunderstand and modify based on application to perform the thinning of\nmulti-dimensional binary patterns. The algorithm was tested on 2D and 3D\npatterns and showed very good results. Moreover, comparisons were also made\nwith two of the state-of-the-art methods used for 2D patterns. The results\nobtained prove the validity of the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 10:52:55 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:19:29 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Jain", "Himanshu", ""], ["Kumar", "Archana Praveen", ""]]}, {"id": "1710.03027", "submitter": "Himanshu Jain", "authors": "Himanshu Jain and Archana Praveen Kumar", "title": "A Bottom Up Procedure for Text Line Segmentation of Latin Script", "comments": "Accepted and presented at the IEEE conference \"International\n  Conference on Advances in Computing, Communications and Informatics (ICACCI)\n  2017\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a bottom up procedure for segmentation of text lines\nwritten or printed in the Latin script. The proposed method uses a combination\nof image morphology, feature extraction and Gaussian mixture model to perform\nthis task. The experimental results show the validity of the procedure.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 11:00:00 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Jain", "Himanshu", ""], ["Kumar", "Archana Praveen", ""]]}, {"id": "1710.03077", "submitter": "Da Li", "authors": "Da Li, Yongxin Yang, Yi-Zhe Song and Timothy M. Hospedales", "title": "Deeper, Broader and Artier Domain Generalization", "comments": "9 pages, 4 figures, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain generalization is to learn from multiple training\ndomains, and extract a domain-agnostic model that can then be applied to an\nunseen domain. Domain generalization (DG) has a clear motivation in contexts\nwhere there are target domains with distinct characteristics, yet sparse data\nfor training. For example recognition in sketch images, which are distinctly\nmore abstract and rarer than photos. Nevertheless, DG methods have primarily\nbeen evaluated on photo-only benchmarks focusing on alleviating the dataset\nbias where both problems of domain distinctiveness and data sparsity can be\nminimal. We argue that these benchmarks are overly straightforward, and show\nthat simple deep learning baselines perform surprisingly well on them. In this\npaper, we make two main contributions: Firstly, we build upon the favorable\ndomain shift-robust properties of deep learning methods, and develop a low-rank\nparameterized CNN model for end-to-end DG learning. Secondly, we develop a DG\nbenchmark dataset covering photo, sketch, cartoon and painting domains. This is\nboth more practically relevant, and harder (bigger domain shift) than existing\nbenchmarks. The results show that our method outperforms existing DG\nalternatives, and our dataset provides a more significant DG challenge to drive\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:19:27 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Li", "Da", ""], ["Yang", "Yongxin", ""], ["Song", "Yi-Zhe", ""], ["Hospedales", "Timothy M.", ""]]}, {"id": "1710.03112", "submitter": "Hongjian Zhan", "authors": "Hongjian Zhan, Qingqing Wang, Yue Lu", "title": "Handwritten digit string recognition by combination of residual network\n  and RNN-CTC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural network (RNN) and connectionist temporal classification\n(CTC) have showed successes in many sequence labeling tasks with the strong\nability of dealing with the problems where the alignment between the inputs and\nthe target labels is unknown. Residual network is a new structure of\nconvolutional neural network and works well in various computer vision tasks.\nIn this paper, we take advantage of the architectures mentioned above to create\na new network for handwritten digit string recognition. First we design a\nresidual network to extract features from input images, then we employ a RNN to\nmodel the contextual information within feature sequences and predict\nrecognition results. At the top of this network, a standard CTC is applied to\ncalculate the loss and yield the final results. These three parts compose an\nend-to-end trainable network. The proposed new architecture achieves the\nhighest performances on ORAND-CAR-A and ORAND-CAR-B with recognition rates\n89.75% and 91.14%, respectively. In addition, the experiments on a generated\ncaptcha dataset which has much longer string length show the potential of the\nproposed network to handle long strings.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 14:16:00 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhan", "Hongjian", ""], ["Wang", "Qingqing", ""], ["Lu", "Yue", ""]]}, {"id": "1710.03113", "submitter": "Dong Huang", "authors": "Dong Huang, Chang-Dong Wang, Jian-Huang Lai, Chee-Keong Kwoh", "title": "Toward Multi-Diversified Ensemble Clustering of High-Dimensional Data:\n  From Subspaces to Metrics and Beyond", "comments": "Accepted by IEEE Transactions on Cybernetics. The MATLAB source code\n  is available at https://github.com/huangdonghere/MDEC", "journal-ref": null, "doi": "10.1109/TCYB.2021.3049633", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid emergence of high-dimensional data in various areas has brought new\nchallenges to current ensemble clustering research. To deal with the curse of\ndimensionality, recently considerable efforts in ensemble clustering have been\nmade by means of different subspace-based techniques. However, besides the\nemphasis on subspaces, rather limited attention has been paid to the potential\ndiversity in similarity/dissimilarity metrics. It remains a surprisingly open\nproblem in ensemble clustering how to create and aggregate a large population\nof diversified metrics, and furthermore, how to jointly investigate the\nmulti-level diversity in the large populations of metrics, subspaces, and\nclusters in a unified framework. To tackle this problem, this paper proposes a\nnovel multi-diversified ensemble clustering approach. In particular, we create\na large number of diversified metrics by randomizing a scaled exponential\nsimilarity kernel, which are then coupled with random subspaces to form a large\nset of metric-subspace pairs. Based on the similarity matrices derived from\nthese metric-subspace pairs, an ensemble of diversified base clusterings can\nthereby be constructed. Further, an entropy-based criterion is utilized to\nexplore the cluster-wise diversity in ensembles, based on which three specific\nensemble clustering algorithms are presented by incorporating three types of\nconsensus functions. Extensive experiments are conducted on 30 high-dimensional\ndatasets, including 18 cancer gene expression datasets and 12 image/speech\ndatasets, which demonstrate the superiority of our algorithms over the\nstate-of-the-art. The source code is available at\nhttps://github.com/huangdonghere/MDEC.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 14:19:04 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 08:06:15 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 17:33:00 GMT"}, {"version": "v4", "created": "Tue, 5 Jan 2021 01:49:43 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Huang", "Dong", ""], ["Wang", "Chang-Dong", ""], ["Lai", "Jian-Huang", ""], ["Kwoh", "Chee-Keong", ""]]}, {"id": "1710.03144", "submitter": "Zibo Meng", "authors": "Jie Cai, Zibo Meng, Ahmed Shehab Khan, Zhiyuan Li, James O'Reilly, Yan\n  Tong", "title": "Island Loss for Learning Discriminative Features in Facial Expression\n  Recognition", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, Convolutional Neural Networks (CNNs) have shown\npromise on facial expression recognition. However, the performance degrades\ndramatically under real-world settings due to variations introduced by subtle\nfacial appearance changes, head pose variations, illumination changes, and\nocclusions.\n  In this paper, a novel island loss is proposed to enhance the discriminative\npower of the deeply learned features. Specifically, the IL is designed to\nreduce the intra-class variations while enlarging the inter-class differences\nsimultaneously. Experimental results on four benchmark expression databases\nhave demonstrated that the CNN with the proposed island loss (IL-CNN)\noutperforms the baseline CNN models with either traditional softmax loss or the\ncenter loss and achieves comparable or better performance compared with the\nstate-of-the-art methods for facial expression recognition.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 15:26:22 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 19:13:20 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 15:32:10 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Cai", "Jie", ""], ["Meng", "Zibo", ""], ["Khan", "Ahmed Shehab", ""], ["Li", "Zhiyuan", ""], ["O'Reilly", "James", ""], ["Tong", "Yan", ""]]}, {"id": "1710.03224", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele", "title": "Person Recognition in Personal Photo Collections", "comments": "18 pages, 20 figures; to appear in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2877588", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People nowadays share large parts of their personal lives through social\nmedia. Being able to automatically recognise people in personal photos may\ngreatly enhance user convenience by easing photo album organisation. For human\nidentification task, however, traditional focus of computer vision has been\nface recognition and pedestrian re-identification. Person recognition in social\nmedia photos sets new challenges for computer vision, including non-cooperative\nsubjects (e.g. backward viewpoints, unusual poses) and great changes in\nappearance. To tackle this problem, we build a simple person recognition\nframework that leverages convnet features from multiple image regions (head,\nbody, etc.). We propose new recognition scenarios that focus on the time and\nappearance gap between training and testing samples. We present an in-depth\nanalysis of the importance of different features according to time and\nviewpoint generalisability. In the process, we verify that our simple approach\nachieves the state of the art result on the PIPA benchmark, arguably the\nlargest social media based benchmark for person recognition to date with\ndiverse poses, viewpoints, social groups, and events.\n  Compared the conference version of the paper, this paper additionally\npresents (1) analysis of a face recogniser (DeepID2+), (2) new method naeil2\nthat combines the conference version method naeil and DeepID2+ to achieve state\nof the art results even compared to post-conference works, (3) discussion of\nrelated work since the conference version, (4) additional analysis including\nthe head viewpoint-wise breakdown of performance, and (5) results on the\nopen-world setup.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 13:33:39 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 23:46:47 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Oh", "Seong Joon", ""], ["Benenson", "Rodrigo", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1710.03255", "submitter": "Bowen Shi", "authors": "Bowen Shi and Karen Livescu", "title": "Multitask training with unlabeled data for end-to-end sign language\n  fingerspelling recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatic American Sign Language fingerspelling\nrecognition from video. Prior work has largely relied on frame-level labels,\nhand-crafted features, or other constraints, and has been hampered by the\nscarcity of data for this task. We introduce a model for fingerspelling\nrecognition that addresses these issues. The model consists of an\nauto-encoder-based feature extractor and an attention-based neural\nencoder-decoder, which are trained jointly. The model receives a sequence of\nimage frames and outputs the fingerspelled word, without relying on any\nframe-level training labels or hand-crafted features. In addition, the\nauto-encoder subcomponent makes it possible to leverage unlabeled data to\nimprove the feature learning. The model achieves 11.6% and 4.4% absolute letter\naccuracy improvement respectively in signer-independent and signer-adapted\nfingerspelling recognition over previous approaches that required frame-level\ntraining labels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 18:21:57 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:52:59 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Shi", "Bowen", ""], ["Livescu", "Karen", ""]]}, {"id": "1710.03337", "submitter": "Jiajun Lu", "authors": "Jiajun Lu, Hussein Sibai, Evan Fabry, David Forsyth", "title": "Standard detectors aren't (currently) fooled by physical adversarial\n  stop signs", "comments": "Follow up for previous adversarial stop sign paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adversarial example is an example that has been adjusted to produce the\nwrong label when presented to a system at test time. If adversarial examples\nexisted that could fool a detector, they could be used to (for example) wreak\nhavoc on roads populated with smart vehicles. Recently, we described our\ndifficulties creating physical adversarial stop signs that fool a detector.\nMore recently, Evtimov et al. produced a physical adversarial stop sign that\nfools a proxy model of a detector. In this paper, we show that these physical\nadversarial stop signs do not fool two standard detectors (YOLO and Faster\nRCNN) in standard configuration. Evtimov et al.'s construction relies on a crop\nof the image to the stop sign; this crop is then resized and presented to a\nclassifier. We argue that the cropping and resizing procedure largely\neliminates the effects of rescaling and of view angle. Whether an adversarial\nattack is robust under rescaling and change of view direction remains moot. We\nargue that attacking a classifier is very different from attacking a detector,\nand that the structure of detectors - which must search for their own bounding\nbox, and which cannot estimate that box very accurately - likely makes it\ndifficult to make adversarial patterns. Finally, an adversarial pattern on a\nphysical object that could fool a detector would have to be adversarial in the\nface of a wide family of parametric distortions (scale; view angle; box shift\ninside the detector; illumination; and so on). Such a pattern would be of great\ntheoretical and practical interest. There is currently no evidence that such\npatterns exist.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 22:20:59 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 21:53:58 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Lu", "Jiajun", ""], ["Sibai", "Hussein", ""], ["Fabry", "Evan", ""], ["Forsyth", "David", ""]]}, {"id": "1710.03344", "submitter": "Kuang Gong", "authors": "Kuang Gong, Jiahui Guan, Kyungsang Kim, Xuezhu Zhang, Georges El\n  Fakhri, Jinyi Qi, Quanzheng Li", "title": "Iterative PET Image Reconstruction Using Convolutional Neural Network\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PET image reconstruction is challenging due to the ill-poseness of the\ninverse problem and limited number of detected photons. Recently deep neural\nnetworks have been widely and successfully used in computer vision tasks and\nattracted growing interests in medical imaging. In this work, we trained a deep\nresidual convolutional neural network to improve PET image quality by using the\nexisting inter-patient information. An innovative feature of the proposed\nmethod is that we embed the neural network in the iterative reconstruction\nframework for image representation, rather than using it as a post-processing\ntool. We formulate the objective function as a constraint optimization problem\nand solve it using the alternating direction method of multipliers (ADMM)\nalgorithm. Both simulation data and hybrid real data are used to evaluate the\nproposed method. Quantification results show that our proposed iterative neural\nnetwork method can outperform the neural network denoising and conventional\npenalized maximum likelihood methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 22:51:28 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Gong", "Kuang", ""], ["Guan", "Jiahui", ""], ["Kim", "Kyungsang", ""], ["Zhang", "Xuezhu", ""], ["Fakhri", "Georges El", ""], ["Qi", "Jinyi", ""], ["Li", "Quanzheng", ""]]}, {"id": "1710.03370", "submitter": "Feng Liu", "authors": "Feng Liu, Tao Xiang, Timothy M. Hospedales, Wankou Yang, Changyin Sun", "title": "iVQA: Inverse Visual Question Answering", "comments": "CVPR18 Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the inverse problem of Visual question answering (iVQA), and\nexplore its suitability as a benchmark for visuo-linguistic understanding. The\niVQA task is to generate a question that corresponds to a given image and\nanswer pair. Since the answers are less informative than the questions, and the\nquestions have less learnable bias, an iVQA model needs to better understand\nthe image to be successful than a VQA model. We pose question generation as a\nmulti-modal dynamic inference process and propose an iVQA model that can\ngradually adjust its focus of attention guided by both a partially generated\nquestion and the answer. For evaluation, apart from existing linguistic\nmetrics, we propose a new ranking metric. This metric compares the ground truth\nquestion's rank among a list of distractors, which allows the drawbacks of\ndifferent algorithms and sources of error to be studied. Experimental results\nshow that our model can generate diverse, grammatically correct and content\ncorrelated questions that match the given answer.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 01:22:52 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 06:54:11 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Liu", "Feng", ""], ["Xiang", "Tao", ""], ["Hospedales", "Timothy M.", ""], ["Yang", "Wankou", ""], ["Sun", "Changyin", ""]]}, {"id": "1710.03383", "submitter": "Cheng-Bin Jin", "authors": "Cheng-Bin Jin, Shengzhe Li, and Hakil Kim", "title": "Real-Time Action Detection in Video Surveillance using Sub-Action\n  Descriptor with Multi-CNN", "comments": "29 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we say a person is texting, can you tell the person is walking or\nsitting? Emphatically, no. In order to solve this incomplete representation\nproblem, this paper presents a sub-action descriptor for detailed action\ndetection. The sub-action descriptor consists of three levels: the posture, the\nlocomotion, and the gesture level. The three levels give three sub-action\ncategories for one action to address the representation problem. The proposed\naction detection model simultaneously localizes and recognizes the actions of\nmultiple individuals in video surveillance using appearance-based temporal\nfeatures with multi-CNN. The proposed approach achieved a mean average\nprecision (mAP) of 76.6% at the frame-based and 83.5% at the video-based\nmeasurement on the new large-scale ICVL video surveillance dataset that the\nauthors introduce and make available to the community with this paper.\nExtensive experiments on the benchmark KTH dataset demonstrate that the\nproposed approach achieved better performance, which in turn boosts the action\nrecognition performance over the state-of-the-art. The action detection model\ncan run at around 25 fps on the ICVL and more than 80 fps on the KTH dataset,\nwhich is suitable for real-time surveillance applications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 02:50:37 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Jin", "Cheng-Bin", ""], ["Li", "Shengzhe", ""], ["Kim", "Hakil", ""]]}, {"id": "1710.03425", "submitter": "Xu-Cheng Yin", "authors": "Chun Yang, Xu-Cheng Yin, Zejun Li, Jianwei Wu, Chunchao Guo, Hongfa\n  Wang, Lei Xiao", "title": "AdaDNNs: Adaptive Ensemble of Deep Neural Networks for Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing text in the wild is a really challenging task because of complex\nbackgrounds, various illuminations and diverse distortions, even with deep\nneural networks (convolutional neural networks and recurrent neural networks).\nIn the end-to-end training procedure for scene text recognition, the outputs of\ndeep neural networks at different iterations are always demonstrated with\ndiversity and complementarity for the target object (text). Here, a simple but\neffective deep learning method, an adaptive ensemble of deep neural networks\n(AdaDNNs), is proposed to simply select and adaptively combine classifier\ncomponents at different iterations from the whole learning system. Furthermore,\nthe ensemble is formulated as a Bayesian framework for classifier weighting and\ncombination. A variety of experiments on several typical acknowledged\nbenchmarks, i.e., ICDAR Robust Reading Competition (Challenge 1, 2 and 4)\ndatasets, verify the surprised improvement from the baseline DNNs, and the\neffectiveness of AdaDNNs compared with the recent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 07:16:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Yang", "Chun", ""], ["Yin", "Xu-Cheng", ""], ["Li", "Zejun", ""], ["Wu", "Jianwei", ""], ["Guo", "Chunchao", ""], ["Wang", "Hongfa", ""], ["Xiao", "Lei", ""]]}, {"id": "1710.03474", "submitter": "Samuele Capobianco", "authors": "Samuele Capobianco and Simone Marinai", "title": "DocEmul: a Toolkit to Generate Structured Historical Documents", "comments": "In Proceedings of the 14th International Conference on Document\n  Analysis and Recognition (ICDAR), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a toolkit to generate structured synthetic documents emulating the\nactual document production process. Synthetic documents can be used to train\nsystems to perform document analysis tasks. In our case we address the record\ncounting task on handwritten structured collections containing a limited number\nof examples. Using the DocEmul toolkit we can generate a larger dataset to\ntrain a deep architecture to predict the number of records for each page. The\ntoolkit is able to generate synthetic collections and also perform data\naugmentation to create a larger trainable dataset. It includes one method to\nextract the page background from real pages which can be used as a substrate\nwhere records can be written on the basis of variable structures and using\ncursive fonts. Moreover, it is possible to extend the synthetic collection by\nadding random noise, page rotations, and other visual variations. We performed\nsome experiments on two different handwritten collections using the toolkit to\ngenerate synthetic data to train a Convolutional Neural Network able to count\nthe number of records in the real collections.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 09:40:19 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Capobianco", "Samuele", ""], ["Marinai", "Simone", ""]]}, {"id": "1710.03488", "submitter": "Wenjing Ke", "authors": "Wenjing Ke, Yuanjie Zhu, Lei Yu", "title": "Automatic Streaming Segmentation of Stereo Video Using Bilateral Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take advantage of binocular camera and propose an\nunsupervised algorithm based on semi-supervised segmentation algorithm and\nextracting foreground part efficiently. We creatively embed depth information\ninto bilateral grid in the graph cut model and achieve considerable segmenting\naccuracy in the case of no user input. The experi- ment approves the high\nprecision, time efficiency of our algorithm and its adaptation to complex\nnatural scenario which is significant for practical application.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 10:04:39 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 04:29:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Ke", "Wenjing", ""], ["Zhu", "Yuanjie", ""], ["Yu", "Lei", ""]]}, {"id": "1710.03553", "submitter": "Shanxin Zhang", "authors": "Shanxin Zhang, Cheng Wang, Zhuang Yang, Chenglu Wen, Jonathan Li,\n  Chenhui Yang", "title": "Traffic Sign Timely Visual Recognizability Evaluation Based on 3D\n  Measurable Point Clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timely provision of traffic sign information to drivers is essential for\nthe drivers to respond, to ensure safe driving, and to avoid traffic accidents\nin a timely manner. We proposed a timely visual recognizability quantitative\nevaluation method for traffic signs in large-scale transportation environments.\nTo achieve this goal, we first address the concept of a visibility field to\nreflect the visible distribution of three-dimensional (3D) space and construct\na traffic sign Visibility Evaluation Model (VEM) to measure the traffic sign\nvisibility for a given viewpoint. Then, based on the VEM, we proposed the\nconcept of the Visual Recognizability Field (VRF) to reflect the visual\nrecognizability distribution in 3D space and established a Visual\nRecognizability Evaluation Model (VREM) to measure a traffic sign visual\nrecognizability for a given viewpoint. Next, we proposed a Traffic Sign Timely\nVisual Recognizability Evaluation Model (TSTVREM) by combining VREM, the actual\nmaximum continuous visual recognizable distance, and traffic big data to\nmeasure a traffic sign visual recognizability in different lanes. Finally, we\npresented an automatic algorithm to implement the TSTVREM model through traffic\nsign and road marking detection and classification, traffic sign environment\npoint cloud segmentation, viewpoints calculation, and TSTVREM model\nrealization. The performance of our method for traffic sign timely visual\nrecognizability evaluation is tested on three road point clouds acquired by a\nmobile laser scanning system (RIEGL VMX-450) according to Road Traffic Signs\nand Markings (GB 5768-1999 in China), showing that our method is feasible and\nefficient.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 13:02:52 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhang", "Shanxin", ""], ["Wang", "Cheng", ""], ["Yang", "Zhuang", ""], ["Wen", "Chenglu", ""], ["Li", "Jonathan", ""], ["Yang", "Chenhui", ""]]}, {"id": "1710.03778", "submitter": "Seung Yeon Shin", "authors": "Seung Yeon Shin, Soochahn Lee, Il Dong Yun, Sun Mi Kim, Kyoung Mu Lee", "title": "Joint Weakly and Semi-Supervised Deep Learning for Localization and\n  Classification of Masses in Breast Ultrasound Images", "comments": "Accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2872031", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for localization and classification of masses in\nbreast ultrasound (BUS) images. We have experimentally found that training\nconvolutional neural network based mass detectors with large, weakly annotated\ndatasets presents a non-trivial problem, while overfitting may occur with those\ntrained with small, strongly annotated datasets. To overcome these problems, we\nuse a weakly annotated dataset together with a smaller strongly annotated\ndataset in a hybrid manner. We propose a systematic weakly and semi-supervised\ntraining scenario with appropriate training loss selection. Experimental\nresults show that the proposed method can successfully localize and classify\nmasses with less annotation effort. The results trained with only 10 strongly\nannotated images along with weakly annotated images were comparable to results\ntrained from 800 strongly annotated images, with the 95% confidence interval of\ndifference -3.00%--5.00%, in terms of the correct localization (CorLoc)\nmeasure, which is the ratio of images with intersection over union with ground\ntruth higher than 0.5. With the same number of strongly annotated images,\nadditional weakly annotated images can be incorporated to give a 4.5% point\nincrease in CorLoc, from 80.00% to 84.50% (with 95% confidence intervals\n76.00%--83.75% and 81.00%--88.00%). The effects of different algorithmic\ndetails and varied amount of data are presented through ablative analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 18:39:24 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 01:08:15 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Shin", "Seung Yeon", ""], ["Lee", "Soochahn", ""], ["Yun", "Il Dong", ""], ["Kim", "Sun Mi", ""], ["Lee", "Kyoung Mu", ""]]}, {"id": "1710.03811", "submitter": "Sachin Mehta", "authors": "Sachin Mehta, Amar P. Azad, Saneem A. Chemmengath, Vikas Raykar, and\n  Shivkumar Kalyanaraman", "title": "DeepSolarEye: Power Loss Prediction and Weakly Supervised Soiling\n  Localization via Fully Convolutional Networks for Solar Panels", "comments": "Accepted for publication at WACV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of soiling on solar panels is an important and well-studied\nproblem in renewable energy sector. In this paper, we present the first\nconvolutional neural network (CNN) based approach for solar panel soiling and\ndefect analysis. Our approach takes an RGB image of solar panel and\nenvironmental factors as inputs to predict power loss, soiling localization,\nand soiling type. In computer vision, localization is a complex task which\ntypically requires manually labeled training data such as bounding boxes or\nsegmentation masks. Our proposed approach consists of specialized four stages\nwhich completely avoids localization ground truth and only needs panel images\nwith power loss labels for training. The region of impact area obtained from\nthe predicted localization masks are classified into soiling types using the\nwebly supervised learning. For improving localization capabilities of CNNs, we\nintroduce a novel bi-directional input-aware fusion (BiDIAF) block that\nreinforces the input at different levels of CNN to learn input-specific feature\nmaps. Our empirical study shows that BiDIAF improves the power loss prediction\naccuracy by about 3% and localization accuracy by about 4%. Our end-to-end\nmodel yields further improvement of about 24% on localization when learned in a\nweakly supervised manner. Our approach is generalizable and showed promising\nresults on web crawled solar panel images. Our system has a frame rate of 22\nfps (including all steps) on a NVIDIA TitanX GPU. Additionally, we collected\nfirst of it's kind dataset for solar panel image analysis consisting 45,000+\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 20:31:42 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 23:10:36 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Mehta", "Sachin", ""], ["Azad", "Amar P.", ""], ["Chemmengath", "Saneem A.", ""], ["Raykar", "Vikas", ""], ["Kalyanaraman", "Shivkumar", ""]]}, {"id": "1710.03823", "submitter": "Paras Lakhani", "authors": "Vishal Desai, Adam E. Flanders, Paras Lakhani", "title": "Application of Deep Learning in Neuroradiology: Automated Detection of\n  Basal Ganglia Hemorrhage using 2D-Convolutional Neural Networks", "comments": "7 pages, 5 figures, 3 tables, 30 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Deep learning techniques have achieved high accuracy in image\nclassification tasks, and there is interest in applicability to neuroimaging\ncritical findings. This study evaluates the efficacy of 2D deep convolutional\nneural networks (DCNNs) for detecting basal ganglia (BG) hemorrhage on\nnoncontrast head CT.\n  Materials and Methods: 170 unique de-identified HIPAA-compliant noncontrast\nhead CTs were obtained, those with and without BG hemorrhage. 110 cases were\nheld-out for test, and 60 were split into training (45) and validation (15),\nconsisting of 20 right, 20 left, and 20 no BG hemorrhage. Data augmentation was\nperformed to increase size and variation of the training dataset by 48-fold.\nTwo DCNNs were used to classify the images-AlexNet and GoogLeNet-using\nuntrained networks and those pre-trained on ImageNet. Area under the curves\n(AUC) for the receiver-operator characteristic (ROC) curves were calculated,\nusing the DeLong method for statistical comparison of ROCs.\n  Results: The best performing model was the pre-trained augmented GoogLeNet,\nwhich had an AUC of 1.00 in classification of hemorrhage. Preprocessing\naugmentation increased accuracy for all networks (p<0.001), and pretrained\nnetworks outperformed untrained ones (p<0.001) for the unaugmented models. The\nbest performing GoogLeNet model (AUC 1.00) outperformed the best performing\nAlexNet model (AUC 0.95)(p=0.01).\n  Conclusion: For this dataset, the best performing DCNN identified BG\nhemorrhage on noncontrast head CT with an AUC of 1.00. Pretrained networks and\ndata augmentation increased classifier accuracy. Future prospective research\nwould be important to determine if the accuracy can be maintained on a larger\ncohort of patients and for very small hemorrhages.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 21:13:58 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 16:23:02 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Desai", "Vishal", ""], ["Flanders", "Adam E.", ""], ["Lakhani", "Paras", ""]]}, {"id": "1710.03923", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad and Daoqiang Zhang", "title": "Deep Hyperalignment", "comments": "31st Conference on Neural Information Processing Systems (NIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Deep Hyperalignment (DHA) as a regularized, deep\nextension, scalable Hyperalignment (HA) method, which is well-suited for\napplying functional alignment to fMRI datasets with nonlinearity,\nhigh-dimensionality (broad ROI), and a large number of subjects. Unlink\nprevious methods, DHA is not limited by a restricted fixed kernel function.\nFurther, it uses a parametric approach, rank-$m$ Singular Value Decomposition\n(SVD), and stochastic gradient descent for optimization. Therefore, DHA has a\nsuitable time complexity for large datasets, and DHA does not require the\ntraining data when it computes the functional alignment for a new subject.\nExperimental studies on multi-subject fMRI analysis confirm that the DHA method\nachieves superior performance to other state-of-the-art HA algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 06:21:45 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1710.03958", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Axel Pinz, Andrew Zisserman", "title": "Detect to Track and Track to Detect", "comments": "ICCV 2017. Code and models:\n  https://github.com/feichtenhofer/Detect-Track Results:\n  https://www.robots.ox.ac.uk/~vgg/research/detect-track/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches for high accuracy detection and tracking of object\ncategories in video consist of complex multistage solutions that become more\ncumbersome each year. In this paper we propose a ConvNet architecture that\njointly performs detection and tracking, solving the task in a simple and\neffective way. Our contributions are threefold: (i) we set up a ConvNet\narchitecture for simultaneous detection and tracking, using a multi-task\nobjective for frame-based object detection and across-frame track regression;\n(ii) we introduce correlation features that represent object co-occurrences\nacross time to aid the ConvNet during tracking; and (iii) we link the frame\nlevel detections based on our across-frame tracklets to produce high accuracy\ndetections at the video level. Our ConvNet architecture for spatiotemporal\nobject detection is evaluated on the large-scale ImageNet VID dataset where it\nachieves state-of-the-art results. Our approach provides better single model\nperformance than the winning method of the last ImageNet challenge while being\nconceptually much simpler. Finally, we show that by increasing the temporal\nstride we can dramatically increase the tracker speed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 08:33:48 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 10:49:41 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Pinz", "Axel", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1710.03959", "submitter": "Devis Tuia", "authors": "Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang,\n  Feng Xu, Friedrich Fraundorfer", "title": "Deep learning in remote sensing: a review", "comments": "Accepted for publication IEEE Geoscience and Remote Sensing Magazine", "journal-ref": null, "doi": "10.1109/MGRS.2017.2762307", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standing at the paradigm shift towards data-intensive science, machine\nlearning techniques are becoming increasingly important. In particular, as a\nmajor breakthrough in the field, deep learning has proven as an extremely\npowerful tool in many fields. Shall we embrace deep learning as the key to all?\nOr, should we resist a 'black-box' solution? There are controversial opinions\nin the remote sensing community. In this article, we analyze the challenges of\nusing deep learning for remote sensing data analysis, review the recent\nadvances, and provide resources to make deep learning in remote sensing\nridiculously simple to start with. More importantly, we advocate remote sensing\nscientists to bring their expertise into deep learning, and use it as an\nimplicit general model to tackle unprecedented large-scale influential\nchallenges, such as climate change and urbanization.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 08:35:05 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Zhu", "Xiao Xiang", ""], ["Tuia", "Devis", ""], ["Mou", "Lichao", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Liangpei", ""], ["Xu", "Feng", ""], ["Fraundorfer", "Friedrich", ""]]}, {"id": "1710.04011", "submitter": "Michael McCann", "authors": "Michael T. McCann, Kyong Hwan Jin, Michael Unser", "title": "A Review of Convolutional Neural Networks for Inverse Problems in\n  Imaging", "comments": null, "journal-ref": "IEEE Signal Processing Magazine, vol. 34, no. 6, pp. 85-95, Nov.\n  2017", "doi": "10.1109/MSP.2017.2739299", "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey paper, we review recent uses of convolution neural networks\n(CNNs) to solve inverse problems in imaging. It has recently become feasible to\ntrain deep CNNs on large databases of images, and they have shown outstanding\nperformance on object classification and segmentation tasks. Motivated by these\nsuccesses, researchers have begun to apply CNNs to the resolution of inverse\nproblems such as denoising, deconvolution, super-resolution, and medical image\nreconstruction, and they have started to report improvements over\nstate-of-the-art methods, including sparsity-based techniques such as\ncompressed sensing. Here, we review the recent experimental work in these\nareas, with a focus on the critical design decisions: Where does the training\ndata come from? What is the architecture of the CNN? and How is the learning\nproblem formulated and solved? We also bring together a few key theoretical\npapers that offer perspective on why CNNs are appropriate for inverse problems\nand point to some next steps in the field.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 11:26:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["McCann", "Michael T.", ""], ["Jin", "Kyong Hwan", ""], ["Unser", "Michael", ""]]}, {"id": "1710.04026", "submitter": "Kai Zhang", "authors": "Kai Zhang, Wangmeng Zuo, Lei Zhang", "title": "FFDNet: Toward a Fast and Flexible Solution for CNN based Image\n  Denoising", "comments": "IEEE Transactions on Image Processing, code:\n  https://github.com/cszn/FFDNet", "journal-ref": null, "doi": "10.1109/TIP.2018.2839891", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fast inference and good performance, discriminative learning\nmethods have been widely studied in image denoising. However, these methods\nmostly learn a specific model for each noise level, and require multiple models\nfor denoising images with different noise levels. They also lack flexibility to\ndeal with spatially variant noise, limiting their applications in practical\ndenoising. To address these issues, we present a fast and flexible denoising\nconvolutional neural network, namely FFDNet, with a tunable noise level map as\nthe input. The proposed FFDNet works on downsampled sub-images, achieving a\ngood trade-off between inference speed and denoising performance. In contrast\nto the existing discriminative denoisers, FFDNet enjoys several desirable\nproperties, including (i) the ability to handle a wide range of noise levels\n(i.e., [0, 75]) effectively with a single network, (ii) the ability to remove\nspatially variant noise by specifying a non-uniform noise level map, and (iii)\nfaster speed than benchmark BM3D even on CPU without sacrificing denoising\nperformance. Extensive experiments on synthetic and real noisy images are\nconducted to evaluate FFDNet in comparison with state-of-the-art denoisers. The\nresults show that FFDNet is effective and efficient, making it highly\nattractive for practical denoising applications.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:04:37 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 15:16:19 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1710.04034", "submitter": "Chun Pong Lau", "authors": "Chun Pong Lau, Chun Pang Yung and Lok Ming Lui", "title": "Image retargeting via Beltrami representation", "comments": "13pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image retargeting aims to resize an image to one with a prescribed aspect\nratio. Simple scaling inevitably introduces unnatural geometric distortions on\nthe important content of the image. In this paper, we propose a simple and yet\neffective method to resize an image, which preserves the geometry of the\nimportant content, using the Beltrami representation. Our algorithm allows\nusers to interactively label content regions as well as line structures. Image\nresizing can then be achieved by warping the image by an orientation-preserving\nbijective warping map with controlled distortion. The warping map is\nrepresented by its Beltrami representation, which captures the local geometric\ndistortion of the map. By carefully prescribing the values of the Beltrami\nrepresentation, images with different complexity can be effectively resized.\nOur method does not require solving any optimization problems and tuning\nparameters throughout the process. This results in a simple and efficient\nalgorithm to solve the image retargeting problem. Extensive experiments have\nbeen carried out, which demonstrate the efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:20:20 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Lau", "Chun Pong", ""], ["Yung", "Chun Pang", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1710.04043", "submitter": "Guotai Wang", "authors": "Guotai Wang, Wenqi Li, Maria A. Zuluaga, Rosalind Pratt, Premal A.\n  Patel, Michael Aertsen, Tom Doel, Anna L. David, Jan Deprest, Sebastien\n  Ourselin and Tom Vercauteren", "title": "Interactive Medical Image Segmentation using Deep Learning with\n  Image-specific Fine-tuning", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TMI.2018.2791721", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved state-of-the-art\nperformance for automatic medical image segmentation. However, they have not\ndemonstrated sufficiently accurate and robust results for clinical use. In\naddition, they are limited by the lack of image-specific adaptation and the\nlack of generalizability to previously unseen object classes. To address these\nproblems, we propose a novel deep learning-based framework for interactive\nsegmentation by incorporating CNNs into a bounding box and scribble-based\nsegmentation pipeline. We propose image-specific fine-tuning to make a CNN\nmodel adaptive to a specific test image, which can be either unsupervised\n(without additional user interactions) or supervised (with additional\nscribbles). We also propose a weighted loss function considering network and\ninteraction-based uncertainty for the fine-tuning. We applied this framework to\ntwo applications: 2D segmentation of multiple organs from fetal MR slices,\nwhere only two types of these organs were annotated for training; and 3D\nsegmentation of brain tumor core (excluding edema) and whole brain tumor\n(including edema) from different MR sequences, where only tumor cores in one MR\nsequence were annotated for training. Experimental results show that 1) our\nmodel is more robust to segment previously unseen objects than state-of-the-art\nCNNs; 2) image-specific fine-tuning with the proposed weighted loss function\nsignificantly improves segmentation accuracy; and 3) our method leads to\naccurate results with fewer user interactions and less user time than\ntraditional interactive segmentation methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 12:57:52 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Wang", "Guotai", ""], ["Li", "Wenqi", ""], ["Zuluaga", "Maria A.", ""], ["Pratt", "Rosalind", ""], ["Patel", "Premal A.", ""], ["Aertsen", "Michael", ""], ["Doel", "Tom", ""], ["David", "Anna L.", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1710.04071", "submitter": "Chunbiao Zhu", "authors": "Chunbiao Zhu, Kan Huang, Ge Li", "title": "Automatic Salient Object Detection for Panoramic Images Using Region\n  Growing and Fixation Prediction Model", "comments": "Previous Project website: https://github.com/ChunbiaoZhu/DCC-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all previous works on saliency detection have been dedicated to\nconventional images, however, with the outbreak of panoramic images due to the\nrapid development of VR or AR technology, it is becoming more challenging,\nmeanwhile valuable for extracting salient contents in panoramic images.\n  In this paper, we propose a novel bottom-up salient object detection\nframework for panoramic images. First, we employ a spatial density estimation\nmethod to roughly extract object proposal regions, with the help of region\ngrowing algorithm. Meanwhile, an eye fixation model is utilized to predict\nvisually attractive parts in the image from the perspective of the human visual\nsearch mechanism. Then, the previous results are combined by the maxima\nnormalization to get the coarse saliency map. Finally, a refinement step based\non geodesic distance is utilized for post-processing to derive the final\nsaliency map.\n  To fairly evaluate the performance of the proposed approach, we propose a\nhigh-quality dataset of panoramic images (SalPan). Extensive evaluations\ndemonstrate the effectiveness of our proposed method on panoramic images and\nthe superiority of the proposed method against other methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 02:18:47 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 07:31:05 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 00:17:54 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 00:27:20 GMT"}, {"version": "v5", "created": "Tue, 3 Apr 2018 02:20:32 GMT"}, {"version": "v6", "created": "Tue, 10 Apr 2018 08:46:49 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Zhu", "Chunbiao", ""], ["Huang", "Kan", ""], ["Li", "Ge", ""]]}, {"id": "1710.04076", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt", "title": "Deep Semantic Abstractions of Everyday Human Activities: On Commonsense\n  Representations of Human Interactions", "comments": "In ROBOT 2017: Third Iberian Robotics Conference. Escuela T\\'ecnica\n  Superior de Ingenier\\'ia, Sevilla (Spain) (November 22-24, 2017).\n  https://grvc.us.es/robot2017/ (to appear). arXiv admin note: substantial text\n  overlap with arXiv:1709.05293", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep semantic characterization of space and motion categorically\nfrom the viewpoint of grounding embodied human-object interactions. Our key\nfocus is on an ontological model that would be adept to formalisation from the\nviewpoint of commonsense knowledge representation, relational learning, and\nqualitative reasoning about space and motion in cognitive robotics settings. We\ndemonstrate key aspects of the space & motion ontology and its formalization as\na representational framework in the backdrop of select examples from a dataset\nof everyday activities. Furthermore, focussing on human-object interaction data\nobtained from RGBD sensors, we also illustrate how declarative\n(spatio-temporal) reasoning in the (constraint) logic programming family may be\nperformed with the developed deep semantic abstractions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 07:40:39 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1710.04097", "submitter": "Hamid Tizhoosh", "authors": "Morteza Babaie, H.R. Tizhoosh, Amin Khatami, M.E. Shiri", "title": "Local Radon Descriptors for Image Search", "comments": "To appear in proceedings of the 7th International Conference on Image\n  Processing Theory, Tools and Applications (IPTA 2017), Nov 28-Dec 1,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radon transform and its inverse operation are important techniques in medical\nimaging tasks. Recently, there has been renewed interest in Radon transform for\napplications such as content-based medical image retrieval. However, all\nstudies so far have used Radon transform as a global or quasi-global image\ndescriptor by extracting projections of the whole image or large sub-images.\nThis paper attempts to show that the dense sampling to generate the histogram\nof local Radon projections has a much higher discrimination capability than the\nglobal one. In this paper, we introduce Local Radon Descriptor (LRD) and apply\nit to the IRMA dataset, which contains 14,410 x-ray images as well as to the\nINRIA Holidays dataset with 1,990 images. Our results show significant\nimprovement in retrieval performance by using LRD versus its global version. We\nalso demonstrate that LRD can deliver results comparable to well-established\ndescriptors like LBP and HOG.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:50:01 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Babaie", "Morteza", ""], ["Tizhoosh", "H. R.", ""], ["Khatami", "Amin", ""], ["Shiri", "M. E.", ""]]}, {"id": "1710.04112", "submitter": "Alejandro Cartas", "authors": "Alejandro Cartas, Juan Marin, Petia Radeva and Mariella Dimiccoli", "title": "Batch-based Activity Recognition from Egocentric Photo-Streams Revisited", "comments": null, "journal-ref": "Cartas, A., Marin, J., Radeva, P. et al. Pattern Anal Applic\n  (2018). https://doi.org/10.1007/s10044-018-0708-1", "doi": "10.1007/s10044-018-0708-1", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras can gather large a\\-mounts of image data that provide rich\nvisual information about the daily activities of the wearer. Motivated by the\nlarge number of health applications that could be enabled by the automatic\nrecognition of daily activities, such as lifestyle characterization for habit\nimprovement, context-aware personal assistance and tele-rehabilitation\nservices, we propose a system to classify 21 daily activities from\nphoto-streams acquired by a wearable photo-camera. Our approach combines the\nadvantages of a Late Fusion Ensemble strategy relying on convolutional neural\nnetworks at image level with the ability of recurrent neural networks to\naccount for the temporal evolution of high level features in photo-streams\nwithout relying on event boundaries. The proposed batch-based approach achieved\nan overall accuracy of 89.85\\%, outperforming state of the art end-to-end\nmethodologies. These results were achieved on a dataset consists of 44,902\negocentric pictures from three persons captured during 26 days in average.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 15:23:07 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 19:02:38 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Cartas", "Alejandro", ""], ["Marin", "Juan", ""], ["Radeva", "Petia", ""], ["Dimiccoli", "Mariella", ""]]}, {"id": "1710.04176", "submitter": "Yueru Chen", "authors": "C.-C. Jay Kuo and Yueru Chen", "title": "On Data-Driven Saak Transform", "comments": "30 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being motivated by the multilayer RECOS (REctified-COrrelations on a Sphere)\ntransform, we develop a data-driven Saak (Subspace approximation with augmented\nkernels) transform in this work. The Saak transform consists of three steps: 1)\nbuilding the optimal linear subspace approximation with orthonormal bases using\nthe second-order statistics of input vectors, 2) augmenting each transform\nkernel with its negative, 3) applying the rectified linear unit (ReLU) to the\ntransform output. The Karhunen-Lo\\'eve transform (KLT) is used in the first\nstep. The integration of Steps 2 and 3 is powerful since they resolve the sign\nconfusion problem, remove the rectification loss and allow a straightforward\nimplementation of the inverse Saak transform at the same time. Multiple Saak\ntransforms are cascaded to transform images of a larger size. All Saak\ntransform kernels are derived from the second-order statistics of input random\nvectors in a one-pass feedforward manner. Neither data labels nor\nbackpropagation is used in kernel determination. Multi-stage Saak transforms\noffer a family of joint spatial-spectral representations between two extremes;\nnamely, the full spatial-domain representation and the full spectral-domain\nrepresentation. We select Saak coefficients of higher discriminant power to\nform a feature vector for pattern recognition, and use the MNIST dataset\nclassification problem as an illustrative example.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 17:08:28 GMT"}, {"version": "v2", "created": "Sat, 14 Oct 2017 17:40:37 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Kuo", "C. -C. Jay", ""], ["Chen", "Yueru", ""]]}, {"id": "1710.04200", "submitter": "Yijun Li", "authors": "Yijun Li, Jia-Bin Huang, Narendra Ahuja, Ming-Hsuan Yang", "title": "Joint Image Filtering with Deep Convolutional Networks", "comments": "Accepted by TPAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint image filters leverage the guidance image as a prior and transfer the\nstructural details from the guidance image to the target image for suppressing\nnoise or enhancing spatial resolution. Existing methods either rely on various\nexplicit filter constructions or hand-designed objective functions, thereby\nmaking it difficult to understand, improve, and accelerate these filters in a\ncoherent framework. In this paper, we propose a learning-based approach for\nconstructing joint filters based on Convolutional Neural Networks. In contrast\nto existing methods that consider only the guidance image, the proposed\nalgorithm can selectively transfer salient structures that are consistent with\nboth guidance and target images. We show that the model trained on a certain\ntype of data, e.g., RGB and depth images, generalizes well to other modalities,\ne.g., flash/non-Flash and RGB/NIR images. We validate the effectiveness of the\nproposed joint filter through extensive experimental evaluations with\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 17:56:59 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 03:31:02 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 05:07:18 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 18:02:13 GMT"}, {"version": "v5", "created": "Wed, 2 Jan 2019 19:30:38 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Li", "Yijun", ""], ["Huang", "Jia-Bin", ""], ["Ahuja", "Narendra", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.04207", "submitter": "Enrico Celeghini", "authors": "Enrico Celeghini", "title": "Algebraic Image Processing", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to image processing related to algebraic operators\nacting in the space of images. In view of the interest in the applications in\noptics and computer science, mathematical aspects of the paper have been\nsimplified as much as possible. Underlying theory, related to rigged Hilbert\nspaces and Lie algebras, is discussed elsewhere\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 08:42:35 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Celeghini", "Enrico", ""]]}, {"id": "1710.04265", "submitter": "David Casillas-Perez", "authors": "David Casillas-Perez, Daniel Pizarro, Manuel Mazo and Adrien Bartoli", "title": "Solutions of Quadratic First-Order ODEs applied to Computer Vision\n  Problems", "comments": "The version 2: New change of variable. Maximal Curve Maximal Solution\n  Convergence Cones The version 3: modifies the author's list and the abstract\n  in metadata", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is a study about the existence and the uniqueness of solutions\nof a specific quadratic first-order ODE that frequently appears in multiple\nreconstruction problems. It is called the \\emph{planar-perspective equation}\ndue to the duality with the geometric problem of reconstruction of\nplanar-perspective curves from their modulus. Solutions of the\n\\emph{planar-perspective equation} are related with planar curves parametrized\nwith perspective parametrization due to this geometric interpretation. The\narticle proves the existence of only two local solutions to the \\emph{initial\nvalue problem} with \\emph{regular initial conditions} and a maximum of two\nanalytic solutions with \\emph{critical initial conditions}. The article also\ngives theorems to extend the local definition domain where the existence of\nboth solutions are guaranteed. It introduces the \\emph{maximal depth function}\nas a function that upper-bound all possible solutions of the\n\\emph{planar-perspective equation} and contains all its possible \\emph{critical\npoints}. Finally, the article describes the \\emph{maximal-depth solution\nproblem} that consists of finding the solution of the referred equation that\nhas maximum the depth and proves its uniqueness. It is an important problem as\nit does not need initial conditions to obtain the unique solution and its the\nfrequent solution that practical algorithms of the state-of-the-art give.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 19:29:42 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 08:24:15 GMT"}, {"version": "v3", "created": "Wed, 27 Jun 2018 09:33:14 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Casillas-Perez", "David", ""], ["Pizarro", "Daniel", ""], ["Mazo", "Manuel", ""], ["Bartoli", "Adrien", ""]]}, {"id": "1710.04346", "submitter": "Nikolaos Kolotouros", "authors": "Nikolaos Kolotouros and Petros Maragos", "title": "A Finite Element Computational Framework for Active Contours on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new framework for the solution of active contour\nmodels on graphs. With the use of the Finite Element Method we generalize\nactive contour models on graphs and reduce the problem from a partial\ndifferential equation to the solution of a sparse non-linear system.\nAdditionally, we extend the proposed framework to solve models where the curve\nevolution is locally constrained around its current location. Based on the\nprevious extension, we propose a fast algorithm for the solution of a wide\nrange active contour models. Last, we present a supervised extension of\nGeodesic Active Contours for image segmentation and provide experimental\nevidence for the effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 02:50:10 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Kolotouros", "Nikolaos", ""], ["Maragos", "Petros", ""]]}, {"id": "1710.04359", "submitter": "Peihan Tu", "authors": "Peihan Tu", "title": "Fast initial guess estimation for digital image correlation", "comments": "The method does not have sufficient validations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image correlation (DIC) is a widely used optical metrology for\nquantitative deformation measurement due to its non-contact, low-cost, highly\nprecise feature. DIC relies on nonlinear optimization algorithm. Thus it is\nquite important to efficiently obtain a reliable initial guess. The most widely\nused method for obtaining initial guess is reliability-guided digital image\ncorrelation (RG-DIC) method, which is reliable but path-dependent. This\npath-dependent method limits the further improvement of computation speed of\nDIC using parallel computing technology, and error of calculation may be spread\nout along the calculation path. Therefore, a reliable and path-independent\nalgorithm which is able to provide reliable initial guess is desirable to reach\nfull potential of the ability of parallel computing. In this paper, an\nalgorithm used for initial guess estimation is proposed. Numerical and real\nexperiments show that the proposed algorithm, adaptive incremental\ndissimilarity approximations algorithm (A-IDA), has the following\ncharacteristics: 1) Compared with inverse compositional Gauss-Newton (IC-GN)\nsub-pixel registration algorithm, the computational time required by A-IDA\nalgorithm is negligible, especially when subset size is relatively large; 2)\nthe efficiency of A-IDA algorithm is less influenced by search range; 3) the\nefficiency is less influenced by subset size; 4) it is easy to select the\nthreshold for the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 04:20:00 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 16:56:25 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Tu", "Peihan", ""]]}, {"id": "1710.04374", "submitter": "Peihan Tu", "authors": "Peihan Tu", "title": "Fast, Accurate and Fully Parallelizable Digital Image Correlation", "comments": "The method does not have sufficient validations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital image correlation (DIC) is a widely used optical metrology for\nsurface deformation measurements. DIC relies on nonlinear optimization method.\nThus an initial guess is quite important due to its influence on the converge\ncharacteristics of the algorithm. In order to obtain a reliable, accurate\ninitial guess, a reliability-guided digital image correlation (RG-DIC) method,\nwhich is able to intelligently obtain a reliable initial guess without using\ntime-consuming integer-pixel registration, was proposed. However, the RG-DIC\nand its improved methods are path-dependent and cannot be fully parallelized.\nBesides, it is highly possible that RG-DIC fails in the full-field analysis of\ndeformation without manual intervention if the deformation fields contain large\nareas of discontinuous deformation. Feature-based initial guess is highly\nrobust while it is relatively time-consuming. Recently, path-independent\nalgorithm, fast Fourier transform-based cross correlation (FFT-CC) algorithm,\nwas proposed to estimate the initial guess. Complete parallelizability is the\nmajor advantage of the FFT-CC algorithm, while it is sensitive to small\ndeformation. Wu et al proposed an efficient integer-pixel search scheme, but\nthe parameters of this algorithm are set by the users empirically. In this\ntechnical note, a fully parallelizable DIC method is proposed. Different from\nRG-DIC method, the proposed method divides DIC algorithm into two parts:\nfull-field initial guess estimation and sub-pixel registration. The proposed\nmethod has the following benefits: 1) providing a pre-knowledge of deformation\nfields; 2) saving computational time; 3) reducing error propagation; 4)\nintegratability with well-established DIC algorithms; 5) fully\nparallelizability.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 05:36:32 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 16:56:21 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Tu", "Peihan", ""]]}, {"id": "1710.04450", "submitter": "Parvin Razzaghi", "authors": "Parvin Razzaghi", "title": "Self-Taught Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new approach for classification of target task using limited\nlabeled target data as well as enormous unlabeled source data is proposed which\nis called self-taught learning. The target and source data can be drawn from\ndifferent distributions. In the previous approaches, covariate shift assumption\nis considered where the marginal distributions p(x) change over domains and the\nconditional distributions p(y|x) remain the same. In our approach, we propose a\nnew objective function which simultaneously learns a common space T(.) where\nthe conditional distributions over domains p(T(x)|y) remain the same and learns\nrobust SVM classifiers for target task using both source and target data in the\nnew representation. Hence, in the proposed objective function, the hidden label\nof the source data is also incorporated. We applied the proposed approach on\nCaltech-256, MSRC+LMO datasets and compared the performance of our algorithm to\nthe available competing methods. Our method has a superior performance to the\nsuccessful existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 11:12:30 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Razzaghi", "Parvin", ""]]}, {"id": "1710.04476", "submitter": "Ketan Bacchuwar", "authors": "Ketan Bacchuwar (GE Healthcare, LIGM), Jean Cousty (LIGM), R\\'egis\n  Vaillant (GE Healthcare), Laurent Najman (LIGM)", "title": "VOIDD: automatic vessel of intervention dynamic detection in PCI\n  procedures", "comments": null, "journal-ref": "CVII-Stent Workshop MICCAI 2017, Sep 2017, Quebec City, Canada. 26\n  (6), pp.136 - 157, 2009", "doi": "10.1109/MSP.2009.934154", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the work towards improving the overall workflow\nof the Percutaneous Coronary Interventions (PCI) procedures by capacitating the\nimaging instruments to precisely monitor the steps of the procedure. In the\nlong term, such capabilities can be used to optimize the image acquisition to\nreduce the amount of dose or contrast media employed during the procedure. We\npresent the automatic VOIDD algorithm to detect the vessel of intervention\nwhich is going to be treated during the procedure by combining information from\nthe vessel image with contrast agent injection and images acquired during\nguidewire tip navigation. Due to the robust guidewire tip segmentation method,\nthis algorithm is also able to automatically detect the sequence corresponding\nto guidewire navigation. We present an evaluation methodology which\ncharacterizes the correctness of the guide wire tip detection and correct\nidentification of the vessel navigated during the procedure. On a dataset of\n2213 images from 8 sequences of 4 patients, VOIDD identifies\nvessel-of-intervention with accuracy in the range of 88% or above and absence\nof tip with accuracy in range of 98% or above depending on the test case.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:40:29 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Bacchuwar", "Ketan", "", "GE Healthcare, LIGM"], ["Cousty", "Jean", "", "LIGM"], ["Vaillant", "R\u00e9gis", "", "GE Healthcare"], ["Najman", "Laurent", "", "LIGM"]]}, {"id": "1710.04486", "submitter": "Dominique Vaufreydaz", "authors": "Thomas Guntz (LIG), Raffaella Balzarini (LIG), Dominique Vaufreydaz\n  (LIG, UGA), James L. Crowley (Grenoble INP, LIG)", "title": "Multimodal Observation and Interpretation of Subjects Engaged in Problem\n  Solving", "comments": null, "journal-ref": "1st Workshop on \"Behavior, Emotion and Representation: Building\n  Blocks of Interaction'', Oct 2017, Bielefeld, Germany. 2017", "doi": null, "report-no": null, "categories": "cs.HC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first results of a pilot experiment in the\ncapture and interpretation of multimodal signals of human experts engaged in\nsolving challenging chess problems. Our goal is to investigate the extent to\nwhich observations of eye-gaze, posture, emotion and other physiological\nsignals can be used to model the cognitive state of subjects, and to explore\nthe integration of multiple sensor modalities to improve the reliability of\ndetection of human displays of awareness and emotion. We observed chess players\nengaged in problems of increasing difficulty while recording their behavior.\nSuch recordings can be used to estimate a participant's awareness of the\ncurrent situation and to predict ability to respond effectively to challenging\nsituations. Results show that a multimodal approach is more accurate than a\nunimodal one. By combining body posture, visual attention and emotion, the\nmultimodal approach can reach up to 93% of accuracy when determining player's\nchess expertise while unimodal approach reaches 86%. Finally this experiment\nvalidates the use of our equipment as a general and reproducible tool for the\nstudy of participants engaged in screen-based interaction and/or problem\nsolving.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 12:59:42 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Guntz", "Thomas", "", "LIG"], ["Balzarini", "Raffaella", "", "LIG"], ["Vaufreydaz", "Dominique", "", "LIG, UGA"], ["Crowley", "James L.", "", "Grenoble INP, LIG"]]}, {"id": "1710.04540", "submitter": "Yading Yuan", "authors": "Yading Yuan", "title": "Hierarchical Convolutional-Deconvolutional Neural Networks for Automatic\n  Liver and Tumor Segmentation", "comments": "2017 MICCAI-LiTS challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of liver and its tumors is an essential step for\nextracting quantitative imaging biomarkers for accurate tumor detection,\ndiagnosis, prognosis and assessment of tumor response to treatment. MICCAI 2017\nLiver Tumor Segmentation Challenge (LiTS) provides a common platform for\ncomparing different automatic algorithms on contrast-enhanced abdominal CT\nimages in tasks including 1) liver segmentation, 2) liver tumor segmentation,\nand 3) tumor burden estimation. We participate this challenge by developing a\nhierarchical framework based on deep fully convolutional-deconvolutional neural\nnetworks (CDNN). A simple CDNN model is firstly trained to provide a quick but\ncoarse segmentation of the liver on the entire CT volume, then another CDNN is\napplied to the liver region for fine liver segmentation. At last, the segmented\nliver region, which is enhanced by histogram equalization, is employed as an\nadditional input to the third CDNN for tumor segmentation. Jaccard distance is\nused as loss function when training CDNN models to eliminate the need of sample\nre-weighting. Our framework is trained using the 130 challenge training cases\nprovided by LiTS. The evaluation on the 70 challenge testing cases resulted in\na mean Dice Similarity Coefficient (DSC) of 0.963 for liver segmentation, a\nmean DSC of 0.657 for tumor segmentation, and a root mean square error (RMSE)\nof 0.017 for tumor burden estimation, which ranked our method in the first,\nfifth and third place, respectively\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 14:32:38 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Yuan", "Yading", ""]]}, {"id": "1710.04623", "submitter": "Sibel Tari", "authors": "Venera Adanova and Sibel Tari", "title": "Analysis of planar ornament patterns via motif asymmetry assumption and\n  local connections", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-018-0835-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planar ornaments, a.k.a. wallpapers, are regular repetitive patterns which\nexhibit translational symmetry in two independent directions. There are exactly\n$17$ distinct planar symmetry groups. We present a fully automatic method for\ncomplete analysis of planar ornaments in $13$ of these groups, specifically,\nthe groups called $p6m, \\, p6, \\, p4g, \\,p4m, \\,p4, \\, p31m, \\,p3m, \\, p3, \\,\ncmm, \\, pgg, \\, pg, \\, p2$ and $p1$. Given the image of an ornament fragment,\nwe present a method to simultaneously classify the input into one of the $13$\ngroups and extract the so called fundamental domain (FD), the minimum region\nthat is sufficient to reconstruct the entire ornament. A nice feature of our\nmethod is that even when the given ornament image is a small portion such that\nit does not contain multiple translational units, the symmetry group as well as\nthe fundamental domain can still be defined. This is because, in contrast to\ncommon approach, we do not attempt to first identify a global translational\nrepetition lattice. Though the presented constructions work for quite a wide\nrange of ornament patterns, a key assumption we make is that the perceivable\nmotifs (shapes that repeat) alone do not provide clues for the underlying\nsymmetries of the ornament. In this sense, our main target is the planar\narrangements of asymmetric interlocking shapes, as in the symmetry art of\nEscher.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 17:19:06 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Adanova", "Venera", ""], ["Tari", "Sibel", ""]]}, {"id": "1710.04647", "submitter": "Jia-Bin Huang", "authors": "Dong Li, Jia-Bin Huang, Yali Li, Shengjin Wang, Ming-Hsuan Yang", "title": "Progressive Representation Adaptation for Weakly Supervised Object\n  Localization", "comments": "Project page: https://sites.google.com/site/lidonggg930/wsl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of weakly supervised object localization where only\nimage-level annotations are available for training object detectors. Numerous\nmethods have been proposed to tackle this problem through mining object\nproposals. However, a substantial amount of noise in object proposals causes\nambiguities for learning discriminative object models. Such approaches are\nsensitive to model initialization and often converge to undesirable local\nminimum solutions. In this paper, we propose to overcome these drawbacks by\nprogressive representation adaptation with two main steps: 1) classification\nadaptation and 2) detection adaptation. In classification adaptation, we\ntransfer a pre-trained network to a multi-label classification task for\nrecognizing the presence of a certain object in an image. Through the\nclassification adaptation step, the network learns discriminative\nrepresentations that are specific to object categories of interest. In\ndetection adaptation, we mine class-specific object proposals by exploiting two\nscoring strategies based on the adapted classification network. Class-specific\nproposal mining helps remove substantial noise from the background clutter and\npotential confusion from similar objects. We further refine these proposals\nusing multiple instance learning and segmentation cues. Using these refined\nobject bounding boxes, we fine-tune all the layer of the classification network\nand obtain a fully adapted detection network. We present detailed experimental\nvalidation on the PASCAL VOC and ILSVRC datasets. Experimental results\ndemonstrate that our progressive representation adaptation algorithm performs\nfavorably against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 17:58:30 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Li", "Dong", ""], ["Huang", "Jia-Bin", ""], ["Li", "Yali", ""], ["Wang", "Shengjin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.04681", "submitter": "Koushik Nagasubramanian", "authors": "Koushik Nagasubramanian, Sarah Jones, Soumik Sarkar, Asheesh K. Singh,\n  Arti Singh, Baskar Ganapathysubramanian", "title": "Hyperspectral band selection using genetic algorithm and support vector\n  machines for early identification of charcoal rot disease in soybean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Charcoal rot is a fungal disease that thrives in warm dry conditions and\naffects the yield of soybeans and other important agronomic crops worldwide.\nThere is a need for robust, automatic and consistent early detection and\nquantification of disease symptoms which are important in breeding programs for\nthe development of improved cultivars and in crop production for the\nimplementation of disease control measures for yield protection. Current\nmethods of plant disease phenotyping are predominantly visual and hence are\nslow and prone to human error and variation. There has been increasing interest\nin hyperspectral imaging applications for early detection of disease symptoms.\nHowever, the high dimensionality of hyperspectral data makes it very important\nto have an efficient analysis pipeline in place for the identification of\ndisease so that effective crop management decisions can be made. The focus of\nthis work is to determine the minimal number of most effective hyperspectral\nbands that can distinguish between healthy and diseased specimens early in the\ngrowing season. Healthy and diseased hyperspectral data cubes were captured at\n3, 6, 9, 12, and 15 days after inoculation. We utilized inoculated and control\nspecimens from 4 different genotypes. Each hyperspectral image was captured at\n240 different wavelengths in the range of 383 to 1032 nm. We used a combination\nof genetic algorithm as an optimizer and support vector machines as a\nclassifier for identification of maximally effective band combinations. A\nbinary classification between healthy and infected samples using six selected\nband combinations obtained a classification accuracy of 97% and a F1 score of\n0.97 for the infected class. The results demonstrated that these carefully\nchosen bands are more informative than RGB images, and could be used in a\nmultispectral camera for remote identification of charcoal rot infection in\nsoybean.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 18:27:41 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Nagasubramanian", "Koushik", ""], ["Jones", "Sarah", ""], ["Sarkar", "Soumik", ""], ["Singh", "Asheesh K.", ""], ["Singh", "Arti", ""], ["Ganapathysubramanian", "Baskar", ""]]}, {"id": "1710.04744", "submitter": "Samuel Dodge", "authors": "Samuel Dodge and Lina Karam", "title": "Can the early human visual system compete with Deep Neural Networks?", "comments": "Accepted as an oral paper at the Mutual Benefits of Cognitive and\n  Computer Vision Workshop (held in conjunction with ICCV2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and compare the human visual system and state-of-the-art deep neural\nnetworks on classification of distorted images. Different from previous works,\nwe limit the display time to 100ms to test only the early mechanisms of the\nhuman visual system, without allowing time for any eye movements or other\nhigher level processes. Our findings show that the human visual system still\noutperforms modern deep neural networks under blurry and noisy images. These\nfindings motivate future research into developing more robust deep networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 22:50:00 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Dodge", "Samuel", ""], ["Karam", "Lina", ""]]}, {"id": "1710.04749", "submitter": "Vijay Manikandan Janakiraman", "authors": "Vijay Manikandan Janakiraman", "title": "Explaining Aviation Safety Incidents Using Deep Temporal Multiple\n  Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although aviation accidents are rare, safety incidents occur more frequently\nand require a careful analysis to detect and mitigate risks in a timely manner.\nAnalyzing safety incidents using operational data and producing event-based\nexplanations is invaluable to airline companies as well as to governing\norganizations such as the Federal Aviation Administration (FAA) in the United\nStates. However, this task is challenging because of the complexity involved in\nmining multi-dimensional heterogeneous time series data, the lack of\ntime-step-wise annotation of events in a flight, and the lack of scalable tools\nto perform analysis over a large number of events. In this work, we propose a\nprecursor mining algorithm that identifies events in the multidimensional time\nseries that are correlated with the safety incident. Precursors are valuable to\nsystems health and safety monitoring and in explaining and forecasting safety\nincidents. Current methods suffer from poor scalability to high dimensional\ntime series data and are inefficient in capturing temporal behavior. We propose\nan approach by combining multiple-instance learning (MIL) and deep recurrent\nneural networks (DRNN) to take advantage of MIL's ability to learn using weakly\nsupervised data and DRNN's ability to model temporal behavior. We describe the\nalgorithm, the data, the intuition behind taking a MIL approach, and a\ncomparative analysis of the proposed algorithm with baseline models. We also\ndiscuss the application to a real-world aviation safety problem using data from\na commercial airline company and discuss the model's abilities and\nshortcomings, with some final remarks about possible deployment directions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 23:42:00 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 05:16:08 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Janakiraman", "Vijay Manikandan", ""]]}, {"id": "1710.04773", "submitter": "Stanis{\\l}aw Jastrz\\k{e}bski", "authors": "Stanis{\\l}aw Jastrz\\k{e}bski, Devansh Arpit, Nicolas Ballas, Vikas\n  Verma, Tong Che, Yoshua Bengio", "title": "Residual Connections Encourage Iterative Inference", "comments": "First two authors contributed equally. Published in ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks (Resnets) have become a prominent architecture in deep\nlearning. However, a comprehensive understanding of Resnets is still a topic of\nongoing research.\n  A recent view argues that Resnets perform iterative refinement of features.\nWe attempt to further expose properties of this aspect. To this end, we study\nResnets both analytically and empirically. We formalize the notion of iterative\nrefinement in Resnets by showing that residual connections naturally encourage\nfeatures of residual blocks to move along the negative gradient of loss as we\ngo from one block to the next. In addition, our empirical analysis suggests\nthat Resnets are able to perform both representation learning and iterative\nrefinement. In general, a Resnet block tends to concentrate representation\nlearning behavior in the first few layers while higher layers perform iterative\nrefinement of features. Finally we observe that sharing residual layers naively\nleads to representation explosion and counterintuitively, overfitting, and we\nshow that simple existing strategies can help alleviating this problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 01:39:32 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 18:45:27 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Jastrz\u0119bski", "Stanis\u0142aw", ""], ["Arpit", "Devansh", ""], ["Ballas", "Nicolas", ""], ["Verma", "Vikas", ""], ["Che", "Tong", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.04778", "submitter": "Donghuan Lu", "authors": "Donghuan Lu, Morgan Heisler, Sieun Lee, Gavin Ding, Marinko V. Sarunic\n  and Mirza Faisal Beg", "title": "Retinal Fluid Segmentation and Detection in Optical Coherence Tomography\n  Images using Fully Convolutional Neural Network", "comments": "9 pages, 5 figures, MICCAI Retinal OCT Fluid Challenge 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a non-invasive imaging modality, optical coherence tomography (OCT) can\nprovide micrometer-resolution 3D images of retinal structures. Therefore it is\ncommonly used in the diagnosis of retinal diseases associated with edema in and\nunder the retinal layers. In this paper, a new framework is proposed for the\ntask of fluid segmentation and detection in retinal OCT images. Based on the\nraw images and layers segmented by a graph-cut algorithm, a fully convolutional\nneural network was trained to recognize and label the fluid pixels. Random\nforest classification was performed on the segmented fluid regions to detect\nand reject the falsely labeled fluid regions. The leave-one-out cross\nvalidation experiments on the RETOUCH database show that our method performs\nwell in both segmentation (mean Dice: 0.7317) and detection (mean AUC: 0.985)\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 01:51:01 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Lu", "Donghuan", ""], ["Heisler", "Morgan", ""], ["Lee", "Sieun", ""], ["Ding", "Gavin", ""], ["Sarunic", "Marinko V.", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "1710.04782", "submitter": "Donghuan Lu", "authors": "Donghuan Lu, Karteek Popuri, Weiguang Ding, Rakesh Balachandar and\n  Mirza Faisal Beg", "title": "Multimodal and Multiscale Deep Neural Networks for the Early Diagnosis\n  of Alzheimer's Disease using structural MR and FDG-PET images", "comments": "12 pages, 4 figures, Alzheimer's disease, deep learning, multimodal,\n  early diagnosis, multiscale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's Disease (AD) is a progressive neurodegenerative disease. Amnestic\nmild cognitive impairment (MCI) is a common first symptom before the conversion\nto clinical impairment where the individual becomes unable to perform\nactivities of daily living independently. Although there is currently no\ntreatment available, the earlier a conclusive diagnosis is made, the earlier\nthe potential for interventions to delay or perhaps even prevent progression to\nfull-blown AD. Neuroimaging scans acquired from MRI and metabolism images\nobtained by FDG-PET provide in-vivo view into the structure and function\n(glucose metabolism) of the living brain. It is hypothesized that combining\ndifferent image modalities could better characterize the change of human brain\nand result in a more accuracy early diagnosis of AD. In this paper, we proposed\na novel framework to discriminate normal control(NC) subjects from subjects\nwith AD pathology (AD and NC, MCI subjects convert to AD in future). Our novel\napproach utilizing a multimodal and multiscale deep neural network was found to\ndeliver a 85.68\\% accuracy in the prediction of subjects within 3 years to\nconversion. Cross validation experiments proved that it has better\ndiscrimination ability compared with results in existing published literature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 02:14:39 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Lu", "Donghuan", ""], ["Popuri", "Karteek", ""], ["Ding", "Weiguang", ""], ["Balachandar", "Rakesh", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "1710.04783", "submitter": "Dwarikanath Mahapatra", "authors": "Dwarikanath Mahapatra, Behzad Bozorgtabar", "title": "Retinal Vasculature Segmentation Using Local Saliency Maps and\n  Generative Adversarial Networks For Image Super Resolution", "comments": "Accepted in MICCAI 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image super resolution(ISR) method using generative adversarial\nnetworks (GANs) that takes a low resolution input fundus image and generates a\nhigh resolution super resolved (SR) image upto scaling factor of $16$. This\nfacilitates more accurate automated image analysis, especially for small or\nblurred landmarks and pathologies. Local saliency maps, which define each\npixel's importance, are used to define a novel saliency loss in the GAN cost\nfunction. Experimental results show the resulting SR images have perceptual\nquality very close to the original images and perform better than competing\nmethods that do not weigh pixels according to their importance. When used for\nretinal vasculature segmentation, our SR images result in accuracy levels close\nto those obtained when using the original images.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 02:17:05 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 23:59:28 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 05:24:11 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Mahapatra", "Dwarikanath", ""], ["Bozorgtabar", "Behzad", ""]]}, {"id": "1710.04803", "submitter": "Daksh Thapar", "authors": "Daksh Thapar, Divyansh Aggarwal, Punjal Agarwal and Aditya Nigam", "title": "VGR-Net: A View Invariant Gait Recognition Network", "comments": "Accepted in ISBA (IEEE International conference on Identity, Security\n  and Behaviour Analysis)-2018", "journal-ref": null, "doi": "10.1109/ISBA.2018.8311475", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric identification systems have become immensely popular and important\nbecause of their high reliability and efficiency. However person identification\nat a distance, still remains a challenging problem. Gait can be seen as an\nessential biometric feature for human recognition and identification. It can be\neasily acquired from a distance and does not require any user cooperation thus\nmaking it suitable for surveillance. But the task of recognizing an individual\nusing gait can be adversely affected by varying view points making this task\nmore and more challenging. Our proposed approach tackles this problem by\nidentifying spatio-temporal features and performing extensive experimentation\nand training mechanism. In this paper, we propose a 3-D Convolution Deep Neural\nNetwork for person identification using gait under multiple view. It is a\n2-stage network, in which we have a classification network that initially\nidentifies the viewing point angle. After that another set of networks (one for\neach angle) has been trained to identify the person under a particular viewing\nangle. We have tested this network over CASIA-B publicly available database and\nhave achieved state-of-the-art results. The proposed system is much more\nefficient in terms of time and space and performing better for almost all\nangles.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 05:02:33 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Thapar", "Daksh", ""], ["Aggarwal", "Divyansh", ""], ["Agarwal", "Punjal", ""], ["Nigam", "Aditya", ""]]}, {"id": "1710.04826", "submitter": "Shangxuan Tian", "authors": "Shangxuan Tian, Shijian Lu and Chongshou Li", "title": "WeText: Scene Text Detection under Weak Supervision", "comments": "accepted by ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The requiring of large amounts of annotated training data has become a common\nconstraint on various deep learning systems. In this paper, we propose a weakly\nsupervised scene text detection method (WeText) that trains robust and accurate\nscene text detection models by learning from unannotated or weakly annotated\ndata. With a \"light\" supervised model trained on a small fully annotated\ndataset, we explore semi-supervised and weakly supervised learning on a large\nunannotated dataset and a large weakly annotated dataset, respectively. For the\nunsupervised learning, the light supervised model is applied to the unannotated\ndataset to search for more character training samples, which are further\ncombined with the small annotated dataset to retrain a superior character\ndetection model. For the weakly supervised learning, the character searching is\nguided by high-level annotations of words/text lines that are widely available\nand also much easier to prepare. In addition, we design an unified scene\ncharacter detector by adapting regression based deep networks, which greatly\nrelieves the error accumulation issue that widely exists in most traditional\napproaches. Extensive experiments across different unannotated and weakly\nannotated datasets show that the scene text detection performance can be\nclearly boosted under both scenarios, where the weakly supervised learning can\nachieve the state-of-the-art performance by using only 229 fully annotated\nscene text images.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 07:08:43 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Tian", "Shangxuan", ""], ["Lu", "Shijian", ""], ["Li", "Chongshou", ""]]}, {"id": "1710.04835", "submitter": "Kenji Enomoto", "authors": "Kenji Enomoto, Ken Sakurada, Weimin Wang, Hiroshi Fukui, Masashi\n  Matsuoka, Ryosuke Nakamura, Nobuo Kawaguchi", "title": "Filmy Cloud Removal on Satellite Imagery with Multispectral Conditional\n  Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": "10.1109/CVPRW.2017.197", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for cloud removal from visible light RGB\nsatellite images by extending the conditional Generative Adversarial Networks\n(cGANs) from RGB images to multispectral images. Satellite images have been\nwidely utilized for various purposes, such as natural environment monitoring\n(pollution, forest or rivers), transportation improvement and prompt emergency\nresponse to disasters. However, the obscurity caused by clouds makes it\nunstable to monitor the situation on the ground with the visible light camera.\nImages captured by a longer wavelength are introduced to reduce the effects of\nclouds. Synthetic Aperture Radar (SAR) is such an example that improves\nvisibility even the clouds exist. On the other hand, the spatial resolution\ndecreases as the wavelength increases. Furthermore, the images captured by long\nwavelengths differs considerably from those captured by visible light in terms\nof their appearance. Therefore, we propose a network that can remove clouds and\ngenerate visible light images from the multispectral images taken as inputs.\nThis is achieved by extending the input channels of cGANs to be compatible with\nmultispectral images. The networks are trained to output images that are close\nto the ground truth using the images synthesized with clouds over the ground\ntruth as inputs. In the available dataset, the proportion of images of the\nforest or the sea is very high, which will introduce bias in the training\ndataset if uniformly sampled from the original dataset. Thus, we utilize the\nt-Distributed Stochastic Neighbor Embedding (t-SNE) to improve the problem of\nbias in the training dataset. Finally, we confirm the feasibility of the\nproposed network on the dataset of four bands images, which include three\nvisible light bands and one near-infrared (NIR) band.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:26:13 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Enomoto", "Kenji", ""], ["Sakurada", "Ken", ""], ["Wang", "Weimin", ""], ["Fukui", "Hiroshi", ""], ["Matsuoka", "Masashi", ""], ["Nakamura", "Ryosuke", ""], ["Kawaguchi", "Nobuo", ""]]}, {"id": "1710.04837", "submitter": "Yanwei  Fu", "authors": "Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue, Leonid Sigal, and\n  Shaogang Gong", "title": "Recent Advances in Zero-shot Recognition", "comments": "accepted by IEEE Signal Processing Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent renaissance of deep convolution neural networks, encouraging\nbreakthroughs have been achieved on the supervised recognition tasks, where\neach class has sufficient training data and fully annotated training data.\nHowever, to scale the recognition to a large number of classes with few or now\ntraining samples for each class remains an unsolved problem. One approach to\nscaling up the recognition is to develop models capable of recognizing unseen\ncategories without any training instances, or zero-shot recognition/ learning.\nThis article provides a comprehensive review of existing zero-shot recognition\ntechniques covering various aspects ranging from representations of models, and\nfrom datasets and evaluation settings. We also overview related recognition\ntasks including one-shot and open set recognition which can be used as natural\nextensions of zero-shot recognition when limited number of class samples become\navailable or when zero-shot recognition is implemented in a real-world setting.\nImportantly, we highlight the limitations of existing approaches and point out\nfuture research directions in this existing new research area.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:29:29 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Fu", "Yanwei", ""], ["Xiang", "Tao", ""], ["Jiang", "Yu-Gang", ""], ["Xue", "Xiangyang", ""], ["Sigal", "Leonid", ""], ["Gong", "Shaogang", ""]]}, {"id": "1710.04842", "submitter": "Ylva Jansson", "authors": "Ylva Jansson and Tony Lindeberg", "title": "Dynamic texture recognition using time-causal and time-recursive\n  spatio-temporal receptive fields", "comments": "29 pages, 16 figures", "journal-ref": "Journal of Mathematical Imaging and Vision, 60(9): 1369-1398, 2018", "doi": "10.1007/s10851-018-0826-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a first evaluation of using spatio-temporal receptive\nfields from a recently proposed time-causal spatio-temporal scale-space\nframework as primitives for video analysis. We propose a new family of video\ndescriptors based on regional statistics of spatio-temporal receptive field\nresponses and evaluate this approach on the problem of dynamic texture\nrecognition. Our approach generalises a previously used method, based on joint\nhistograms of receptive field responses, from the spatial to the\nspatio-temporal domain and from object recognition to dynamic texture\nrecognition. The time-recursive formulation enables computationally efficient\ntime-causal recognition. The experimental evaluation demonstrates competitive\nperformance compared to state-of-the-art. Especially, it is shown that binary\nversions of our dynamic texture descriptors achieve improved performance\ncompared to a large range of similar methods using different primitives either\nhandcrafted or learned from data. Further, our qualitative and quantitative\ninvestigation into parameter choices and the use of different sets of receptive\nfields highlights the robustness and flexibility of our approach. Together,\nthese results support the descriptive power of this family of time-causal\nspatio-temporal receptive fields, validate our approach for dynamic texture\nrecognition and point towards the possibility of designing a range of video\nanalysis methods based on these new time-causal spatio-temporal primitives.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:47:13 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 12:10:27 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 12:32:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Jansson", "Ylva", ""], ["Lindeberg", "Tony", ""]]}, {"id": "1710.04934", "submitter": "Muktabh Mayank Srivastava", "authors": "Monika Grewal, Muktabh Mayank Srivastava, Pulkit Kumar, Srikrishna\n  Varadarajan", "title": "RADNET: Radiologist Level Accuracy using Deep Learning for HEMORRHAGE\n  detection in CT Scans", "comments": "Accepted at IEEE Symposium on Biomedical Imaging (ISBI) 2018 as\n  conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a deep learning approach for automated brain hemorrhage detection\nfrom computed tomography (CT) scans. Our model emulates the procedure followed\nby radiologists to analyse a 3D CT scan in real-world. Similar to radiologists,\nthe model sifts through 2D cross-sectional slices while paying close attention\nto potential hemorrhagic regions. Further, the model utilizes 3D context from\nneighboring slices to improve predictions at each slice and subsequently,\naggregates the slice-level predictions to provide diagnosis at CT level. We\nrefer to our proposed approach as Recurrent Attention DenseNet (RADnet) as it\nemploys original DenseNet architecture along with adding the components of\nattention for slice level predictions and recurrent neural network layer for\nincorporating 3D context. The real-world performance of RADnet has been\nbenchmarked against independent analysis performed by three senior radiologists\nfor 77 brain CTs. RADnet demonstrates 81.82% hemorrhage prediction accuracy at\nCT level that is comparable to radiologists. Further, RADnet achieves higher\nrecall than two of the three radiologists, which is remarkable.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:14:39 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 12:05:54 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Grewal", "Monika", ""], ["Srivastava", "Muktabh Mayank", ""], ["Kumar", "Pulkit", ""], ["Varadarajan", "Srikrishna", ""]]}, {"id": "1710.04943", "submitter": "Bernhard Bermeitinger", "authors": "Bernhard Bermeitinger, Maria Christoforaki, Simon Donig, Siegfried\n  Handschuh", "title": "Object Classification in Images of Neoclassical Artifacts Using Deep\n  Learning", "comments": "Published in Digital Humanities 2017, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we report on our efforts for using Deep Learning for\nclassifying artifacts and their features in digital visuals as a part of the\nNeoclassica framework. It was conceived to provide scholars with new methods\nfor analyzing and classifying artifacts and aesthetic forms from the era of\nClassicism. The framework accommodates both traditional knowledge\nrepresentation as a formal ontology and data-driven knowledge discovery, where\ncultural patterns will be identified by means of algorithms in statistical\nanalysis and machine learning. We created a Deep Learning approach trained on\nphotographs to classify the objects inside these photographs. In a next step,\nwe will apply a different Deep Learning approach. It is capable of locating\nmultiple objects inside an image and classifying them with a high accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 14:35:27 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Bermeitinger", "Bernhard", ""], ["Christoforaki", "Maria", ""], ["Donig", "Simon", ""], ["Handschuh", "Siegfried", ""]]}, {"id": "1710.05006", "submitter": "Noel Codella", "authors": "Noel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael\n  A. Marchetti, Stephen W. Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin\n  Mishra, Harald Kittler, Allan Halpern", "title": "Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017\n  International Symposium on Biomedical Imaging (ISBI), Hosted by the\n  International Skin Imaging Collaboration (ISIC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the design, implementation, and results of the latest\ninstallment of the dermoscopic image analysis benchmark challenge. The goal is\nto support research and development of algorithms for automated diagnosis of\nmelanoma, the most lethal skin cancer. The challenge was divided into 3 tasks:\nlesion segmentation, feature detection, and disease classification.\nParticipation involved 593 registrations, 81 pre-submissions, 46 finalized\nsubmissions (including a 4-page manuscript), and approximately 50 attendees,\nmaking this the largest standardized and comparative study in this field to\ndate. While the official challenge duration and ranking of participants has\nconcluded, the dataset snapshots remain available for further research and\ndevelopment.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 17:08:53 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 20:42:31 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 16:37:20 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Codella", "Noel C. F.", ""], ["Gutman", "David", ""], ["Celebi", "M. Emre", ""], ["Helba", "Brian", ""], ["Marchetti", "Michael A.", ""], ["Dusza", "Stephen W.", ""], ["Kalloo", "Aadi", ""], ["Liopyris", "Konstantinos", ""], ["Mishra", "Nabin", ""], ["Kittler", "Harald", ""], ["Halpern", "Allan", ""]]}, {"id": "1710.05027", "submitter": "Shadrokh Samavi", "authors": "Eman Alibeigi, Shadrokh Samavi, Shahram Shirani, Zahra Rahmani", "title": "Real time ridge orientation estimation for fingerprint images", "comments": "8 pages, 15 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fingerprint verification is an important bio-metric technique for personal\nidentification. Most of the automatic verification systems are based on\nmatching of fingerprint minutiae. Extraction of minutiae is an essential\nprocess which requires estimation of orientation of the lines in an image. Most\nof the existing methods involve intense mathematical computations and hence are\nperformed through software means. In this paper a hardware scheme to perform\nreal time orientation estimation is presented which is based on pipelined\narchitecture. Synthesized circuits proved the functionality and accuracy of the\nsuggested method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 15:23:18 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Alibeigi", "Eman", ""], ["Samavi", "Shadrokh", ""], ["Shirani", "Shahram", ""], ["Rahmani", "Zahra", ""]]}, {"id": "1710.05073", "submitter": "Wuming Zhang", "authors": "Wuming Zhang, Xi Zhao, Jean-Marie Morvan, Liming Chen", "title": "Improving Shadow Suppression for Illumination Robust Face Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D face analysis techniques, such as face landmarking, face recognition and\nface verification, are reasonably dependent on illumination conditions which\nare usually uncontrolled and unpredictable in the real world. An illumination\nrobust preprocessing method thus remains a significant challenge in reliable\nface analysis. In this paper we propose a novel approach for improving lighting\nnormalization through building the underlying reflectance model which\ncharacterizes interactions between skin surface, lighting source and camera\nsensor, and elaborates the formation of face color appearance. Specifically,\nthe proposed illumination processing pipeline enables the generation of\nChromaticity Intrinsic Image (CII) in a log chromaticity space which is robust\nto illumination variations. Moreover, as an advantage over most prevailing\nmethods, a photo-realistic color face image is subsequently reconstructed which\neliminates a wide variety of shadows whilst retaining the color information and\nidentity details. Experimental results under different scenarios and using\nvarious face databases show the effectiveness of the proposed approach to deal\nwith lighting variations, including both soft and hard shadows, in face\nrecognition.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 20:56:23 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhang", "Wuming", ""], ["Zhao", "Xi", ""], ["Morvan", "Jean-Marie", ""], ["Chen", "Liming", ""]]}, {"id": "1710.05104", "submitter": "Farnoosh Ghadiri", "authors": "Farnoosh Ghadiri, Robert Bergevin, Masoud Shafiee", "title": "An adaptive thresholding approach for automatic optic disk segmentation", "comments": "Accepted in VISAPP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optic disk segmentation is a prerequisite step in automatic retinal screening\nsystems. In this paper, we propose an algorithm for optic disk segmentation\nbased on a local adaptive thresholding method. Location of the optic disk is\nvalidated by intensity and average vessel width of retinal images. Then an\nadaptive thresholding is applied on the temporal and nasal part of the optic\ndisc separately. Adaptive thresholding, makes our algorithm robust to\nillumination variations and various image acquisition conditions. Moreover,\nexperimental results on the DRIVE and KHATAM databases show promising results\ncompared to the recent literature. In the DRIVE database, the optic disk in all\nimages is correctly located and the mean overlap reached to 43.21%. The optic\ndisk is correctly detected in 98% of the images with the mean overlap of 36.32%\nin the KHATAM database.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 00:04:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Ghadiri", "Farnoosh", ""], ["Bergevin", "Robert", ""], ["Shafiee", "Masoud", ""]]}, {"id": "1710.05106", "submitter": "Yuxin Peng", "authors": "Yuxin Peng, Jinwei Qi and Yuxin Yuan", "title": "CM-GANs: Cross-modal Generative Adversarial Networks for Common\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the inconsistent distribution and representation of\ndifferent modalities, such as image and text, cause the heterogeneity gap that\nmakes it challenging to correlate such heterogeneous data. Generative\nadversarial networks (GANs) have shown its strong ability of modeling data\ndistribution and learning discriminative representation, existing GANs-based\nworks mainly focus on generative problem to generate new data. We have\ndifferent goal, aim to correlate heterogeneous data, by utilizing the power of\nGANs to model cross-modal joint distribution. Thus, we propose Cross-modal GANs\nto learn discriminative common representation for bridging heterogeneity gap.\nThe main contributions are: (1) Cross-modal GANs architecture is proposed to\nmodel joint distribution over data of different modalities. The inter-modality\nand intra-modality correlation can be explored simultaneously in generative and\ndiscriminative models. Both of them beat each other to promote cross-modal\ncorrelation learning. (2) Cross-modal convolutional autoencoders with\nweight-sharing constraint are proposed to form generative model. They can not\nonly exploit cross-modal correlation for learning common representation, but\nalso preserve reconstruction information for capturing semantic consistency\nwithin each modality. (3) Cross-modal adversarial mechanism is proposed, which\nutilizes two kinds of discriminative models to simultaneously conduct\nintra-modality and inter-modality discrimination. They can mutually boost to\nmake common representation more discriminative by adversarial training process.\nTo the best of our knowledge, our proposed CM-GANs approach is the first to\nutilize GANs to perform cross-modal common representation learning. Experiments\nare conducted to verify the performance of our proposed approach on cross-modal\nretrieval paradigm, compared with 10 methods on 3 cross-modal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 00:15:56 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 16:38:56 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Peng", "Yuxin", ""], ["Qi", "Jinwei", ""], ["Yuan", "Yuxin", ""]]}, {"id": "1710.05112", "submitter": "Aaron Chadha", "authors": "Aaron Chadha, Alhabib Abbas and Yiannis Andreopoulos", "title": "Video Classification With CNNs: Using The Codec As A Spatio-Temporal\n  Activity Sensor", "comments": "Accepted in IEEE Transactions on Circuits and Systems for Video\n  Technology. Extension of ICIP 2017 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate video classification via a two-stream convolutional neural\nnetwork (CNN) design that directly ingests information extracted from\ncompressed video bitstreams. Our approach begins with the observation that all\nmodern video codecs divide the input frames into macroblocks (MBs). We\ndemonstrate that selective access to MB motion vector (MV) information within\ncompressed video bitstreams can also provide for selective, motion-adaptive, MB\npixel decoding (a.k.a., MB texture decoding). This in turn allows for the\nderivation of spatio-temporal video activity regions at extremely high speed in\ncomparison to conventional full-frame decoding followed by optical flow\nestimation. In order to evaluate the accuracy of a video classification\nframework based on such activity data, we independently train two CNN\narchitectures on MB texture and MV correspondences and then fuse their scores\nto derive the final classification of each test video. Evaluation on two\nstandard datasets shows that the proposed approach is competitive to the best\ntwo-stream video classification approaches found in the literature. At the same\ntime: (i) a CPU-based realization of our MV extraction is over 977 times faster\nthan GPU-based optical flow methods; (ii) selective decoding is up to 12 times\nfaster than full-frame decoding; (iii) our proposed spatial and temporal CNNs\nperform inference at 5 to 49 times lower cloud computing cost than the fastest\nmethods from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 00:43:18 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 22:11:49 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Chadha", "Aaron", ""], ["Abbas", "Alhabib", ""], ["Andreopoulos", "Yiannis", ""]]}, {"id": "1710.05126", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Hierarchical semantic segmentation using modular convolutional neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Image recognition tasks that involve identifying parts of an object or the\ncontents of a vessel can be viewed as a hierarchical problem, which can be\nsolved by initial recognition of the main object, followed by recognition of\nits parts or contents. To achieve such modular recognition, it is necessary to\nuse the output of one recognition method (which identifies the general object)\nas the input for a second method (which identifies the parts or contents). In\nrecent years, convolutional neural networks have emerged as the dominant method\nfor segmentation and classification of images. This work examines a method for\nserially connecting convolutional neural networks for semantic segmentation of\nmaterials inside transparent vessels. It applies one fully convolutional neural\nnet to segment the image into vessel and background, and the vessel region is\nused as an input for a second net which recognizes the contents of the glass\nvessel. Transferring the segmentation map generated by the first nets to the\nsecond net was performed using the valve filter attention method that involves\nusing different filters on different segments of the image. This modular\nsemantic segmentation method outperforms a single step method in which both the\nvessel and its contents are identified using a single net. An advantage of the\nmodular neural net is that it allows networks to be built from existing trained\nmodules, as well the transfer and reuse of trained net modules without the need\nfor any retraining of the assembled net.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 02:44:31 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1710.05152", "submitter": "Avantika Singh Ms", "authors": "Avantika Singh, Vishesh Mistry, Dhananjay Yadav, Aditya Nigam", "title": "GHCLNet: A Generalized Hierarchically tuned Contact Lens detection\n  Network", "comments": "Accepted in ISBA 2018: International Conference on Identity, Security\n  and Behavior Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iris serves as one of the best biometric modality owing to its complex,\nunique and stable structure. However, it can still be spoofed using fabricated\neyeballs and contact lens. Accurate identification of contact lens is must for\nreliable performance of any biometric authentication system based on this\nmodality. In this paper, we present a novel approach for detecting contact lens\nusing a Generalized Hierarchically tuned Contact Lens detection Network\n(GHCLNet) . We have proposed hierarchical architecture for three class oculus\nclassification namely: no lens, soft lens and cosmetic lens. Our network\narchitecture is inspired by ResNet-50 model. This network works on raw input\niris images without any pre-processing and segmentation requirement and this is\none of its prodigious strength. We have performed extensive experimentation on\ntwo publicly available data-sets namely: 1)IIIT-D 2)ND and on IIT-K data-set\n(not publicly available) to ensure the generalizability of our network. The\nproposed architecture results are quite promising and outperforms the available\nstate-of-the-art lens detection algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 08:27:13 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Singh", "Avantika", ""], ["Mistry", "Vishesh", ""], ["Yadav", "Dhananjay", ""], ["Nigam", "Aditya", ""]]}, {"id": "1710.05158", "submitter": "Daksh Thapar", "authors": "Tushar Gupta, Shreyas Malakarjun Patil, Mukkaram Tailor, Daksh Thapar\n  and Aditya Nigam", "title": "BrainSegNet : A Segmentation Network for Human Brain Fiber Tractography\n  Data into Anatomically Meaningful Clusters", "comments": "Deep Learning in Irregular Domains - British Machine Vision\n  Conference (DLID-BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The segregation of brain fiber tractography data into distinct and\nanatomically meaningful clusters can help to comprehend the complex brain\nstructure and early investigation and management of various neural disorders.\nWe propose a novel stacked bidirectional long short-term memory(LSTM) based\nsegmentation network, (BrainSegNet) for human brain fiber tractography data\nclassification. We perform a two-level hierarchical classification a) White vs\nGrey matter (Macro) and b) White matter clusters (Micro). BrainSegNet is\ntrained over three brain tractography data having over 250,000 fibers each. Our\nexperimental evaluation shows that our model achieves state-of-the-art results.\nWe have performed inter as well as intra class testing over three patient's\nbrain tractography data and achieved a high classification accuracy for both\nmacro and micro levels both under intra as well as inter brain testing\nscenario.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 09:43:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Gupta", "Tushar", ""], ["Patil", "Shreyas Malakarjun", ""], ["Tailor", "Mukkaram", ""], ["Thapar", "Daksh", ""], ["Nigam", "Aditya", ""]]}, {"id": "1710.05172", "submitter": "Runmin Cong", "authors": "Runmin Cong, Jianjun Lei, Huazhu Fu, Qingming Huang, Xiaochun Cao,\n  Chunping Hou", "title": "Co-saliency Detection for RGBD Images Based on Multi-constraint Feature\n  Matching and Cross Label Propagation", "comments": "11 pages, 8 figures, Accepted by IEEE Transactions on Image\n  Processing, Project URL: https://rmcong.github.io/proj_RGBD_cosal.html", "journal-ref": null, "doi": "10.1109/TIP.2017.2763819", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-saliency detection aims at extracting the common salient regions from an\nimage group containing two or more relevant images. It is a newly emerging\ntopic in computer vision community. Different from the most existing\nco-saliency methods focusing on RGB images, this paper proposes a novel\nco-saliency detection model for RGBD images, which utilizes the depth\ninformation to enhance identification of co-saliency. First, the intra saliency\nmap for each image is generated by the single image saliency model, while the\ninter saliency map is calculated based on the multi-constraint feature\nmatching, which represents the constraint relationship among multiple images.\nThen, the optimization scheme, namely Cross Label Propagation (CLP), is used to\nrefine the intra and inter saliency maps in a cross way. Finally, all the\noriginal and optimized saliency maps are integrated to generate the final\nco-saliency result. The proposed method introduces the depth information and\nmulti-constraint feature matching to improve the performance of co-saliency\ndetection. Moreover, the proposed method can effectively exploit any existing\nsingle image saliency model to work well in co-saliency scenarios. Experiments\non two RGBD co-saliency datasets demonstrate the effectiveness of our proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 12:28:35 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Cong", "Runmin", ""], ["Lei", "Jianjun", ""], ["Fu", "Huazhu", ""], ["Huang", "Qingming", ""], ["Cao", "Xiaochun", ""], ["Hou", "Chunping", ""]]}, {"id": "1710.05174", "submitter": "Runmin Cong", "authors": "Runmin Cong, Jianjun Lei, Changqing Zhang, Qingming Huang, Xiaochun\n  Cao, Chunping Hou", "title": "Saliency Detection for Stereoscopic Images Based on Depth Confidence\n  Analysis and Multiple Cues Fusion", "comments": "5 pages, 6 figures, Published on IEEE Signal Processing Letters 2016,\n  Project URL: https://rmcong.github.io/proj_RGBD_sal.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereoscopic perception is an important part of human visual system that\nallows the brain to perceive depth. However, depth information has not been\nwell explored in existing saliency detection models. In this letter, a novel\nsaliency detection method for stereoscopic images is proposed. Firstly, we\npropose a measure to evaluate the reliability of depth map, and use it to\nreduce the influence of poor depth map on saliency detection. Then, the input\nimage is represented as a graph, and the depth information is introduced into\ngraph construction. After that, a new definition of compactness using color and\ndepth cues is put forward to compute the compactness saliency map. In order to\ncompensate the detection errors of compactness saliency when the salient\nregions have similar appearances with background, foreground saliency map is\ncalculated based on depth-refined foreground seeds selection mechanism and\nmultiple cues contrast. Finally, these two saliency maps are integrated into a\nfinal saliency map through weighted-sum method according to their importance.\nExperiments on two publicly available stereo datasets demonstrate that the\nproposed method performs better than other 10 state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 12:34:10 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Cong", "Runmin", ""], ["Lei", "Jianjun", ""], ["Zhang", "Changqing", ""], ["Huang", "Qingming", ""], ["Cao", "Xiaochun", ""], ["Hou", "Chunping", ""]]}, {"id": "1710.05179", "submitter": "Hyeonwoo Noh", "authors": "Hyeonwoo Noh, Tackgeun You, Jonghwan Mun, Bohyung Han", "title": "Regularizing Deep Neural Networks by Noise: Its Interpretation and\n  Optimization", "comments": "NIPS 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overfitting is one of the most critical challenges in deep neural networks,\nand there are various types of regularization methods to improve generalization\nperformance. Injecting noises to hidden units during training, e.g., dropout,\nis known as a successful regularizer, but it is still not clear enough why such\ntraining techniques work well in practice and how we can maximize their benefit\nin the presence of two conflicting objectives---optimizing to true data\ndistribution and preventing overfitting by regularization. This paper addresses\nthe above issues by 1) interpreting that the conventional training methods with\nregularization by noise injection optimize the lower bound of the true\nobjective and 2) proposing a technique to achieve a tighter lower bound using\nmultiple noise samples per training example in a stochastic gradient descent\niteration. We demonstrate the effectiveness of our idea in several computer\nvision applications.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 13:10:59 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 13:50:43 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Noh", "Hyeonwoo", ""], ["You", "Tackgeun", ""], ["Mun", "Jonghwan", ""], ["Han", "Bohyung", ""]]}, {"id": "1710.05191", "submitter": "Noushin Eftekhari", "authors": "Noushin Eftekheri, Mojtaba Masoudi, Hamidreza Pourreza, Kamaledin\n  Ghiasi Shirazi, Ehsan Saeedi", "title": "Microaneurysm Detection in Fundus Images Using a Two-step Convolutional\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetic Retinopathy (DR) is a prominent cause of blindness in the world. The\nearly treatment of DR can be conducted from detection of microaneurysms (MAs)\nwhich appears as reddish spots in retinal images. An automated microaneurysm\ndetection can be a helpful system for ophthalmologists. In this paper, deep\nlearning, in particular convolutional neural network (CNN), is used as a\npowerful tool to efficiently detect MAs from fundus images. In our method a new\ntechnique is used to utilise a two-stage training process which results in an\naccurate detection, while decreasing computational complexity in comparison\nwith previous works. To validate our proposed method, an experiment is\nconducted using Keras library to implement our proposed CNN on two standard\npublicly available datasets. Our results show a promising sensitivity value of\nabout 0.8 at the average number of false positive per image greater than 6\nwhich is a competitive value with the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:15:56 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 12:01:27 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Eftekheri", "Noushin", ""], ["Masoudi", "Mojtaba", ""], ["Pourreza", "Hamidreza", ""], ["Shirazi", "Kamaledin Ghiasi", ""], ["Saeedi", "Ehsan", ""]]}, {"id": "1710.05193", "submitter": "Zutao Jiang", "authors": "Zutao Jiang, Jihua Zhu, Georgios D. Evangelidis, Changqing Zhang,\n  Shanmin Pang, Yaochen Li", "title": "K-means clustering for efficient and robust registration of multi-view\n  point sets", "comments": null, "journal-ref": null, "doi": "10.1016/j.ins.2019.03.024", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generally, there are three main factors that determine the practical\nusability of registration, i.e., accuracy, robustness, and efficiency. In\nreal-time applications, efficiency and robustness are more important. To\npromote these two abilities, we cast the multi-view registration into a\nclustering task. All the centroids are uniformly sampled from the initially\naligned point sets involved in the multi-view registration, which makes it\nrather efficient and effective for the clustering. Then, each point is assigned\nto a single cluster and each cluster centroid is updated accordingly.\nSubsequently, the shape comprised by all cluster centroids is used to\nsequentially estimate the rigid transformation for each point set. For accuracy\nand stability, clustering and transformation estimation are alternately and\niteratively applied to all point sets. We tested our proposed approach on\nseveral benchmark datasets and compared it with state-of-the-art approaches.\nExperimental results validate its efficiency and robustness for the\nregistration of multi-view point sets.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 14:29:14 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 07:52:11 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 13:15:15 GMT"}, {"version": "v4", "created": "Mon, 30 Apr 2018 08:42:34 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Jiang", "Zutao", ""], ["Zhu", "Jihua", ""], ["Evangelidis", "Georgios D.", ""], ["Zhang", "Changqing", ""], ["Pang", "Shanmin", ""], ["Li", "Yaochen", ""]]}, {"id": "1710.05221", "submitter": "Rajer Sindhu", "authors": "Rajer Sindhu, Jayesh Ananya", "title": "An Adaptive Framework for Missing Depth Inference Using Joint Bilateral\n  Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth imaging has largely focused on sensor and intrinsics properties.\nHowever, the accuracy of acquire pixel is largely dependent on the capture. We\npropose a new depth estimation and approximation algorithm which takes an\narbitrary 3D point cloud as input, with potentially complex geometric\nstructures, and generates automatically a bounding box which is used to clamp\nthe 3D distribution into a valid range. We then infer the desired compact\ngeometric network from complex 3D geometries by using a series of adaptive\njoint bilateral filters. Our approach leverages these input depth in the\nconstruction of a compact descriptive adaptive filter framework. The built\nsystem that allows a user to control the result of capture depth map to fit the\ntarget geometry. In addition, it is desirable to visualize structurally\nproblematic areas of the depth data in a dynamic environment. To provide this\nfeature, we investigate a fast update algorithm for the fragility of each\npixel's corresponding 3D point using machine learning. We present a new for of\nfeature vector analysis and demonstrate the effectiveness in the dataset. In\nour experiment, we demonstrate the practicality and benefits of our proposed\nmethod by computing accurate solutions captured depth map from different types\nof sensors and shows better results than existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Oct 2017 18:45:04 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Sindhu", "Rajer", ""], ["Ananya", "Jayesh", ""]]}, {"id": "1710.05267", "submitter": "Ouri Cohen", "authors": "Ouri Cohen, Bo Zhu, Matthew S. Rosen", "title": "MR fingerprinting Deep RecOnstruction NEtwork (DRONE)", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PURPOSE: Demonstrate a novel fast method for reconstruction of\nmulti-dimensional MR Fingerprinting (MRF) data using Deep Learning methods.\n  METHODS: A neural network (NN) is defined using the TensorFlow framework and\ntrained on simulated MRF data computed using the Bloch equations. The accuracy\nof the NN reconstruction of noisy data is compared to conventional MRF template\nmatching as a function of training data size, and quantified in a both\nsimulated numerical brain phantom data and acquired data from the ISMRM/NIST\nphantom. The utility of the method is demonstrated in a healthy subject in vivo\nat 1.5 T.\n  RESULTS: Network training required 10 minutes and once trained, data\nreconstruction required approximately 10 ms. Reconstruction of simulated brain\ndata using the NN resulted in a root-mean-square error (RMSE) of 3.5 ms for T1\nand 7.8 ms for T2. The RMSE for the NN trained on sparse dictionaries was\napproximately 6 fold lower for T1 and 2 fold lower for T2 than conventional MRF\ndot-product dictionary matching on the same dictionaries. Phantom measurements\nyielded good agreement (R2=0.99) between the T1 and T2 estimated by the NN and\nreference values from the ISMRM/NIST phantom.\n  CONCLUSION: Reconstruction of MRF data with a NN is accurate, 300 fold faster\nand more robust to noise and undersampling than conventional MRF dictionary\nmatching.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 02:58:14 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 01:58:23 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 21:51:19 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Cohen", "Ouri", ""], ["Zhu", "Bo", ""], ["Rosen", "Matthew S.", ""]]}, {"id": "1710.05268", "submitter": "Frederik Ebert", "authors": "Frederik Ebert, Chelsea Finn, Alex X. Lee, Sergey Levine", "title": "Self-Supervised Visual Planning with Temporal Skip Connections", "comments": "accepted at the Conference on Robot Learning (CoRL) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to autonomously learn wide repertoires of complex skills, robots\nmust be able to learn from their own autonomously collected data, without human\nsupervision. One learning signal that is always available for autonomously\ncollected data is prediction: if a robot can learn to predict the future, it\ncan use this predictive model to take actions to produce desired outcomes, such\nas moving an object to a particular location. However, in complex open-world\nscenarios, designing a representation for prediction is difficult. In this\nwork, we instead aim to enable self-supervised robotic learning through direct\nvideo prediction: instead of attempting to design a good representation, we\ndirectly predict what the robot will see next, and then use this model to\nachieve desired goals. A key challenge in video prediction for robotic\nmanipulation is handling complex spatial arrangements such as occlusions. To\nthat end, we introduce a video prediction model that can keep track of objects\nthrough occlusion by incorporating temporal skip-connections. Together with a\nnovel planning criterion and action space formulation, we demonstrate that this\nmodel substantially outperforms prior work on video prediction-based control.\nOur results show manipulation of objects not seen during training, handling\nmultiple objects, and pushing objects around obstructions. These results\nrepresent a significant advance in the range and complexity of skills that can\nbe performed entirely with self-supervised robotic learning.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 02:58:20 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ebert", "Frederik", ""], ["Finn", "Chelsea", ""], ["Lee", "Alex X.", ""], ["Levine", "Sergey", ""]]}, {"id": "1710.05285", "submitter": "Haipeng Zeng", "authors": "Haipeng Zeng, Hammad Haleem, Xavier Plantaz, Nan Cao, Huamin Qu", "title": "CNNComparator: Comparative Analytics of Convolutional Neural Networks", "comments": "5 pages. This paper has been accepted by VADL 2017: Workshop on\n  Visual Analytics for Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are widely used in many image\nrecognition tasks due to their extraordinary performance. However, training a\ngood CNN model can still be a challenging task. In a training process, a CNN\nmodel typically learns a large number of parameters over time, which usually\nresults in different performance. Often, it is difficult to explore the\nrelationships between the learned parameters and the model performance due to a\nlarge number of parameters and different random initializations. In this paper,\nwe present a visual analytics approach to compare two different snapshots of a\ntrained CNN model taken after different numbers of epochs, so as to provide\nsome insight into the design or the training of a better CNN model. Our system\ncompares snapshots by exploring the differences in operation parameters and the\ncorresponding blob data at different levels. A case study has been conducted to\ndemonstrate the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 06:43:29 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zeng", "Haipeng", ""], ["Haleem", "Hammad", ""], ["Plantaz", "Xavier", ""], ["Cao", "Nan", ""], ["Qu", "Huamin", ""]]}, {"id": "1710.05311", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Vector Quantization using the Improved Differential Evolution Algorithm\n  for Image Compression", "comments": "11 pages", "journal-ref": null, "doi": "10.17605/OSF.IO/M9RNZ", "report-no": null, "categories": "cs.CV cs.MM cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector Quantization, VQ is a popular image compression technique with a\nsimple decoding architecture and high compression ratio. Codebook designing is\nthe most essential part in Vector Quantization. LindeBuzoGray, LBG is a\ntraditional method of generation of VQ Codebook which results in lower PSNR\nvalue. A Codebook affects the quality of image compression, so the choice of an\nappropriate codebook is a must. Several optimization techniques have been\nproposed for global codebook generation to enhance the quality of image\ncompression. In this paper, a novel algorithm called IDE-LBG is proposed which\nuses Improved Differential Evolution Algorithm coupled with LBG for generating\noptimum VQ Codebooks. The proposed IDE works better than the traditional DE\nwith modifications in the scaling factor and the boundary control mechanism.\nThe IDE generates better solutions by efficient exploration and exploitation of\nthe search space. Then the best optimal solution obtained by the IDE is\nprovided as the initial Codebook for the LBG. This approach produces an\nefficient Codebook with less computational time and the consequences include\nexcellent PSNR values and superior quality reconstructed images. It is observed\nthat the proposed IDE-LBG find better VQ Codebooks as compared to IPSO-LBG,\nBA-LBG and FA-LBG.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 10:31:52 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "1710.05379", "submitter": "Shuqing Chen", "authors": "Shuqing Chen, Holger Roth, Sabrina Dorn, Matthias May, Alexander\n  Cavallaro, Michael M. Lell, Marc Kachelrie{\\ss}, Hirohisa Oda, Kensaku Mori,\n  Andreas Maier", "title": "Towards Automatic Abdominal Multi-Organ Segmentation in Dual Energy CT\n  using Cascaded 3D Fully Convolutional Network", "comments": "5 pagens, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic multi-organ segmentation of the dual energy computed tomography\n(DECT) data can be beneficial for biomedical research and clinical\napplications. However, it is a challenging task. Recent advances in deep\nlearning showed the feasibility to use 3-D fully convolutional networks (FCN)\nfor voxel-wise dense predictions in single energy computed tomography (SECT).\nIn this paper, we proposed a 3D FCN based method for automatic multi-organ\nsegmentation in DECT. The work was based on a cascaded FCN and a general model\nfor the major organs trained on a large set of SECT data. We preprocessed the\nDECT data by using linear weighting and fine-tuned the model for the DECT data.\nThe method was evaluated using 42 torso DECT data acquired with a clinical\ndual-source CT system. Four abdominal organs (liver, spleen, left and right\nkidneys) were evaluated. Cross-validation was tested. Effect of the weight on\nthe accuracy was researched. In all the tests, we achieved an average Dice\ncoefficient of 93% for the liver, 90% for the spleen, 91% for the right kidney\nand 89% for the left kidney, respectively. The results show our method is\nfeasible and promising.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 18:56:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Chen", "Shuqing", ""], ["Roth", "Holger", ""], ["Dorn", "Sabrina", ""], ["May", "Matthias", ""], ["Cavallaro", "Alexander", ""], ["Lell", "Michael M.", ""], ["Kachelrie\u00df", "Marc", ""], ["Oda", "Hirohisa", ""], ["Mori", "Kensaku", ""], ["Maier", "Andreas", ""]]}, {"id": "1710.05381", "submitter": "Mateusz Buda", "authors": "Mateusz Buda, Atsuto Maki, Maciej A. Mazurowski", "title": "A systematic study of the class imbalance problem in convolutional\n  neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2018.07.011", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we systematically investigate the impact of class imbalance on\nclassification performance of convolutional neural networks (CNNs) and compare\nfrequently used methods to address the issue. Class imbalance is a common\nproblem that has been comprehensively studied in classical machine learning,\nyet very limited systematic research is available in the context of deep\nlearning. In our study, we use three benchmark datasets of increasing\ncomplexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of\nimbalance on classification and perform an extensive comparison of several\nmethods to address the issue: oversampling, undersampling, two-phase training,\nand thresholding that compensates for prior class probabilities. Our main\nevaluation metric is area under the receiver operating characteristic curve\n(ROC AUC) adjusted to multi-class tasks since overall accuracy metric is\nassociated with notable difficulties in the context of imbalanced data. Based\non results from our experiments we conclude that (i) the effect of class\nimbalance on classification performance is detrimental; (ii) the method of\naddressing class imbalance that emerged as dominant in almost all analyzed\nscenarios was oversampling; (iii) oversampling should be applied to the level\nthat completely eliminates the imbalance, whereas the optimal undersampling\nratio depends on the extent of imbalance; (iv) as opposed to some classical\nmachine learning models, oversampling does not cause overfitting of CNNs; (v)\nthresholding should be applied to compensate for prior class probabilities when\noverall number of properly classified cases is of interest.\n", "versions": [{"version": "v1", "created": "Sun, 15 Oct 2017 19:01:43 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 02:02:17 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Buda", "Mateusz", ""], ["Maki", "Atsuto", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1710.05477", "submitter": "Bin Li", "authors": "Bin Li, Hu Luo, Haoxin Zhang, Shunquan Tan, Zhongzhou Ji", "title": "A multi-branch convolutional neural network for detecting double JPEG\n  compression", "comments": "This paper was accepted by the 3rd International Workshop on Digital\n  Crime and Forensics (IWDCF2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of double JPEG compression is important to forensics analysis. A\nfew methods were proposed based on convolutional neural networks (CNNs). These\nmethods only accept inputs from pre-processed data, such as histogram features\nand/or decompressed images. In this paper, we present a CNN solution by using\nraw DCT (discrete cosine transformation) coefficients from JPEG images as\ninput. Considering the DCT sub-band nature in JPEG, a multiple-branch CNN\nstructure has been designed to reveal whether a JPEG format image has been\ndoubly compressed. Comparing with previous methods, the proposed method\nprovides end-to-end detection capability. Extensive experiments have been\ncarried out to demonstrate the effectiveness of the proposed network.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 02:54:57 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Li", "Bin", ""], ["Luo", "Hu", ""], ["Zhang", "Haoxin", ""], ["Tan", "Shunquan", ""], ["Ji", "Zhongzhou", ""]]}, {"id": "1710.05512", "submitter": "Roberto Calandra", "authors": "Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin\n  Lin, Edward H. Adelson, Sergey Levine", "title": "The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?", "comments": "10 pages, accepted at the 1st Annual Conference on Robot Learning\n  (CoRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful grasp requires careful balancing of the contact forces. Deducing\nwhether a particular grasp will be successful from indirect measurements, such\nas vision, is therefore quite challenging, and direct sensing of contacts\nthrough touch sensing provides an appealing avenue toward more successful and\nconsistent robotic grasping. However, in order to fully evaluate the value of\ntouch sensing for grasp outcome prediction, we must understand how touch\nsensing can influence outcome prediction accuracy when combined with other\nmodalities. Doing so using conventional model-based techniques is exceptionally\ndifficult. In this work, we investigate the question of whether touch sensing\naids in predicting grasp outcomes within a multimodal sensing framework that\ncombines vision and touch. To that end, we collected more than 9,000 grasping\ntrials using a two-finger gripper equipped with GelSight high-resolution\ntactile sensors on each finger, and evaluated visuo-tactile deep neural network\nmodels to directly predict grasp outcomes from either modality individually,\nand from both modalities together. Our experimental results indicate that\nincorporating tactile readings substantially improve grasping performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 05:32:38 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Calandra", "Roberto", ""], ["Owens", "Andrew", ""], ["Upadhyaya", "Manu", ""], ["Yuan", "Wenzhen", ""], ["Lin", "Justin", ""], ["Adelson", "Edward H.", ""], ["Levine", "Sergey", ""]]}, {"id": "1710.05520", "submitter": "Ya-Hui Zhang", "authors": "Ya-Hui Zhang", "title": "Entanglement Entropy of Target Functions for Image Classification and\n  Convolutional Neural Network", "comments": "9pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.str-el cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep convolutional neural network (CNN) in computer vision\nespecially image classification problems requests a new information theory for\nfunction of image, instead of image itself. In this article, after establishing\na deep mathematical connection between image classification problem and quantum\nspin model, we propose to use entanglement entropy, a generalization of\nclassical Boltzmann-Shannon entropy, as a powerful tool to characterize the\ninformation needed for representation of general function of image. We prove\nthat there is a sub-volume-law bound for entanglement entropy of target\nfunctions of reasonable image classification problems. Therefore target\nfunctions of image classification only occupy a small subspace of the whole\nHilbert space. As a result, a neural network with polynomial number of\nparameters is efficient for representation of such target functions of image.\nThe concept of entanglement entropy can also be useful to characterize the\nexpressive power of different neural networks. For example, we show that to\nmaintain the same expressive power, number of channels $D$ in a convolutional\nneural network should scale with the number of convolution layers $n_c$ as\n$D\\sim D_0^{\\frac{1}{n_c}}$. Therefore, deeper CNN with large $n_c$ is more\nefficient than shallow ones.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 05:54:38 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhang", "Ya-Hui", ""]]}, {"id": "1710.05664", "submitter": "Ilker Bozcan", "authors": "\\.Ilker Bozcan, Ya\\u{g}mur Oymak, \\.Idil Zeynep Alemdar, Sinan Kalkan", "title": "What is (missing or wrong) in the scene? A Hybrid Deep Boltzmann Machine\n  For Contextualized Scene Modeling", "comments": "6 pages, 7 figures, submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Scene models allow robots to reason about what is in the scene, what else\nshould be in it, and what should not be in it. In this paper, we propose a\nhybrid Boltzmann Machine (BM) for scene modeling where relations between\nobjects are integrated. To be able to do that, we extend BM to include tri-way\nedges between visible (object) nodes and make the network to share the\nrelations across different objects. We evaluate our method against several\nbaseline models (Deep Boltzmann Machines, and Restricted Boltzmann Machines) on\na scene classification dataset, and show that it performs better in several\nscene reasoning tasks.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 12:54:52 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 13:53:11 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Bozcan", "\u0130lker", ""], ["Oymak", "Ya\u011fmur", ""], ["Alemdar", "\u0130dil Zeynep", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1710.05703", "submitter": "Noman Islam Dr.", "authors": "Noman Islam, Zeeshan Islam, Nazia Noor", "title": "A Survey on Optical Character Recognition System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) has been a topic of interest for many\nyears. It is defined as the process of digitizing a document image into its\nconstituent characters. Despite decades of intense research, developing OCR\nwith capabilities comparable to that of human still remains an open challenge.\nDue to this challenging nature, researchers from industry and academic circles\nhave directed their attentions towards Optical Character Recognition. Over the\nlast few years, the number of academic laboratories and companies involved in\nresearch on Character Recognition has increased dramatically. This research\naims at summarizing the research so far done in the field of OCR. It provides\nan overview of different aspects of OCR and discusses corresponding proposals\naimed at resolving issues of OCR.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 15:08:49 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Islam", "Noman", ""], ["Islam", "Zeeshan", ""], ["Noor", "Nazia", ""]]}, {"id": "1710.05705", "submitter": "Leon Bungert", "authors": "Leon Bungert, David A. Coomes, Matthias J. Ehrhardt, Jennifer Rasch,\n  Rafael Reisenhofer, Carola-Bibiane Sch\\\"onlieb", "title": "Blind Image Fusion for Hyperspectral Imaging with the Directional Total\n  Variation", "comments": "24 pages, 18 figures, published in Inverse Problems, typo corrected,\n  figure added", "journal-ref": null, "doi": "10.1088/1361-6420/aaaf63", "report-no": null, "categories": "cs.CV cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral imaging is a cutting-edge type of remote sensing used for\nmapping vegetation properties, rock minerals and other materials. A major\ndrawback of hyperspectral imaging devices is their intrinsic low spatial\nresolution. In this paper, we propose a method for increasing the spatial\nresolution of a hyperspectral image by fusing it with an image of higher\nspatial resolution that was obtained with a different imaging modality. This is\naccomplished by solving a variational problem in which the regularization\nfunctional is the directional total variation. To accommodate for possible\nmis-registrations between the two images, we consider a non-convex blind\nsuper-resolution problem where both a fused image and the corresponding\nconvolution kernel are estimated. Using this approach, our model can realign\nthe given images if needed. Our experimental results indicate that the\nnon-convexity is negligible in practice and that reliable solutions can be\ncomputed using a variety of different optimization algorithms. Numerical\nresults on real remote sensing data from plant sciences and urban monitoring\nshow the potential of the proposed method and suggests that it is robust with\nrespect to the regularization parameters, mis-registration and the shape of the\nkernel.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 15:18:13 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 13:50:06 GMT"}, {"version": "v3", "created": "Sun, 17 Dec 2017 23:10:45 GMT"}, {"version": "v4", "created": "Mon, 9 Apr 2018 14:40:20 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Bungert", "Leon", ""], ["Coomes", "David A.", ""], ["Ehrhardt", "Matthias J.", ""], ["Rasch", "Jennifer", ""], ["Reisenhofer", "Rafael", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""]]}, {"id": "1710.05711", "submitter": "Sanping Zhou", "authors": "Sanping Zhou, Jinjun Wang, Deyu Meng, Xiaomeng Xin, Yubing Li, Yihong\n  Gong, Nanning Zheng", "title": "Deep Self-Paced Learning for Person Re-Identification", "comments": "Accepted by Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) usually suffers from noisy samples with\nbackground clutter and mutual occlusion, which makes it extremely difficult to\ndistinguish different individuals across the disjoint camera views. In this\npaper, we propose a novel deep self-paced learning (DSPL) algorithm to\nalleviate this problem, in which we apply a self-paced constraint and symmetric\nregularization to help the relative distance metric training the deep neural\nnetwork, so as to learn the stable and discriminative features for person\nRe-ID. Firstly, we propose a soft polynomial regularizer term which can derive\nthe adaptive weights to samples based on both the training loss and model age.\nAs a result, the high-confidence fidelity samples will be emphasized and the\nlow-confidence noisy samples will be suppressed at early stage of the whole\ntraining process. Such a learning regime is naturally implemented under a\nself-paced learning (SPL) framework, in which samples weights are adaptively\nupdated based on both model age and sample loss using an alternative\noptimization method. Secondly, we introduce a symmetric regularizer term to\nrevise the asymmetric gradient back-propagation derived by the relative\ndistance metric, so as to simultaneously minimize the intra-class distance and\nmaximize the inter-class distance in each triplet unit. Finally, we build a\npart-based deep neural network, in which the features of different body parts\nare first discriminately learned in the lower convolutional layers and then\nfused in the higher fully connected layers. Experiments on several benchmark\ndatasets have demonstrated the superior performance of our method as compared\nwith the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 01:32:38 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Meng", "Deyu", ""], ["Xin", "Xiaomeng", ""], ["Li", "Yubing", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1710.05718", "submitter": "Samuele Capobianco", "authors": "Samuele Capobianco, Luca Facheris, Fabrizio Cuccoli and Simone Marinai", "title": "Vehicle classification based on convolutional networks applied to FM-CW\n  radar signals", "comments": "in Proceedings of 1st European Conference on Traffic Mining Applied\n  to Police Activities (TRAP 2017)", "journal-ref": "Springer, Cham, 2018, ISBN: 978-3-319-75608-0", "doi": "10.1007/978-3-319-75608-0_9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the processing of Frequency Modulated-Continuos Wave\n(FM-CW) radar signals for vehicle classification. In the last years deep\nlearning has gained interest in several scientific fields and signal processing\nis not one exception. In this work we address the recognition of the vehicle\ncategory using a Convolutional Neural Network (CNN) applied to range Doppler\nsignature. The developed system first transforms the 1-dimensional signal into\na 3-dimensional signal that is subsequently used as input to the CNN. When\nusing the trained model to predict the vehicle category we obtain good\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 22:23:30 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 06:16:22 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 10:14:03 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Capobianco", "Samuele", ""], ["Facheris", "Luca", ""], ["Cuccoli", "Fabrizio", ""], ["Marinai", "Simone", ""]]}, {"id": "1710.05719", "submitter": "Hien Nguyen", "authors": "Aryan Mobiny, Supratik Moulik, Hien Van Nguyen", "title": "Lung Cancer Screening Using Adaptive Memory-Augmented Recurrent Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effectiveness of deep learning techniques\nfor lung nodule classification in computed tomography scans. Using less than\n10,000 training examples, our deep networks perform two times better than a\nstandard radiology software. Visualization of the networks' neurons reveals\nsemantically meaningful features that are consistent with the clinical\nknowledge and radiologists' perception. Our paper also proposes a novel\nframework for rapidly adapting deep networks to the radiologists' feedback, or\nchange in the data due to the shift in sensor's resolution or patient\npopulation. The classification accuracy of our approach remains above 80% while\npopular deep networks' accuracy is around chance. Finally, we provide in-depth\nanalysis of our framework by asking a radiologist to examine important\nnetworks' features and perform blind re-labeling of networks' mistakes.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 14:54:04 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:41:56 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Mobiny", "Aryan", ""], ["Moulik", "Supratik", ""], ["Van Nguyen", "Hien", ""]]}, {"id": "1710.05726", "submitter": "Hamid Tizhoosh", "authors": "Brady Kieffer, Morteza Babaie, Shivam Kalra, H.R.Tizhoosh", "title": "Convolutional Neural Networks for Histopathology Image Classification:\n  Training vs. Using Pre-Trained Networks", "comments": "To appear in proceedings of the 7th International Conference on Image\n  Processing Theory, Tools and Applications (IPTA 2017), Nov 28-Dec 1,\n  Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of classification within a medical image data-set\nbased on a feature vector extracted from the deepest layer of pre-trained\nConvolution Neural Networks. We have used feature vectors from several\npre-trained structures, including networks with/without transfer learning to\nevaluate the performance of pre-trained deep features versus CNNs which have\nbeen trained by that specific dataset as well as the impact of transfer\nlearning with a small number of samples. All experiments are done on Kimia\nPath24 dataset which consists of 27,055 histopathology training patches in 24\ntissue texture classes along with 1,325 test patches for evaluation. The result\nshows that pre-trained networks are quite competitive against training from\nscratch. As well, fine-tuning does not seem to add any tangible improvement for\nVGG16 to justify additional training while we observed considerable improvement\nin retrieval and classification accuracy when we fine-tuned the Inception\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 00:02:53 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Kieffer", "Brady", ""], ["Babaie", "Morteza", ""], ["Kalra", "Shivam", ""], ["Tizhoosh", "H. R.", ""]]}, {"id": "1710.05732", "submitter": "Scott Burns", "authors": "Scott Allen Burns", "title": "Generating Reflectance Curves from sRGB Triplets", "comments": "v3 minor editing to clarify some points, and some webpage link\n  updates, v4 adds the LHTSS method, v5 indicates LHTSS should be preferred to\n  ILLSS generally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The color sensation evoked by an object depends on both the spectral power\ndistribution of the illumination and the reflectance properties of the object\nbeing illuminated. The color sensation can be characterized by three\ncolor-space values, such as XYZ, RGB, HSV, L*a*b*, etc. It is straightforward\nto compute the three values given the illuminant and reflectance curves. The\nconverse process of computing a reflectance curve given the color-space values\nand the illuminant is complicated by the fact that an infinite number of\ndifferent reflectance curves can give rise to a single set of color-space\nvalues (metamerism). This paper presents five algorithms for generating a\nreflectance curve from a specified sRGB triplet, written for a general\naudience. The algorithms are designed to generate reflectance curves that are\nsimilar to those found with naturally occurring colored objects. The computed\nreflectance curves are compared to a database of thousands of reflectance\ncurves measured from paints and pigments available both commercially and in\nnature, and the similarity is quantified. One particularly useful application\nof these algorithms is in the field of computer graphics, where modeling color\ntransformations sometimes requires wavelength-specific information, such as\nwhen modeling subtractive color mixture.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 19:02:54 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 12:15:02 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 11:16:18 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 00:27:52 GMT"}, {"version": "v5", "created": "Thu, 9 Jan 2020 16:36:40 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Burns", "Scott Allen", ""]]}, {"id": "1710.05749", "submitter": "Shadrokh Samavi", "authors": "Farshad Kheiri, Shadrokh Samavi, Nader Karimi", "title": "Hardware design for binarization and thinning of fingerprint images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two critical steps in fingerprint recognition are binarization and thinning\nof the image. The need for real time processing motivates us to select local\nadaptive thresholding approach for the binarization step. We introduce a new\nhardware for this purpose based on pipeline architecture. We propose a formula\nfor selecting an optimal block size for the thresholding purpose. To decrease\nminutiae false detection, the binarized image is dilated. We also present in\nthis paper a new pipeline structure for implementing the thinning algorithm\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 19:50:47 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Kheiri", "Farshad", ""], ["Samavi", "Shadrokh", ""], ["Karimi", "Nader", ""]]}, {"id": "1710.05758", "submitter": "Dominik Marek Loroch", "authors": "Dominik Marek Loroch, Norbert Wehn, Franz-Josef Pfreundt, Janis Keuper", "title": "TensorQuant - A Simulation Toolbox for Deep Neural Network Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research implies that training and inference of deep neural networks\n(DNN) can be computed with low precision numerical representations of the\ntraining/test data, weights and gradients without a general loss in accuracy.\nThe benefit of such compact representations is twofold: they allow a\nsignificant reduction of the communication bottleneck in distributed DNN\ntraining and faster neural network implementations on hardware accelerators\nlike FPGAs. Several quantization methods have been proposed to map the original\n32-bit floating point problem to low-bit representations. While most related\npublications validate the proposed approach on a single DNN topology, it\nappears to be evident, that the optimal choice of the quantization method and\nnumber of coding bits is topology dependent. To this end, there is no general\ntheory available, which would allow users to derive the optimal quantization\nduring the design of a DNN topology. In this paper, we present a quantization\ntool box for the TensorFlow framework. TensorQuant allows a transparent\nquantization simulation of existing DNN topologies during training and\ninference. TensorQuant supports generic quantization methods and allows\nexperimental evaluation of the impact of the quantization on single layers as\nwell as on the full topology. In a first series of experiments with\nTensorQuant, we show an analysis of fix-point quantizations of popular CNN\ntopologies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 10:15:27 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Loroch", "Dominik Marek", ""], ["Wehn", "Norbert", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Janis", ""]]}, {"id": "1710.05817", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Saman Parvaneh, Asif Rahman, Bryan Conroy and Saeed\n  Babaeizadeh", "title": "Densely Connected Convolutional Networks and Signal Quality Analysis to\n  Detect Atrial Fibrillation Using Short Single-Lead ECG Recordings", "comments": "Computing in Cardiology 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of new technology such as wearables that record high-quality\nsingle channel ECG, provides an opportunity for ECG screening in a larger\npopulation, especially for atrial fibrillation screening. The main goal of this\nstudy is to develop an automatic classification algorithm for normal sinus\nrhythm (NSR), atrial fibrillation (AF), other rhythms (O), and noise from a\nsingle channel short ECG segment (9-60 seconds). For this purpose, signal\nquality index (SQI) along with dense convolutional neural networks was used.\nTwo convolutional neural network (CNN) models (main model that accepts 15\nseconds ECG and secondary model that processes 9 seconds shorter ECG) were\ntrained using the training data set. If the recording is determined to be of\nlow quality by SQI, it is immediately classified as noisy. Otherwise, it is\ntransformed to a time-frequency representation and classified with the CNN as\nNSR, AF, O, or noise. At the final step, a feature-based post-processing\nalgorithm classifies the rhythm as either NSR or O in case the CNN model's\ndiscrimination between the two is indeterminate. The best result achieved at\nthe official phase of the PhysioNet/CinC challenge on the blind test set was\n0.80 (F1 for NSR, AF, and O were 0.90, 0.80, and 0.70, respectively).\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 18:58:45 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Rubin", "Jonathan", ""], ["Parvaneh", "Saman", ""], ["Rahman", "Asif", ""], ["Conroy", "Bryan", ""], ["Babaeizadeh", "Saeed", ""]]}, {"id": "1710.05941", "submitter": "Prajit Ramachandran", "authors": "Prajit Ramachandran, Barret Zoph, Quoc V. Le", "title": "Searching for Activation Functions", "comments": "Updated version of \"Swish: a Self-Gated Activation Function\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of activation functions in deep networks has a significant effect\non the training dynamics and task performance. Currently, the most successful\nand widely-used activation function is the Rectified Linear Unit (ReLU).\nAlthough various hand-designed alternatives to ReLU have been proposed, none\nhave managed to replace it due to inconsistent gains. In this work, we propose\nto leverage automatic search techniques to discover new activation functions.\nUsing a combination of exhaustive and reinforcement learning-based search, we\ndiscover multiple novel activation functions. We verify the effectiveness of\nthe searches by conducting an empirical evaluation with the best discovered\nactivation function. Our experiments show that the best discovered activation\nfunction, $f(x) = x \\cdot \\text{sigmoid}(\\beta x)$, which we name Swish, tends\nto work better than ReLU on deeper models across a number of challenging\ndatasets. For example, simply replacing ReLUs with Swish units improves top-1\nclassification accuracy on ImageNet by 0.9\\% for Mobile NASNet-A and 0.6\\% for\nInception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it\neasy for practitioners to replace ReLUs with Swish units in any neural network.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:05:45 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 17:45:21 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Ramachandran", "Prajit", ""], ["Zoph", "Barret", ""], ["Le", "Quoc V.", ""]]}, {"id": "1710.05956", "submitter": "Jose Dolz", "authors": "Jose Dolz, Ismail Ben Ayed, Jing Yuan and Christian Desrosiers", "title": "Isointense Infant Brain Segmentation with a Hyper-dense Connected\n  Convolutional Neural Network", "comments": "Oral presentation at ISBI 2018. The last version of the paper is\n  updated with the reference of the iSEG comparative study, published in 2019\n  at IEEE TMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neonatal brain segmentation in magnetic resonance (MR) is a challenging\nproblem due to poor image quality and low contrast between white and gray\nmatter regions. Most existing approaches for this problem are based on\nmulti-atlas label fusion strategies, which are time-consuming and sensitive to\nregistration errors. As alternative to these methods, we propose a\nhyper-densely connected 3D convolutional neural network that employs MR-T1 and\nT2 images as input, which are processed independently in two separated paths.\nAn important difference with previous densely connected networks is the use of\ndirect connections between layers from the same and different paths. Adopting\nsuch dense connectivity helps the learning process by including deep\nsupervision and improving gradient flow. We evaluated our approach on data from\nthe MICCAI Grand Challenge on 6-month infant Brain MRI Segmentation (iSEG),\nobtaining very competitive results. Among 21 teams, our approach ranked first\nor second in most metrics, translating into a state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:43:45 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 13:04:45 GMT"}, {"version": "v3", "created": "Sun, 18 Feb 2018 01:04:13 GMT"}, {"version": "v4", "created": "Sat, 2 Mar 2019 02:44:54 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Dolz", "Jose", ""], ["Ayed", "Ismail Ben", ""], ["Yuan", "Jing", ""], ["Desrosiers", "Christian", ""]]}, {"id": "1710.05958", "submitter": "Sayna Ebrahimi", "authors": "Sayna Ebrahimi, Anna Rohrbach, Trevor Darrell", "title": "Gradient-free Policy Architecture Search and Adaptation", "comments": "Accepted in Conference on Robot Learning, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for policy architecture search and adaptation via\ngradient-free optimization which can learn to perform autonomous driving tasks.\nBy learning from both demonstration and environmental reward we develop a model\nthat can learn with relatively few early catastrophic failures. We first learn\nan architecture of appropriate complexity to perceive aspects of world state\nrelevant to the expert demonstration, and then mitigate the effect of\ndomain-shift during deployment by adapting a policy demonstrated in a source\ndomain to rewards obtained in a target environment. We show that our approach\nallows safer learning than baseline methods, offering a reduced cumulative\ncrash metric over the agent's lifetime as it learns to drive in a realistic\nsimulated environment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 18:47:35 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ebrahimi", "Sayna", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""]]}, {"id": "1710.05982", "submitter": "Lorenzo Alvino", "authors": "Lorenzo Alvino", "title": "Pushing the envelope in deep visual recognition for mobile platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is the task of assigning to an input image a label from\na fixed set of categories. One of its most important applicative fields is that\nof robotics, in particular the needing of a robot to be aware of what's around\nand the consequent exploitation of that information as a benefit for its tasks.\nIn this work we consider the problem of a robot that enters a new environment\nand wants to understand visual data coming from its camera, so to extract\nknowledge from them. As main novelty we want to overcome the needing of a\nphysical robot, as it could be expensive and unhandy, so to hopefully enhance,\nspeed up and ease the research in this field. That's why we propose to develop\nan application for a mobile platform that wraps several deep visual recognition\ntasks. First we deal with a simple Image classification, testing a model\nobtained from an AlexNet trained on the ILSVRC 2012 dataset. Several photo\nsettings are considered to better understand which factors affect most the\nquality of classification. For the same purpose we are interested to integrate\nthe classification task with an extra module dealing with segmentation of the\nobject inside the image. In particular we propose a technique for extracting\nthe object shape and moving out all the background, so to focus the\nclassification only on the region occupied by the object. Another significant\ntask that is included is that of object discovery. Its purpose is to simulate\nthe situation in which the robot needs a certain object to complete one of its\nactivities. It starts searching for what it needs by looking around and trying\nto understand the location of the object by scanning the surrounding\nenvironment. Finally we provide a tool for dealing with the creation of\ncustomized task-specific databases, meant to better suit to one's needing in a\nparticular vision task.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 20:11:23 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 11:37:48 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Alvino", "Lorenzo", ""]]}, {"id": "1710.05985", "submitter": "Leonid Yaroslavsky", "authors": "Leonid P. Yaroslavsky", "title": "Compressed Sensing, ASBSR-method of image sampling and reconstruction\n  and the problem of digital image acquisition with the lowest possible\n  sampling rate", "comments": "28 pages, 19 figures", "journal-ref": "Compressed Sensing: Methods, Theory and Applications, Chapt.1.,\n  Ed. Jonathon M. Sheppard, Nova Publishers, 2018", "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of minimization of the number of measurements needed for digital\nimage acquisition and reconstruction with a given accuracy is addressed. Basics\nof the sampling theory are outlined to show that the lower bound of signal\nsampling rate sufficient for signal reconstruction with a given accuracy is\nequal to the spectrum sparsity of the signal sparse approximation that has this\naccuracy. It is revealed that the compressed sensing approach, which was\nadvanced as a solution to the sampling rate minimization problem, is far from\nreaching the sampling rate theoretical minimum. Potentials and limitations of\ncompressed sensing are demystified using a simple and intutive model, A method\nof image Arbitrary Sampling and Bounded Spectrum Reconstruction (ASBSR-method)\nis described that allows to draw near the image sampling rate theoretical\nminimum. Presented and discussed are also results of experimental verification\nof the ASBSR-method and its possible applicability extensions to solving\nvarious underdetermined inverse problems such as color image demosaicing, image\nin-painting, image reconstruction from their sparsely sampled or decimated\nprojections, image reconstruction from the modulus of its Fourier spectrum, and\nimage reconstruction from its sparse samples in Fourier domain\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 08:05:32 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 08:49:10 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Yaroslavsky", "Leonid P.", ""]]}, {"id": "1710.05994", "submitter": "Yawei Hui", "authors": "Yawei Hui and Yaohua Liu", "title": "Volumetric Data Exploration with Machine Learning-Aided Visualization in\n  Neutron Science", "comments": "14 pages, 7 figures; the Computer Vision Conference (CVC), Las Vegas,\n  Nevada, 2019; accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in neutron and X-ray sources, instrumentation and data\ncollection modes have significantly increased the experimental data size (which\ncould easily contain 10$^{8}$ -- 10$^{10}$ data points), so that conventional\nvolumetric visualization approaches become inefficient for both still imaging\nand interactive OpenGL rendition in a 3D setting. We introduce a new approach\nbased on the unsupervised machine learning algorithm, Density-Based Spatial\nClustering of Applications with Noise (DBSCAN), to efficiently analyze and\nvisualize large volumetric datasets. Here we present two examples of analyzing\nand visualizing datasets from the diffuse scattering experiment of a single\ncrystal sample and the tomographic reconstruction of a neutron scanning of a\nturbine blade. We found that by using the intensity as the weighting factor in\nthe clustering process, DBSCAN becomes very effective in de-noising and\nfeature/boundary detection, and thus enables better visualization of the\nhierarchical internal structures of the neutron scattering data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 20:32:10 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 20:23:05 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 13:37:30 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Hui", "Yawei", ""], ["Liu", "Yaohua", ""]]}, {"id": "1710.06090", "submitter": "Runze Xu", "authors": "Runze Xu, Zhiming Zhou, Weinan Zhang, Yong Yu", "title": "Face Transfer with Generative Adversarial Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face transfer animates the facial performances of the character in the target\nvideo by a source actor. Traditional methods are typically based on face\nmodeling. We propose an end-to-end face transfer method based on Generative\nAdversarial Network. Specifically, we leverage CycleGAN to generate the face\nimage of the target character with the corresponding head pose and facial\nexpression of the source. In order to improve the quality of generated videos,\nwe adopt PatchGAN and explore the effect of different receptive field sizes on\ngenerated images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:31:39 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Xu", "Runze", ""], ["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1710.06096", "submitter": "Ricky Fok", "authors": "Ricky Fok, Aijun An, and Xiaogang Wang", "title": "Spontaneous Symmetry Breaking in Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to understand the unprecedented performance and\nrobustness of deep neural networks using field theory. Correlations between the\nweights within the same layer can be described by symmetries in that layer, and\nnetworks generalize better if such symmetries are broken to reduce the\nredundancies of the weights. Using a two parameter field theory, we find that\nthe network can break such symmetries itself towards the end of training in a\nprocess commonly known in physics as spontaneous symmetry breaking. This\ncorresponds to a network generalizing itself without any user input layers to\nbreak the symmetry, but by communication with adjacent layers. In the layer\ndecoupling limit applicable to residual networks (He et al., 2015), we show\nthat the remnant symmetries that survive the non-linear layers are\nspontaneously broken. The Lagrangian for the non-linear and weight layers\ntogether has striking similarities with the one in quantum field theory of a\nscalar. Using results from quantum field theory we show that our framework is\nable to explain many experimentally observed phenomena,such as training on\nrandom labels with zero error (Zhang et al., 2017), the information bottleneck,\nthe phase transition out of it and gradient variance explosion (Shwartz-Ziv &\nTishby, 2017), shattered gradients (Balduzzi et al., 2017), and many more.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 04:55:14 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Fok", "Ricky", ""], ["An", "Aijun", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1710.06104", "submitter": "Li Yi", "authors": "Li Yi, Lin Shao, Manolis Savva, Haibin Huang, Yang Zhou, Qirui Wang,\n  Benjamin Graham, Martin Engelcke, Roman Klokov, Victor Lempitsky, Yuan Gan,\n  Pengyu Wang, Kun Liu, Fenggen Yu, Panpan Shui, Bingyang Hu, Yan Zhang,\n  Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Minki Jeong, Jaehoon Choi, Changick\n  Kim, Angom Geetchandra, Narasimha Murthy, Bhargava Ramu, Bharadwaj Manda, M\n  Ramanathan, Gautam Kumar, P Preetham, Siddharth Srivastava, Swati Bhugra,\n  Brejesh Lall, Christian Haene, Shubham Tulsiani, Jitendra Malik, Jared Lafer,\n  Ramsey Jones, Siyuan Li, Jie Lu, Shi Jin, Jingyi Yu, Qixing Huang, Evangelos\n  Kalogerakis, Silvio Savarese, Pat Hanrahan, Thomas Funkhouser, Hao Su,\n  Leonidas Guibas", "title": "Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet\n  Core55", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a large-scale 3D shape understanding benchmark using data and\nannotation from ShapeNet 3D object database. The benchmark consists of two\ntasks: part-level segmentation of 3D shapes and 3D reconstruction from single\nview images. Ten teams have participated in the challenge and the best\nperforming teams have outperformed state-of-the-art approaches on both tasks. A\nfew novel deep learning architectures have been proposed on various 3D\nrepresentations on both tasks. We report the techniques used by each team and\nthe corresponding performances. In addition, we summarize the major discoveries\nfrom the reported results and possible trends for the future work in the field.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 05:34:22 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 22:26:43 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Yi", "Li", ""], ["Shao", "Lin", ""], ["Savva", "Manolis", ""], ["Huang", "Haibin", ""], ["Zhou", "Yang", ""], ["Wang", "Qirui", ""], ["Graham", "Benjamin", ""], ["Engelcke", "Martin", ""], ["Klokov", "Roman", ""], ["Lempitsky", "Victor", ""], ["Gan", "Yuan", ""], ["Wang", "Pengyu", ""], ["Liu", "Kun", ""], ["Yu", "Fenggen", ""], ["Shui", "Panpan", ""], ["Hu", "Bingyang", ""], ["Zhang", "Yan", ""], ["Li", "Yangyan", ""], ["Bu", "Rui", ""], ["Sun", "Mingchao", ""], ["Wu", "Wei", ""], ["Jeong", "Minki", ""], ["Choi", "Jaehoon", ""], ["Kim", "Changick", ""], ["Geetchandra", "Angom", ""], ["Murthy", "Narasimha", ""], ["Ramu", "Bhargava", ""], ["Manda", "Bharadwaj", ""], ["Ramanathan", "M", ""], ["Kumar", "Gautam", ""], ["Preetham", "P", ""], ["Srivastava", "Siddharth", ""], ["Bhugra", "Swati", ""], ["Lall", "Brejesh", ""], ["Haene", "Christian", ""], ["Tulsiani", "Shubham", ""], ["Malik", "Jitendra", ""], ["Lafer", "Jared", ""], ["Jones", "Ramsey", ""], ["Li", "Siyuan", ""], ["Lu", "Jie", ""], ["Jin", "Shi", ""], ["Yu", "Jingyi", ""], ["Huang", "Qixing", ""], ["Kalogerakis", "Evangelos", ""], ["Savarese", "Silvio", ""], ["Hanrahan", "Pat", ""], ["Funkhouser", "Thomas", ""], ["Su", "Hao", ""], ["Guibas", "Leonidas", ""]]}, {"id": "1710.06130", "submitter": "Vladislav Golyanik", "authors": "Mohammad Dawud Ansari, Vladislav Golyanik, Didier Stricker", "title": "Scalable Dense Monocular Surface Reconstruction", "comments": "International Conference on 3D Vision (3DV), Qingdao, China, October\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a novel template-free monocular non-rigid surface\nreconstruction approach. Existing techniques using motion and deformation cues\nrely on multiple prior assumptions, are often computationally expensive and do\nnot perform equally well across the variety of data sets. In contrast, the\nproposed Scalable Monocular Surface Reconstruction (SMSR) combines strengths of\nseveral algorithms, i.e., it is scalable with the number of points, can handle\nsparse and dense settings as well as different types of motions and\ndeformations. We estimate camera pose by singular value thresholding and\nproximal gradient. Our formulation adopts alternating direction method of\nmultipliers which converges in linear time for large point track matrices. In\nthe proposed SMSR, trajectory space constraints are integrated by smoothing of\nthe measurement matrix. In the extensive experiments, SMSR is demonstrated to\nconsistently achieve state-of-the-art accuracy on a wide variety of data sets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 07:17:01 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Ansari", "Mohammad Dawud", ""], ["Golyanik", "Vladislav", ""], ["Stricker", "Didier", ""]]}, {"id": "1710.06160", "submitter": "Damien Matti", "authors": "Damien Matti, Haz{\\i}m Kemal Ekenel and Jean-Philippe Thiran", "title": "Combining LiDAR Space Clustering and Convolutional Neural Networks for\n  Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection is an important component for safety of autonomous\nvehicles, as well as for traffic and street surveillance. There are extensive\nbenchmarks on this topic and it has been shown to be a challenging problem when\napplied on real use-case scenarios. In purely image-based pedestrian detection\napproaches, the state-of-the-art results have been achieved with convolutional\nneural networks (CNN) and surprisingly few detection frameworks have been built\nupon multi-cue approaches. In this work, we develop a new pedestrian detector\nfor autonomous vehicles that exploits LiDAR data, in addition to visual\ninformation. In the proposed approach, LiDAR data is utilized to generate\nregion proposals by processing the three dimensional point cloud that it\nprovides. These candidate regions are then further processed by a\nstate-of-the-art CNN classifier that we have fine-tuned for pedestrian\ndetection. We have extensively evaluated the proposed detection process on the\nKITTI dataset. The experimental results show that the proposed LiDAR space\nclustering approach provides a very efficient way of generating region\nproposals leading to higher recall rates and fewer misses for pedestrian\ndetection. This indicates that LiDAR data can provide auxiliary information for\nCNN-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 08:40:16 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Matti", "Damien", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1710.06177", "submitter": "Linjun Zhou", "authors": "Linjun Zhou, Peng Cui, Shiqiang Yang, Wenwu Zhu, Qi Tian", "title": "Learning to Learn Image Classifiers with Visual Analogy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are far better learners who can learn a new concept very fast with\nonly a few samples compared with machines. The plausible mystery making the\ndifference is two fundamental learning mechanisms: learning to learn and\nlearning by analogy. In this paper, we attempt to investigate a new human-like\nlearning method by organically combining these two mechanisms. In particular,\nwe study how to generalize the classification parameters from previously\nlearned concepts to a new concept. we first propose a novel Visual Analogy\nGraph Embedded Regression (VAGER) model to jointly learn a low-dimensional\nembedding space and a linear mapping function from the embedding space to\nclassification parameters for base classes. We then propose an out-of-sample\nembedding method to learn the embedding of a new class represented by a few\nsamples through its visual analogy with base classes and derive the\nclassification parameters for the new class. We conduct extensive experiments\non ImageNet dataset and the results show that our method could consistently and\nsignificantly outperform state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 09:17:33 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 12:11:30 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhou", "Linjun", ""], ["Cui", "Peng", ""], ["Yang", "Shiqiang", ""], ["Zhu", "Wenwu", ""], ["Tian", "Qi", ""]]}, {"id": "1710.06194", "submitter": "Da Chen", "authors": "Da Chen, Laurent D. Cohen", "title": "A New Coherence-Penalized Minimal Path Model with Application to Retinal\n  Vessel Centerline Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new minimal path model for minimally interactive\nretinal vessel centerline extraction. The main contribution lies at the\nconstruction of a novel coherence-penalized Riemannian metric in a lifted\nspace, dependently of the local geometry of tubularity and an external\nscalar-valued reference feature map. The globally minimizing curves associated\nto the proposed metric favour to pass through a set of retinal vessel segments\nwith low variations of the feature map, thus can avoid the short branches\ncombination problem and shortcut problem, commonly suffered by the existing\nminimal path models in the application of retinal imaging. We validate our\nmodel on a series of retinal vessel patches obtained from the DRIVE and IOSTAR\ndatasets, showing that our model indeed get promising results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 10:23:57 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Chen", "Da", ""], ["Cohen", "Laurent D.", ""]]}, {"id": "1710.06205", "submitter": "Makoto Miura", "authors": "Atsushi Ito, Makoto Miura and Kazushi Ueda", "title": "Projective reconstruction in algebraic vision", "comments": "15 pages", "journal-ref": "Can. Math. Bull. 63 (2020) 592-609", "doi": "10.4153/S0008439519000687", "report-no": null, "categories": "math.AG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the geometry of rational maps from a projective space of an\narbitrary dimension to the product of projective spaces of lower dimensions\ninduced by linear projections. In particular, we give an algebro-geometric\nvariant of the projective reconstruction theorem by Hartley and Schaffalitzky\n[HS09].\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 11:03:00 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 19:00:57 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 14:54:58 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Ito", "Atsushi", ""], ["Miura", "Makoto", ""], ["Ueda", "Kazushi", ""]]}, {"id": "1710.06230", "submitter": "Varuna De Silva D", "authors": "Varuna De Silva, Jamie Roche, and Ahmet Kondoz", "title": "Robust Fusion of LiDAR and Wide-Angle Camera Data for Autonomous Mobile\n  Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots that assist humans in day to day living tasks are becoming\nincreasingly popular. Autonomous mobile robots operate by sensing and\nperceiving their surrounding environment to make accurate driving decisions. A\ncombination of several different sensors such as LiDAR, radar, ultrasound\nsensors and cameras are utilized to sense the surrounding environment of\nautonomous vehicles. These heterogeneous sensors simultaneously capture various\nphysical attributes of the environment. Such multimodality and redundancy of\nsensing need to be positively utilized for reliable and consistent perception\nof the environment through sensor data fusion. However, these multimodal sensor\ndata streams are different from each other in many ways, such as temporal and\nspatial resolution, data format, and geometric alignment. For the subsequent\nperception algorithms to utilize the diversity offered by multimodal sensing,\nthe data streams need to be spatially, geometrically and temporally aligned\nwith each other. In this paper, we address the problem of fusing the outputs of\na Light Detection and Ranging (LiDAR) scanner and a wide-angle monocular image\nsensor for free space detection. The outputs of LiDAR scanner and the image\nsensor are of different spatial resolutions and need to be aligned with each\nother. A geometrical model is used to spatially align the two sensor outputs,\nfollowed by a Gaussian Process (GP) regression-based resolution matching\nalgorithm to interpolate the missing data with quantifiable uncertainty. The\nresults indicate that the proposed sensor data fusion framework significantly\naids the subsequent perception steps, as illustrated by the performance\nimprovement of a uncertainty aware free space detection algorithm\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:01:19 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:15:33 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 04:05:50 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["De Silva", "Varuna", ""], ["Roche", "Jamie", ""], ["Kondoz", "Ahmet", ""]]}, {"id": "1710.06231", "submitter": "Wim Abbeloos", "authors": "Wim Abbeloos, Esra Ataer-Cansizoglu, Sergio Caccamo, Yuichi Taguchi,\n  Yukiyasu Domae", "title": "3D Object Discovery and Modeling Using Single RGB-D Images Containing\n  Multiple Object Instances", "comments": null, "journal-ref": "Proceedings International Conference on 3D Vision 2017 (pp.\n  431-439)", "doi": "10.1109/3dv.2017.00056", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised object modeling is important in robotics, especially for\nhandling a large set of objects. We present a method for unsupervised 3D object\ndiscovery, reconstruction, and localization that exploits multiple instances of\nan identical object contained in a single RGB-D image. The proposed method does\nnot rely on segmentation, scene knowledge, or user input, and thus is easily\nscalable. Our method aims to find recurrent patterns in a single RGB-D image by\nutilizing appearance and geometry of the salient regions. We extract keypoints\nand match them in pairs based on their descriptors. We then generate triplets\nof the keypoints matching with each other using several geometric criteria to\nminimize false matches. The relative poses of the matched triplets are computed\nand clustered to discover sets of triplet pairs with similar relative poses.\nTriplets belonging to the same set are likely to belong to the same object and\nare used to construct an initial object model. Detection of remaining instances\nwith the initial object model using RANSAC allows to further expand and refine\nthe model. The automatically generated object models are both compact and\ndescriptive. We show quantitative and qualitative results on RGB-D images with\nvarious objects including some from the Amazon Picking Challenge. We also\ndemonstrate the use of our method in an object picking scenario with a robotic\narm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:03:34 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Abbeloos", "Wim", ""], ["Ataer-Cansizoglu", "Esra", ""], ["Caccamo", "Sergio", ""], ["Taguchi", "Yuichi", ""], ["Domae", "Yukiyasu", ""]]}, {"id": "1710.06232", "submitter": "Ertugrul Bayraktar", "authors": "Ertugrul Bayraktar and Pinar Boyraz", "title": "Analysis of feature detector and descriptor combinations with a\n  localization experiment for various performance metrics", "comments": "11 pages, 3 figures, 1 table", "journal-ref": "Turkish Journal of Electrical Engineering & Computer Sciences,\n  (2017) 25: 2444 - 2454", "doi": "10.3906/elk-1602-225", "report-no": null, "categories": "cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to provide a detailed performance comparison of\nfeature detector/descriptor methods, particularly when their various\ncombinations are used for image-matching. The localization experiments of a\nmobile robot in an indoor environment are presented as a case study. In these\nexperiments, 3090 query images and 127 dataset images were used. This study\nincludes five methods for feature detectors (features from accelerated segment\ntest (FAST), oriented FAST and rotated binary robust independent elementary\nfeatures (BRIEF) (ORB), speeded-up robust features (SURF), scale invariant\nfeature transform (SIFT), and binary robust invariant scalable keypoints\n(BRISK)) and five other methods for feature descriptors (BRIEF, BRISK, SIFT,\nSURF, and ORB). These methods were used in 23 different combinations and it was\npossible to obtain meaningful and consistent comparison results using the\nperformance criteria defined in this study. All of these methods were used\nindependently and separately from each other as either feature detector or\ndescriptor. The performance analysis shows the discriminative power of various\ncombinations of detector and descriptor methods. The analysis is completed\nusing five parameters: (i) accuracy, (ii) time, (iii) angle difference between\nkeypoints, (iv) number of correct matches, and (v) distance between correctly\nmatched keypoints. In a range of 60{\\deg}, covering five rotational pose points\nfor our system, the FAST-SURF combination had the lowest distance and angle\ndifference values and the highest number of matched keypoints. SIFT-SURF was\nthe most accurate combination with a 98.41% correct classification rate. The\nfastest algorithm was ORB-BRIEF, with a total running time of 21,303.30 s to\nmatch 560 images captured during motion with 127 dataset images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:10:37 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Bayraktar", "Ertugrul", ""], ["Boyraz", "Pinar", ""]]}, {"id": "1710.06235", "submitter": "Marco Carraro", "authors": "Marco Carraro, Matteo Munaro, Jeff Burke, Emanuele Menegatti", "title": "Real-time marker-less multi-person 3D pose estimation in RGB-Depth\n  camera networks", "comments": "Submitted to the 2018 IEEE International Conference on Robotics and\n  Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel system to estimate and track the 3D poses of\nmultiple persons in calibrated RGB-Depth camera networks. The multi-view 3D\npose of each person is computed by a central node which receives the\nsingle-view outcomes from each camera of the network. Each single-view outcome\nis computed by using a CNN for 2D pose estimation and extending the resulting\nskeletons to 3D by means of the sensor depth. The proposed system is\nmarker-less, multi-person, independent of background and does not make any\nassumption on people appearance and initial pose. The system provides real-time\noutcomes, thus being perfectly suited for applications requiring user\ninteraction. Experimental results show the effectiveness of this work with\nrespect to a baseline multi-view approach in different scenarios. To foster\nresearch and applications based on this work, we released the source code in\nOpenPTrack, an open source project for RGB-D people tracking.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:27:23 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Carraro", "Marco", ""], ["Munaro", "Matteo", ""], ["Burke", "Jeff", ""], ["Menegatti", "Emanuele", ""]]}, {"id": "1710.06236", "submitter": "Tianwei Lin", "authors": "Tianwei Lin, Xu Zhao, Zheng Shou", "title": "Single Shot Temporal Action Detection", "comments": "ACM Multimedia 2017", "journal-ref": null, "doi": "10.1145/3123266.3123343", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action detection is a very important yet challenging problem, since\nvideos in real applications are usually long, untrimmed and contain multiple\naction instances. This problem requires not only recognizing action categories\nbut also detecting start time and end time of each action instance. Many\nstate-of-the-art methods adopt the \"detection by classification\" framework:\nfirst do proposal, and then classify proposals. The main drawback of this\nframework is that the boundaries of action instance proposals have been fixed\nduring the classification step. To address this issue, we propose a novel\nSingle Shot Action Detector (SSAD) network based on 1D temporal convolutional\nlayers to skip the proposal generation step via directly detecting action\ninstances in untrimmed video. On pursuit of designing a particular SSAD network\nthat can work effectively for temporal action detection, we empirically search\nfor the best network architecture of SSAD due to lacking existing models that\ncan be directly adopted. Moreover, we investigate into input feature types and\nfusion strategies to further improve detection accuracy. We conduct extensive\nexperiments on two challenging datasets: THUMOS 2014 and MEXaction2. When\nsetting Intersection-over-Union threshold to 0.5 during evaluation, SSAD\nsignificantly outperforms other state-of-the-art systems by increasing mAP from\n19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 12:41:17 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Lin", "Tianwei", ""], ["Zhao", "Xu", ""], ["Shou", "Zheng", ""]]}, {"id": "1710.06270", "submitter": "Jonas Unger", "authors": "Apostolia Tsirikoglou (1), Joel Kronander (1), Magnus Wrenninge (2)\n  and Jonas Unger (1) ((1) Link\\\"oping University (2) 7DLabs)", "title": "Procedural Modeling and Physically Based Rendering for Synthetic Data\n  Generation in Automotive Applications", "comments": "The project web page at\n  http://vcl.itn.liu.se/publications/2017/TKWU17/ contains a version of the\n  paper with high-resolution images as well as additional material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview and evaluation of a new, systematic approach for\ngeneration of highly realistic, annotated synthetic data for training of deep\nneural networks in computer vision tasks. The main contribution is a procedural\nworld modeling approach enabling high variability coupled with physically\naccurate image synthesis, and is a departure from the hand-modeled virtual\nworlds and approximate image synthesis methods used in real-time applications.\nThe benefits of our approach include flexible, physically accurate and scalable\nimage synthesis, implicit wide coverage of classes and features, and complete\ndata introspection for annotations, which all contribute to quality and cost\nefficiency. To evaluate our approach and the efficacy of the resulting data, we\nuse semantic segmentation for autonomous vehicles and robotic navigation as the\nmain application, and we train multiple deep learning architectures using\nsynthetic data with and without fine tuning on organic (i.e. real-world) data.\nThe evaluation shows that our approach improves the neural network's\nperformance and that even modest implementation efforts produce\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 13:38:16 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 06:46:05 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Tsirikoglou", "Apostolia", "", "Link\u00f6ping University"], ["Kronander", "Joel", "", "Link\u00f6ping University"], ["Wrenninge", "Magnus", "", "7DLabs"], ["Unger", "Jonas", "", "Link\u00f6ping University"]]}, {"id": "1710.06287", "submitter": "Christopher Syben", "authors": "Christopher Syben and Bernhard Stimpel and Katharina Breininger and\n  Tobias W\\\"urfl and Rebecca Fahrig and Arnd D\\\"orfler and Andreas Maier", "title": "Precision Learning: Reconstruction Filter Kernel Discretization", "comments": "Accepted at The Fifth International Conference on Image Formation in\n  X-Ray Computed Tomography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present substantial evidence that a deep neural network\nwill intrinsically learn the appropriate way to discretize the ideal continuous\nreconstruction filter. Currently, the Ram-Lak filter or heuristic filters which\nimpose different noise assumptions are used for filtered back-projection. All\nof these, however, inhibit a fully data-driven reconstruction deep learning\napproach. In addition, the heuristic filters are not chosen in an optimal\nsense. To tackle this issue, we propose a formulation to directly learn the\nreconstruction filter. The filter is initialized with the ideal Ramp filter as\na strong pre-training and learned in frequency domain. We compare the learned\nfilter with the Ram-Lak and the Ramp filter on a numerical phantom as well as\non a real CT dataset. The results show that the network properly discretizes\nthe continuous Ramp filter and converges towards the Ram-Lak solution. In our\nview these observations are interesting to gain a better understanding of deep\nlearning techniques and traditional analytic techniques such as Wiener\nfiltering and discretization theory. Furthermore, this will allow fully\ntrainable data-driven reconstruction deep learning approaches.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 13:57:00 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 13:15:14 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Syben", "Christopher", ""], ["Stimpel", "Bernhard", ""], ["Breininger", "Katharina", ""], ["W\u00fcrfl", "Tobias", ""], ["Fahrig", "Rebecca", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1710.06288", "submitter": "Seokju Lee", "authors": "Seokju Lee, Junsik Kim, Jae Shin Yoon, Seunghak Shin, Oleksandr Bailo,\n  Namil Kim, Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, In So Kweon", "title": "VPGNet: Vanishing Point Guided Network for Lane and Road Marking\n  Detection and Recognition", "comments": "To appear on ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a unified end-to-end trainable multi-task network\nthat jointly handles lane and road marking detection and recognition that is\nguided by a vanishing point under adverse weather conditions. We tackle rainy\nand low illumination conditions, which have not been extensively studied until\nnow due to clear challenges. For example, images taken under rainy days are\nsubject to low illumination, while wet roads cause light reflection and distort\nthe appearance of lane and road markings. At night, color distortion occurs\nunder limited illumination. As a result, no benchmark dataset exists and only a\nfew developed algorithms work under poor weather conditions. To address this\nshortcoming, we build up a lane and road marking benchmark which consists of\nabout 20,000 images with 17 lane and road marking classes under four different\nscenarios: no rain, rain, heavy rain, and night. We train and evaluate several\nversions of the proposed multi-task network and validate the importance of each\ntask. The resulting approach, VPGNet, can detect and classify lanes and road\nmarkings, and predict a vanishing point with a single forward pass.\nExperimental results show that our approach achieves high accuracy and\nrobustness under various conditions in real-time (20 fps). The benchmark and\nthe VPGNet model will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 13:57:29 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Lee", "Seokju", ""], ["Kim", "Junsik", ""], ["Yoon", "Jae Shin", ""], ["Shin", "Seunghak", ""], ["Bailo", "Oleksandr", ""], ["Kim", "Namil", ""], ["Lee", "Tae-Hee", ""], ["Hong", "Hyun Seok", ""], ["Han", "Seung-Hoon", ""], ["Kweon", "In So", ""]]}, {"id": "1710.06303", "submitter": "Aditya Mogadala", "authors": "Aditya Mogadala, Umanga Bista, Lexing Xie and Achim Rettinger", "title": "Describing Natural Images Containing Novel Objects with Knowledge Guided\n  Assitance", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images in the wild encapsulate rich knowledge about varied abstract concepts\nand cannot be sufficiently described with models built only using image-caption\npairs containing selected objects. We propose to handle such a task with the\nguidance of a knowledge base that incorporate many abstract concepts. Our\nmethod is a two-step process where we first build a multi-entity-label image\nrecognition model to predict abstract concepts as image labels and then\nleverage them in the second step as an external semantic attention and\nconstrained inference in the caption generation model for describing images\nthat depict unseen/novel objects. Evaluations show that our models outperform\nmost of the prior work for out-of-domain captioning on MSCOCO and are useful\nfor integration of knowledge and vision in general.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:11:37 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mogadala", "Aditya", ""], ["Bista", "Umanga", ""], ["Xie", "Lexing", ""], ["Rettinger", "Achim", ""]]}, {"id": "1710.06304", "submitter": "Sanketh Vedula", "authors": "Sanketh Vedula, Ortal Senouf, Alex M. Bronstein, Oleg V. Michailovich,\n  Michael Zibulevsky", "title": "Towards CT-quality Ultrasound Imaging using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV physics.med-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The cost-effectiveness and practical harmlessness of ultrasound imaging have\nmade it one of the most widespread tools for medical diagnosis. Unfortunately,\nthe beam-forming based image formation produces granular speckle noise,\nblurring, shading and other artifacts. To overcome these effects, the ultimate\ngoal would be to reconstruct the tissue acoustic properties by solving a full\nwave propagation inverse problem. In this work, we make a step towards this\ngoal, using Multi-Resolution Convolutional Neural Networks (CNN). As a result,\nwe are able to reconstruct CT-quality images from the reflected ultrasound\nradio-frequency(RF) data obtained by simulation from real CT scans of a human\nbody. We also show that CNN is able to imitate existing computationally heavy\ndespeckling methods, thereby saving orders of magnitude in computations and\nmaking them amenable to real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:11:57 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Vedula", "Sanketh", ""], ["Senouf", "Ortal", ""], ["Bronstein", "Alex M.", ""], ["Michailovich", "Oleg V.", ""], ["Zibulevsky", "Michael", ""]]}, {"id": "1710.06319", "submitter": "Patrick Schwab", "authors": "Patrick Schwab, Gaetano Scebba, Jia Zhang, Marco Delai, Walter Karlen", "title": "Beat by Beat: Classifying Cardiac Arrhythmias with Recurrent Neural\n  Networks", "comments": "Accepted at Computing in Cardiology (CinC) 2017", "journal-ref": null, "doi": "10.22489/CinC.2017.363-223", "report-no": null, "categories": "cs.LG cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With tens of thousands of electrocardiogram (ECG) records processed by mobile\ncardiac event recorders every day, heart rhythm classification algorithms are\nan important tool for the continuous monitoring of patients at risk. We utilise\nan annotated dataset of 12,186 single-lead ECG recordings to build a diverse\nensemble of recurrent neural networks (RNNs) that is able to distinguish\nbetween normal sinus rhythms, atrial fibrillation, other types of arrhythmia\nand signals that are too noisy to interpret. In order to ease learning over the\ntemporal dimension, we introduce a novel task formulation that harnesses the\nnatural segmentation of ECG signals into heartbeats to drastically reduce the\nnumber of time steps per sequence. Additionally, we extend our RNNs with an\nattention mechanism that enables us to reason about which heartbeats our RNNs\nfocus on to make their decisions. Through the use of attention, our model\nmaintains a high degree of interpretability, while also achieving\nstate-of-the-art classification performance with an average F1 score of 0.79 on\nan unseen test set (n=3,658).\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 14:39:17 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 09:51:04 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Schwab", "Patrick", ""], ["Scebba", "Gaetano", ""], ["Zhang", "Jia", ""], ["Delai", "Marco", ""], ["Karlen", "Walter", ""]]}, {"id": "1710.06368", "submitter": "Zhiyu Sun", "authors": "Zhiyu Sun and Yusen He and Andrey Gritsenko and Amaury Lendasse and\n  Stephen Baek", "title": "Embedded Spectral Descriptors: Learning the point-wise correspondence\n  metric via Siamese neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust and informative local shape descriptor plays an important role in\nmesh registration. In this regard, spectral descriptors that are based on the\nspectrum of the Laplace-Beltrami operator have been a popular subject of\nresearch for the last decade due to their advantageous properties, such as\nisometry invariance. Despite such, however, spectral descriptors often fail to\ngive a correct similarity measure for non-isometric cases where the metric\ndistortion between the models is large. Hence, they are not reliable for\ncorrespondence matching problems when the models are not isometric. In this\npaper, it is proposed a method to improve the similarity metric of spectral\ndescriptors for correspondence matching problems. We embed a spectral shape\ndescriptor into a different metric space where the Euclidean distance between\nthe elements directly indicates the geometric dissimilarity. We design and\ntrain a Siamese neural network to find such an embedding, where the embedded\ndescriptors are promoted to rearrange based on the geometric similarity. We\ndemonstrate our approach can significantly enhance the performance of the\nconventional spectral descriptors by the simple augmentation achieved via the\nSiamese neural network in comparison to other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 16:26:04 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 21:30:24 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2019 07:37:30 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Sun", "Zhiyu", ""], ["He", "Yusen", ""], ["Gritsenko", "Andrey", ""], ["Lendasse", "Amaury", ""], ["Baek", "Stephen", ""]]}, {"id": "1710.06422", "submitter": "Kuan Fang", "authors": "Kuan Fang, Yunfei Bai, Stefan Hinterstoisser, Silvio Savarese, Mrinal\n  Kalakrishnan", "title": "Multi-Task Domain Adaptation for Deep Learning of Instance Grasping from\n  Simulation", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning-based approaches to robotic manipulation are limited by the\nscalability of data collection and accessibility of labels. In this paper, we\npresent a multi-task domain adaptation framework for instance grasping in\ncluttered scenes by utilizing simulated robot experiments. Our neural network\ntakes monocular RGB images and the instance segmentation mask of a specified\ntarget object as inputs, and predicts the probability of successfully grasping\nthe specified object for each candidate motor command. The proposed transfer\nlearning framework trains a model for instance grasping in simulation and uses\na domain-adversarial loss to transfer the trained model to real robots using\nindiscriminate grasping data, which is available both in simulation and the\nreal world. We evaluate our model in real-world robot experiments, comparing it\nwith alternative model architectures as well as an indiscriminate grasping\nbaseline.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 17:54:50 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 04:08:58 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Fang", "Kuan", ""], ["Bai", "Yunfei", ""], ["Hinterstoisser", "Stefan", ""], ["Savarese", "Silvio", ""], ["Kalakrishnan", "Mrinal", ""]]}, {"id": "1710.06473", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi, Annie Vahedipour Tabrizi, Benjamin Robertson,\n  Andrew Spence", "title": "Superpixels Based Marker Tracking Vs. Hue Thresholding In Rodent\n  Biomechanics Application", "comments": "This paper has been accepted for 2017 Asilomar conference, IEEE", "journal-ref": null, "doi": "10.1109/ACSSC.2017.8335168", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examining locomotion has improved our basic understanding of motor control\nand aided in treating motor impairment. Mice and rats are premier models of\nhuman disease and increasingly the model systems of choice for basic\nneuroscience. High frame rates (250 Hz) are needed to quantify the kinematics\nof these running rodents. Manual tracking, especially for multiple markers,\nbecomes time-consuming and impossible for large sample sizes. Therefore, the\nneed for automatic segmentation of these markers has grown in recent years. We\npropose two methods to segment and track these markers: first, using SLIC\nsuperpixels segmentation with a tracker based on position, speed, shape, and\ncolor information of the segmented region in the previous frame; second, using\na thresholding on hue channel following up with the same tracker. The\ncomparison showed that the SLIC superpixels method was superior because the\nsegmentation was more reliable and based on both color and spatial information.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 19:08:29 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 15:31:45 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 20:50:33 GMT"}, {"version": "v4", "created": "Mon, 28 May 2018 15:48:19 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Maghsoudi", "Omid Haji", ""], ["Tabrizi", "Annie Vahedipour", ""], ["Robertson", "Benjamin", ""], ["Spence", "Andrew", ""]]}, {"id": "1710.06495", "submitter": "Ashraf Qadir", "authors": "Ashraf Qadir, Jeremiah Neubert", "title": "A Line-Point Unified Solution to Relative Camera Pose Estimation", "comments": "Submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a unified method of relative camera pose estimation\nfrom points and lines correspondences. Given a set of 2D points and lines\ncorrespondences in three views, of which two are known, a method has been\ndeveloped for estimating the camera pose of the third view. Novelty of this\nalgorithm is to combine both points and lines correspondences in the camera\npose estimation which enables us to compute relative camera pose with a small\nnumber of feature correspondences. Our central idea is to exploit the\ntri-linear relationship between three views and generate a set of linear\nequations from the points and lines correspondences in the three views. The\ndesired solution to the system of equations are expressed as a linear\ncombination of the singular vectors and the coefficients are computed by\nsolving a small set of quadratic equations generated by imposing orthonormality\nconstraints for general camera motion. The advantages of the proposed method\nare demonstrated by experimenting on publicly available data set. Results show\nthe robustness and efficiency of the method in relative camera pose estimation\nfor both small and large camera motion with a small set of points and line\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 20:27:51 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Qadir", "Ashraf", ""], ["Neubert", "Jeremiah", ""]]}, {"id": "1710.06501", "submitter": "Bilal Alsallakh", "authors": "Bilal Alsallakh, Amin Jourabloo, Mao Ye, Xiaoming Liu, Liu Ren", "title": "Do Convolutional Neural Networks Learn Class Hierarchy?", "comments": "Video demo at https://vimeo.com/228263798", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, Volume:\n  24, Issue: 1 (2018)", "doi": "10.1109/TVCG.2017.2744683", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) currently achieve state-of-the-art\naccuracy in image classification. With a growing number of classes, the\naccuracy usually drops as the possibilities of confusion increase.\nInterestingly, the class confusion patterns follow a hierarchical structure\nover the classes. We present visual-analytics methods to reveal and analyze\nthis hierarchy of similar classes in relation with CNN-internal data. We found\nthat this hierarchy not only dictates the confusion patterns between the\nclasses, it furthermore dictates the learning behavior of CNNs. In particular,\nthe early layers in these networks develop feature detectors that can separate\nhigh-level groups of classes quite well, even after a few training epochs. In\ncontrast, the latter layers require substantially more epochs to develop\nspecialized feature detectors that can separate individual classes. We\ndemonstrate how these insights are key to significant improvement in accuracy\nby designing hierarchy-aware CNNs that accelerate model convergence and\nalleviate overfitting. We further demonstrate how our methods help in\nidentifying various quality issues in the training data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 21:02:59 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Alsallakh", "Bilal", ""], ["Jourabloo", "Amin", ""], ["Ye", "Mao", ""], ["Liu", "Xiaoming", ""], ["Ren", "Liu", ""]]}, {"id": "1710.06507", "submitter": "Yi-Hsuan Tsai", "authors": "Wei-Chih Hung, Yi-Hsuan Tsai, Xiaohui Shen, Zhe Lin, Kalyan\n  Sunkavalli, Xin Lu, Ming-Hsuan Yang", "title": "Scene Parsing with Global Context Embedding", "comments": "Accepted in ICCV'17. Code available at\n  https://github.com/hfslyc/GCPNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scene parsing method that utilizes global context information\nbased on both the parametric and non- parametric models. Compared to previous\nmethods that only exploit the local relationship between objects, we train a\ncontext network based on scene similarities to generate feature representations\nfor global contexts. In addition, these learned features are utilized to\ngenerate global and spatial priors for explicit classes inference. We then\ndesign modules to embed the feature representations and the priors into the\nsegmentation network as additional global context cues. We show that the\nproposed method can eliminate false positives that are not compatible with the\nglobal context representations. Experiments on both the MIT ADE20K and PASCAL\nContext datasets show that the proposed method performs favorably against\nexisting methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 21:36:03 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 22:04:47 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Hung", "Wei-Chih", ""], ["Tsai", "Yi-Hsuan", ""], ["Shen", "Xiaohui", ""], ["Lin", "Zhe", ""], ["Sunkavalli", "Kalyan", ""], ["Lu", "Xin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1710.06511", "submitter": "Mostafa Amin-Naji", "authors": "Mostafa Amin-Naji and Ali Aghagolzadeh", "title": "Multi-focus image fusion using VOL and EOL in DCT domain", "comments": "2016 1st International Conference on New Research Achievements in\n  Electrical and Computer Engineering (ICNRAECE), pp. 728-733, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of multi-focus image fusion is gathering the essential\ninformation and the focused parts from the input multi-focus images into a\nsingle image. These multi-focused images are captured with different depths of\nfocus of cameras. Multi-focus image fusion is very time-saving and appropriate\nin discrete cosine transform (DCT) domain, especially when JPEG images are used\nin visual sensor networks (VSN). The previous works in DCT domain have some\nerrors in selection of the suitable divided blocks according to their criterion\nfor measurement of the block contrast. In this paper, we used variance of\nLaplacian (VOL) and energy of Laplacian (EOL) as criterion to measure the\ncontrast of image. Also in this paper, the EOL and VOL calculations directly in\nDCT domain are prepared using vector processing. We developed four matrices\nwhich calculate the Laplacian of block easily in DCT domain. Our works greatly\nreduce error due to unsuitable block selection. The results of the proposed\nalgorithms are compared with the previous algorithms in order to demonstrate\nthe superiority of the output image quality in the proposed methods. The\nseveral JPEG multi-focus images are used in experiments and their fused image\nby our proposed methods and the other algorithms are compared with different\nmeasurement criteria.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 21:53:47 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 20:24:36 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Amin-Naji", "Mostafa", ""], ["Aghagolzadeh", "Ali", ""]]}, {"id": "1710.06512", "submitter": "Anna Sokolova", "authors": "Anna Sokolova, Anton Konushin", "title": "Pose-based Deep Gait Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human gait or walking manner is a biometric feature that allows\nidentification of a person when other biometric features such as the face or\niris are not visible. In this paper, we present a new pose-based convolutional\nneural network model for gait recognition. Unlike many methods that consider\nthe full-height silhouette of a moving person, we consider the motion of points\nin the areas around human joints. To extract motion information, we estimate\nthe optical flow between consecutive frames. We propose a deep convolutional\nmodel that computes pose-based gait descriptors. We compare different network\narchitectures and aggregation methods and experimentally assess various sets of\nbody parts to determine which are the most important for gait recognition. In\naddition, we investigate the generalization ability of the developed algorithms\nby transferring them between datasets. The results of these experiments show\nthat our approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 21:58:02 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 14:08:22 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 13:04:40 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Sokolova", "Anna", ""], ["Konushin", "Anton", ""]]}, {"id": "1710.06513", "submitter": "Yuanlu Xu", "authors": "Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu, Song-Chun Zhu", "title": "Learning Pose Grammar to Encode Human Body Configuration for 3D Pose\n  Estimation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pose grammar to tackle the problem of 3D human\npose estimation. Our model directly takes 2D pose as input and learns a\ngeneralized 2D-3D mapping function. The proposed model consists of a base\nnetwork which efficiently captures pose-aligned features and a hierarchy of\nBi-directional RNNs (BRNN) on the top to explicitly incorporate a set of\nknowledge regarding human body configuration (i.e., kinematics, symmetry, motor\ncoordination). The proposed model thus enforces high-level constraints over\nhuman poses. In learning, we develop a pose sample simulator to augment\ntraining samples in virtual camera views, which further improves our model\ngeneralizability. We validate our method on public 3D human pose benchmarks and\npropose a new evaluation protocol working on cross-view setting to verify the\ngeneralization capability of different methods. We empirically observe that\nmost state-of-the-art methods encounter difficulty under such setting while our\nmethod can well handle such challenges.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 22:05:19 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 10:27:33 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 22:56:40 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 08:49:06 GMT"}, {"version": "v5", "created": "Tue, 12 Dec 2017 21:37:59 GMT"}, {"version": "v6", "created": "Thu, 4 Jan 2018 22:50:45 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Fang", "Haoshu", ""], ["Xu", "Yuanlu", ""], ["Wang", "Wenguan", ""], ["Liu", "Xiaobai", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1710.06518", "submitter": "Michel Meneses", "authors": "Michel Conrado Cardoso Meneses", "title": "Sistema de Navega\\c{c}\\~ao Aut\\^onomo Baseado em Vis\\~ao Computacional", "comments": "in Portuguese. Thesis presented to the Federal University of Sergipe,\n  at Sergipe, Brazil in partial fulfillment of the requirement for the degree\n  of Bachelor of Science in Computer Engineering. A demonstration of this\n  project can be watched by this link: https://youtu.be/hzyKAGhQExg Advisors:\n  Dr. Leonardo Nogueira Matos, Dr. Bruno Otavio Piedade Prado", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous robots are used as the tool to solve many kinds of problems, such\nas environmental mapping and monitoring. Either for adverse conditions related\nto the human presence or even for the need to reduce costs, it is certain that\nmany efforts have been made to develop robots with an increasingly high level\nof autonomy. They must be capable of locomotion through dynamic environments,\nwithout human operators or assistant systems' help. It is noted, thus, that the\nform of perception and modeling of the environment becomes significantly\nrelevant to navigation. Among the main sensing methods are those based on\nvision. Through this, it is possible to create highly-detailed models about the\nenvironment, since many characteristics can be measured, such as texture,\ncolor, and illumination. However, the most accurate vision-based navigation\ntechniques are computationally expensive to run on low-cost mobile platforms.\nTherefore, the goal of this work was to develop a low-cost robot, controlled by\na Raspberry Pi, whose navigation system is based on vision. For this purpose,\nthe strategy used consisted in identifying obstacles via optical flow pattern\nrecognition. Through this signal, it is possible to infer the relative\ndisplacement between the robot and other elements in the environment. Its\nestimation was done using the Lucas-Kanade algorithm, which can be executed by\nthe Raspberry Pi without harming its performance. Finally, an SVM based\nclassifier was used to identify patterns of this signal associated with\nobstacles movement. The developed system was evaluated considering its\nexecution over an optical flow pattern dataset extracted from a real navigation\nenvironment. In the end, it was verified that the processing frequency of the\nsystem was superior to the others. Furthermore, its accuracy and acquisition\ncost were, respectively, higher and lower than most of the cited works.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 22:37:07 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Meneses", "Michel Conrado Cardoso", ""]]}, {"id": "1710.06555", "submitter": "Dangwei Li", "authors": "Dangwei Li, Xiaotang Chen, Zhang Zhang and Kaiqi Huang", "title": "Learning Deep Context-aware Features over Body and Latent Parts for\n  Person Re-identification", "comments": "Accepted by CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-identification (ReID) is to identify the same person across\ndifferent cameras. It is a challenging task due to the large variations in\nperson pose, occlusion, background clutter, etc How to extract powerful\nfeatures is a fundamental problem in ReID and is still an open problem today.\nIn this paper, we design a Multi-Scale Context-Aware Network (MSCAN) to learn\npowerful features over full body and body parts, which can well capture the\nlocal context knowledge by stacking multi-scale convolutions in each layer.\nMoreover, instead of using predefined rigid parts, we propose to learn and\nlocalize deformable pedestrian parts using Spatial Transformer Networks (STN)\nwith novel spatial constraints. The learned body parts can release some\ndifficulties, eg pose variations and background clutters, in part-based\nrepresentation. Finally, we integrate the representation learning processes of\nfull body and body parts into a unified framework for person ReID through\nmulti-class person identification tasks. Extensive evaluations on current\nchallenging large-scale person ReID datasets, including the image-based\nMarket1501, CUHK03 and sequence-based MARS datasets, show that the proposed\nmethod achieves the state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 01:48:02 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Li", "Dangwei", ""], ["Chen", "Xiaotang", ""], ["Zhang", "Zhang", ""], ["Huang", "Kaiqi", ""]]}, {"id": "1710.06608", "submitter": "Johannes Stegmaier", "authors": "Johannes Stegmaier, Thiago V. Spina, Alexandre X. Falc\\~ao, Andreas\n  Bartschat, Ralf Mikut, Elliot Meyerowitz, Alexandre Cunha", "title": "Cell Segmentation in 3D Confocal Images using Supervoxel Merge-Forests\n  with CNN-based Hypothesis Selection", "comments": "5 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation approaches are crucial to quantitatively analyze\nlarge-scale 3D microscopy images. Particularly in deep tissue regions,\nautomatic methods still fail to provide error-free segmentations. To improve\nthe segmentation quality throughout imaged samples, we present a new\nsupervoxel-based 3D segmentation approach that outperforms current methods and\nreduces the manual correction effort. The algorithm consists of gentle\npreprocessing and a conservative super-voxel generation method followed by\nsupervoxel agglomeration based on local signal properties and a postprocessing\nstep to fix under-segmentation errors using a Convolutional Neural Network. We\nvalidate the functionality of the algorithm on manually labeled 3D confocal\nimages of the plant Arabidopis thaliana and compare the results to a\nstate-of-the-art meristem segmentation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 07:53:27 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Stegmaier", "Johannes", ""], ["Spina", "Thiago V.", ""], ["Falc\u00e3o", "Alexandre X.", ""], ["Bartschat", "Andreas", ""], ["Mikut", "Ralf", ""], ["Meyerowitz", "Elliot", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1710.06617", "submitter": "Dimosthenis Karatzas", "authors": "Dimosthenis Karatzas, Lluis G\\'omez, Anguelos Nicolaou, Mar\\c{c}al\n  Rusi\\~nol", "title": "The Robust Reading Competition Annotation and Evaluation Platform", "comments": "6 pages, accepted to DAS 2018", "journal-ref": "Proc. of the 13th IAPR Int. W. on Document Analysis Systems (DAS\n  2018), IEEE CPS, pp. 61-66, 2018", "doi": "10.1109/DAS.2018.22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ICDAR Robust Reading Competition (RRC), initiated in 2003 and\nre-established in 2011, has become a de-facto evaluation standard for robust\nreading systems and algorithms. Concurrent with its second incarnation in 2011,\na continuous effort started to develop an on-line framework to facilitate the\nhosting and management of competitions. This paper outlines the Robust Reading\nCompetition Annotation and Evaluation Platform, the backbone of the\ncompetitions. The RRC Annotation and Evaluation Platform is a modular\nframework, fully accessible through on-line interfaces. It comprises a\ncollection of tools and services for managing all processes involved with\ndefining and evaluating a research task, from dataset definition to annotation\nmanagement, evaluation specification and results analysis. Although the\nframework has been designed with robust reading research in mind, many of the\nprovided tools are generic by design. All aspects of the RRC Annotation and\nEvaluation Framework are available for research use.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 08:27:31 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 17:10:01 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Karatzas", "Dimosthenis", ""], ["G\u00f3mez", "Lluis", ""], ["Nicolaou", "Anguelos", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""]]}, {"id": "1710.06647", "submitter": "Tom Tirer", "authors": "Tom Tirer, Raja Giryes", "title": "Image Restoration by Iterative Denoising and Backward Projections", "comments": "To appear in IEEE Transactions on Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems appear in many applications, such as image deblurring and\ninpainting. The common approach to address them is to design a specific\nalgorithm for each problem. The Plug-and-Play (P&P) framework, which has been\nrecently introduced, allows solving general inverse problems by leveraging the\nimpressive capabilities of existing denoising algorithms. While this fresh\nstrategy has found many applications, a burdensome parameter tuning is often\nrequired in order to obtain high-quality results. In this work, we propose an\nalternative method for solving inverse problems using off-the-shelf denoisers,\nwhich requires less parameter tuning. First, we transform a typical cost\nfunction, composed of fidelity and prior terms, into a closely related, novel\noptimization problem. Then, we propose an efficient minimization scheme with a\nplug-and-play property, i.e., the prior term is handled solely by a denoising\noperation. Finally, we present an automatic tuning mechanism to set the\nmethod's parameters. We provide a theoretical analysis of the method, and\nempirically demonstrate its competitiveness with task-specific techniques and\nthe P&P approach for image inpainting and deblurring.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 09:39:30 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 14:20:16 GMT"}, {"version": "v3", "created": "Thu, 4 Jan 2018 12:54:02 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 20:35:55 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Tirer", "Tom", ""], ["Giryes", "Raja", ""]]}, {"id": "1710.06668", "submitter": "Raphael Sznitman", "authors": "Thomas Kurmann, Pablo Marquez Neila, Xiaofei Du, Pascal Fua, Danail\n  Stoyanov, Sebastian Wolf, Raphael Sznitman", "title": "Simultaneous Recognition and Pose Estimation of Instruments in Minimally\n  Invasive Surgery", "comments": "8 pages, 2 figures, MICCAI 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66185-8_57", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detection of surgical instruments plays a key role in ensuring patient safety\nin minimally invasive surgery. In this paper, we present a novel method for 2D\nvision-based recognition and pose estimation of surgical instruments that\ngeneralizes to different surgical applications. At its core, we propose a novel\nscene model in order to simultaneously recognize multiple instruments as well\nas their parts. We use a Convolutional Neural Network architecture to embody\nour model and show that the cross-entropy loss is well suited to optimize its\nparameters which can be trained in an end-to-end fashion. An additional\nadvantage of our approach is that instrument detection at test time is achieved\nwhile avoiding the need for scale-dependent sliding window evaluation. This\nallows our approach to be relatively parameter free at test time and shows good\nperformance for both instrument detection and tracking. We show that our\napproach surpasses state-of-the-art results on in-vivo retinal microsurgery\nimage data, as well as ex-vivo laparoscopic sequences.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 10:37:01 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Kurmann", "Thomas", ""], ["Neila", "Pablo Marquez", ""], ["Du", "Xiaofei", ""], ["Fua", "Pascal", ""], ["Stoyanov", "Danail", ""], ["Wolf", "Sebastian", ""], ["Sznitman", "Raphael", ""]]}, {"id": "1710.06677", "submitter": "Feras Dayoub", "authors": "Dimity Miller, Lachlan Nicholson, Feras Dayoub, Niko S\\\"underhauf", "title": "Dropout Sampling for Robust Object Detection in Open-Set Conditions", "comments": "to appear in IEEE International Conference on Robotics and Automation\n  2018 (ICRA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dropout Variational Inference, or Dropout Sampling, has been recently\nproposed as an approximation technique for Bayesian Deep Learning and evaluated\nfor image classification and regression tasks. This paper investigates the\nutility of Dropout Sampling for object detection for the first time. We\ndemonstrate how label uncertainty can be extracted from a state-of-the-art\nobject detection system via Dropout Sampling. We evaluate this approach on a\nlarge synthetic dataset of 30,000 images, and a real-world dataset captured by\na mobile robot in a versatile campus environment. We show that this uncertainty\ncan be utilized to increase object detection performance under the open-set\nconditions that are typically encountered in robotic vision. A Dropout Sampling\nnetwork is shown to achieve a 12.3% increase in recall (for the same precision\nscore as a standard network) and a 15.1% increase in precision (for the same\nrecall score as the standard network).\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 11:16:53 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 06:10:02 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Miller", "Dimity", ""], ["Nicholson", "Lachlan", ""], ["Dayoub", "Feras", ""], ["S\u00fcnderhauf", "Niko", ""]]}, {"id": "1710.06805", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Kyung-Ah Sohn", "title": "Enhancing the Performance of Convolutional Neural Networks on Quality\n  Degraded Datasets", "comments": "The International Conference on Digital Image Computing: Techniques\n  and Applications (DICTA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the appeal of deep neural networks that largely replace the\ntraditional handmade filters, they still suffer from isolated cases that cannot\nbe properly handled only by the training of convolutional filters. Abnormal\nfactors, including real-world noise, blur, or other quality degradations, ruin\nthe output of a neural network. These unexpected problems can produce critical\ncomplications, and it is surprising that there has only been minimal research\ninto the effects of noise in the deep neural network model. Therefore, we\npresent an exhaustive investigation into the effect of noise in image\nclassification and suggest a generalized architecture of a dual-channel model\nto treat quality degraded input images. We compare the proposed dual-channel\nmodel with a simple single model and show it improves the overall performance\nof neural networks on various types of quality degraded input datasets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 15:57:41 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["Yim", "Jonghwa", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1710.06824", "submitter": "Shervin Minaee", "authors": "Shervin Minaee, Siyun Wang, Yao Wang, Sohae Chung, Xiuyuan Wang, Els\n  Fieremans, Steven Flanagan, Joseph Rath, Yvonne W. Lui", "title": "Identifying Mild Traumatic Brain Injury Patients From MR Images Using\n  Bag of Visual Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mild traumatic brain injury (mTBI) is a growing public health problem with an\nestimated incidence of one million people annually in US. Neurocognitive tests\nare used to both assess the patient condition and to monitor the patient\nprogress. This work aims to directly use MR images taken shortly after injury\nto detect whether a patient suffers from mTBI, by incorporating machine\nlearning and computer vision techniques to learn features suitable\ndiscriminating between mTBI and normal patients. We focus on 3 regions in\nbrain, and extract multiple patches from them, and use bag-of-visual-word\ntechnique to represent each subject as a histogram of representative patterns\nderived from patches from all training subjects. After extracting the features,\nwe use greedy forward feature selection, to choose a subset of features which\nachieves highest accuracy. We show through experimental studies that BoW\nfeatures perform better than the simple mean value features which were used\npreviously.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 16:55:52 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 22:42:25 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 22:16:08 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Siyun", ""], ["Wang", "Yao", ""], ["Chung", "Sohae", ""], ["Wang", "Xiuyuan", ""], ["Fieremans", "Els", ""], ["Flanagan", "Steven", ""], ["Rath", "Joseph", ""], ["Lui", "Yvonne W.", ""]]}, {"id": "1710.06836", "submitter": "Dianna Radpour", "authors": "Vivek Bheda and Dianna Radpour", "title": "Using Deep Convolutional Networks for Gesture Recognition in American\n  Sign Language", "comments": "12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the realm of multimodal communication, sign language is, and continues to\nbe, one of the most understudied areas. In line with recent advances in the\nfield of deep learning, there are far reaching implications and applications\nthat neural networks can have for sign language interpretation. In this paper,\nwe present a method for using deep convolutional networks to classify images of\nboth the the letters and digits in American Sign Language.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 17:35:04 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 14:55:18 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 16:49:10 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Bheda", "Vivek", ""], ["Radpour", "Dianna", ""]]}, {"id": "1710.06854", "submitter": "Anca Sticlaru", "authors": "Anca Sticlaru", "title": "Material Classification using Neural Networks", "comments": "45 pages, BSc thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition and classification of the diversity of materials that exist\nin the environment around us are a key visual competence that computer vision\nsystems focus on in recent years. Understanding the identification of materials\nin distinct images involves a deep process that has made usage of the recent\nprogress in neural networks which has brought the potential to train\narchitectures to extract features for this challenging task. This project uses\nstate-of-the-art Convolutional Neural Network (CNN) techniques and Support\nVector Machine (SVM) classifiers in order to classify materials and analyze the\nresults. Building on various widely used material databases collected, a\nselection of CNN architectures is evaluated to understand which is the best\napproach to extract features in order to achieve outstanding results for the\ntask. The results gathered over four material datasets and nine CNNs outline\nthat the best overall performance of a CNN using a linear SVM can achieve up to\n~92.5% mean average precision, while applying a new relevant direction in\ncomputer vision, transfer learning. By limiting the amount of information\nextracted from the layer before the last fully connected layer, transfer\nlearning aims at analyzing the contribution of shading information and\nreflectance to identify which main characteristics decide the material category\nthe image belongs to. In addition to the main topic of my project, the\nevaluation of the nine different CNN architectures, it is questioned if, by\nusing the transfer learning instead of extracting the information from the last\nconvolutional layer, the total accuracy of the system created improves. The\nresults of the comparison emphasize the fact that the accuracy and performance\nof the system improve, especially in the datasets which consist of a large\nnumber of images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 23:24:39 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Sticlaru", "Anca", ""]]}, {"id": "1710.06924", "submitter": "Xingchao Peng", "authors": "Xingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang,\n  Kate Saenko", "title": "VisDA: The Visual Domain Adaptation Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the 2017 Visual Domain Adaptation (VisDA) dataset and challenge, a\nlarge-scale testbed for unsupervised domain adaptation across visual domains.\nUnsupervised domain adaptation aims to solve the real-world problem of domain\nshift, where machine learning models trained on one domain must be transferred\nand adapted to a novel visual domain without additional supervision. The\nVisDA2017 challenge is focused on the simulation-to-reality shift and has two\nassociated tasks: image classification and image segmentation. The goal in both\ntracks is to first train a model on simulated, synthetic data in the source\ndomain and then adapt it to perform well on real image data in the unlabeled\ntest domain. Our dataset is the largest one to date for cross-domain object\nclassification, with over 280K images across 12 categories in the combined\ntraining, validation and testing domains. The image segmentation dataset is\nalso large-scale with over 30K images across 18 categories in the three\ndomains. We compare VisDA to existing cross-domain adaptation datasets and\nprovide a baseline performance analysis using various domain adaptation models\nthat are currently popular in the field.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 20:20:49 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 04:04:18 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Peng", "Xingchao", ""], ["Usman", "Ben", ""], ["Kaushik", "Neela", ""], ["Hoffman", "Judy", ""], ["Wang", "Dequan", ""], ["Saenko", "Kate", ""]]}, {"id": "1710.06929", "submitter": "Johan Ekekrantz", "authors": "Johan Ekekrantz, Nils Bore, Rares Ambrus, John Folkesson and Patric\n  Jensfelt", "title": "Unsupervised Object Discovery and Segmentation of RGBD-images", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a system for unsupervised object discovery and\nsegmentation of RGBD-images. The system models the sensor noise directly from\ndata, allowing accurate segmentation without sensor specific hand tuning of\nmeasurement noise models making use of the recently introduced Statistical\nInlier Estimation (SIE) method. Through a fully probabilistic formulation, the\nsystem is able to apply probabilistic inference, enabling reliable segmentation\nin previously challenging scenarios. In addition, we introduce new methods for\nfiltering out false positives, significantly improving the signal to noise\nratio. We show that the system significantly outperform state-of-the-art in on\na challenging real-world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 20:33:48 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Ekekrantz", "Johan", ""], ["Bore", "Nils", ""], ["Ambrus", "Rares", ""], ["Folkesson", "John", ""], ["Jensfelt", "Patric", ""]]}, {"id": "1710.06993", "submitter": "Hanjiang Lai", "authors": "Hanjiang Lai and Yan Pan", "title": "Improved Search in Hamming Space using Deep Multi-Index Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity-preserving hashing is a widely-used method for nearest neighbour\nsearch in large-scale image retrieval tasks. There has been considerable\nresearch on generating efficient image representation via the\ndeep-network-based hashing methods. However, the issue of efficient searching\nin the deep representation space remains largely unsolved. To this end, we\npropose a simple yet efficient deep-network-based multi-index hashing method\nfor simultaneously learning the powerful image representation and the efficient\nsearching. To achieve these two goals, we introduce the multi-index hashing\n(MIH) mechanism into the proposed deep architecture, which divides the binary\ncodes into multiple substrings. Due to the non-uniformly distributed codes will\nresult in inefficiency searching, we add the two balanced constraints at\nfeature-level and instance-level, respectively. Extensive evaluations on\nseveral benchmark image retrieval datasets show that the learned balanced\nbinary codes bring dramatic speedups and achieve comparable performance over\nthe existing baselines.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 02:51:12 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Lai", "Hanjiang", ""], ["Pan", "Yan", ""]]}, {"id": "1710.07035", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa\n  Sengupta and Anil A Bharath", "title": "Generative Adversarial Networks: An Overview", "comments": "Accepted in the IEEE Signal Processing Magazine Special Issue on Deep\n  Learning for Visual Understanding", "journal-ref": null, "doi": "10.1109/MSP.2017.2765202", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) provide a way to learn deep\nrepresentations without extensively annotated training data. They achieve this\nthrough deriving backpropagation signals through a competitive process\ninvolving a pair of networks. The representations that can be learned by GANs\nmay be used in a variety of applications, including image synthesis, semantic\nimage editing, style transfer, image super-resolution and classification. The\naim of this review paper is to provide an overview of GANs for the signal\nprocessing community, drawing on familiar analogies and concepts where\npossible. In addition to identifying different methods for training and\nconstructing GANs, we also point to remaining challenges in their theory and\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 08:29:50 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Creswell", "Antonia", ""], ["White", "Tom", ""], ["Dumoulin", "Vincent", ""], ["Arulkumaran", "Kai", ""], ["Sengupta", "Biswa", ""], ["Bharath", "Anil A", ""]]}, {"id": "1710.07084", "submitter": "Chongyi Li", "authors": "Chongyi Li and Jichang Guo and Chunle Guo", "title": "Emerging from Water: Underwater Image Color Correction Based on Weakly\n  Supervised Color Transfer", "comments": "Submitted to IEEE Signal Processing Letters", "journal-ref": "IEEE Signal Processing Letters 2018", "doi": "10.1109/LSP.2018.2792050", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater vision suffers from severe effects due to selective attenuation\nand scattering when light propagates through water. Such degradation not only\naffects the quality of underwater images but limits the ability of vision\ntasks. Different from existing methods which either ignore the wavelength\ndependency of the attenuation or assume a specific spectral profile, we tackle\ncolor distortion problem of underwater image from a new view. In this letter,\nwe propose a weakly supervised color transfer method to correct color\ndistortion, which relaxes the need of paired underwater images for training and\nallows for the underwater images unknown where were taken. Inspired by\nCycle-Consistent Adversarial Networks, we design a multi-term loss function\nincluding adversarial loss, cycle consistency loss, and SSIM (Structural\nSimilarity Index Measure) loss, which allows the content and structure of the\ncorrected result the same as the input, but the color as if the image was taken\nwithout the water. Experiments on underwater images captured under diverse\nscenes show that our method produces visually pleasing results, even\noutperforms the art-of-the-state methods. Besides, our method can improve the\nperformance of vision tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 11:09:19 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 09:01:02 GMT"}, {"version": "v3", "created": "Wed, 3 Jan 2018 10:19:32 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Li", "Chongyi", ""], ["Guo", "Jichang", ""], ["Guo", "Chunle", ""]]}, {"id": "1710.07096", "submitter": "Ribana Roscher", "authors": "Anika Bettge, Ribana Roscher, Susanne Wenzel", "title": "Deep Self-taught Learning for Remote Sensing Image Classification", "comments": "This is a corrected version of the final paper published in the\n  proceedings", "journal-ref": "Proceedings of the 2017 conference on Big Data from Space", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the land cover classification task for remote sensing\nimages by deep self-taught learning. Our self-taught learning approach learns\nsuitable feature representations of the input data using sparse representation\nand undercomplete dictionary learning. We propose a deep learning framework\nwhich extracts representations in multiple layers and use the output of the\ndeepest layer as input to a classification algorithm. We evaluate our approach\nusing a multispectral Landsat 5 TM image of a study area in the North of Novo\nProgresso (South America) and the Zurich Summer Data Set provided by the\nUniversity of Zurich. Experiments indicate that features learned by a deep\nself-taught learning framework can be used for classification and improve the\nresults compared to classification results using the original feature\nrepresentation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 11:32:53 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 20:55:12 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Bettge", "Anika", ""], ["Roscher", "Ribana", ""], ["Wenzel", "Susanne", ""]]}, {"id": "1710.07099", "submitter": "Ribana Roscher", "authors": "Anne Braakmann-Folgmann, Ribana Roscher, Susanne Wenzel, Bernd Uebbing\n  and J\\\"urgen Kusche", "title": "Sea Level Anomaly Prediction using Recurrent Neural Networks", "comments": null, "journal-ref": "Proceedings of the 2017 conference on Big Data from Space", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sea level change, one of the most dire impacts of anthropogenic global\nwarming, will affect a large amount of the world's population. However, sea\nlevel change is not uniform in time and space, and the skill of conventional\nprediction methods is limited due to the ocean's internal variabi-lity on\ntimescales from weeks to decades. Here we study the potential of neural network\nmethods which have been used successfully in other applications, but rarely\nbeen applied for this task. We develop a combination of a convolutional neural\nnetwork (CNN) and a recurrent neural network (RNN) to ana-lyse both the spatial\nand the temporal evolution of sea level and to suggest an independent, accurate\nmethod to predict interannual sea level anomalies (SLA). We test our method for\nthe northern and equatorial Pacific Ocean, using gridded altimeter-derived SLA\ndata. We show that the used network designs outperform a simple regression and\nthat adding a CNN improves the skill significantly. The predictions are stable\nover several years.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 11:43:34 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Braakmann-Folgmann", "Anne", ""], ["Roscher", "Ribana", ""], ["Wenzel", "Susanne", ""], ["Uebbing", "Bernd", ""], ["Kusche", "J\u00fcrgen", ""]]}, {"id": "1710.07120", "submitter": "Elif Vural", "authors": "Cem Ornek and Elif Vural", "title": "Nonlinear Supervised Dimensionality Reduction via Smooth Regular\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recovery of the intrinsic geometric structures of data collections is an\nimportant problem in data analysis. Supervised extensions of several manifold\nlearning approaches have been proposed in the recent years. Meanwhile, existing\nmethods primarily focus on the embedding of the training data, and the\ngeneralization of the embedding to initially unseen test data is rather\nignored. In this work, we build on recent theoretical results on the\ngeneralization performance of supervised manifold learning algorithms.\nMotivated by these performance bounds, we propose a supervised manifold\nlearning method that computes a nonlinear embedding while constructing a smooth\nand regular interpolation function that extends the embedding to the whole data\nspace in order to achieve satisfactory generalization. The embedding and the\ninterpolator are jointly learnt such that the Lipschitz regularity of the\ninterpolator is imposed while ensuring the separation between different\nclasses. Experimental results on several image data sets show that the proposed\nmethod outperforms traditional classifiers and the supervised dimensionality\nreduction algorithms in comparison in terms of classification accuracy in most\nsettings.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 12:48:52 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 15:33:18 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Ornek", "Cem", ""], ["Vural", "Elif", ""]]}, {"id": "1710.07161", "submitter": "Marina Zimmermann", "authors": "Marina Zimmermann, Mostafa Mehdipour Ghazi, Haz{\\i}m Kemal Ekenel,\n  Jean-Philippe Thiran", "title": "Visual Speech Recognition Using PCA Networks and LSTMs in a Tandem\n  GMM-HMM System", "comments": null, "journal-ref": "ACCV 2016 Workshops. ACCV 2016. Lecture Notes in Computer Science,\n  vol 10117. Springer, Cham", "doi": "10.1007/978-3-319-54427-4_20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic visual speech recognition is an interesting problem in pattern\nrecognition especially when audio data is noisy or not readily available. It is\nalso a very challenging task mainly because of the lower amount of information\nin the visual articulations compared to the audible utterance. In this work,\nprinciple component analysis is applied to the image patches - extracted from\nthe video data - to learn the weights of a two-stage convolutional network.\nBlock histograms are then extracted as the unsupervised learning features.\nThese features are employed to learn a recurrent neural network with a set of\nlong short-term memory cells to obtain spatiotemporal features. Finally, the\nobtained features are used in a tandem GMM-HMM system for speech recognition.\nOur results show that the proposed method has outperformed the baseline\ntechniques applied to the OuluVS2 audiovisual database for phrase recognition\nwith the frontal view cross-validation and testing sentence correctness\nreaching 79% and 73%, respectively, as compared to the baseline of 74% on\ncross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 14:41:25 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Zimmermann", "Marina", ""], ["Ghazi", "Mostafa Mehdipour", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1710.07168", "submitter": "Marina Zimmermann", "authors": "Marina Zimmermann, Mostafa Mehdipour Ghazi, Haz{\\i}m Kemal Ekenel,\n  Jean-Philippe Thiran", "title": "Combining Multiple Views for Visual Speech Recognition", "comments": null, "journal-ref": "Proceedings of the 14th International Conference on\n  Auditory-Visual Speech Processing (AVSP2017)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual speech recognition is a challenging research problem with a particular\npractical application of aiding audio speech recognition in noisy scenarios.\nMultiple camera setups can be beneficial for the visual speech recognition\nsystems in terms of improved performance and robustness. In this paper, we\nexplore this aspect and provide a comprehensive study on combining multiple\nviews for visual speech recognition. The thorough analysis covers fusion of all\npossible view angle combinations both at feature level and decision level. The\nemployed visual speech recognition system in this study extracts features\nthrough a PCA-based convolutional neural network, followed by an LSTM network.\nFinally, these features are processed in a tandem system, being fed into a\nGMM-HMM scheme. The decision fusion acts after this point by combining the\nViterbi path log-likelihoods. The results show that the complementary\ninformation contained in recordings from different view angles improves the\nresults significantly. For example, the sentence correctness on the test set is\nincreased from 76% for the highest performing single view ($30^\\circ$) to up to\n83% when combining this view with the frontal and $60^\\circ$ view angles.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 14:52:34 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 11:49:58 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zimmermann", "Marina", ""], ["Ghazi", "Mostafa Mehdipour", ""], ["Ekenel", "Haz\u0131m Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1710.07177", "submitter": "Desmond Elliott", "authors": "Desmond Elliott, Stella Frank, Lo\\\"ic Barrault, Fethi Bougares, Lucia\n  Specia", "title": "Findings of the Second Shared Task on Multimodal Machine Translation and\n  Multilingual Image Description", "comments": null, "journal-ref": "Proceedings of the Second Conference on Machine Translation, 2017,\n  pp. 215--233", "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results from the second shared task on multimodal machine\ntranslation and multilingual image description. Nine teams submitted 19 systems\nto two tasks. The multimodal translation task, in which the source sentence is\nsupplemented by an image, was extended with a new language (French) and two new\ntest sets. The multilingual image description task was changed such that at\ntest time, only the image is given. Compared to last year, multimodal systems\nimproved, but text-only systems remain competitive.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:20:14 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Elliott", "Desmond", ""], ["Frank", "Stella", ""], ["Barrault", "Lo\u00efc", ""], ["Bougares", "Fethi", ""], ["Specia", "Lucia", ""]]}, {"id": "1710.07193", "submitter": "Mostafa Amin-Naji", "authors": "Mostafa Amin-Naji and Ali Aghagolzadeh", "title": "Block DCT filtering using vector processing", "comments": "2016 1st International Conference on New Research Achievements in\n  Electrical and Computer Engineering (ICNRAECE), pp. 722-727, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering is an important issue in signals and images processing. Many images\nand videos are compressed using discrete cosine transform (DCT). For reducing\nthe computation complexity, we are interested in filtering block and images\ndirectly in DCT domain. This article proposed an efficient and yet very simple\nfiltering method directly in DCT domain for any symmetric, asymmetric,\nseparable, inseparable and one or two dimensional filter. The proposed method\nis achieved by mathematical relations using vector processing for image\nfiltering which it is equivalent to the spatial domain zero padding filtering.\nAlso to avoid the zero padding artifacts around the edge of the block, we\nprepare preliminary matrices in DCT domain by implementation elements of\nselected mask which satisfies border replication for a block in the spatial\ndomain. To evaluate the performance of the proposed algorithm, we compared the\nspatial domain filtering results with the results of the proposed method in DCT\ndomain. The experiments show that the results of our proposed method in DCT are\nexactly the same as the spatial domain filtering.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:38:33 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Amin-Naji", "Mostafa", ""], ["Aghagolzadeh", "Ali", ""]]}, {"id": "1710.07198", "submitter": "Noa Garcia", "authors": "Noa Garcia and George Vogiatzis", "title": "Dress like a Star: Retrieving Fashion Products from Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a system for retrieving clothing and fashion products from\nvideo content. Although films and television are the perfect showcase for\nfashion brands to promote their products, spectators are not always aware of\nwhere to buy the latest trends they see on screen. Here, a framework for\nbreaking the gap between fashion products shown on videos and users is\npresented. By relating clothing items and video frames in an indexed database\nand performing frame retrieval with temporal aggregation and fast indexing\ntechniques, we can find fashion products from videos in a simple and\nnon-intrusive way. Experiments in a large-scale dataset conducted here show\nthat, by using the proposed framework, memory requirements can be reduced by\n42.5X with respect to linear search, whereas accuracy is maintained at around\n90%.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:45:32 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Garcia", "Noa", ""], ["Vogiatzis", "George", ""]]}, {"id": "1710.07300", "submitter": "Vincent Michalski", "authors": "Samira Ebrahimi Kahou, Vincent Michalski, Adam Atkinson, Akos Kadar,\n  Adam Trischler, Yoshua Bengio", "title": "FigureQA: An Annotated Figure Dataset for Visual Reasoning", "comments": "workshop paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce FigureQA, a visual reasoning corpus of over one million\nquestion-answer pairs grounded in over 100,000 images. The images are\nsynthetic, scientific-style figures from five classes: line plots, dot-line\nplots, vertical and horizontal bar graphs, and pie charts. We formulate our\nreasoning task by generating questions from 15 templates; questions concern\nvarious relationships between plot elements and examine characteristics like\nthe maximum, the minimum, area-under-the-curve, smoothness, and intersection.\nTo resolve, such questions often require reference to multiple plot elements\nand synthesis of information distributed spatially throughout a figure. To\nfacilitate the training of machine learning systems, the corpus also includes\nside data that can be used to formulate auxiliary objectives. In particular, we\nprovide the numerical data used to generate each figure as well as bounding-box\nannotations for all plot elements. We study the proposed visual reasoning task\nby training several models, including the recently proposed Relation Network as\na strong baseline. Preliminary results indicate that the task poses a\nsignificant machine learning challenge. We envision FigureQA as a first step\ntowards developing models that can intuitively recognize patterns from visual\nrepresentations of data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:01:38 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:50:42 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Kahou", "Samira Ebrahimi", ""], ["Michalski", "Vincent", ""], ["Atkinson", "Adam", ""], ["Kadar", "Akos", ""], ["Trischler", "Adam", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1710.07307", "submitter": "Daniel Worrall", "authors": "Daniel E. Worrall, Stephan J. Garbin, Daniyar Turmukhambetov, Gabriel\n  J. Brostow", "title": "Interpretable Transformations with Encoder-Decoder Networks", "comments": "Accepted at ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feature spaces have the capacity to encode complex transformations of\ntheir input data. However, understanding the relative feature-space\nrelationship between two transformed encoded images is difficult. For instance,\nwhat is the relative feature space relationship between two rotated images?\nWhat is decoded when we interpolate in feature space? Ideally, we want to\ndisentangle confounding factors, such as pose, appearance, and illumination,\nfrom object identity. Disentangling these is difficult because they interact in\nvery nonlinear ways. We propose a simple method to construct a deep feature\nspace, with explicitly disentangled representations of several known\ntransformations. A person or algorithm can then manipulate the disentangled\nrepresentation, for example, to re-render an image with explicit control over\nparameterized degrees of freedom. The feature space is constructed using a\ntransforming encoder-decoder network with a custom feature transform layer,\nacting on the hidden representations. We demonstrate the advantages of explicit\ndisentangling on a variety of datasets and transformations, and as an aid for\ntraditional tasks, such as classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:28:15 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Worrall", "Daniel E.", ""], ["Garbin", "Stephan J.", ""], ["Turmukhambetov", "Daniyar", ""], ["Brostow", "Gabriel J.", ""]]}, {"id": "1710.07346", "submitter": "Zhu Shizhan", "authors": "Shizhan Zhu, Sanja Fidler, Raquel Urtasun, Dahua Lin, Chen Change Loy", "title": "Be Your Own Prada: Fashion Synthesis with Structural Coherence", "comments": "This is the updated version of our original paper appeared in ICCV\n  2017 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel and effective approach for generating new clothing on a\nwearer through generative adversarial learning. Given an input image of a\nperson and a sentence describing a different outfit, our model \"redresses\" the\nperson as desired, while at the same time keeping the wearer and her/his pose\nunchanged. Generating new outfits with precise regions conforming to a language\ndescription while retaining wearer's body structure is a new challenging task.\nExisting generative adversarial networks are not ideal in ensuring global\ncoherence of structure given both the input photograph and language description\nas conditions. We address this challenge by decomposing the complex generative\nprocess into two conditional stages. In the first stage, we generate a\nplausible semantic segmentation map that obeys the wearer's pose as a latent\nspatial arrangement. An effective spatial constraint is formulated to guide the\ngeneration of this semantic segmentation map. In the second stage, a generative\nmodel with a newly proposed compositional mapping layer is used to render the\nfinal image with precise regions and textures conditioned on this map. We\nextended the DeepFashion dataset [8] by collecting sentence descriptions for\n79K images. We demonstrate the effectiveness of our approach through both\nquantitative and qualitative evaluations. A user study is also conducted. The\ncodes and the data are available at http://mmlab.ie.cuhk.\nedu.hk/projects/FashionGAN/.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 20:46:26 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Zhu", "Shizhan", ""], ["Fidler", "Sanja", ""], ["Urtasun", "Raquel", ""], ["Lin", "Dahua", ""], ["Loy", "Chen Change", ""]]}, {"id": "1710.07354", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda and Narayan Srinivasa", "title": "Learning to Recognize Actions from Limited Training Examples Using a\n  Recurrent Spiking Neural Model", "comments": "13 figures (includes supplementary information)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in machine learning today is to build a model that\ncan learn from few examples. Here, we describe a reservoir based spiking neural\nmodel for learning to recognize actions with a limited number of labeled\nvideos. First, we propose a novel encoding, inspired by how microsaccades\ninfluence visual perception, to extract spike information from raw video data\nwhile preserving the temporal correlation across different frames. Using this\nencoding, we show that the reservoir generalizes its rich dynamical activity\ntoward signature action/movements enabling it to learn from few training\nexamples. We evaluate our approach on the UCF-101 dataset. Our experiments\ndemonstrate that our proposed reservoir achieves 81.3%/87% Top-1/Top-5\naccuracy, respectively, on the 101-class data while requiring just 8 video\nexamples per class for training. Our results establish a new benchmark for\naction recognition from limited video examples for spiking neural models while\nyielding competetive accuracy with respect to state-of-the-art non-spiking\nneural models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 21:07:02 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Srinivasa", "Narayan", ""]]}, {"id": "1710.07363", "submitter": "Michele Alberti", "authors": "Michele Alberti, Mathias Seuret, Vinaychandran Pondenkandath, Rolf\n  Ingold, Marcus Liwicki", "title": "Historical Document Image Segmentation with LDA-Initialized Deep Neural\n  Networks", "comments": "5 pages", "journal-ref": "ICDAR-HIP 2017", "doi": "10.1145/3151509.3151519", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to perform deep neural networks\nlayer-wise weight initialization using Linear Discriminant Analysis (LDA).\nTypically, the weights of a deep neural network are initialized with: random\nvalues, greedy layer-wise pre-training (usually as Deep Belief Network or as\nauto-encoder) or by re-using the layers from another network (transfer\nlearning). Hence, many training epochs are needed before meaningful weights are\nlearned, or a rather similar dataset is required for seeding a fine-tuning of\ntransfer learning. In this paper, we describe how to turn an LDA into either a\nneural layer or a classification layer. We analyze the initialization technique\non historical documents. First, we show that an LDA-based initialization is\nquick and leads to a very stable initialization. Furthermore, for the task of\nlayout analysis at pixel level, we investigate the effectiveness of LDA-based\ninitialization and show that it outperforms state-of-the-art random weight\ninitialization methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 22:43:47 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Alberti", "Michele", ""], ["Seuret", "Mathias", ""], ["Pondenkandath", "Vinaychandran", ""], ["Ingold", "Rolf", ""], ["Liwicki", "Marcus", ""]]}, {"id": "1710.07368", "submitter": "Bichen Wu", "authors": "Bichen Wu, Alvin Wan, Xiangyu Yue and Kurt Keutzer", "title": "SqueezeSeg: Convolutional Neural Nets with Recurrent CRF for Real-Time\n  Road-Object Segmentation from 3D LiDAR Point Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address semantic segmentation of road-objects from 3D LiDAR\npoint clouds. In particular, we wish to detect and categorize instances of\ninterest, such as cars, pedestrians and cyclists. We formulate this problem as\na point- wise classification problem, and propose an end-to-end pipeline called\nSqueezeSeg based on convolutional neural networks (CNN): the CNN takes a\ntransformed LiDAR point cloud as input and directly outputs a point-wise label\nmap, which is then refined by a conditional random field (CRF) implemented as a\nrecurrent layer. Instance-level labels are then obtained by conventional\nclustering algorithms. Our CNN model is trained on LiDAR point clouds from the\nKITTI dataset, and our point-wise segmentation labels are derived from 3D\nbounding boxes from KITTI. To obtain extra training data, we built a LiDAR\nsimulator into Grand Theft Auto V (GTA-V), a popular video game, to synthesize\nlarge amounts of realistic training data. Our experiments show that SqueezeSeg\nachieves high accuracy with astonishingly fast and stable runtime (8.7 ms per\nframe), highly desirable for autonomous driving applications. Furthermore,\nadditionally training on synthesized data boosts validation accuracy on\nreal-world data. Our source code and synthesized data will be open-sourced.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 23:03:33 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Wu", "Bichen", ""], ["Wan", "Alvin", ""], ["Yue", "Xiangyu", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1710.07390", "submitter": "Omid Haji Maghsoudi", "authors": "Omid Haji Maghsoudi", "title": "Superpixel Based Segmentation and Classification of Polyps in Wireless\n  Capsule Endoscopy", "comments": "This paper has been published in SPMB 2017", "journal-ref": null, "doi": "10.1109/SPMB.2017.8257027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Capsule Endoscopy (WCE) is a relatively new technology to record the\nentire GI trace, in vivo. The large amounts of frames captured during an\nexamination cause difficulties for physicians to review all these frames. The\nneed for reducing the reviewing time using some intelligent methods has been a\nchallenge. Polyps are considered as growing tissues on the surface of\nintestinal tract not inside of an organ. Most polyps are not cancerous, but if\none becomes larger than a centimeter, it can turn into cancer by great chance.\nThe WCE frames provide the early stage possibility for detection of polyps.\nHere, the application of simple linear iterative clustering (SLIC) superpixel\nfor segmentation of polyps in WCE frames is evaluated. Different SLIC\nsuperpixel numbers are examined to find the highest sensitivity for detection\nof polyps. The SLIC superpixel segmentation is promising to improve the results\nof previous studies. Finally, the superpixels were classified using a support\nvector machine (SVM) by extracting some texture and color features. The\nclassification results showed a sensitivity of 91%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 01:32:53 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 15:59:16 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Maghsoudi", "Omid Haji", ""]]}, {"id": "1710.07393", "submitter": "Muneki Yasuda", "authors": "Muneki Yasuda, Junpei Watanabe, Shun Kataoka, kazuyuki Tanaka", "title": "Linear-Time Algorithm in Bayesian Image Denoising based on Gaussian\n  Markov Random Field", "comments": null, "journal-ref": null, "doi": "10.1587/transinf.2017EDP7346", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider Bayesian image denoising based on a Gaussian\nMarkov random field (GMRF) model, for which we propose an new algorithm. Our\nmethod can solve Bayesian image denoising problems, including hyperparameter\nestimation, in $O(n)$-time, where $n$ is the number of pixels in a given image.\nFrom the perspective of the order of the computational time, this is a\nstate-of-the-art algorithm for the present problem setting. Moreover, the\nresults of our numerical experiments we show our method is in fact effective in\npractice.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 02:06:41 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 06:30:43 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Yasuda", "Muneki", ""], ["Watanabe", "Junpei", ""], ["Kataoka", "Shun", ""], ["Tanaka", "kazuyuki", ""]]}, {"id": "1710.07434", "submitter": "Oleksandr Bailo", "authors": "Oleksandr Bailo, Francois Rameau, In So Kweon", "title": "Light-weight place recognition and loop detection using road markings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an efficient algorithm for robust place recognition\nand loop detection using camera information only. Our pipeline purely relies on\nspatial localization and semantic information of road markings. The creation of\nthe database of road markings sequences is performed online, which makes the\nmethod applicable for real-time loop closure for visual SLAM techniques.\nFurthermore, our algorithm is robust to various weather conditions, occlusions\nfrom vehicles, and shadows. We have performed an extensive number of\nexperiments which highlight the effectiveness and scalability of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 07:15:17 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Bailo", "Oleksandr", ""], ["Rameau", "Francois", ""], ["Kweon", "In So", ""]]}, {"id": "1710.07455", "submitter": "Liu Kun", "authors": "Kun Liu, Wu Liu, Huadong Ma, Wenbing Huang, Xiongxiong Dong", "title": "Generalized Zero-Shot Learning for Action Recognition with Web-Scale\n  Video Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action recognition in surveillance video makes our life safer by detecting\nthe criminal events or predicting violent emergencies. However, efficient\naction recognition is not free of difficulty. First, there are so many action\nclasses in daily life that we cannot pre-define all possible action classes\nbeforehand. Moreover, it is very hard to collect real-word videos for certain\nparticular actions such as steal and street fight due to legal restrictions and\nprivacy protection. These challenges make existing data-driven recognition\nmethods insufficient to attain desired performance. Zero-shot learning is\npotential to be applied to solve these issues since it can perform\nclassification without positive example. Nevertheless, current zero-shot\nlearning algorithms have been studied under the unreasonable setting where seen\nclasses are absent during the testing phase. Motivated by this, we study the\ntask of action recognition in surveillance video under a more realistic\n\\emph{generalized zero-shot setting}, where testing data contains both seen and\nunseen classes. To our best knowledge, this is the first work to study video\naction recognition under the generalized zero-shot setting. We firstly perform\nextensive empirical studies on several existing zero-shot leaning approaches\nunder this new setting on a web-scale video data. Our experimental results\ndemonstrate that, under the generalize setting, typical zero-shot learning\nmethods are no longer effective for the dataset we applied. Then, we propose a\nmethod for action recognition by deploying generalized zero-shot learning,\nwhich transfers the knowledge of web video to detect the anomalous actions in\nsurveillance videos. To verify the effectiveness of our proposed method, we\nfurther construct a new surveillance video dataset consisting of nine action\nclasses related to the public safety situation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 08:49:11 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Liu", "Kun", ""], ["Liu", "Wu", ""], ["Ma", "Huadong", ""], ["Huang", "Wenbing", ""], ["Dong", "Xiongxiong", ""]]}, {"id": "1710.07457", "submitter": "Remi Flamary", "authors": "Nicolas Courty, R\\'emi Flamary, M\\'elanie Ducoffe", "title": "Learning Wasserstein Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wasserstein distance received a lot of attention recently in the\ncommunity of machine learning, especially for its principled way of comparing\ndistributions. It has found numerous applications in several hard problems,\nsuch as domain adaptation, dimensionality reduction or generative models.\nHowever, its use is still limited by a heavy computational cost. Our goal is to\nalleviate this problem by providing an approximation mechanism that allows to\nbreak its inherent complexity. It relies on the search of an embedding where\nthe Euclidean distance mimics the Wasserstein distance. We show that such an\nembedding can be found with a siamese architecture associated with a decoder\nnetwork that allows to move from the embedding space back to the original input\nspace. Once this embedding has been found, computing optimization problems in\nthe Wasserstein space (e.g. barycenters, principal directions or even\narchetypes) can be conducted extremely fast. Numerical experiments supporting\nthis idea are conducted on image datasets, and show the wide potential benefits\nof our method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 09:09:34 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Courty", "Nicolas", ""], ["Flamary", "R\u00e9mi", ""], ["Ducoffe", "M\u00e9lanie", ""]]}, {"id": "1710.07477", "submitter": "Cheng-Sheng Chan", "authors": "Tz-Ying Wu, Ting-An Chien, Cheng-Sheng Chan, Chan-Wei Hu, Min Sun", "title": "Anticipating Daily Intention using On-Wrist Motion Triggered Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anticipating human intention by observing one's actions has many\napplications. For instance, picking up a cellphone, then a charger (actions)\nimplies that one wants to charge the cellphone (intention). By anticipating the\nintention, an intelligent system can guide the user to the closest power\noutlet. We propose an on-wrist motion triggered sensing system for anticipating\ndaily intentions, where the on-wrist sensors help us to persistently observe\none's actions. The core of the system is a novel Recurrent Neural Network (RNN)\nand Policy Network (PN), where the RNN encodes visual and motion observation to\nanticipate intention, and the PN parsimoniously triggers the process of visual\nobservation to reduce computation requirement. We jointly trained the whole\nnetwork using policy gradient and cross-entropy loss. To evaluate, we collect\nthe first daily \"intention\" dataset consisting of 2379 videos with 34\nintentions and 164 unique action sequences. Our method achieves 92.68%, 90.85%,\n97.56% accuracy on three users while processing only 29% of the visual\nobservation on average.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 10:39:58 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Wu", "Tz-Ying", ""], ["Chien", "Ting-An", ""], ["Chan", "Cheng-Sheng", ""], ["Hu", "Chan-Wei", ""], ["Sun", "Min", ""]]}, {"id": "1710.07480", "submitter": "Gabriel Eilertsen", "authors": "Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafa{\\l} K. Mantiuk,\n  Jonas Unger", "title": "HDR image reconstruction from a single exposure using deep CNNs", "comments": "15 pages, 19 figures, Siggraph Asia 2017. Project webpage located at\n  http://hdrv.org/hdrcnn/ where paper with high quality images is available, as\n  well as supplementary material (document, images, video and source code)", "journal-ref": "ACM Trans. Graph. 36, 6, Article 178 (2017)", "doi": "10.1145/3130800.3130816", "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera sensors can only capture a limited range of luminance simultaneously,\nand in order to create high dynamic range (HDR) images a set of different\nexposures are typically combined. In this paper we address the problem of\npredicting information that have been lost in saturated image areas, in order\nto enable HDR reconstruction from a single exposure. We show that this problem\nis well-suited for deep learning algorithms, and propose a deep convolutional\nneural network (CNN) that is specifically designed taking into account the\nchallenges in predicting HDR values. To train the CNN we gather a large dataset\nof HDR images, which we augment by simulating sensor saturation for a range of\ncameras. To further boost robustness, we pre-train the CNN on a simulated HDR\ndataset created from a subset of the MIT Places database. We demonstrate that\nour approach can reconstruct high-resolution visually convincing HDR results in\na wide range of situations, and that it generalizes well to reconstruction of\nimages captured with arbitrary and low-end cameras that use unknown camera\nresponse functions and post-processing. Furthermore, we compare to existing\nmethods for HDR expansion, and show high quality results also for image based\nlighting. Finally, we evaluate the results in a subjective experiment performed\non an HDR display. This shows that the reconstructed HDR images are visually\nconvincing, with large improvements as compared to existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 10:48:22 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Eilertsen", "Gabriel", ""], ["Kronander", "Joel", ""], ["Denes", "Gyorgy", ""], ["Mantiuk", "Rafa\u0142 K.", ""], ["Unger", "Jonas", ""]]}, {"id": "1710.07498", "submitter": "Bernhard Stimpel", "authors": "Bernhard Stimpel, Christopher Syben, Tobias W\\\"urfl, Katrin Mentl,\n  Arnd D\\\"orfler, and Andreas Maier", "title": "MR to X-Ray Projection Image Synthesis", "comments": "In Proceedings of the 5th International Conference on Image Formation\n  in X-ray Computed Tomography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid imaging promises large potential in medical imaging applications. To\nfully utilize the possibilities of corresponding information from different\nmodalities, the information must be transferable between the domains. In\nradiation therapy planning, existing methods make use of reconstructed 3D\nmagnetic resonance imaging data to synthesize corresponding X-ray attenuation\nmaps. In contrast, for fluoroscopic procedures only line integral data, i.e.,\n2D projection images, are present. The question arises which approaches could\npotentially be used for this MR to X-ray projection image-to-image translation.\nWe examine three network architectures and two loss-functions regarding their\nsuitability as generator networks for this task. All generators proved to yield\nsuitable results for this task. A cascaded refinement network paired with a\nperceptual-loss function achieved the best qualitative results in our\nevaluation. The perceptual-loss showed to be able to preserve most of the\nhigh-frequency details in the projection images and, thus, is recommended for\nthe underlying task and similar problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 12:15:06 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 15:35:35 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["W\u00fcrfl", "Tobias", ""], ["Mentl", "Katrin", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1710.07557", "submitter": "Octavio Arriaga", "authors": "Octavio Arriaga, Matias Valdenegro-Toro, Paul Pl\\\"oger", "title": "Real-time Convolutional Neural Networks for Emotion and Gender\n  Classification", "comments": "Submitted to ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an implement a general convolutional neural network\n(CNN) building framework for designing real-time CNNs. We validate our models\nby creating a real-time vision system which accomplishes the tasks of face\ndetection, gender classification and emotion classification simultaneously in\none blended step using our proposed CNN architecture. After presenting the\ndetails of the training procedure setup we proceed to evaluate on standard\nbenchmark sets. We report accuracies of 96% in the IMDB gender dataset and 66%\nin the FER-2013 emotion dataset. Along with this we also introduced the very\nrecent real-time enabled guided back-propagation visualization technique.\nGuided back-propagation uncovers the dynamics of the weight changes and\nevaluates the learned features. We argue that the careful implementation of\nmodern CNN architectures, the use of the current regularization methods and the\nvisualization of previously hidden features are necessary in order to reduce\nthe gap between slow performances and real-time architectures. Our system has\nbeen validated by its deployment on a Care-O-bot 3 robot used during\nRoboCup@Home competitions. All our code, demos and pre-trained architectures\nhave been released under an open-source license in our public repository.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:53:57 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Arriaga", "Octavio", ""], ["Valdenegro-Toro", "Matias", ""], ["Pl\u00f6ger", "Paul", ""]]}, {"id": "1710.07558", "submitter": "Vivek Sharma", "authors": "Vivek Sharma, Ali Diba, Davy Neven, Michael S. Brown, Luc Van Gool,\n  Rainer Stiefelhagen", "title": "Classification Driven Dynamic Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks rely on image texture and structure to serve as\ndiscriminative features to classify the image content. Image enhancement\ntechniques can be used as preprocessing steps to help improve the overall image\nquality and in turn improve the overall effectiveness of a CNN. Existing image\nenhancement methods, however, are designed to improve the perceptual quality of\nan image for a human observer. In this paper, we are interested in learning\nCNNs that can emulate image enhancement and restoration, but with the overall\ngoal to improve image classification and not necessarily human perception. To\nthis end, we present a unified CNN architecture that uses a range of\nenhancement filters that can enhance image-specific details via end-to-end\ndynamic filter learning. We demonstrate the effectiveness of this strategy on\nfour challenging benchmark datasets for fine-grained, object, scene, and\ntexture classification: CUB-200-2011, PASCAL-VOC2007, MIT-Indoor, and DTD.\nExperiments using our proposed enhancement show promising results on all the\ndatasets. In addition, our approach is capable of improving the performance of\nall generic CNN architectures.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 14:54:29 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 19:17:42 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 19:11:33 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Sharma", "Vivek", ""], ["Diba", "Ali", ""], ["Neven", "Davy", ""], ["Brown", "Michael S.", ""], ["Van Gool", "Luc", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "1710.07563", "submitter": "Lyne Tchapmi Petse", "authors": "Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak,\n  Silvio Savarese", "title": "SEGCloud: Semantic Segmentation of 3D Point Clouds", "comments": "Accepted as a spotlight at the International Conference of 3D Vision\n  (3DV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D semantic scene labeling is fundamental to agents operating in the real\nworld. In particular, labeling raw 3D point sets from sensors provides\nfine-grained semantics. Recent works leverage the capabilities of Neural\nNetworks (NNs), but are limited to coarse voxel predictions and do not\nexplicitly enforce global consistency. We present SEGCloud, an end-to-end\nframework to obtain 3D point-level segmentation that combines the advantages of\nNNs, trilinear interpolation(TI) and fully connected Conditional Random Fields\n(FC-CRF). Coarse voxel predictions from a 3D Fully Convolutional NN are\ntransferred back to the raw 3D points via trilinear interpolation. Then the\nFC-CRF enforces global consistency and provides fine-grained semantics on the\npoints. We implement the latter as a differentiable Recurrent NN to allow joint\noptimization. We evaluate the framework on two indoor and two outdoor 3D\ndatasets (NYU V2, S3DIS, KITTI, Semantic3D.net), and show performance\ncomparable or superior to the state-of-the-art on all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 15:05:41 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Tchapmi", "Lyne P.", ""], ["Choy", "Christopher B.", ""], ["Armeni", "Iro", ""], ["Gwak", "JunYoung", ""], ["Savarese", "Silvio", ""]]}, {"id": "1710.07662", "submitter": "Maur\\'icio Pamplona Segundo", "authors": "Earnest E. Hansley, Mauricio Pamplona Segundo, Sudeep Sarkar", "title": "Employing Fusion of Learned and Handcrafted Features for Unconstrained\n  Ear Recognition", "comments": "23 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unconstrained ear recognition framework that outperforms\nstate-of-the-art systems in different publicly available image databases. To\nthis end, we developed CNN-based solutions for ear normalization and\ndescription, we used well-known handcrafted descriptors, and we fused learned\nand handcrafted features to improve recognition. We designed a two-stage\nlandmark detector that successfully worked under untrained scenarios. We used\nthe results generated to perform a geometric image normalization that boosted\nthe performance of all evaluated descriptors. Our CNN descriptor outperformed\nother CNN-based works in the literature, specially in more difficult scenarios.\nThe fusion of learned and handcrafted matchers appears to be complementary as\nit achieved the best performance in all experiments. The obtained results\noutperformed all other reported results for the UERC challenge, which contains\nthe most difficult database nowadays.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 18:43:21 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Hansley", "Earnest E.", ""], ["Segundo", "Mauricio Pamplona", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1710.07723", "submitter": "Tales Cesar de Oliveira Imbiriba", "authors": "Tales Imbiriba, Ricardo Augusto Borsoi, Jos\\'e Carlos Moreira Bermudez", "title": "Generalized linear mixing model accounting for endmember variability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endmember variability is an important factor for accurately unveiling vital\ninformation relating the pure materials and their distribution in hyperspectral\nimages. Recently, the extended linear mixing model (ELMM) has been proposed as\na modification of the linear mixing model (LMM) to consider endmember\nvariability effects resulting mainly from illumination changes. In this paper,\nwe further generalize the ELMM leading to a new model (GLMM) to account for\nmore complex spectral distortions where different wavelength intervals can be\naffected unevenly. We also extend the existing methodology to jointly estimate\nthe variability and the abundances for the GLMM. Simulations with real and\nsynthetic data show that the unmixing process can benefit from the extra\nflexibility introduced by the GLMM.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 22:46:12 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Imbiriba", "Tales", ""], ["Borsoi", "Ricardo Augusto", ""], ["Bermudez", "Jos\u00e9 Carlos Moreira", ""]]}, {"id": "1710.07735", "submitter": "Sima Behpour", "authors": "Sima Behpour, Kris M. Kitani, Brian D. Ziebart", "title": "ADA: A Game-Theoretic Perspective on Data Augmentation for Object\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of random perturbations of ground truth data, such as random\ntranslation or scaling of bounding boxes, is a common heuristic used for data\naugmentation that has been shown to prevent overfitting and improve\ngeneralization. Since the design of data augmentation is largely guided by\nreported best practices, it is difficult to understand if those design choices\nare optimal. To provide a more principled perspective, we develop a\ngame-theoretic interpretation of data augmentation in the context of object\ndetection. We aim to find an optimal adversarial perturbations of the ground\ntruth data (i.e., the worst case perturbations) that forces the object bounding\nbox predictor to learn from the hardest distribution of perturbed examples for\nbetter test-time performance. We establish that the game theoretic solution,\nthe Nash equilibrium, provides both an optimal predictor and optimal data\naugmentation distribution. We show that our adversarial method of training a\npredictor can significantly improve test time performance for the task of\nobject detection. On the ImageNet object detection task, our adversarial\napproach improves performance by over 16\\% compared to the best performing data\naugmentation method\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 00:51:49 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 15:20:22 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Behpour", "Sima", ""], ["Kitani", "Kris M.", ""], ["Ziebart", "Brian D.", ""]]}, {"id": "1710.07750", "submitter": "Qi Heng", "authors": "Heng Qi, Wu Liu, Liang Liu", "title": "An efficient deep learning hashing neural network for mobile visual\n  search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile visual search applications are emerging that enable users to sense\ntheir surroundings with smart phones. However, because of the particular\nchallenges of mobile visual search, achieving a high recognition bitrate has\nbecomes a consistent target of previous related works. In this paper, we\npropose a few-parameter, low-latency, and high-accuracy deep hashing approach\nfor constructing binary hash codes for mobile visual search. First, we exploit\nthe architecture of the MobileNet model, which significantly decreases the\nlatency of deep feature extraction by reducing the number of model parameters\nwhile maintaining accuracy. Second, we add a hash-like layer into MobileNet to\ntrain the model on labeled mobile visual data. Evaluations show that the\nproposed system can exceed state-of-the-art accuracy performance in terms of\nthe MAP. More importantly, the memory consumption is much less than that of\nother deep learning models. The proposed method requires only $13$ MB of memory\nfor the neural network and achieves a MAP of $97.80\\%$ on the mobile location\nrecognition dataset used for testing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 04:37:23 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Qi", "Heng", ""], ["Liu", "Wu", ""], ["Liu", "Liang", ""]]}, {"id": "1710.07782", "submitter": "Xintao Duan", "authors": "Xintao Duan, Haoxian Song, En Zhang and Jingjing Liu", "title": "Image Disguise based on Generative Model", "comments": "4 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To protect image contents, most existing encryption algorithms are designed\nto transform an original image into a texture-like or noise-like image, which\nis, however, an obvious visual sign indicating the presence of an encrypted\nimage, results in a significantly large number of attacks. To solve this\nproblem, in this paper, we propose a new image encryption method to generate a\nvisually same image as the original one by sending a meaning-normal and\nindependent image to a corresponding well-trained generative model to achieve\nthe effect of disguising the original image. This image disguise method not\nonly solves the problem of obvious visual implication, but also guarantees the\nsecurity of the information.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 10:26:44 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 00:45:38 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 14:43:33 GMT"}, {"version": "v4", "created": "Tue, 2 Jan 2018 13:44:55 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Duan", "Xintao", ""], ["Song", "Haoxian", ""], ["Zhang", "En", ""], ["Liu", "Jingjing", ""]]}, {"id": "1710.07830", "submitter": "Surat Teerapittayanon", "authors": "Bradley McDanel, Surat Teerapittayanon and H.T. Kung", "title": "Incomplete Dot Products for Dynamic Computation Scaling in Neural\n  Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of incomplete dot products (IDP) to dynamically adjust the\nnumber of input channels used in each layer of a convolutional neural network\nduring feedforward inference. IDP adds monotonically non-increasing\ncoefficients, referred to as a \"profile\", to the channels during training. The\nprofile orders the contribution of each channel in non-increasing order. At\ninference time, the number of channels used can be dynamically adjusted to\ntrade off accuracy for lowered power consumption and reduced latency by\nselecting only a beginning subset of channels. This approach allows for a\nsingle network to dynamically scale over a computation range, as opposed to\ntraining and deploying multiple networks to support different levels of\ncomputation scaling. Additionally, we extend the notion to multiple profiles,\neach optimized for some specific range of computation scaling. We present\nexperiments on the computation and accuracy trade-offs of IDP for popular image\nclassification models and datasets. We demonstrate that, for MNIST and\nCIFAR-10, IDP reduces computation significantly, e.g., by 75%, without\nsignificantly compromising accuracy. We argue that IDP provides a convenient\nand effective means for devices to lower computation costs dynamically to\nreflect the current computation budget of the system. For example, VGG-16 with\n50% IDP (using only the first 50% of channels) achieves 70% in accuracy on the\nCIFAR-10 dataset compared to the standard network which achieves only 35%\naccuracy when using the reduced channel set.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 17:37:11 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["McDanel", "Bradley", ""], ["Teerapittayanon", "Surat", ""], ["Kung", "H. T.", ""]]}, {"id": "1710.07831", "submitter": "Siqi Nie", "authors": "Siqi Nie, Ziheng Wang, Qiang Ji", "title": "A Generative Restricted Boltzmann Machine Based Method for\n  High-Dimensional Motion Data Modeling", "comments": null, "journal-ref": "Computer Vision and Image Understanding 136 (2015): 14-22", "doi": "10.1016/j.cviu.2014.12.005", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision applications involve modeling complex spatio-temporal\npatterns in high-dimensional motion data. Recently, restricted Boltzmann\nmachines (RBMs) have been widely used to capture and represent spatial patterns\nin a single image or temporal patterns in several time slices. To model global\ndynamics and local spatial interactions, we propose to theoretically extend the\nconventional RBMs by introducing another term in the energy function to\nexplicitly model the local spatial interactions in the input data. A learning\nmethod is then proposed to perform efficient learning for the proposed model.\nWe further introduce a new method for multi-class classification that can\neffectively estimate the infeasible partition functions of different RBMs such\nthat RBM is treated as a generative model for classification purpose. The\nimproved RBM model is evaluated on two computer vision applications: facial\nexpression recognition and human action recognition. Experimental results on\nbenchmark databases demonstrate the effectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 17:40:12 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Nie", "Siqi", ""], ["Wang", "Ziheng", ""], ["Ji", "Qiang", ""]]}, {"id": "1710.07849", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Yanli Wang, Gurong Wu", "title": "Heat Kernel Smoothing in Irregular Image Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the discrete version of heat kernel smoothing on graph data\nstructure. The method is used to smooth data in an irregularly shaped domains\nin 3D images.\n  New statistical properties are derived. As an application, we show how to\nfilter out data in the lung blood vessel trees obtained from computed\ntomography. The method can be further used in representing the complex vessel\ntrees parametrically and extracting the skeleton representation of the trees.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 19:53:36 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chung", "Moo K.", ""], ["Wang", "Yanli", ""], ["Wu", "Gurong", ""]]}, {"id": "1710.07859", "submitter": "Matthew Wicker", "authors": "Matthew Wicker, Xiaowei Huang, Marta Kwiatkowska", "title": "Feature-Guided Black-Box Safety Testing of Deep Neural Networks", "comments": "35 pages, 5 tables, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the improved accuracy of deep neural networks, the discovery of\nadversarial examples has raised serious safety concerns. Most existing\napproaches for crafting adversarial examples necessitate some knowledge\n(architecture, parameters, etc.) of the network at hand. In this paper, we\nfocus on image classifiers and propose a feature-guided black-box approach to\ntest the safety of deep neural networks that requires no such knowledge. Our\nalgorithm employs object detection techniques such as SIFT (Scale Invariant\nFeature Transform) to extract features from an image. These features are\nconverted into a mutable saliency distribution, where high probability is\nassigned to pixels that affect the composition of the image with respect to the\nhuman visual system. We formulate the crafting of adversarial examples as a\ntwo-player turn-based stochastic game, where the first player's objective is to\nminimise the distance to an adversarial example by manipulating the features,\nand the second player can be cooperative, adversarial, or random. We show that,\ntheoretically, the two-player game can con- verge to the optimal strategy, and\nthat the optimal strategy represents a globally minimal adversarial image. For\nLipschitz networks, we also identify conditions that provide safety guarantees\nthat no adversarial examples exist. Using Monte Carlo tree search we gradually\nexplore the game state space to search for adversarial examples. Our\nexperiments show that, despite the black-box setting, manipulations guided by a\nperception-based saliency distribution are competitive with state-of-the-art\nmethods that rely on white-box saliency matrices or sophisticated optimization\nprocedures. Finally, we show how our method can be used to evaluate robustness\nof neural networks in safety-critical applications such as traffic sign\nrecognition in self-driving cars.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 22:20:06 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 07:51:56 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Wicker", "Matthew", ""], ["Huang", "Xiaowei", ""], ["Kwiatkowska", "Marta", ""]]}, {"id": "1710.07965", "submitter": "Jianhui Chen Mr", "authors": "Lili Meng, Jianhui Chen, Frederick Tung, James J. Little, Julien\n  Valentin, Clarence W. de Silva", "title": "Backtracking Regression Forests for Accurate Camera Relocalization", "comments": "8 pages. Appear in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera relocalization plays a vital role in many robotics and computer vision\ntasks, such as global localization, recovery from tracking failure, and loop\nclosure detection. Recent random forests based methods directly predict 3D\nworld locations for 2D image locations to guide the camera pose optimization.\nDuring training, each tree greedily splits the samples to minimize the spatial\nvariance. However, these greedy splits often produce uneven sub-trees in\ntraining or incorrect 2D-3D correspondences in testing. To address these\nproblems, we propose a sample-balanced objective to encourage equal numbers of\nsamples in the left and right sub-trees, and a novel backtracking scheme to\nremedy the incorrect 2D-3D correspondence predictions. Furthermore, we extend\nthe regression forests based methods to use local features in both training and\ntesting stages for outdoor RGB-only applications. Experimental results on\npublicly available indoor and outdoor datasets demonstrate the efficacy of our\napproach, which shows superior or on-par accuracy with several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 15:43:26 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Meng", "Lili", ""], ["Chen", "Jianhui", ""], ["Tung", "Frederick", ""], ["Little", "James J.", ""], ["Valentin", "Julien", ""], ["de Silva", "Clarence W.", ""]]}, {"id": "1710.07991", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi", "title": "Rethinking Convolutional Semantic Segmentation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional semantic segmentation (DCSS) learning doesn't converge to\nan optimal local minimum with random parameters initializations; a pre-trained\nmodel on the same domain becomes necessary to achieve convergence.In this work,\nwe propose a joint cooperative end-to-end learning method for DCSS. It\naddresses many drawbacks with existing deep semantic segmentation learning; the\nproposed approach simultaneously learn both segmentation and classification;\ntaking away the essential need of the pre-trained model for learning\nconvergence. We present an improved inception based architecture with partial\nattention gating (PAG) over encoder information. The PAG also adds to achieve\nfaster convergence and better accuracy for segmentation task. We will show the\neffectiveness of this learning on a diabetic retinopathy classification and\nsegmentation dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 18:13:24 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Haloi", "Mrinal", ""]]}, {"id": "1710.08011", "submitter": "Humam Alwassel", "authors": "Bernard Ghanem, Juan Carlos Niebles, Cees Snoek, Fabian Caba Heilbron,\n  Humam Alwassel, Ranjay Khrisna, Victor Escorcia, Kenji Hata, Shyamal Buch", "title": "ActivityNet Challenge 2017 Summary", "comments": "76 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ActivityNet Large Scale Activity Recognition Challenge 2017 Summary:\nresults and challenge participants papers.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 20:48:49 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ghanem", "Bernard", ""], ["Niebles", "Juan Carlos", ""], ["Snoek", "Cees", ""], ["Heilbron", "Fabian Caba", ""], ["Alwassel", "Humam", ""], ["Khrisna", "Ranjay", ""], ["Escorcia", "Victor", ""], ["Hata", "Kenji", ""], ["Buch", "Shyamal", ""]]}, {"id": "1710.08014", "submitter": "Wenguan Wang", "authors": "Wenguan Wang and Jianbing Shen", "title": "Deep Cropping via Attention Box Prediction and Aesthetics Assessment", "comments": "Accepted by ICCV2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We model the photo cropping problem as a cascade of attention box regression\nand aesthetic quality classification, based on deep learning. A neural network\nis designed that has two branches for predicting attention bounding box and\nanalyzing aesthetics, respectively. The predicted attention box is treated as\nan initial crop window where a set of cropping candidates are generated around\nit, without missing important information. Then, aesthetics assessment is\nemployed to select the final crop as the one with the best aesthetic quality.\nWith our network, cropping candidates share features within full-image\nconvolutional feature maps, thus avoiding repeated feature computation and\nleading to higher computation efficiency. Via leveraging rich data for\nattention prediction and aesthetics assessment, the proposed method produces\nhigh-quality cropping results, even with the limited availability of training\ndata for photo cropping. The experimental results demonstrate the competitive\nresults and fast processing speed (5 fps with all steps).\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 21:03:01 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Wang", "Wenguan", ""], ["Shen", "Jianbing", ""]]}, {"id": "1710.08049", "submitter": "Tianlu Wang", "authors": "Tianlu Wang, Kota Yamaguchi, Vicente Ordonez", "title": "Feedback-prop: Convolutional Neural Network Inference under Partial\n  Evidence", "comments": "Accepted to CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an inference procedure for deep convolutional neural networks\n(CNNs) when partial evidence is available. Our method consists of a general\nfeedback-based propagation approach (feedback-prop) that boosts the prediction\naccuracy for an arbitrary set of unknown target labels when the values for a\nnon-overlapping arbitrary set of target labels are known. We show that existing\nmodels trained in a multi-label or multi-task setting can readily take\nadvantage of feedback-prop without any retraining or fine-tuning. Our\nfeedback-prop inference procedure is general, simple, reliable, and works on\ndifferent challenging visual recognition tasks. We present two variants of\nfeedback-prop based on layer-wise and residual iterative updates. We experiment\nusing several multi-task models and show that feedback-prop is effective in all\nof them. Our results unveil a previously unreported but interesting dynamic\nproperty of deep CNNs. We also present an associated technical approach that\ntakes advantage of this property for inference under partial evidence in\ngeneral visual recognition tasks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 00:29:49 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 19:12:46 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Wang", "Tianlu", ""], ["Yamaguchi", "Kota", ""], ["Ordonez", "Vicente", ""]]}, {"id": "1710.08092", "submitter": "Weidi Xie", "authors": "Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi and Andrew Zisserman", "title": "VGGFace2: A dataset for recognising faces across pose and age", "comments": "This paper has been accepted by IEEE Conference on Automatic Face and\n  Gesture Recognition (F&G), 2018. (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new large-scale face dataset named VGGFace2.\nThe dataset contains 3.31 million images of 9131 subjects, with an average of\n362.6 images for each subject. Images are downloaded from Google Image Search\nand have large variations in pose, age, illumination, ethnicity and profession\n(e.g. actors, athletes, politicians). The dataset was collected with three\ngoals in mind: (i) to have both a large number of identities and also a large\nnumber of images for each identity; (ii) to cover a large range of pose, age\nand ethnicity; and (iii) to minimize the label noise. We describe how the\ndataset was collected, in particular the automated and manual filtering stages\nto ensure a high accuracy for the images of each identity. To assess face\nrecognition performance using the new dataset, we train ResNet-50 (with and\nwithout Squeeze-and-Excitation blocks) Convolutional Neural Networks on\nVGGFace2, on MS- Celeb-1M, and on their union, and show that training on\nVGGFace2 leads to improved recognition performance over pose and age. Finally,\nusing the models trained on these datasets, we demonstrate state-of-the-art\nperformance on all the IARPA Janus face recognition benchmarks, e.g. IJB-A,\nIJB-B and IJB-C, exceeding the previous state-of-the-art by a large margin.\nDatasets and models are publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 05:26:32 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 10:35:21 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Cao", "Qiong", ""], ["Shen", "Li", ""], ["Xie", "Weidi", ""], ["Parkhi", "Omkar M.", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1710.08124", "submitter": "Charles-Alban Deledalle", "authors": "Shibin Parameswaran (UC San Diego), Charles-Alban Deledalle (IMB, UC\n  San Diego), Lo\\\"ic Denis (UJM, IOGS), Truong Q. Nguyen (UC San Diego)", "title": "Accelerating GMM-based patch priors for image restoration: Three\n  ingredients for a 100$\\times$ speed-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration methods aim to recover the underlying clean image from\ncorrupted observations. The Expected Patch Log-likelihood (EPLL) algorithm is a\npowerful image restoration method that uses a Gaussian mixture model (GMM)\nprior on the patches of natural images. Although it is very effective for\nrestoring images, its high runtime complexity makes EPLL ill-suited for most\npractical applications. In this paper, we propose three approximations to the\noriginal EPLL algorithm. The resulting algorithm, which we call the fast-EPLL\n(FEPLL), attains a dramatic speed-up of two orders of magnitude over EPLL while\nincurring a negligible drop in the restored image quality (less than 0.5 dB).\nWe demonstrate the efficacy and versatility of our algorithm on a number of\ninverse problems such as denoising, deblurring, super-resolution, inpainting\nand devignetting. To the best of our knowledge, FEPLL is the first algorithm\nthat can competitively restore a 512x512 pixel image in under 0.5s for all the\ndegradations mentioned above without specialized code optimizations such as CPU\nparallelization or GPU implementation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 07:39:35 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Parameswaran", "Shibin", "", "UC San Diego"], ["Deledalle", "Charles-Alban", "", "IMB, UC\n  San Diego"], ["Denis", "Lo\u00efc", "", "UJM, IOGS"], ["Nguyen", "Truong Q.", "", "UC San Diego"]]}, {"id": "1710.08135", "submitter": "Philippe Thomas", "authors": "Cyrine Selma (CRAN), Hind Haouzi (CRAN), Philippe Thomas (CRAN),\n  Jonathan Gaudreault, Michael Morin", "title": "An iterative closest point method for measuring the level of similarity\n  of 3d log scans in wood industry", "comments": "7th Workshop on Service Orientation in Holonic and Multi Agent\n  Manufacturing SOHOMA'17, Oct 2017, Nantes, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Canadian's lumber industry, simulators are used to predict the lumbers\nresulting from the sawing of a log at a given sawmill. Giving a log or several\nlogs' 3D scans as input, simulators perform a real-time job to predict the\nlumbers. These simulators, however, tend to be slow at processing large volume\nof wood. We thus explore an alternative approximation techniques based on the\nIterative Closest Point (ICP) algorithm to identify the already processed log\nto which an unseen log resembles the most. The main benefit of the ICP approach\nis that it can easily handle 3D scans with a variable number of points. We\ncompare this ICP-based nearest neighbor predictor, to predictors built using\nmachine learning algorithms such as the K-nearest-neighbor (kNN) and Random\nForest (RF). The implemented ICP-based predictor enabled us to identify key\npoints in using the 3D scans directly for distance calculation. The long-term\ngoal of this ongoing research is to integrated ICP distance calculations and\nmachine learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 08:12:45 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Selma", "Cyrine", "", "CRAN"], ["Haouzi", "Hind", "", "CRAN"], ["Thomas", "Philippe", "", "CRAN"], ["Gaudreault", "Jonathan", ""], ["Morin", "Michael", ""]]}, {"id": "1710.08149", "submitter": "Mo Zhang", "authors": "Mo Zhang, Xiang Li, Mengjia Xu, Quanzheng Li", "title": "Image Segmentation and Classification for Sickle Cell Disease using\n  Deformable U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.CB cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable cell segmentation and classification from biomedical images is a\ncrucial step for both scientific research and clinical practice. A major\nchallenge for more robust segmentation and classification methods is the large\nvariations in the size, shape and viewpoint of the cells, combining with the\nlow image quality caused by noise and artifacts. To address this issue, in this\nwork we propose a learning-based, simultaneous cell segmentation and\nclassification method based on the deep U-Net structure with deformable\nconvolution layers. The U-Net architecture for deep learning has been shown to\noffer a precise localization for image semantic segmentation. Moreover,\ndeformable convolution layer enables the free form deformation of the feature\nlearning process, thus makes the whole network more robust to various cell\nmorphologies and image settings. The proposed method is tested on microscopic\nred blood cell images from patients with sickle cell disease. The results show\nthat U-Net with deformable convolution achieves the highest accuracy for\nsegmentation and classification, comparing with original U-Net structure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 08:53:07 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 02:26:00 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 04:02:32 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhang", "Mo", ""], ["Li", "Xiang", ""], ["Xu", "Mengjia", ""], ["Li", "Quanzheng", ""]]}, {"id": "1710.08177", "submitter": "Saikat  Chatterjee", "authors": "Saikat Chatterjee, Alireza M. Javid, Mostafa Sadeghi, Partha P. Mitra,\n  Mikael Skoglund", "title": "Progressive Learning for Systematic Design of Large Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm for systematic design of a large artificial neural\nnetwork using a progression property. We find that some non-linear functions,\nsuch as the rectifier linear unit and its derivatives, hold the property. The\nsystematic design addresses the choice of network size and regularization of\nparameters. The number of nodes and layers in network increases in progression\nwith the objective of consistently reducing an appropriate cost. Each layer is\noptimized at a time, where appropriate parameters are learned using convex\noptimization. Regularization parameters for convex optimization do not need a\nsignificant manual effort for tuning. We also use random instances for some\nweight matrices, and that helps to reduce the number of parameters we learn.\nThe developed network is expected to show good generalization power due to\nappropriate regularization and use of random weights in the layers. This\nexpectation is verified by extensive experiments for classification and\nregression problems, using standard databases.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 10:06:15 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Chatterjee", "Saikat", ""], ["Javid", "Alireza M.", ""], ["Sadeghi", "Mostafa", ""], ["Mitra", "Partha P.", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1710.08192", "submitter": "Jonghwa Yim", "authors": "Jonghwa Yim, Kyung-Ah Sohn", "title": "Investigating the feature collection for semantic segmentation via\n  single skip connection", "comments": "(In pressing) Journal of KIISE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the study of deep convolutional neural network became prevalent, one of\nthe important discoveries is that a feature map from a convolutional network\ncan be extracted before going into the fully connected layer and can be used as\na saliency map for object detection. Furthermore, the model can use features\nfrom each different layer for accurate object detection: the features from\ndifferent layers can have different properties. As the model goes deeper, it\nhas many latent skip connections and feature maps to elaborate object\ndetection. Although there are many intermediate layers that we can use for\nsemantic segmentation through skip connection, still the characteristics of\neach skip connection and the best skip connection for this task are uncertain.\nTherefore, in this study, we exhaustively research skip connections of\nstate-of-the-art deep convolutional networks and investigate the\ncharacteristics of the features from each intermediate layer. In addition, this\nstudy would suggest how to use a recent deep neural network model for semantic\nsegmentation and it would therefore become a cornerstone for later studies with\nthe state-of-the-art network models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 10:40:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Yim", "Jonghwa", ""], ["Sohn", "Kyung-Ah", ""]]}, {"id": "1710.08247", "submitter": "Tilman Wekel", "authors": "Amir R. Zamir, Tilman Wekel, Pulkit Argrawal, Colin Weil, Jitendra\n  Malik, Silvio Savarese", "title": "Generic 3D Representation via Pose Estimation and Matching", "comments": "Published in ECCV16. See the project website\n  http://3drepresentation.stanford.edu/ and dataset website\n  https://github.com/amir32002/3D_Street_View", "journal-ref": "ECCV 2016 535-553", "doi": "10.1007/978-3-319-46487-9_33", "report-no": null, "categories": "cs.CV cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though a large body of computer vision research has investigated developing\ngeneric semantic representations, efforts towards developing a similar\nrepresentation for 3D has been limited. In this paper, we learn a generic 3D\nrepresentation through solving a set of foundational proxy 3D tasks:\nobject-centric camera pose estimation and wide baseline feature matching. Our\nmethod is based upon the premise that by providing supervision over a set of\ncarefully selected foundational tasks, generalization to novel tasks and\nabstraction capabilities can be achieved. We empirically show that the internal\nrepresentation of a multi-task ConvNet trained to solve the above core problems\ngeneralizes to novel 3D tasks (e.g., scene layout estimation, object pose\nestimation, surface normal estimation) without the need for fine-tuning and\nshows traits of abstraction abilities (e.g., cross-modality pose estimation).\nIn the context of the core supervised tasks, we demonstrate our representation\nachieves state-of-the-art wide baseline feature matching results without\nrequiring apriori rectification (unlike SIFT and the majority of learned\nfeatures). We also show 6DOF camera pose estimation given a pair local image\npatches. The accuracy of both supervised tasks come comparable to humans.\nFinally, we contribute a large-scale dataset composed of object-centric street\nview scenes along with point correspondences and camera pose information, and\nconclude with a discussion on the learned representation and open research\nquestions.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:01:05 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Zamir", "Amir R.", ""], ["Wekel", "Tilman", ""], ["Argrawal", "Pulkit", ""], ["Weil", "Colin", ""], ["Malik", "Jitendra", ""], ["Savarese", "Silvio", ""]]}, {"id": "1710.08299", "submitter": "Jiang Lu", "authors": "Jiang Lu, Jie Hu, Guannan Zhao, Fenghua Mei, Changshui Zhang", "title": "An In-field Automatic Wheat Disease Diagnosis System", "comments": "15 pages", "journal-ref": "Computers and Electronics in Agriculture, 142PA (2017): 369-379", "doi": "10.1016/j.compag.2017.09.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop diseases are responsible for the major production reduction and economic\nlosses in agricultural industry world- wide. Monitoring for health status of\ncrops is critical to control the spread of diseases and implement effective\nmanagement. This paper presents an in-field automatic wheat disease diagnosis\nsystem based on a weakly super- vised deep learning framework, i.e. deep\nmultiple instance learning, which achieves an integration of identification for\nwheat diseases and localization for disease areas with only image-level\nannotation for training images in wild conditions. Furthermore, a new in-field\nimage dataset for wheat disease, Wheat Disease Database 2017 (WDD2017), is\ncollected to verify the effectiveness of our system. Under two different\narchitectures, i.e. VGG-FCN-VD16 and VGG-FCN-S, our system achieves the mean\nrecognition accuracies of 97.95% and 95.12% respectively over 5-fold\ncross-validation on WDD2017, exceeding the results of 93.27% and 73.00% by two\nconventional CNN frameworks, i.e. VGG-CNN-VD16 and VGG-CNN-S. Experimental\nresults demonstrate that the proposed system outperforms conventional CNN\narchitectures on recognition accuracy under the same amount of parameters,\nmeanwhile main- taining accurate localization for corresponding disease areas.\nMoreover, the proposed system has been packed into a real-time mobile app to\nprovide support for agricultural disease diagnosis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 05:20:12 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Lu", "Jiang", ""], ["Hu", "Jie", ""], ["Zhao", "Guannan", ""], ["Mei", "Fenghua", ""], ["Zhang", "Changshui", ""]]}, {"id": "1710.08343", "submitter": "Tomoyoshi Shimobaba Dr.", "authors": "Tomoyoshi Shimobaba, Yutaka Endo, Takashi Nishitsuji, Takayuki\n  Takahashi, Yuki Nagahama, Satoki Hasegawa, Marie Sano, Ryuji Hirayama,\n  Takashi Kakue, Atsushi Shiraki, Tomoyoshi Ito", "title": "Computational ghost imaging using deep learning", "comments": null, "journal-ref": null, "doi": "10.1016/j.optcom.2017.12.041", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational ghost imaging (CGI) is a single-pixel imaging technique that\nexploits the correlation between known random patterns and the measured\nintensity of light transmitted (or reflected) by an object. Although CGI can\nobtain two- or three- dimensional images with a single or a few bucket\ndetectors, the quality of the reconstructed images is reduced by noise due to\nthe reconstruction of images from random patterns. In this study, we improve\nthe quality of CGI images using deep learning. A deep neural network is used to\nautomatically learn the features of noise-contaminated CGI images. After\ntraining, the network is able to predict low-noise images from new\nnoise-contaminated CGI images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 01:54:52 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Shimobaba", "Tomoyoshi", ""], ["Endo", "Yutaka", ""], ["Nishitsuji", "Takashi", ""], ["Takahashi", "Takayuki", ""], ["Nagahama", "Yuki", ""], ["Hasegawa", "Satoki", ""], ["Sano", "Marie", ""], ["Hirayama", "Ryuji", ""], ["Kakue", "Takashi", ""], ["Shiraki", "Atsushi", ""], ["Ito", "Tomoyoshi", ""]]}, {"id": "1710.08518", "submitter": "Wonmin Byeon", "authors": "Wonmin Byeon, Qin Wang, Rupesh Kumar Srivastava, Petros Koumoutsakos", "title": "ContextVP: Fully Context-Aware Video Prediction", "comments": "19 pages. ECCV 2018 oral presentation. Project webpage is at\n  https://wonmin-byeon.github.io/publication/2018-eccv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction models based on convolutional networks, recurrent networks,\nand their combinations often result in blurry predictions. We identify an\nimportant contributing factor for imprecise predictions that has not been\nstudied adequately in the literature: blind spots, i.e., lack of access to all\nrelevant past information for accurately predicting the future. To address this\nissue, we introduce a fully context-aware architecture that captures the entire\navailable past context for each pixel using Parallel Multi-Dimensional LSTM\nunits and aggregates it using blending units. Our model outperforms a strong\nbaseline network of 20 recurrent convolutional layers and yields\nstate-of-the-art performance for next step prediction on three challenging\nreal-world video datasets: Human 3.6M, Caltech Pedestrian, and UCF-101.\nMoreover, it does so with fewer parameters than several recently proposed\nmodels, and does not rely on deep convolutional networks, multi-scale\narchitectures, separation of background and foreground modeling, motion flow\nlearning, or adversarial training. These results highlight that full awareness\nof past context is of crucial importance for video prediction.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 21:55:12 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 01:18:16 GMT"}, {"version": "v3", "created": "Sun, 9 Sep 2018 09:55:04 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Byeon", "Wonmin", ""], ["Wang", "Qin", ""], ["Srivastava", "Rupesh Kumar", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1710.08543", "submitter": "Sungbin Lim", "authors": "Hyungjoo Cho, Sungbin Lim, Gunho Choi, Hyunseok Min", "title": "Neural Stain-Style Transfer Learning using GAN for Histopathological\n  Images", "comments": "10 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance of data-driven network for tumor classification varies with\nstain-style of histopathological images. This article proposes the stain-style\ntransfer (SST) model based on conditional generative adversarial networks\n(GANs) which is to learn not only the certain color distribution but also the\ncorresponding histopathological pattern. Our model considers feature-preserving\nloss in addition to well-known GAN loss. Consequently our model does not only\ntransfers initial stain-styles to the desired one but also prevent the\ndegradation of tumor classifier on transferred images. The model is examined\nusing the CAMELYON16 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 23:02:25 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 11:15:25 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Cho", "Hyungjoo", ""], ["Lim", "Sungbin", ""], ["Choi", "Gunho", ""], ["Min", "Hyunseok", ""]]}, {"id": "1710.08577", "submitter": "Chaitanya Mitash", "authors": "Chaitanya Mitash, Abdeslam Boularias and Kostas E. Bekris", "title": "Improving 6D Pose Estimation of Objects in Clutter via Physics-aware\n  Monte Carlo Tree Search", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a process for efficiently searching over combinations of\nindividual object 6D pose hypotheses in cluttered scenes, especially in cases\ninvolving occlusions and objects resting on each other. The initial set of\ncandidate object poses is generated from state-of-the-art object detection and\nglobal point cloud registration techniques. The best-scored pose per object by\nusing these techniques may not be accurate due to overlaps and occlusions.\nNevertheless, experimental indications provided in this work show that object\nposes with lower ranks may be closer to the real poses than ones with high\nranks according to registration techniques. This motivates a global\noptimization process for improving these poses by taking into account\nscene-level physical interactions between objects. It also implies that the\nCartesian product of candidate poses for interacting objects must be searched\nso as to identify the best scene-level hypothesis. To perform the search\nefficiently, the candidate poses for each object are clustered so as to reduce\ntheir number but still keep a sufficient diversity. Then, searching over the\ncombinations of candidate object poses is performed through a Monte Carlo Tree\nSearch (MCTS) process that uses the similarity between the observed depth image\nof the scene and a rendering of the scene given the hypothesized pose as a\nscore that guides the search procedure. MCTS handles in a principled way the\ntradeoff between fine-tuning the most promising poses and exploring new ones,\nby using the Upper Confidence Bound (UCB) technique. Experimental results\nindicate that this process is able to quickly identify in cluttered scenes\nphysically-consistent object poses that are significantly closer to ground\ntruth compared to poses found by point cloud registration methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 02:13:17 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Mitash", "Chaitanya", ""], ["Boularias", "Abdeslam", ""], ["Bekris", "Kostas E.", ""]]}, {"id": "1710.08585", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Ashwin A. Kannan, Gautam Arakalgud, Marios Savvides", "title": "Max-Margin Invariant Features from Transformed Unlabeled Data", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to a unitary group while having theoretical guarantees in addressing\nthe important practical issue of unavailability of transformed versions of\nlabelled data. A problem we call the Unlabeled Transformation Problem which is\na special form of semi-supervised learning and one-shot learning. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM based on\nwhich we propose Max-Margin Invariant Features (MMIF) to solve this problem. As\nan illustration, we design an framework for face recognition and demonstrate\nthe efficacy of our approach on a large scale semi-synthetic dataset with\n153,000 images and a new challenging protocol on Labelled Faces in the Wild\n(LFW) while out-performing strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 02:57:37 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Pal", "Dipan K.", ""], ["Kannan", "Ashwin A.", ""], ["Arakalgud", "Gautam", ""], ["Savvides", "Marios", ""]]}, {"id": "1710.08798", "submitter": "Saed Khawaldeh", "authors": "Yeman B. Hagos, Vu H. Minh, Saed Khawaldeh, Usama Pervaiz, Tajwar A.\n  Aleef", "title": "Fast PET Scan Tumor Segmentation using Superpixels, Principal Component\n  Analysis and K-means Clustering", "comments": "7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV physics.data-an q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positron Emission Tomography scan images are extensively used in radiotherapy\nplanning, clinical diagnosis, assessment of growth and treatment of a tumor.\nThese all rely on fidelity and speed of detection and delineation algorithm.\nDespite intensive research, segmentation remained a challenging problem due to\nthe diverse image content, resolution, shape, and noise. This paper presents a\nfast positron emission tomography tumor segmentation method in which\nsuperpixels are extracted first from the input image. Principal component\nanalysis is then applied on the superpixels and also on their average. Distance\nvector of each superpixel from the average is computed in principal components\ncoordinate system. Finally, k-means clustering is applied on distance vector to\nrecognize tumor and non-tumor superpixels. The proposed approach is implemented\nin MATLAB 2016 which resulted in an average Dice similarity of 84.2% on the\ndataset. Additionally, a very fast execution time was achieved as the number of\nsuperpixels and the size of distance vector on which clustering was done was\nvery small compared to the number of raw pixels in dataset images.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 00:19:57 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Hagos", "Yeman B.", ""], ["Minh", "Vu H.", ""], ["Khawaldeh", "Saed", ""], ["Pervaiz", "Usama", ""], ["Aleef", "Tajwar A.", ""]]}, {"id": "1710.08864", "submitter": "Jiawei Su", "authors": "Jiawei Su, Danilo Vasconcellos Vargas and Sakurai Kouichi", "title": "One pixel attack for fooling deep neural networks", "comments": null, "journal-ref": "IEEE Transactions on Evolutionary Computation}, Vol.23 , Issue.5 ,\n  pp. 828--841. Publisher: IEEE. 2019", "doi": "10.1109/TEVC.2019.2890858", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has revealed that the output of Deep Neural Networks (DNN)\ncan be easily altered by adding relatively small perturbations to the input\nvector. In this paper, we analyze an attack in an extremely limited scenario\nwhere only one pixel can be modified. For that we propose a novel method for\ngenerating one-pixel adversarial perturbations based on differential evolution\n(DE). It requires less adversarial information (a black-box attack) and can\nfool more types of networks due to the inherent features of DE. The results\nshow that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and\n16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least\none target class by modifying just one pixel with 74.03% and 22.91% confidence\non average. We also show the same vulnerability on the original CIFAR-10\ndataset. Thus, the proposed attack explores a different take on adversarial\nmachine learning in an extreme limited scenario, showing that current DNNs are\nalso vulnerable to such low dimension attacks. Besides, we also illustrate an\nimportant application of DE (or broadly speaking, evolutionary computation) in\nthe domain of adversarial machine learning: creating tools that can effectively\ngenerate low-cost adversarial attacks against neural networks for evaluating\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 16:02:19 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 07:58:35 GMT"}, {"version": "v3", "created": "Fri, 16 Feb 2018 08:53:44 GMT"}, {"version": "v4", "created": "Thu, 22 Feb 2018 09:18:34 GMT"}, {"version": "v5", "created": "Mon, 28 Jan 2019 04:39:30 GMT"}, {"version": "v6", "created": "Fri, 3 May 2019 08:32:24 GMT"}, {"version": "v7", "created": "Thu, 17 Oct 2019 07:46:53 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Su", "Jiawei", ""], ["Vargas", "Danilo Vasconcellos", ""], ["Kouichi", "Sakurai", ""]]}, {"id": "1710.08873", "submitter": "Andrew Wagenmaker", "authors": "Andrew J. Wagenmaker and Brian E. Moore and Raj Rao Nadakuditi", "title": "Robust Photometric Stereo via Dictionary Learning", "comments": "To appear in IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photometric stereo is a method that seeks to reconstruct the normal vectors\nof an object from a set of images of the object illuminated under different\nlight sources. While effective in some situations, classical photometric stereo\nrelies on a diffuse surface model that cannot handle objects with complex\nreflectance patterns, and it is sensitive to non-idealities in the images. In\nthis work, we propose a novel approach to photometric stereo that relies on\ndictionary learning to produce robust normal vector reconstructions.\nSpecifically, we develop two formulations for applying dictionary learning to\nphotometric stereo. We propose a model that applies dictionary learning to\nregularize and reconstruct the normal vectors from the images under the classic\nLambertian reflectance model. We then generalize this model to explicitly model\nnon-Lambertian objects. We investigate both approaches through extensive\nexperimentation on synthetic and real benchmark datasets and observe\nstate-of-the-art performance compared to existing robust photometric stereo\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 16:30:29 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 23:11:01 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 21:23:23 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Wagenmaker", "Andrew J.", ""], ["Moore", "Brian E.", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1710.09008", "submitter": "Mustafa Hajij", "authors": "Alejandro Robles, Mustafa Hajij and Paul Rosen", "title": "The Shape of an Image: A Study of Mapper on Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the topological construction called Mapper in the context of simply\nconnected domains, in particular on images. The Mapper construction can be\nconsidered as a generalization for contour, split, and joint trees on simply\nconnected domains. A contour tree on an image domain assumes the height\nfunction to be a piecewise linear Morse function. This is a rather restrictive\nclass of functions and does not allow us to explore the topology for most real\nworld images. The Mapper construction avoids this limitation by assuming only\ncontinuity on the height function allowing this construction to robustly deal\nwith a significant larger set of images. We provide a customized construction\nfor Mapper on images, give a fast algorithm to compute it, and show how to\nsimplify the Mapper structure in this case. Finally, we provide a simple\nprocedure that guarantees the equivalence of Mapper to contour, join, and split\ntrees on a simply connected domain.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:22:48 GMT"}, {"version": "v2", "created": "Thu, 7 Dec 2017 19:16:05 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Robles", "Alejandro", ""], ["Hajij", "Mustafa", ""], ["Rosen", "Paul", ""]]}, {"id": "1710.09077", "submitter": "Gunjan Sehgal", "authors": "Gunjan Sehgal, Bindu Gupta, Kaushal Paneri, Karamjit Singh, Geetika\n  Sharma, Gautam Shroff", "title": "Crop Planning using Stochastic Visual Optimization", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world population increases and arable land decreases, it becomes vital\nto improve the productivity of the agricultural land available. Given the\nweather and soil properties, farmers need to take critical decisions such as\nwhich seed variety to plant and in what proportion, in order to maximize\nproductivity. These decisions are irreversible and any unusual behavior of\nexternal factors, such as weather, can have catastrophic impact on the\nproductivity of crop. A variety which is highly desirable to a farmer might be\nunavailable or in short supply, therefore, it is very critical to evaluate\nwhich variety or varieties are more likely to be chosen by farmers from a\ngrowing region in order to meet demand. In this paper, we present our visual\nanalytics tool, ViSeed, showcased on the data given in Syngenta 2016 crop data\nchallenge 1 . This tool helps to predict optimal soybean seed variety or mix of\nvarieties in appropriate proportions which is more likely to be chosen by\nfarmers from a growing region. It also allows to analyse solutions generated\nfrom our approach and helps in the decision making process by providing\ninsightful visualizations\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 05:16:28 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Sehgal", "Gunjan", ""], ["Gupta", "Bindu", ""], ["Paneri", "Kaushal", ""], ["Singh", "Karamjit", ""], ["Sharma", "Geetika", ""], ["Shroff", "Gautam", ""]]}, {"id": "1710.09160", "submitter": "Huynh Van Luong", "authors": "Srivatsa Prativadibhayankaram, Huynh Van Luong, Thanh-Ha Le, Andre\n  Kaup", "title": "Compressive Online Robust Principal Component Analysis with Optical Flow\n  for Video Foreground-Background Separation", "comments": "preprint accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of online Robust Principle Component Analysis (RPCA) for the\nvideo foreground-background separation, we propose a compressive online RPCA\nwith optical flow that separates recursively a sequence of frames into sparse\n(foreground) and low-rank (background) components. Our method considers a small\nset of measurements taken per data vector (frame), which is different from\nconventional batch RPCA, processing all the data directly. The proposed method\nalso incorporates multiple prior information, namely previous foreground and\nbackground frames, to improve the separation and then updates the prior\ninformation for the next frame. Moreover, the foreground prior frames are\nimproved by estimating motions between the previous foreground frames using\noptical flow and compensating the motions to achieve higher quality foreground\nprior. The proposed method is applied to online video foreground and background\nseparation from compressive measurements. The visual and quantitative results\nshow that our method outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 10:39:47 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Prativadibhayankaram", "Srivatsa", ""], ["Van Luong", "Huynh", ""], ["Le", "Thanh-Ha", ""], ["Kaup", "Andre", ""]]}, {"id": "1710.09180", "submitter": "Muktabh Mayank Srivastava", "authors": "Srikrishna Varadarajan, Muktabh Mayank Srivastava, Monika Grewal,\n  Pulkit Kumar", "title": "Anatomical labeling of brain CT scan anomalies using multi-context\n  nearest neighbor relation networks", "comments": "Accepted as a one page abstract at IEEE International Symposium on\n  Biomedical Imaging (ISBI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work is an endeavor to develop a deep learning methodology for automated\nanatomical labeling of a given region of interest (ROI) in brain computed\ntomography (CT) scans. We combine both local and global context to obtain a\nrepresentation of the ROI. We then use Relation Networks (RNs) to predict the\ncorresponding anatomy of the ROI based on its relationship score for each\nclass. Further, we propose a novel strategy employing nearest neighbors\napproach for training RNs. We train RNs to learn the relationship of the target\nROI with the joint representation of its nearest neighbors in each class\ninstead of all data-points in each class. The proposed strategy leads to better\ntraining of RNs along with increased performance as compared to training\nbaseline RN network.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 11:42:36 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 14:48:26 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Varadarajan", "Srikrishna", ""], ["Srivastava", "Muktabh Mayank", ""], ["Grewal", "Monika", ""], ["Kumar", "Pulkit", ""]]}, {"id": "1710.09183", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Terry Ferrett, Matthew C. Valenti, Arun Ross", "title": "Biometrics-as-a-Service: A Framework to Promote Innovative Biometric\n  Recognition in the Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric recognition, or simply biometrics, is the use of biological\nattributes such as face, fingerprints or iris in order to recognize an\nindividual in an automated manner. A key application of biometrics is\nauthentication; i.e., using said biological attributes to provide access by\nverifying the claimed identity of an individual. This paper presents a\nframework for Biometrics-as-a-Service (BaaS) that performs biometric matching\noperations in the cloud, while relying on simple and ubiquitous consumer\ndevices such as smartphones. Further, the framework promotes innovation by\nproviding interfaces for a plurality of software developers to upload their\nmatching algorithms to the cloud. When a biometric authentication request is\nsubmitted, the system uses a criteria to automatically select an appropriate\nmatching algorithm. Every time a particular algorithm is selected, the\ncorresponding developer is rendered a micropayment. This creates an innovative\nand competitive ecosystem that benefits both software developers and the\nconsumers. As a case study, we have implemented the following: (a) an ocular\nrecognition system using a mobile web interface providing user access to a\nbiometric authentication service, and (b) a Linux-based virtual machine\nenvironment used by software developers for algorithm development and\nsubmission.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 11:46:52 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Talreja", "Veeru", ""], ["Ferrett", "Terry", ""], ["Valenti", "Matthew C.", ""], ["Ross", "Arun", ""]]}, {"id": "1710.09230", "submitter": "Marco Loog", "authors": "Marco Loog", "title": "Supervised Classification: Quite a Brief Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original problem of supervised classification considers the task of\nautomatically assigning objects to their respective classes on the basis of\nnumerical measurements derived from these objects. Classifiers are the tools\nthat implement the actual functional mapping from these measurements---also\ncalled features or inputs---to the so-called class label---or output. The\nfields of pattern recognition and machine learning study ways of constructing\nsuch classifiers. The main idea behind supervised methods is that of learning\nfrom examples: given a number of example input-output relations, to what extent\ncan the general mapping be learned that takes any new and unseen feature vector\nto its correct class? This chapter provides a basic introduction to the\nunderlying ideas of how to come to a supervised classification problem. In\naddition, it provides an overview of some specific classification techniques,\ndelves into the issues of object representation and classifier evaluation, and\n(very) briefly covers some variations on the basic supervised classification\ntask that may also be of interest to the practitioner.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 13:42:40 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Loog", "Marco", ""]]}, {"id": "1710.09282", "submitter": "Yu Cheng", "authors": "Yu Cheng, Duo Wang, Pan Zhou, Tao Zhang", "title": "A Survey of Model Compression and Acceleration for Deep Neural Networks", "comments": "Published in IEEE Signal Processing Magazine, updated version\n  including more recent works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have recently achieved great success in many\nvisual recognition tasks. However, existing deep neural network models are\ncomputationally expensive and memory intensive, hindering their deployment in\ndevices with low memory resources or in applications with strict latency\nrequirements. Therefore, a natural thought is to perform model compression and\nacceleration in deep networks without significantly decreasing the model\nperformance. During the past five years, tremendous progress has been made in\nthis area. In this paper, we review the recent techniques for compacting and\naccelerating DNN models. In general, these techniques are divided into four\ncategories: parameter pruning and quantization, low-rank factorization,\ntransferred/compact convolutional filters, and knowledge distillation. Methods\nof parameter pruning and quantization are described first, after that the other\ntechniques are introduced. For each category, we also provide insightful\nanalysis about the performance, related applications, advantages, and\ndrawbacks. Then we go through some very recent successful methods, for example,\ndynamic capacity networks and stochastic depths networks. After that, we survey\nthe evaluation matrices, the main datasets used for evaluating the model\nperformance, and recent benchmark efforts. Finally, we conclude this paper,\ndiscuss remaining the challenges and possible directions for future work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 20:16:55 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 01:22:14 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 00:12:34 GMT"}, {"version": "v4", "created": "Sat, 18 Nov 2017 07:54:57 GMT"}, {"version": "v5", "created": "Wed, 13 Dec 2017 21:10:49 GMT"}, {"version": "v6", "created": "Mon, 21 Jan 2019 23:34:25 GMT"}, {"version": "v7", "created": "Thu, 7 Feb 2019 05:07:15 GMT"}, {"version": "v8", "created": "Sun, 8 Sep 2019 16:30:38 GMT"}, {"version": "v9", "created": "Sun, 14 Jun 2020 19:10:03 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cheng", "Yu", ""], ["Wang", "Duo", ""], ["Zhou", "Pan", ""], ["Zhang", "Tao", ""]]}, {"id": "1710.09288", "submitter": "Wentao Zhu", "authors": "Wentao Zhu, Xiang Xiang, Trac D. Tran, Gregory D. Hager, Xiaohui Xie", "title": "Adversarial Deep Structured Nets for Mass Segmentation from Mammograms", "comments": "Accepted by ISBI2018. arXiv admin note: substantial text overlap with\n  arXiv:1612.05970", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Mass segmentation provides effective morphological features which are\nimportant for mass diagnosis. In this work, we propose a novel end-to-end\nnetwork for mammographic mass segmentation which employs a fully convolutional\nnetwork (FCN) to model a potential function, followed by a CRF to perform\nstructured learning. Because the mass distribution varies greatly with pixel\nposition, the FCN is combined with a position priori. Further, we employ\nadversarial training to eliminate over-fitting due to the small sizes of\nmammogram datasets. Multi-scale FCN is employed to improve the segmentation\nperformance. Experimental results on two public datasets, INbreast and\nDDSM-BCRP, demonstrate that our end-to-end network achieves better performance\nthan state-of-the-art approaches.\n\\footnote{https://github.com/wentaozhu/adversarial-deep-structural-networks.git}\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 06:54:43 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 07:50:09 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zhu", "Wentao", ""], ["Xiang", "Xiang", ""], ["Tran", "Trac D.", ""], ["Hager", "Gregory D.", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1710.09289", "submitter": "Wenjia Bai", "authors": "Wenjia Bai, Matthew Sinclair, Giacomo Tarroni, Ozan Oktay, Martin\n  Rajchl, Ghislain Vaillant, Aaron M. Lee, Nay Aung, Elena Lukaschuk, Mihir M.\n  Sanghvi, Filip Zemrak, Kenneth Fung, Jose Miguel Paiva, Valentina Carapella,\n  Young Jin Kim, Hideaki Suzuki, Bernhard Kainz, Paul M. Matthews, Steffen E.\n  Petersen, Stefan K. Piechnik, Stefan Neubauer, Ben Glocker, Daniel Rueckert", "title": "Automated cardiovascular magnetic resonance image analysis with fully\n  convolutional networks", "comments": "Accepted for publication by Journal of Cardiovascular Magnetic\n  Resonance", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular magnetic resonance (CMR) imaging is a standard imaging\nmodality for assessing cardiovascular diseases (CVDs), the leading cause of\ndeath globally. CMR enables accurate quantification of the cardiac chamber\nvolume, ejection fraction and myocardial mass, providing information for\ndiagnosis and monitoring of CVDs. However, for years, clinicians have been\nrelying on manual approaches for CMR image analysis, which is time consuming\nand prone to subjective errors. It is a major clinical challenge to\nautomatically derive quantitative and clinically relevant information from CMR\nimages. Deep neural networks have shown a great potential in image pattern\nrecognition and segmentation for a variety of tasks. Here we demonstrate an\nautomated analysis method for CMR images, which is based on a fully\nconvolutional network (FCN). The network is trained and evaluated on a\nlarge-scale dataset from the UK Biobank, consisting of 4,875 subjects with\n93,500 pixelwise annotated images. The performance of the method has been\nevaluated using a number of technical metrics, including the Dice metric, mean\ncontour distance and Hausdorff distance, as well as clinically relevant\nmeasures, including left ventricle (LV) end-diastolic volume (LVEDV) and\nend-systolic volume (LVESV), LV mass (LVM); right ventricle (RV) end-diastolic\nvolume (RVEDV) and end-systolic volume (RVESV). By combining FCN with a\nlarge-scale annotated dataset, the proposed automated method achieves a high\nperformance on par with human experts in segmenting the LV and RV on short-axis\nCMR images and the left atrium (LA) and right atrium (RA) on long-axis CMR\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 15:06:15 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 12:16:50 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 11:51:49 GMT"}, {"version": "v4", "created": "Tue, 22 May 2018 11:46:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bai", "Wenjia", ""], ["Sinclair", "Matthew", ""], ["Tarroni", "Giacomo", ""], ["Oktay", "Ozan", ""], ["Rajchl", "Martin", ""], ["Vaillant", "Ghislain", ""], ["Lee", "Aaron M.", ""], ["Aung", "Nay", ""], ["Lukaschuk", "Elena", ""], ["Sanghvi", "Mihir M.", ""], ["Zemrak", "Filip", ""], ["Fung", "Kenneth", ""], ["Paiva", "Jose Miguel", ""], ["Carapella", "Valentina", ""], ["Kim", "Young Jin", ""], ["Suzuki", "Hideaki", ""], ["Kainz", "Bernhard", ""], ["Matthews", "Paul M.", ""], ["Petersen", "Steffen E.", ""], ["Piechnik", "Stefan K.", ""], ["Neubauer", "Stefan", ""], ["Glocker", "Ben", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1710.09317", "submitter": "Tapabrata Chakraborti", "authors": "Tapabrata Chakraborti, Brendan McCane, Steven Mills, Umapada Pal", "title": "LOOP Descriptor: Local Optimal Oriented Pattern", "comments": null, "journal-ref": "IEEE Signal Processing Letters, 25(5): 635-639, 2018", "doi": "10.1109/LSP.2018.2817176", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter introduces the LOOP binary descriptor (local optimal oriented\npattern) that encodes rotation invariance into the main formulation itself.\nThis makes any post processing stage for rotation invariance redundant and\nimproves on both accuracy and time complexity. We consider fine-grained\nlepidoptera (moth/butterfly) species recognition as the representative problem\nsince it involves repetition of localized patterns and textures that may be\nexploited for discrimination. We evaluate the performance of LOOP against its\npredecessors as well as few other popular descriptors. Besides experiments on\nstandard benchmarks, we also introduce a new small image dataset on NZ\nLepidoptera. Loop performs as well or better on all datasets evaluated compared\nto previous binary descriptors. The new dataset and demo code of the proposed\nmethod are to be made available through the lead author's academic webpage and\nGitHub.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 16:06:13 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 19:11:56 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 21:30:09 GMT"}, {"version": "v4", "created": "Fri, 22 Mar 2019 13:22:09 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Chakraborti", "Tapabrata", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Pal", "Umapada", ""]]}, {"id": "1710.09338", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Seyed Raein Hashemi, Clemente\n  Velasco-Annis, Abdelhakim Ouaalam, Judy A. Estroff, Deniz Erdogmus, Simon K.\n  Warfield, Ali Gholipour", "title": "Real-Time Automatic Fetal Brain Extraction in Fetal MRI by Deep Learning", "comments": "This work has been submitted to ISBI 2018", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363675", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain segmentation is a fundamental first step in neuroimage analysis. In the\ncase of fetal MRI, it is particularly challenging and important due to the\narbitrary orientation of the fetus, organs that surround the fetal head, and\nintermittent fetal motion. Several promising methods have been proposed but are\nlimited in their performance in challenging cases and in real-time\nsegmentation. We aimed to develop a fully automatic segmentation method that\nindependently segments sections of the fetal brain in 2D fetal MRI slices in\nreal-time. To this end, we developed and evaluated a deep fully convolutional\nneural network based on 2D U-net and autocontext, and compared it to two\nalternative fast methods based on 1) a voxelwise fully convolutional network\nand 2) a method based on SIFT features, random forest and conditional random\nfield. We trained the networks with manual brain masks on 250 stacks of\ntraining images, and tested on 17 stacks of normal fetal brain images as well\nas 18 stacks of extremely challenging cases based on extreme motion, noise, and\nseverely abnormal brain shape. Experimental results show that our U-net\napproach outperformed the other methods and achieved average Dice metrics of\n96.52% and 78.83% in the normal and challenging test sets, respectively. With\nan unprecedented performance and a test run time of about 1 second, our network\ncan be used to segment the fetal brain in real-time while fetal MRI slices are\nbeing acquired. This can enable real-time motion tracking, motion detection,\nand 3D reconstruction of fetal brain MRI.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 16:54:44 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Hashemi", "Seyed Raein", ""], ["Velasco-Annis", "Clemente", ""], ["Ouaalam", "Abdelhakim", ""], ["Estroff", "Judy A.", ""], ["Erdogmus", "Deniz", ""], ["Warfield", "Simon K.", ""], ["Gholipour", "Ali", ""]]}, {"id": "1710.09441", "submitter": "Diman Zad Tootaghaj", "authors": "Diman Zad Tootaghaj, Adrian Sampson, Todd Mytkowicz, Kathryn S\n  McKinley", "title": "High Five: Improving Gesture Recognition by Embracing Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensors on mobile devices---accelerometers, gyroscopes, pressure meters, and\nGPS---invite new applications in gesture recognition, gaming, and fitness\ntracking. However, programming them remains challenging because human gestures\ncaptured by sensors are noisy. This paper illustrates that noisy gestures\ndegrade training and classification accuracy for gesture recognition in\nstate-of-the-art deterministic Hidden Markov Models (HMM). We introduce a new\nstatistical quantization approach that mitigates these problems by (1) during\ntraining, producing gesture-specific codebooks, HMMs, and error models for\ngesture sequences; and (2) during classification, exploiting the error model to\nexplore multiple feasible HMM state sequences. We implement classification in\nUncertain<t>, a probabilistic programming system that encapsulates HMMs and\nerror models and then automates sampling and inference in the runtime.\nUncertain<T> developers directly express a choice of application-specific\ntrade-off between recall and precision at gesture recognition time, rather than\nat training time. We demonstrate benefits in configurability, precision,\nrecall, and recognition on two data sets with 25 gestures from 28 people and\n4200 total gestures. Incorporating gesture error more accurately in modeling\nimproves the average recognition rate of 20 gestures from 34\\% in prior work to\n62\\%. Incorporating the error model during classification further improves the\naverage gesture recognition rate to 71\\%. As far as we are aware, no prior work\nshows how to generate an HMM error model during training and use it to improve\nclassification rates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 20:04:37 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Tootaghaj", "Diman Zad", ""], ["Sampson", "Adrian", ""], ["Mytkowicz", "Todd", ""], ["McKinley", "Kathryn S", ""]]}, {"id": "1710.09490", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Ruiqi Guo, Zhizhong Li, Derek Hoiem", "title": "Complete 3D Scene Parsing from an RGBD Image", "comments": "Accepted to International Journal of Computer Vision (IJCV), 2018\n  arXiv admin note: text overlap with arXiv:1504.02437", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major goal of vision is to infer physical models of objects, surfaces,\nand their layout from sensors. In this paper, we aim to interpret indoor scenes\nfrom one RGBD image. Our representation encodes the layout of orthogonal walls\nand the extent of objects, modeled with CAD-like 3D shapes. We parse both the\nvisible and occluded portions of the scene and all observable objects,\nproducing a complete 3D parse. Such a scene interpretation is useful for\nrobotics and visual reasoning, but difficult to produce due to the well-known\nchallenge of segmentation, the high degree of occlusion, and the diversity of\nobjects in indoor scenes. We take a data-driven approach, generating sets of\npotential object regions, matching to regions in training images, and\ntransferring and aligning associated 3D models while encouraging fit to\nobservations and spatial consistency. We use support inference to aid\ninterpretation and propose a retrieval scheme that uses convolutional neural\nnetworks (CNNs) to classify regions and retrieve objects with similar shapes.\nWe demonstrate the performance of our method on our newly annotated NYUd v2\ndataset with detailed 3D shapes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 23:04:14 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 18:05:14 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Zou", "Chuhang", ""], ["Guo", "Ruiqi", ""], ["Li", "Zhizhong", ""], ["Hoiem", "Derek", ""]]}, {"id": "1710.09505", "submitter": "Zhi Zhang", "authors": "Zhi Zhang, Guanghan Ning, Zhihai He", "title": "Knowledge Projection for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deeper and wider neural networks are actively pushing the performance\nlimits of various computer vision and machine learning tasks, they often\nrequire large sets of labeled data for effective training and suffer from\nextremely high computational complexity. In this paper, we will develop a new\nframework for training deep neural networks on datasets with limited labeled\nsamples using cross-network knowledge projection which is able to improve the\nnetwork performance while reducing the overall computational complexity\nsignificantly. Specifically, a large pre-trained teacher network is used to\nobserve samples from the training data. A projection matrix is learned to\nproject this teacher-level knowledge and its visual representations from an\nintermediate layer of the teacher network to an intermediate layer of a thinner\nand faster student network to guide and regulate its training process. Both the\nintermediate layers from the teacher network and the injection layers from the\nstudent network are adaptively selected during training by evaluating a joint\nloss function in an iterative manner. This knowledge projection framework\nallows us to use crucial knowledge learned by large networks to guide the\ntraining of thinner student networks, avoiding over-fitting, achieving better\nnetwork performance, and significantly reducing the complexity. Extensive\nexperimental results on benchmark datasets have demonstrated that our proposed\nknowledge projection approach outperforms existing methods, improving accuracy\nby up to 4% while reducing network complexity by 4 to 10 times, which is very\nattractive for practical applications of deep neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 01:30:00 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Zhang", "Zhi", ""], ["Ning", "Guanghan", ""], ["He", "Zhihai", ""]]}, {"id": "1710.09545", "submitter": "Joshua Levine", "authors": "Matthew Berger, Jixian Li, Joshua A. Levine", "title": "A Generative Model for Volume Rendering", "comments": null, "journal-ref": "IEEE Trans. Vis. Comput. Graph. 25(4) (2019) 1636-1650", "doi": "10.1109/TVCG.2018.2816059", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to synthesize and analyze volume-rendered images using\ngenerative models. We use the Generative Adversarial Network (GAN) framework to\ncompute a model from a large collection of volume renderings, conditioned on\n(1) viewpoint and (2) transfer functions for opacity and color. Our approach\nfacilitates tasks for volume analysis that are challenging to achieve using\nexisting rendering techniques such as ray casting or texture-based methods. We\nshow how to guide the user in transfer function editing by quantifying expected\nchange in the output image. Additionally, the generative model transforms\ntransfer functions into a view-invariant latent space specifically designed to\nsynthesize volume-rendered images. We use this space directly for rendering,\nenabling the user to explore the space of volume-rendered images. As our model\nis independent of the choice of volume rendering process, we show how to\nanalyze volume-rendered images produced by direct and global illumination\nlighting, for a variety of volume datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 05:20:05 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 22:50:24 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Berger", "Matthew", ""], ["Li", "Jixian", ""], ["Levine", "Joshua A.", ""]]}, {"id": "1710.09552", "submitter": "Kunal Narayan Chaudhury", "authors": "Sanjay Ghosh and Kunal N. Chaudhury", "title": "Artifact reduction for separable non-local means", "comments": "To appear in Journal of Electronic Imaging", "journal-ref": null, "doi": "10.1117/1.JEI.26.6.063012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently demonstrated [J. Electron. Imaging, 25(2), 2016] that one can\nperform fast non-local means (NLM) denoising of one-dimensional signals using a\nmethod called lifting. The cost of lifting is independent of the patch length,\nwhich dramatically reduces the run-time for large patches. Unfortunately, it is\ndifficult to directly extend lifting for non-local means denoising of images.\nTo bypass this, the authors proposed a separable approximation in which the\nimage rows and columns are filtered using lifting. The overall algorithm is\nsignificantly faster than NLM, and the results are comparable in terms of PSNR.\nHowever, the separable processing often produces vertical and horizontal\nstripes in the image. This problem was previously addressed by using a\nbilateral filter-based post-smoothing, which was effective in removing some of\nthe stripes. In this letter, we demonstrate that stripes can be mitigated in\nthe first place simply by involving the neighboring rows (or columns) in the\nfiltering. In other words, we use a two-dimensional search (similar to NLM),\nwhile still using one-dimensional patches (as in the previous proposal). The\nnovelty is in the observation that one can use lifting for performing\ntwo-dimensional searches. The proposed approach produces artifact-free images,\nwhose quality and PSNR are comparable to NLM, while being significantly faster.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 06:05:06 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Ghosh", "Sanjay", ""], ["Chaudhury", "Kunal N.", ""]]}, {"id": "1710.09671", "submitter": "Matthew Parno", "authors": "Brendan A. West, Taylor S. Hodgdon, Matthew D. Parno, Arnold J. Song", "title": "Improved Workflow for Unsupervised Multiphase Image Segmentation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cageo.2018.05.013", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative image analysis often depends on accurate classification of\npixels through a segmentation process. However, imaging artifacts such as the\npartial volume effect and sensor noise complicate the classification process.\nThese effects increase the pixel intensity variance of each constituent class,\ncausing intensities from one class to overlap with another. This increased\nvariance makes threshold based segmentation methods insufficient due to\nambiguous overlap regions in the pixel intensity distributions. The class\nambiguity becomes even more complex for systems with more than two\nconstituents, such as unsaturated moist granular media. In this paper, we\npropose an image processing workflow that improves segmentation accuracy for\nmultiphase systems. First, the ambiguous transition regions between classes are\nidentified and removed, which allows for global thresholding of single-class\nregions. Then the transition regions are classified using a distance function,\nand finally both segmentations are combined into one classified image. This\nworkflow includes three methodologies for identifying transition pixels and we\ndemonstrate on a variety of synthetic images that these approaches are able to\naccurately separate the ambiguous transition pixels from the single-class\nregions. For situations with typical amounts of image noise, misclassification\nerrors and area differences calculated between each class of the synthetic\nimages and the resultant segmented images range from 0.69-1.48% and 0.01-0.74%,\nrespectively, showing the segmentation accuracy of this approach. We\ndemonstrate that we are able to accurately segment x-ray microtomography images\nof moist granular media using these computationally efficient methodologies.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 12:58:09 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["West", "Brendan A.", ""], ["Hodgdon", "Taylor S.", ""], ["Parno", "Matthew D.", ""], ["Song", "Arnold J.", ""]]}, {"id": "1710.09685", "submitter": "Dipan Pal", "authors": "Pokkalla Harsha Vardhan, Kunal Sekhri, Dipan K. Pal and Marios\n  Savvides", "title": "Class Correlation affects Single Object Localization using Pre-trained\n  ConvNets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of object localization has become one of the mainstream problems\nof vision. Most of the algorithms proposed involve the design for the model to\nbe specifically for localizing objects. In this paper, we explore whether a\npre-trained canonical ConvNet (without fine-tuning) trained purely for object\nclassification on one dataset with global image level labels can be used to\nlocalize objects in images containing a single instance on a separate dataset\nwhile generalizing to novel classes. We propose a simple algorithm involving\ncropping and blackening out regions in the image space called Explicit Image\nSpace based Search (EISS) for locating the most responsive regions in an image\nin the context of object localization. EISS brings to light the interesting\nphenomenon of a ConvNets responding more to features within objects as opposed\nto object level descriptors, as the classes in the training data get more\ncorrelated (visually/semantically similar).\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 13:25:51 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 05:11:01 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Vardhan", "Pokkalla Harsha", ""], ["Sekhri", "Kunal", ""], ["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1710.09757", "submitter": "Haiyan Yao", "authors": "Haiyan Yao, Kang Han, Wanggen Wan, Li Hou", "title": "Deep Spatial Regression Model for Image Crowd Counting", "comments": "15pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision techniques have been used to produce accurate and generic\ncrowd count estimators in recent years. Due to severe occlusions, appearance\nvariations, perspective distortions and illumination conditions, crowd counting\nis a very challenging task. To this end, we propose a deep spatial regression\nmodel(DSRM) for counting the number of individuals present in a still image\nwith arbitrary perspective and arbitrary resolution. Our proposed model is\nbased on Convolutional Neural Network (CNN) and long short term memory (LSTM).\nFirst, we put the images into a pretrained CNN to extract a set of high-level\nfeatures. Then the features in adjacent regions are used to regress the local\ncounts with a LSTM structure which takes the spatial information into\nconsideration. The final global count is obtained by a sum of the local\npatches. We apply our framework on several challenging crowd counting datasets,\nand the experiment results illustrate that our method on the crowd counting and\ndensity estimation problem outperforms state-of-the-art methods in terms of\nreliability and effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 15:28:41 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Yao", "Haiyan", ""], ["Han", "Kang", ""], ["Wan", "Wanggen", ""], ["Hou", "Li", ""]]}, {"id": "1710.09762", "submitter": "Sarfaraz Hussein", "authors": "Maria J. M. Chuquicusma, Sarfaraz Hussein, Jeremy Burt, and Ulas Bagci", "title": "How to Fool Radiologists with Generative Adversarial Networks? A Visual\n  Turing Test for Lung Cancer Diagnosis", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating lung nodules as malignant or benign is still an underlying\nchallenge. To address this challenge, radiologists need computer aided\ndiagnosis (CAD) systems which can assist in learning discriminative imaging\nfeatures corresponding to malignant and benign nodules. However, learning\nhighly discriminative imaging features is an open problem. In this paper, our\naim is to learn the most discriminative features pertaining to lung nodules by\nusing an adversarial learning methodology. Specifically, we propose to use\nunsupervised learning with Deep Convolutional-Generative Adversarial Networks\n(DC-GANs) to generate lung nodule samples realistically. We hypothesize that\nimaging features of lung nodules will be discriminative if it is hard to\ndifferentiate them (fake) from real (true) nodules. To test this hypothesis, we\npresent Visual Turing tests to two radiologists in order to evaluate the\nquality of the generated (fake) nodules. Extensive comparisons are performed in\ndiscerning real, generated, benign, and malignant nodules. This experimental\nset up allows us to validate the overall quality of the generated nodules,\nwhich can then be used to (1) improve diagnostic decisions by mining highly\ndiscriminative imaging features, (2) train radiologists for educational\npurposes, and (3) generate realistic samples to train deep networks with big\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 15:38:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 04:31:54 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Chuquicusma", "Maria J. M.", ""], ["Hussein", "Sarfaraz", ""], ["Burt", "Jeremy", ""], ["Bagci", "Ulas", ""]]}, {"id": "1710.09779", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Pujan Kandel, Juan E. Corral, Candice W. Bolan,\n  Michael B. Wallace and Ulas Bagci", "title": "Deep Multi-Modal Classification of Intraductal Papillary Mucinous\n  Neoplasms (IPMN) with Canonical Correlation Analysis", "comments": "Accepted for publication in IEEE International Symposium on\n  Biomedical Imaging (ISBI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pancreatic cancer has the poorest prognosis among all cancer types.\nIntraductal Papillary Mucinous Neoplasms (IPMNs) are radiographically\nidentifiable precursors to pancreatic cancer; hence, early detection and\nprecise risk assessment of IPMN are vital. In this work, we propose a\nConvolutional Neural Network (CNN) based computer aided diagnosis (CAD) system\nto perform IPMN diagnosis and risk assessment by utilizing multi-modal MRI. In\nour proposed approach, we use minimum and maximum intensity projections to ease\nthe annotation variations among different slices and type of MRIs. Then, we\npresent a CNN to obtain deep feature representation corresponding to each MRI\nmodality (T1-weighted and T2-weighted). At the final step, we employ canonical\ncorrelation analysis (CCA) to perform a fusion operation at the feature level,\nleading to discriminative canonical correlation features. Extracted features\nare used for classification. Our results indicate significant improvements over\nother potential approaches to solve this important problem. The proposed\napproach doesn't require explicit sample balancing in cases of imbalance\nbetween positive and negative examples. To the best of our knowledge, our study\nis the first to automatically diagnose IPMN using multi-modal MRI.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:01:31 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 04:27:29 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2018 16:47:53 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Kandel", "Pujan", ""], ["Corral", "Juan E.", ""], ["Bolan", "Candice W.", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1710.09798", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Himani Arora, Liangliang Cao, Nima Mesgarani", "title": "Lip2AudSpec: Speech reconstruction from silent lip movements video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, we propose a deep neural network for reconstructing\nintelligible speech from silent lip movement videos. We use auditory\nspectrogram as spectral representation of speech and its corresponding sound\ngeneration method resulting in a more natural sounding reconstructed speech.\nOur proposed network consists of an autoencoder to extract bottleneck features\nfrom the auditory spectrogram which is then used as target to our main lip\nreading network comprising of CNN, LSTM and fully connected layers. Our\nexperiments show that the autoencoder is able to reconstruct the original\nauditory spectrogram with a 98% correlation and also improves the quality of\nreconstructed speech from the main lip reading network. Our model, trained\njointly on different speakers is able to extract individual speaker\ncharacteristics and gives promising results of reconstructing intelligible\nspeech with superior word recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 16:39:05 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Akbari", "Hassan", ""], ["Arora", "Himani", ""], ["Cao", "Liangliang", ""], ["Mesgarani", "Nima", ""]]}, {"id": "1710.09820", "submitter": "Germain Haessig", "authors": "Germain Haessig, Andrew Cassidy, Rodrigo Alvarez, Ryad Benosman,\n  Garrick Orchard", "title": "Spiking Optical Flow for Event-based Sensors Using IBM's TrueNorth\n  Neurosynaptic System", "comments": "11 pages, 11 figures without biography figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a fully spike-based neural network for optical flow\nestimation from Dynamic Vision Sensor data. A low power embedded implementation\nof the method which combines the Asynchronous Time-based Image Sensor with\nIBM's TrueNorth Neurosynaptic System is presented. The sensor generates spikes\nwith sub-millisecond resolution in response to scene illumination changes.\nThese spike are processed by a spiking neural network running on TrueNorth with\na 1 millisecond resolution to accurately determine the order and time\ndifference of spikes from neighboring pixels, and therefore infer the velocity.\nThe spiking neural network is a variant of the Barlow Levick method for optical\nflow estimation. The system is evaluated on two recordings for which ground\ntruth motion is available, and achieves an Average Endpoint Error of 11% at an\nestimated power budget of under 80mW for the sensor and computation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:33:34 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Haessig", "Germain", ""], ["Cassidy", "Andrew", ""], ["Alvarez", "Rodrigo", ""], ["Benosman", "Ryad", ""], ["Orchard", "Garrick", ""]]}, {"id": "1710.09829", "submitter": "Sara Sabour", "authors": "Sara Sabour, Nicholas Frosst, Geoffrey E Hinton", "title": "Dynamic Routing Between Capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A capsule is a group of neurons whose activity vector represents the\ninstantiation parameters of a specific type of entity such as an object or an\nobject part. We use the length of the activity vector to represent the\nprobability that the entity exists and its orientation to represent the\ninstantiation parameters. Active capsules at one level make predictions, via\ntransformation matrices, for the instantiation parameters of higher-level\ncapsules. When multiple predictions agree, a higher level capsule becomes\nactive. We show that a discrimininatively trained, multi-layer capsule system\nachieves state-of-the-art performance on MNIST and is considerably better than\na convolutional net at recognizing highly overlapping digits. To achieve these\nresults we use an iterative routing-by-agreement mechanism: A lower-level\ncapsule prefers to send its output to higher level capsules whose activity\nvectors have a big scalar product with the prediction coming from the\nlower-level capsule.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:49:04 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 19:26:38 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Sabour", "Sara", ""], ["Frosst", "Nicholas", ""], ["Hinton", "Geoffrey E", ""]]}, {"id": "1710.09859", "submitter": "Guilherme Fran\\c{c}a", "authors": "Guilherme Fran\\c{c}a, Maria L. Rizzo, Joshua T. Vogelstein", "title": "Kernel k-Groups via Hartigan's Method", "comments": "several improvements; connections with community detection and\n  stochastic block model. Matches published version", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2020", "doi": "10.1109/TPAMI.2020.2998120", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy statistics was proposed by Sz\\' ekely in the 80's inspired by Newton's\ngravitational potential in classical mechanics and it provides a model-free\nhypothesis test for equality of distributions. In its original form, energy\nstatistics was formulated in Euclidean spaces. More recently, it was\ngeneralized to metric spaces of negative type. In this paper, we consider a\nformulation for the clustering problem using a weighted version of energy\nstatistics in spaces of negative type. We show that this approach leads to a\nquadratically constrained quadratic program in the associated kernel space,\nestablishing connections with graph partitioning problems and kernel methods in\nmachine learning. To find local solutions of such an optimization problem, we\npropose kernel k-groups, which is an extension of Hartigan's method to kernel\nspaces. Kernel k-groups is cheaper than spectral clustering and has the same\ncomputational cost as kernel k-means (which is based on Lloyd's heuristic) but\nour numerical results show an improved performance, especially in higher\ndimensions. Moreover, we verify the efficiency of kernel k-groups in community\ndetection in sparse stochastic block models which has fascinating applications\nin several areas of science.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:38:28 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 14:02:55 GMT"}, {"version": "v3", "created": "Mon, 9 Dec 2019 15:29:58 GMT"}, {"version": "v4", "created": "Thu, 11 Jun 2020 19:57:09 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Fran\u00e7a", "Guilherme", ""], ["Rizzo", "Maria L.", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1710.09868", "submitter": "Luciano Oliveira", "authors": "Luiz Souza, Mauricio Pamplona, Luciano Oliveira and Jo\\~ao Papa", "title": "How far did we get in face spoofing detection?", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence, vol 72, pp.\n  368-381, 2018", "doi": "10.1016/j.engappai.2018.04.013", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The growing use of control access systems based on face recognition shed\nlight over the need for even more accurate systems to detect face spoofing\nattacks. In this paper, an extensive analysis on face spoofing detection works\npublished in the last decade is presented. The analyzed works are categorized\nby their fundamental parts, i.e., descriptors and classifiers. This structured\nsurvey also brings the temporal evolution of the face spoofing detection field,\nas well as a comparative analysis of the works considering the most important\npublic data sets in the field. The methodology followed in this work is\nparticularly relevant to observe trends in the existing approaches, to discuss\nstill opened issues, and to propose new perspectives for the future of face\nspoofing detection.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:49:01 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 18:55:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Souza", "Luiz", ""], ["Pamplona", "Mauricio", ""], ["Oliveira", "Luciano", ""], ["Papa", "Jo\u00e3o", ""]]}, {"id": "1710.09875", "submitter": "Jacob Carroll", "authors": "Jacob Carroll, Nils Carlson, and Garrett T. Kenyon", "title": "Phase Transitions in Image Denoising via Sparsely Coding Convolutional\n  Neural Networks", "comments": "4 pages, 3 figures, submitted to NIPS 2017 workshop: Advances in\n  Modeling and Learning Interactions from Complex Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cond-mat.stat-mech cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural networks are analogous in many ways to spin glasses, systems which are\nknown for their rich set of dynamics and equally complex phase diagrams. We\napply well-known techniques in the study of spin glasses to a convolutional\nsparsely encoding neural network and observe power law finite-size scaling\nbehavior in the sparsity and reconstruction error as the network denoises\n32$\\times$32 RGB CIFAR-10 images. This finite-size scaling indicates the\npresence of a continuous phase transition at a critical value of this sparsity.\nBy using the power law scaling relations inherent to finite-size scaling, we\ncan determine the optimal value of sparsity for any network size by tuning the\nsystem to the critical point and operate the system at the minimum denoising\nerror.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 19:09:54 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Carroll", "Jacob", ""], ["Carlson", "Nils", ""], ["Kenyon", "Garrett T.", ""]]}, {"id": "1710.09926", "submitter": "Yijing Watkins", "authors": "Yijing Watkins, Mohammad Sayeh, Oleksandr Iaroshenko and Garrett\n  Kenyon", "title": "Image Compression: Sparse Coding vs. Bottleneck Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottleneck autoencoders have been actively researched as a solution to image\ncompression tasks. However, we observed that bottleneck autoencoders produce\nsubjectively low quality reconstructed images. In this work, we explore the\nability of sparse coding to improve reconstructed image quality for the same\ndegree of compression. We observe that sparse image compression produces\nvisually superior reconstructed images and yields higher values of pixel-wise\nmeasures of reconstruction quality (PSNR and SSIM) compared to bottleneck\nautoencoders. % In addition, we find that using alternative metrics that\ncorrelate better with human perception, such as feature perceptual loss and the\nclassification accuracy, sparse image compression scores up to 18.06\\% and\n2.7\\% higher, respectively, compared to bottleneck autoencoders. Although\ncomputationally much more intensive, we find that sparse coding is otherwise\nsuperior to bottleneck autoencoders for the same degree of compression.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 21:49:30 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 20:38:26 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Watkins", "Yijing", ""], ["Sayeh", "Mohammad", ""], ["Iaroshenko", "Oleksandr", ""], ["Kenyon", "Garrett", ""]]}, {"id": "1710.09933", "submitter": "Thiago Vallin Spina PhD", "authors": "Thiago V. Spina, Johannes Stegmaier, Alexandre X. Falc\\~ao, Elliot\n  Meyerowitz, Alexandre Cunha", "title": "SEGMENT3D: A Web-based Application for Collaborative Segmentation of 3D\n  images used in the Shoot Apical Meristem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantitative analysis of 3D confocal microscopy images of the shoot\napical meristem helps understanding the growth process of some plants. Cell\nsegmentation in these images is crucial for computational plant analysis and\nmany automated methods have been proposed. However, variations in signal\nintensity across the image mitigate the effectiveness of those approaches with\nno easy way for user correction. We propose a web-based collaborative 3D image\nsegmentation application, SEGMENT3D, to leverage automatic segmentation\nresults. The image is divided into 3D tiles that can be either segmented\ninteractively from scratch or corrected from a pre-existing segmentation.\nIndividual segmentation results per tile are then automatically merged via\nconsensus analysis and then stitched to complete the segmentation for the\nentire image stack. SEGMENT3D is a comprehensive application that can be\napplied to other 3D imaging modalities and general objects. It also provides an\neasy way to create supervised data to advance segmentation using machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 22:40:28 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Spina", "Thiago V.", ""], ["Stegmaier", "Johannes", ""], ["Falc\u00e3o", "Alexandre X.", ""], ["Meyerowitz", "Elliot", ""], ["Cunha", "Alexandre", ""]]}, {"id": "1710.09934", "submitter": "William Severa", "authors": "William M. Severa, Jerilyn A. Timlin, Suraj Kholwadwala, Conrad D.\n  James, James B. Aimone", "title": "Data-driven Feature Sampling for Deep Hyperspectral Classification and\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high dimensionality of hyperspectral imaging forces unique challenges in\nscope, size and processing requirements. Motivated by the potential for an\nin-the-field cell sorting detector, we examine a $\\textit{Synechocystis sp.}$\nPCC 6803 dataset wherein cells are grown alternatively in nitrogen rich or\ndeplete cultures. We use deep learning techniques to both successfully classify\ncells and generate a mask segmenting the cells/condition from the background.\nFurther, we use the classification accuracy to guide a data-driven, iterative\nfeature selection method, allowing the design neural networks requiring 90%\nfewer input features with little accuracy degradation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 22:45:28 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Severa", "William M.", ""], ["Timlin", "Jerilyn A.", ""], ["Kholwadwala", "Suraj", ""], ["James", "Conrad D.", ""], ["Aimone", "James B.", ""]]}, {"id": "1710.09979", "submitter": "Xiao-Bo Jin", "authors": "Xiao-Bo Jin, Xu-Yao Zhang, Kaizhu Huang and Guang-Gang Geng", "title": "Stochastic Conjugate Gradient Algorithm with Variance Reduction", "comments": "10 pages, 4 figures, appeared in IEEE TRANSACTIONS ON NEURAL NETWORKS\n  AND LEARNING SYSTEMS, CGVR algorithm is available on github:\n  https://github.com/xbjin/cgvr", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems,2018", "doi": "10.1109/TNNLS.2018.2868835", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjugate gradient (CG) methods are a class of important methods for solving\nlinear equations and nonlinear optimization problems. In this paper, we propose\na new stochastic CG algorithm with variance reduction and we prove its linear\nconvergence with the Fletcher and Reeves method for strongly convex and smooth\nfunctions. We experimentally demonstrate that the CG with variance reduction\nalgorithm converges faster than its counterparts for four learning models,\nwhich may be convex, nonconvex or nonsmooth. In addition, its area under the\ncurve performance on six large-scale data sets is comparable to that of the\nLIBLINEAR solver for the L2-regularized L2-loss but with a significant\nimprovement in computational efficiency\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 03:47:41 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:33:01 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Zhang", "Xu-Yao", ""], ["Huang", "Kaizhu", ""], ["Geng", "Guang-Gang", ""]]}, {"id": "1710.10000", "submitter": "Umar Iqbal", "authors": "Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin,\n  Anton Milan, Juergen Gall and Bernt Schiele", "title": "PoseTrack: A Benchmark for Human Pose Estimation and Tracking", "comments": "www.posetrack.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human poses and motions are important cues for analysis of videos with people\nand there is strong evidence that representations based on body pose are highly\neffective for a variety of tasks such as activity recognition, content\nretrieval and social signal processing. In this work, we aim to further advance\nthe state of the art by establishing \"PoseTrack\", a new large-scale benchmark\nfor video-based human pose estimation and articulated tracking, and bringing\ntogether the community of researchers working on visual human analysis. The\nbenchmark encompasses three competition tracks focusing on i) single-frame\nmulti-person pose estimation, ii) multi-person pose estimation in videos, and\niii) multi-person articulated tracking. To facilitate the benchmark and\nchallenge we collect, annotate and release a new %large-scale benchmark dataset\nthat features videos with multiple people labeled with person tracks and\narticulated pose. A centralized evaluation server is provided to allow\nparticipants to evaluate on a held-out test set. We envision that the proposed\nbenchmark will stimulate productive research both by providing a large and\nrepresentative training dataset as well as providing a platform to objectively\nevaluate and compare the proposed methods. The benchmark is freely accessible\nat https://posetrack.net.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 06:20:30 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 18:20:56 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Andriluka", "Mykhaylo", ""], ["Iqbal", "Umar", ""], ["Insafutdinov", "Eldar", ""], ["Pishchulin", "Leonid", ""], ["Milan", "Anton", ""], ["Gall", "Juergen", ""], ["Schiele", "Bernt", ""]]}, {"id": "1710.10003", "submitter": "Huu Le", "authors": "Huu Le, Tat-Jun Chin, Anders Eriksson, Thanh-Toan Do and David Suter", "title": "Deterministic Approximate Methods for Maximum Consensus Robust Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum consensus estimation plays a critically important role in robust\nfitting problems in computer vision. Currently, the most prevalent algorithms\nfor consensus maximization draw from the class of randomized\nhypothesize-and-verify algorithms, which are cheap but can usually deliver only\nrough approximate solutions. On the other extreme, there are exact algorithms\nwhich are exhaustive search in nature and can be costly for practical-sized\ninputs. This paper fills the gap between the two extremes by proposing\ndeterministic algorithms to approximately optimize the maximum consensus\ncriterion. Our work begins by reformulating consensus maximization with linear\ncomplementarity constraints. Then, we develop two novel algorithms: one based\non non-smooth penalty method with a Frank-Wolfe style optimization scheme, the\nother based on the Alternating Direction Method of Multipliers (ADMM). Both\nalgorithms solve convex subproblems to efficiently perform the optimization. We\ndemonstrate the capability of our algorithms to greatly improve a rough initial\nestimate, such as those obtained using least squares or a randomized algorithm.\nCompared to the exact algorithms, our approach is much more practical on\nrealistic input sizes. Further, our approach is naturally applicable to\nestimation problems with geometric residuals\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 06:40:26 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 04:56:16 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Le", "Huu", ""], ["Chin", "Tat-Jun", ""], ["Eriksson", "Anders", ""], ["Do", "Thanh-Toan", ""], ["Suter", "David", ""]]}, {"id": "1710.10006", "submitter": "Jong Chul Ye", "authors": "Yeo Hun Yoon and Jong Chul Ye", "title": "Deep Learning for Accelerated Ultrasound Imaging", "comments": "Invited paper for ICASSP 2018 Special Session for \"Machine Learning\n  in Medical Imaging: from Measurement to Diagnosis\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In portable, 3-D, or ultra-fast ultrasound (US) imaging systems, there is an\nincreasing demand to reconstruct high quality images from limited number of\ndata. However, the existing solutions require either hardware changes or\ncomputationally expansive algorithms. To overcome these limitations, here we\npropose a novel deep learning approach that interpolates the missing RF data by\nutilizing the sparsity of the RF data in the Fourier domain. Extensive\nexperimental results from sub-sampled RF data from a real US system confirmed\nthat the proposed method can effectively reduce the data rate without\nsacrificing the image quality.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 06:49:37 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Yoon", "Yeo Hun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1710.10088", "submitter": "Rong Kang", "authors": "Rong Kang, Chen Wang, Peng Wang, Yuting Ding, Jianmin Wang", "title": "Fine-grained Pattern Matching Over Streaming Time Series", "comments": "14 pages, 14 figures, 29 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching of streaming time series with lower latency under limited\ncomputing resource comes to a critical problem, especially as the growth of\nIndustry 4.0 and Industry Internet of Things. However, against traditional\nsingle pattern matching problem, a pattern may contain multiple segments\nrepresenting different statistical properties or physical meanings for more\nprecise and expressive matching in real world. Hence, we formulate a new\nproblem, called \"fine-grained pattern matching\", which allows users to specify\nvaried granularities of matching deviation to different segments of a given\npattern, and fuzzy regions for adaptive breakpoints determination between\nconsecutive segments. In this paper, we propose a novel two-phase approach. In\nthe pruning phase, we introduce Equal-Length Block (ELB) representation\ntogether with Block-Skipping Pruning (BSP) policy, which guarantees low cost\nfeature calculation, effective pruning and no false dismissals. In the\npost-processing phase, a delta-function is proposed to enable us to conduct\nexact matching in linear complexity. Extensive experiments are conducted to\nevaluate on synthetic and real-world datasets, which illustrates that our\nalgorithm outperforms the brute-force method and MSM, a multi-step filter\nmechanism over the multi-scaled representation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 11:45:14 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 02:51:43 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 23:45:48 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Kang", "Rong", ""], ["Wang", "Chen", ""], ["Wang", "Peng", ""], ["Ding", "Yuting", ""], ["Wang", "Jianmin", ""]]}, {"id": "1710.10096", "submitter": "Ren\\'e Schuster", "authors": "Ren\\'e Schuster, Oliver Wasenm\\\"uller, Georg Kuschk, Christian Bailer,\n  Didier Stricker", "title": "SceneFlowFields: Dense Interpolation of Sparse Scene Flow\n  Correspondences", "comments": "IEEE Winter Conference on Applications of Computer Vision (WACV),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most scene flow methods use either variational optimization or a strong\nrigid motion assumption, we show for the first time that scene flow can also be\nestimated by dense interpolation of sparse matches. To this end, we find sparse\nmatches across two stereo image pairs that are detected without any prior\nregularization and perform dense interpolation preserving geometric and motion\nboundaries by using edge information. A few iterations of variational energy\nminimization are performed to refine our results, which are thoroughly\nevaluated on the KITTI benchmark and additionally compared to state-of-the-art\non MPI Sintel. For application in an automotive context, we further show that\nan optional ego-motion model helps to boost performance and blends smoothly\ninto our approach to produce a segmentation of the scene into static and\ndynamic parts.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:03:37 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""], ["Kuschk", "Georg", ""], ["Bailer", "Christian", ""], ["Stricker", "Didier", ""]]}, {"id": "1710.10101", "submitter": "Ping Li", "authors": "Ping Li, Tingyan Duan, Yongfeng Cao", "title": "Image matting with normalized weight and semi-supervised learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image matting is an important vision problem. The main stream methods for it\ncombine sampling-based methods and propagation-based methods. In this paper, we\ndeal with the combination with a normalized weighting parameter, which could\nwell control the relative relationship between information from sampling and\nfrom propagation. A reasonable value range for this parameter is given based on\nstatistics from the standard benchmark dataset. The matting is further improved\nby introducing semi-supervised learning iterations, which automatically refine\nthe trimap without user's interaction. This is especially beneficial when the\ntrimap is coarse. The experimental results on standard benchmark dataset have\nshown that both the normalized weighting parameter and the semi-supervised\nlearning iteration could significantly improve the matting performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:14:47 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Li", "Ping", ""], ["Duan", "Tingyan", ""], ["Cao", "Yongfeng", ""]]}, {"id": "1710.10121", "submitter": "Yiping Lu", "authors": "Yiping Lu, Aoxiao Zhong, Quanzheng Li, Bin Dong", "title": "Beyond Finite Layer Neural Networks: Bridging Deep Architectures and\n  Numerical Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our work, we bridge deep neural network design with numerical differential\nequations. We show that many effective networks, such as ResNet, PolyNet,\nFractalNet and RevNet, can be interpreted as different numerical\ndiscretizations of differential equations. This finding brings us a brand new\nperspective on the design of effective deep architectures. We can take\nadvantage of the rich knowledge in numerical analysis to guide us in designing\nnew and potentially more effective deep networks. As an example, we propose a\nlinear multi-step architecture (LM-architecture) which is inspired by the\nlinear multi-step method solving ordinary differential equations. The\nLM-architecture is an effective structure that can be used on any ResNet-like\nnetworks. In particular, we demonstrate that LM-ResNet and LM-ResNeXt (i.e. the\nnetworks obtained by applying the LM-architecture on ResNet and ResNeXt\nrespectively) can achieve noticeably higher accuracy than ResNet and ResNeXt on\nboth CIFAR and ImageNet with comparable numbers of trainable parameters. In\nparticular, on both CIFAR and ImageNet, LM-ResNet/LM-ResNeXt can significantly\ncompress ($>50$\\%) the original networks while maintaining a similar\nperformance. This can be explained mathematically using the concept of modified\nequation from numerical analysis. Last but not least, we also establish a\nconnection between stochastic control and noise injection in the training\nprocess which helps to improve generalization of the networks. Furthermore, by\nrelating stochastic training strategy with stochastic dynamic system, we can\neasily apply stochastic training to the networks with the LM-architecture. As\nan example, we introduced stochastic depth to LM-ResNet and achieve significant\nimprovement over the original LM-ResNet on CIFAR10.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 13:19:59 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 09:19:19 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 04:20:58 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Lu", "Yiping", ""], ["Zhong", "Aoxiao", ""], ["Li", "Quanzheng", ""], ["Dong", "Bin", ""]]}, {"id": "1710.10182", "submitter": "Lidan Wang", "authors": "Lidan Wang, Vishwanath A. Sindagi, Vishal M. Patel", "title": "High-Quality Facial Photo-Sketch Synthesis Using Multi-Adversarial\n  Networks", "comments": "Accepted by 2018 13th IEEE International Conference on Automatic Face\n  & Gesture Recognition (FG 2018)(Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing face sketches from real photos and its inverse have many\napplications. However, photo/sketch synthesis remains a challenging problem due\nto the fact that photo and sketch have different characteristics. In this work,\nwe consider this task as an image-to-image translation problem and explore the\nrecently popular generative models (GANs) to generate high-quality realistic\nphotos from sketches and sketches from photos. Recent GAN-based methods have\nshown promising results on image-to-image translation problems and\nphoto-to-sketch synthesis in particular, however, they are known to have\nlimited abilities in generating high-resolution realistic images. To this end,\nwe propose a novel synthesis framework called Photo-Sketch Synthesis using\nMulti-Adversarial Networks, (PS2-MAN) that iteratively generates low resolution\nto high resolution images in an adversarial way. The hidden layers of the\ngenerator are supervised to first generate lower resolution images followed by\nimplicit refinement in the network to generate higher resolution images.\nFurthermore, since photo-sketch synthesis is a coupled/paired translation\nproblem, we leverage the pair information using CycleGAN framework. Both Image\nQuality Assessment (IQA) and Photo-Sketch Matching experiments are conducted to\ndemonstrate the superior performance of our framework in comparison to existing\nstate-of-the-art solutions. Code available at:\nhttps://github.com/lidan1/PhotoSketchMAN.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 14:56:45 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 03:30:48 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Wang", "Lidan", ""], ["Sindagi", "Vishwanath A.", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1710.10188", "submitter": "Yanfeng Lu", "authors": "Yan-Feng Lu, Li-Hao Jia, Hong Qaio, Yi Li", "title": "Enhanced Biologically Inspired Model for Image Recognition Based on a\n  Novel Patch Selection Method with Moment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologically inspired model (BIM) for image recognition is a robust\ncomputational architecture, which has attracted widespread attention. BIM can\nbe described as a four-layer structure based on the mechanisms of the visual\ncortex. Although the performance of BIM for image recognition is robust, it\ntakes the randomly selected ways for the patch selection, which is sightless,\nand results in heavy computing burden. To address this issue, we propose a\nnovel patch selection method with oriented Gaussian-Hermite moment (PSGHM), and\nwe enhanced the BIM based on the proposed PSGHM, named as PBIM. In contrast to\nthe conventional BIM which adopts the random method to select patches within\nthe feature representation layers processed by multi-scale Gabor filter banks,\nthe proposed PBIM takes the PSGHM way to extract a small number of\nrepresentation features while offering promising distinctiveness. To show the\neffectiveness of the proposed PBIM, experimental studies on object\ncategorization are conducted on the CalTech05, TU Darmstadt (TUD), and GRAZ01\ndatabases. Experimental results demonstrate that the performance of PBIM is a\nsignificant improvement on that of the conventional BIM.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 15:04:45 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Lu", "Yan-Feng", ""], ["Jia", "Li-Hao", ""], ["Qaio", "Hong", ""], ["Li", "Yi", ""]]}, {"id": "1710.10192", "submitter": "Guanghan Ning", "authors": "Guanghan Ning, Zhihai He", "title": "Dual Path Networks for Multi-Person Human Pose Estimation", "comments": "ICCV 2017 Workshop on PoseTrack Challenge. Challenge results\n  available at:\n  https://posetrack.net/workshops/iccv2017/posetrack-challenge-results.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of multi-person human pose estimation in natural scenes is quite\nchallenging. Existing methods include both top-down and bottom-up approaches.\nThe main advantage of bottom-up methods is its excellent tradeoff between\nestimation accuracy and computational cost. We follow this path and aim to\ndesign smaller, faster, and more accurate neural networks for the regression of\nkeypoints and limb association vectors. These two regression tasks are\nnaturally dependent on each other. In this work, we propose a dual-path network\nspecially designed for multi-person human pose estimation, and compare our\nperformance with the openpose network in aspects of model size, forward speed,\nand estimation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 15:16:51 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Ning", "Guanghan", ""], ["He", "Zhihai", ""]]}, {"id": "1710.10198", "submitter": "Poorna Dasgupta", "authors": "Poorna Banerjee Dasgupta", "title": "Detection and Analysis of Human Emotions through Voice and Speech\n  Pattern Processing", "comments": "3 pages, Published with International Journal of Computer Trends and\n  Technology (IJCTT), Volume-52 Number-1, 2017", "journal-ref": "International Journal of Computer Trends and Technology (IJCTT)\n  V52(1):01-03, October 2017", "doi": "10.14445/22312803/IJCTT-V52P101", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to modulate vocal sounds and generate speech is one of the\nfeatures which set humans apart from other living beings. The human voice can\nbe characterized by several attributes such as pitch, timbre, loudness, and\nvocal tone. It has often been observed that humans express their emotions by\nvarying different vocal attributes during speech generation. Hence, deduction\nof human emotions through voice and speech analysis has a practical\nplausibility and could potentially be beneficial for improving human\nconversational and persuasion skills. This paper presents an algorithmic\napproach for detection and analysis of human emotions with the help of voice\nand speech processing. The proposed approach has been developed with the\nobjective of incorporation with futuristic artificial intelligence systems for\nimproving human-computer interactions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 15:30:30 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Dasgupta", "Poorna Banerjee", ""]]}, {"id": "1710.10304", "submitter": "Scott Reed", "authors": "Scott Reed, Yutian Chen, Thomas Paine, A\\\"aron van den Oord, S. M. Ali\n  Eslami, Danilo Rezende, Oriol Vinyals, Nando de Freitas", "title": "Few-shot Autoregressive Density Estimation: Towards Learning to Learn\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep autoregressive models have shown state-of-the-art performance in density\nestimation for natural images on large-scale datasets such as ImageNet.\nHowever, such models require many thousands of gradient-based weight updates\nand unique image examples for training. Ideally, the models would rapidly learn\nvisual concepts from only a handful of examples, similar to the manner in which\nhumans learns across many vision tasks. In this paper, we show how 1) neural\nattention and 2) meta learning techniques can be used in combination with\nautoregressive models to enable effective few-shot density estimation. Our\nproposed modifications to PixelCNN result in state-of-the art few-shot density\nestimation on the Omniglot dataset. Furthermore, we visualize the learned\nattention policy and find that it learns intuitive algorithms for simple tasks\nsuch as image mirroring on ImageNet and handwriting on Omniglot without\nsupervision. Finally, we extend the model to natural images and demonstrate\nfew-shot image generation on the Stanford Online Products dataset.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 18:58:51 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 19:42:35 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 04:30:30 GMT"}, {"version": "v4", "created": "Wed, 28 Feb 2018 18:00:49 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Reed", "Scott", ""], ["Chen", "Yutian", ""], ["Paine", "Thomas", ""], ["Oord", "A\u00e4ron van den", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo", ""], ["Vinyals", "Oriol", ""], ["de Freitas", "Nando", ""]]}, {"id": "1710.10330", "submitter": "Xiaowei Zhao", "authors": "Chen Chen, Xiaowei Zhao, Yang Liu", "title": "Multi-modal Aggregation for Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a solution to Large-Scale Video Classification\nChallenge (LSVC2017) [1] that ranked the 1st place. We focused on a variety of\nmodalities that cover visual, motion and audio. Also, we visualized the\naggregation process to better understand how each modality takes effect. Among\nthe extracted modalities, we found Temporal-Spatial features calculated by 3D\nconvolution quite promising that greatly improved the performance. We attained\nthe official metric mAP 0.8741 on the testing set with the ensemble model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 20:56:35 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Chen", "Chen", ""], ["Zhao", "Xiaowei", ""], ["Liu", "Yang", ""]]}, {"id": "1710.10348", "submitter": "Bo Chang", "authors": "Bo Chang, Lili Meng, Eldad Haber, Frederick Tung, David Begert", "title": "Multi-level Residual Networks from Dynamical Systems View", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep residual networks (ResNets) and their variants are widely used in many\ncomputer vision applications and natural language processing tasks. However,\nthe theoretical principles for designing and training ResNets are still not\nfully understood. Recently, several points of view have emerged to try to\ninterpret ResNet theoretically, such as unraveled view, unrolled iterative\nestimation and dynamical systems view. In this paper, we adopt the dynamical\nsystems point of view, and analyze the lesioning properties of ResNet both\ntheoretically and experimentally. Based on these analyses, we additionally\npropose a novel method for accelerating ResNet training. We apply the proposed\nmethod to train ResNets and Wide ResNets for three image classification\nbenchmarks, reducing training time by more than 40% with superior or on-par\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 22:06:58 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 23:25:10 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Chang", "Bo", ""], ["Meng", "Lili", ""], ["Haber", "Eldad", ""], ["Tung", "Frederick", ""], ["Begert", "David", ""]]}, {"id": "1710.10386", "submitter": "Changmao Cheng", "authors": "Changmao Cheng, Yanwei Fu, Yu-Gang Jiang, Wei Liu, Wenlian Lu,\n  Jianfeng Feng, Xiangyang Xue", "title": "Dual Skipping Networks", "comments": "CVPR 2018 (poster); fix typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent neuroscience studies on the left-right asymmetry of\nthe human brain in processing low and high spatial frequency information, this\npaper introduces a dual skipping network which carries out coarse-to-fine\nobject categorization. Such a network has two branches to simultaneously deal\nwith both coarse and fine-grained classification tasks. Specifically, we\npropose a layer-skipping mechanism that learns a gating network to predict\nwhich layers to skip in the testing stage. This layer-skipping mechanism endows\nthe network with good flexibility and capability in practice. Evaluations are\nconducted on several widely used coarse-to-fine object categorization\nbenchmarks, and promising results are achieved by our proposed network model.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 04:18:11 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 06:59:48 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 12:31:43 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Cheng", "Changmao", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Liu", "Wei", ""], ["Lu", "Wenlian", ""], ["Feng", "Jianfeng", ""], ["Xue", "Xiangyang", ""]]}, {"id": "1710.10393", "submitter": "Xu Sun", "authors": "Xu Sun, Bingzhen Wei, Xuancheng Ren, Shuming Ma", "title": "Label Embedding Network: Learning Label Representation for Soft Training\n  of Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method, called Label Embedding Network, which can learn label\nrepresentation (label embedding) during the training process of deep networks.\nWith the proposed method, the label embedding is adaptively and automatically\nlearned through back propagation. The original one-hot represented loss\nfunction is converted into a new loss function with soft distributions, such\nthat the originally unrelated labels have continuous interactions with each\nother during the training process. As a result, the trained model can achieve\nsubstantially higher accuracy and with faster convergence speed. Experimental\nresults based on competitive tasks demonstrate the effectiveness of the\nproposed method, and the learned label embedding is reasonable and\ninterpretable. The proposed method achieves comparable or even better results\nthan the state-of-the-art systems. The source code is available at\n\\url{https://github.com/lancopku/LabelEmb}.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 05:42:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Sun", "Xu", ""], ["Wei", "Bingzhen", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""]]}, {"id": "1710.10400", "submitter": "Chee Seng Chan", "authors": "Chee Kheng Chng, Chee Seng Chan", "title": "Total-Text: A Comprehensive Dataset for Scene Text Detection and\n  Recognition", "comments": "Accepted as Oral presentation in ICDAR2017 (Extended version, 13\n  pages 17 figures). We introduce a new scene text dataset namely as\n  Total-Text, which is more comprehensive than the existing scene text datasets\n  as it consists of 1555 natural images with more than 3 different text\n  orientations, one of a kind", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text in curve orientation, despite being one of the common text orientations\nin real world environment, has close to zero existence in well received scene\ntext datasets such as ICDAR2013 and MSRA-TD500. The main motivation of\nTotal-Text is to fill this gap and facilitate a new research direction for the\nscene text community. On top of the conventional horizontal and multi-oriented\ntexts, it features curved-oriented text. Total-Text is highly diversified in\norientations, more than half of its images have a combination of more than two\norientations. Recently, a new breed of solutions that casted text detection as\na segmentation problem has demonstrated their effectiveness against\nmulti-oriented text. In order to evaluate its robustness against curved text,\nwe fine-tuned DeconvNet and benchmark it on Total-Text. Total-Text with its\nannotation is available at https://github.com/cs-chan/Total-Text-Dataset\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 06:39:43 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Chng", "Chee Kheng", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1710.10460", "submitter": "Emmanuel Dauc\\'e", "authors": "Emmanuel Dauc\\'e", "title": "Toward predictive machine learning for active vision", "comments": "submitted to ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a comprehensive description of the active inference framework, as\nproposed by Friston (2010), under a machine-learning compliant perspective.\nStemming from a biological inspiration and the auto-encoding principles, the\nsketch of a cognitive architecture is proposed that should provide ways to\nimplement estimation-oriented control policies. Computer simulations illustrate\nthe effectiveness of the approach through a foveated inspection of the input\ndata. The pros and cons of the control policy are analyzed in detail, showing\ninteresting promises in terms of processing compression. Though optimizing\nfuture posterior entropy over the actions set is shown enough to attain locally\noptimal action selection, offline calculation using class-specific saliency\nmaps is shown better for it saves processing costs through saccades pathways\npre-processing, with a negligible effect on the recognition/compression rates.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 13:08:19 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 08:41:59 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 11:01:59 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Dauc\u00e9", "Emmanuel", ""]]}, {"id": "1710.10473", "submitter": "Moos Hueting", "authors": "Moos Hueting, Pradyumna Reddy, Vladimir Kim, Ersin Yumer, Nathan Carr,\n  Niloy Mitra", "title": "SeeThrough: Finding Chairs in Heavily Occluded Indoor Scene Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering 3D arrangements of objects from single indoor images is important\ngiven its many applications including interior design, content creation, etc.\nAlthough heavily researched in the recent years, existing approaches break down\nunder medium or heavy occlusion as the core object detection module starts\nfailing in absence of directly visible cues. Instead, we take into account\nholistic contextual 3D information, exploiting the fact that objects in indoor\nscenes co-occur mostly in typical near-regular configurations. First, we use a\nneural network trained on real indoor annotated images to extract 2D keypoints,\nand feed them to a 3D candidate object generation stage. Then, we solve a\nglobal selection problem among these 3D candidates using pairwise co-occurrence\nstatistics discovered from a large 3D scene database. We iterate the process\nallowing for candidates with low keypoint response to be incrementally detected\nbased on the location of the already discovered nearby objects. Focusing on\nchairs, we demonstrate significant performance improvement over combinations of\nstate-of-the-art methods, especially for scenes with moderately to severely\noccluded objects.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 14:30:39 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 15:23:45 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Hueting", "Moos", ""], ["Reddy", "Pradyumna", ""], ["Kim", "Vladimir", ""], ["Yumer", "Ersin", ""], ["Carr", "Nathan", ""], ["Mitra", "Niloy", ""]]}, {"id": "1710.10501", "submitter": "Li Yao", "authors": "Li Yao, Eric Poblenz, Dmitry Dagunts, Ben Covington, Devon Bernard,\n  Kevin Lyman", "title": "Learning to diagnose from scratch by exploiting dependencies among\n  labels", "comments": "include the link for the dataset split", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of medical diagnostics contains a wealth of challenges which\nclosely resemble classical machine learning problems; practical constraints,\nhowever, complicate the translation of these endpoints naively into classical\narchitectures. Many tasks in radiology, for example, are largely problems of\nmulti-label classification wherein medical images are interpreted to indicate\nmultiple present or suspected pathologies. Clinical settings drive the\nnecessity for high accuracy simultaneously across a multitude of pathological\noutcomes and greatly limit the utility of tools which consider only a subset.\nThis issue is exacerbated by a general scarcity of training data and maximizes\nthe need to extract clinically relevant features from available samples --\nideally without the use of pre-trained models which may carry forward\nundesirable biases from tangentially related tasks. We present and evaluate a\npartial solution to these constraints in using LSTMs to leverage\ninterdependencies among target labels in predicting 14 pathologic patterns from\nchest x-rays and establish state of the art results on the largest publicly\navailable chest x-ray dataset from the NIH without pre-training. Furthermore,\nwe propose and discuss alternative evaluation metrics and their relevance in\nclinical practice.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 17:25:23 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 22:16:56 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Yao", "Li", ""], ["Poblenz", "Eric", ""], ["Dagunts", "Dmitry", ""], ["Covington", "Ben", ""], ["Bernard", "Devon", ""], ["Lyman", "Kevin", ""]]}, {"id": "1710.10519", "submitter": "Lili Meng", "authors": "Lili Meng, Frederick Tung, James J. Little, Julien Valentin, Clarence\n  de Silva", "title": "Exploiting Points and Lines in Regression Forests for RGB-D Camera\n  Relocalization", "comments": "published as a conference paper at 2018 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera relocalization plays a vital role in many robotics and computer vision\ntasks, such as global localization, recovery from tracking failure and loop\nclosure detection. Recent random forests based methods exploit randomly sampled\npixel comparison features to predict 3D world locations for 2D image locations\nto guide the camera pose optimization. However, these image features are only\nsampled randomly in the images, without considering the spatial structures or\ngeometric information, leading to large errors or failure cases with the\nexistence of poorly textured areas or in motion blur. Line segment features are\nmore robust in these environments. In this work, we propose to jointly exploit\npoints and lines within the framework of uncertainty driven regression forests.\nThe proposed approach is thoroughly evaluated on three publicly available\ndatasets against several strong state-of-the-art baselines in terms of several\ndifferent error metrics. Experimental results prove the efficacy of our method,\nshowing superior or on-par state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 19:37:33 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 05:00:42 GMT"}, {"version": "v3", "created": "Sat, 28 Jul 2018 22:29:09 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Meng", "Lili", ""], ["Tung", "Frederick", ""], ["Little", "James J.", ""], ["Valentin", "Julien", ""], ["de Silva", "Clarence", ""]]}, {"id": "1710.10522", "submitter": "Yang Cheng", "authors": "Yang Cheng, Timeo Dubois", "title": "Object Recognition by Using Multi-level Feature Point Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach for object recognition in\nreal-time by employing multilevel feature analysis and demonstrate the\npracticality of adapting feature extraction into a Naive Bayesian\nclassification framework that enables simple, efficient, and robust\nperformance. We also show the proposed method scales well as the number of\nlevel-classes grows. To effectively understand the patches surrounding a\nkeypoint, the trained classifier uses hundreds of simple binary features and\nmodels class posterior probabilities. In addition, the classification process\nis computationally cheap under the assumed independence between arbitrary sets\nof features. Even though for some particular scenarios, this assumption can be\ninvalid. We demonstrate that the efficient classifier nevertheless performs\nremarkably well on image datasets with a large variation in the illumination\nenvironment and image capture perspectives. The experiment results show\nconsistent accuracy can be achieved on many challenging dataset while offer\ninteractive speed for large resolution images. The method demonstrates\npromising results that outperform the state-of-the-art methods on pattern\nrecognition.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 19:54:21 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Cheng", "Yang", ""], ["Dubois", "Timeo", ""]]}, {"id": "1710.10553", "submitter": "Muhan Ma", "authors": "Yichi Ma and Muhan Ma", "title": "A Novel Approach to Artistic Textual Visualization via GAN", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the visualization of statistical data tends to a mature technology, the\nvisualization of textual data is still in its infancy, especially for the\nartistic text. Due to the fact that visualization of artistic text is valuable\nand attractive in both art and information science, we attempt to realize this\ntentative idea in this article. We propose the Generative Adversarial Network\nbased Artistic Textual Visualization (GAN-ATV) which can create paintings after\nanalyzing the semantic content of existing poems. Our GAN-ATV consists of two\nmain sections: natural language analysis section and visual information\nsynthesis section. In natural language analysis section, we use Bag-of-Word\n(BoW) feature descriptors and a two-layer network to mine and analyze the\nhigh-level semantic information from poems. In visual information synthesis\nsection, we design a cross-modal semantic understanding module and integrate it\nwith Generative Adversarial Network (GAN) to create paintings, whose content\nare corresponding to the original poems. Moreover, in order to train our\nGAN-ATV and verify its performance, we establish a cross-modal artistic dataset\nnamed \"Cross-Art\". In the Cross-Art dataset, there are six topics and each\ntopic has their corresponding paintings and poems. The experimental results on\nCross-Art dataset are shown in this article.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 02:39:16 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ma", "Yichi", ""], ["Ma", "Muhan", ""]]}, {"id": "1710.10564", "submitter": "Trung Pham", "authors": "Toan Tran, Trung Pham, Gustavo Carneiro, Lyle Palmer and Ian Reid", "title": "A Bayesian Data Augmentation Approach for Learning Deep Models", "comments": "Accepted to NISP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is an essential part of the training process applied to\ndeep learning models. The motivation is that a robust training process for deep\nlearning models depends on large annotated datasets, which are expensive to be\nacquired, stored and processed. Therefore a reasonable alternative is to be\nable to automatically generate new annotated training samples using a process\nknown as data augmentation. The dominant data augmentation approach in the\nfield assumes that new training samples can be obtained via random geometric or\nappearance transformations applied to annotated training samples, but this is a\nstrong assumption because it is unclear if this is a reliable generative model\nfor producing new training samples. In this paper, we provide a novel Bayesian\nformulation to data augmentation, where new annotated training points are\ntreated as missing variables and generated based on the distribution learned\nfrom the training set. For learning, we introduce a theoretically sound\nalgorithm --- generalised Monte Carlo expectation maximisation, and demonstrate\none possible implementation via an extension of the Generative Adversarial\nNetwork (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the\nbetter performance of our proposed method compared to the current dominant data\naugmentation approach mentioned above --- the results also show that our\napproach produces better classification results than similar GAN models.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 05:02:14 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tran", "Toan", ""], ["Pham", "Trung", ""], ["Carneiro", "Gustavo", ""], ["Palmer", "Lyle", ""], ["Reid", "Ian", ""]]}, {"id": "1710.10565", "submitter": "Naman Kohli", "authors": "Naman Kohli, Daksha Yadav, Mayank Vatsa, Richa Singh, and Afzel Noore", "title": "Synthetic Iris Presentation Attack using iDCGAN", "comments": "International Joint Conference on Biometrics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability and accuracy of iris biometric modality has prompted its\nlarge-scale deployment for critical applications such as border control and\nnational ID projects. The extensive growth of iris recognition systems has\nraised apprehensions about susceptibility of these systems to various attacks.\nIn the past, researchers have examined the impact of various iris presentation\nattacks such as textured contact lenses and print attacks. In this research, we\npresent a novel presentation attack using deep learning based synthetic iris\ngeneration. Utilizing the generative capability of deep convolutional\ngenerative adversarial networks and iris quality metrics, we propose a new\nframework, named as iDCGAN (iris deep convolutional generative adversarial\nnetwork) for generating realistic appearing synthetic iris images. We\ndemonstrate the effect of these synthetically generated iris images as\npresentation attack on iris recognition by using a commercial system. The\nstate-of-the-art presentation attack detection framework, DESIST is utilized to\nanalyze if it can discriminate these synthetically generated iris images from\nreal images. The experimental results illustrate that mitigating the proposed\nsynthetic presentation attack is of paramount importance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 05:19:49 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kohli", "Naman", ""], ["Yadav", "Daksha", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""], ["Noore", "Afzel", ""]]}, {"id": "1710.10577", "submitter": "Quanshi Zhang", "authors": "Quanshi Zhang, Wenguan Wang, Song-Chun Zhu", "title": "Examining CNN Representations with respect to Dataset Bias", "comments": "in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pre-trained CNN without any testing samples, this paper proposes a\nsimple yet effective method to diagnose feature representations of the CNN. We\naim to discover representation flaws caused by potential dataset bias. More\nspecifically, when the CNN is trained to estimate image attributes, we mine\nlatent relationships between representations of different attributes inside the\nCNN. Then, we compare the mined attribute relationships with ground-truth\nattribute relationships to discover the CNN's blind spots and failure modes due\nto dataset bias. In fact, representation flaws caused by dataset bias cannot be\nexamined by conventional evaluation strategies based on testing images, because\ntesting images may also have a similar bias. Experiments have demonstrated the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 08:25:51 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 02:19:32 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Zhang", "Quanshi", ""], ["Wang", "Wenguan", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1710.10589", "submitter": "Aleksei Tiulpin", "authors": "Aleksei Tiulpin, J\\'er\\^ome Thevenot, Esa Rahtu, Petri Lehenkari and\n  Simo Saarakkala", "title": "Automatic Knee Osteoarthritis Diagnosis from Plain Radiographs: A Deep\n  Learning-Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee osteoarthritis (OA) is the most common musculoskeletal disorder. OA\ndiagnosis is currently conducted by assessing symptoms and evaluating plain\nradiographs, but this process suffers from subjectivity. In this study, we\npresent a new transparent computer-aided diagnosis method based on the Deep\nSiamese Convolutional Neural Network to automatically score knee OA severity\naccording to the Kellgren-Lawrence grading scale. We trained our method using\nthe data solely from the Multicenter Osteoarthritis Study and validated it on\nrandomly selected 3,000 subjects (5,960 knees) from Osteoarthritis Initiative\ndataset. Our method yielded a quadratic Kappa coefficient of 0.83 and average\nmulticlass accuracy of 66.71\\% compared to the annotations given by a committee\nof clinical experts. Here, we also report a radiological OA diagnosis area\nunder the ROC curve of 0.93. We also present attention maps -- given as a class\nprobability distribution -- highlighting the radiological features affecting\nthe network decision. This information makes the decision process transparent\nfor the practitioner, which builds better trust toward automatic methods. We\nbelieve that our model is useful for clinical decision making and for OA\nresearch; therefore, we openly release our training codes and the data set\ncreated in this study.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 10:11:14 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Tiulpin", "Aleksei", ""], ["Thevenot", "J\u00e9r\u00f4me", ""], ["Rahtu", "Esa", ""], ["Lehenkari", "Petri", ""], ["Saarakkala", "Simo", ""]]}, {"id": "1710.10648", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley and John Mwangi Wandeto", "title": "Using the quantization error from Self-Organized Map (SOM) output for\n  detecting critical variability in large bodies of image time series in less\n  than a minute", "comments": "12 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quantization error (QE) from SOM applied on time series of spatial\ncontrast images with variable relative amount of white and dark pixel contents,\nas in monochromatic medical images or satellite images, is proven a reliable\nindicator of potentially critical changes in image homogeneity. The QE is shown\nto increase linearly with the variability in spatial contrast contents across\ntime when contrast intensity is kept constant.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 17:05:12 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Dresp-Langley", "Birgitta", ""], ["Wandeto", "John Mwangi", ""]]}, {"id": "1710.10662", "submitter": "Matthias Zeppelzauer", "authors": "Matthias Zeppelzauer, Bartosz Zielinski, Mateusz Juda, Markus Seidl", "title": "A Study on Topological Descriptors for the Analysis of 3D Surface\n  Texture", "comments": "Preprint of Article \"A Study on Topological Descriptors for the\n  Analysis of 3D Surface Texture\" in Elsevier Journal on Computer Vision and\n  Image Understanding (CVIU): https://doi.org/10.1016/j.cviu.2017.10.012, 17\n  Pages, 19 Figures, 4 Tables", "journal-ref": null, "doi": "10.1016/j.cviu.2017.10.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods from computational topology are becoming more and more popular in\ncomputer vision and have shown to improve the state-of-the-art in several\ntasks. In this paper, we investigate the applicability of topological\ndescriptors in the context of 3D surface analysis for the classification of\ndifferent surface textures. We present a comprehensive study on topological\ndescriptors, investigate their robustness and expressiveness and compare them\nwith state-of-the-art methods including Convolutional Neural Networks (CNNs).\nResults show that class-specific information is reflected well in topological\ndescriptors. The investigated descriptors can directly compete with\nnon-topological descriptors and capture complementary information. As a\nconsequence they improve the state-of-the-art when combined with\nnon-topological descriptors.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 18:27:35 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zeppelzauer", "Matthias", ""], ["Zielinski", "Bartosz", ""], ["Juda", "Mateusz", ""], ["Seidl", "Markus", ""]]}, {"id": "1710.10675", "submitter": "Devinder Kumar", "authors": "Devinder Kumar, Graham W. Taylor and Alexander Wong", "title": "Discovery Radiomics with CLEAR-DR: Interpretable Computer Aided\n  Diagnosis of Diabetic Retinopathy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Radiomics-driven Computer Aided Diagnosis (CAD) has shown\nconsiderable promise in recent years as a potential tool for improving clinical\ndecision support in medical oncology, particularly those based around the\nconcept of Discovery Radiomics, where radiomic sequencers are discovered\nthrough the analysis of medical imaging data. One of the main limitations with\ncurrent CAD approaches is that it is very difficult to gain insight or\nrationale as to how decisions are made, thus limiting their utility to\nclinicians. Methods: In this study, we propose CLEAR-DR, a novel interpretable\nCAD system based on the notion of CLass-Enhanced Attentive Response Discovery\nRadiomics for the purpose of clinical decision support for diabetic\nretinopathy. Results: In addition to disease grading via the discovered deep\nradiomic sequencer, the CLEAR-DR system also produces a visual interpretation\nof the decision-making process to provide better insight and understanding into\nthe decision-making process of the system. Conclusion: We demonstrate the\neffectiveness and utility of the proposed CLEAR-DR system of enhancing the\ninterpretability of diagnostic grading results for the application of diabetic\nretinopathy grading. Significance: CLEAR-DR can act as a potential powerful\ntool to address the uninterpretability issue of current CAD systems, thus\nimproving their utility to clinicians.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 19:26:19 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kumar", "Devinder", ""], ["Taylor", "Graham W.", ""], ["Wong", "Alexander", ""]]}, {"id": "1710.10686", "submitter": "Vladimir Golkov", "authors": "Jan Kuka\\v{c}ka, Vladimir Golkov, Daniel Cremers", "title": "Regularization for Deep Learning: A Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is one of the crucial ingredients of deep learning, yet the\nterm regularization has various definitions, and regularization methods are\noften studied separately from each other. In our work we present a systematic,\nunifying taxonomy to categorize existing methods. We distinguish methods that\naffect data, network architectures, error terms, regularization terms, and\noptimization procedures. We do not provide all details about the listed\nmethods; instead, we present an overview of how the methods can be sorted into\nmeaningful categories and sub-categories. This helps revealing links and\nfundamental similarities between them. Finally, we include practical\nrecommendations both for users and for developers of new regularization\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 20:27:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kuka\u010dka", "Jan", ""], ["Golkov", "Vladimir", ""], ["Cremers", "Daniel", ""]]}, {"id": "1710.10687", "submitter": "Linguang Zhang", "authors": "Linguang Zhang, Adam Finkelstein, Szymon Rusinkiewicz", "title": "High-Precision Localization Using Ground Texture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-aware applications play an increasingly critical role in everyday\nlife. However, satellite-based localization (e.g., GPS) has limited accuracy\nand can be unusable in dense urban areas and indoors. We introduce an\nimage-based global localization system that is accurate to a few millimeters\nand performs reliable localization both indoors and outside. The key idea is to\ncapture and index distinctive local keypoints in ground textures. This is based\non the observation that ground textures including wood, carpet, tile, concrete,\nand asphalt may look random and homogeneous, but all contain cracks, scratches,\nor unique arrangements of fibers. These imperfections are persistent, and can\nserve as local features. Our system incorporates a downward-facing camera to\ncapture the fine texture of the ground, together with an image processing\npipeline that locates the captured texture patch in a compact database\nconstructed offline. We demonstrate the capability of our system to robustly,\naccurately, and quickly locate test images on various types of outdoor and\nindoor ground surfaces.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 20:39:22 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 15:57:45 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 22:48:48 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Zhang", "Linguang", ""], ["Finkelstein", "Adam", ""], ["Rusinkiewicz", "Szymon", ""]]}, {"id": "1710.10695", "submitter": "Dat Thanh Tran", "authors": "Dat Thanh Tran, Moncef Gabbouj and Alexandros Iosifidis", "title": "Multilinear Class-Specific Discriminant Analysis", "comments": "accepted in PRL", "journal-ref": "Pattern Recognition Letters, vol. 100, pp. 131-136, 2017", "doi": "10.1016/j.patrec.2017.10.027", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a great effort to transfer linear discriminant techniques that\noperate on vector data to high-order data, generally referred to as Multilinear\nDiscriminant Analysis (MDA) techniques. Many existing works focus on maximizing\nthe inter-class variances to intra-class variances defined on tensor data\nrepresentations. However, there has not been any attempt to employ\nclass-specific discrimination criteria for the tensor data. In this paper, we\npropose a multilinear subspace learning technique suitable for applications\nrequiring class-specific tensor models. The method maximizes the discrimination\nof each individual class in the feature space while retains the spatial\nstructure of the input. We evaluate the efficiency of the proposed method on\ntwo problems, i.e. facial image analysis and stock price prediction based on\nlimit order book data.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 21:17:09 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Tran", "Dat Thanh", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1710.10710", "submitter": "Stefan Hinterstoisser", "authors": "Stefan Hinterstoisser, Vincent Lepetit, Paul Wohlhart, Kurt Konolige", "title": "On Pre-Trained Image Features and Synthetic Images for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning methods usually require huge amounts of training data to\nperform at their full potential, and often require expensive manual labeling.\nUsing synthetic images is therefore very attractive to train object detectors,\nas the labeling comes for free, and several approaches have been proposed to\ncombine synthetic and real images for training.\n  In this paper, we show that a simple trick is sufficient to train very\neffectively modern object detectors with synthetic images only: We freeze the\nlayers responsible for feature extraction to generic layers pre-trained on real\nimages, and train only the remaining layers with plain OpenGL rendering. Our\nexperiments with very recent deep architectures for object recognition\n(Faster-RCNN, R-FCN, Mask-RCNN) and image feature extractors (InceptionResnet\nand Resnet) show this simple approach performs surprisingly well.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 22:48:58 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 21:46:24 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Hinterstoisser", "Stefan", ""], ["Lepetit", "Vincent", ""], ["Wohlhart", "Paul", ""], ["Konolige", "Kurt", ""]]}, {"id": "1710.10714", "submitter": "Yueru Chen", "authors": "Yueru Chen, Zhuwei Xu, Shanshan Cai, Yujian Lang and C.-C. Jay Kuo", "title": "A Saak Transform Approach to Efficient, Scalable and Robust Handwritten\n  Digits Recognition", "comments": "5 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient, scalable and robust approach to the handwritten digits\nrecognition problem based on the Saak transform is proposed in this work.\nFirst, multi-stage Saak transforms are used to extract a family of joint\nspatial-spectral representations of input images. Then, the Saak coefficients\nare used as features and fed into the SVM classifier for the classification\ntask. In order to control the size of Saak coefficients, we adopt a lossy Saak\ntransform that uses the principal component analysis (PCA) to select a smaller\nset of transform kernels. The handwritten digits recognition problem is well\nsolved by the convolutional neural network (CNN) such as the LeNet-5. We\nconduct a comparative study on the performance of the LeNet-5 and the\nSaak-transform-based solutions in terms of scalability and robustness as well\nas the efficiency of lossless and lossy Saak transforms under a comparable\naccuracy level.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 23:05:03 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Chen", "Yueru", ""], ["Xu", "Zhuwei", ""], ["Cai", "Shanshan", ""], ["Lang", "Yujian", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1710.10736", "submitter": "Saeed Ranjbar Alvar", "authors": "Saeed Ranjbar Alvar, Hyomin Choi, and Ivan V. Bajic", "title": "Can you find a face in a HEVC bitstream?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding faces in images is one of the most important tasks in computer\nvision, with applications in biometrics, surveillance, human-computer\ninteraction, and other areas. In our earlier work, we demonstrated that it is\npossible to tell whether or not an image contains a face by only examining the\nHEVC syntax, without fully reconstructing the image. In the present work we\nmove further in this direction by showing how to localize faces in HEVC-coded\nimages, without full reconstruction. We also demonstrate the benefits that such\napproach can have in privacy-friendly face localization.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 01:48:38 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 22:27:51 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Alvar", "Saeed Ranjbar", ""], ["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1710.10741", "submitter": "Yanan Sun", "authors": "Yanan Sun, Bing Xue, Mengjie Zhang and Gary G. Yen", "title": "Evolving Deep Convolutional Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary computation methods have been successfully applied to neural\nnetworks since two decades ago, while those methods cannot scale well to the\nmodern deep neural networks due to the complicated architectures and large\nquantities of connection weights. In this paper, we propose a new method using\ngenetic algorithms for evolving the architectures and connection weight\ninitialization values of a deep convolutional neural network to address image\nclassification problems. In the proposed algorithm, an efficient\nvariable-length gene encoding strategy is designed to represent the different\nbuilding blocks and the unpredictable optimal depth in convolutional neural\nnetworks. In addition, a new representation scheme is developed for effectively\ninitializing connection weights of deep convolutional neural networks, which is\nexpected to avoid networks getting stuck into local minima which is typically a\nmajor issue in the backward gradient-based optimization. Furthermore, a novel\nfitness evaluation method is proposed to speed up the heuristic search with\nsubstantially less computational resource. The proposed algorithm is examined\nand compared with 22 existing algorithms on nine widely used image\nclassification tasks, including the state-of-the-art methods. The experimental\nresults demonstrate the remarkable superiority of the proposed algorithm over\nthe state-of-the-art algorithms in terms of classification error rate and the\nnumber of parameters (weights).\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 02:04:07 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 00:54:22 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 23:23:51 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Yanan", ""], ["Xue", "Bing", ""], ["Zhang", "Mengjie", ""], ["Yen", "Gary G.", ""]]}, {"id": "1710.10749", "submitter": "Di Xie", "authors": "Qiaoyong Zhong and Chao Li and Yingying Zhang and Di Xie and Shicai\n  Yang and Shiliang Pu", "title": "Cascade Region Proposal and Global Context for Deep Object Detection", "comments": "Preprint to appear in Neurocomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep region-based object detector consists of a region proposal step and a\ndeep object recognition step. In this paper, we make significant improvements\non both of the two steps. For region proposal we propose a novel lightweight\ncascade structure which can effectively improve RPN proposal quality. For\nobject recognition we re-implement global context modeling with a few\nmodications and obtain a performance boost (4.2% mAP gain on the ILSVRC 2016\nvalidation set). Besides, we apply the idea of pre-training extensively and\nshow its importance in both steps. Together with common training and testing\ntricks, we improve Faster R-CNN baseline by a large margin. In particular, we\nobtain 87.9% mAP on the PASCAL VOC 2012 test set, 65.3% on the ILSVRC 2016 test\nset and 36.8% on the COCO test-std set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 03:00:04 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhong", "Qiaoyong", ""], ["Li", "Chao", ""], ["Zhang", "Yingying", ""], ["Xie", "Di", ""], ["Yang", "Shicai", ""], ["Pu", "Shiliang", ""]]}, {"id": "1710.10755", "submitter": "Jianyi Wang", "authors": "Yuhang Song, Mai Xu, Jianyi Wang, Minglang Qiao, Liangyu Huo, Zulin\n  Wang", "title": "Predicting Head Movement in Panoramic Video: A Deep Reinforcement\n  Learning Approach", "comments": "15 pages, 10 figures, published on TPAMI 2018", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence.\n  2018 Jul 24", "doi": "10.1109/TPAMI.2018.2858783", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Panoramic video provides immersive and interactive experience by enabling\nhumans to control the field of view (FoV) through head movement (HM). Thus, HM\nplays a key role in modeling human attention on panoramic video. This paper\nestablishes a database collecting subjects' HM in panoramic video sequences.\nFrom this database, we find that the HM data are highly consistent across\nsubjects. Furthermore, we find that deep reinforcement learning (DRL) can be\napplied to predict HM positions, via maximizing the reward of imitating human\nHM scanpaths through the agent's actions. Based on our findings, we propose a\nDRL-based HM prediction (DHP) approach with offline and online versions, called\noffline-DHP and online-DHP. In offline-DHP, multiple DRL workflows are run to\ndetermine potential HM positions at each panoramic frame. Then, a heat map of\nthe potential HM positions, named the HM map, is generated as the output of\noffline-DHP. In online-DHP, the next HM position of one subject is estimated\ngiven the currently observed HM position, which is achieved by developing a DRL\nalgorithm upon the learned offline-DHP model. Finally, the experiments validate\nthat our approach is effective in both offline and online prediction of HM\npositions for panoramic video, and that the learned offline-DHP model can\nimprove the performance of online-DHP.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 03:32:22 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 09:39:17 GMT"}, {"version": "v3", "created": "Tue, 2 Jan 2018 01:04:53 GMT"}, {"version": "v4", "created": "Fri, 21 Sep 2018 01:51:30 GMT"}, {"version": "v5", "created": "Thu, 28 Nov 2019 09:33:09 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Song", "Yuhang", ""], ["Xu", "Mai", ""], ["Wang", "Jianyi", ""], ["Qiao", "Minglang", ""], ["Huo", "Liangyu", ""], ["Wang", "Zulin", ""]]}, {"id": "1710.10781", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai", "title": "Stochastic variance reduced multiplicative update for nonnegative matrix\n  factorization", "comments": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative matrix factorization (NMF), a dimensionality reduction and factor\nanalysis method, is a special case in which factor matrices have low-rank\nnonnegative constraints. Considering the stochastic learning in NMF, we\nspecifically address the multiplicative update (MU) rule, which is the most\npopular, but which has slow convergence property. This present paper introduces\non the stochastic MU rule a variance-reduced technique of stochastic gradient.\nNumerical comparisons suggest that our proposed algorithms robustly outperform\nstate-of-the-art algorithms across different synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 06:14:17 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 21:45:46 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Kasai", "Hiroyuki", ""]]}, {"id": "1710.10800", "submitter": "Bharath Ramesh", "authors": "Bharath Ramesh, Hong Yang, Garrick Orchard, Ngoc Anh Le Thi, Shihao\n  Zhang and Cheng Xiang", "title": "DART: Distribution Aware Retinal Transform for Event-based Cameras", "comments": "12 pages, revision submitted to TPAMI in Nov 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generic visual descriptor, termed as distribution aware\nretinal transform (DART), that encodes the structural context using log-polar\ngrids for event cameras. The DART descriptor is applied to four different\nproblems, namely object classification, tracking, detection and feature\nmatching: (1) The DART features are directly employed as local descriptors in a\nbag-of-features classification framework and testing is carried out on four\nstandard event-based object datasets (N-MNIST, MNIST-DVS, CIFAR10-DVS,\nNCaltech-101). (2) Extending the classification system, tracking is\ndemonstrated using two key novelties: (i) For overcoming the low-sample problem\nfor the one-shot learning of a binary classifier, statistical bootstrapping is\nleveraged with online learning; (ii) To achieve tracker robustness, the scale\nand rotation equivariance property of the DART descriptors is exploited for the\none-shot learning. (3) To solve the long-term object tracking problem, an\nobject detector is designed using the principle of cluster majority voting. The\ndetection scheme is then combined with the tracker to result in a high\nintersection-over-union score with augmented ground truth annotations on the\npublicly available event camera dataset. (4) Finally, the event context encoded\nby DART greatly simplifies the feature correspondence problem, especially for\nspatio-temporal slices far apart in time, which has not been explicitly tackled\nin the event-based vision domain.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 08:08:57 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 02:37:41 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 07:40:55 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Ramesh", "Bharath", ""], ["Yang", "Hong", ""], ["Orchard", "Garrick", ""], ["Thi", "Ngoc Anh Le", ""], ["Zhang", "Shihao", ""], ["Xiang", "Cheng", ""]]}, {"id": "1710.10891", "submitter": "Christian Herrmann", "authors": "Andras T\\\"uzk\\\"o, Christian Herrmann, Daniel Manger, J\\\"urgen Beyerer", "title": "Open Set Logo Detection and Retrieval", "comments": "accepted at VISAPP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current logo retrieval research focuses on closed set scenarios. We argue\nthat the logo domain is too large for this strategy and requires an open set\napproach. To foster research in this direction, a large-scale logo dataset,\ncalled Logos in the Wild, is collected and released to the public. A typical\nopen set logo retrieval application is, for example, assessing the\neffectiveness of advertisement in sports event broadcasts. Given a query sample\nin shape of a logo image, the task is to find all further occurrences of this\nlogo in a set of images or videos. Currently, common logo retrieval approaches\nare unsuitable for this task because of their closed world assumption. Thus, an\nopen set logo retrieval method is proposed in this work which allows searching\nfor previously unseen logos by a single query sample. A two stage concept with\nseparate logo detection and comparison is proposed where both modules are based\non task specific CNNs. If trained with the Logos in the Wild data, significant\nperformance improvements are observed, especially compared with\nstate-of-the-art closed set approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:01:32 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["T\u00fczk\u00f6", "Andras", ""], ["Herrmann", "Christian", ""], ["Manger", "Daniel", ""], ["Beyerer", "J\u00fcrgen", ""]]}, {"id": "1710.10898", "submitter": "Jonas Adler", "authors": "Jonas Adler, Axel Ringh, Ozan \\\"Oktem and Johan Karlsson", "title": "Learning to solve inverse problems using Wasserstein loss", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.FA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using the Wasserstein loss for training in inverse problems. In\nparticular, we consider a learned primal-dual reconstruction scheme for\nill-posed inverse problems using the Wasserstein distance as loss function in\nthe learning. This is motivated by miss-alignments in training data, which when\nusing standard mean squared error loss could severely degrade reconstruction\nquality. We prove that training with the Wasserstein loss gives a\nreconstruction operator that correctly compensates for miss-alignments in\ncertain cases, whereas training with the mean squared error gives a smeared\nreconstruction. Moreover, we demonstrate these effects by training a\nreconstruction algorithm using both mean squared error and optimal transport\nloss for a problem in computerized tomography.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:30:16 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Adler", "Jonas", ""], ["Ringh", "Axel", ""], ["\u00d6ktem", "Ozan", ""], ["Karlsson", "Johan", ""]]}, {"id": "1710.10916", "submitter": "Han Zhang", "authors": "Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang,\n  Xiaolei Huang, Dimitris Metaxas", "title": "StackGAN++: Realistic Image Synthesis with Stacked Generative\n  Adversarial Networks", "comments": "In IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI),\n  2018. (16 pages, 15 figures.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Generative Adversarial Networks (GANs) have shown remarkable success\nin various tasks, they still face challenges in generating high quality images.\nIn this paper, we propose Stacked Generative Adversarial Networks (StackGAN)\naiming at generating high-resolution photo-realistic images. First, we propose\na two-stage generative adversarial network architecture, StackGAN-v1, for\ntext-to-image synthesis. The Stage-I GAN sketches the primitive shape and\ncolors of the object based on given text description, yielding low-resolution\nimages. The Stage-II GAN takes Stage-I results and text descriptions as inputs,\nand generates high-resolution images with photo-realistic details. Second, an\nadvanced multi-stage generative adversarial network architecture, StackGAN-v2,\nis proposed for both conditional and unconditional generative tasks. Our\nStackGAN-v2 consists of multiple generators and discriminators in a tree-like\nstructure; images at multiple scales corresponding to the same scene are\ngenerated from different branches of the tree. StackGAN-v2 shows more stable\ntraining behavior than StackGAN-v1 by jointly approximating multiple\ndistributions. Extensive experiments demonstrate that the proposed stacked\ngenerative adversarial networks significantly outperform other state-of-the-art\nmethods in generating photo-realistic images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:45:59 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 06:43:57 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 00:49:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Zhang", "Han", ""], ["Xu", "Tao", ""], ["Li", "Hongsheng", ""], ["Zhang", "Shaoting", ""], ["Wang", "Xiaogang", ""], ["Huang", "Xiaolei", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1710.10928", "submitter": "Quynh Nguyen", "authors": "Quynh Nguyen and Matthias Hein", "title": "Optimization Landscape and Expressivity of Deep CNNs", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the loss landscape and expressiveness of practical deep\nconvolutional neural networks (CNNs) with shared weights and max pooling\nlayers. We show that such CNNs produce linearly independent features at a\n\"wide\" layer which has more neurons than the number of training samples. This\ncondition holds e.g. for the VGG network. Furthermore, we provide for such wide\nCNNs necessary and sufficient conditions for global minima with zero training\nerror. For the case where the wide layer is followed by a fully connected layer\nwe show that almost every critical point of the empirical loss is a global\nminimum with zero training error. Our analysis suggests that both depth and\nwidth are very important in deep learning. While depth brings more\nrepresentational power and allows the network to learn high level features,\nwidth smoothes the optimization landscape of the loss function in the sense\nthat a sufficiently wide network has a well-behaved loss surface with almost no\nbad local minima.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 13:24:28 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 12:49:58 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Nguyen", "Quynh", ""], ["Hein", "Matthias", ""]]}, {"id": "1710.10948", "submitter": "Eric Ferguson", "authors": "Eric L. Ferguson, Stefan B. Williams and Craig T. Jin", "title": "Sound Source Localization in a Multipath Environment Using Convolutional\n  Neural Networks", "comments": "5 pages, 5 figures, Final draft of paper submitted to 2018 IEEE\n  International Conference on Acoustics, Speech and Signal Processing (ICASSP)\n  15-20 April 2018 in Calgary, Alberta, Canada. arXiv admin note: text overlap\n  with arXiv:1612.03505", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The propagation of sound in a shallow water environment is characterized by\nboundary reflections from the sea surface and sea floor. These reflections\nresult in multiple (indirect) sound propagation paths, which can degrade the\nperformance of passive sound source localization methods. This paper proposes\nthe use of convolutional neural networks (CNNs) for the localization of sources\nof broadband acoustic radiated noise (such as motor vessels) in shallow water\nmultipath environments. It is shown that CNNs operating on cepstrogram and\ngeneralized cross-correlogram inputs are able to more reliably estimate the\ninstantaneous range and bearing of transiting motor vessels when the source\nlocalization performance of conventional passive ranging methods is degraded.\nThe ensuing improvement in source localization performance is demonstrated\nusing real data collected during an at-sea experiment.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 01:14:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Ferguson", "Eric L.", ""], ["Williams", "Stefan B.", ""], ["Jin", "Craig T.", ""]]}, {"id": "1710.10985", "submitter": "Niels Christian Overaard", "authors": "Niels Chr. Overgaard", "title": "On the Taut String Interpretation of the One-dimensional\n  Rudin-Osher-Fatemi Model: A New Proof, a Fundamental Estimate and Some\n  Applications", "comments": "19 pages, 2 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new proof of the equivalence of the Taut String Algorithm and the\none-dimensional Rudin-Osher-Fatemi model is presented. Based on duality and the\nprojection theorem in Hilbert space, the proof is strictly elementary.\nExistence and uniqueness of solutions to both denoising models follow as\nby-products. The standard convergence properties of the denoised signal, as the\nregularizing parameter tends to zero, are recalled and efficient proofs\nprovided. Moreover, a new and fundamental bound on the denoised signal is\nderived. This bound implies, among other things, the strong convergence (in the\nspace of functions of bounded variation) of the denoised signal to the insignal\nas the regularization parameter vanishes. The methods developed in the paper\ncan be modified to cover other interesting applications such as isotonic\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 12:59:56 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Overgaard", "Niels Chr.", ""]]}, {"id": "1710.11004", "submitter": "Akisato Kimura", "authors": "Masaya Hibino, Akisato Kimura, Takayoshi Yamashita, Yuji Yamauchi,\n  Hironobu Fujiyoshi", "title": "Denoising random forests", "comments": "20 pages, 10 figures, submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel type of random forests called a denoising random\nforests that are robust against noises contained in test samples. Such\nnoise-corrupted samples cause serious damage to the estimation performances of\nrandom forests, since unexpected child nodes are often selected and the leaf\nnodes that the input sample reaches are sometimes far from those for a clean\nsample. Our main idea for tackling this problem originates from a binary\nindicator vector that encodes a traversal path of a sample in the forest. Our\nproposed method effectively employs this vector by introducing denoising\nautoencoders into random forests. A denoising autoencoder can be trained with\nindicator vectors produced from clean and noisy input samples, and non-leaf\nnodes where incorrect decisions are made can be identified by comparing the\ninput and output of the trained denoising autoencoder. Multiple traversal paths\nwith respect to the nodes with incorrect decisions caused by the noises can\nthen be considered for the estimation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 15:16:51 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Hibino", "Masaya", ""], ["Kimura", "Akisato", ""], ["Yamashita", "Takayoshi", ""], ["Yamauchi", "Yuji", ""], ["Fujiyoshi", "Hironobu", ""]]}, {"id": "1710.11052", "submitter": "Dmitrij Schlesinger", "authors": "Dmitrij Schlesinger", "title": "A Connection between Feed-Forward Neural Networks and Probabilistic\n  Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two of the most popular modelling paradigms in computer vision are\nfeed-forward neural networks (FFNs) and probabilistic graphical models (GMs).\nVarious connections between the two have been studied in recent works, such as\ne.g. expressing mean-field based inference in a GM as an FFN. This paper\nestablishes a new connection between FFNs and GMs. Our key observation is that\nany FFN implements a certain approximation of a corresponding Bayesian network\n(BN). We characterize various benefits of having this connection. In\nparticular, it results in a new learning algorithm for BNs. We validate the\nproposed methods for a classification problem on CIFAR-10 dataset and for\nbinary image segmentation on Weizmann Horse dataset. We show that statistically\nlearned BNs improve performance, having at the same time essentially better\ngeneralization capability, than their FFN counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:31:24 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Schlesinger", "Dmitrij", ""]]}, {"id": "1710.11063", "submitter": "Anirban Sarkar", "authors": "Aditya Chattopadhyay, Anirban Sarkar, Prantik Howlader and Vineeth N\n  Balasubramanian", "title": "Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks", "comments": "17 Pages, 15 Figures, 11 Tables. Accepted in the proceedings of IEEE\n  Winter Conf. on Applications of Computer Vision (WACV2018). Extended version\n  is under review at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": "10.1109/WACV.2018.00097", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, Convolutional Neural Network (CNN) models have been\nhighly successful in solving complex vision problems. However, these deep\nmodels are perceived as \"black box\" methods considering the lack of\nunderstanding of their internal functioning. There has been a significant\nrecent interest in developing explainable deep learning models, and this paper\nis an effort in this direction. Building on a recently proposed method called\nGrad-CAM, we propose a generalized method called Grad-CAM++ that can provide\nbetter visual explanations of CNN model predictions, in terms of better object\nlocalization as well as explaining occurrences of multiple object instances in\na single image, when compared to state-of-the-art. We provide a mathematical\nderivation for the proposed method, which uses a weighted combination of the\npositive partial derivatives of the last convolutional layer feature maps with\nrespect to a specific class score as weights to generate a visual explanation\nfor the corresponding class label. Our extensive experiments and evaluations,\nboth subjective and objective, on standard datasets showed that Grad-CAM++\nprovides promising human-interpretable visual explanations for a given CNN\narchitecture across multiple tasks including classification, image caption\ngeneration and 3D action recognition; as well as in new settings such as\nknowledge distillation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:55:43 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 13:39:48 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 19:21:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chattopadhyay", "Aditya", ""], ["Sarkar", "Anirban", ""], ["Howlader", "Prantik", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "1710.11075", "submitter": "Rajesh  Kumar", "authors": "Rajesh Kumar, Partha Pratim Kundu, Vir V. Phoha", "title": "Continuous Authentication Using One-class Classifiers and their Fusion", "comments": "2018 IEEE 4th International Conference on Identity, Security, and\n  Behavior Analysis (ISBA) 978-1-5386-2248-3/18/$31.00 (c) 2018 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While developing continuous authentication systems (CAS), we generally assume\nthat samples from both genuine and impostor classes are readily available.\nHowever, the assumption may not be true in certain circumstances. Therefore, we\nexplore the possibility of implementing CAS using only genuine samples.\nSpecifically, we investigate the usefulness of four one-class classifiers OCC\n(elliptic envelope, isolation forest, local outliers factor, and one-class\nsupport vector machines) and their fusion. The performance of these classifiers\nwas evaluated on four distinct behavioral biometric datasets, and compared with\neight multi-class classifiers (MCC). The results demonstrate that if we have\nsufficient training data from the genuine user the OCC, and their fusion can\nclosely match the performance of the majority of MCC. Our findings encourage\nthe research community to use OCC in order to build CAS as they do not require\nknowledge of impostor class during the enrollment process.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 17:16:21 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Kumar", "Rajesh", ""], ["Kundu", "Partha Pratim", ""], ["Phoha", "Vir V.", ""]]}, {"id": "1710.11087", "submitter": "Neha Bhargava", "authors": "Neha Bhargava, Subhasis Chaudhuri", "title": "An Integrated Approach to Crowd Video Analysis: From Tracking to\n  Multi-level Activity Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an integrated framework for simultaneous tracking, group detection\nand multi-level activity recognition in crowd videos. Instead of solving these\nproblems independently and sequentially, we solve them together in a unified\nframework to utilize the strong correlation that exists among individual\nmotion, groups, and activities. We explore the hierarchical structure hidden in\nthe video that connects individuals over time to produce tracks, connects\nindividuals to form groups and also connects groups together to form a crowd.\nWe show that estimation of this hidden structure corresponds to track\nassociation and group detection. We estimate this hidden structure under a\nlinear programming formulation. The obtained graphical representation is\nfurther explored to recognize the node values that corresponds to multi-level\nactivity recognition. This problem is solved under a structured SVM framework.\nThe results on publicly available dataset show very competitive performance at\nall levels of granularity with the state-of-the-art batch processing methods\ndespite the proposed technique being an online (causal) one.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 17:34:00 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Bhargava", "Neha", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1710.11121", "submitter": "Pranay Manocha Mr.", "authors": "Pranay Manocha, Snehal Bhasme, Tanvi Gupta, BK Panigrahi, Tapan K.\n  Gandhi", "title": "Automated Tumor Segmentation and Brain Mapping for the Tumor Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is an important diagnostic tool for precise\ndetection of various pathologies. Magnetic Resonance (MR) is more preferred\nthan Computed Tomography (CT) due to the high resolution in MR images which\nhelp in better detection of neurological conditions. Graphical user interface\n(GUI) aided disease detection has become increasingly useful due to the\nincreasing workload of doctors. In this proposed work, a novel two steps GUI\ntechnique for brain tumor segmentation as well as Brodmann area detec-tion of\nthe segmented tumor is proposed. A data set of T2 weighted images of 15\npatients is used for validating the proposed method. The patient data\nincor-porates variations in ethnicities, gender (male and female) and age\n(25-50), thus enhancing the authenticity of the proposed method. The tumors\nwere segmented using Fuzzy C Means Clustering and Brodmann area detection was\ndone using a known template, mapping each area to the segmented tumor image.\nThe proposed method was found to be fairly accurate and robust in detecting\ntumor.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 20:49:25 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Manocha", "Pranay", ""], ["Bhasme", "Snehal", ""], ["Gupta", "Tanvi", ""], ["Panigrahi", "BK", ""], ["Gandhi", "Tapan K.", ""]]}, {"id": "1710.11151", "submitter": "Hyomin Choi", "authors": "Hyomin Choi and Ivan V. Bajic", "title": "High efficiency compression for object detection", "comments": "The paper is published in IEEE ICASSP 18'", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video compression has traditionally been tailored to human vision.\nHowever, modern applications such as visual analytics and surveillance rely on\ncomputers seeing and analyzing the images before (or instead of) humans. For\nthese applications, it is important to adjust compression to computer vision.\nIn this paper we present a bit allocation and rate control strategy that is\ntailored to object detection. Using the initial convolutional layers of a\nstate-of-the-art object detector, we create an importance map that can guide\nbit allocation to areas that are important for object detection. The proposed\nmethod enables bit rate savings of 7% or more compared to default HEVC, at the\nequivalent object detection rate.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:03:22 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 01:50:22 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Choi", "Hyomin", ""], ["Bajic", "Ivan V.", ""]]}, {"id": "1710.11176", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Nishant Vishwamitra, Hongxin Hu, Feng Luo", "title": "CrescendoNet: A Simple Deep Convolutional Neural Network with Ensemble\n  Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new deep convolutional neural network, CrescendoNet, by\nstacking simple building blocks without residual connections. Each Crescendo\nblock contains independent convolution paths with increased depths. The numbers\nof convolution layers and parameters are only increased linearly in Crescendo\nblocks. In experiments, CrescendoNet with only 15 layers outperforms almost all\nnetworks without residual connections on benchmark datasets, CIFAR10, CIFAR100,\nand SVHN. Given sufficient amount of data as in SVHN dataset, CrescendoNet with\n15 layers and 4.1M parameters can match the performance of DenseNet-BC with 250\nlayers and 15.3M parameters. CrescendoNet provides a new way to construct high\nperformance deep convolutional neural networks without residual connections.\nMoreover, through investigating the behavior and performance of subnetworks in\nCrescendoNet, we note that the high performance of CrescendoNet may come from\nits implicit ensemble behavior, which differs from the FractalNet that is also\na deep convolutional neural network without residual connections. Furthermore,\nthe independence between paths in CrescendoNet allows us to introduce a new\npath-wise training procedure, which can reduce the memory needed for training.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 18:35:01 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 17:01:21 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Zhang", "Xiang", ""], ["Vishwamitra", "Nishant", ""], ["Hu", "Hongxin", ""], ["Luo", "Feng", ""]]}, {"id": "1710.11201", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis and Georgios Tzimiropoulos", "title": "Deep word embeddings for visual speech recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a deep learning architecture for extracting word\nembeddings for visual speech recognition. The embeddings summarize the\ninformation of the mouth region that is relevant to the problem of word\nrecognition, while suppressing other types of variability such as speaker, pose\nand illumination. The system is comprised of a spatiotemporal convolutional\nlayer, a Residual Network and bidirectional LSTMs and is trained on the\nLipreading in-the-wild database. We first show that the proposed architecture\ngoes beyond state-of-the-art on closed-set word identification, by attaining\n11.92% error rate on a vocabulary of 500 words. We then examine the capacity of\nthe embeddings in modelling words unseen during training. We deploy\nProbabilistic Linear Discriminant Analysis (PLDA) to model the embeddings and\nperform low-shot learning experiments on words unseen during training. The\nexperiments demonstrate that word-level visual speech recognition is feasible\neven in cases where the target words are not included in the training set.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:09:29 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Stafylakis", "Themos", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1710.11216", "submitter": "Faisal Mahmood", "authors": "Faisal Mahmood and Nicholas J. Durr", "title": "Deep Learning and Conditional Random Fields-based Depth Estimation and\n  Topographical Reconstruction from Conventional Endoscopy", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer is the fourth leading cause of cancer deaths worldwide and\nthe second leading cause in the United States. The risk of colorectal cancer\ncan be mitigated by the identification and removal of premalignant lesions\nthrough optical colonoscopy. Unfortunately, conventional colonoscopy misses\nmore than 20% of the polyps that should be removed, due in part to poor\ncontrast of lesion topography. Imaging tissue topography during a colonoscopy\nis difficult because of the size constraints of the endoscope and the deforming\nmucosa. Most existing methods make geometric assumptions or incorporate a\npriori information, which limits accuracy and sensitivity. In this paper, we\npresent a method that avoids these restrictions, using a joint deep\nconvolutional neural network-conditional random field (CNN-CRF) framework.\nEstimated depth is used to reconstruct the topography of the surface of the\ncolon from a single image. We train the unary and pairwise potential functions\nof a CRF in a CNN on synthetic data, generated by developing an endoscope\ncamera model and rendering over 100,000 images of an anatomically-realistic\ncolon. We validate our approach with real endoscopy images from a porcine\ncolon, transferred to a synthetic-like domain, with ground truth from\nregistered computed tomography measurements. The CNN-CRF approach estimates\ndepths with a relative error of 0.152 for synthetic endoscopy images and 0.242\nfor real endoscopy images. We show that the estimated depth maps can be used\nfor reconstructing the topography of the mucosa from conventional colonoscopy\nimages. This approach can easily be integrated into existing endoscopy systems\nand provides a foundation for improving computer-aided detection algorithms for\ndetection, segmentation and classification of lesions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:56:52 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 13:34:27 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 20:42:13 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mahmood", "Faisal", ""], ["Durr", "Nicholas J.", ""]]}, {"id": "1710.11252", "submitter": "Mohammad Babaeizadeh", "authors": "Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H. Campbell,\n  Sergey Levine", "title": "Stochastic Variational Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the future in real-world settings, particularly from raw sensory\nobservations such as images, is exceptionally challenging. Real-world events\ncan be stochastic and unpredictable, and the high dimensionality and complexity\nof natural images requires the predictive model to build an intricate\nunderstanding of the natural world. Many existing methods tackle this problem\nby making simplifying assumptions about the environment. One common assumption\nis that the outcome is deterministic and there is only one plausible future.\nThis can lead to low-quality predictions in real-world settings with stochastic\ndynamics. In this paper, we develop a stochastic variational video prediction\n(SV2P) method that predicts a different possible future for each sample of its\nlatent variables. To the best of our knowledge, our model is the first to\nprovide effective stochastic multi-frame prediction for real-world video. We\ndemonstrate the capability of the proposed method in predicting detailed future\nframes of videos on multiple real-world datasets, both action-free and\naction-conditioned. We find that our proposed method produces substantially\nimproved video predictions when compared to the same model without\nstochasticity, and to other stochastic video prediction methods. Our SV2P\nimplementation will be open sourced upon publication.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 21:48:54 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 16:35:06 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Babaeizadeh", "Mohammad", ""], ["Finn", "Chelsea", ""], ["Erhan", "Dumitru", ""], ["Campbell", "Roy H.", ""], ["Levine", "Sergey", ""]]}, {"id": "1710.11309", "submitter": "Pranay Manocha Mr.", "authors": "Tanvi Gupta, Pranay Manocha, Tapan K. Gandhi, RK Gupta, BK Panigrahi", "title": "Tumor Classification and Segmentation of MR Brain Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis and segmentation of tumors using any medical diagnostic tool\ncan be challenging due to the varying nature of this pathology. Magnetic Reso-\nnance Imaging (MRI) is an established diagnostic tool for various diseases and\ndisorders and plays a major role in clinical neuro-diagnosis. Supplementing\nthis technique with automated classification and segmentation tools is gaining\nimportance, to reduce errors and time needed to make a conclusive diagnosis. In\nthis paper a simple three-step algorithm is proposed; (1) identification of\npatients that present with tumors, (2) automatic selection of abnormal slices\nof the patients, and (3) segmentation and detection of the tumor. Features were\nextracted by using discrete wavelet transform on the normalized images and\nclassified by support vector machine (for step (1)) and random forest (for step\n(2)). The 400 subjects were divided in a 3:1 ratio between training and test\nwith no overlap. This study is novel in terms of use of data, as it employed\nthe entire T2 weighted slices as a single image for classification and a unique\ncombination of contralateral approach with patch thresholding for segmentation,\nwhich does not require a training set or a template as is used by most\nsegmentation studies. Using the proposed method, the tumors were segmented\naccurately with a classification accuracy of 95% with 100% specificity and 90%\nsensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 03:12:47 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Gupta", "Tanvi", ""], ["Manocha", "Pranay", ""], ["Gandhi", "Tapan K.", ""], ["Gupta", "RK", ""], ["Panigrahi", "BK", ""]]}, {"id": "1710.11342", "submitter": "Zhengli Zhao", "authors": "Zhengli Zhao, Dheeru Dua, Sameer Singh", "title": "Generating Natural Adversarial Examples", "comments": "Published as a conference paper at the International Conference on\n  Learning Representations (ICLR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their complex nature, it is hard to characterize the ways in which\nmachine learning models can misbehave or be exploited when deployed. Recent\nwork on adversarial examples, i.e. inputs with minor perturbations that result\nin substantially different model predictions, is helpful in evaluating the\nrobustness of these models by exposing the adversarial scenarios where they\nfail. However, these malicious perturbations are often unnatural, not\nsemantically meaningful, and not applicable to complicated domains such as\nlanguage. In this paper, we propose a framework to generate natural and legible\nadversarial examples that lie on the data manifold, by searching in semantic\nspace of dense and continuous data representation, utilizing the recent\nadvances in generative adversarial networks. We present generated adversaries\nto demonstrate the potential of the proposed approach for black-box classifiers\nfor a wide range of applications such as image classification, textual\nentailment, and machine translation. We include experiments to show that the\ngenerated adversaries are natural, legible to humans, and useful in evaluating\nand analyzing black-box classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 06:22:26 GMT"}, {"version": "v2", "created": "Fri, 23 Feb 2018 23:28:31 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Zhao", "Zhengli", ""], ["Dua", "Dheeru", ""], ["Singh", "Sameer", ""]]}, {"id": "1710.11354", "submitter": "Neha Bhargava", "authors": "Neha Bhargava, Subhasis Chaudhuri", "title": "Spatio-temporal interaction model for crowd video analysis", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an unsupervised approach to analyze crowd at various levels of\ngranularity $-$ individual, group and collective. We also propose a motion\nmodel to represent the collective motion of the crowd. The model captures the\nspatio-temporal interaction pattern of the crowd from the trajectory data\ncaptured over a time period. Furthermore, we also propose an effective group\ndetection algorithm that utilizes the eigenvectors of the interaction matrix of\nthe model. We also show that the eigenvalues of the interaction matrix\ncharacterize various group activities such as being stationary, walking,\nsplitting and approaching. The algorithm is also extended trivially to\nrecognize individual activity. Finally, we discover the overall crowd behavior\nby classifying a crowd video in one of the eight categories. Since the crowd\nbehavior is determined by its constituent groups, we demonstrate the usefulness\nof group level features during classification. Extensive experimentation on\nvarious datasets demonstrates a superlative performance of our algorithms over\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 07:24:40 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Bhargava", "Neha", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1710.11359", "submitter": "Iaroslav Melekhov", "authors": "Iaroslav Melekhov, Juho Kannala, Esa Rahtu", "title": "Image Patch Matching Using Convolutional Descriptors with Euclidean\n  Distance", "comments": "The paper was published in ACCV 2016 Workshops proceedings (Workshop\n  on Interpretation and Visualization of Deep Neural Nets)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a neural network based image descriptor suitable for\nimage patch matching, which is an important task in many computer vision\napplications. Our approach is influenced by recent success of deep\nconvolutional neural networks (CNNs) in object detection and classification\ntasks. We develop a model which maps the raw input patch to a low dimensional\nfeature vector so that the distance between representations is small for\nsimilar patches and large otherwise. As a distance metric we utilize L2 norm,\ni.e. Euclidean distance, which is fast to evaluate and used in most popular\nhand-crafted descriptors, such as SIFT. According to the results, our approach\noutperforms state-of-the-art L2-based descriptors and can be considered as a\ndirect replacement of SIFT. In addition, we conducted experiments with batch\nnormalization and histogram equalization as a preprocessing method of the input\ndata. The results confirm that these techniques further improve the performance\nof the proposed descriptor. Finally, we show promising preliminary results by\nappending our CNNs with recently proposed spatial transformer networks and\nprovide a visualisation and interpretation of their impact.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 07:48:11 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Melekhov", "Iaroslav", ""], ["Kannala", "Juho", ""], ["Rahtu", "Esa", ""]]}, {"id": "1710.11374", "submitter": "Mohammad Saeed Rad", "authors": "Mohammad Saeed Rad, Andreas von Kaenel, Andre Droux, Francois Tieche,\n  Nabil Ouerhani, Hazim Kemal Ekenel, Jean-Philippe Thiran", "title": "A Computer Vision System to Localize and Classify Wastes on the Streets", "comments": null, "journal-ref": "Liu M., Chen H., Vincze M. (eds) Computer Vision Systems. pp\n  195-204. ICVS 2017. Lecture Notes in Computer Science, vol 10528. Springer,\n  Cham", "doi": "10.1007/978-3-319-68345-4_18", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Littering quantification is an important step for improving cleanliness of\ncities. When human interpretation is too cumbersome or in some cases\nimpossible, an objective index of cleanliness could reduce the littering by\nawareness actions. In this paper, we present a fully automated computer vision\napplication for littering quantification based on images taken from the streets\nand sidewalks. We have employed a deep learning based framework to localize and\nclassify different types of wastes. Since there was no waste dataset available,\nwe built our acquisition system mounted on a vehicle. Collected images\ncontaining different types of wastes. These images are then annotated for\ntraining and benchmarking the developed system. Our results on real case\nscenarios show accurate detection of littering on variant backgrounds.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 08:57:23 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Rad", "Mohammad Saeed", ""], ["von Kaenel", "Andreas", ""], ["Droux", "Andre", ""], ["Tieche", "Francois", ""], ["Ouerhani", "Nabil", ""], ["Ekenel", "Hazim Kemal", ""], ["Thiran", "Jean-Philippe", ""]]}, {"id": "1710.11397", "submitter": "Andrew Warrington", "authors": "Andrew Warrington and Frank Wood", "title": "Updating the VESICLE-CNN Synapse Detector", "comments": "Submitted as two side extended abstract to NIPS 2017 workshop:\n  BigNeuro 2017: Analyzing brain data from nano to macroscale", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an updated version of the VESICLE-CNN algorithm presented by\nRoncal et al. (2014). The original implementation makes use of a patch-based\napproach. This methodology is known to be slow due to repeated computations. We\nupdate this implementation to be fully convolutional through the use of dilated\nconvolutions, recovering the expanded field of view achieved through the use of\nstrided maxpools, but without a degradation of spatial resolution. This updated\nimplementation performs as well as the original implementation, but with a\n$600\\times$ speedup at test time. We release source code and data into the\npublic domain.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 10:17:15 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Warrington", "Andrew", ""], ["Wood", "Frank", ""]]}, {"id": "1710.11431", "submitter": "Anuj Karpatne", "authors": "Anuj Karpatne, William Watkins, Jordan Read, and Vipin Kumar", "title": "Physics-guided Neural Networks (PGNN): An Application in Lake\n  Temperature Modeling", "comments": "submitted to ACM SIGKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel framework for combining scientific knowledge of\nphysics-based models with neural networks to advance scientific discovery. This\nframework, termed as physics-guided neural network (PGNN), leverages the output\nof physics-based model simulations along with observational features to\ngenerate predictions using a neural network architecture. Further, this paper\npresents a novel framework for using physics-based loss functions in the\nlearning objective of neural networks, to ensure that the model predictions not\nonly show lower errors on the training set but are also scientifically\nconsistent with the known physics on the unlabeled set. We illustrate the\neffectiveness of PGNN for the problem of lake temperature modeling, where\nphysical relationships between the temperature, density, and depth of water are\nused to design a physics-based loss function. By using scientific knowledge to\nguide the construction and learning of neural networks, we are able to show\nthat the proposed framework ensures better generalizability as well as\nscientific consistency of results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 12:24:26 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 17:33:48 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Karpatne", "Anuj", ""], ["Watkins", "William", ""], ["Read", "Jordan", ""], ["Kumar", "Vipin", ""]]}, {"id": "1710.11445", "submitter": "Ya Zhang", "authors": "Yuefu Zhou, Shanshan Huang, Ya Zhang, Yanfeng Wang", "title": "Deep Hashing with Triplet Quantization Loss", "comments": "4 pages, to be presented at IEEE VCIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the explosive growth of image databases, deep hashing, which learns\ncompact binary descriptors for images, has become critical for fast image\nretrieval. Many existing deep hashing methods leverage quantization loss,\ndefined as distance between the features before and after quantization, to\nreduce the error from binarizing features. While minimizing the quantization\nloss guarantees that quantization has minimal effect on retrieval accuracy, it\nunfortunately significantly reduces the expressiveness of features even before\nthe quantization. In this paper, we show that the above definition of\nquantization loss is too restricted and in fact not necessary for maintaining\nhigh retrieval accuracy. We therefore propose a new form of quantization loss\nmeasured in triplets. The core idea of the triplet quantization loss is to\nlearn discriminative real-valued descriptors which lead to minimal loss on\nretrieval accuracy after quantization. Extensive experiments on two widely used\nbenchmark data sets of different scales, CIFAR-10 and In-shop, demonstrate that\nthe proposed method outperforms the state-of-the-art deep hashing methods.\nMoreover, we show that the compact binary descriptors obtained with triplet\nquantization loss lead to very small performance drop after quantization.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 13:08:42 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Zhou", "Yuefu", ""], ["Huang", "Shanshan", ""], ["Zhang", "Ya", ""], ["Wang", "Yanfeng", ""]]}, {"id": "1710.11446", "submitter": "Ya Zhang", "authors": "Zhonghao Wang, Yujun Gu, Ya Zhang, Jun Zhou, Xiao Gu", "title": "Clothing Retrieval with Visual Attention Model", "comments": "4 pages, to be presented at IEEE VCIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clothing retrieval is a challenging problem in computer vision. With the\nadvance of Convolutional Neural Networks (CNNs), the accuracy of clothing\nretrieval has been significantly improved. FashionNet[1], a recent study,\nproposes to employ a set of artificial features in the form of landmarks for\nclothing retrieval, which are shown to be helpful for retrieval. However, the\nlandmark detection module is trained with strong supervision which requires\nconsiderable efforts to obtain. In this paper, we propose a self-learning\nVisual Attention Model (VAM) to extract attention maps from clothing images.\nThe VAM is further connected to a global network to form an end-to-end network\nstructure through Impdrop connection which randomly Dropout on the feature maps\nwith the probabilities given by the attention map. Extensive experiments on\nseveral widely used benchmark clothing retrieval data sets have demonstrated\nthe promise of the proposed method. We also show that compared to the trivial\nProduct connection, the Impdrop connection makes the network structure more\nrobust when training sets of limited size are used.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 13:14:59 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Wang", "Zhonghao", ""], ["Gu", "Yujun", ""], ["Zhang", "Ya", ""], ["Zhou", "Jun", ""], ["Gu", "Xiao", ""]]}, {"id": "1710.11454", "submitter": "Shahin Vakilinia Dr", "authors": "Yao Yao, Mustafa Mehmet Ali, Shahin Vakilinia", "title": "Optimal Resource Allocation in Distributed Broadband Wireless\n  Communication Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper is concerned with optimization of distributed broadband wireless\ncommunication (BWC) systems. BWC systems contain a distributed antenna system\n(DAS) connected to a base station with optical fiber. Distributed BWC systems\nhave been proposed as a solution to the power constraint problem in traditional\ncellular networks. So far, the research on BWC systems have advanced on two\nseparate tracks, the design of the system to meet the quality of service\nrequirements (QoS) and optimization of the location of the DAS. In this paper,\nwe consider a combined optimization of BWC systems. We consider uplink\ncommunications in distributed BWC systems with multiple levels of priority\ntraffic with arrivals and departures forming renewal processes. We develop an\nanalysis that determines packet delay violation probability for each priority\nlevel as a function of the outage probability of the DAS through the\napplication of results from renewal theory. Then, we determine the optimal\nlocations of the antennas that minimize the antenna outage probability. We also\nstudy the tradeoff between the packet delay violation probability and packet\nloss probability. This work will be helpful in the designing of the distributed\nBWC systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 01:40:49 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Yao", "Yao", ""], ["Ali", "Mustafa Mehmet", ""], ["Vakilinia", "Shahin", ""]]}, {"id": "1710.11473", "submitter": "Emad Grais", "authors": "Emad M. Grais, Hagen Wierstorf, Dominic Ward, Mark D. Plumbley", "title": "Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio\n  Source Separation", "comments": "arXiv admin note: text overlap with arXiv:1703.08019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In deep neural networks with convolutional layers, each layer typically has\nfixed-size/single-resolution receptive field (RF). Convolutional layers with a\nlarge RF capture global information from the input features, while layers with\nsmall RF size capture local details with high resolution from the input\nfeatures. In this work, we introduce novel deep multi-resolution fully\nconvolutional neural networks (MR-FCNN), where each layer has different RF\nsizes to extract multi-resolution features that capture the global and local\ndetails information from its input features. The proposed MR-FCNN is applied to\nseparate a target audio source from a mixture of many audio sources.\nExperimental results show that using MR-FCNN improves the performance compared\nto feedforward deep neural networks (DNNs) and single resolution deep fully\nconvolutional neural networks (FCNNs) on the audio source separation problem.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 22:12:08 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Grais", "Emad M.", ""], ["Wierstorf", "Hagen", ""], ["Ward", "Dominic", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "1710.11510", "submitter": "Sohrab Ferdowsi", "authors": "Sohrab Ferdowsi, Slava Voloshynovskiy, Dimche Kostadinov", "title": "A multi-layer network based on Sparse Ternary Codes for universal vector\n  compression", "comments": "Submitted to ICASSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the multi-layer extension of the Sparse Ternary Codes (STC) for\nfast similarity search where we focus on the reconstruction of the database\nvectors from the ternary codes. To consider the trade-offs between the\ncompactness of the STC and the quality of the reconstructed vectors, we study\nthe rate-distortion behavior of these codes under different setups. We show\nthat a single-layer code cannot achieve satisfactory results at high rates.\nTherefore, we extend the concept of STC to multiple layers and design the\nML-STC, a codebook-free system that successively refines the reconstruction of\nthe residuals of previous layers. While the ML-STC keeps the sparse ternary\nstructure of the single-layer STC and hence is suitable for fast similarity\nsearch in large-scale databases, we show its superior rate-distortion\nperformance on both model-based synthetic data and public large-scale\ndatabases, as compared to several binary hashing methods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 14:33:29 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Ferdowsi", "Sohrab", ""], ["Voloshynovskiy", "Slava", ""], ["Kostadinov", "Dimche", ""]]}, {"id": "1710.11573", "submitter": "Abram Friesen", "authors": "Abram L. Friesen and Pedro Domingos", "title": "Deep Learning as a Mixed Convex-Combinatorial Optimization Problem", "comments": "14 pages (9 body, 5 pages of references and appendices)", "journal-ref": "In Proceedings of the International Conference on Learning\n  Representations (ICLR) 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural networks grow deeper and wider, learning networks with\nhard-threshold activations is becoming increasingly important, both for network\nquantization, which can drastically reduce time and energy requirements, and\nfor creating large integrated systems of deep networks, which may have\nnon-differentiable components and must avoid vanishing and exploding gradients\nfor effective learning. However, since gradient descent is not applicable to\nhard-threshold functions, it is not clear how to learn networks of them in a\nprincipled way. We address this problem by observing that setting targets for\nhard-threshold hidden units in order to minimize loss is a discrete\noptimization problem, and can be solved as such. The discrete optimization goal\nis to find a set of targets such that each unit, including the output, has a\nlinearly separable problem to solve. Given these targets, the network\ndecomposes into individual perceptrons, which can then be learned with standard\nconvex approaches. Based on this, we develop a recursive mini-batch algorithm\nfor learning deep hard-threshold networks that includes the popular but poorly\njustified straight-through estimator as a special case. Empirically, we show\nthat our algorithm improves classification accuracy in a number of settings,\nincluding for AlexNet and ResNet-18 on ImageNet, when compared to the\nstraight-through estimator.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 16:42:44 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 17:58:16 GMT"}, {"version": "v3", "created": "Mon, 16 Apr 2018 20:46:14 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Friesen", "Abram L.", ""], ["Domingos", "Pedro", ""]]}, {"id": "1710.11599", "submitter": "Changzhe Jiao", "authors": "Changzhe Jiao, Chao Chen, Ronald G. McGarvey, Stephanie Bohlman,\n  Licheng Jiao, Alina Zare", "title": "Multiple Instance Hybrid Estimator for Hyperspectral Target\n  Characterization and Sub-pixel Target Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiple Instance Hybrid Estimator for discriminative target\ncharacterization from imprecisely labeled hyperspectral data is presented. In\nmany hyperspectral target detection problems, acquiring accurately labeled\ntraining data is difficult. Furthermore, each pixel containing target is likely\nto be a mixture of both target and non-target signatures (i.e., sub-pixel\ntargets), making extracting a pure prototype signature for the target class\nfrom the data extremely difficult. The proposed approach addresses these\nproblems by introducing a data mixing model and optimizing the response of the\nhybrid sub-pixel detector within a multiple instance learning framework. The\nproposed approach iterates between estimating a set of discriminative target\nand non-target signatures and solving a sparse unmixing problem. After learning\ntarget signatures, a signature based detector can then be applied on test data.\nBoth simulated and real hyperspectral target detection experiments show the\nproposed algorithm is effective at learning discriminative target signatures\nand achieves superior performance over state-of-the-art comparison algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:19:57 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 03:38:53 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Jiao", "Changzhe", ""], ["Chen", "Chao", ""], ["McGarvey", "Ronald G.", ""], ["Bohlman", "Stephanie", ""], ["Jiao", "Licheng", ""], ["Zare", "Alina", ""]]}, {"id": "1710.11601", "submitter": "Lea Frermann", "authors": "Lea Frermann and Shay B. Cohen and Mirella Lapata", "title": "Whodunnit? Crime Drama as a Case for Natural Language Understanding", "comments": "To appear in Transactions of the Association for Computational\n  Linguistics (TACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we argue that crime drama exemplified in television programs\nsuch as CSI:Crime Scene Investigation is an ideal testbed for approximating\nreal-world natural language understanding and the complex inferences associated\nwith it. We propose to treat crime drama as a new inference task, capitalizing\non the fact that each episode poses the same basic question (i.e., who\ncommitted the crime) and naturally provides the answer when the perpetrator is\nrevealed. We develop a new dataset based on CSI episodes, formalize perpetrator\nidentification as a sequence labeling problem, and develop an LSTM-based model\nwhich learns from multi-modal data. Experimental results show that an\nincremental inference strategy is key to making accurate guesses as well as\nlearning from representations fusing textual, visual, and acoustic input.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 17:27:44 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Frermann", "Lea", ""], ["Cohen", "Shay B.", ""], ["Lapata", "Mirella", ""]]}]