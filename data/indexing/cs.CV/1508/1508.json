[{"id": "1508.00092", "submitter": "Giovanni Poggi", "authors": "Marco Castelluccio, Giovanni Poggi, Carlo Sansone, Luisa Verdoliva", "title": "Land Use Classification in Remote Sensing Images by Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of convolutional neural networks for the semantic\nclassification of remote sensing scenes. Two recently proposed architectures,\nCaffeNet and GoogLeNet, are adopted, with three different learning modalities.\nBesides conventional training from scratch, we resort to pre-trained networks\nthat are only fine-tuned on the target data, so as to avoid overfitting\nproblems and reduce design time. Experiments on two remote sensing datasets,\nwith markedly different characteristics, testify on the effectiveness and wide\napplicability of the proposed solution, which guarantees a significant\nperformance improvement over all state-of-the-art references.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 07:15:19 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Castelluccio", "Marco", ""], ["Poggi", "Giovanni", ""], ["Sansone", "Carlo", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "1508.00102", "submitter": "Axel Angel Engineer", "authors": "Axel Angel", "title": "Towards Distortion-Predictable Embedding of Neural Networks", "comments": "54 pages, 28 figures. Master project at EPFL (Switzerland) in 2015.\n  For source code on GitHub, see https://github.com/axel-angel/master-project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in Computer Vision has shown that Convolutional Neural\nNetworks (CNN) give state-of-the-art performance in many classification tasks\nand Computer Vision problems. The embedding of CNN, which is the internal\nrepresentation produced by the last layer, can indirectly learn topological and\nrelational properties. Moreover, by using a suitable loss function, CNN models\ncan learn invariance to a wide range of non-linear distortions such as\nrotation, viewpoint angle or lighting condition. In this work, new insights are\ndiscovered about CNN embeddings and a new loss function is proposed, derived\nfrom the contrastive loss, that creates models with more predicable mappings\nand also quantifies distortions. In typical distortion-dependent methods, there\nis no simple relation between the features corresponding to one image and the\nfeatures of this image distorted. Therefore, these methods require to\nfeed-forward inputs under every distortions in order to find the corresponding\nfeatures representations. Our contribution makes a step towards embeddings\nwhere features of distorted inputs are related and can be derived from each\nothers by the intensity of the distortion.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2015 09:37:30 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Angel", "Axel", ""]]}, {"id": "1508.00217", "submitter": "Ruoyu Liu", "authors": "Ruoyu Liu, Yao Zhao, Shikui Wei, Yi Yang", "title": "Indexing of CNN Features for Large Scale Image Search", "comments": "21 pages, 9 figures, submitted to Multimedia Tools and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) features can give a good description\nof image content, which usually represent images with unique global vectors.\nAlthough they are compact compared to local descriptors, they still cannot\nefficiently deal with large-scale image retrieval due to the cost of the linear\nincremental computation and storage. To address this issue, we build a simple\nbut effective indexing framework based on inverted table, which significantly\ndecreases both the search time and memory usage. In addition, several\nstrategies are fully investigated under an indexing framework to adapt it to\nCNN features and compensate for quantization errors. First, we use multiple\nassignment for the query and database images to increase the probability of\nrelevant images' co-existing in the same Voronoi cells obtained via the\nclustering algorithm. Then, we introduce embedding codes to further improve\nprecision by removing false matches during a search. We demonstrate that by\nusing hashing schemes to calculate the embedding codes and by changing the\nranking rule, indexing framework speeds can be greatly improved. Extensive\nexperiments conducted on several unsupervised and supervised benchmarks support\nthese results and the superiority of the proposed indexing framework. We also\nprovide a fair comparison between the popular CNN features.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 10:43:25 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 01:51:27 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 14:25:30 GMT"}, {"version": "v4", "created": "Thu, 1 Feb 2018 09:39:54 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Liu", "Ruoyu", ""], ["Zhao", "Yao", ""], ["Wei", "Shikui", ""], ["Yang", "Yi", ""]]}, {"id": "1508.00239", "submitter": "Dongmei Liang", "authors": "Dongmei Liang and Wushan Cheng", "title": "Partial matching face recognition method for rehabilitation nursing\n  robots beds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to establish face recognition system in rehabilitation nursing\nrobots beds and achieve real-time monitor the patient on the bed. We propose a\nface recognition method based on partial matching Hu moments which apply for\nrehabilitation nursing robots beds. Firstly we using Haar classifier to detect\nhuman faces automatically in dynamic video frames. Secondly we using Otsu\nthreshold method to extract facial features (eyebrows, eyes, mouth) in the face\nimage and its Hu moments. Finally, we using Hu moment feature set to achieve\nthe automatic face recognition. Experimental results show that this method can\nefficiently identify face in a dynamic video and it has high practical value\n(the accuracy rate is 91% and the average recognition time is 4.3s).\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 14:09:02 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Liang", "Dongmei", ""], ["Cheng", "Wushan", ""]]}, {"id": "1508.00271", "submitter": "Katerina Fragkiadaki", "authors": "Katerina Fragkiadaki, Sergey Levine, Panna Felsen, Jitendra Malik", "title": "Recurrent Network Models for Human Dynamics", "comments": "International Conference on Computer Vision 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Encoder-Recurrent-Decoder (ERD) model for recognition and\nprediction of human body pose in videos and motion capture. The ERD model is a\nrecurrent neural network that incorporates nonlinear encoder and decoder\nnetworks before and after recurrent layers. We test instantiations of ERD\narchitectures in the tasks of motion capture (mocap) generation, body pose\nlabeling and body pose forecasting in videos. Our model handles mocap training\ndata across multiple subjects and activity domains, and synthesizes novel\nmotions while avoid drifting for long periods of time. For human pose labeling,\nERD outperforms a per frame body part detector by resolving left-right body\npart confusions. For video pose forecasting, ERD predicts body joint\ndisplacements across a temporal horizon of 400ms and outperforms a first order\nmotion model based on optical flow. ERDs extend previous Long Short Term Memory\n(LSTM) models in the literature to jointly learn representations and their\ndynamics. Our experiments show such representation learning is crucial for both\nlabeling and prediction in space-time. We find this is a distinguishing feature\nbetween the spatio-temporal visual domain in comparison to 1D text, speech or\nhandwriting, where straightforward hard coded representations have shown\nexcellent results when directly combined with recurrent units.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 18:59:52 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2015 01:28:23 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Fragkiadaki", "Katerina", ""], ["Levine", "Sergey", ""], ["Felsen", "Panna", ""], ["Malik", "Jitendra", ""]]}, {"id": "1508.00278", "submitter": "Mohammad Aghagolzadeh", "authors": "Mohammad Aghagolzadeh and Hayder Radha", "title": "Dictionary and Image Recovery from Incomplete and Random Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles algorithmic and theoretical aspects of dictionary learning\nfrom incomplete and random block-wise image measurements and the performance of\nthe adaptive dictionary for sparse image recovery. This problem is related to\nblind compressed sensing in which the sparsifying dictionary or basis is viewed\nas an unknown variable and subject to estimation during sparse recovery.\nHowever, unlike existing guarantees for a successful blind compressed sensing,\nour results do not rely on additional structural constraints on the learned\ndictionary or the measured signal. In particular, we rely on the spatial\ndiversity of compressive measurements to guarantee that the solution is unique\nwith a high probability. Moreover, our distinguishing goal is to measure and\nreduce the estimation error with respect to the ideal dictionary that is based\non the complete image. Using recent results from random matrix theory, we show\nthat applying a slightly modified dictionary learning algorithm over\ncompressive measurements results in accurate estimation of the ideal dictionary\nfor large-scale images. Empirically, we experiment with both space-invariant\nand space-varying sensing matrices and demonstrate the critical role of spatial\ndiversity in measurements. Simulation results confirm that the presented\nalgorithm outperforms the typical non-adaptive sparse recovery based on\noffline-learned universal dictionaries.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 19:55:27 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Aghagolzadeh", "Mohammad", ""], ["Radha", "Hayder", ""]]}, {"id": "1508.00282", "submitter": "Mohammad Aghagolzadeh", "authors": "Mohammad Aghagolzadeh and Hayder Radha", "title": "On Hyperspectral Classification in the Compressed Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of hyperspectral pixel classification\nbased on the recently proposed architectures for compressive whisk-broom\nhyperspectral imagers without the need to reconstruct the complete data cube. A\nclear advantage of classification in the compressed domain is its suitability\nfor real-time on-site processing of the sensed data. Moreover, it is assumed\nthat the training process also takes place in the compressed domain, thus,\nisolating the classification unit from the recovery unit at the receiver's\nside. We show that, perhaps surprisingly, using distinct measurement matrices\nfor different pixels results in more accuracy of the learned classifier and\nconsistent classification performance, supporting the role of information\ndiversity in learning.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2015 20:40:21 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Aghagolzadeh", "Mohammad", ""], ["Radha", "Hayder", ""]]}, {"id": "1508.00307", "submitter": "Weilin Huang", "authors": "Sheng Guo and Weilin Huang and Yu Qiao", "title": "Local Color Contrastive Descriptor for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image representation and classification are two fundamental tasks towards\nmultimedia content retrieval and understanding. The idea that shape and texture\ninformation (e.g. edge or orientation) are the key features for visual\nrepresentation is ingrained and dominated in current multimedia and computer\nvision communities. A number of low-level features have been proposed by\ncomputing local gradients (e.g. SIFT, LBP and HOG), and have achieved great\nsuccesses on numerous multimedia applications. In this paper, we present a\nsimple yet efficient local descriptor for image classification, referred as\nLocal Color Contrastive Descriptor (LCCD), by leveraging the neural mechanisms\nof color contrast. The idea originates from the observation in neural science\nthat color and shape information are linked inextricably in visual cortical\nprocessing. The color contrast yields key information for visual color\nperception and provides strong linkage between color and shape. We propose a\nnovel contrastive mechanism to compute the color contrast in both spatial\nlocation and multiple channels. The color contrast is computed by measuring\n\\emph{f}-divergence between the color distributions of two regions. Our\ndescriptor enriches local image representation with both color and contrast\ninformation. We verified experimentally that it can compensate strongly for the\nshape based descriptor (e.g. SIFT), while keeping computationally simple.\nExtensive experimental results on image classification show that our descriptor\nimproves the performance of SIFT substantially by combinations, and achieves\nthe state-of-the-art performance on three challenging benchmark datasets. It\nimproves recent Deep Learning model (DeCAF) [1] largely from the accuracy of\n40.94% to 49.68% in the large scale SUN397 database. Codes for the LCCD will be\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 03:29:50 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""]]}, {"id": "1508.00330", "submitter": "Zhibin Liao", "authors": "Zhibin Liao, Gustavo Carneiro", "title": "On the Importance of Normalisation Layers in Deep Learning with\n  Piecewise Linear Activation Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep feedforward neural networks with piecewise linear activations are\ncurrently producing the state-of-the-art results in several public datasets.\nThe combination of deep learning models and piecewise linear activation\nfunctions allows for the estimation of exponentially complex functions with the\nuse of a large number of subnetworks specialized in the classification of\nsimilar input examples. During the training process, these subnetworks avoid\noverfitting with an implicit regularization scheme based on the fact that they\nmust share their parameters with other subnetworks. Using this framework, we\nhave made an empirical observation that can improve even more the performance\nof such models. We notice that these models assume a balanced initial\ndistribution of data points with respect to the domain of the piecewise linear\nactivation function. If that assumption is violated, then the piecewise linear\nactivation units can degenerate into purely linear activation units, which can\nresult in a significant reduction of their capacity to learn complex functions.\nFurthermore, as the number of model layers increases, this unbalanced initial\ndistribution makes the model ill-conditioned. Therefore, we propose the\nintroduction of batch normalisation units into deep feedforward neural networks\nwith piecewise linear activations, which drives a more balanced use of these\nactivation units, where each region of the activation function is trained with\na relatively large proportion of training samples. Also, this batch\nnormalisation promotes the pre-conditioning of very deep learning models. We\nshow that by introducing maxout and batch normalisation units to the network in\nnetwork model results in a model that produces classification results that are\nbetter than or comparable to the current state of the art in CIFAR-10,\nCIFAR-100, MNIST, and SVHN datasets.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 07:24:07 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 06:44:10 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Liao", "Zhibin", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1508.00413", "submitter": "Tingshao Zhu", "authors": "Liqing Cui, Shun Li, Wan Zhang, Zhan Zhang, Tingshao Zhu", "title": "Identifying Emotion from Natural Walking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion identification from gait aims to automatically determine persons\naffective state, it has attracted a great deal of interests and offered immense\npotential value in action tendency, health care, psychological detection and\nhuman-computer(robot) interaction.In this paper, we propose a new method of\nidentifying emotion from natural walking, and analyze the relevance between the\ntraits of walking and affective states. After obtaining the pure acceleration\ndata of wrist and ankle, we set a moving average filter window with different\nsizes w, then extract 114 features including time-domain, frequency-domain,\npower and distribution features from each data slice, and run principal\ncomponent analysis (PCA) to reduce dimension. In experiments, we train SVM,\nDecision Tree, multilayerperception, Random Tree and Random Forest\nclassification models, and compare the classification accuracy on data of wrist\nand ankle with respect to different w. The performance of emotion\nidentification on acceleration data of ankle is better than wrist.Comparing\ndifferent classification models' results, SVM has best accuracy of identifying\nanger and happy could achieve 90:31% and 89:76% respectively, and\nidentification ratio of anger-happy is 87:10%.The anger-neutral-happy\nclassification reaches 85%-78%-78%.The results show that it is capable of\nidentifying personal emotional states through the gait of walking.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 13:48:46 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 06:38:36 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Cui", "Liqing", ""], ["Li", "Shun", ""], ["Zhang", "Wan", ""], ["Zhang", "Zhan", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1508.00430", "submitter": "Mengyang Yu", "authors": "Mengyang Yu, Li Liu, Ling Shao", "title": "Kernelized Multiview Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional vision algorithms adopt a single type of feature or a simple\nconcatenation of multiple features, which is always represented in a\nhigh-dimensional space. In this paper, we propose a novel unsupervised spectral\nembedding algorithm called Kernelized Multiview Projection (KMP) to better fuse\nand embed different feature representations. Computing the kernel matrices from\ndifferent features/views, KMP can encode them with the corresponding weights to\nachieve a low-dimensional and semantically meaningful subspace where the\ndistribution of each view is sufficiently smooth and discriminative. More\ncrucially, KMP is linear for the reproducing kernel Hilbert space (RKHS) and\nsolves the out-of-sample problem, which allows it to be competent for various\npractical applications. Extensive experiments on three popular image datasets\ndemonstrate the effectiveness of our multiview embedding algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 14:33:03 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 09:42:14 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Yu", "Mengyang", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""]]}, {"id": "1508.00451", "submitter": "Rein Houthooft", "authors": "Rein Houthooft, Filip De Turck", "title": "Integrated Inference and Learning of Neural Factors in Structural\n  Support Vector Machines", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.03.014", "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling pattern recognition problems in areas such as computer vision,\nbioinformatics, speech or text recognition is often done best by taking into\naccount task-specific statistical relations between output variables. In\nstructured prediction, this internal structure is used to predict multiple\noutputs simultaneously, leading to more accurate and coherent predictions.\nStructural support vector machines (SSVMs) are nonprobabilistic models that\noptimize a joint input-output function through margin-based learning. Because\nSSVMs generally disregard the interplay between unary and interaction factors\nduring the training phase, final parameters are suboptimal. Moreover, its\nfactors are often restricted to linear combinations of input features, limiting\nits generalization power. To improve prediction accuracy, this paper proposes:\n(i) Joint inference and learning by integration of back-propagation and\nloss-augmented inference in SSVM subgradient descent; (ii) Extending SSVM\nfactors to neural networks that form highly nonlinear functions of input\nfeatures. Image segmentation benchmark results demonstrate improvements over\nconventional SSVM training methods in terms of accuracy, highlighting the\nfeasibility of end-to-end SSVM training with neural factors.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 15:29:57 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2015 12:41:52 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2015 16:02:00 GMT"}, {"version": "v4", "created": "Thu, 3 Mar 2016 22:46:17 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Houthooft", "Rein", ""], ["De Turck", "Filip", ""]]}, {"id": "1508.00537", "submitter": "Rodrigo Nogueira", "authors": "Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, Rubens Campos\n  Machado", "title": "Evaluating software-based fingerprint liveness detection using\n  Convolutional Networks and Local Binary Patterns", "comments": "arXiv admin note: text overlap with arXiv:1301.3557 by other authors", "journal-ref": "Biometric Measurements and Systems for Security and Medical\n  Applications (BIOMS) Proceedings, 2014 IEEE Workshop on", "doi": "10.1109/BIOMS.2014.6951531", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing use of biometric authentication systems in the past years,\nspoof fingerprint detection has become increasingly important. In this work, we\nimplement and evaluate two different feature extraction techniques for\nsoftware-based fingerprint liveness detection: Convolutional Networks with\nrandom weights and Local Binary Patterns. Both techniques were used in\nconjunction with a Support Vector Machine (SVM) classifier. Dataset\nAugmentation was used to increase classifier's performance and a variety of\npreprocessing operations were tested, such as frequency filtering, contrast\nequalization, and region of interest filtering. The experiments were made on\nthe datasets used in The Liveness Detection Competition of years 2009, 2011 and\n2013, which comprise almost 50,000 real and fake fingerprints' images. Our best\nmethod achieves an overall rate of 95.2% of correctly classified samples - an\nimprovement of 35% in test error when compared with the best previously\npublished results.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 19:21:03 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Nogueira", "Rodrigo Frassetto", ""], ["Lotufo", "Roberto de Alencar", ""], ["Machado", "Rubens Campos", ""]]}, {"id": "1508.00761", "submitter": "Tingshao Zhu", "authors": "Shun Li, Changye Zhu, Liqing Cui, Nan Zhao, Baobin Li and Tingshao Zhu", "title": "Recognition of Emotions using Kinects", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological studies indicate that emotional states are expressed in the way\npeople walk and the human gait is investigated in terms of its ability to\nreveal a person's emotional state. And Microsoft Kinect is a rapidly\ndeveloping, inexpensive, portable and no-marker motion capture system. This\npaper gives a new referable method to do emotion recognition, by using\nMicrosoft Kinect to do gait pattern analysis, which has not been reported. $59$\nsubjects are recruited in this study and their gait patterns are record by two\nKinect cameras. Significant joints selecting, Coordinate system transforming,\nSlider window gauss filter, Differential operation, and Data segmentation are\nused in data preprocessing. Feature extracting is based on Fourier\ntransformation. By using the NaiveBayes, RandomForests, libSVM and SMO\nclassification, the recognition rate of natural and unnatural emotions can\nreach above 70%.It is concluded that using the Kinect system can be a new\nmethod in recognition of emotions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 13:03:27 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Li", "Shun", ""], ["Zhu", "Changye", ""], ["Cui", "Liqing", ""], ["Zhao", "Nan", ""], ["Li", "Baobin", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1508.00776", "submitter": "Eleonora Vig", "authors": "Adrien Gaidon and Eleonora Vig", "title": "Online Domain Adaptation for Multi-Object Tracking", "comments": "To appear at BMVC 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically detecting, labeling, and tracking objects in videos depends\nfirst and foremost on accurate category-level object detectors. These might,\nhowever, not always be available in practice, as acquiring high-quality large\nscale labeled training datasets is either too costly or impractical for all\npossible real-world application scenarios. A scalable solution consists in\nre-using object detectors pre-trained on generic datasets. This work is the\nfirst to investigate the problem of on-line domain adaptation of object\ndetectors for causal multi-object tracking (MOT). We propose to alleviate the\ndataset bias by adapting detectors from category to instances, and back: (i) we\njointly learn all target models by adapting them from the pre-trained one, and\n(ii) we also adapt the pre-trained model on-line. We introduce an on-line\nmulti-task learning algorithm to efficiently share parameters and reduce drift,\nwhile gradually improving recall. Our approach is applicable to any linear\nobject detector, and we evaluate both cheap \"mini-Fisher Vectors\" and expensive\n\"off-the-shelf\" ConvNet features. We quantitatively measure the benefit of our\ndomain adaptation strategy on the KITTI tracking benchmark and on a new dataset\n(PASCAL-to-KITTI) we introduce to study the domain mismatch problem in MOT.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 14:01:55 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Gaidon", "Adrien", ""], ["Vig", "Eleonora", ""]]}, {"id": "1508.00835", "submitter": "Jeremie Papon", "authors": "Jeremie Papon, Markus Schoeler", "title": "Semantic Pose using Deep Networks Trained on Synthetic RGB-D", "comments": "ICCV 2015 Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we address the problem of indoor scene understanding from RGB-D\nimages. Specifically, we propose to find instances of common furniture classes,\ntheir spatial extent, and their pose with respect to generalized class models.\nTo accomplish this, we use a deep, wide, multi-output convolutional neural\nnetwork (CNN) that predicts class, pose, and location of possible objects\nsimultaneously. To overcome the lack of large annotated RGB-D training sets\n(especially those with pose), we use an on-the-fly rendering pipeline that\ngenerates realistic cluttered room scenes in parallel to training. We then\nperform transfer learning on the relatively small amount of publicly available\nannotated RGB-D data, and find that our model is able to successfully annotate\neven highly challenging real scenes. Importantly, our trained network is able\nto understand noisy and sparse observations of highly cluttered scenes with a\nremarkable degree of accuracy, inferring class and pose from a very limited set\nof cues. Additionally, our neural network is only moderately deep and computes\nclass, pose and position in tandem, so the overall run-time is significantly\nfaster than existing methods, estimating all output parameters simultaneously\nin parallel on a GPU in seconds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 17:07:51 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Papon", "Jeremie", ""], ["Schoeler", "Markus", ""]]}, {"id": "1508.00966", "submitter": "Yankui Sun", "authors": "Yankui Sun, Tian Zhang, Yue Zhao, Yufan He", "title": "3D Automatic Segmentation Method for Retinal Optical Coherence\n  Tomography Volume Data Using Boundary Surface Enhancement", "comments": "27 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the introduction of spectral-domain optical coherence tomography\n(SDOCT), much larger image datasets are routinely acquired compared to what was\npossible using the previous generation of time-domain OCT. Thus, there is a\ncritical need for the development of 3D segmentation methods for processing\nthese data. We present here a novel 3D automatic segmentation method for\nretinal OCT volume data. Briefly, to segment a boundary surface, two OCT volume\ndatasets are obtained by using a 3D smoothing filter and a 3D differential\nfilter. Their linear combination is then calculated to generate new volume data\nwith an enhanced boundary surface, where pixel intensity, boundary position\ninformation, and intensity changes on both sides of the boundary surface are\nused simultaneously. Next, preliminary discrete boundary points are detected\nfrom the A-Scans of the volume data. Finally, surface smoothness constraints\nand a dynamic threshold are applied to obtain a smoothed boundary surface by\ncorrecting a small number of error points. Our method can extract retinal layer\nboundary surfaces sequentially with a decreasing search region of volume data.\nWe performed automatic segmentation on eight human OCT volume datasets acquired\nfrom a commercial Spectralis OCT system, where each volume of data consisted of\n97 OCT images with a resolution of 496 512; experimental results show that this\nmethod can accurately segment seven layer boundary surfaces in normal as well\nas some abnormal eyes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 03:42:54 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Sun", "Yankui", ""], ["Zhang", "Tian", ""], ["Zhao", "Yue", ""], ["He", "Yufan", ""]]}, {"id": "1508.00998", "submitter": "Simone Bianco", "authors": "Simone Bianco, Claudio Cusano, Raimondo Schettini", "title": "Single and Multiple Illuminant Estimation Using Convolutional Neural\n  Networks", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for the estimation of the color of the\nilluminant in RAW images. The method includes a Convolutional Neural Network\nthat has been specially designed to produce multiple local estimates. A\nmultiple illuminant detector determines whether or not the local outputs of the\nnetwork must be aggregated into a single estimate. We evaluated our method on\nstandard datasets with single and multiple illuminants, obtaining lower\nestimation errors with respect to those obtained by other general purpose\nmethods in the state of the art.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 08:25:27 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 14:35:20 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Bianco", "Simone", ""], ["Cusano", "Claudio", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1508.01055", "submitter": "Roman Fedorov", "authors": "Roman Fedorov, Alessandro Camerada, Piero Fraternali, Marco\n  Tagliasacchi", "title": "Estimating snow cover from publicly available images", "comments": "submitted to IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2016.2535356", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of estimating snow cover in mountainous\nregions, that is, the spatial extent of the earth surface covered by snow. We\nargue that publicly available visual content, in the form of user generated\nphotographs and image feeds from outdoor webcams, can both be leveraged as\nadditional measurement sources, complementing existing ground, satellite and\nairborne sensor data. To this end, we describe two content acquisition and\nprocessing pipelines that are tailored to such sources, addressing the specific\nchallenges posed by each of them, e.g., identifying the mountain peaks,\nfiltering out images taken in bad weather conditions, handling varying\nillumination conditions. The final outcome is summarized in a snow cover index,\nwhich indicates for a specific mountain and day of the year, the fraction of\nvisible area covered by snow, possibly at different elevations. We created a\nmanually labelled dataset to assess the accuracy of the image snow covered area\nestimation, achieving 90.0% precision at 91.1% recall. In addition, we show\nthat seasonal trends related to air temperature are captured by the snow cover\nindex.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 12:46:26 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Fedorov", "Roman", ""], ["Camerada", "Alessandro", ""], ["Fraternali", "Piero", ""], ["Tagliasacchi", "Marco", ""]]}, {"id": "1508.01057", "submitter": "Spyridoula Xenaki", "authors": "Spyridoula D. Xenaki and Konstantinos D. Koutroumbas and Athanasios A.\n  Rontogiannis", "title": "On the convergence of the sparse possibilistic c-means algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a convergence proof for the recently proposed sparse\npossibilistic c-means (SPCM) algorithm is provided, utilizing the celebrated\nZangwill convergence theorem. It is shown that the iterative sequence generated\nby SPCM converges to a stationary point or there exists a subsequence of it\nthat converges to a stationary point of the cost function of the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 13:02:48 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 08:19:29 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Xenaki", "Spyridoula D.", ""], ["Koutroumbas", "Konstantinos D.", ""], ["Rontogiannis", "Athanasios A.", ""]]}, {"id": "1508.01081", "submitter": "Paolo Nesi", "authors": "Paolo Nesi, Gianni Pantaleo", "title": "Detection of Critical Number of People in Interlocked Doors for Security\n  Access Control by Exploiting a Microwave Transceiver-Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Counting the number of people is something many security application focus\non, when dealing with controlling accesses in restricted areas, as it occurs\nwith banks, airports, railway stations and governmental offices. This paper\npresents an automated solution for detecting the presence of more than one\nperson into interlocked doors adopted in many accesses. In most cases,\ninterlocked doors are small areas where other pieces of information and sensors\nare placed in order to detect the presence of guns, explosive, etc. The general\ngoals and the required environmental condition, allowed us to implement a\ndetection system at lower costs and complexity, with respect to other existing\ntechniques. The system consists of a fixed array of microwave transceiver\nmodules, whose received signals are processed to collect information related to\na sort of volume occupied in the interlocked door cabin. The proposed solution\nhas been statistically validated by using statistical analysis. The whole\nsolution has been also implemented to be used in a real time environment and\nthus validated against real experimental measures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 14:09:23 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Nesi", "Paolo", ""], ["Pantaleo", "Gianni", ""]]}, {"id": "1508.01108", "submitter": "Claudio Cusano", "authors": "Claudio Cusano, Paolo Napoletano, Raimondo Schettini", "title": "Evaluating color texture descriptors under large variations of\n  controlled lighting conditions", "comments": "Submitted to the Journal of the Optical Society of America A", "journal-ref": null, "doi": "10.1364/JOSAA.33.000017", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of color texture under varying lighting conditions is still\nan open issue. Several features have been proposed for this purpose, ranging\nfrom traditional statistical descriptors to features extracted with neural\nnetworks. Still, it is not completely clear under what circumstances a feature\nperforms better than the others. In this paper we report an extensive\ncomparison of old and new texture features, with and without a color\nnormalization step, with a particular focus on how they are affected by small\nand large variation in the lighting conditions. The evaluation is performed on\na new texture database including 68 samples of raw food acquired under 46\nconditions that present single and combined variations of light color,\ndirection and intensity. The database allows to systematically investigate the\nrobustness of texture descriptors across a large range of variations of imaging\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 15:40:21 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Cusano", "Claudio", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1508.01128", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Juan J. Cerrolaza, Robert A. Avery, Marius G. Linguraru", "title": "Partitioned Shape Modeling with On-the-Fly Sparse Appearance Learning\n  for Anterior Visual Pathway Segmentation", "comments": "8 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MRI quantification of cranial nerves such as anterior visual pathway (AVP) in\nMRI is challenging due to their thin small size, structural variation along its\npath, and adjacent anatomic structures. Segmentation of pathologically abnormal\noptic nerve (e.g. optic nerve glioma) poses additional challenges due to\nchanges in its shape at unpredictable locations. In this work, we propose a\npartitioned joint statistical shape model approach with sparse appearance\nlearning for the segmentation of healthy and pathological AVP. Our main\ncontributions are: (1) optimally partitioned statistical shape models for the\nAVP based on regional shape variations for greater local flexibility of\nstatistical shape model; (2) refinement model to accommodate pathological\nregions as well as areas of subtle variation by training the model on-the-fly\nusing the initial segmentation obtained in (1); (3) hierarchical deformable\nframework to incorporate scale information in partitioned shape and appearance\nmodels. Our method, entitled PAScAL (PArtitioned Shape and Appearance\nLearning), was evaluated on 21 MRI scans (15 healthy + 6 glioma cases) from\npediatric patients (ages 2-17). The experimental results show that the proposed\nlocalized shape and sparse appearance-based learning approach significantly\noutperforms segmentation approaches in the analysis of pathological data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 17:00:24 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Mansoor", "Awais", ""], ["Cerrolaza", "Juan J.", ""], ["Avery", "Robert A.", ""], ["Linguraru", "Marius G.", ""]]}, {"id": "1508.01158", "submitter": "Francesco Solera", "authors": "Francesco Solera, Simone Calderara and Rita Cucchiara", "title": "Socially Constrained Structural Learning for Groups Detection in Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern crowd theories agree that collective behavior is the result of the\nunderlying interactions among small groups of individuals. In this work, we\npropose a novel algorithm for detecting social groups in crowds by means of a\nCorrelation Clustering procedure on people trajectories. The affinity between\ncrowd members is learned through an online formulation of the Structural SVM\nframework and a set of specifically designed features characterizing both their\nphysical and social identity, inspired by Proxemic theory, Granger causality,\nDTW and Heat-maps. To adhere to sociological observations, we introduce a loss\nfunction (G-MITRE) able to deal with the complexity of evaluating group\ndetection performances. We show our algorithm achieves state-of-the-art results\nwhen relying on both ground truth trajectories and tracklets previously\nextracted by available detector/tracker systems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 18:31:42 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 17:08:31 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Solera", "Francesco", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1508.01176", "submitter": "Garrick Orchard", "authors": "Garrick Orchard and Cedric Meyer and Ralph Etienne-Cummings and\n  Christoph Posch and Nitish Thakor and Ryad Benosman", "title": "HFirst: A Temporal Approach to Object Recognition", "comments": "13 pages, 10 figures", "journal-ref": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,\n  vol.37, no.10, pp.2028-2040, Oct 2015", "doi": "10.1109/TPAMI.2015.2392947", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a spiking hierarchical model for object recognition\nwhich utilizes the precise timing information inherently present in the output\nof biologically inspired asynchronous Address Event Representation (AER) vision\nsensors. The asynchronous nature of these systems frees computation and\ncommunication from the rigid predetermined timing enforced by system clocks in\nconventional systems. Freedom from rigid timing constraints opens the\npossibility of using true timing to our advantage in computation. We show not\nonly how timing can be used in object recognition, but also how it can in fact\nsimplify computation. Specifically, we rely on a simple\ntemporal-winner-take-all rather than more computationally intensive synchronous\noperations typically used in biologically inspired neural networks for object\nrecognition. This approach to visual computation represents a major paradigm\nshift from conventional clocked systems and can find application in other\nsensory modalities and computational tasks. We showcase effectiveness of the\napproach by achieving the highest reported accuracy to date (97.5\\%$\\pm$3.5\\%)\nfor a previously published four class card pip recognition task and an accuracy\nof 84.9\\%$\\pm$1.9\\% for a new more difficult 36 class character recognition\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 19:03:32 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Orchard", "Garrick", ""], ["Meyer", "Cedric", ""], ["Etienne-Cummings", "Ralph", ""], ["Posch", "Christoph", ""], ["Thakor", "Nitish", ""], ["Benosman", "Ryad", ""]]}, {"id": "1508.01244", "submitter": "Qiong Huang", "authors": "Qiong Huang, Ashok Veeraraghavan and Ashutosh Sabharwal", "title": "TabletGaze: Unconstrained Appearance-based Gaze Estimation in Mobile\n  Tablets", "comments": "18 pages, 17 figures, submitted to journal, website hosting the\n  dataset: http://sh.rice.edu/tablet_gaze.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study gaze estimation on tablets, our key design goal is uncalibrated gaze\nestimation using the front-facing camera during natural use of tablets, where\nthe posture and method of holding the tablet is not constrained. We collected\nthe first large unconstrained gaze dataset of tablet users, labeled Rice\nTabletGaze dataset. The dataset consists of 51 subjects, each with 4 different\npostures and 35 gaze locations. Subjects vary in race, gender and in their need\nfor prescription glasses, all of which might impact gaze estimation accuracy.\nDriven by our observations on the collected data, we present a TabletGaze\nalgorithm for automatic gaze estimation using multi-level HoG feature and\nRandom Forests regressor. The TabletGaze algorithm achieves a mean error of\n3.17 cm. We perform extensive evaluation on the impact of various factors such\nas dataset size, race, wearing glasses and user posture on the gaze estimation\naccuracy and make important observations about the impact of these factors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2015 22:38:53 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 16:44:14 GMT"}, {"version": "v3", "created": "Sat, 16 Jul 2016 09:06:23 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Huang", "Qiong", ""], ["Veeraraghavan", "Ashok", ""], ["Sabharwal", "Ashutosh", ""]]}, {"id": "1508.01292", "submitter": "Ilya Kalinowski", "authors": "Ilya Kalinovskii, Vladimir Spitsyn", "title": "Compact Convolutional Neural Network Cascade for Face Detection", "comments": "Demo video and test results available at\n  http://github.com/Bkmz21/FD-Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of faces detection in images or video streams is a classical\nproblem of computer vision. The multiple solutions of this problem have been\nproposed, but the question of their optimality is still open. Many algorithms\nachieve a high quality face detection, but at the cost of high computational\ncomplexity. This restricts their application in the real-time systems. This\npaper presents a new solution of the frontal face detection problem based on\ncompact convolutional neural networks cascade. The test results on FDDB dataset\nshow that it is competitive with state-of-the-art algorithms. This proposed\ndetector is implemented using three technologies: SSE/AVX/AVX2 instruction sets\nfor Intel CPUs, Nvidia CUDA, OpenCL. The detection speed of our approach\nconsiderably exceeds all the existing CPU-based and GPU-based algorithms.\nBecause of high computational efficiency, our detector can processing 4K Ultra\nHD video stream in real time (up to 27 fps) on mobile platforms (Intel Ivy\nBridge CPUs and Nvidia Kepler GPUs) in searching objects with the dimension\n60x60 pixels or higher. At the same time its performance weakly dependent on\nthe background and number of objects in scene. This is achieved by the\nasynchronous computation of stages in the cascade.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 07:01:55 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 09:10:27 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 20:01:06 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Kalinovskii", "Ilya", ""], ["Spitsyn", "Vladimir", ""]]}, {"id": "1508.01308", "submitter": "Michael Moeller", "authors": "Joan Duran, Michael Moeller, Catalina Sbert and Daniel Cremers", "title": "Collaborative Total Variation: A General Framework for Vectorial TV\n  Models", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences, vol. 9(1), pp. 116-151, 2016", "doi": "10.1137/15M102873X", "report-no": null, "categories": "cs.CV math.HO math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even after over two decades, the total variation (TV) remains one of the most\npopular regularizations for image processing problems and has sparked a\ntremendous amount of research, particularly to move from scalar to\nvector-valued functions. In this paper, we consider the gradient of a color\nimage as a three dimensional matrix or tensor with dimensions corresponding to\nthe spatial extend, the differences to other pixels, and the spectral channels.\nThe smoothness of this tensor is then measured by taking different norms along\nthe different dimensions. Depending on the type of these norms one obtains very\ndifferent properties of the regularization, leading to novel models for color\nimages. We call this class of regularizations collaborative total variation\n(CTV). On the theoretical side, we characterize the dual norm, the\nsubdifferential and the proximal mapping of the proposed regularizers. We\nfurther prove, with the help of the generalized concept of singular vectors,\nthat an $\\ell^{\\infty}$ channel coupling makes the most prior assumptions and\nhas the greatest potential to reduce color artifacts. Our practical\ncontributions consist of an extensive experimental section where we compare the\nperformance of a large number of collaborative TV methods for inverse problems\nlike denoising, deblurring and inpainting.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 08:02:56 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Duran", "Joan", ""], ["Moeller", "Michael", ""], ["Sbert", "Catalina", ""], ["Cremers", "Daniel", ""]]}, {"id": "1508.01521", "submitter": "Saif Dawood Salman Al-Shaikhli", "authors": "Saif Dawood Salman Al-Shaikhli, Michael Ying Yang, Bodo Rosenhahn", "title": "Automatic 3D Liver Segmentation Using Sparse Representation of Global\n  and Local Image Information via Level Set Formulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel framework for automated liver segmentation via a level\nset formulation is presented. A sparse representation of both global\n(region-based) and local (voxel-wise) image information is embedded in a level\nset formulation to innovate a new cost function. Two dictionaries are build: A\nregion-based feature dictionary and a voxel-wise dictionary. These dictionaries\nare learned, using the K-SVD method, from a public database of liver\nsegmentation challenge (MICCAI-SLiver07). The learned dictionaries provide\nprior knowledge to the level set formulation. For the quantitative evaluation,\nthe proposed method is evaluated using the testing data of MICCAI-SLiver07\ndatabase. The results are evaluated using different metric scores computed by\nthe challenge organizers. The experimental results demonstrate the superiority\nof the proposed framework by achieving the highest segmentation accuracy\n(79.6\\%) in comparison to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 20:07:45 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2015 15:55:20 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Al-Shaikhli", "Saif Dawood Salman", ""], ["Yang", "Michael Ying", ""], ["Rosenhahn", "Bodo", ""]]}, {"id": "1508.01534", "submitter": "Jundong Liu", "authors": "Bibo Shi, Jundong Liu", "title": "Nonlinear Metric Learning for kNN and SVMs through Geometric\n  Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research efforts to extend linear metric learning models to\nhandle nonlinear structures have attracted great interests. In this paper, we\npropose a novel nonlinear solution through the utilization of deformable\ngeometric models to learn spatially varying metrics, and apply the strategy to\nboost the performance of both kNN and SVM classifiers. Thin-plate splines (TPS)\nare chosen as the geometric model due to their remarkable versatility and\nrepresentation power in accounting for high-order deformations. By transforming\nthe input space through TPS, we can pull same-class neighbors closer while\npushing different-class points farther away in kNN, as well as make the input\ndata points more linearly separable in SVMs. Improvements in the performance of\nkNN classification are demonstrated through experiments on synthetic and real\nworld datasets, with comparisons made with several state-of-the-art metric\nlearning solutions. Our SVM-based models also achieve significant improvements\nover traditional linear and kernel SVMs with the same datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2015 20:29:28 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Shi", "Bibo", ""], ["Liu", "Jundong", ""]]}, {"id": "1508.01667", "submitter": "Limin Wang", "authors": "Limin Wang, Sheng Guo, Weilin Huang, Yu Qiao", "title": "Places205-VGGNet Models for Scene Recognition", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VGGNets have turned out to be effective for object recognition in still\nimages. However, it is unable to yield good performance by directly adapting\nthe VGGNet models trained on the ImageNet dataset for scene recognition. This\nreport describes our implementation of training the VGGNets on the large-scale\nPlaces205 dataset. Specifically, we train three VGGNet models, namely\nVGGNet-11, VGGNet-13, and VGGNet-16, by using a Multi-GPU extension of Caffe\ntoolbox with high computational efficiency. We verify the performance of\ntrained Places205-VGGNet models on three datasets: MIT67, SUN397, and\nPlaces205. Our trained models achieve the state-of-the-art performance on these\ndatasets and are made public available.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 12:11:06 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Wang", "Limin", ""], ["Guo", "Sheng", ""], ["Huang", "Weilin", ""], ["Qiao", "Yu", ""]]}, {"id": "1508.01720", "submitter": "Jure Sokolic", "authors": "Jure Sokolic, Francesco Renna, Robert Calderbank, Miguel R. D.\n  Rodrigues", "title": "Mismatch in the Classification of Linear Subspaces: Sufficient\n  Conditions for Reliable Classification", "comments": "17 pages, 7 figures, submitted to IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2016.2537272", "report-no": null, "categories": "cs.IT cs.CV math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the classification of linear subspaces with mismatched\nclassifiers. In particular, we assume a model where one observes signals in the\npresence of isotropic Gaussian noise and the distribution of the signals\nconditioned on a given class is Gaussian with a zero mean and a low-rank\ncovariance matrix. We also assume that the classifier knows only a mismatched\nversion of the parameters of input distribution in lieu of the true parameters.\nBy constructing an asymptotic low-noise expansion of an upper bound to the\nerror probability of such a mismatched classifier, we provide sufficient\nconditions for reliable classification in the low-noise regime that are able to\nsharply predict the absence of a classification error floor. Such conditions\nare a function of the geometry of the true signal distribution, the geometry of\nthe mismatched signal distributions as well as the interplay between such\ngeometries, namely, the principal angles and the overlap between the true and\nthe mismatched signal subspaces. Numerical results demonstrate that our\nconditions for reliable classification can sharply predict the behavior of a\nmismatched classifier both with synthetic data and in a motion segmentation and\na hand-written digit classification applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:16:39 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 18:59:48 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Sokolic", "Jure", ""], ["Renna", "Francesco", ""], ["Calderbank", "Robert", ""], ["Rodrigues", "Miguel R. D.", ""]]}, {"id": "1508.01722", "submitter": "Jun-Cheng Chen", "authors": "Jun-Cheng Chen and Vishal M. Patel and Rama Chellappa", "title": "Unconstrained Face Verification using Deep CNN Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for unconstrained face verification\nbased on deep convolutional features and evaluate it on the newly released\nIARPA Janus Benchmark A (IJB-A) dataset. The IJB-A dataset includes real-world\nunconstrained faces from 500 subjects with full pose and illumination\nvariations which are much harder than the traditional Labeled Face in the Wild\n(LFW) and Youtube Face (YTF) datasets. The deep convolutional neural network\n(DCNN) is trained using the CASIA-WebFace dataset. Extensive experiments on the\nIJB-A dataset are provided.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2015 15:21:19 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 19:41:42 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Chen", "Jun-Cheng", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1508.01859", "submitter": "Upul Sonnadara", "authors": "G.D. Illeperuma, D.U.J. Sonnadara", "title": "Simulation of optical flow and fuzzy based obstacle avoidance system for\n  mobile robots", "comments": "4 pages, Published in 30 April 2015", "journal-ref": "International Journal of Artificial Intelligence and Neural\n  Networks, 5-1 (2015) 53-56", "doi": null, "report-no": "ISSN : 2250-3749", "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Honey bees use optical flow to avoid obstacles effectively. In this research\nwork similar methodology was tested on a simulated mobile robot. Simulation\nframework was based on VRML and Simulink in a 3D world. Optical flow vectors\nwere calculated from a video scene captured by a virtual camera which was used\nas inputs to a fuzzy logic controller. Fuzzy logic controller decided the\nlocomotion of the robot. Different fuzzy logic rules were evaluated. The robot\nwas able to navigate through complex static and dynamic environments\neffectively, avoiding obstacles on its path.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 06:40:55 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Illeperuma", "G. D.", ""], ["Sonnadara", "D. U. J.", ""]]}, {"id": "1508.01887", "submitter": "Zhanglin Peng", "authors": "Zhanglin Peng, Ya Li, Zhaoquan Cai and Liang Lin", "title": "Deep Boosting: Joint Feature Selection and Analysis Dictionary Learning\n  in Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how the traditional image classification pipelines can\nbe extended into a deep architecture, inspired by recent successes of deep\nneural networks. We propose a deep boosting framework based on layer-by-layer\njoint feature boosting and dictionary learning. In each layer, we construct a\ndictionary of filters by combining the filters from the lower layer, and\niteratively optimize the image representation with a joint\ndiscriminative-generative formulation, i.e. minimization of empirical\nclassification error plus regularization of analysis image generation over\ntraining images. For optimization, we perform two iterating steps: i) to\nminimize the classification error, select the most discriminative features\nusing the gentle adaboost algorithm; ii) according to the feature selection,\nupdate the filters to minimize the regularization on analysis image\nrepresentation using the gradient descent method. Once the optimization is\nconverged, we learn the higher layer representation in the same way. Our model\ndelivers several distinct advantages. First, our layer-wise optimization\nprovides the potential to build very deep architectures. Second, the generated\nimage representation is compact and meaningful. In several visual recognition\ntasks, our framework outperforms existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 11:42:21 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 09:45:07 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Peng", "Zhanglin", ""], ["Li", "Ya", ""], ["Cai", "Zhaoquan", ""], ["Lin", "Liang", ""]]}, {"id": "1508.01983", "submitter": "Amr Bakry", "authors": "Amr Bakry, Mohamed Elhoseiny, Tarek El-Gaaly and Ahmed Elgammal", "title": "Digging Deep into the layers of CNNs: In Search of How CNNs Achieve View\n  Invariance", "comments": "This paper accepted in ICLR 2016 main conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on studying the view-manifold structure in the feature\nspaces implied by the different layers of Convolutional Neural Networks (CNN).\nThere are several questions that this paper aims to answer: Does the learned\nCNN representation achieve viewpoint invariance? How does it achieve viewpoint\ninvariance? Is it achieved by collapsing the view manifolds, or separating them\nwhile preserving them? At which layer is view invariance achieved? How can the\nstructure of the view manifold at each layer of a deep convolutional neural\nnetwork be quantified experimentally? How does fine-tuning of a pre-trained CNN\non a multi-view dataset affect the representation at each layer of the network?\nIn order to answer these questions we propose a methodology to quantify the\ndeformation and degeneracy of view manifolds in CNN layers. We apply this\nmethodology and report interesting results in this paper that answer the\naforementioned questions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 04:02:51 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 09:22:40 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 06:56:49 GMT"}, {"version": "v4", "created": "Mon, 20 Jun 2016 10:05:15 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Bakry", "Amr", ""], ["Elhoseiny", "Mohamed", ""], ["El-Gaaly", "Tarek", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1508.02091", "submitter": "Jack Hessel", "authors": "Jack Hessel, Nicolas Savva, Michael J. Wilber", "title": "Image Representations and New Domains in Neural Image Captioning", "comments": "11 Pages, 5 Images, To appear at EMNLP 2015's Vision + Learning\n  workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the possibility that recent promising results in automatic caption\ngeneration are due primarily to language models. By varying image\nrepresentation quality produced by a convolutional neural network, we find that\na state-of-the-art neural captioning algorithm is able to produce quality\ncaptions even when provided with surprisingly poor image representations. We\nreplicate this result in a new, fine-grained, transfer learned captioning\ndomain, consisting of 66K recipe image/title pairs. We also provide some\nexperiments regarding the appropriateness of datasets for automatic captioning,\nand find that having multiple captions per image is beneficial, but not an\nabsolute requirement.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 22:52:10 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Hessel", "Jack", ""], ["Savva", "Nicolas", ""], ["Wilber", "Michael J.", ""]]}, {"id": "1508.02171", "submitter": "L\\'aszl\\'o Gyarmati", "authors": "Laszlo Gyarmati and Xavier Anguera", "title": "Automatic Extraction of the Passing Strategies of Soccer Teams", "comments": "2015 KDD Workshop on Large-Scale Sports Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology offers new ways to measure the locations of the players and of the\nball in sports. This translates to the trajectories the ball takes on the field\nas a result of the tactics the team applies. The challenge professionals in\nsoccer are facing is to take the reverse path: given the trajectories of the\nball is it possible to infer the underlying strategy/tactic of a team? We\npropose a method based on Dynamic Time Warping to reveal the tactics of a team\nthrough the analysis of repeating series of events. Based on the analysis of an\nentire season, we derive insights such as passing strategies for maintaining\nball possession or counter attacks, and passing styles with a focus on the team\nor on the capabilities of the individual players.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 09:00:33 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Gyarmati", "Laszlo", ""], ["Anguera", "Xavier", ""]]}, {"id": "1508.02246", "submitter": "Ngu Nguyen", "authors": "Ngu Nguyen", "title": "Feature Learning for Interaction Activity Recognition in RGBD Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a human activity recognition method which is based on\nfeatures learned from 3D video data without incorporating domain knowledge. The\nexperiments on data collected by RGBD cameras produce results outperforming\nother techniques. Our feature encoding method follows the bag-of-visual-word\nmodel, then we use a SVM classifier to recognise the activities. We do not use\nskeleton or tracking information and the same technique is applied on color and\ndepth data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 13:51:35 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Nguyen", "Ngu", ""]]}, {"id": "1508.02405", "submitter": "Farnood Gholami", "authors": "Farnood Gholami, Daria A. Trojan, Jozsef K\u007fovecses, Wassim M. Haddad,\n  Behnood Gholami", "title": "Gait Assessment for Multiple Sclerosis Patients Using Microsoft Kinect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gait analysis of patients with neurological disorders, including multiple\nsclerosis (MS), is important for rehabilitation and treatment. The Mircrosoft\nKinect sensor, which was developed for motion recognition in gaming\napplications, is an ideal candidate for an inexpensive system providing the\ncapability for human gait analysis. In this research, we develop a framework to\nquantify the gait abnormality of MS patients using a Kinect for Windows camera.\nIn addition to the previously introduced gait indices, a novel set of MS gait\nindices based on the concept of dynamic time warping is introduced. The newly\nintroduced indices can characterize a patient's gait pattern as a whole and\nquantify a subject's gait distance from the healthy population. We will\ninvestigate the correlation of gait indices with the multiple sclerosis walking\nscale (MSWS) and the clinical ambulation score. This work establishes the\nfeasibility of using the Kinect sensor for clinical gait assessment for MS\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2015 20:09:28 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Gholami", "Farnood", ""], ["Trojan", "Daria A.", ""], ["K\u007fovecses", "Jozsef", ""], ["Haddad", "Wassim M.", ""], ["Gholami", "Behnood", ""]]}, {"id": "1508.02496", "submitter": "Olivier Mor\\`ere", "authors": "Vijay Chandrasekhar, Jie Lin, Olivier Mor\\`ere, Hanlin Goh, Antoine\n  Veillard", "title": "A Practical Guide to CNNs and Fisher Vectors for Image Instance\n  Retrieval", "comments": "Deep Convolutional Neural Networks for instance retrieval, Fisher\n  Vectors, instance retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With deep learning becoming the dominant approach in computer vision, the use\nof representations extracted from Convolutional Neural Nets (CNNs) is quickly\ngaining ground on Fisher Vectors (FVs) as favoured state-of-the-art global\nimage descriptors for image instance retrieval. While the good performance of\nCNNs for image classification are unambiguously recognised, which of the two\nhas the upper hand in the image retrieval context is not entirely clear yet. In\nthis work, we propose a comprehensive study that systematically evaluates FVs\nand CNNs for image retrieval. The first part compares the performances of FVs\nand CNNs on multiple publicly available data sets. We investigate a number of\ndetails specific to each method. For FVs, we compare sparse descriptors based\non interest point detectors with dense single-scale and multi-scale variants.\nFor CNNs, we focus on understanding the impact of depth, architecture and\ntraining data on retrieval results. Our study shows that no descriptor is\nsystematically better than the other and that performance gains can usually be\nobtained by using both types together. The second part of the study focuses on\nthe impact of geometrical transformations such as rotations and scale changes.\nFVs based on interest point detectors are intrinsically resilient to such\ntransformations while CNNs do not have a built-in mechanism to ensure such\ninvariance. We show that performance of CNNs can quickly degrade in presence of\nrotations while they are far less affected by changes in scale. We then propose\na number of ways to incorporate the required invariances in the CNN pipeline.\nOverall, our work is intended as a reference guide offering practically useful\nand simply implementable guidelines to anyone looking for state-of-the-art\nglobal descriptors best suited to their specific image instance retrieval\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 07:15:07 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2015 10:51:50 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2015 11:30:22 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Chandrasekhar", "Vijay", ""], ["Lin", "Jie", ""], ["Mor\u00e8re", "Olivier", ""], ["Goh", "Hanlin", ""], ["Veillard", "Antoine", ""]]}, {"id": "1508.02606", "submitter": "Hainan Cui", "authors": "Hao Hu, Hainan Cui", "title": "InAR:Inverse Augmented Reality", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality is the art to seamlessly fuse virtual objects into real\nones. In this short note, we address the opposite problem, the inverse\naugmented reality, that is, given a perfectly augmented reality scene where\nhuman is unable to distinguish real objects from virtual ones, how the machine\ncould help do the job. We show by structure from motion (SFM), a simple 3D\nreconstruction technique from images in computer vision, the real and virtual\nobjects can be easily separated in the reconstructed 3D scene.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2015 14:17:28 GMT"}], "update_date": "2015-08-12", "authors_parsed": [["Hu", "Hao", ""], ["Cui", "Hainan", ""]]}, {"id": "1508.02844", "submitter": "Bojan Pepik", "authors": "Bojan Pepik, Rodrigo Benenson, Tobias Ritschel, Bernt Schiele", "title": "What is Holding Back Convnets for Detection?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have recently shown excellent results in\ngeneral object detection and many other tasks. Albeit very effective, they\ninvolve many user-defined design choices. In this paper we want to better\nunderstand these choices by inspecting two key aspects \"what did the network\nlearn?\", and \"what can the network learn?\". We exploit new annotations\n(Pascal3D+), to enable a new empirical analysis of the R-CNN detector. Despite\ncommon belief, our results indicate that existing state-of-the-art convnet\narchitectures are not invariant to various appearance factors. In fact, all\nconsidered networks have similar weak points which cannot be mitigated by\nsimply increasing the training data (architectural changes are needed). We show\nthat overall performance can improve when using image renderings for data\naugmentation. We report the best known results on the Pascal3D+ detection and\nview-point estimation tasks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 08:22:04 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 13:54:22 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Pepik", "Bojan", ""], ["Benenson", "Rodrigo", ""], ["Ritschel", "Tobias", ""], ["Schiele", "Bernt", ""]]}, {"id": "1508.02848", "submitter": "Yunjin Chen", "authors": "Yunjin Chen and Thomas Pock", "title": "Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast\n  and Effective Image Restoration", "comments": "14 pages, 13 figures, to appear in IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (TPAMI)", "journal-ref": null, "doi": "10.1109/TPAMI.2016.2596743", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration is a long-standing problem in low-level computer vision\nwith many interesting applications. We describe a flexible learning framework\nbased on the concept of nonlinear reaction diffusion models for various image\nrestoration problems. By embodying recent improvements in nonlinear diffusion\nmodels, we propose a dynamic nonlinear reaction diffusion model with\ntime-dependent parameters (\\ie, linear filters and influence functions). In\ncontrast to previous nonlinear diffusion models, all the parameters, including\nthe filters and the influence functions, are simultaneously learned from\ntraining data through a loss based approach. We call this approach TNRD --\n\\textit{Trainable Nonlinear Reaction Diffusion}. The TNRD approach is\napplicable for a variety of image restoration tasks by incorporating\nappropriate reaction force. We demonstrate its capabilities with three\nrepresentative applications, Gaussian image denoising, single image super\nresolution and JPEG deblocking. Experiments show that our trained nonlinear\ndiffusion models largely benefit from the training of the parameters and\nfinally lead to the best reported performance on common test datasets for the\ntested applications. Our trained models preserve the structural simplicity of\ndiffusion models and take only a small number of diffusion steps, thus are\nhighly efficient. Moreover, they are also well-suited for parallel computation\non GPUs, which makes the inference procedure extremely fast.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 08:40:48 GMT"}, {"version": "v2", "created": "Sat, 20 Aug 2016 04:48:54 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Chen", "Yunjin", ""], ["Pock", "Thomas", ""]]}, {"id": "1508.02849", "submitter": "Xiaobao Sheng", "authors": "Fei Jiang, Lili Jia, Xiaobao Sheng, Riley LeMieux", "title": "Manifold regularization in structured output space for semi-supervised\n  structured output prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured output prediction aims to learn a predictor to predict a\nstructured output from a input data vector. The structured outputs include\nvector, tree, sequence, etc. We usually assume that we have a training set of\ninput-output pairs to train the predictor. However, in many real-world appli-\ncations, it is difficult to obtain the output for a input, thus for many\ntraining input data points, the structured outputs are missing. In this paper,\nwe dis- cuss how to learn from a training set composed of some input-output\npairs, and some input data points without outputs. This problem is called semi-\nsupervised structured output prediction. We propose a novel method for this\nproblem by constructing a nearest neighbor graph from the input space to\npresent the manifold structure, and using it to regularize the structured out-\nput space directly. We define a slack structured output for each training data\npoint, and proposed to predict it by learning a structured output predictor.\nThe learning of both slack structured outputs and the predictor are unified\nwithin one single minimization problem. In this problem, we propose to mini-\nmize the structured loss between the slack structured outputs of neighboring\ndata points, and the prediction error measured by the structured loss. The\nproblem is optimized by an iterative algorithm. Experiment results over three\nbenchmark data sets show its advantage.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 08:44:47 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Jiang", "Fei", ""], ["Jia", "Lili", ""], ["Sheng", "Xiaobao", ""], ["LeMieux", "Riley", ""]]}, {"id": "1508.02959", "submitter": "Roman Fedorov", "authors": "Roman Fedorov", "title": "Mountain Peak Detection in Online Social Media", "comments": "M.Sc. Thesis, Politecnico di Milano, Italy, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for the classification of mountain panoramas from\nuser-generated photographs followed by identification and extraction of\nmountain peaks from those panoramas. We have developed an automatic technique\nthat, given as input a geo-tagged photograph, estimates its FOV (Field Of View)\nand the direction of the camera using a matching algorithm on the photograph\nedge maps and a rendered view of the mountain silhouettes that should be seen\nfrom the observer's point of view. The extraction algorithm then identifies the\nmountain peaks present in the photograph and their profiles. We discuss\npossible applications in social fields such as photograph peak tagging on\nsocial portals, augmented reality on mobile devices when viewing a mountain\npanorama, and generation of collective intelligence systems (such as\nenvironmental models) from massive social media collections (e.g. snow water\navailability maps based on mountain peak states extracted from photograph\nhosting services).\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 15:43:16 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Fedorov", "Roman", ""]]}, {"id": "1508.02977", "submitter": "Diane Gilliocq-Hirtz", "authors": "Diane Gilliocq-Hirtz and Zakaria Belhachmi", "title": "A massively parallel multi-level approach to a domain decomposition\n  method for the optical flow estimation with varying illumination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variational method to solve the optical flow problem with\nvarying illumination. We apply an adaptive control of the regularization\nparameter which allows us to preserve the edges and fine features of the\ncomputed flow. To reduce the complexity of the estimation for high resolution\nimages and the time of computations, we implement a multi-level parallel\napproach based on the domain decomposition with the Schwarz overlapping method.\nThe second level of parallelism uses the massively parallel solver MUMPS. We\nperform some numerical simulations to show the efficiency of our approach and\nto validate it on classical and real-world image sequences.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 16:16:10 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Gilliocq-Hirtz", "Diane", ""], ["Belhachmi", "Zakaria", ""]]}, {"id": "1508.03269", "submitter": "Mario Corsolini", "authors": "Mario Corsolini and Andrea Carta", "title": "A New Approach to an Old Problem: The Reconstruction of a Go Game\n  through a Series of Photographs", "comments": "13 pages, 18 figures, datasets available from\n  http://www.oipaz.net/VideoKifu.html - added references in section 1, updated\n  addresses, added indication that both authors contributed equally", "journal-ref": "\"Proceedings of the Second International Go Game Science\n  Conference\", Liberec, Czech Republic, 2015/07/30. ISBN 978-80-7378-299-3", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a series of photographs taken during a Go game, we describe the\ntechniques we successfully employ for pinpointing the grid lines of the Go\nboard and for tracking their small movements between consecutive photographs;\nthen we discuss how to approximate the location and orientation of the\nobserver's point of view, in order to compensate for projection effects.\nFinally we describe the different criteria that jointly form the algorithm for\nstones' detection, thus enabling us to automatically reconstruct the whole move\nsequence.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:01:12 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 17:56:19 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Corsolini", "Mario", ""], ["Carta", "Andrea", ""]]}, {"id": "1508.03276", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Harshita Jhavar", "title": "Talking about the Moving Image: A Declarative Model for Image Schema\n  Based Embodied Perception Grounding and Language Generation", "comments": "19 pages. Unpublished report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory and corresponding declarative model for the\nembodied grounding and natural language based analytical summarisation of\ndynamic visuo-spatial imagery. The declarative model ---ecompassing\nspatio-linguistic abstractions, image schemas, and a spatio-temporal feature\nbased language generator--- is modularly implemented within Constraint Logic\nProgramming (CLP). The implemented model is such that primitives of the theory,\ne.g., pertaining to space and motion, image schemata, are available as\nfirst-class objects with `deep semantics' suited for inference and query. We\ndemonstrate the model with select examples broadly motivated by areas such as\nfilm, design, geography, smart environments where analytical natural language\nbased externalisations of the moving image are central from the viewpoint of\nhuman interaction, evidence-based qualitative analysis, and sensemaking.\n  Keywords: moving image, visual semantics and embodiment, visuo-spatial\ncognition and computation, cognitive vision, computational models of narrative,\ndeclarative spatial reasoning\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:34:07 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Jhavar", "Harshita", ""]]}, {"id": "1508.03422", "submitter": "Salman Khan Mr.", "authors": "Salman H. Khan, Munawar Hayat, Mohammed Bennamoun, Ferdous Sohel,\n  Roberto Togneri", "title": "Cost Sensitive Learning of Deep Feature Representations from Imbalanced\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class imbalance is a common problem in the case of real-world object\ndetection and classification tasks. Data of some classes is abundant making\nthem an over-represented majority, and data of other classes is scarce, making\nthem an under-represented minority. This imbalance makes it challenging for a\nclassifier to appropriately learn the discriminating boundaries of the majority\nand minority classes. In this work, we propose a cost sensitive deep neural\nnetwork which can automatically learn robust feature representations for both\nthe majority and minority classes. During training, our learning procedure\njointly optimizes the class dependent costs and the neural network parameters.\nThe proposed approach is applicable to both binary and multi-class problems\nwithout any modification. Moreover, as opposed to data level approaches, we do\nnot alter the original data distribution which results in a lower computational\ncost during the training process. We report the results of our experiments on\nsix major image classification datasets and show that the proposed approach\nsignificantly outperforms the baseline algorithms. Comparisons with popular\ndata sampling techniques and cost sensitive classifiers demonstrate the\nsuperior performance of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 05:23:30 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 08:37:37 GMT"}, {"version": "v3", "created": "Thu, 23 Mar 2017 10:57:10 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Khan", "Salman H.", ""], ["Hayat", "Munawar", ""], ["Bennamoun", "Mohammed", ""], ["Sohel", "Ferdous", ""], ["Togneri", "Roberto", ""]]}, {"id": "1508.03498", "submitter": "Xin Yuan", "authors": "Xin Yuan, Hong Jiang, Gang Huang, Paul Wilford", "title": "Lensless Compressive Imaging", "comments": "37 pages, 10 figures. Submitted to SIAM Journal on Imaging Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a lensless compressive imaging architecture, which consists of an\naperture assembly and a single sensor, without using any lens. An anytime\nalgorithm is proposed to reconstruct images from the compressive measurements;\nthe algorithm produces a sequence of solutions that monotonically converge to\nthe true signal (thus, anytime). The algorithm is developed based on the\nsparsity of local overlapping patches (in the transformation domain) and\nstate-of-the-art results have been obtained. Experiments on real data\ndemonstrate that encouraging results are obtained by measuring about 10% (of\nthe image pixels) compressive measurements. The reconstruction results of the\nproposed algorithm are compared with the JPEG compression (based on file sizes)\nand the reconstructed image quality is close to the JPEG compression, in\nparticular at a high compression rate.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 13:34:10 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Yuan", "Xin", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Wilford", "Paul", ""]]}, {"id": "1508.03590", "submitter": "Lo\\\"is Mignard-Debise", "authors": "Lois Mignard-Debise, Ivo Ihrke", "title": "Light-field Microscopy with a Consumer Light-field Camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of inexpensive consumer light- field camera technology for\nthe purpose of light-field mi- croscopy. Our experiments are based on the Lytro\n(first gen- eration) camera. Unfortunately, the optical systems of the Lytro\nand those of microscopes are not compatible, lead- ing to a loss of light-field\ninformation due to angular and spatial vignetting when directly recording\nmicroscopic pic- tures. We therefore consider an adaptation of the Lytro op-\ntical system. We demonstrate that using the Lytro directly as an oc- ular\nreplacement, leads to unacceptable spatial vignetting. However, we also found a\nsetting that allows the use of the Lytro camera in a virtual imaging mode which\nprevents the information loss to a large extent. We analyze the new vir- tual\nimaging mode and use it in two different setups for im- plementing light-field\nmicroscopy using a Lytro camera. As a practical result, we show that the camera\ncan be used for low magnification work, as e.g. common in quality control,\nsurface characterization, etc. We achieve a maximum spa- tial resolution of\nabout 6.25{\\mu}m, albeit at a limited SNR for the side views.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 12:17:09 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 09:12:02 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Mignard-Debise", "Lois", ""], ["Ihrke", "Ivo", ""]]}, {"id": "1508.03649", "submitter": "Hokky Situngkir", "authors": "Hokky Situngkir", "title": "Borobudur was Built Algorithmically", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": "BFI Working Paper Series, WP082010", "categories": "cs.CY cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-similarity of Indonesian Borobudur Temple is observed through the\ndimensionality of stupa that is hypothetically closely related to whole\narchitectural body. Fractal dimension is calculated by using the cube counting\nmethod and found that the dimension is 2.325, which is laid between the\ntwo-dimensional plane and three dimensional space. The applied fractal geometry\nand self-similarity of the building is emerged as the building process\nimplement the metric rules, since there is no universal metric standard known\nin ancient traditional Javanese culture thus the architecture is not based on\nfinal master plan. The paper also proposes how the hypothetical algorithmic\narchitecture might be applied computationally in order to see some experimental\ngenerations of similar building. The paper ends with some conjectures for\nfurther challenge and insights related to fractal geometry in Javanese\ntraditional cultural heritages.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 04:08:27 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Situngkir", "Hokky", ""]]}, {"id": "1508.03710", "submitter": "Mohammad Sabokrou", "authors": "Mohsen Fayyaz, Masoud PourReza, Mohammad Hajizadeh Saffar, Mohammad\n  Sabokrou, Mahmood Fathy", "title": "A Novel Approach For Finger Vein Verification Based on Self-Taught\n  Learning", "comments": "4 pages, 4 figures, Submitted Iranian Conference on Machine Vision\n  and Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a method for user Finger Vein Authentication (FVA)\nas a biometric system. Using the discriminative features for classifying theses\nfinger veins is one of the main tips that make difference in related works,\nThus we propose to learn a set of representative features, based on\nautoencoders. We model the user finger vein using a Gaussian distribution.\nExperimental results show that our algorithm perform like a state-of-the-art on\nSDUMLA-HMT benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 09:03:19 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Fayyaz", "Mohsen", ""], ["PourReza", "Masoud", ""], ["Saffar", "Mohammad Hajizadeh", ""], ["Sabokrou", "Mohammad", ""], ["Fathy", "Mahmood", ""]]}, {"id": "1508.03755", "submitter": "Team Lear", "authors": "Danila Potapov (LEAR), Matthijs Douze (LEAR), Jerome Revaud (LEAR),\n  Zaid Harchaoui (LEAR, CIMS), Cordelia Schmid (LEAR)", "title": "Beat-Event Detection in Action Movie Franchises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While important advances were recently made towards temporally localizing and\nrecognizing specific human actions or activities in videos, efficient detection\nand classification of long video chunks belonging to semantically defined\ncategories such as \"pursuit\" or \"romance\" remains challenging.We introduce a\nnew dataset, Action Movie Franchises, consisting of a collection of Hollywood\naction movie franchises. We define 11 non-exclusive semantic categories -\ncalled beat-categories - that are broad enough to cover most of the movie\nfootage. The corresponding beat-events are annotated as groups of video shots,\npossibly overlapping.We propose an approach for localizing beat-events based on\nclassifying shots into beat-categories and learning the temporal constraints\nbetween shots. We show that temporal constraints significantly improve the\nclassification performance. We set up an evaluation protocol for beat-event\nlocalization as well as for shot classification, depending on whether movies\nfrom the same franchise are present or not in the training data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 17:04:50 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Potapov", "Danila", "", "LEAR"], ["Douze", "Matthijs", "", "LEAR"], ["Revaud", "Jerome", "", "LEAR"], ["Harchaoui", "Zaid", "", "LEAR, CIMS"], ["Schmid", "Cordelia", "", "LEAR"]]}, {"id": "1508.03868", "submitter": "Brendan Jou", "authors": "Brendan Jou, Tao Chen, Nikolaos Pappas, Miriam Redi, Mercan Topkara,\n  Shih-Fu Chang", "title": "Visual Affect Around the World: A Large-scale Multilingual Visual\n  Sentiment Ontology", "comments": "11 pages, to appear at ACM MM'15", "journal-ref": null, "doi": "10.1145/2733373.2806246", "report-no": null, "categories": "cs.MM cs.CL cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every culture and language is unique. Our work expressly focuses on the\nuniqueness of culture and language in relation to human affect, specifically\nsentiment and emotion semantics, and how they manifest in social multimedia. We\ndevelop sets of sentiment- and emotion-polarized visual concepts by adapting\nsemantic structures called adjective-noun pairs, originally introduced by Borth\net al. (2013), but in a multilingual context. We propose a new\nlanguage-dependent method for automatic discovery of these adjective-noun\nconstructs. We show how this pipeline can be applied on a social multimedia\nplatform for the creation of a large-scale multilingual visual sentiment\nconcept ontology (MVSO). Unlike the flat structure in Borth et al. (2013), our\nunified ontology is organized hierarchically by multilingual clusters of\nvisually detectable nouns and subclusters of emotionally biased versions of\nthese nouns. In addition, we present an image-based prediction task to show how\ngeneralizable language-specific models are in a multilingual context. A new,\npublicly available dataset of >15.6K sentiment-biased visual concepts across 12\nlanguages with language-specific detector banks, >7.36M images and their\nmetadata is also released.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2015 21:43:59 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 16:33:13 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2015 19:07:14 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Jou", "Brendan", ""], ["Chen", "Tao", ""], ["Pappas", "Nikolaos", ""], ["Redi", "Miriam", ""], ["Topkara", "Mercan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1508.03881", "submitter": "Fangting Xia", "authors": "Fangting Xia, Jun Zhu, Peng Wang, Alan Yuille", "title": "Pose-Guided Human Parsing with Deep Learned Features", "comments": "12 pages, 10 figures, a shortened version of this paper was accepted\n  by AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing human body into semantic regions is crucial to human-centric\nanalysis. In this paper, we propose a segment-based parsing pipeline that\nexplores human pose information, i.e. the joint location of a human model,\nwhich improves the part proposal, accelerates the inference and regularizes the\nparsing process at the same time. Specifically, we first generate part segment\nproposals with respect to human joints predicted by a deep model, then part-\nspecific ranking models are trained for segment selection using both pose-based\nfeatures and deep-learned part potential features. Finally, the best ensemble\nof the proposed part segments are inferred though an And-Or Graph.\n  We evaluate our approach on the popular Penn-Fudan pedestrian parsing\ndataset, and demonstrate the effectiveness of using the pose information for\neach stage of the parsing pipeline. Finally, we show that our approach yields\nsuperior part segmentation accuracy comparing to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 00:05:38 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 02:07:11 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Xia", "Fangting", ""], ["Zhu", "Jun", ""], ["Wang", "Peng", ""], ["Yuille", "Alan", ""]]}, {"id": "1508.03928", "submitter": "Hongyang Li", "authors": "Hongyang Li, Huchuan Lu, Zhe Lin, Xiaohui Shen, Brian Price", "title": "LCNN: Low-level Feature Embedded CNN for Salient Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel deep neural network framework embedded with\nlow-level features (LCNN) for salient object detection in complex images. We\nutilise the advantage of convolutional neural networks to automatically learn\nthe high-level features that capture the structured information and semantic\ncontext in the image. In order to better adapt a CNN model into the saliency\ntask, we redesign the network architecture based on the small-scale datasets.\nSeveral low-level features are extracted, which can effectively capture\ncontrast and spatial information in the salient regions, and incorporated to\ncompensate with the learned high-level features at the output of the last fully\nconnected layer. The concatenated feature vector is further fed into a\nhinge-loss SVM detector in a joint discriminative learning manner and the final\nsaliency score of each region within the bounding box is obtained by the linear\ncombination of the detector's weights. Experiments on three challenging\nbenchmark (MSRA-5000, PASCAL-S, ECCSD) demonstrate our algorithm to be\neffective and superior than most low-level oriented state-of-the-arts in terms\nof P-R curves, F-measure and mean absolute errors.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 05:45:12 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Li", "Hongyang", ""], ["Lu", "Huchuan", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Price", "Brian", ""]]}, {"id": "1508.03929", "submitter": "Saeed Reza Kheradpisheh", "authors": "Saeed Reza Kheradpisheh, Masoud Ghodrati, Mohammad Ganjtabesh,\n  Timoth\\'ee Masquelier", "title": "Deep Networks Can Resemble Human Feed-forward Vision in Invariant Object\n  Recognition", "comments": null, "journal-ref": "Scientific Reports (2016) 6: 32672", "doi": "10.1038/srep32672", "report-no": null, "categories": "cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have attracted much attention\nrecently, and have shown to be able to recognize thousands of object categories\nin natural image databases. Their architecture is somewhat similar to that of\nthe human visual system: both use restricted receptive fields, and a hierarchy\nof layers which progressively extract more and more abstracted features. Yet it\nis unknown whether DCNNs match human performance at the task of view-invariant\nobject recognition, whether they make similar errors and use similar\nrepresentations for this task, and whether the answers depend on the magnitude\nof the viewpoint variations. To investigate these issues, we benchmarked eight\nstate-of-the-art DCNNs, the HMAX model, and a baseline shallow model and\ncompared their results to those of humans with backward masking. Unlike in all\nprevious DCNN studies, we carefully controlled the magnitude of the viewpoint\nvariations to demonstrate that shallow nets can outperform deep nets and humans\nwhen variations are weak. When facing larger variations, however, more layers\nwere needed to match human performance and error distributions, and to have\nrepresentations that are consistent with human behavior. A very deep net with\n18 layers even outperformed humans at the highest variation level, using the\nmost human-like representations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 05:46:24 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 14:34:44 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 15:54:25 GMT"}, {"version": "v4", "created": "Tue, 28 Jun 2016 10:55:18 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Kheradpisheh", "Saeed Reza", ""], ["Ghodrati", "Masoud", ""], ["Ganjtabesh", "Mohammad", ""], ["Masquelier", "Timoth\u00e9e", ""]]}, {"id": "1508.03953", "submitter": "Tam Nguyen", "authors": "Kang Wang, Tam V. Nguyen, Jiashi Feng, Jose Sepulveda", "title": "Sense Beyond Expressions: Cuteness", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Internet culture, cuteness has become a popular\nconcept. Many people are curious about what factors making a person look cute.\nHowever, there is rare research to answer this interesting question. In this\nwork, we construct a dataset of personal images with comprehensively annotated\ncuteness scores and facial attributes to investigate this high-level concept in\ndepth. Based on this dataset, through an automatic attributes mining process,\nwe find several critical attributes determining the cuteness of a person. We\nalso develop a novel Continuous Latent Support Vector Machine (C-LSVM) method\nto predict the cuteness score of one person given only his image. Extensive\nevaluations validate the effectiveness of the proposed method for cuteness\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 08:48:54 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Wang", "Kang", ""], ["Nguyen", "Tam V.", ""], ["Feng", "Jiashi", ""], ["Sepulveda", "Jose", ""]]}, {"id": "1508.04028", "submitter": "Lex Fridman", "authors": "Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor", "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze\n  Classification", "comments": "Accepted for Publication in IET Computer Vision. arXiv admin note:\n  text overlap with arXiv:1507.04760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate, robust, inexpensive gaze tracking in the car can help keep a driver\nsafe by facilitating the more effective study of how to improve (1) vehicle\ninterfaces and (2) the design of future Advanced Driver Assistance Systems. In\nthis paper, we estimate head pose and eye pose from monocular video using\nmethods developed extensively in prior work and ask two new interesting\nquestions. First, how much better can we classify driver gaze using head and\neye pose versus just using head pose? Second, are there individual-specific\ngaze strategies that strongly correlate with how much gaze classification\nimproves with the addition of eye pose information? We answer these questions\nby evaluating data drawn from an on-road study of 40 drivers. The main insight\nof the paper is conveyed through the analogy of an \"owl\" and \"lizard\" which\ndescribes the degree to which the eyes and the head move when shifting gaze.\nWhen the head moves a lot (\"owl\"), not much classification improvement is\nattained by estimating eye pose on top of head pose. On the other hand, when\nthe head stays still and only the eyes move (\"lizard\"), classification accuracy\nincreases significantly from adding in eye pose. We characterize how that\naccuracy varies between people, gaze strategies, and gaze regions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 13:54:05 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 18:10:49 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 04:46:26 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Fridman", "Lex", ""], ["Lee", "Joonbum", ""], ["Reimer", "Bryan", ""], ["Victor", "Trent", ""]]}, {"id": "1508.04035", "submitter": "Emmanuel Osegi", "authors": "Emmanuel N. Osegi", "title": "A Generative Model for Multi-Dialect Representation", "comments": "19 pages, 3 figures, 2 tables, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the era of deep learning several unsupervised models have been developed\nto capture the key features in unlabeled handwritten data. Popular among them\nis the Restricted Boltzmann Machines RBM. However, due to the novelty in\nhandwritten multidialect data, the RBM may fail to generate an efficient\nrepresentation. In this paper we propose a generative model, the Mode\nSynthesizing Machine MSM for on-line representation of real life handwritten\nmultidialect language data. The MSM takes advantage of the hierarchical\nrepresentation of the modes of a data distribution using a two-point error\nupdate to learn a sequence of representative multidialects in a generative way.\nExperiments were performed to evaluate the performance of the MSM over the RBM\nwith the former attaining much lower error values than the latter on both\nindependent and mixed data set.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 14:05:44 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Osegi", "Emmanuel N.", ""]]}, {"id": "1508.04190", "submitter": "Zongbo Hao", "authors": "Hao Zongbo, Lu Linlin, Zhang Qianni, Wu Jie, Izquierdo Ebroul, Yang\n  Juanyu, Zhao Jun", "title": "Action Recognition based on Subdivision-Fusion Model", "comments": "Accepted by BMVC2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel Subdivision-Fusion Model (SFM) to recognize human\nactions. In most action recognition tasks, overlapping feature distribution is\na common problem leading to overfitting. In the subdivision stage of the\nproposed SFM, samples in each category are clustered. Then, such samples are\ngrouped into multiple more concentrated subcategories. Boundaries for the\nsubcategories are easier to find and as consequence overfitting is avoided. In\nthe subsequent fusion stage, the multi-subcategories classification results are\nconverted back to the original category recognition problem. Two methods to\ndetermine the number of clusters are provided. The proposed model has been\nthoroughly tested with four popular datasets. In the Hollywood2 dataset, an\naccuracy of 79.4% is achieved, outperforming the state-of-the-art accuracy of\n64.3%. The performance on the YouTube Action dataset has been improved from\n75.8% to 82.5%, while considerably improvements are also observed on the KTH\nand UCF50 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 01:38:08 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Zongbo", "Hao", ""], ["Linlin", "Lu", ""], ["Qianni", "Zhang", ""], ["Jie", "Wu", ""], ["Ebroul", "Izquierdo", ""], ["Juanyu", "Yang", ""], ["Jun", "Zhao", ""]]}, {"id": "1508.04198", "submitter": "Yifan Fu", "authors": "Yifan Fu and Junbin Gao and Xia Hong and David Tien", "title": "Low Rank Representation on Riemannian Manifold of Square Root Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel low rank representation (LRR) algorithm for\ndata lying on the manifold of square root densities. Unlike traditional LRR\nmethods which rely on the assumption that the data points are vectors in the\nEuclidean space, our new algorithm is designed to incorporate the intrinsic\ngeometric structure and geodesic distance of the manifold. Experiments on\nseveral computer vision datasets showcase its noise robustness and superior\nperformance on classification and subspace clustering compared to other\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 02:33:30 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Fu", "Yifan", ""], ["Gao", "Junbin", ""], ["Hong", "Xia", ""], ["Tien", "David", ""]]}, {"id": "1508.04221", "submitter": "Jingbin Wang", "authors": "Xuejie Liu, Jingbin Wang, Ming Yin, Benjamin Edwards, Peijuan Xu", "title": "Supervised learning of sparse context reconstruction coefficients for\n  data representation and classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1507.00019", "journal-ref": null, "doi": "10.1007/s00521-015-2042-5", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context of data points, which is usually defined as the other data points in\na data set, has been found to play important roles in data representation and\nclassification. In this paper, we study the problem of using context of a data\npoint for its classification problem. Our work is inspired by the observation\nthat actually only very few data points are critical in the context of a data\npoint for its representation and classification. We propose to represent a data\npoint as the sparse linear combination of its context, and learn the sparse\ncontext in a supervised way to increase its discriminative ability. To this\nend, we proposed a novel formulation for context learning, by modeling the\nlearning of context parameter and classifier in a unified objective, and\noptimizing it with an alternative strategy in an iterative algorithm.\nExperiments on three benchmark data set show its advantage over\nstate-of-the-art context-based data representation and classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 05:59:17 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Liu", "Xuejie", ""], ["Wang", "Jingbin", ""], ["Yin", "Ming", ""], ["Edwards", "Benjamin", ""], ["Xu", "Peijuan", ""]]}, {"id": "1508.04224", "submitter": "Jingbin Wang", "authors": "Jingyan Wang, Yihua Zhou, Haoxiang Wang, Xiaohong Yang, Feng Yang,\n  Austin Peterson", "title": "Image tag completion by local learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of tag completion is to learn the missing tags of an image. In\nthis paper, we propose to learn a tag scoring vector for each image by local\nlinear learning. A local linear function is used in the neighborhood of each\nimage to predict the tag scoring vectors of its neighboring images. We\nconstruct a unified objective function for the learning of both tag scoring\nvectors and local linear function parame- ters. In the objective, we impose the\nlearned tag scoring vectors to be consistent with the known associations to the\ntags of each image, and also minimize the prediction error of each local linear\nfunction, while reducing the complexity of each local function. The objective\nfunction is optimized by an alternate optimization strategy and gradient\ndescent methods in an iterative algorithm. We compare the proposed algorithm\nagainst different state-of-the-art tag completion methods, and the results show\nits advantages.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 06:22:42 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Wang", "Jingyan", ""], ["Zhou", "Yihua", ""], ["Wang", "Haoxiang", ""], ["Yang", "Xiaohong", ""], ["Yang", "Feng", ""], ["Peterson", "Austin", ""]]}, {"id": "1508.04238", "submitter": "Zhihan Lv", "authors": "Xiaolei Zhang, Yong Han, DongSheng Hao, Zhihan Lv", "title": "Preprint ARPPS Augmented Reality Pipeline Prospect System", "comments": "This is the preprint version of our paper on ICONIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on ICONIP. Outdoor augmented\nreality geographic information system (ARGIS) is the hot application of\naugmented reality over recent years. This paper concludes the key solutions of\nARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),\nand respectively realizes the machine vision based pipeline prospect system\n(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the\nMVBPPS's realization, this paper studies the neural network based 3D features\nmatching method.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 08:18:55 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Zhang", "Xiaolei", ""], ["Han", "Yong", ""], ["Hao", "DongSheng", ""], ["Lv", "Zhihan", ""]]}, {"id": "1508.04326", "submitter": "Jiale Cao", "authors": "Yanwei Pang, Jiale Cao, and Xuelong Li", "title": "Cascade Learning by Optimally Partitioning", "comments": "17 pages, 20 figures", "journal-ref": null, "doi": "10.1109/TCYB.2016.2601438", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cascaded AdaBoost classifier is a well-known efficient object detection\nalgorithm. The cascade structure has many parameters to be determined. Most of\nexisting cascade learning algorithms are designed by assigning detection rate\nand false positive rate to each stage either dynamically or statically. Their\nobjective functions are not directly related to minimum computation cost. These\nalgorithms are not guaranteed to have optimal solution in the sense of\nminimizing computation cost. On the assumption that a strong classifier is\ngiven, in this paper we propose an optimal cascade learning algorithm (we call\nit iCascade) which iteratively partitions the strong classifiers into two parts\nuntil predefined number of stages are generated. iCascade searches the optimal\nnumber ri of weak classifiers of each stage i by directly minimizing the\ncomputation cost of the cascade. Theorems are provided to guarantee the\nexistence of the unique optimal solution. Theorems are also given for the\nproposed efficient algorithm of searching optimal parameters ri. Once a new\nstage is added, the parameter ri for each stage decreases gradually as\niteration proceeds, which we call decreasing phenomenon. Moreover, with the\ngoal of minimizing computation cost, we develop an effective algorithm for\nsetting the optimal threshold of each stage classifier. In addition, we prove\nin theory why more new weak classifiers are required compared to the last\nstage. Experimental results on face detection demonstrate the effectiveness and\nefficiency of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 14:12:45 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pang", "Yanwei", ""], ["Cao", "Jiale", ""], ["Li", "Xuelong", ""]]}, {"id": "1508.04389", "submitter": "Rajeev Ranjan", "authors": "Rajeev Ranjan, Vishal M. Patel, Rama Chellappa", "title": "A Deep Pyramid Deformable Part Model for Face Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a face detection algorithm based on Deformable Part Models and\ndeep pyramidal features. The proposed method called DP2MFD is able to detect\nfaces of various sizes and poses in unconstrained conditions. It reduces the\ngap in training and testing of DPM on deep features by adding a normalization\nlayer to the deep convolutional neural network (CNN). Extensive experiments on\nfour publicly available unconstrained face detection datasets show that our\nmethod is able to capture the meaningful structure of faces and performs\nsignificantly better than many competitive face detection algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 17:24:09 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Ranjan", "Rajeev", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1508.04458", "submitter": "Soysal Degirmenci", "authors": "S. Degirmenci, Joseph A. O'Sullivan, David G. Politte", "title": "Multiresolution Approach to Acceleration of Iterative Image\n  Reconstruction for X-Ray Imaging for Security Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional x-ray CT image reconstruction in baggage scanning in\nsecurity applications is an important research field. The variety of materials\nto be reconstructed is broader than medical x-ray imaging. Presence of high\nattenuating materials such as metal may cause artifacts if analytical\nreconstruction methods are used. Statistical modeling and the resultant\niterative algorithms are known to reduce these artifacts and present good\nquantitative accuracy in estimates of linear attenuation coefficients. However,\niterative algorithms may require computations in order to achieve\nquantitatively accurate results. For the case of baggage scanning, in order to\nprovide fast accurate inspection throughput, they must be accelerated\ndrastically. There are many approaches proposed in the literature to increase\nspeed of convergence. This paper presents a new method that estimates the\nwavelet coefficients of the images in the discrete wavelet transform domain\ninstead of the image space itself. Initially, surrogate functions are created\naround approximation coefficients only. As the iterations proceed, the wavelet\ntree on which the updates are made is expanded based on a criterion and detail\ncoefficients at each level are updated and the tree is expanded this way. For\nexample, in the smooth regions of the image the detail coefficients are not\nupdated while the coefficients that represent the high-frequency component\naround edges are being updated, thus saving time by focusing computations where\nthey are needed. This approach is implemented on real data from a SureScan (TM)\nx1000 Explosive Detection System and compared to straightforward implementation\nof the unregularized alternating minimization of O'Sullivan and Benac [1].\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 22:03:28 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Degirmenci", "S.", ""], ["O'Sullivan", "Joseph A.", ""], ["Politte", "David G.", ""]]}, {"id": "1508.04467", "submitter": "Zhao Kang", "authors": "Zhao Kang, Chong Peng, Qiang Cheng", "title": "Robust Subspace Clustering via Smoothed Rank Approximation", "comments": "Journal, code is available", "journal-ref": "IEEE Signal Processing Letters, 22(2015)2088-2092", "doi": "10.1109/LSP.2015.2460737", "report-no": null, "categories": "cs.CV cs.IT cs.LG cs.NA math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix rank minimizing subject to affine constraints arises in many\napplication areas, ranging from signal processing to machine learning. Nuclear\nnorm is a convex relaxation for this problem which can recover the rank exactly\nunder some restricted and theoretically interesting conditions. However, for\nmany real-world applications, nuclear norm approximation to the rank function\ncan only produce a result far from the optimum. To seek a solution of higher\naccuracy than the nuclear norm, in this paper, we propose a rank approximation\nbased on Logarithm-Determinant. We consider using this rank approximation for\nsubspace clustering application. Our framework can model different kinds of\nerrors and noise. Effective optimization strategy is developed with theoretical\nguarantee to converge to a stationary point. The proposed method gives\npromising results on face clustering and motion segmentation tasks compared to\nthe state-of-the-art subspace clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2015 21:54:03 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Kang", "Zhao", ""], ["Peng", "Chong", ""], ["Cheng", "Qiang", ""]]}, {"id": "1508.04535", "submitter": "Ruimao Zhang", "authors": "Ruimao Zhang, Liang Lin, Rui Zhang, Wangmeng Zuo, Lei Zhang", "title": "Bit-Scalable Deep Hashing with Regularized Similarity Learning for Image\n  Retrieval and Person Re-identification", "comments": "14 pages, 5 figures. IEEE Transactions on Image Processing 2015", "journal-ref": null, "doi": "10.1109/TIP.2015.2467315", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting informative image features and learning effective approximate\nhashing functions are two crucial steps in image retrieval . Conventional\nmethods often study these two steps separately, e.g., learning hash functions\nfrom a predefined hand-crafted feature space. Meanwhile, the bit lengths of\noutput hashing codes are preset in most previous methods, neglecting the\nsignificance level of different bits and restricting their practical\nflexibility. To address these issues, we propose a supervised learning\nframework to generate compact and bit-scalable hashing codes directly from raw\nimages. We pose hashing learning as a problem of regularized similarity\nlearning. Specifically, we organize the training images into a batch of triplet\nsamples, each sample containing two images with the same label and one with a\ndifferent label. With these triplet samples, we maximize the margin between\nmatched pairs and mismatched pairs in the Hamming space. In addition, a\nregularization term is introduced to enforce the adjacency consistency, i.e.,\nimages of similar appearances should have similar codes. The deep convolutional\nneural network is utilized to train the model in an end-to-end fashion, where\ndiscriminative image features and hash functions are simultaneously optimized.\nFurthermore, each bit of our hashing codes is unequally weighted so that we can\nmanipulate the code lengths by truncating the insignificant bits. Our framework\noutperforms state-of-the-arts on public benchmarks of similar image search and\nalso achieves promising results in the application of person re-identification\nin surveillance. It is also shown that the generated bit-scalable hashing codes\nwell preserve the discriminative powers with shorter code lengths.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 06:26:34 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2015 06:02:54 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Zhang", "Ruimao", ""], ["Lin", "Liang", ""], ["Zhang", "Rui", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1508.04546", "submitter": "Alexander Krull", "authors": "Alexander Krull, Eric Brachmann, Frank Michel, Michael Ying Yang,\n  Stefan Gumhold, Carsten Rother", "title": "Learning Analysis-by-Synthesis for 6D Pose Estimation in RGB-D Images", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis-by-synthesis has been a successful approach for many tasks in\ncomputer vision, such as 6D pose estimation of an object in an RGB-D image\nwhich is the topic of this work. The idea is to compare the observation with\nthe output of a forward process, such as a rendered image of the object of\ninterest in a particular pose. Due to occlusion or complicated sensor noise, it\ncan be difficult to perform this comparison in a meaningful way. We propose an\napproach that \"learns to compare\", while taking these difficulties into\naccount. This is done by describing the posterior density of a particular\nobject pose with a convolutional neural network (CNN) that compares an observed\nand rendered image. The network is trained with the maximum likelihood\nparadigm. We observe empirically that the CNN does not specialize to the\ngeometry or appearance of specific objects, and it can be used with objects of\nvastly different shapes and appearances, and in different backgrounds. Compared\nto state-of-the-art, we demonstrate a significant improvement on two different\ndatasets which include a total of eleven objects, cluttered background, and\nheavy occlusion.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 07:24:14 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Krull", "Alexander", ""], ["Brachmann", "Eric", ""], ["Michel", "Frank", ""], ["Yang", "Michael Ying", ""], ["Gumhold", "Stefan", ""], ["Rother", "Carsten", ""]]}, {"id": "1508.04554", "submitter": "Bokai Cao", "authors": "Bokai Cao, Xiangnan Kong, Jingyuan Zhang, Philip S. Yu and Ann B.\n  Ragin", "title": "Mining Brain Networks using Multiple Side Views for Neurological\n  Disorder Identification", "comments": "in Proceedings of IEEE International Conference on Data Mining (ICDM)\n  2015", "journal-ref": null, "doi": "10.1109/ICDM.2015.50", "report-no": null, "categories": "cs.LG cs.CV cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining discriminative subgraph patterns from graph data has attracted great\ninterest in recent years. It has a wide variety of applications in disease\ndiagnosis, neuroimaging, etc. Most research on subgraph mining focuses on the\ngraph representation alone. However, in many real-world applications, the side\ninformation is available along with the graph data. For example, for\nneurological disorder identification, in addition to the brain networks derived\nfrom neuroimaging data, hundreds of clinical, immunologic, serologic and\ncognitive measures may also be documented for each subject. These measures\ncompose multiple side views encoding a tremendous amount of supplemental\ninformation for diagnostic purposes, yet are often ignored. In this paper, we\nstudy the problem of discriminative subgraph selection using multiple side\nviews and propose a novel solution to find an optimal set of subgraph features\nfor graph classification by exploring a plurality of side views. We derive a\nfeature evaluation criterion, named gSide, to estimate the usefulness of\nsubgraph patterns based upon side views. Then we develop a branch-and-bound\nalgorithm, called gMSV, to efficiently search for optimal subgraph features by\nintegrating the subgraph mining process and the procedure of discriminative\nfeature selection. Empirical studies on graph classification tasks for\nneurological disorders using brain networks demonstrate that subgraph patterns\nselected by the multi-side-view guided subgraph selection approach can\neffectively boost graph classification performances and are relevant to disease\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 07:51:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Cao", "Bokai", ""], ["Kong", "Xiangnan", ""], ["Zhang", "Jingyuan", ""], ["Yu", "Philip S.", ""], ["Ragin", "Ann B.", ""]]}, {"id": "1508.04586", "submitter": "Veronica Vilaplana", "authors": "Ver\\'onica Vilaplana", "title": "Saliency maps on image hierarchies", "comments": "Accepted for publication in Signal Processing: Image Communications,\n  2015", "journal-ref": null, "doi": "10.1016/j.image.2015.07.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose two saliency models for salient object segmentation\nbased on a hierarchical image segmentation, a tree-like structure that\nrepresents regions at different scales from the details to the whole image\n(e.g. gPb-UCM, BPT). The first model is based on a hierarchy of image\npartitions. The saliency at each level is computed on a region basis, taking\ninto account the contrast between regions. The maps obtained for the different\npartitions are then integrated into a final saliency map. The second model\ndirectly works on the structure created by the segmentation algorithm,\ncomputing saliency at each node and integrating these cues in a straightforward\nmanner into a single saliency map. We show that the proposed models produce\nhigh quality saliency maps. Objective evaluation demonstrates that the two\nmethods achieve state-of-the-art performance in several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 10:07:07 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Vilaplana", "Ver\u00f3nica", ""]]}, {"id": "1508.04785", "submitter": "KuanTing Chen", "authors": "KuanTing Chen, Kezhen Chen, Peizhong Cong, Winston H. Hsu, Jiebo Luo", "title": "Who are the Devils Wearing Prada in New York City?", "comments": null, "journal-ref": null, "doi": "10.1145/2733373.2809930", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is a perpetual topic in human social life, and the mass has the\npenchant to emulate what large city residents and celebrities wear. Undeniably,\nNew York City is such a bellwether large city with all kinds of fashion\nleadership. Consequently, to study what the fashion trends are during this\nyear, it is very helpful to learn the fashion trends of New York City.\nDiscovering fashion trends in New York City could boost many applications such\nas clothing recommendation and advertising. Does the fashion trend in the New\nYork Fashion Show actually influence the clothing styles on the public? To\nanswer this question, we design a novel system that consists of three major\ncomponents: (1) constructing a large dataset from the New York Fashion Shows\nand New York street chic in order to understand the likely clothing fashion\ntrends in New York, (2) utilizing a learning-based approach to discover fashion\nattributes as the representative characteristics of fashion trends, and (3)\ncomparing the analysis results from the New York Fashion Shows and street-chic\nimages to verify whether the fashion shows have actual influence on the people\nin New York City. Through the preliminary experiments over a large clothing\ndataset, we demonstrate the effectiveness of our proposed system, and obtain\nuseful insights on fashion trends and fashion influence.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 20:28:31 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Chen", "KuanTing", ""], ["Chen", "Kezhen", ""], ["Cong", "Peizhong", ""], ["Hsu", "Winston H.", ""], ["Luo", "Jiebo", ""]]}, {"id": "1508.04843", "submitter": "Kisuk Lee", "authors": "Kisuk Lee, Aleksandar Zlateski, Ashwin Vishwanathan, and H. Sebastian\n  Seung", "title": "Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efforts to automate the reconstruction of neural circuits from 3D electron\nmicroscopic (EM) brain images are critical for the field of connectomics. An\nimportant computation for reconstruction is the detection of neuronal\nboundaries. Images acquired by serial section EM, a leading 3D EM technique,\nare highly anisotropic, with inferior quality along the third dimension. For\nsuch images, the 2D max-pooling convolutional network has set the standard for\nperformance at boundary detection. Here we achieve a substantial gain in\naccuracy through three innovations. Following the trend towards deeper networks\nfor object recognition, we use a much deeper network than previously employed\nfor boundary detection. Second, we incorporate 3D as well as 2D filters, to\nenable computations that use 3D context. Finally, we adopt a recursively\ntrained architecture in which a first network generates a preliminary boundary\nmap that is provided as input along with the original image to a second network\nthat generates a final boundary map. Backpropagation training is accelerated by\nZNN, a new implementation of 3D convolutional networks that uses multicore CPU\nparallelism for speed. Our hybrid 2D-3D architecture could be more generally\napplicable to other types of anisotropic 3D images, including video, and our\nrecursive framework for any image labeling problem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 00:46:38 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Lee", "Kisuk", ""], ["Zlateski", "Aleksandar", ""], ["Vishwanathan", "Ashwin", ""], ["Seung", "H. Sebastian", ""]]}, {"id": "1508.04887", "submitter": "Ko-Jen Hsiao", "authors": "Ko-Jen Hsiao, Kevin S. Xu, Jeff Calder and Alfred O. Hero III", "title": "Multi-criteria Similarity-based Anomaly Detection using Pareto Depth\n  Analysis", "comments": "The work is submitted to IEEE TNNLS Special Issue on Learning in\n  Non-(geo)metric Spaces for review on October 28, 2013, revised on July 26,\n  2015 and accepted on July 30, 2015. A preliminary version of this work is\n  reported in the conference Advances in Neural Information Processing Systems\n  (NIPS) 2012", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 27\n  (2016) 1307-1321", "doi": "10.1109/TNNLS.2015.2466686", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying patterns in a data set that exhibit\nanomalous behavior, often referred to as anomaly detection. Similarity-based\nanomaly detection algorithms detect abnormally large amounts of similarity or\ndissimilarity, e.g.~as measured by nearest neighbor Euclidean distances between\na test sample and the training samples. In many application domains there may\nnot exist a single dissimilarity measure that captures all possible anomalous\npatterns. In such cases, multiple dissimilarity measures can be defined,\nincluding non-metric measures, and one can test for anomalies by scalarizing\nusing a non-negative linear combination of them. If the relative importance of\nthe different dissimilarity measures are not known in advance, as in many\nanomaly detection applications, the anomaly detection algorithm may need to be\nexecuted multiple times with different choices of weights in the linear\ncombination. In this paper, we propose a method for similarity-based anomaly\ndetection using a novel multi-criteria dissimilarity measure, the Pareto depth.\nThe proposed Pareto depth analysis (PDA) anomaly detection algorithm uses the\nconcept of Pareto optimality to detect anomalies under multiple criteria\nwithout having to run an algorithm multiple times with different choices of\nweights. The proposed PDA approach is provably better than using linear\ncombinations of the criteria and shows superior performance on experiments with\nsynthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 06:25:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Hsiao", "Ko-Jen", ""], ["Xu", "Kevin S.", ""], ["Calder", "Jeff", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1508.04924", "submitter": "Hamid Palangi", "authors": "Hamid Palangi, Rabab Ward, Li Deng", "title": "Distributed Compressive Sensing: A Deep Learning Approach", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Volume: 64, Issue: 17, pp.\n  4504-4518, 2016", "doi": "10.1109/TSP.2016.2557301", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various studies that address the compressed sensing problem with Multiple\nMeasurement Vectors (MMVs) have been recently carried. These studies assume the\nvectors of the different channels to be jointly sparse. In this paper, we relax\nthis condition. Instead we assume that these sparse vectors depend on each\nother but that this dependency is unknown. We capture this dependency by\ncomputing the conditional probability of each entry in each vector being\nnon-zero, given the \"residuals\" of all previous vectors. To estimate these\nprobabilities, we propose the use of the Long Short-Term Memory (LSTM)[1], a\ndata driven model for sequence modelling that is deep in time. To calculate the\nmodel parameters, we minimize a cross entropy cost function. To reconstruct the\nsparse vectors at the decoder, we propose a greedy solver that uses the above\nmodel to estimate the conditional probabilities. By performing extensive\nexperiments on two real world datasets, we show that the proposed method\nsignificantly outperforms the general MMV solver (the Simultaneous Orthogonal\nMatching Pursuit (SOMP)) and a number of the model-based Bayesian methods. The\nproposed method does not add any complexity to the general compressive sensing\nencoder. The trained model is used just at the decoder. As the proposed method\nis a data driven method, it is only applicable when training data is available.\nIn many applications however, training data is indeed available, e.g. in\nrecorded images and videos.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 08:57:29 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 01:15:11 GMT"}, {"version": "v3", "created": "Wed, 11 May 2016 22:18:13 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Palangi", "Hamid", ""], ["Ward", "Rabab", ""], ["Deng", "Li", ""]]}, {"id": "1508.04945", "submitter": "Lianwen Jin", "authors": "Weixin Yang, Lianwen Jin, Manfei Liu", "title": "DeepWriterID: An End-to-end Online Text-independent Writer\n  Identification System", "comments": "7 pages5 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the rapid growth of touchscreen mobile terminals and pen-based\ninterfaces, handwriting-based writer identification systems are attracting\nincreasing attention for personal authentication, digital forensics, and other\napplications. However, most studies on writer identification have not been\nsatisfying because of the insufficiency of data and difficulty of designing\ngood features under various conditions of handwritings. Hence, we introduce an\nend-to-end system, namely DeepWriterID, employed a deep convolutional neural\nnetwork (CNN) to address these problems. A key feature of DeepWriterID is a new\nmethod we are proposing, called DropSegment. It designs to achieve data\naugmentation and improve the generalized applicability of CNN. For sufficient\nfeature representation, we further introduce path signature feature maps to\nimprove performance. Experiments were conducted on the NLPR handwriting\ndatabase. Even though we only use pen-position information in the pen-down\nstate of the given handwriting samples, we achieved new state-of-the-art\nidentification rates of 95.72% for Chinese text and 98.51% for English text.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 10:39:19 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 14:05:48 GMT"}], "update_date": "2015-12-23", "authors_parsed": [["Yang", "Weixin", ""], ["Jin", "Lianwen", ""], ["Liu", "Manfei", ""]]}, {"id": "1508.04955", "submitter": "Ksenia Konyushkova", "authors": "Ksenia Konyushkova, Raphael Sznitman, Pascal Fua", "title": "Introducing Geometry in Active Learning for Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an Active Learning approach to training a segmentation classifier\nthat exploits geometric priors to streamline the annotation process in 3D image\nvolumes. To this end, we use these priors not only to select voxels most in\nneed of annotation but to guarantee that they lie on 2D planar patch, which\nmakes it much easier to annotate than if they were randomly distributed in the\nvolume. A simplified version of this approach is effective in natural 2D\nimages. We evaluated our approach on Electron Microscopy and Magnetic Resonance\nimage volumes, as well as on natural images. Comparing our approach against\nseveral accepted baselines demonstrates a marked performance increase.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 11:03:44 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Konyushkova", "Ksenia", ""], ["Sznitman", "Raphael", ""], ["Fua", "Pascal", ""]]}, {"id": "1508.04981", "submitter": "Changsoo Je", "authors": "Changsoo Je, Sang Wook Lee, and Rae-Hong Park", "title": "High-Contrast Color-Stripe Pattern for Rapid Structured-Light Range\n  Imaging", "comments": "13 pages, 12 figures, 8th European Conference on Computer Vision\n  (ECCV), Prague, Czech Republic, May 2004, Proceedings, Part I", "journal-ref": "Computer Vision - ECCV 2004, LNCS 3021, pp. 95-107,\n  Springer-Verlag Berlin Heidelberg, May 10, 2004", "doi": "10.1007/978-3-540-24670-1_8", "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For structured-light range imaging, color stripes can be used for increasing\nthe number of distinguishable light patterns compared to binary BW stripes.\nTherefore, an appropriate use of color patterns can reduce the number of light\nprojections and range imaging is achievable in single video frame or in \"one\nshot\". On the other hand, the reliability and range resolution attainable from\ncolor stripes is generally lower than those from multiply projected binary BW\npatterns since color contrast is affected by object color reflectance and\nambient light. This paper presents new methods for selecting stripe colors and\ndesigning multiple-stripe patterns for \"one-shot\" and \"two-shot\" imaging. We\nshow that maximizing color contrast between the stripes in one-shot imaging\nreduces the ambiguities resulting from colored object surfaces and limitations\nin sensor/projector resolution. Two-shot imaging adds an extra video frame and\nmaximizes the color contrast between the first and second video frames to\ndiminish the ambiguities even further. Experimental results demonstrate the\neffectiveness of the presented one-shot and two-shot color-stripe imaging\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 13:43:46 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Je", "Changsoo", ""], ["Lee", "Sang Wook", ""], ["Park", "Rae-Hong", ""]]}, {"id": "1508.05028", "submitter": "Yuncheng Li", "authors": "Yuncheng Li, Jifei Huang, Jiebo Luo", "title": "Using User Generated Online Photos to Estimate and Monitor Air Pollution\n  in Major Cities", "comments": "ICIMCS '15", "journal-ref": null, "doi": "10.1145/2808492.2808564", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of economy in China over the past decade, air\npollution has become an increasingly serious problem in major cities and caused\ngrave public health concerns in China. Recently, a number of studies have dealt\nwith air quality and air pollution. Among them, some attempt to predict and\nmonitor the air quality from different sources of information, ranging from\ndeployed physical sensors to social media. These methods are either too\nexpensive or unreliable, prompting us to search for a novel and effective way\nto sense the air quality. In this study, we propose to employ the state of the\nart in computer vision techniques to analyze photos that can be easily acquired\nfrom online social media. Next, we establish the correlation between the haze\nlevel computed directly from photos with the official PM 2.5 record of the\ntaken city at the taken time. Our experiments based on both synthetic and real\nphotos have shown the promise of this image-based approach to estimating and\nmonitoring air pollution.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 16:20:35 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Li", "Yuncheng", ""], ["Huang", "Jifei", ""], ["Luo", "Jiebo", ""]]}, {"id": "1508.05038", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Seeing Behind the Camera: Identifying the Authorship of a Photograph", "comments": "Dataset downloadable at http://www.cs.pitt.edu/~chris/photographer To\n  Appear in CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the novel problem of identifying the photographer behind a\nphotograph. To explore the feasibility of current computer vision techniques to\naddress this problem, we created a new dataset of over 180,000 images taken by\n41 well-known photographers. Using this dataset, we examined the effectiveness\nof a variety of features (low and high-level, including CNN features) at\nidentifying the photographer. We also trained a new deep convolutional neural\nnetwork for this task. Our results show that high-level features greatly\noutperform low-level features. We provide qualitative results using these\nlearned models that give insight into our method's ability to distinguish\nbetween photographers, and allow us to draw interesting conclusions about what\nspecific photographers shoot. We also demonstrate two applications of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 16:45:17 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 06:38:08 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 01:09:08 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1508.05046", "submitter": "Xing Mei", "authors": "Xing Mei and Honggang Qi and Bao-Gang Hu and Siwei Lyu", "title": "Improving Image Restoration with Soft-Rounding", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several important classes of images such as text, barcode and pattern images\nhave the property that pixels can only take a distinct subset of values. This\nknowledge can benefit the restoration of such images, but it has not been\nwidely considered in current restoration methods. In this work, we describe an\neffective and efficient approach to incorporate the knowledge of distinct pixel\nvalues of the pristine images into the general regularized least squares\nrestoration framework. We introduce a new regularizer that attains zero at the\ndesignated pixel values and becomes a quadratic penalty function in the\nintervals between them. When incorporated into the regularized least squares\nrestoration framework, this regularizer leads to a simple and efficient step\nthat resembles and extends the rounding operation, which we term as\nsoft-rounding. We apply the soft-rounding enhanced solution to the restoration\nof binary text/barcode images and pattern images with multiple distinct pixel\nvalues. Experimental results show that soft-rounding enhanced restoration\nmethods achieve significant improvement in both visual quality and quantitative\nmeasures (PSNR and SSIM). Furthermore, we show that this regularizer can also\nbenefit the restoration of general natural images.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:04:34 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Mei", "Xing", ""], ["Qi", "Honggang", ""], ["Hu", "Bao-Gang", ""], ["Lyu", "Siwei", ""]]}, {"id": "1508.05056", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Amaia Salvador, Brendan Jou and Xavier Gir\\'o-i-Nieto", "title": "Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual\n  Sentiment Prediction", "comments": "Preprint of the paper accepted at the 1st Workshop on Affect and\n  Sentiment in Multimedia (ASM), in ACM MultiMedia 2015. Brisbane, Australia", "journal-ref": null, "doi": "10.1145/2813524.2813530", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual media are powerful means of expressing emotions and sentiments. The\nconstant generation of new content in social networks highlights the need of\nautomated visual sentiment analysis tools. While Convolutional Neural Networks\n(CNNs) have established a new state-of-the-art in several vision problems,\ntheir application to the task of sentiment analysis is mostly unexplored and\nthere are few studies regarding how to design CNNs for this purpose. In this\nwork, we study the suitability of fine-tuning a CNN for visual sentiment\nprediction as well as explore performance boosting techniques within this deep\nlearning setting. Finally, we provide a deep-dive analysis into a benchmark,\nstate-of-the-art network architecture to gain insight about how to design\npatterns for CNNs on the task of visual sentiment prediction.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2015 17:36:48 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 09:43:18 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Campos", "Victor", ""], ["Salvador", "Amaia", ""], ["Jou", "Brendan", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""]]}, {"id": "1508.05151", "submitter": "Christian Bailer", "authors": "Christian Bailer, Bertram Taetz, Didier Stricker", "title": "Flow Fields: Dense Correspondence Fields for Highly Accurate Large\n  Displacement Optical Flow Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern large displacement optical flow algorithms usually use an\ninitialization by either sparse descriptor matching techniques or dense\napproximate nearest neighbor fields. While the latter have the advantage of\nbeing dense, they have the major disadvantage of being very outlier prone as\nthey are not designed to find the optical flow, but the visually most similar\ncorrespondence. In this paper we present a dense correspondence field approach\nthat is much less outlier prone and thus much better suited for optical flow\nestimation than approximate nearest neighbor fields. Our approach is\nconceptually novel as it does not require explicit regularization, smoothing\n(like median filtering) or a new data term, but solely our novel purely data\nbased search strategy that finds most inliers (even for small objects), while\nit effectively avoids finding outliers. Moreover, we present novel enhancements\nfor outlier filtering. We show that our approach is better suited for large\ndisplacement optical flow estimation than state-of-the-art descriptor matching\ntechniques. We do so by initializing EpicFlow (so far the best method on\nMPI-Sintel) with our Flow Fields instead of their originally used\nstate-of-the-art descriptor matching technique. We significantly outperform the\noriginal EpicFlow on MPI-Sintel, KITTI and Middlebury.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 00:03:31 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 13:47:15 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Bailer", "Christian", ""], ["Taetz", "Bertram", ""], ["Stricker", "Didier", ""]]}, {"id": "1508.05306", "submitter": "Zhen Zuo PHD", "authors": "Zhen Zuo, Gang Wang, Bing Shuai, Lifan Zhao, Qingxiong Yang", "title": "Exemplar Based Deep Discriminative and Shareable Feature Learning for\n  Scene Image Classification", "comments": "Pattern Recognition, Elsevier, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to encode the class correlation and class specific information in\nimage representation, we propose a new local feature learning approach named\nDeep Discriminative and Shareable Feature Learning (DDSFL). DDSFL aims to\nhierarchically learn feature transformation filter banks to transform raw pixel\nimage patches to features. The learned filter banks are expected to: (1) encode\ncommon visual patterns of a flexible number of categories; (2) encode\ndiscriminative information; and (3) hierarchically extract patterns at\ndifferent visual levels. Particularly, in each single layer of DDSFL, shareable\nfilters are jointly learned for classes which share the similar patterns.\nDiscriminative power of the filters is achieved by enforcing the features from\nthe same category to be close, while features from different categories to be\nfar away from each other. Furthermore, we also propose two exemplar selection\nmethods to iteratively select training data for more efficient and effective\nlearning. Based on the experimental results, DDSFL can achieve very promising\nperformance, and it also shows great complementary effect to the\nstate-of-the-art Caffe features.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2015 15:16:18 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Zuo", "Zhen", ""], ["Wang", "Gang", ""], ["Shuai", "Bing", ""], ["Zhao", "Lifan", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1508.05463", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Parthipan Siva, and Alexander Wong", "title": "StochasticNet: Forming Deep Neural Networks via Stochastic Connectivity", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks is a branch in machine learning that has seen a meteoric\nrise in popularity due to its powerful abilities to represent and model\nhigh-level abstractions in highly complex data. One area in deep neural\nnetworks that is ripe for exploration is neural connectivity formation. A\npivotal study on the brain tissue of rats found that synaptic formation for\nspecific functional connectivity in neocortical neural microcircuits can be\nsurprisingly well modeled and predicted as a random formation. Motivated by\nthis intriguing finding, we introduce the concept of StochasticNet, where deep\nneural networks are formed via stochastic connectivity between neurons. As a\nresult, any type of deep neural networks can be formed as a StochasticNet by\nallowing the neuron connectivity to be stochastic. Stochastic synaptic\nformations, in a deep neural network architecture, can allow for efficient\nutilization of neurons for performing specific tasks. To evaluate the\nfeasibility of such a deep neural network architecture, we train a\nStochasticNet using four different image datasets (CIFAR-10, MNIST, SVHN, and\nSTL-10). Experimental results show that a StochasticNet, using less than half\nthe number of neural connections as a conventional deep neural network,\nachieves comparable accuracy and reduces overfitting on the CIFAR-10, MNIST and\nSVHN dataset. Interestingly, StochasticNet with less than half the number of\nneural connections, achieved a higher accuracy (relative improvement in test\nerror rate of ~6% compared to ConvNet) on the STL-10 dataset than a\nconventional deep neural network. Finally, StochasticNets have faster\noperational speeds while achieving better or similar accuracy performances.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 03:36:43 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 19:05:03 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2015 01:34:17 GMT"}, {"version": "v4", "created": "Tue, 10 Nov 2015 20:30:05 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Siva", "Parthipan", ""], ["Wong", "Alexander", ""]]}, {"id": "1508.05514", "submitter": "Tohid Ardeshiri", "authors": "Tohid Ardeshiri, Umut Orguner, Emre \\\"Ozkan", "title": "Gaussian Mixture Reduction Using Reverse Kullback-Leibler Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a greedy mixture reduction algorithm which is capable of pruning\nmixture components as well as merging them based on the Kullback-Leibler\ndivergence (KLD). The algorithm is distinct from the well-known Runnalls' KLD\nbased method since it is not restricted to merging operations. The capability\nof pruning (in addition to merging) gives the algorithm the ability of\npreserving the peaks of the original mixture during the reduction. Analytical\napproximations are derived to circumvent the computational intractability of\nthe KLD which results in a computationally efficient method. The proposed\nalgorithm is compared with Runnalls' and Williams' methods in two numerical\nexamples, using both simulated and real world data. The results indicate that\nthe performance and computational complexity of the proposed approach make it\nan efficient alternative to existing mixture reduction methods.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2015 13:41:17 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Ardeshiri", "Tohid", ""], ["Orguner", "Umut", ""], ["\u00d6zkan", "Emre", ""]]}, {"id": "1508.05581", "submitter": "Yanwei Pang", "authors": "Yanwei Pang, Jiale Cao, and Xuelong Li", "title": "Learning Sampling Distributions for Efficient Object Detection", "comments": "14 pages, 13 figures", "journal-ref": "IEEE Transactions on Cybernetics (2016)", "doi": "10.1109/TCYB.2015.2508603", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is an important task in computer vision and learning\nsystems. Multistage particle windows (MPW), proposed by Gualdi et al., is an\nalgorithm of fast and accurate object detection. By sampling particle windows\nfrom a proposal distribution (PD), MPW avoids exhaustively scanning the image.\nDespite its success, it is unknown how to determine the number of stages and\nthe number of particle windows in each stage. Moreover, it has to generate too\nmany particle windows in the initialization step and it redraws unnecessary too\nmany particle windows around object-like regions. In this paper, we attempt to\nsolve the problems of MPW. An important fact we used is that there is large\nprobability for a randomly generated particle window not to contain the object\nbecause the object is a sparse event relevant to the huge number of candidate\nwindows. Therefore, we design the proposal distribution so as to efficiently\nreject the huge number of non-object windows. Specifically, we propose the\nconcepts of rejection, acceptance, and ambiguity windows and regions. This\ncontrasts to MPW which utilizes only on region of support. The PD of MPW is\nacceptance-oriented whereas the PD of our method (called iPW) is\nrejection-oriented. Experimental results on human and face detection\ndemonstrate the efficiency and effectiveness of the iPW algorithm. The source\ncode is publicly accessible.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 09:17:49 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 12:52:08 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Pang", "Yanwei", ""], ["Cao", "Jiale", ""], ["Li", "Xuelong", ""]]}, {"id": "1508.05683", "submitter": "Siqi Liu", "authors": "Siqi Liu, Sidong Liu, Sonia Pujol, Ron Kikinis, Dagan Feng, Michael\n  Fulham, Weidong Cai", "title": "Morphometry-Based Longitudinal Neurodegeneration Simulation with MR\n  Imaging", "comments": "6 pages, 3 figures, preprint for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a longitudinal MR simulation framework which simulates the future\nneurodegenerative progression by outputting the predicted follow-up MR image\nand the voxel-based morphometry (VBM) map. This framework expects the patients\nto have at least 2 historical MR images available. The longitudinal and\ncross-sectional VBM maps are extracted to measure the affinity between the\ntarget subject and the template subjects collected for simulation. Then the\nfollow-up simulation is performed by resampling the latest available target MR\nimage with a weighted sum of non-linear transformations derived from the\nbest-matched templates. The leave-one-out strategy was used to compare\ndifferent simulation methods. Compared to the state-of-the-art voxel-based\nmethod, our proposed morphometry-based simulation achieves better accuracy in\nmost cases.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 01:54:54 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Liu", "Siqi", ""], ["Liu", "Sidong", ""], ["Pujol", "Sonia", ""], ["Kikinis", "Ron", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""], ["Cai", "Weidong", ""]]}, {"id": "1508.05704", "submitter": "Muhammad  Ali Qadar", "authors": "Muhammad Ali Qadar, Yan Zhaowen, Li Hua", "title": "Iterative Thresholded Bi-Histogram Equalization for Medical Image\n  Enhancement", "comments": "8 Pages, 8 Figures, International Journal of Computer Applications\n  (IJCA)", "journal-ref": "International Journal of Computer Applications (0975 8887) Volume\n  114 No. 8, March 2015", "doi": "10.5120/19999-1753", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement of human vision to get an insight to information content is of\nvital importance. The traditional histogram equalization methods have been\nsuffering from amplified contrast with the addition of artifacts and a\nsurprising unnatural visibility of the processed images. In order to overcome\nthese drawbacks, this paper proposes interative, mean, and multi-threshold\nselection criterion with plateau limits, which consist of histogram\nsegmentation, clipping and transformation modules. The histogram partition\nconsists of multiple thresholding processes that divide the histogram into two\nparts, whereas the clipping process nicely enhances the contrast by having a\ncheck on the rate of enhancement that could be tuned. Histogram equalization to\neach segmented sub-histogram provides the output image with preserved\nbrightness and enhanced contrast. Results of the present study showed that the\nproposed method efficiently handles the noise amplification. Further, it also\npreserves the brightness by retaining natural look of targeted image.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 06:28:56 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Qadar", "Muhammad Ali", ""], ["Zhaowen", "Yan", ""], ["Hua", "Li", ""]]}, {"id": "1508.05879", "submitter": "Alejandro Frery", "authors": "Gilberto P. Silva Junior and Alejandro C. Frery and Sandra Sandri and\n  Humberto Bustince and Edurne Barrenechea and C\\'edric Marco-Detchart", "title": "Optical images-based edge detection in Synthetic Aperture Radar images", "comments": "Accepted for publication in Knowledge-Based Systems", "journal-ref": null, "doi": "10.1016/j.knosys.2015.07.030", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of adapting optical images-based edge detection\ntechniques for use in Polarimetric Synthetic Aperture Radar (PolSAR) imagery.\nWe modify the gravitational edge detection technique (inspired by the Law of\nUniversal Gravity) proposed by Lopez-Molina et al, using the non-standard\nneighbourhood configuration proposed by Fu et al, to reduce the speckle noise\nin polarimetric SAR imagery. We compare the modified and unmodified versions of\nthe gravitational edge detection technique with the well-established one\nproposed by Canny, as well as with a recent multiscale fuzzy-based technique\nproposed by Lopez-Molina et Alejandro We also address the issues of aggregation\nof gray level images before and after edge detection and of filtering. All\ntechniques addressed here are applied to a mosaic built using class\ndistributions obtained from a real scene, as well as to the true PolSAR image;\nthe mosaic results are assessed using Baddeley's Delta Metric. Our experiments\nshow that modifying the gravitational edge detection technique with a\nnon-standard neighbourhood configuration produces better results than the\noriginal technique, as well as the other techniques used for comparison. The\nexperiments show that adapting edge detection methods from Computational\nIntelligence for use in PolSAR imagery is a new field worthy of exploration.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 16:49:30 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Junior", "Gilberto P. Silva", ""], ["Frery", "Alejandro C.", ""], ["Sandri", "Sandra", ""], ["Bustince", "Humberto", ""], ["Barrenechea", "Edurne", ""], ["Marco-Detchart", "C\u00e9dric", ""]]}, {"id": "1508.05995", "submitter": "Jianrui Ding", "authors": "Jianrui Ding, Min Xian, H.D.Cheng, Yang Li, Fei Xu, Yingtao Zhang", "title": "An algorithm for Left Atrial Thrombi detection using Transesophageal\n  Echocardiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transesophageal echocardiography (TEE) is widely used to detect left atrium\n(LA)/left atrial appendage (LAA) thrombi. In this paper, the local binary\npattern variance (LBPV) features are extracted from region of interest (ROI).\nAnd the dynamic features are formed by using the information of its neighbor\nframes in the sequence. The sequence is viewed as a bag, and the images in the\nsequence are considered as the instances. Multiple-instance learning (MIL)\nmethod is employed to solve the LAA thrombi detection. The experimental results\nshow that the proposed method can achieve better performance than that by using\nother methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 23:29:19 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Ding", "Jianrui", ""], ["Xian", "Min", ""], ["Cheng", "H. D.", ""], ["Li", "Yang", ""], ["Xu", "Fei", ""], ["Zhang", "Yingtao", ""]]}, {"id": "1508.06010", "submitter": "Muhammad Zubair Ahmad", "authors": "Muhammad Zubair Ahmad, Amir Ali Khan, Sihem Mezghani, Eric Perrin,\n  Kamel Mouhoubi, Jean-Luc Bodnar, Valeriu Vrabie", "title": "Wavelet subspace decomposition of thermal infrared images for defect\n  detection in artworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the health of ancient artworks requires adequate prudence because\nof the sensitive nature of these materials. Classical techniques for\nidentifying the development of faults rely on acoustic testing. These\ntechniques, being invasive, may result in causing permanent damage to the\nmaterial, especially if the material is inspected periodically. Non destructive\ntesting has been carried out for different materials since long. In this\nregard, non-invasive systems were developed based on infrared thermometry\nprinciple to identify the faults in artworks. The test artwork is heated and\nthe thermal response of the different layers is captured with the help of a\nthermal infrared camera. However, prolonged heating risks overheating and thus\ncausing damage to artworks and an alternate approach is to use pseudo-random\nbinary sequence excitations. The faults in the artwork, though, cannot be\ndetected on the captured images, especially if their strength is weak. The\nweaker faults are either masked by the stronger ones, by the pictorial layer of\nthe artwork or by the non-uniform heating. This work addresses the detection\nand localization of the faults through a wavelet based subspace decomposition\nscheme. The proposed scheme, on one hand, allows to remove the background\nwhile, on the other hand, removes the undesired high frequency noise. It is\nshown that the detection parameter is proportional to the diameter and the\ndepth of the fault. A criterion is proposed to select the optimal wavelet basis\nalong with suitable level selection for wavelet decomposition and\nreconstruction. The proposed approach is tested on a laboratory developed test\nsample with known fault locations and dimensions as well as real artworks. A\ncomparison with a previously reported method demonstrates the efficacy of the\nproposed approach for fault detection in artworks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 02:10:48 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Ahmad", "Muhammad Zubair", ""], ["Khan", "Amir Ali", ""], ["Mezghani", "Sihem", ""], ["Perrin", "Eric", ""], ["Mouhoubi", "Kamel", ""], ["Bodnar", "Jean-Luc", ""], ["Vrabie", "Valeriu", ""]]}, {"id": "1508.06073", "submitter": "Hilde Kuehne", "authors": "Hilde Kuehne and Juergen Gall and Thomas Serre", "title": "Cooking in the kitchen: Recognizing and Segmenting Human Activities in\n  Videos", "comments": "15 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As research on action recognition matures, the focus is shifting away from\ncategorizing basic task-oriented actions using hand-segmented video datasets to\nunderstanding complex goal-oriented daily human activities in real-world\nsettings. Temporally structured models would seem obvious to tackle this set of\nproblems, but so far, cases where these models have outperformed simpler\nunstructured bag-of-word types of models are scarce. With the increasing\navailability of large human activity datasets, combined with the development of\nnovel feature coding techniques that yield more compact representations, it is\ntime to revisit structured generative approaches.\n  Here, we describe an end-to-end generative approach from the encoding of\nfeatures to the structural modeling of complex human activities by applying\nFisher vectors and temporal models for the analysis of video sequences.\n  We systematically evaluate the proposed approach on several available\ndatasets (ADL, MPIICooking, and Breakfast datasets) using a variety of\nperformance metrics. Through extensive system evaluations, we demonstrate that\ncombining compact video representations based on Fisher Vectors with HMM-based\nmodeling yields very significant gains in accuracy and when properly trained\nwith sufficient training samples, structured temporal models outperform\nunstructured bag-of-word types of models by a large margin on the tested\nperformance metric.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 08:59:46 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 10:04:21 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Kuehne", "Hilde", ""], ["Gall", "Juergen", ""], ["Serre", "Thomas", ""]]}, {"id": "1508.06163", "submitter": "Guangliang Cheng", "authors": "Guangliang Cheng, Feiyun Zhu, Shiming Xiang and Chunhong Pan", "title": "Accurate Urban Road Centerline Extraction from VHR Imagery via\n  Multiscale Segmentation and Tensor Voting", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is very useful and increasingly popular to extract accurate road\ncenterlines from very-high-resolution (VHR) re- mote sensing imagery for\nvarious applications, such as road map generation and updating etc. There are\nthree shortcomings of current methods: (a) Due to the noise and occlusions\n(owing to vehicles and trees), most road extraction methods bring in\nheterogeneous classification results; (b) Morphological thinning algorithm is\nwidely used to extract road centerlines, while it pro- duces small spurs around\nthe centerlines; (c) Many methods are ineffective to extract centerlines around\nthe road intersections. To address the above three issues, we propose a novel\nmethod to ex- tract smooth and complete road centerlines via three techniques:\nthe multiscale joint collaborative representation (MJCR) & graph cuts (GC),\ntensor voting (TV) & non-maximum suppression (NMS) and fitting based connection\nalgorithm. Specifically, a MJCR-GC based road area segmentation method is\nproposed by incorporating mutiscale features and spatial information. In this\nway, a homogenous road segmentation result is achieved. Then, to obtain a\nsmooth and correct road centerline network, a TV-NMS based centerline\nextraction method is introduced. This method not only extracts smooth road\ncenterlines, but also connects the discontinuous road centerlines. Finally, to\novercome the ineffectiveness of current methods in the road intersection, a\nfitting based road centerline connection algorithm is proposed. As a result, we\ncan get a complete road centerline network. Extensive experiments on two\ndatasets demonstrate that our method achieves higher quantitative results, as\nwell as more satisfactory visual performances by comparing with state-of-the-\nart methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:18:34 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 15:29:25 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Cheng", "Guangliang", ""], ["Zhu", "Feiyun", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1508.06171", "submitter": "Changsoo Je", "authors": "Changsoo Je and Hyung-Min Park", "title": "BREN: Body Reflection Essence-Neuter Model for Separation of Reflection\n  Components", "comments": "4 pages, 4 figures", "journal-ref": "Optics Letters, Volume 40, Issue 9, pp. 1940-1943, May 1, 2015", "doi": "10.1364/OL.40.001940", "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel reflection color model consisting of body essence and\n(mixed) neuter, and present an effective method for separating dichromatic\nreflection components using a single image. Body essence is an entity invariant\nto interface reflection, and has two degrees of freedom unlike hue and maximum\nchromaticity. As a result, the proposed method is insensitive to noise and\nproper for colors around CMY (cyan, magenta, and yellow) as well as RGB (red,\ngreen, and blue), contrary to the maximum chromaticity-based methods. Interface\nreflection is separated by using a Gaussian function, which removes a critical\nthresholding problem. Furthermore, the method does not require any region\nsegmentation. Experimental results show the efficacy of the proposed model and\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:47:18 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Je", "Changsoo", ""], ["Park", "Hyung-Min", ""]]}, {"id": "1508.06264", "submitter": "Jingbin Wang", "authors": "Jingbin Wang, Haoxiang Wang, Yihua Zhou, Nancy McDonald", "title": "Multiple kernel multivariate performance learning using cutting plane\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-kernel classifier learning algorithm to\noptimize a given nonlinear and nonsmoonth multivariate classifier performance\nmeasure. Moreover, to solve the problem of kernel function selection and kernel\nparameter tuning, we proposed to construct an optimal kernel by weighted linear\ncombination of some candidate kernels. The learning of the classifier parameter\nand the kernel weight are unified in a single objective function considering to\nminimize the upper boundary of the given multivariate performance measure. The\nobjective function is optimized with regard to classifier parameter and kernel\nweight alternately in an iterative algorithm by using cutting plane algorithm.\nThe developed algorithm is evaluated on two different pattern classification\nmethods with regard to various multivariate performance measure optimization\nproblems. The experiment results show the proposed algorithm outperforms the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 19:45:06 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Wang", "Jingbin", ""], ["Wang", "Haoxiang", ""], ["Zhou", "Yihua", ""], ["McDonald", "Nancy", ""]]}, {"id": "1508.06464", "submitter": "Osamu Hirose", "authors": "Osamu Hirose, Shotaro Kawaguchi, Terumasa Tokunaga, Yu Toyoshima,\n  Takayuki Teramoto, Sayuri Kuge, Takeshi Ishihara, Yuichi Iino, Ryo Yoshida", "title": "SPF-CellTracker: Tracking multiple cells with strongly-correlated moves\n  using a spatial particle filter", "comments": "14 pages, 6 figures", "journal-ref": "IEEE/ACM Trans.Comput.Biol.Bioinform. 2017", "doi": "10.1109/TCBB.2017.2782255", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking many cells in time-lapse 3D image sequences is an important\nchallenging task of bioimage informatics. Motivated by a study of brain-wide 4D\nimaging of neural activity in C. elegans, we present a new method of multi-cell\ntracking. Data types to which the method is applicable are characterized as\nfollows: (i) cells are imaged as globular-like objects, (ii) it is difficult to\ndistinguish cells based only on shape and size, (iii) the number of imaged\ncells ranges in several hundreds, (iv) moves of nearly-located cells are\nstrongly correlated and (v) cells do not divide. We developed a tracking\nsoftware suite which we call SPF-CellTracker. Incorporating dependency on\ncells' moves into prediction model is the key to reduce the tracking errors:\ncell-switching and coalescence of tracked positions. We model target cells'\ncorrelated moves as a Markov random field and we also derive a fast computation\nalgorithm, which we call spatial particle filter. With the live-imaging data of\nnuclei of C. elegans neurons in which approximately 120 nuclei of neurons are\nimaged, we demonstrate an advantage of the proposed method over the standard\nparticle filter and a method developed by Tokunaga et al. (2014).\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 12:27:17 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 00:08:55 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 19:52:44 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Hirose", "Osamu", ""], ["Kawaguchi", "Shotaro", ""], ["Tokunaga", "Terumasa", ""], ["Toyoshima", "Yu", ""], ["Teramoto", "Takayuki", ""], ["Kuge", "Sayuri", ""], ["Ishihara", "Takeshi", ""], ["Iino", "Yuichi", ""], ["Yoshida", "Ryo", ""]]}, {"id": "1508.06535", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Deep Convolutional Neural Networks for Smile Recognition", "comments": "MSc thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis describes the design and implementation of a smile detector based\non deep convolutional neural networks. It starts with a summary of neural\nnetworks, the difficulties of training them and new training methods, such as\nRestricted Boltzmann Machines or autoencoders. It then provides a literature\nreview of convolutional neural networks and recurrent neural networks. In order\nto select databases for smile recognition, comprehensive statistics of\ndatabases popular in the field of facial expression recognition were generated\nand are summarized in this thesis. It then proposes a model for smile\ndetection, of which the main part is implemented. The experimental results are\ndiscussed in this thesis and justified based on a comprehensive model selection\nperformed. All experiments were run on a Tesla K40c GPU benefiting from a\nspeedup of up to factor 10 over the computations on a CPU. A smile detection\ntest accuracy of 99.45% is achieved for the Denver Intensity of Spontaneous\nFacial Action (DISFA) database, significantly outperforming existing approaches\nwith accuracies ranging from 65.55% to 79.67%. This experiment is re-run under\nvarious variations, such as retaining less neutral images or only the low or\nhigh intensities, of which the results are extensively compared.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 15:39:09 GMT"}], "update_date": "2015-08-27", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1508.06576", "submitter": "Leon Gatys", "authors": "Leon A. Gatys, Alexander S. Ecker and Matthias Bethge", "title": "A Neural Algorithm of Artistic Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:14:42 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 08:24:59 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Gatys", "Leon A.", ""], ["Ecker", "Alexander S.", ""], ["Bethge", "Matthias", ""]]}, {"id": "1508.06585", "submitter": "Galin Georgiev", "authors": "Galin Georgiev", "title": "Towards universal neural nets: Gibbs machines and ACE", "comments": "v5: added thermodynamic identities and variational error estimation;\n  expanded references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study from a physics viewpoint a class of generative neural nets, Gibbs\nmachines, designed for gradual learning. While including variational\nauto-encoders, they offer a broader universal platform for incrementally adding\nnewly learned features, including physical symmetries. Their direct connection\nto statistical physics and information geometry is established. A variational\nPythagorean theorem justifies invoking the exponential/Gibbs class of\nprobabilities for creating brand new objects. Combining these nets with\nclassifiers, gives rise to a brand of universal generative neural nets -\nstochastic auto-classifier-encoders (ACE). ACE have state-of-the-art\nperformance in their class, both for classification and density estimation for\nthe MNIST data set.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2015 17:43:08 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2015 21:49:06 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 03:35:59 GMT"}, {"version": "v4", "created": "Fri, 8 Apr 2016 22:11:23 GMT"}, {"version": "v5", "created": "Thu, 30 Jun 2016 06:26:34 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Georgiev", "Galin", ""]]}, {"id": "1508.06708", "submitter": "Sijin Li", "authors": "Sijin Li, Weichen Zhang, Antoni B. Chan", "title": "Maximum-Margin Structured Learning with Deep Networks for 3D Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on structured-output learning using deep neural networks\nfor 3D human pose estimation from monocular images. Our network takes an image\nand 3D pose as inputs and outputs a score value, which is high when the\nimage-pose pair matches and low otherwise. The network structure consists of a\nconvolutional neural network for image feature extraction, followed by two\nsub-networks for transforming the image features and pose into a joint\nembedding. The score function is then the dot-product between the image and\npose embeddings. The image-pose embedding and score function are jointly\ntrained using a maximum-margin cost function. Our proposed framework can be\ninterpreted as a special form of structured support vector machines where the\njoint feature space is discriminatively learned using deep neural networks. We\ntest our framework on the Human3.6m dataset and obtain state-of-the-art results\ncompared to other recent methods. Finally, we present visualizations of the\nimage-pose embedding space, demonstrating the network has learned a high-level\nembedding of body-orientation and pose-configuration.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 03:21:15 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Li", "Sijin", ""], ["Zhang", "Weichen", ""], ["Chan", "Antoni B.", ""]]}, {"id": "1508.06725", "submitter": "Ying Liu", "authors": "Ying Liu, Yan-bin Han, Yu-lin Zhang", "title": "Image Type Water Meter Character Recognition Based on Embedded DSP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we combined DSP processor with image processing algorithm and\nstudied the method of water meter character recognition. We collected water\nmeter image through camera at a fixed angle, and the projection method is used\nto recognize those digital images. The experiment results show that the method\ncan recognize the meter characters accurately and artificial meter reading is\nreplaced by automatic digital recognition, which improves working efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 05:36:00 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Liu", "Ying", ""], ["Han", "Yan-bin", ""], ["Zhang", "Yu-lin", ""]]}, {"id": "1508.06728", "submitter": "Mohini Sardey", "authors": "Mohini P. Sardey and G. K. Kharate", "title": "A Comparative Analysis of Retrieval Techniques In Content Based Image\n  Retrieval", "comments": "9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic group of visual techniques such as color, shape, texture are used in\nContent Based Image Retrievals (CBIR) to retrieve query image or subregion of\nimage to find similar images in image database. To improve query result,\nrelevance feedback is used many times in CBIR to help user to express their\npreference and improve query results.In this paper, a new approach for image\nretrieval is proposed which is based on the features such as Color Histogram,\nEigen Values and Match Point. Images from various types of database are first\nidentified by using edge detection techniques.Once the image is identified,\nthen the image is searched in the particular database, then all related images\nare displayed. This will save the retrieval time. Further to retrieve the\nprecise query image, any of the three techniques are used and comparison is\ndone w.r.t. average retrieval time. Eigen value technique found to be the best\nas compared with other two techniques\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 06:12:34 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Sardey", "Mohini P.", ""], ["Kharate", "G. K.", ""]]}, {"id": "1508.06853", "submitter": "Daniele Liciotti", "authors": "Daniele Liciotti, Marco Contigiani, Emanuele Frontoni, Adriano\n  Mancini, Primo Zingaretti, Valerio Placidi", "title": "Shopper Analytics: a customer activity recognition system using a\n  distributed RGB-D camera network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present an integrated system consisted of a RGB-D\ncamera and a software able to monitor shoppers in intelligent retail\nenvironments. We propose an innovative low cost smart system that can\nunderstand the shoppers' behavior and, in particular, their interactions with\nthe products in the shelves, with the aim to develop an automatic RGB-D\ntechnique for video analysis. The system of cameras detects the presence of\npeople and univocally identifies them. Through the depth frames, the system\ndetects the interactions of the shoppers with the products on the shelf and\ndetermines if a product is picked up or if the product is taken and then put\nback and finally, if there is not contact with the products. The system is low\ncost and easy to install, and experimental results demonstrated that its\nperformances are satisfactory also in real environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 13:31:09 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Liciotti", "Daniele", ""], ["Contigiani", "Marco", ""], ["Frontoni", "Emanuele", ""], ["Mancini", "Adriano", ""], ["Zingaretti", "Primo", ""], ["Placidi", "Valerio", ""]]}, {"id": "1508.06904", "submitter": "Markus Thom", "authors": "Markus Thom and Franz Gritschneder", "title": "Rapid Exact Signal Scanning with Deep Convolutional Neural Networks", "comments": "Pages 1-16 only: Copyright (c) 2016 IEEE. Personal use is permitted,\n  but republication/redistribution requires IEEE permission", "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 5, pp.\n  1235-1250 (2017)", "doi": "10.1109/TSP.2016.2631454", "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous formulation of the dynamics of a signal processing scheme aimed at\ndense signal scanning without any loss in accuracy is introduced and analyzed.\nRelated methods proposed in the recent past lack a satisfactory analysis of\nwhether they actually fulfill any exactness constraints. This is improved\nthrough an exact characterization of the requirements for a sound sliding\nwindow approach. The tools developed in this paper are especially beneficial if\nConvolutional Neural Networks are employed, but can also be used as a more\ngeneral framework to validate related approaches to signal scanning. The\nproposed theory helps to eliminate redundant computations and renders special\ncase treatment unnecessary, resulting in a dramatic boost in efficiency\nparticularly on massively parallel processors. This is demonstrated both\ntheoretically in a computational complexity analysis and empirically on modern\nparallel processors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 15:50:26 GMT"}, {"version": "v2", "created": "Wed, 23 Mar 2016 18:49:52 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 12:18:13 GMT"}, {"version": "v4", "created": "Fri, 3 Mar 2017 17:38:00 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 13:23:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Thom", "Markus", ""], ["Gritschneder", "Franz", ""]]}, {"id": "1508.06936", "submitter": "Alex Barnett", "authors": "Alex H. Barnett, Jeremy F. Magland, and Leslie F. Greengard", "title": "Validation of neural spike sorting algorithms without ground-truth\n  information", "comments": "22 pages, 7 figures; submitted to J. Neurosci. Meth", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a suite of validation metrics that assess the credibility of a\ngiven automatic spike sorting algorithm applied to a given electrophysiological\nrecording, when ground-truth is unavailable. By rerunning the spike sorter two\nor more times, the metrics measure stability under various perturbations\nconsistent with variations in the data itself, making no assumptions about the\nnoise model, nor about the internal workings of the sorting algorithm. Such\nstability is a prerequisite for reproducibility of results. We illustrate the\nmetrics on standard sorting algorithms for both in vivo and ex vivo recordings.\nWe believe that such metrics could reduce the significant human labor currently\nspent on validation, and should form an essential part of large-scale automated\nspike sorting and systematic benchmarking of algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 16:53:20 GMT"}], "update_date": "2015-08-28", "authors_parsed": [["Barnett", "Alex H.", ""], ["Magland", "Jeremy F.", ""], ["Greengard", "Leslie F.", ""]]}, {"id": "1508.07148", "submitter": "Thanh-Toan Do", "authors": "Thanh-Toan Do, Anh-Zung Doan, Ngai-Man Cheung", "title": "Discrete Hashing with Deep Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of learning binary hash codes for large\nscale image search by proposing a novel hashing method based on deep neural\nnetwork. The advantage of our deep model over previous deep model used in\nhashing is that our model contains necessary criteria for producing good codes\nsuch as similarity preserving, balance and independence. Another advantage of\nour method is that instead of relaxing the binary constraint of codes during\nthe learning process as most previous works, in this paper, by introducing the\nauxiliary variable, we reformulate the optimization into two sub-optimization\nsteps allowing us to efficiently solve binary constraints without any\nrelaxation.\n  The proposed method is also extended to the supervised hashing by leveraging\nthe label information such that the learned binary codes preserve the pairwise\nlabel of inputs.\n  The experimental results on three benchmark datasets show the proposed\nmethods outperform state-of-the-art hashing methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 09:38:05 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Do", "Thanh-Toan", ""], ["Doan", "Anh-Zung", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1508.07243", "submitter": "Tuomo Valkonen", "authors": "J.C. De los Reyes, C.-B. Sch\\\"onlieb, T. Valkonen", "title": "Bilevel parameter learning for higher-order total variation\n  regularisation models", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0662-8", "report-no": null, "categories": "math.OC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a bilevel optimisation approach for parameter learning in\nhigher-order total variation image reconstruction models. Apart from the least\nsquares cost functional, naturally used in bilevel learning, we propose and\nanalyse an alternative cost, based on a Huber regularised TV-seminorm.\nDifferentiability properties of the solution operator are verified and a\nfirst-order optimality system is derived. Based on the adjoint information, a\nquasi-Newton algorithm is proposed for the numerical solution of the bilevel\nproblems. Numerical experiments are carried out to show the suitability of our\napproach and the improved performance of the new cost functional. Thanks to the\nbilevel optimisation framework, also a detailed comparison between TGV$^2$ and\nICTV is carried out, showing the advantages and shortcomings of both\nregularisers, depending on the structure of the processed images and their\nnoise level.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 15:36:15 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Reyes", "J. C. De los", ""], ["Sch\u00f6nlieb", "C. -B.", ""], ["Valkonen", "T.", ""]]}, {"id": "1508.07415", "submitter": "Nasser Eslahi", "authors": "Nasser Eslahi, Hami Mahdavinataj, Ali Aghagolzadeh", "title": "Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via\n  Adaptive Local and Nonlocal Statistical Priors", "comments": "11 Pages, 7 Figures, 2 Tables, In Proceeding of 9th Iranian Conf.\n  Machine Vis. Image Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation of this paper is to introduce a novel framework for the\nrestoration of images corrupted by mixed Gaussian-impulse noise. To this aim,\nfirst, an adaptive curvelet thresholding criterion is proposed which tries to\nadaptively remove the perturbations appeared during denoising process. Then, a\nnew statistical regularization term, called joint adaptive statistical prior\n(JASP), is established which enforces both the local and nonlocal statistical\nconsistencies, simultaneously, in a unified manner. Furthermore, a novel\ntechnique for mixed Gaussian plus impulse noise removal using JASP in a\nvariational scheme is developed--we refer to it as De-JASP. To efficiently\nsolve the above variational scheme, an efficient alternating minimization\nalgorithm based on split Bregman iterative framework is developed. Extensive\nexperimental results manifest the effectiveness of the proposed method\ncomparing with the current state-of-the-art methods in mixed Gaussian-impulse\nnoise removal.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 08:08:19 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2015 08:03:10 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Eslahi", "Nasser", ""], ["Mahdavinataj", "Hami", ""], ["Aghagolzadeh", "Ali", ""]]}, {"id": "1508.07468", "submitter": "Yuqing Hou", "authors": "Yuqing Hou", "title": "Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation\n  and Inhomogeneous Errors", "comments": "This paper has been withdrawn by the author to update more\n  experiments and some errors in the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tag-based image retrieval (TBIR) has drawn much attention in recent years due\nto the explosive amount of digital images and crowdsourcing tags. However, TBIR\nis still suffering from the incomplete and inaccurate tags provided by users,\nposing a great challenge for tag-based image management applications. In this\nwork, we proposed a novel method for image annotation, incorporating several\npriors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors.\nHighly representative CNN feature vectors are adopt to model the tag-visual\ncorrelation and narrow the semantic gap. And we extract word vectors for tags\nto measure similarity between tags in the semantic level, which is more\naccurate than traditional frequency-based or graph-based methods. We utilize\nthe accelerated proximal gradient (APG) method to solve our model efficiently.\nExtensive experiments conducted on multiple benchmark datasets demonstrate the\neffectiveness and robustness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2015 15:47:20 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 04:43:51 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 02:15:36 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Hou", "Yuqing", ""]]}, {"id": "1508.07569", "submitter": "Pui Tung Choi", "authors": "Gary Pui-Tung Choi, Kin Tat Ho, Lok Ming Lui", "title": "Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 9, 1582-1618 (2016)", "doi": "10.1137/15M1037561", "report-no": null, "categories": "cs.CG cs.CV cs.GR math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud is the most fundamental representation of 3D geometric objects.\nAnalyzing and processing point cloud surfaces is important in computer graphics\nand computer vision. However, most of the existing algorithms for surface\nanalysis require connectivity information. Therefore, it is desirable to\ndevelop a mesh structure on point clouds. This task can be simplified with the\naid of a parameterization. In particular, conformal parameterizations are\nadvantageous in preserving the geometric information of the point cloud data.\nIn this paper, we extend a state-of-the-art spherical conformal\nparameterization algorithm for genus-0 closed meshes to the case of point\nclouds, using an improved approximation of the Laplace-Beltrami operator on\ndata points. Then, we propose an iterative scheme called the North-South\nreiteration for achieving a spherical conformal parameterization. A balancing\nscheme is introduced to enhance the distribution of the spherical\nparameterization. High quality triangulations and quadrangulations can then be\nbuilt on the point clouds with the aid of the parameterizations. Also, the\nmeshes generated are guaranteed to be genus-0 closed meshes. Moreover, using\nour proposed spherical conformal parameterization, multilevel representations\nof point clouds can be easily constructed. Experimental results demonstrate the\neffectiveness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 13:42:43 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2016 06:05:49 GMT"}, {"version": "v3", "created": "Wed, 16 Mar 2016 08:47:28 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Choi", "Gary Pui-Tung", ""], ["Ho", "Kin Tat", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1508.07647", "submitter": "Lamberto Ballan", "authors": "Justin Johnson and Lamberto Ballan and Fei-Fei Li", "title": "Love Thy Neighbors: Image Annotation by Exploiting Image Metadata", "comments": "Accepted to ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some images that are difficult to recognize on their own may become more\nclear in the context of a neighborhood of related images with similar\nsocial-network metadata. We build on this intuition to improve multilabel image\nannotation. Our model uses image metadata nonparametrically to generate\nneighborhoods of related images using Jaccard similarities, then uses a deep\nneural network to blend visual information from the image and its neighbors.\nPrior work typically models image metadata parametrically, in contrast, our\nnonparametric treatment allows our model to perform well even when the\nvocabulary of metadata changes between training and testing. We perform\ncomprehensive experiments on the NUS-WIDE dataset, where we show that our model\noutperforms state-of-the-art methods for multilabel image annotation even when\nour model is forced to generalize to new types of metadata.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2015 23:34:13 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2015 00:12:06 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Johnson", "Justin", ""], ["Ballan", "Lamberto", ""], ["Li", "Fei-Fei", ""]]}, {"id": "1508.07654", "submitter": "Tian Lan", "authors": "Tian Lan, Yuke Zhu, Amir Roshan Zamir, Silvio Savarese", "title": "Action Recognition by Hierarchical Mid-level Action Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic videos of human actions exhibit rich spatiotemporal structures at\nmultiple levels of granularity: an action can always be decomposed into\nmultiple finer-grained elements in both space and time. To capture this\nintuition, we propose to represent videos by a hierarchy of mid-level action\nelements (MAEs), where each MAE corresponds to an action-related spatiotemporal\nsegment in the video. We introduce an unsupervised method to generate this\nrepresentation from videos. Our method is capable of distinguishing\naction-related segments from background segments and representing actions at\nmultiple spatiotemporal resolutions. Given a set of spatiotemporal segments\ngenerated from the training data, we introduce a discriminative clustering\nalgorithm that automatically discovers MAEs at multiple levels of granularity.\nWe develop structured models that capture a rich set of spatial, temporal and\nhierarchical relations among the segments, where the action label and multiple\nlevels of MAE labels are jointly inferred. The proposed model achieves\nstate-of-the-art performance in multiple action recognition benchmarks.\nMoreover, we demonstrate the effectiveness of our model in real-world\napplications such as action recognition in large-scale untrimmed videos and\naction parsing.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 00:56:20 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Lan", "Tian", ""], ["Zhu", "Yuke", ""], ["Zamir", "Amir Roshan", ""], ["Savarese", "Silvio", ""]]}, {"id": "1508.07680", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi", "title": "Domain Generalization for Object Recognition with Multi-task\n  Autoencoders", "comments": "accepted in ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of domain generalization is to take knowledge acquired from a\nnumber of related domains where training data is available, and to then\nsuccessfully apply it to previously unseen domains. We propose a new feature\nlearning algorithm, Multi-Task Autoencoder (MTAE), that provides good\ngeneralization performance for cross-domain object recognition.\n  Our algorithm extends the standard denoising autoencoder framework by\nsubstituting artificially induced corruption with naturally occurring\ninter-domain variability in the appearance of objects. Instead of\nreconstructing images from noisy versions, MTAE learns to transform the\noriginal image into analogs in multiple related domains. It thereby learns\nfeatures that are robust to variations across domains. The learnt features are\nthen used as inputs to a classifier.\n  We evaluated the performance of the algorithm on benchmark image recognition\ndatasets, where the task is to learn features from multiple datasets and to\nthen predict the image label from unseen datasets. We found that (denoising)\nMTAE outperforms alternative autoencoder-based models as well as the current\nstate-of-the-art algorithms for domain generalization.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 04:15:31 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""]]}, {"id": "1508.07859", "submitter": "Changsoo Je", "authors": "Changsoo Je, Kwang Hee Lee, Sang Wook Lee", "title": "Multi-Projector Color Structured-Light Vision", "comments": "25 pages, 13 figures", "journal-ref": "Signal Processing: Image Communication, Volume 28, Issue 9, pp.\n  1046-1058, October, 2013", "doi": "10.1016/j.image.2013.05.005", "report-no": null, "categories": "cs.CV cs.GR physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research interest in rapid structured-light imaging has grown increasingly\nfor the modeling of moving objects, and a number of methods have been suggested\nfor the range capture in a single video frame. The imaging area of a 3D object\nusing a single projector is restricted since the structured light is projected\nonly onto a limited area of the object surface. Employing additional projectors\nto broaden the imaging area is a challenging problem since simultaneous\nprojection of multiple patterns results in their superposition in the\nlight-intersected areas and the recognition of original patterns is by no means\ntrivial. This paper presents a novel method of multi-projector color\nstructured-light vision based on projector-camera triangulation. By analyzing\nthe behavior of superposed-light colors in a chromaticity domain, we show that\nthe original light colors cannot be properly extracted by the conventional\ndirect estimation. We disambiguate multiple projectors by multiplexing the\norientations of projector patterns so that the superposed patterns can be\nseparated by explicit derivative computations. Experimental studies are carried\nout to demonstrate the validity of the presented method. The proposed method\nincreases the efficiency of range acquisition compared to conventional active\nstereo using multiple projectors.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 14:55:59 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Je", "Changsoo", ""], ["Lee", "Kwang Hee", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1508.07902", "submitter": "Alexander Shekhovtsov", "authors": "Alexander Shekhovtsov, Paul Swoboda, Bogdan Savchynskyy", "title": "Maximum Persistency via Iterative Relaxed Inference with Graphical\n  Models", "comments": "Reworked version, submitted to PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the NP-hard problem of MAP-inference for undirected discrete\ngraphical models. We propose a polynomial time and practically efficient\nalgorithm for finding a part of its optimal solution. Specifically, our\nalgorithm marks some labels of the considered graphical model either as (i)\noptimal, meaning that they belong to all optimal solutions of the inference\nproblem; (ii) non-optimal if they provably do not belong to any solution. With\naccess to an exact solver of a linear programming relaxation to the\nMAP-inference problem, our algorithm marks the maximal possible (in a specified\nsense) number of labels. We also present a version of the algorithm, which has\naccess to a suboptimal dual solver only and still can ensure the\n(non-)optimality for the marked labels, although the overall number of the\nmarked labels may decrease. We propose an efficient implementation, which runs\nin time comparable to a single run of a suboptimal dual solver. Our method is\nwell-scalable and shows state-of-the-art results on computational benchmarks\nfrom machine learning and computer vision.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 16:28:55 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2015 23:30:53 GMT"}, {"version": "v3", "created": "Fri, 3 Feb 2017 13:21:01 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Shekhovtsov", "Alexander", ""], ["Swoboda", "Paul", ""], ["Savchynskyy", "Bogdan", ""]]}, {"id": "1508.07953", "submitter": "Lihi Zelnik-Manor", "authors": "Nir Ben-Zrihem and Lihi Zelnik-Manor", "title": "Approximate Nearest Neighbor Fields in Video", "comments": "A CVPR 2015 oral paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search),\nan algorithm for matching patches of a video to a set of reference patches in\nreal-time. For each query, RIANN finds potential matches by intersecting rings\naround key points in appearance space. Its search complexity is reversely\ncorrelated to the amount of temporal change, making it a good fit for videos,\nwhere typically most patches change slowly with time. Experiments show that\nRIANN is up to two orders of magnitude faster than previous ANN methods, and is\nthe only solution that operates in real-time. We further demonstrate how RIANN\ncan be used for real-time video processing and provide examples for a range of\nreal-time video applications, including colorization, denoising, and several\nartistic effects.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2015 18:43:31 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ben-Zrihem", "Nir", ""], ["Zelnik-Manor", "Lihi", ""]]}]