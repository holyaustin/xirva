[{"id": "1807.00046", "submitter": "Jelena Milosevic", "authors": "Jelena Milosevic, Dexmont Pena, Andrew Forembsky, David Moloney,\n  Miroslaw Malek", "title": "It All Matters: Reporting Accuracy, Inference Time and Power Consumption\n  for Face Emotion Recognition on Embedded Systems", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several approaches to face emotion recognition task are proposed in\nliterature, none of them reports on power consumption nor inference time\nrequired to run the system in an embedded environment. Without adequate\nknowledge about these factors it is not clear whether we are actually able to\nprovide accurate face emotion recognition in the embedded environment or not,\nand if not, how far we are from making it feasible and what are the biggest\nbottlenecks we face.\n  The main goal of this paper is to answer these questions and to convey the\nmessage that instead of reporting only detection accuracy also power\nconsumption and inference time should be reported as real usability of the\nproposed systems and their adoption in human computer interaction strongly\ndepends on it. In this paper, we identify the state-of-the art face emotion\nrecognition methods that are potentially suitable for embedded environment and\nthe most frequently used datasets for this task. Our study shows that most of\nthe performed experiments use datasets with posed expressions or in a\nparticular experimental setup with special conditions for image collection.\nSince our goal is to evaluate the performance of the identified promising\nmethods in the realistic scenario, we collect a new dataset with\nnon-exaggerated emotions and we use it, in addition to the publicly available\ndatasets, for the evaluation of detection accuracy, power consumption and\ninference time on three frequently used embedded devices with different\ncomputational capabilities. Our results show that gray images are still more\nsuitable for embedded environment than color ones and that for most of the\nanalyzed systems either inference time or energy consumption or both are\nlimiting factor for their adoption in real-life embedded applications.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 19:47:24 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Milosevic", "Jelena", ""], ["Pena", "Dexmont", ""], ["Forembsky", "Andrew", ""], ["Moloney", "David", ""], ["Malek", "Miroslaw", ""]]}, {"id": "1807.00053", "submitter": "Aran Nayebi", "authors": "Aran Nayebi, Daniel Bear, Jonas Kubilius, Kohitij Kar, Surya Ganguli,\n  David Sussillo, James J. DiCarlo, Daniel L. K. Yamins", "title": "Task-Driven Convolutional Recurrent Models of the Visual System", "comments": "NIPS 2018 Camera Ready Version, 16 pages including supplementary\n  information, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feed-forward convolutional neural networks (CNNs) are currently\nstate-of-the-art for object classification tasks such as ImageNet. Further,\nthey are quantitatively accurate models of temporally-averaged responses of\nneurons in the primate brain's visual system. However, biological visual\nsystems have two ubiquitous architectural features not shared with typical\nCNNs: local recurrence within cortical areas, and long-range feedback from\ndownstream areas to upstream areas. Here we explored the role of recurrence in\nimproving classification performance. We found that standard forms of\nrecurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the\nImageNet task. In contrast, novel cells that incorporated two structural\nfeatures, bypassing and gating, were able to boost task accuracy substantially.\nWe extended these design principles in an automated search over thousands of\nmodel architectures, which identified novel local recurrent cells and\nlong-range feedback connections useful for object recognition. Moreover, these\ntask-optimized ConvRNNs matched the dynamics of neural activity in the primate\nvisual system better than feedforward networks, suggesting a role for the\nbrain's recurrent connections in performing difficult visual behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 20:27:23 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 03:49:01 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Nayebi", "Aran", ""], ["Bear", "Daniel", ""], ["Kubilius", "Jonas", ""], ["Kar", "Kohitij", ""], ["Ganguli", "Surya", ""], ["Sussillo", "David", ""], ["DiCarlo", "James J.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1807.00094", "submitter": "Min Zhang", "authors": "Min Zhang, Qianli Ma, Chengfeng Wen, Hai Chen, Deruo Liu, Xianfeng Gu,\n  Jie He, Xiaoyin Xu", "title": "Classification of lung nodules in CT images based on Wasserstein\n  distance in differential geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodules are commonly detected in screening for patients with a risk for\nlung cancer. Though the status of large nodules can be easily diagnosed by fine\nneedle biopsy or bronchoscopy, small nodules are often difficult to classify on\ncomputed tomography (CT). Recent works have shown that shape analysis of lung\nnodules can be used to differentiate benign lesions from malignant ones, though\nexisting methods are limited in their sensitivity and specificity. In this work\nwe introduced a new 3D shape analysis within the framework of differential\ngeometry to calculate the Wasserstein distance between benign and malignant\nlung nodules to derive an accurate classification scheme. The Wasserstein\ndistance between the nodules is calculated based on our new spherical optimal\nmass transport, this new algorithm works directly on sphere by using spherical\nmetric, which is much more accurate and efficient than previous methods. In the\nprocess of deformation, the area-distortion factor gives a probability measure\non the unit sphere, which forms the Wasserstein space. From known cases of\nbenign and malignant lung nodules, we can calculate a unique optimal mass\ntransport map between their correspondingly deformed Wasserstein spaces. This\ntransportation cost defines the Wasserstein distance between them and can be\nused to classify new lung nodules into either the benign or malignant class. To\nthe best of our knowledge, this is the first work that utilizes Wasserstein\ndistance for lung nodule classification. The advantages of Wasserstein distance\nare it is invariant under rigid motions and scalings, thus it intrinsically\nmeasures shape distance even when the underlying shapes are of high complexity,\nmaking it well suited to classify lung nodules as they have different sizes,\norientations, and appearances.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 00:30:53 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhang", "Min", ""], ["Ma", "Qianli", ""], ["Wen", "Chengfeng", ""], ["Chen", "Hai", ""], ["Liu", "Deruo", ""], ["Gu", "Xianfeng", ""], ["He", "Jie", ""], ["Xu", "Xiaoyin", ""]]}, {"id": "1807.00119", "submitter": "Yong Liu", "authors": "Yong Liu and Ruiping Wang and Shiguang Shan and Xilin Chen", "title": "Structure Inference Net: Object Detection Using Scene-Level Context and\n  Instance-Level Relationships", "comments": "published in CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context is important for accurate visual recognition. In this work we propose\nan object detection algorithm that not only considers object visual appearance,\nbut also makes use of two kinds of context including scene contextual\ninformation and object relationships within a single image. Therefore, object\ndetection is regarded as both a cognition problem and a reasoning problem when\nleveraging these structured information. Specifically, this paper formulates\nobject detection as a problem of graph structure inference, where given an\nimage the objects are treated as nodes in a graph and relationships between the\nobjects are modeled as edges in such graph. To this end, we present a so-called\nStructure Inference Network (SIN), a detector that incorporates into a typical\ndetection framework (e.g. Faster R-CNN) with a graphical model which aims to\ninfer object state. Comprehensive experiments on PASCAL VOC and MS COCO\ndatasets indicate that scene context and object relationships truly improve the\nperformance of object detection with more desirable and reasonable outputs.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 03:33:55 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Liu", "Yong", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1807.00141", "submitter": "Jiasong Wu", "authors": "Li Liu, Jiasong Wu, Dengwang Li, Lotfi Senhadji, Huazhong Shu", "title": "Fractional Wavelet Scattering Network and Applications", "comments": "11 pages, 6 figures, 3 tables, IEEE Transactions on Biomedical\n  Engineering, 2018", "journal-ref": null, "doi": "10.1109/TBME.2018.2850356", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The present study introduces a fractional wavelet scattering\nnetwork (FrScatNet), which is a generalized translation invariant version of\nthe classical wavelet scattering network (ScatNet). Methods: In our approach,\nthe FrScatNet is constructed based on the fractional wavelet transform (FRWT).\nThe fractional scattering coefficients are iteratively computed using FRWTs and\nmodulus operators. The feature vectors constructed by fractional scattering\ncoefficients are usually used for signal classification. In this work, an\napplication example of FrScatNet is provided in order to assess its performance\non pathological images. Firstly, the FrScatNet extracts feature vectors from\npatches of the original histological images under different orders. Then we\nclassify those patches into target (benign or malignant) and background groups.\nAnd the FrScatNet property is analyzed by comparing error rates computed from\ndifferent fractional orders respectively. Based on the above pathological image\nclassification, a gland segmentation algorithm is proposed by combining the\nboundary information and the gland location. Results: The error rates for\ndifferent fractional orders of FrScatNet are examined and show that the\nclassification accuracy is significantly improved in fractional scattering\ndomain. We also compare the FrScatNet based gland segmentation method with\nthose proposed in the 2015 MICCAI Gland Segmentation Challenge and our method\nachieves comparable results. Conclusion: The FrScatNet is shown to achieve\naccurate and robust results. More stable and discriminative fractional\nscattering coefficients are obtained by the FrScatNet in this work.\nSignificance: The added fractional order parameter is able to analyze the image\nin the fractional scattering domain.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 08:38:22 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Liu", "Li", ""], ["Wu", "Jiasong", ""], ["Li", "Dengwang", ""], ["Senhadji", "Lotfi", ""], ["Shu", "Huazhong", ""]]}, {"id": "1807.00147", "submitter": "Liang Lin", "authors": "Keze Wang and Liang Lin and Xiaopeng Yan and Ziliang Chen and Dongyu\n  Zhang and Lei Zhang", "title": "Cost-effective Object Detection: Active Sample Mining with Switchable\n  Selection Criteria", "comments": "Automatically determining whether an unlabeled sample should be\n  manually annotated or pseudo-labeled via a novel self-learning process\n  (Accepted by TNNLS 2018) The source code is available at\n  http://kezewang.com/codes/ASM_ver1.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though quite challenging, leveraging large-scale unlabeled or partially\nlabeled data in learning systems (e.g., model/classifier training) has\nattracted increasing attentions due to its fundamental importance. To address\nthis problem, many active learning (AL) methods have been proposed that employ\nup-to-date detectors to retrieve representative minority samples according to\npredefined confidence or uncertainty thresholds. However, these AL methods\ncause the detectors to ignore the remaining majority samples (i.e., those with\nlow uncertainty or high prediction confidence). In this work, by developing a\nprincipled active sample mining (ASM) framework, we demonstrate that\ncost-effectively mining samples from these unlabeled majority data is key to\ntraining more powerful object detectors while minimizing user effort.\nSpecifically, our ASM framework involves a switchable sample selection\nmechanism for determining whether an unlabeled sample should be manually\nannotated via AL or automatically pseudo-labeled via a novel self-learning\nprocess. The proposed process can be compatible with mini-batch based training\n(i.e., using a batch of unlabeled or partially labeled data as a one-time\ninput) for object detection. In addition, a few samples with low-confidence\npredictions are selected and annotated via AL. Notably, our method is suitable\nfor object categories that are not seen in the unlabeled data during the\nlearning process. Extensive experiments clearly demonstrate that our ASM\nframework can achieve performance comparable to that of alternative methods but\nwith significantly fewer annotations.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 09:07:33 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 03:25:18 GMT"}, {"version": "v3", "created": "Sat, 12 Jan 2019 03:32:19 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Wang", "Keze", ""], ["Lin", "Liang", ""], ["Yan", "Xiaopeng", ""], ["Chen", "Ziliang", ""], ["Zhang", "Dongyu", ""], ["Zhang", "Lei", ""]]}, {"id": "1807.00202", "submitter": "Yu Liu", "authors": "Yu Liu, Guanlong Zhao, Boyuan Gong, Yang Li, Ritu Raj, Niraj Goel,\n  Satya Kesav, Sandeep Gottimukkala, Zhangyang Wang, Wenqi Ren, Dacheng Tao", "title": "Improved Techniques for Learning to Dehaze and Beyond: A Collective\n  Study", "comments": "updated: typo fixed and some other improvements on writing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we explore two related but important tasks based on the recently\nreleased REalistic Single Image DEhazing (RESIDE) benchmark dataset: (i) single\nimage dehazing as a low-level image restoration problem; and (ii) high-level\nvisual understanding (e.g., object detection) of hazy images. For the first\ntask, we investigated a variety of loss functions and show that\nperception-driven loss significantly improves dehazing performance. In the\nsecond task, we provide multiple solutions including using advanced modules in\nthe dehazing-detection cascade and domain-adaptive object detectors. In both\ntasks, our proposed solutions significantly improve performance. GitHub\nrepository URL is: https://github.com/guanlongzhao/dehaze\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 16:52:33 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 03:58:30 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Liu", "Yu", ""], ["Zhao", "Guanlong", ""], ["Gong", "Boyuan", ""], ["Li", "Yang", ""], ["Raj", "Ritu", ""], ["Goel", "Niraj", ""], ["Kesav", "Satya", ""], ["Gottimukkala", "Sandeep", ""], ["Wang", "Zhangyang", ""], ["Ren", "Wenqi", ""], ["Tao", "Dacheng", ""]]}, {"id": "1807.00230", "submitter": "Bruno Korbar", "authors": "Bruno Korbar, Du Tran, Lorenzo Torresani", "title": "Cooperative Learning of Audio and Video Models from Self-Supervised\n  Synchronization", "comments": "Note: Changed name - added experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a natural correlation between the visual and auditive elements of a\nvideo. In this work we leverage this connection to learn general and effective\nmodels for both audio and video analysis from self-supervised temporal\nsynchronization. We demonstrate that a calibrated curriculum learning scheme, a\ncareful choice of negative examples, and the use of a contrastive loss are\ncritical ingredients to obtain powerful multi-sensory representations from\nmodels optimized to discern temporal synchronization of audio-video pairs.\nWithout further finetuning, the resulting audio features achieve performance\nsuperior or comparable to the state-of-the-art on established audio\nclassification benchmarks (DCASE2014 and ESC-50). At the same time, our visual\nsubnet provides a very effective initialization to improve the accuracy of\nvideo-based action recognition models: compared to learning from scratch, our\nself-supervised pretraining yields a remarkable gain of +19.9% in action\nrecognition accuracy on UCF101 and a boost of +17.7% on HMDB51.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 21:50:21 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 22:20:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Korbar", "Bruno", ""], ["Tran", "Du", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1807.00273", "submitter": "Rahul Iyer", "authors": "Michael Honke, Rahul Iyer and Dishant Mittal", "title": "Photorealistic Style Transfer for Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photorealistic style transfer is a technique which transfers colour from one\nreference domain to another domain by using deep learning and optimization\ntechniques. Here, we present a technique which we use to transfer style and\ncolour from a reference image to a video.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 05:28:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Honke", "Michael", ""], ["Iyer", "Rahul", ""], ["Mittal", "Dishant", ""]]}, {"id": "1807.00275", "submitter": "Fangchang Ma", "authors": "Fangchang Ma, Guilherme Venturelli Cavalheiro, Sertac Karaman", "title": "Self-supervised Sparse-to-Dense: Self-supervised Depth Completion from\n  LiDAR and Monocular Camera", "comments": "Software:\n  https://github.com/fangchangma/self-supervised-depth-completion . Video:\n  https://youtu.be/bGXfvF261pc . 12 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth completion, the technique of estimating a dense depth image from sparse\ndepth measurements, has a variety of applications in robotics and autonomous\ndriving. However, depth completion faces 3 main challenges: the irregularly\nspaced pattern in the sparse depth input, the difficulty in handling multiple\nsensor modalities (when color images are available), as well as the lack of\ndense, pixel-level ground truth depth labels. In this work, we address all\nthese challenges. Specifically, we develop a deep regression model to learn a\ndirect mapping from sparse depth (and color images) to dense depth. We also\npropose a self-supervised training framework that requires only sequences of\ncolor and sparse depth images, without the need for dense depth labels. Our\nexperiments demonstrate that our network, when trained with semi-dense\nannotations, attains state-of-the- art accuracy and is the winning approach on\nthe KITTI depth completion benchmark at the time of submission. Furthermore,\nthe self-supervised framework outperforms a number of existing solutions\ntrained with semi- dense annotations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 06:02:48 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 00:47:09 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ma", "Fangchang", ""], ["Cavalheiro", "Guilherme Venturelli", ""], ["Karaman", "Sertac", ""]]}, {"id": "1807.00284", "submitter": "Yong Xia", "authors": "Benteng Ma, Yong Xia", "title": "Autonomous Deep Learning: A Genetic DCNN Designer for Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the breakthrough success of deep convolutional\nneural networks (DCNNs) in image classification and other vision applications.\nAlthough freeing users from the troublesome handcrafted feature extraction by\nproviding a uniform feature extraction-classification framework, DCNNs still\nrequire a handcrafted design of their architectures. In this paper, we propose\nthe genetic DCNN designer, an autonomous learning algorithm can generate a DCNN\narchitecture automatically based on the data available for a specific image\nclassification problem. We first partition a DCNN into multiple stacked meta\nconvolutional blocks and fully connected blocks, each containing the operations\nof convolution, pooling, fully connection, batch normalization, activation and\ndrop out, and thus convert the architecture into an integer vector. Then, we\nuse refined evolutionary operations, including selection, mutation and\ncrossover to evolve a population of DCNN architectures. Our results on the\nMNIST, Fashion-MNIST, EMNISTDigit, EMNIST-Letter, CIFAR10 and CIFAR100 datasets\nsuggest that the proposed genetic DCNN designer is able to produce\nautomatically DCNN architectures, whose performance is comparable to, if not\nbetter than, that of stateof- the-art DCNN models\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 07:11:54 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ma", "Benteng", ""], ["Xia", "Yong", ""]]}, {"id": "1807.00301", "submitter": "Julian Faraone", "authors": "Julian Faraone, Nicholas Fraser, Michaela Blott and Philip H.W. Leong", "title": "SYQ: Learning Symmetric Quantization For Efficient Deep Neural Networks", "comments": "Published as a conference paper at the 2018 IEEE Conference on\n  Computer Vision and Pattern Recognition (CVPR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for state-of-the-art deep neural networks is computationally\nexpensive, making them difficult to deploy on constrained hardware\nenvironments. An efficient way to reduce this complexity is to quantize the\nweight parameters and/or activations during training by approximating their\ndistributions with a limited entry codebook. For very low-precisions, such as\nbinary or ternary networks with 1-8-bit activations, the information loss from\nquantization leads to significant accuracy degradation due to large gradient\nmismatches between the forward and backward functions. In this paper, we\nintroduce a quantization method to reduce this loss by learning a symmetric\ncodebook for particular weight subgroups. These subgroups are determined based\non their locality in the weight matrix, such that the hardware simplicity of\nthe low-precision representations is preserved. Empirically, we show that\nsymmetric quantization can substantially improve accuracy for networks with\nextremely low-precision weights and activations. We also demonstrate that this\nrepresentation imposes minimal or no hardware implications to more\ncoarse-grained approaches. Source code is available at\nhttps://www.github.com/julianfaraone/SYQ.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 09:29:19 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Faraone", "Julian", ""], ["Fraser", "Nicholas", ""], ["Blott", "Michaela", ""], ["Leong", "Philip H. W.", ""]]}, {"id": "1807.00340", "submitter": "Pengqian Yu", "authors": "Xinhan Di, Pengqian Yu, Meng Tian", "title": "Towards Adversarial Training with Moderate Performance Improvement for\n  Neural Network Classification", "comments": "Accepted for publication in Uncertainty in Deep Learning Workshop at\n  Uncertainty in Artificial Intelligence (UAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been demonstrated that deep neural networks are prone to noisy\nexamples particular adversarial samples during inference process. The gap\nbetween robust deep learning systems in real world applications and vulnerable\nneural networks is still large. Current adversarial training strategies improve\nthe robustness against adversarial samples. However, these methods lead to\naccuracy reduction when the input examples are clean thus hinders the\npracticability. In this paper, we investigate an approach that protects the\nneural network classification from the adversarial samples and improves its\naccuracy when the input examples are clean. We demonstrate the versatility and\neffectiveness of our proposed approach on a variety of different networks and\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 15:08:52 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Tian", "Meng", ""]]}, {"id": "1807.00399", "submitter": "Konstantinos Zampogiannis", "authors": "Konstantinos Zampogiannis, Cornelia Fermuller, Yiannis Aloimonos", "title": "cilantro: A Lean, Versatile, and Efficient Library for Point Cloud Data\n  Processing", "comments": null, "journal-ref": null, "doi": "10.1145/3240508.3243655", "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce cilantro, an open-source C++ library for geometric and\ngeneral-purpose point cloud data processing. The library provides functionality\nthat covers low-level point cloud operations, spatial reasoning, various\nmethods for point cloud segmentation and generic data clustering, flexible\nalgorithms for robust or local geometric alignment, model fitting, as well as\npowerful visualization tools. To accommodate all kinds of workflows, cilantro\nis almost fully templated, and most of its generic algorithms operate in\narbitrary data dimension. At the same time, the library is easy to use and\nhighly expressive, promoting a clean and concise coding style. cilantro is\nhighly optimized, has a minimal set of external dependencies, and supports\nrapid development of performant point cloud processing software in a wide\nvariety of contexts.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 21:47:51 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 17:51:26 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 17:30:17 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zampogiannis", "Konstantinos", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1807.00431", "submitter": "John Zech", "authors": "John R. Zech, Marcus A. Badgeley, Manway Liu, Anthony B. Costa, Joseph\n  J. Titano, Eric K. Oermann", "title": "Confounding variables can degrade generalization performance of\n  radiological deep learning models", "comments": null, "journal-ref": "PLoS Med 15(11):e1002683 (2019)", "doi": "10.1371/journal.pmed.1002683", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early results in using convolutional neural networks (CNNs) on x-rays to\ndiagnose disease have been promising, but it has not yet been shown that models\ntrained on x-rays from one hospital or one group of hospitals will work equally\nwell at different hospitals. Before these tools are used for computer-aided\ndiagnosis in real-world clinical settings, we must verify their ability to\ngeneralize across a variety of hospital systems. A cross-sectional design was\nused to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays\nfrom NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904\npatients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural\ncomparisons, performance on chest x-rays from outside hospitals was\nsignificantly lower than on held-out x-rays from the original hospital systems.\nCNNs were able to detect where an x-ray was acquired (hospital system, hospital\ndepartment) with extremely high accuracy and calibrate predictions accordingly.\nThe performance of CNNs in diagnosing diseases on x-rays may reflect not only\ntheir ability to identify disease-specific imaging findings on x-rays, but also\ntheir ability to exploit confounding information. Estimates of CNN performance\nbased on test data from hospital systems used for model training may overstate\ntheir likely real-world performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 01:57:38 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 01:07:41 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zech", "John R.", ""], ["Badgeley", "Marcus A.", ""], ["Liu", "Manway", ""], ["Costa", "Anthony B.", ""], ["Titano", "Joseph J.", ""], ["Oermann", "Eric K.", ""]]}, {"id": "1807.00436", "submitter": "Sang-gil Lee", "authors": "Sang-gil Lee, Jae Seok Bae, Hyunjae Kim, Jung Hoon Kim, Sungroh Yoon", "title": "Liver Lesion Detection from Weakly-labeled Multi-phase CT Volumes with a\n  Grouped Single Shot MultiBox Detector", "comments": "Accepted at MICCAI 2018. 8 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a focal liver lesion detection model leveraged by custom-designed\nmulti-phase computed tomography (CT) volumes, which reflects real-world\nclinical lesion detection practice using a Single Shot MultiBox Detector (SSD).\nWe show that grouped convolutions effectively harness richer information of the\nmulti-phase data for the object detection model, while a naive application of\nSSD suffers from a generalization gap. We trained and evaluated the modified\nSSD model and recently proposed variants with our CT dataset of 64 subjects by\nfive-fold cross validation. Our model achieved a 53.3% average precision score\nand ran in under three seconds per volume, outperforming the original model and\nstate-of-the-art variants. Results show that the one-stage object detection\nmodel is a practical solution, which runs in near real-time and can learn an\nunbiased feature representation from a large-volume real-world detection\ndataset, which requires less tedious and time consuming construction of the\nweak phase-level bounding box labels.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 02:31:33 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lee", "Sang-gil", ""], ["Bae", "Jae Seok", ""], ["Kim", "Hyunjae", ""], ["Kim", "Jung Hoon", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1807.00453", "submitter": "Yue Bai", "authors": "Yue Bai, Shuvra S. Bhattacharyya, Antti P. Happonen, Heikki Huttunen", "title": "Elastic Neural Networks: A Scalable Framework for Embedded Computer\n  Vision", "comments": "EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for image classification with deep neural\nnetworks. The framework introduces intermediate outputs to the computational\ngraph of a network. This enables flexible control of the computational load and\nbalances the tradeoff between accuracy and execution time.\n  Moreover, we present an interesting finding that the intermediate outputs can\nact as a regularizer at training time, improving the prediction accuracy. In\nthe experimental section we demonstrate the performance of our proposed\nframework with various commonly used pretrained deep networks in the use case\nof apparent age estimation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 03:51:53 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 09:50:04 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bai", "Yue", ""], ["Bhattacharyya", "Shuvra S.", ""], ["Happonen", "Antti P.", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1807.00456", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Chinmaya Devaraj, Michael Maynord, Cornelia Ferm\\\"uller,\n  Yiannis Aloimonos", "title": "Evenly Cascaded Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Evenly Cascaded convolutional Network (ECN), a neural network\ntaking inspiration from the cascade algorithm of wavelet analysis. ECN employs\ntwo feature streams - a low-level and high-level steam. At each layer these\nstreams interact, such that low-level features are modulated using advanced\nperspectives from the high-level stream. ECN is evenly structured through\nresizing feature map dimensions by a consistent ratio, which removes the burden\nof ad-hoc specification of feature map dimensions. ECN produces easily\ninterpretable features maps, a result whose intuition can be understood in the\ncontext of scale-space theory. We demonstrate that ECN's design facilitates the\ntraining process through providing easily trainable shortcuts. We report new\nstate-of-the-art results for small networks, without the need for additional\ntreatment such as pruning or compression - a consequence of ECN's simple\nstructure and direct training. A 6-layered ECN design with under 500k\nparameters achieves 95.24% and 78.99% accuracy on CIFAR-10 and CIFAR-100\ndatasets, respectively, outperforming the current state-of-the-art on small\nparameter networks, and a 3 million parameter ECN produces results competitive\nto the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 04:12:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:49:01 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ye", "Chengxi", ""], ["Devaraj", "Chinmaya", ""], ["Maynord", "Michael", ""], ["Ferm\u00fcller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1807.00458", "submitter": "Shasha Li", "authors": "Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V.\n  Krishnamurthy, Amit K. Roy Chowdhury, Ananthram Swami", "title": "Adversarial Perturbations Against Real-Time Video Classification Systems", "comments": null, "journal-ref": "Network and Distributed Systems Security (NDSS) Symposium 2019\n  24-27 February 2019, San Diego, CA, USA", "doi": "10.14722/ndss.2019.23202", "report-no": null, "categories": "cs.LG cs.CR cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has demonstrated the brittleness of machine learning systems\nto adversarial perturbations. However, the studies have been mostly limited to\nperturbations on images and more generally, classification that does not deal\nwith temporally varying inputs. In this paper we ask \"Are adversarial\nperturbations possible in real-time video classification systems and if so,\nwhat properties must they satisfy?\" Such systems find application in\nsurveillance applications, smart vehicles, and smart elderly care and thus,\nmisclassification could be particularly harmful (e.g., a mishap at an elderly\ncare facility may be missed). We show that accounting for temporal structure is\nkey to generating adversarial examples in such systems. We exploit recent\nadvances in generative adversarial network (GAN) architectures to account for\ntemporal correlations and generate adversarial samples that can cause\nmisclassification rates of over 80% for targeted activities. More importantly,\nthe samples also leave other activities largely unaffected making them\nextremely stealthy. Finally, we also surprisingly find that in many scenarios,\nthe same perturbation can be applied to every frame in a video clip that makes\nthe adversary's ability to achieve misclassification relatively easy.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 04:25:46 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Li", "Shasha", ""], ["Neupane", "Ajaya", ""], ["Paul", "Sujoy", ""], ["Song", "Chengyu", ""], ["Krishnamurthy", "Srikanth V.", ""], ["Chowdhury", "Amit K. Roy", ""], ["Swami", "Ananthram", ""]]}, {"id": "1807.00467", "submitter": "Mattias Heinrich", "authors": "Jan R\\\"uhaak, Thomas Polzin, Stefan Heldmann, Ivor J. A. Simpson,\n  Heinz Handels, Jan Modersitzki, Mattias P. Heinrich", "title": "Estimation of Large Motion in Lung CT by Integrating Regularized\n  Keypoint Correspondences into Dense Deformable Registration", "comments": "12 pages, 7 figures, \\c{opyright} 2017 IEEE. Personal use is\n  permitted, but republication/redistribution requires IEEE permission", "journal-ref": "IEEE Transactions on Medical Imaging, 2018, Vol. 36 (8), pp.\n  1746-1757", "doi": "10.1109/TMI.2017.2691259", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for the registration of pulmonary CT scans. Our\nmethod is designed for large respiratory motion by integrating sparse keypoint\ncorrespondences into a dense continuous optimization framework. The detection\nof keypoint correspondences enables robustness against large deformations by\njointly optimizing over a large number of potential discrete displacements,\nwhereas the dense continuous registration achieves subvoxel alignment with\nsmooth transformations. Both steps are driven by the same normalized gradient\nfields data term. We employ curvature regularization and a volume change\ncontrol mechanism to prevent foldings of the deformation grid and restrict the\ndeterminant of the Jacobian to physiologically meaningful values. Keypoint\ncorrespondences are integrated into the dense registration by a quadratic\npenalty with adaptively determined weight. Using a parallel matrix-free\nderivative calculation scheme, a runtime of about 5 min was realized on a\nstandard PC. The proposed algorithm ranks first in the EMPIRE10 challenge on\npulmonary image registration. Moreover, it achieves an average landmark\ndistance of 0.82 mm on the DIR-Lab COPD database, thereby improving upon the\nstate of the art in accuracy by 15%. Our algorithm is the first to reach the\ninter-observer variability in landmark annotation on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 05:27:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["R\u00fchaak", "Jan", ""], ["Polzin", "Thomas", ""], ["Heldmann", "Stefan", ""], ["Simpson", "Ivor J. A.", ""], ["Handels", "Heinz", ""], ["Modersitzki", "Jan", ""], ["Heinrich", "Mattias P.", ""]]}, {"id": "1807.00487", "submitter": "Gayan Illeperuma", "authors": "G.D. Illeperuma", "title": "An initial study on estimating area of a leaf using image processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating leaf area is very important. Computer aided image processing can\nmake this faster and more accurate. This include scanning the leaf , converting\nit to binary image and calculation of number of pixels covered. Later this is\nconverted to mm2.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 06:47:47 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Illeperuma", "G. D.", ""]]}, {"id": "1807.00493", "submitter": "Phuc Nguyen X", "authors": "Phuc Nguyen, Deva Ramanan, Charless Fowlkes", "title": "Active Testing: An Efficient and Robust Framework for Estimating\n  Accuracy", "comments": "accepted to ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Much recent work on visual recognition aims to scale up learning to massive,\nnoisily-annotated datasets. We address the problem of scaling- up the\nevaluation of such models to large-scale datasets with noisy labels. Current\nprotocols for doing so require a human user to either vet (re-annotate) a small\nfraction of the test set and ignore the rest, or else correct errors in\nannotation as they are found through manual inspection of results. In this\nwork, we re-formulate the problem as one of active testing, and examine\nstrategies for efficiently querying a user so as to obtain an accu- rate\nperformance estimate with minimal vetting. We demonstrate the effectiveness of\nour proposed active testing framework on estimating two performance metrics,\nPrecision@K and mean Average Precision, for two popular computer vision tasks,\nmulti-label classification and instance segmentation. We further show that our\napproach is able to save significant human annotation effort and is more robust\nthan alternative evaluation protocols.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:18:44 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Nguyen", "Phuc", ""], ["Ramanan", "Deva", ""], ["Fowlkes", "Charless", ""]]}, {"id": "1807.00498", "submitter": "Javier Ribera", "authors": "Javier Ribera and Fangning He and Yuhao Chen and Ayman F. Habib and\n  Edward J. Delp", "title": "Estimating Phenotypic Traits From UAV Based RGB Imagery", "comments": "8 pages, double-column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many agricultural applications one wants to characterize physical\nproperties of plants and use the measurements to predict, for example biomass\nand environmental influence. This process is known as phenotyping. Traditional\ncollection of phenotypic information is labor-intensive and time-consuming. Use\nof imagery is becoming popular for phenotyping. In this paper, we present\nmethods to estimate traits of sorghum plants from RBG cameras on board of an\nunmanned aerial vehicle (UAV). The position and orientation of the imagery\ntogether with the coordinates of sparse points along the area of interest are\nderived through a new triangulation method. A rectified orthophoto mosaic is\nthen generated from the imagery. The number of leaves is estimated and a\nmodel-based method to analyze the leaf morphology for leaf segmentation is\nproposed. We present a statistical model to find the location of each\nindividual sorghum plant.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:32:46 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ribera", "Javier", ""], ["He", "Fangning", ""], ["Chen", "Yuhao", ""], ["Habib", "Ayman F.", ""], ["Delp", "Edward J.", ""]]}, {"id": "1807.00502", "submitter": "Terrance DeVries", "authors": "Terrance DeVries and Graham W. Taylor", "title": "Leveraging Uncertainty Estimates for Predicting Segmentation Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep learning for medical imaging has seen tremendous growth in\nthe research community. One reason for the slow uptake of these systems in the\nclinical setting is that they are complex, opaque and tend to fail silently.\nOutside of the medical imaging domain, the machine learning community has\nrecently proposed several techniques for quantifying model uncertainty (i.e.~a\nmodel knowing when it has failed). This is important in practical settings, as\nwe can refer such cases to manual inspection or correction by humans. In this\npaper, we aim to bring these recent results on estimating uncertainty to bear\non two important outputs in deep learning-based segmentation. The first is\nproducing spatial uncertainty maps, from which a clinician can observe where\nand why a system thinks it is failing. The second is quantifying an image-level\nprediction of failure, which is useful for isolating specific cases and\nremoving them from automated pipelines. We also show that reasoning about\nspatial uncertainty, the first output, is a useful intermediate representation\nfor generating segmentation quality predictions, the second output. We propose\na two-stage architecture for producing these measures of uncertainty, which can\naccommodate any deep learning-based medical segmentation pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:42:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["DeVries", "Terrance", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1807.00504", "submitter": "Tianshui Chen", "authors": "Zhouxia Wang, Tianshui Chen, Jimmy Ren, Weihao Yu, Hui Cheng, Liang\n  Lin", "title": "Deep Reasoning with Knowledge Graph for Social Relationship\n  Understanding", "comments": "Accepted at IJCAI 2018. The first work that integrates high-level\n  knowledge graph to reason about social relationships between person pair of\n  interest in still image", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social relationships (e.g., friends, couple etc.) form the basis of the\nsocial network in our daily life. Automatically interpreting such relationships\nbears a great potential for the intelligent systems to understand human\nbehavior in depth and to better interact with people at a social level. Human\nbeings interpret the social relationships within a group not only based on the\npeople alone, and the interplay between such social relationships and the\ncontextual information around the people also plays a significant role.\nHowever, these additional cues are largely overlooked by the previous studies.\nWe found that the interplay between these two factors can be effectively\nmodeled by a novel structured knowledge graph with proper message propagation\nand attention. And this structured knowledge can be efficiently integrated into\nthe deep neural network architecture to promote social relationship\nunderstanding by an end-to-end trainable Graph Reasoning Model (GRM), in which\na propagation mechanism is learned to propagate node message through the graph\nto explore the interaction between persons of interest and the contextual\nobjects. Meanwhile, a graph attentional mechanism is introduced to explicitly\nreason about the discriminative objects to promote recognition. Extensive\nexperiments on the public benchmarks demonstrate the superiority of our method\nover the existing leading competitors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:48:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wang", "Zhouxia", ""], ["Chen", "Tianshui", ""], ["Ren", "Jimmy", ""], ["Yu", "Weihao", ""], ["Cheng", "Hui", ""], ["Lin", "Liang", ""]]}, {"id": "1807.00505", "submitter": "Tianshui Chen", "authors": "Tianshui Chen, Liang Lin, Riquan Chen, Yang Wu, Xiaonan Luo", "title": "Knowledge-Embedded Representation Learning for Fine-Grained Image\n  Recognition", "comments": "Accepted at IJCAI 2018. The first work that introduces high-level\n  knowledge to enhance representation learning for fine-grained image\n  classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can naturally understand an image in depth with the aid of rich\nknowledge accumulated from daily lives or professions. For example, to achieve\nfine-grained image recognition (e.g., categorizing hundreds of subordinate\ncategories of birds) usually requires a comprehensive visual concept\norganization including category labels and part-level attributes. In this work,\nwe investigate how to unify rich professional knowledge with deep neural\nnetwork architectures and propose a Knowledge-Embedded Representation Learning\n(KERL) framework for handling the problem of fine-grained image recognition.\nSpecifically, we organize the rich visual concepts in the form of knowledge\ngraph and employ a Gated Graph Neural Network to propagate node message through\nthe graph for generating the knowledge representation. By introducing a novel\ngated mechanism, our KERL framework incorporates this knowledge representation\ninto the discriminative image feature learning, i.e., implicitly associating\nthe specific attributes with the feature maps. Compared with existing methods\nof fine-grained image classification, our KERL framework has several appealing\nproperties: i) The embedded high-level knowledge enhances the feature\nrepresentation, thus facilitating distinguishing the subtle differences among\nsubordinate categories. ii) Our framework can learn feature maps with a\nmeaningful configuration that the highlighted regions finely accord with the\nnodes (specific attributes) of the knowledge graph. Extensive experiments on\nthe widely used Caltech-UCSD bird dataset demonstrate the superiority of our\nKERL framework over existing state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:49:06 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Chen", "Tianshui", ""], ["Lin", "Liang", ""], ["Chen", "Riquan", ""], ["Wu", "Yang", ""], ["Luo", "Xiaonan", ""]]}, {"id": "1807.00511", "submitter": "Ilker Bozcan", "authors": "Ilker Bozcan and Sinan Kalkan", "title": "COSMO: Contextualized Scene Modeling with Boltzmann Machines", "comments": "40 pages, 15 figures, 9 tables, accepted to the Robotics and\n  Autonomous Systems (RAS) special issue on Semantic Policy and Action\n  Representations for Autonomous Robots (SPAR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene modeling is very crucial for robots that need to perceive, reason about\nand manipulate the objects in their environments. In this paper, we adapt and\nextend Boltzmann Machines (BMs) for contextualized scene modeling. Although\nthere are many models on the subject, ours is the first to bring together\nobjects, relations, and affordances in a highly-capable generative model. For\nthis end, we introduce a hybrid version of BMs where relations and affordances\nare introduced with shared, tri-way connections into the model. Moreover, we\ncontribute a dataset for relation estimation and modeling studies. We evaluate\nour method in comparison with several baselines on object estimation,\nout-of-context object detection, relation estimation, and affordance estimation\ntasks. Moreover, to illustrate the generative capability of the model, we show\nseveral example scenes that the model is able to generate.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 08:07:36 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:20:39 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Bozcan", "Ilker", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1807.00517", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, Anna\n  Rohrbach", "title": "Women also Snowboard: Overcoming Bias in Captioning Models (Extended\n  Abstract)", "comments": "Burns and Hendricks contributed equally. 2018 ICML Workshop on\n  Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning methods are known to capture and exploit biases of the\ntraining data. While some biases are beneficial for learning, others are\nharmful. Specifically, image captioning models tend to exaggerate biases\npresent in training data. This can lead to incorrect captions in domains where\nunbiased captions are desired, or required, due to over reliance on the learned\nprior and image context. We investigate generation of gender specific caption\nwords (e.g. man, woman) based on the person's appearance or the image context.\nWe introduce a new Equalizer model that ensures equal gender probability when\ngender evidence is occluded in a scene and confident predictions when gender\nevidence is present. The resulting model is forced to look at a person rather\nthan use contextual cues to make a gender specific prediction. The losses that\ncomprise our model, the Appearance Confusion Loss and the Confident Loss, are\ngeneral, and can be added to any description model in order to mitigate impacts\nof unwanted bias in a description dataset. Our proposed model has lower error\nthan prior work when describing images with people and mentioning their gender\nand more closely matches the ground truth ratio of sentences including women to\nsentences including men.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 08:15:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Burns", "Kaylee", ""], ["Saenko", "Kate", ""], ["Darrell", "Trevor", ""], ["Rohrbach", "Anna", ""]]}, {"id": "1807.00537", "submitter": "Hao Luo", "authors": "Xing Fan, Wei Jiang, Hao Luo, Mengjuan Fei,", "title": "SphereReID: Deep Hypersphere Manifold Embedding for Person\n  Re-Identification", "comments": "Contribute to Journal of Visual Communication and Image\n  Representation", "journal-ref": null, "doi": "10.1016/j.jvcir.2019.01.010", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many current successful Person Re-Identification(ReID) methods train a model\nwith the softmax loss function to classify images of different persons and\nobtain the feature vectors at the same time. However, the underlying feature\nembedding space is ignored. In this paper, we use a modified softmax function,\ntermed Sphere Softmax, to solve the classification problem and learn a\nhypersphere manifold embedding simultaneously. A balanced sampling strategy is\nalso introduced. Finally, we propose a convolutional neural network called\nSphereReID adopting Sphere Softmax and training a single model end-to-end with\na new warming-up learning rate schedule on four challenging datasets including\nMarket-1501, DukeMTMC-reID, CHHK-03, and CUHK-SYSU. Experimental results\ndemonstrate that this single model outperforms the state-of-the-art methods on\nall four datasets without fine-tuning or re-ranking. For example, it achieves\n94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on\nDukeMTMC-reID. The code and trained weights of our model will be released.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 08:53:01 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Fan", "Xing", ""], ["Jiang", "Wei", ""], ["Luo", "Hao", ""], ["Fei", "Mengjuan", ""]]}, {"id": "1807.00556", "submitter": "Julia Lasserre", "authors": "Julia Lasserre, Katharina Rasch, Roland Vollgraf", "title": "Studio2Shop: from studio photo shoots to fashion articles", "comments": "12 pages, 9 figures (Figure 1 has 5 subfigures, Figure 2 has 3\n  subfigures), 7 tables", "journal-ref": "Proceedings of the 7th International Conference on Pattern\n  Recognition Applications and Methods (January 16-18, 2018, in Funchal,\n  Madeira, Portugal), Vol. 1 (ISBN 978-989-758-276-9), P. 37-48", "doi": "10.5220/0006544500370048", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is an increasingly important topic in computer vision, in particular\nthe so-called street-to-shop task of matching street images with shop images\ncontaining similar fashion items. Solving this problem promises new means of\nmaking fashion searchable and helping shoppers find the articles they are\nlooking for. This paper focuses on finding pieces of clothing worn by a person\nin full-body or half-body images with neutral backgrounds. Such images are\nubiquitous on the web and in fashion blogs, and are typically studio photos, we\nrefer to this setting as studio-to-shop. Recent advances in computational\nfashion include the development of domain-specific numerical representations.\nOur model Studio2Shop builds on top of such representations and uses a deep\nconvolutional network trained to match a query image to the numerical feature\nvectors of all the articles annotated in this image. Top-$k$ retrieval\nevaluation on test query images shows that the correct items are most often\nfound within a range that is sufficiently small for building realistic visual\nsearch engines for the studio-to-shop setting.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 09:26:58 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lasserre", "Julia", ""], ["Rasch", "Katharina", ""], ["Vollgraf", "Roland", ""]]}, {"id": "1807.00578", "submitter": "Roshan Gopalakrishnan", "authors": "Roshan Gopalakrishnan, Yansong Chua and Laxmi R Iyer", "title": "Classifying neuromorphic data using a deep learning framework for image\n  classification", "comments": "4 pages, 3 figures, submitted to ICARCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of artificial intelligence, neuromorphic computing has been\naround for several decades. Deep learning has however made much recent progress\nsuch that it consistently outperforms neuromorphic learning algorithms in\nclassification tasks in terms of accuracy. Specifically in the field of image\nclassification, neuromorphic computing has been traditionally using either the\ntemporal or rate code for encoding static images in datasets into spike trains.\nIt is only till recently, that neuromorphic vision sensors are widely used by\nthe neuromorphic research community, and provides an alternative to such\nencoding methods. Since then, several neuromorphic datasets as obtained by\napplying such sensors on image datasets (e.g. the neuromorphic CALTECH 101)\nhave been introduced. These data are encoded in spike trains and hence seem\nideal for benchmarking of neuromorphic learning algorithms. Specifically, we\ntrain a deep learning framework used for image classification on the CALTECH\n101 and a collapsed version of the neuromorphic CALTECH 101 datasets. We\nobtained an accuracy of 91.66% and 78.01% for the CALTECH 101 and neuromorphic\nCALTECH 101 datasets respectively. For CALTECH 101, our accuracy is close to\nthe best reported accuracy, while for neuromorphic CALTECH 101, it outperforms\nthe last best reported accuracy by over 10%. This raises the question of the\nsuitability of such datasets as benchmarks for neuromorphic learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:18:37 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Gopalakrishnan", "Roshan", ""], ["Chua", "Yansong", ""], ["Iyer", "Laxmi R", ""]]}, {"id": "1807.00583", "submitter": "Jasper Linmans", "authors": "Jasper Linmans, Jim Winkens, Bastiaan S. Veeling, Taco S. Cohen, Max\n  Welling", "title": "Sample Efficient Semantic Segmentation using Rotation Equivariant\n  Convolutional Networks", "comments": "Presented at the ICML workshop: Towards learning with limited labels:\n  Equivariance, Invariance, and Beyond, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semantic segmentation model that exploits rotation and\nreflection symmetries. We demonstrate significant gains in sample efficiency\ndue to increased weight sharing, as well as improvements in robustness to\nsymmetry transformations. The group equivariant CNN framework is extended for\nsegmentation by introducing a new equivariant (G->Z2)-convolution that\ntransforms feature maps on a group to planar feature maps. Also, equivariant\ntransposed convolution is formulated for up-sampling in an encoder-decoder\nnetwork. To demonstrate improvements in sample efficiency we evaluate on\nmultiple data regimes of a rotation-equivariant segmentation task: cancer\nmetastases detection in histopathology images. We further show the\neffectiveness of exploiting more symmetries by varying the size of the group.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:31:05 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Linmans", "Jasper", ""], ["Winkens", "Jim", ""], ["Veeling", "Bastiaan S.", ""], ["Cohen", "Taco S.", ""], ["Welling", "Max", ""]]}, {"id": "1807.00598", "submitter": "Yong Xia", "authors": "Junjie Zhang, Yong Xia, Yanning Zhang", "title": "A Pulmonary Nodule Detection Model Based on Progressive Resolution and\n  Hierarchical Saliency", "comments": "8 pages,4 figures,1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of pulmonary nodules on chest CT is an essential step in the early\ndiagnosis of lung cancer, which is critical for best patient care. Although a\nnumber of computer-aided nodule detection methods have been published in the\nliterature, these methods still have two major drawbacks: missing out true\nnodules during the detection of nodule candidates and less-accurate\nidentification of nodules from non-nodule. In this paper, we propose an\nautomated pulmonary nodule detection algorithm that jointly combines\nprogressive resolution and hierarchical saliency. Specifically, we design a 3D\nprogressive resolution-based densely dilated FCN, namely the progressive\nresolution network (PRN), to detect nodule candidates inside the lung, and\nconstruct a densely dilated 3D CNN with hierarchical saliency, namely the\nhierarchical saliency network (HSN), to simultaneously identify genuine nodules\nfrom those candidates and estimate the diameters of nodules. We evaluated our\nalgorithm on the benchmark LUng Nodule Analysis 2016 (LUNA16) dataset and\nachieved a state-of-the-art detection score. Our results suggest that the\nproposed algorithm can effectively detect pulmonary nodules on chest CT and\naccurately estimate their diameters.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 11:05:30 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhang", "Junjie", ""], ["Xia", "Yong", ""], ["Zhang", "Yanning", ""]]}, {"id": "1807.00601", "submitter": "Lingbo Liu", "authors": "Lingbo Liu, Hongjun Wang, Guanbin Li, Wanli Ouyang and Liang Lin", "title": "Crowd Counting using Deep Recurrent Spatial-Aware Network", "comments": "Accepted to IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting from unconstrained scene images is a crucial task in many\nreal-world applications like urban surveillance and management, but it is\ngreatly challenged by the camera's perspective that causes huge appearance\nvariations in people's scales and rotations. Conventional methods address such\nchallenges by resorting to fixed multi-scale architectures that are often\nunable to cover the largely varied scales while ignoring the rotation\nvariations. In this paper, we propose a unified neural network framework, named\nDeep Recurrent Spatial-Aware Network, which adaptively addresses the two issues\nin a learnable spatial transform module with a region-wise refinement process.\nSpecifically, our framework incorporates a Recurrent Spatial-Aware Refinement\n(RSAR) module iteratively conducting two components: i) a Spatial Transformer\nNetwork that dynamically locates an attentional region from the crowd density\nmap and transforms it to the suitable scale and rotation for optimal crowd\nestimation; ii) a Local Refinement Network that refines the density map of the\nattended region with residual learning. Extensive experiments on four\nchallenging benchmarks show the effectiveness of our approach. Specifically,\ncomparing with the existing best-performing methods, we achieve an improvement\nof 12% on the largest dataset WorldExpo'10 and 22.8% on the most challenging\ndataset UCF_CC_50.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 11:21:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Liu", "Lingbo", ""], ["Wang", "Hongjun", ""], ["Li", "Guanbin", ""], ["Ouyang", "Wanli", ""], ["Lin", "Liang", ""]]}, {"id": "1807.00612", "submitter": "Alptekin Temizel", "authors": "Mehmet Ali Arabac{\\i}, Fatih \\\"Ozkan, Elif Surer, Peter\n  Jan\\v{c}ovi\\v{c}, Alptekin Temizel", "title": "Multi-modal Egocentric Activity Recognition using Audio-Visual Features", "comments": null, "journal-ref": "Multimedia Tools and Applications (2020)", "doi": "10.1007/s11042-020-08789-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric activity recognition in first-person videos has an increasing\nimportance with a variety of applications such as lifelogging, summarization,\nassisted-living and activity tracking. Existing methods for this task are based\non interpretation of various sensor information using pre-determined weights\nfor each feature. In this work, we propose a new framework for egocentric\nactivity recognition problem based on combining audio-visual features with\nmulti-kernel learning (MKL) and multi-kernel boosting (MKBoost). For that\npurpose, firstly grid optical-flow, virtual-inertia feature, log-covariance,\ncuboid are extracted from the video. The audio signal is characterized using a\n\"supervector\", obtained based on Gaussian mixture modelling of frame-level\nfeatures, followed by a maximum a-posteriori adaptation. Then, the extracted\nmulti-modal features are adaptively fused by MKL classifiers in which both the\nfeature and kernel selection/weighing and recognition tasks are performed\ntogether. The proposed framework was evaluated on a number of egocentric\ndatasets. The results showed that using multi-modal features with MKL\noutperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 12:04:24 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 17:06:33 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 08:31:52 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Arabac\u0131", "Mehmet Ali", ""], ["\u00d6zkan", "Fatih", ""], ["Surer", "Elif", ""], ["Jan\u010dovi\u010d", "Peter", ""], ["Temizel", "Alptekin", ""]]}, {"id": "1807.00637", "submitter": "Shaked Perek", "authors": "Shaked Perek, Alon Hazan, Ella Barkan, Ayelet Akselrod-Ballin", "title": "Mammography Dual View Mass Correspondence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard breast cancer screening involves the acquisition of two mammography\nX-ray projections for each breast. Typically, a comparison of both views\nsupports the challenging task of tumor detection and localization. We introduce\na deep learning, patch-based Siamese network for lesion matching in dual-view\nmammography. Our locally-fitted approach generates a joint patch pair\nrepresentation and comparison with a shared configuration between the two\nviews. We performed a comprehensive set of experiments with the network on\nstandard datasets, among them the large Digital Database for Screening\nMammography (DDSM). We analyzed the effect of transfer learning with the\nnetwork between different types of datasets and compared the network-based\nmatching to using Euclidean distance by template matching. Finally, we\nevaluated the contribution of the matching network in a full detection\npipeline. Experimental results demonstrate the promise of improved detection\naccuracy using our approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 12:52:24 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Perek", "Shaked", ""], ["Hazan", "Alon", ""], ["Barkan", "Ella", ""], ["Akselrod-Ballin", "Ayelet", ""]]}, {"id": "1807.00652", "submitter": "Mingyang Jiang", "authors": "Mingyang Jiang, Yiran Wu, Tianqi Zhao, Zelin Zhao, Cewu Lu", "title": "PointSIFT: A SIFT-like Network Module for 3D Point Cloud Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, 3D understanding research sheds light on extracting features from\npoint cloud directly, which requires effective shape pattern description of\npoint clouds. Inspired by the outstanding 2D shape descriptor SIFT, we design a\nmodule called PointSIFT that encodes information of different orientations and\nis adaptive to scale of shape. Specifically, an orientation-encoding unit is\ndesigned to describe eight crucial orientations, and multi-scale representation\nis achieved by stacking several orientation-encoding units. PointSIFT module\ncan be integrated into various PointNet-based architecture to improve the\nrepresentation ability. Extensive experiments show our PointSIFT-based\nframework outperforms state-of-the-art method on standard benchmark datasets.\nThe code and trained model will be published accompanied by this paper.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 13:29:47 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 03:41:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Jiang", "Mingyang", ""], ["Wu", "Yiran", ""], ["Zhao", "Tianqi", ""], ["Zhao", "Zelin", ""], ["Lu", "Cewu", ""]]}, {"id": "1807.00664", "submitter": "Erik Lind\\'en", "authors": "Erik Lind\\'en and Jonas Sj\\\"ostrand and Alexandre Proutiere", "title": "Learning to Personalize in Appearance-Based Gaze Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal variations severely limit the performance of appearance-based gaze\ntracking. Adapting to these variations using standard neural network model\nadaptation methods is difficult. The problems range from overfitting, due to\nsmall amounts of training data, to underfitting, due to restrictive model\narchitectures. We tackle these problems by introducing the SPatial Adaptive\nGaZe Estimator (SPAZE). By modeling personal variations as a low-dimensional\nlatent parameter space, SPAZE provides just enough adaptability to capture the\nrange of personal variations without being prone to overfitting. Calibrating\nSPAZE for a new person reduces to solving a small optimization problem. SPAZE\nachieves an error of 2.70 degrees with 9 calibration samples on MPIIGaze,\nimproving on the state-of-the-art by 14 %. We contribute to gaze tracking\nresearch by empirically showing that personal variations are well-modeled as a\n3-dimensional latent parameter space for each eye. We show that this\nlow-dimensionality is expected by examining model-based approaches to gaze\ntracking. We also show that accurate head pose-free gaze tracking is possible.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 13:59:51 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 10:32:01 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 17:37:12 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lind\u00e9n", "Erik", ""], ["Sj\u00f6strand", "Jonas", ""], ["Proutiere", "Alexandre", ""]]}, {"id": "1807.00676", "submitter": "Anis Kacem", "authors": "Anis Kacem, Mohamed Daoudi, Boulbaba Ben Amor, Stefano Berretti, and\n  Juan Carlos Alvarez-Paiva", "title": "A Novel Geometric Framework on Gram Matrix Trajectories for Human\n  Behavior Understanding", "comments": "Under minor revisions in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (T-PAMI). A preliminary version of this work appeared in\n  ICCV 17 (A Kacem, M Daoudi, BB Amor, JC Alvarez-Paiva, A Novel Space-Time\n  Representation on the Positive Semidefinite Cone for Facial Expression\n  Recognition, ICCV 17). arXiv admin note: substantial text overlap with\n  arXiv:1707.06440", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel space-time geometric representation of\nhuman landmark configurations and derive tools for comparison and\nclassification. We model the temporal evolution of landmarks as parametrized\ntrajectories on the Riemannian manifold of positive semidefinite matrices of\nfixed-rank. Our representation has the benefit to bring naturally a second\ndesirable quantity when comparing shapes, the spatial covariance, in addition\nto the conventional affine-shape representation. We derived then geometric and\ncomputational tools for rate-invariant analysis and adaptive re-sampling of\ntrajectories, grounding on the Riemannian geometry of the underlying manifold.\nSpecifically, our approach involves three steps: (1) landmarks are first mapped\ninto the Riemannian manifold of positive semidefinite matrices of fixed-rank to\nbuild time-parameterized trajectories; (2) a temporal warping is performed on\nthe trajectories, providing a geometry-aware (dis-)similarity measure between\nthem; (3) finally, a pairwise proximity function SVM is used to classify them,\nincorporating the (dis-)similarity measure into the kernel function. We show\nthat such representation and metric achieve competitive results in applications\nas action recognition and emotion recognition from 3D skeletal data, and facial\nexpression recognition from videos. Experiments have been conducted on several\npublicly available up-to-date benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:15:58 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kacem", "Anis", ""], ["Daoudi", "Mohamed", ""], ["Amor", "Boulbaba Ben", ""], ["Berretti", "Stefano", ""], ["Alvarez-Paiva", "Juan Carlos", ""]]}, {"id": "1807.00686", "submitter": "Ting Yao", "authors": "Ting Yao and Xue Li", "title": "YH Technologies at ActivityNet Challenge 2018", "comments": "Rank 2 in both Temporal Activity Detection Task & Kinetics Task @\n  ActivityNet 2018. arXiv admin note: substantial text overlap with\n  arXiv:1710.08011 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This notebook paper presents an overview and comparative analysis of our\nsystems designed for the following five tasks in ActivityNet Challenge 2018:\ntemporal action proposals, temporal action localization, dense-captioning\nevents in videos, trimmed action recognition, and spatio-temporal action\nlocalization.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 07:49:08 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Yao", "Ting", ""], ["Li", "Xue", ""]]}, {"id": "1807.00703", "submitter": "Fabio Ferreira", "authors": "Fabio Ferreira, Jonas Rothfuss, Eren Erdal Aksoy, You Zhou, Tamim\n  Asfour", "title": "Introducing the Simulated Flying Shapes and Simulated Planar Manipulator\n  Datasets", "comments": "technical documentation, 2 figures, links to repositories", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We release two artificial datasets, Simulated Flying Shapes and Simulated\nPlanar Manipulator that allow to test the learning ability of video processing\nsystems. In particular, the dataset is meant as a tool which allows to easily\nassess the sanity of deep neural network models that aim to encode, reconstruct\nor predict video frame sequences. The datasets each consist of 90000 videos.\nThe Simulated Flying Shapes dataset comprises scenes showing two objects of\nequal shape (rectangle, triangle and circle) and size in which one object\napproaches its counterpart. The Simulated Planar Manipulator shows a 3-DOF\nplanar manipulator that executes a pick-and-place task in which it has to place\na size-varying circle on a squared platform. Different from other widely used\ndatasets such as moving MNIST [1], [2], the two presented datasets involve\ngoal-oriented tasks (e.g. the manipulator grasping an object and placing it on\na platform), rather than showing random movements. This makes our datasets more\nsuitable for testing prediction capabilities and the learning of sophisticated\nmotions by a machine learning model. This technical document aims at providing\nan introduction into the usage of both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 14:20:24 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Ferreira", "Fabio", ""], ["Rothfuss", "Jonas", ""], ["Aksoy", "Eren Erdal", ""], ["Zhou", "You", ""], ["Asfour", "Tamim", ""]]}, {"id": "1807.00751", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Yuxuan Song, Lantao Yu, Hongwei Wang, Jiadong Liang,\n  Weinan Zhang, Zhihua Zhang, Yong Yu", "title": "Understanding the Effectiveness of Lipschitz-Continuity in Generative\n  Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the underlying factor that leads to failure and\nsuccess in the training of GANs. We study the property of the optimal\ndiscriminative function and show that in many GANs, the gradient from the\noptimal discriminative function is not reliable, which turns out to be the\nfundamental cause of failure in training of GANs. We further demonstrate that a\nwell-defined distance metric does not necessarily guarantee the convergence of\nGANs. Finally, we prove in this paper that Lipschitz-continuity condition is a\ngeneral solution to make the gradient of the optimal discriminative function\nreliable, and characterized the necessary condition where Lipschitz-continuity\nensures the convergence, which leads to a broad family of valid GAN objectives\nunder Lipschitz-continuity condition, where Wasserstein distance is one special\ncase. We experiment with several new objectives, which are sound according to\nour theorems, and we found that, compared with Wasserstein distance, the\noutputs of the discriminator with new objectives are more stable and the final\nqualities of generated samples are also consistently higher than those produced\nby Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 15:41:34 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 03:52:20 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 09:27:16 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 07:04:43 GMT"}, {"version": "v5", "created": "Mon, 19 Nov 2018 16:55:18 GMT"}, {"version": "v6", "created": "Sun, 23 Dec 2018 15:09:29 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhou", "Zhiming", ""], ["Song", "Yuxuan", ""], ["Yu", "Lantao", ""], ["Wang", "Hongwei", ""], ["Liang", "Jiadong", ""], ["Zhang", "Weinan", ""], ["Zhang", "Zhihua", ""], ["Yu", "Yong", ""]]}, {"id": "1807.00780", "submitter": "Pengqian Yu", "authors": "Xinhan Di, Pengqian Yu, Meng Tian", "title": "Ambient Hidden Space of Generative Adversarial Networks", "comments": "Accepted for publication in Uncertainty in Deep Learning Workshop at\n  Uncertainty in Artificial Intelligence (UAI) 2018", "journal-ref": "Uncertainty in Deep Learning Workshop at Uncertainty in Artificial\n  Intelligence (UAI) 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial models are powerful tools to model structure in\ncomplex distributions for a variety of tasks. Current techniques for learning\ngenerative models require an access to samples which have high quality, and\nadvanced generative models are applied to generate samples from noisy training\ndata through ambient modules. However, the modules are only practical for the\noutput space of the generator, and their application in the hidden space is not\nwell studied. In this paper, we extend the ambient module to the hidden space\nof the generator, and provide the uniqueness condition and the corresponding\nstrategy for the ambient hidden generator in the adversarial training process.\nWe report the practicality of the proposed method on the benchmark dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 16:51:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Di", "Xinhan", ""], ["Yu", "Pengqian", ""], ["Tian", "Meng", ""]]}, {"id": "1807.00847", "submitter": "Jiayi Liu", "authors": "Jiayi Liu, Samarth Tripathi, Unmesh Kurup, Mohak Shah", "title": "Make (Nearly) Every Neural Network Better: Generating Neural Network\n  Ensembles by Weight Parameter Resampling", "comments": "Accepted at UAI Workshop on Uncertainty in Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have become increasingly popular in computer\nvision, natural language processing, and other areas. However, training and\nfine-tuning a deep learning model is computationally intensive and\ntime-consuming. We propose a new method to improve the performance of nearly\nevery model including pre-trained models. The proposed method uses an ensemble\napproach where the networks in the ensemble are constructed by reassigning\nmodel parameter values based on the probabilistic distribution of these\nparameters, calculated towards the end of the training process. For pre-trained\nmodels, this approach results in an additional training step (usually less than\none epoch). We perform a variety of analysis using the MNIST dataset and\nvalidate the approach with a number of DNN models using pre-trained models on\nthe ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 18:12:32 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Liu", "Jiayi", ""], ["Tripathi", "Samarth", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "1807.00848", "submitter": "Shervin Rahimzadeh Arashloo", "authors": "Shervin Rahimzadeh Arashloo and Josef Kittler", "title": "Client-Specific Anomaly Detection for Face Presentation Attack Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The one-class anomaly detection approach has previously been found to be\neffective in face presentation attack detection, especially in an\n\\textit{unseen} attack scenario, where the system is exposed to novel types of\nattacks. This work follows the same anomaly-based formulation of the problem\nand analyses the merits of deploying \\textit{client-specific} information for\nface spoofing detection. We propose training one-class client-specific\nclassifiers (both generative and discriminative) using representations obtained\nfrom pre-trained deep convolutional neural networks. Next, based on\nsubject-specific score distributions, a distinct threshold is set for each\nclient, which is then used for decision making regarding a test query. Through\nextensive experiments using different one-class systems, it is shown that the\nuse of client-specific information in a one-class anomaly detection formulation\n(both in model construction as well as decision threshold tuning) improves the\nperformance significantly. In addition, it is demonstrated that the same set of\ndeep convolutional features used for the recognition purposes is effective for\nface presentation attack detection in the class-specific one-class anomaly\ndetection paradigm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 18:19:03 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Arashloo", "Shervin Rahimzadeh", ""], ["Kittler", "Josef", ""]]}, {"id": "1807.00864", "submitter": "Srikantth Malla", "authors": "Athma Narayanan, Yi-Ting Chen, Srikanth Malla", "title": "Semi-supervised Learning: Fusion of Self-supervised, Supervised\n  Learning, and Multimodal Cues for Tactical Driver Behavior Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we presented a preliminary study for tactical driver behavior\ndetection from untrimmed naturalistic driving recordings. While supervised\nlearning based detection is a common approach, it suffers when labeled data is\nscarce. Manual annotation is both time-consuming and expensive. To emphasize\nthis problem, we experimented on a 104-hour real-world naturalistic driving\ndataset with a set of predefined driving behaviors annotated. There are three\nchallenges in the dataset. First, predefined driving behaviors are sparse in a\nnaturalistic driving setting. Second, the distribution of driving behaviors is\nlong-tail. Third, a huge intra-class variation is observed. To address these\nissues, recent self-supervised and supervised learning and fusion of multimodal\ncues are leveraged into our architecture design. Preliminary experiments and\ndiscussions are reported.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 19:24:19 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Narayanan", "Athma", ""], ["Chen", "Yi-Ting", ""], ["Malla", "Srikanth", ""]]}, {"id": "1807.00898", "submitter": "Shile Li", "authors": "Jan W\\\"ohlke, Shile Li and Dongheui Lee", "title": "Model-based Hand Pose Estimation for Generalized Hand Shape with\n  Appearance Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the emergence of large annotated datasets, state-of-the-art hand pose\nestimation methods have been mostly based on discriminative learning. Recently,\na hybrid approach has embedded a kinematic layer into the deep learning\nstructure in such a way that the pose estimates obey the physical constraints\nof human hand kinematics. However, the existing approach relies on a single\nperson's hand shape parameters, which are fixed constants. Therefore, the\nexisting hybrid method has problems to generalize to new, unseen hands. In this\nwork, we extend the kinematic layer to make the hand shape parameters\nlearnable. In this way, the learnt network can generalize towards arbitrary\nhand shapes. Furthermore, inspired by the idea of Spatial Transformer Networks,\nwe apply a cascade of appearance normalization networks to decrease the\nvariance in the input data. The input images are shifted, rotated, and globally\nscaled to a similar appearance. The effectiveness and limitations of our\nproposed approach are extensively evaluated on the Hands 2017 challenge dataset\nand the NYU dataset.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 21:27:08 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["W\u00f6hlke", "Jan", ""], ["Li", "Shile", ""], ["Lee", "Dongheui", ""]]}, {"id": "1807.00911", "submitter": "Isay Katsman", "authors": "Isay Katsman, Rohun Tripathi, Andreas Veit, Serge Belongie", "title": "Semantic Segmentation with Scarce Data", "comments": "ICML 2018 Workshop, camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation is a challenging vision problem that usually\nnecessitates the collection of large amounts of finely annotated data, which is\noften quite expensive to obtain. Coarsely annotated data provides an\ninteresting alternative as it is usually substantially more cheap. In this\nwork, we present a method to leverage coarsely annotated data along with fine\nsupervision to produce better segmentation results than would be obtained when\ntraining using only the fine data. We validate our approach by simulating a\nscarce data setting with less than 200 low resolution images from the\nCityscapes dataset and show that our method substantially outperforms solely\ntraining on the fine annotation data by an average of 15.52% mIoU and\noutperforms the coarse mask by an average of 5.28% mIoU.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 22:06:11 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 03:23:04 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Katsman", "Isay", ""], ["Tripathi", "Rohun", ""], ["Veit", "Andreas", ""], ["Belongie", "Serge", ""]]}, {"id": "1807.00925", "submitter": "Li Sun Dr", "authors": "Li Sun and Zhi Yan and Anestis Zaganidis and Cheng Zhao and Tom\n  Duckett", "title": "Recurrent-OctoMap: Learning State-based Map Refinement for Long-Term\n  Semantic Mapping with 3D-Lidar Data", "comments": "Accepted by IEEE Robotics and Automation Letters RA-L, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel semantic mapping approach, Recurrent-OctoMap,\nlearned from long-term 3D Lidar data. Most existing semantic mapping approaches\nfocus on improving semantic understanding of single frames, rather than 3D\nrefinement of semantic maps (i.e. fusing semantic observations). The most\nwidely-used approach for 3D semantic map refinement is a Bayesian update, which\nfuses the consecutive predictive probabilities following a Markov-Chain model.\nInstead, we propose a learning approach to fuse the semantic features, rather\nthan simply fusing predictions from a classifier. In our approach, we represent\nand maintain our 3D map as an OctoMap, and model each cell as a recurrent\nneural network (RNN), to obtain a Recurrent-OctoMap. In this case, the semantic\nmapping process can be formulated as a sequence-to-sequence encoding-decoding\nproblem. Moreover, in order to extend the duration of observations in our\nRecurrent-OctoMap, we developed a robust 3D localization and mapping system for\nsuccessively mapping a dynamic environment using more than two weeks of data,\nand the system can be trained and deployed with arbitrary memory length. We\nvalidate our approach on the ETH long-term 3D Lidar dataset [1]. The\nexperimental results show that our proposed approach outperforms the\nconventional \"Bayesian update\" approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 23:28:37 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 01:37:51 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sun", "Li", ""], ["Yan", "Zhi", ""], ["Zaganidis", "Anestis", ""], ["Zhao", "Cheng", ""], ["Duckett", "Tom", ""]]}, {"id": "1807.00947", "submitter": "Hyunjung Shim Dr.", "authors": "Duhyeon Bang, Hyunjung Shim", "title": "Resembled Generative Adversarial Networks: Two Domains with Similar\n  Attributes", "comments": "Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel algorithm, namely Resembled Generative Adversarial\nNetworks (GAN), that generates two different domain data simultaneously where\nthey resemble each other. Although recent GAN algorithms achieve the great\nsuccess in learning the cross-domain relationship, their application is limited\nto domain transfers, which requires the input image. The first attempt to\ntackle the data generation of two domains was proposed by CoGAN. However, their\nsolution is inherently vulnerable for various levels of domain similarities.\nUnlike CoGAN, our Resembled GAN implicitly induces two generators to match\nfeature covariance from both domains, thus leading to share semantic\nattributes. Hence, we effectively handle a wide range of structural and\nsemantic similarities between various two domains. Based on experimental\nanalysis on various datasets, we verify that the proposed algorithm is\neffective for generating two domains with similar attributes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 01:42:20 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Bang", "Duhyeon", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1807.00951", "submitter": "Ibrahim Sadek", "authors": "Ibrahim Sadek", "title": "Ballistocardiogram Signal Processing: A Literature Review", "comments": "Review Paper", "journal-ref": null, "doi": "10.1007/s13755-019-0071-7", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-domain algorithms are focused on detecting local maxima or local minima\nusing a moving window, and therefore finding the interval between the dominant\nJ-peaks of ballistocardiogram (BCG) signal. However, this approach has many\nlimitations due to the nonlinear and nonstationary behavior of the BCG signal.\nThis is because the BCG signal does not display consistent J-peaks, which can\nusually be the case for overnight, in-home monitoring, particularly with frail\nelderly. Additionally, its accuracy will be undoubtedly affected by motion\nartifacts. Second, frequency-domain algorithms do not provide information about\ninterbeat intervals. Nevertheless, they can provide information about heart\nrate variability. This is usually done by taking the fast Fourier transform or\nthe inverse Fourier transform of the logarithm of the estimated spectrum, i.e.,\ncepstrum of the signal using a sliding window. Thereafter, the dominant\nfrequency is obtained in a particular frequency range. The limit of these\nalgorithms is that the peak in the spectrum may get wider and multiple peaks\nmay appear, which might cause a problem in measuring the vital signs. At last,\nthe objective of wavelet-domain algorithms is to decompose the signal into\ndifferent components, hence the component which shows an agreement with the\nvital signs can be selected i.e., the selected component contains only\ninformation about the heart cycles or respiratory cycles, respectively. An\nempirical mode decomposition is an alternative approach to wavelet\ndecomposition, and it is also a very suitable approach to cope with nonlinear\nand nonstationary signals such as cardiorespiratory signals. Apart from the\nabove-mentioned algorithms, machine learning approaches have been implemented\nfor measuring heartbeats. However, manual labeling of training data is a\nrestricting property.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 01:57:36 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Sadek", "Ibrahim", ""]]}, {"id": "1807.00958", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Le Lu, Adam P. Harrison, Xiaoshuang Shi, Pingjun Chen,\n  and Lin Yang", "title": "Iterative Attention Mining for Weakly Supervised Thoracic Disease\n  Pattern Localization in Chest X-Rays", "comments": "9 pages, 2 figures, 3 tables (with supplementary), accepted to MICCAI\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given image labels as the only supervisory signal, we focus on harvesting, or\nmining, thoracic disease localizations from chest X-ray images. Harvesting such\nlocalizations from existing datasets allows for the creation of improved data\nsources for computer-aided diagnosis and retrospective analyses. We train a\nconvolutional neural network (CNN) for image classification and propose an\nattention mining (AM) strategy to improve the model's sensitivity or saliency\nto disease patterns. The intuition of AM is that once the most salient disease\narea is blocked or hidden from the CNN model, it will pay attention to\nalternative image regions, while still attempting to make correct predictions.\nHowever, the model requires to be properly constrained during AM, otherwise, it\nmay overfit to uncorrelated image parts and forget the valuable knowledge that\nit has learned from the original image classification task. To alleviate such\nside effects, we then design a knowledge preservation (KP) loss, which\nminimizes the discrepancy between responses for X-ray images from the original\nand the updated networks. Furthermore, we modify the CNN model to include\nmulti-scale aggregation (MSA), improving its localization ability on\nsmall-scale disease findings, e.g., lung nodules. We experimentally validate\nour method on the publicly-available ChestX-ray14 dataset, outperforming a\nclass activation map (CAM)-based approach, and demonstrating the value of our\nnovel framework for mining disease locations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 02:56:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Cai", "Jinzheng", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Shi", "Xiaoshuang", ""], ["Chen", "Pingjun", ""], ["Yang", "Lin", ""]]}, {"id": "1807.00959", "submitter": "Ang Li", "authors": "Ang Li, Zejian Yuan", "title": "SymmNet: A Symmetric Convolutional Neural Network for Occlusion\n  Detection", "comments": "BMVC 2018 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the occlusion from stereo images or video frames is important to\nmany computer vision applications. Previous efforts focus on bundling it with\nthe computation of disparity or optical flow, leading to a chicken-and-egg\nproblem. In this paper, we leverage convolutional neural network to liberate\nthe occlusion detection task from the interleaved, traditional calculation\nframework. We propose a Symmetric Network (SymmNet) to directly exploit\ninformation from an image pair, without estimating disparity or motion in\nadvance. The proposed network is structurally left-right symmetric to learn the\nbinocular occlusion simultaneously, aimed at jointly improving both results.\nThe comprehensive experiments show that our model achieves state-of-the-art\nresults on detecting the stereo and motion occlusion.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 03:11:17 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 07:58:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Li", "Ang", ""], ["Yuan", "Zejian", ""]]}, {"id": "1807.00966", "submitter": "Xuanyi Dong", "authors": "Xuanyi Dong, Shoou-I Yu, Xinshuo Weng, Shih-En Wei, Yi Yang, Yaser\n  Sheikh", "title": "Supervision-by-Registration: An Unsupervised Approach to Improve the\n  Precision of Facial Landmark Detectors", "comments": "Minor modifications to the CVPR 2018 version (add missing references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present supervision-by-registration, an unsupervised\napproach to improve the precision of facial landmark detectors on both images\nand video. Our key observation is that the detections of the same landmark in\nadjacent frames should be coherent with registration, i.e., optical flow.\nInterestingly, the coherency of optical flow is a source of supervision that\ndoes not require manual labeling, and can be leveraged during detector\ntraining. For example, we can enforce in the training loss function that a\ndetected landmark at frame$_{t-1}$ followed by optical flow tracking from\nframe$_{t-1}$ to frame$_t$ should coincide with the location of the detection\nat frame$_t$. Essentially, supervision-by-registration augments the training\nloss function with a registration loss, thus training the detector to have\noutput that is not only close to the annotations in labeled images, but also\nconsistent with registration on large amounts of unlabeled videos. End-to-end\ntraining with the registration loss is made possible by a differentiable\nLucas-Kanade operation, which computes optical flow registration in the forward\npass, and back-propagates gradients that encourage temporal coherency in the\ndetector. The output of our method is a more precise image-based facial\nlandmark detector, which can be applied to single images or video. With\nsupervision-by-registration, we demonstrate (1) improvements in facial landmark\ndetection on both images (300W, ALFW) and video (300VW, Youtube-Celebrities),\nand (2) significant reduction of jittering in video detections.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 03:52:45 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 04:05:52 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Dong", "Xuanyi", ""], ["Yu", "Shoou-I", ""], ["Weng", "Xinshuo", ""], ["Wei", "Shih-En", ""], ["Yang", "Yi", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1807.00975", "submitter": "Jie Liu", "authors": "Jie Liu, Cheng Sun, Xiang Xu, Baomin Xu, Shuangyuan Yu", "title": "A Spatial and Temporal Features Mixture Model with Body Parts for\n  Video-based Person Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video-based person re-identification is to recognize a person under\ndifferent cameras, which is a crucial task applied in visual surveillance\nsystem. Most previous methods mainly focused on the feature of full body in the\nframe. In this paper we propose a novel Spatial and Temporal Features Mixture\nModel (STFMM) based on convolutional neural network (CNN) and recurrent neural\nnetwork (RNN), in which the human body is split into $N$ parts in horizontal\ndirection so that we can obtain more specific features. The proposed method\nskillfully integrates features of each part to achieve more expressive\nrepresentation of each person. We first split the video sequence into $N$ part\nsequences which include the information of head, waist, legs and so on. Then\nthe features are extracted by STFMM whose $2N$ inputs are obtained from the\ndeveloped Siamese network, and these features are combined into a\ndiscriminative representation for one person. Experiments are conducted on the\niLIDS-VID and PRID-2011 datasets. The results demonstrate that our approach\noutperforms existing methods for video-based person re-identification. It\nachieves a rank-1 CMC accuracy of 74\\% on the iLIDS-VID dataset, exceeding the\nthe most recently developed method ASTPN by 12\\%. For the cross-data testing,\nour method achieves a rank-1 CMC accuracy of 48\\% exceeding the ASTPN method by\n18\\%, which shows that our model has significant stability.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 04:33:22 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Liu", "Jie", ""], ["Sun", "Cheng", ""], ["Xu", "Xiang", ""], ["Xu", "Baomin", ""], ["Yu", "Shuangyuan", ""]]}, {"id": "1807.00980", "submitter": "Xiangyu Zhang", "authors": "Tong Yang, Xiangyu Zhang, Zeming Li, Wenqiang Zhang, Jian Sun", "title": "MetaAnchor: Learning to Detect Objects with Customized Anchors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and flexible anchor mechanism named MetaAnchor for object\ndetection frameworks. Unlike many previous detectors model anchors via a\npredefined manner, in MetaAnchor anchor functions could be dynamically\ngenerated from the arbitrary customized prior boxes. Taking advantage of weight\nprediction, MetaAnchor is able to work with most of the anchor-based object\ndetection systems such as RetinaNet. Compared with the predefined anchor\nscheme, we empirically find that MetaAnchor is more robust to anchor settings\nand bounding box distributions; in addition, it also shows the potential on\ntransfer tasks. Our experiment on COCO detection task shows that MetaAnchor\nconsistently outperforms the counterparts in various scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 05:20:20 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 13:52:55 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Yang", "Tong", ""], ["Zhang", "Xiangyu", ""], ["Li", "Zeming", ""], ["Zhang", "Wenqiang", ""], ["Sun", "Jian", ""]]}, {"id": "1807.00983", "submitter": "Ahmad Babaeian Jelodar", "authors": "Ahmad Babaeian Jelodar, David Paulius, Yu Sun", "title": "Long Activity Video Understanding using Functional Object-Oriented\n  Network", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video understanding is one of the most challenging topics in computer vision.\nIn this paper, a four-stage video understanding pipeline is presented to\nsimultaneously recognize all atomic actions and the single on-going activity in\na video. This pipeline uses objects and motions from the video and a\ngraph-based knowledge representation network as prior reference. Two deep\nnetworks are trained to identify objects and motions in each video sequence\nassociated with an action. Low Level image features are then used to identify\nobjects of interest in that video sequence. Confidence scores are assigned to\nobjects of interest based on their involvement in the action and to motion\nclasses based on results from a deep neural network that classifies the\non-going action in video into motion classes. Confidence scores are computed\nfor each candidate functional unit associated with an action using a knowledge\nrepresentation network, object confidences, and motion confidences. Each action\nis therefore associated with a functional unit and the sequence of actions is\nfurther evaluated to identify the single on-going activity in the video. The\nknowledge representation used in the pipeline is called the functional\nobject-oriented network which is a graph-based network useful for encoding\nknowledge about manipulation tasks. Experiments are performed on a dataset of\ncooking videos to test the proposed algorithm with action inference and\nactivity classification. Experiments show that using functional object oriented\nnetwork improves video understanding significantly.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 05:33:44 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Jelodar", "Ahmad Babaeian", ""], ["Paulius", "David", ""], ["Sun", "Yu", ""]]}, {"id": "1807.01001", "submitter": "Patrick Wenzel", "authors": "Patrick Wenzel, Qadeer Khan, Daniel Cremers, Laura Leal-Taix\\'e", "title": "Modular Vehicle Control for Transferring Semantic Information Between\n  Weather Conditions Using GANs", "comments": "2nd Conference on Robot Learning (CoRL 2018), Z\\\"urich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though end-to-end supervised learning has shown promising results for\nsensorimotor control of self-driving cars, its performance is greatly affected\nby the weather conditions under which it was trained, showing poor\ngeneralization to unseen conditions. In this paper, we show how knowledge can\nbe transferred using semantic maps to new weather conditions without the need\nto obtain new ground truth data. To this end, we propose to divide the task of\nvehicle control into two independent modules: a control module which is only\ntrained on one weather condition for which labeled steering data is available,\nand a perception module which is used as an interface between new weather\nconditions and the fixed control module. To generate the semantic data needed\nto train the perception module, we propose to use a generative adversarial\nnetwork (GAN)-based model to retrieve the semantic information for the new\nconditions in an unsupervised manner. We introduce a master-servant\narchitecture, where the master model (semantic labels available) trains the\nservant model (semantic labels not available). We show that our proposed method\ntrained with ground truth data for a single weather condition is capable of\nachieving similar results on the task of steering angle prediction as an\nend-to-end model trained with ground truth data of 15 different weather\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 07:29:19 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 14:01:46 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Wenzel", "Patrick", ""], ["Khan", "Qadeer", ""], ["Cremers", "Daniel", ""], ["Leal-Taix\u00e9", "Laura", ""]]}, {"id": "1807.01026", "submitter": "Mikel Bober-Irizar", "authors": "Eng-Jon Ong, Sameed Husain, Mikel Bober-Irizar, Miroslaw Bober", "title": "Deep Architectures and Ensembles for Semantic Video Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of accurate semantic labelling of short\nvideos. To this end, a multitude of different deep nets, ranging from\ntraditional recurrent neural networks (LSTM, GRU), temporal agnostic networks\n(FV,VLAD,BoW), fully connected neural networks mid-stage AV fusion and others.\nAdditionally, we also propose a residual architecture-based DNN for video\nclassification, with state-of-the art classification performance at\nsignificantly reduced complexity. Furthermore, we propose four new approaches\nto diversity-driven multi-net ensembling, one based on fast correlation measure\nand three incorporating a DNN-based combiner. We show that significant\nperformance gains can be achieved by ensembling diverse nets and we investigate\nfactors contributing to high diversity. Based on the extensive YouTube8M\ndataset, we provide an in-depth evaluation and analysis of their behaviour. We\nshow that the performance of the ensemble is state-of-the-art achieving the\nhighest accuracy on the YouTube-8M Kaggle test data. The performance of the\nensemble of classifiers was also evaluated on the HMDB51 and UCF101 datasets,\nand show that the resulting method achieves comparable accuracy with\nstate-of-the-art methods using similar input features.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:49:47 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:22:05 GMT"}, {"version": "v3", "created": "Sun, 7 Oct 2018 14:51:29 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ong", "Eng-Jon", ""], ["Husain", "Sameed", ""], ["Bober-Irizar", "Mikel", ""], ["Bober", "Miroslaw", ""]]}, {"id": "1807.01028", "submitter": "Massimiliano Mancini", "authors": "Massimiliano Mancini, Hakan Karaoguz, Elisa Ricci, Patric Jensfelt and\n  Barbara Caputo", "title": "Kitting in the Wild through Online Domain Adaptation", "comments": "Accepted to IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological developments call for increasing perception and action\ncapabilities of robots. Among other skills, vision systems that can adapt to\nany possible change in the working conditions are needed. Since these\nconditions are unpredictable, we need benchmarks which allow to assess the\ngeneralization and robustness capabilities of our visual recognition\nalgorithms. In this work we focus on robotic kitting in unconstrained\nscenarios. As a first contribution, we present a new visual dataset for the\nkitting task. Differently from standard object recognition datasets, we provide\nimages of the same objects acquired under various conditions where camera,\nillumination and background are changed. This novel dataset allows for testing\nthe robustness of robot visual recognition algorithms to a series of different\ndomain shifts both in isolation and unified. Our second contribution is a novel\nonline adaptation algorithm for deep models, based on batch-normalization\nlayers, which allows to continuously adapt a model to the current working\nconditions. Differently from standard domain adaptation algorithms, it does not\nrequire any image from the target domain at training time. We benchmark the\nperformance of the algorithm on the proposed dataset, showing its capability to\nfill the gap between the performances of a standard architecture and its\ncounterpart adapted offline to the given target domain.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:53:27 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Mancini", "Massimiliano", ""], ["Karaoguz", "Hakan", ""], ["Ricci", "Elisa", ""], ["Jensfelt", "Patric", ""], ["Caputo", "Barbara", ""]]}, {"id": "1807.01052", "submitter": "Romain Cohendet", "authors": "Romain Cohendet, Claire-H\\'el\\`ene Demarty, Ngoc Duong, Mats\n  Sj\\\"oberg, Bogdan Ionescu, Thanh-Toan Do, France Rennes", "title": "MediaEval 2018: Predicting Media Memorability Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Predicting Media Memorability task, which is\nproposed as part of the MediaEval 2018 Benchmarking Initiative for Multimedia\nEvaluation. Participants are expected to design systems that automatically\npredict memorability scores for videos, which reflect the probability of a\nvideo being remembered. In contrast to previous work in image memorability\nprediction, where memorability was measured a few minutes after memorization,\nthe proposed dataset comes with short-term and long-term memorability\nannotations. All task characteristics are described, namely: the task's\nchallenges and breakthrough, the released data set and ground truth, the\nrequired participant runs and the evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 09:47:38 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Cohendet", "Romain", ""], ["Demarty", "Claire-H\u00e9l\u00e8ne", ""], ["Duong", "Ngoc", ""], ["Sj\u00f6berg", "Mats", ""], ["Ionescu", "Bogdan", ""], ["Do", "Thanh-Toan", ""], ["Rennes", "France", ""]]}, {"id": "1807.01068", "submitter": "Marco Reisert", "authors": "Marco Reisert, Volker A. Coenen, Christoph Kaller, Karl Egger, Henrik\n  Skibbe", "title": "HAMLET: Hierarchical Harmonic Filters for Learning Tracts from Diffusion\n  MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose HAMLET, a novel tract learning algorithm, which,\nafter training, maps raw diffusion weighted MRI directly onto an image which\nsimultaneously indicates tract direction and tract presence. The automatic\nlearning of fiber tracts based on diffusion MRI data is a rather new idea,\nwhich tries to overcome limitations of atlas-based techniques. HAMLET takes a\nsuch an approach. Unlike the current trend in machine learning, HAMLET has only\na small number of free parameters HAMLET is based on spherical tensor algebra\nwhich allows a translation and rotation covariant treatment of the problem.\nHAMLET is based on a repeated application of convolutions and non-linearities,\nwhich all respect the rotation covariance. The intrinsic treatment of such\nbasic image transformations in HAMLET allows the training and generalization of\nthe algorithm without any additional data augmentation. We demonstrate the\nperformance of our approach for twelve prominent bundles, and show that the\nobtained tract estimates are robust and reliable. It is also shown that the\nlearned models are portable from one sequence to another.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 10:25:05 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Reisert", "Marco", ""], ["Coenen", "Volker A.", ""], ["Kaller", "Christoph", ""], ["Egger", "Karl", ""], ["Skibbe", "Henrik", ""]]}, {"id": "1807.01103", "submitter": "Jie Guo", "authors": "Jie Guo, Tingfa Xu, Shenwang Jiang, Ziyi Shen", "title": "Stochastic Channel Decorrelation Network and Its Application to Visual\n  Tracking", "comments": "8 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have dominated many computer vision\ndomains because of their great power to extract good features automatically.\nHowever, many deep CNNs-based computer vison tasks suffer from lack of training\ndata while there are millions of parameters in the deep models. Obviously,\nthese two biphase violation facts will result in parameter redundancy of many\npoorly designed deep CNNs. Therefore, we look deep into the existing CNNs and\nfind that the redundancy of network parameters comes from the correlation\nbetween features in different channels within a convolutional layer. To solve\nthis problem, we propose the stochastic channel decorrelation (SCD) block\nwhich, in every iteration, randomly selects multiple pairs of channels within a\nconvolutional layer and calculates their normalized cross correlation (NCC).\nThen a squared max-margin loss is proposed as the objective of SCD to suppress\ncorrelation and keep diversity between channels explicitly. The proposed SCD is\nvery flexible and can be applied to any current existing CNN models simply.\nBased on the SCD and the Fully-Convolutional Siamese Networks, we proposed a\nvisual tracking algorithm to verify the effectiveness of SCD.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:03:21 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 12:40:50 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Guo", "Jie", ""], ["Xu", "Tingfa", ""], ["Jiang", "Shenwang", ""], ["Shen", "Ziyi", ""]]}, {"id": "1807.01122", "submitter": "Nathaniel Blanchard", "authors": "Nathaniel Blanchard, Daniel Moreira, Aparna Bharati, Walter J.\n  Scheirer", "title": "Getting the subtext without the text: Scalable multimodal sentiment\n  classification from visual and acoustic modalities", "comments": "Published in the First Workshop on Computational Modeling of Human\n  Multimodal Language - ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, video blogs (vlogs) have become an extremely popular\nmethod through which people express sentiment. The ubiquitousness of these\nvideos has increased the importance of multimodal fusion models, which\nincorporate video and audio features with traditional text features for\nautomatic sentiment detection. Multimodal fusion offers a unique opportunity to\nbuild models that learn from the full depth of expression available to human\nviewers. In the detection of sentiment in these videos, acoustic and video\nfeatures provide clarity to otherwise ambiguous transcripts. In this paper, we\npresent a multimodal fusion model that exclusively uses high-level video and\naudio features to analyze spoken sentences for sentiment. We discard\ntraditional transcription features in order to minimize human intervention and\nto maximize the deployability of our model on at-scale real-world data. We\nselect high-level features for our model that have been successful in nonaffect\ndomains in order to test their generalizability in the sentiment detection\ndomain. We train and test our model on the newly released CMU Multimodal\nOpinion Sentiment and Emotion Intensity (CMUMOSEI) dataset, obtaining an F1\nscore of 0.8049 on the validation set and an F1 score of 0.6325 on the held-out\nchallenge test set.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:38:11 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Blanchard", "Nathaniel", ""], ["Moreira", "Daniel", ""], ["Bharati", "Aparna", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1807.01136", "submitter": "Masanari Kimura", "authors": "Masanari Kimura, Takashi Yanagihara", "title": "Anomaly Detection Using GANs for Visual Inspection in Noisy Training\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection and the quantification of anomalies in image data are critical\ntasks in industrial scenes such as detecting micro scratches on product. In\nrecent years, due to the difficulty of defining anomalies and the limit of\ncorrecting their labels, research on unsupervised anomaly detection using\ngenerative models has attracted attention. Generally, in those studies, only\nnormal images are used for training to model the distribution of normal images.\nThe model measures the anomalies in the target images by reproducing the most\nsimilar images and scoring image patches indicating their fit to the learned\ndistribution. This approach is based on a strong presumption; the trained model\nshould not be able to generate abnormal images. However, in reality, the model\ncan generate abnormal images mainly due to noisy normal data which include\nsmall abnormal pixels, and such noise severely affects the accuracy of the\nmodel. Therefore, we propose a novel anomaly detection method to distort the\ndistribution of the model with existing abnormal images. The proposed method\ndetects pixel-level micro anomalies with a high accuracy from 1024x1024 high\nresolution images which are actually used in an industrial scene. In this\npaper, we share experimental results on open datasets, due to the\nconfidentiality of the data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 13:03:11 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 15:44:07 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Kimura", "Masanari", ""], ["Yanagihara", "Takashi", ""]]}, {"id": "1807.01172", "submitter": "Jinzheng Cai", "authors": "Jinzheng Cai, Youbao Tang, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao,\n  Lin Yang, Ronald M. Summers", "title": "Accurate Weakly-Supervised Deep Lesion Segmentation using Large-Scale\n  Clinical Annotations: Slice-Propagated 3D Mask Generation from 2D RECIST", "comments": "9 pages, 3 figures, accepted to MICCAI 2018. arXiv admin note:\n  substantial text overlap with arXiv:1801.08614", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric lesion segmentation from computed tomography (CT) images is a\npowerful means to precisely assess multiple time-point lesion/tumor changes.\nHowever, because manual 3D segmentation is prohibitively time consuming,\ncurrent practices rely on an imprecise surrogate called response evaluation\ncriteria in solid tumors (RECIST). Despite their coarseness, RECIST markers are\ncommonly found in current hospital picture and archiving systems (PACS),\nmeaning they can provide a potentially powerful, yet extraordinarily\nchallenging, source of weak supervision for full 3D segmentation. Toward this\nend, we introduce a convolutional neural network (CNN) based weakly supervised\nslice-propagated segmentation (WSSS) method to 1) generate the initial lesion\nsegmentation on the axial RECIST-slice; 2) learn the data distribution on\nRECIST-slices; 3) extrapolate to segment the whole lesion slice by slice to\nfinally obtain a volumetric segmentation. To validate the proposed method, we\nfirst test its performance on a fully annotated lymph node dataset, where WSSS\nperforms comparably to its fully supervised counterparts. We then test on a\ncomprehensive lesion dataset with 32,735 RECIST marks, where we report a mean\nDice score of 92% on RECIST-marked slices and 76% on the entire 3D volumes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 00:17:42 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Cai", "Jinzheng", ""], ["Tang", "Youbao", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Yan", "Ke", ""], ["Xiao", "Jing", ""], ["Yang", "Lin", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1807.01197", "submitter": "Chang Gao", "authors": "Chang Gao, Derun Gu, Fangjun Zhang, Yizhou Yu", "title": "ReCoNet: Real-time Coherent Video Style Transfer Network", "comments": "16 pages, 7 figures. For supplementary material, see\n  https://www.dropbox.com/s/go6f7uopjjsala7/ReCoNet%20Supplementary%20Material.pdf?dl=0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image style transfer models based on convolutional neural networks usually\nsuffer from high temporal inconsistency when applied to videos. Some video\nstyle transfer models have been proposed to improve temporal consistency, yet\nthey fail to guarantee fast processing speed, nice perceptual style quality and\nhigh temporal consistency at the same time. In this paper, we propose a novel\nreal-time video style transfer model, ReCoNet, which can generate temporally\ncoherent style transfer videos while maintaining favorable perceptual styles. A\nnovel luminance warping constraint is added to the temporal loss at the output\nlevel to capture luminance changes between consecutive frames and increase\nstylization stability under illumination effects. We also propose a novel\nfeature-map-level temporal loss to further enhance temporal consistency on\ntraceable objects. Experimental results indicate that our model exhibits\noutstanding performance both qualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 14:11:16 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 05:48:14 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Gao", "Chang", ""], ["Gu", "Derun", ""], ["Zhang", "Fangjun", ""], ["Yu", "Yizhou", ""]]}, {"id": "1807.01216", "submitter": "Muzammal Naseer", "authors": "Muzammal Naseer, Salman H. Khan, Fatih Porikli", "title": "Local Gradients Smoothing: Defense against localized adversarial attacks", "comments": "Accepted At WACV-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have shown vulnerability to adversarial attacks,\ni.e., carefully perturbed inputs designed to mislead the network at inference\ntime. Recently introduced localized attacks, Localized and Visible Adversarial\nNoise (LaVAN) and Adversarial patch, pose a new challenge to deep learning\nsecurity by adding adversarial noise only within a specific region without\naffecting the salient objects in an image. Driven by the observation that such\nattacks introduce concentrated high-frequency changes at a particular image\nlocation, we have developed an effective method to estimate noise location in\ngradient domain and transform those high activation regions caused by\nadversarial noise in image domain while having minimal effect on the salient\nobject that is important for correct classification. Our proposed Local\nGradients Smoothing (LGS) scheme achieves this by regularizing gradients in the\nestimated noisy region before feeding the image to DNN for inference. We have\nshown the effectiveness of our method in comparison to other defense methods\nincluding Digital Watermarking, JPEG compression, Total Variance Minimization\n(TVM) and Feature squeezing on ImageNet dataset. In addition, we systematically\nstudy the robustness of the proposed defense mechanism against Back Pass\nDifferentiable Approximation (BPDA), a state of the art attack recently\ndeveloped to break defenses that transform an input sample to minimize the\nadversarial effect. Compared to other defense mechanisms, LGS is by far the\nmost resistant to BPDA in localized adversarial attack setting.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 14:48:05 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 05:45:03 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Naseer", "Muzammal", ""], ["Khan", "Salman H.", ""], ["Porikli", "Fatih", ""]]}, {"id": "1807.01232", "submitter": "Adam Van Etten", "authors": "Adam Van Etten, Dave Lindenbaum, Todd M. Bacastow", "title": "SpaceNet: A Remote Sensing Dataset and Challenge Series", "comments": "10 pages, 5 figures, 2 tables, 5 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foundational mapping remains a challenge in many parts of the world,\nparticularly in dynamic scenarios such as natural disasters when timely updates\nare critical. Updating maps is currently a highly manual process requiring a\nlarge number of human labelers to either create features or rigorously validate\nautomated outputs. We propose that the frequent revisits of earth imaging\nsatellite constellations may accelerate existing efforts to quickly update\nfoundational maps when combined with advanced machine learning techniques.\nAccordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and\nNVIDIA), released a large corpus of labeled satellite imagery on Amazon Web\nServices (AWS) called SpaceNet. The SpaceNet partners also launched a series of\npublic prize competitions to encourage improvement of remote sensing machine\nlearning algorithms. The first two of these competitions focused on automated\nbuilding footprint extraction, and the most recent challenge focused on road\nnetwork extraction. In this paper we discuss the SpaceNet imagery, labels,\nevaluation metrics, prize challenge results to date, and future plans for the\nSpaceNet challenge series.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:12:59 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 21:11:12 GMT"}, {"version": "v3", "created": "Mon, 15 Jul 2019 02:40:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Van Etten", "Adam", ""], ["Lindenbaum", "Dave", ""], ["Bacastow", "Todd M.", ""]]}, {"id": "1807.01238", "submitter": "Sai Charan Jajimi", "authors": "Sai Charan Jajimi", "title": "Fast Fourier-Based Generation of the Compression Matrix for\n  Deterministic Compressed Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary goal of this work is to review the importance of data compression\nand present a fast Fourier-based method for generating the deterministic\ncompression matrix in the area of deterministic compressed sensing. The\nprinciple concepts of data compression such as general process of data\ncompression, sparse signals, coherence matrix and Restricted Isometry Property\n(RIP) have been defined. We have introduced two methods of sparse data\ncompression. The first method is formed by utilizing a stochastic matrix which\nis a common approach, and the second method is created by utilizing a\ndeterministic matrix which is proposed more recently. The main goal of this\nwork is to improve the execution time of the deterministic matrix generation.\nThe execution time is related to the generation method of the deterministic\nmatrix. Furthermore, we have implemented a software which makes it possible to\ncompare different methods of reconstructing data compression. To make this\ncomparison, it is necessary to draw and compare certain graphs, e.g. phase\ntransition, the ratio of output signal to noise and input signal to noise,\nsignal to noise output and also the ratio of percentage of accurate\nreconstructing and order of sparse signals for various reconstructing methods.\nTo facilitate this process, the user would be able to draw his/her favorite\ngraphs in GUI environment.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 04:51:35 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Jajimi", "Sai Charan", ""]]}, {"id": "1807.01253", "submitter": "Wenbo Li", "authors": "Wenbo Li, Ming-Ching Chang, Siwei Lyu", "title": "Who did What at Where and When: Simultaneous Multi-Person Tracking and\n  Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a bootstrapping framework to simultaneously improve multi-person\ntracking and activity recognition at individual, interaction and social group\nactivity levels. The inference consists of identifying trajectories of all\npedestrian actors, individual activities, pairwise interactions, and collective\nactivities, given the observed pedestrian detections. Our method uses a\ngraphical model to represent and solve the joint tracking and recognition\nproblems via multi-stages: (1) activity-aware tracking, (2) joint interaction\nrecognition and occlusion recovery, and (3) collective activity recognition. We\nsolve the where and when problem with visual tracking, as well as the who and\nwhat problem with recognition. High-order correlations among the visible and\noccluded individuals, pairwise interactions, groups, and activities are then\nsolved using a hypergraph formulation within the Bayesian framework.\nExperiments on several benchmarks show the advantages of our approach over\nstate-of-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:51:25 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Li", "Wenbo", ""], ["Chang", "Ming-Ching", ""], ["Lyu", "Siwei", ""]]}, {"id": "1807.01257", "submitter": "Bo Zhou", "authors": "Bo Zhou, Yuemeng Li, Jiangcong Wang", "title": "A Weakly Supervised Adaptive DenseNet for Classifying Thoracic Diseases\n  and Identifying Abnormalities", "comments": "10 pages, 6 figures; accepted by IEEE Winter Conference on\n  Applications of Computer Vision (2019 WACV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a weakly supervised deep learning model for classifying thoracic\ndiseases and identifying abnormalities in chest radiography. In this work,\ninstead of learning from medical imaging data with region-level annotations,\nour model was merely trained on imaging data with image-level labels to\nclassify diseases, and is able to identify abnormal image regions\nsimultaneously. Our model consists of a customized pooling structure and an\nadaptive DenseNet front-end, which can effectively recognize possible disease\nfeatures for classification and localization tasks. Our method has been\nvalidated on the publicly available ChestX-ray14 dataset. Experimental results\nhave demonstrated that our classification and localization prediction\nperformance achieved significant improvement over the previous models on the\nChestX-ray14 dataset. In summary, our network can produce accurate disease\nclassification and localization, which can potentially support clinical\ndecisions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 16:03:10 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 20:09:24 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Zhou", "Bo", ""], ["Li", "Yuemeng", ""], ["Wang", "Jiangcong", ""]]}, {"id": "1807.01276", "submitter": "Angang Cui", "authors": "Angang Cui, Meng Wen, Haiyang Li, Jigen Peng", "title": "A non-convex approach to low-rank and sparse matrix decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a nonconvex approach to the problem of low-rank and\nsparse matrix decomposition. In our nonconvex method, we replace the rank\nfunction and the $l_{0}$-norm of a given matrix with a non-convex fraction\nfunction on the singular values and the elements of the matrix respectively. An\nalternative direction method of multipliers algorithm is utilized to solve our\nproposed nonconvex problem with the nonconvex fraction function penalty.\nNumerical experiments on some low-rank and sparse matrix decomposition problems\nshow that our method performs very well in recovering low-rank matrices which\nare heavily corrupted by large sparse errors.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 03:25:42 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 07:40:01 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Cui", "Angang", ""], ["Wen", "Meng", ""], ["Li", "Haiyang", ""], ["Peng", "Jigen", ""]]}, {"id": "1807.01312", "submitter": "Gilad Divon", "authors": "Gilad Divon, Ayellet Tal", "title": "Viewpoint Estimation-Insights & Model", "comments": "17 pages, ECCV2018 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of viewpoint estimation of an object in a\ngiven image. It presents five key insights that should be taken into\nconsideration when designing a CNN that solves the problem. Based on these\ninsights, the paper proposes a network in which (i) The architecture jointly\nsolves detection, classification, and viewpoint estimation. (ii) New types of\ndata are added and trained on. (iii) A novel loss function, which takes into\naccount both the geometry of the problem and the new types of data, is propose.\nOur network improves the state-of-the-art results for this problem by 9.8%.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 17:57:58 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Divon", "Gilad", ""], ["Tal", "Ayellet", ""]]}, {"id": "1807.01347", "submitter": "Sebastian Kaltwang", "authors": "Brook Roberts, Sebastian Kaltwang, Sina Samangooei, Mark Pender-Bare,\n  Konstantinos Tertikas and John Redford", "title": "A Dataset for Lane Instance Segmentation in Urban Environments", "comments": "ECCV camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous vehicles require knowledge of the surrounding road layout, which\ncan be predicted by state-of-the-art CNNs. This work addresses the current lack\nof data for determining lane instances, which are needed for various driving\nmanoeuvres. The main issue is the time-consuming manual labelling process,\ntypically applied per image. We notice that driving the car is itself a form of\nannotation. Therefore, we propose a semi-automated method that allows for\nefficient labelling of image sequences by utilising an estimated road plane in\n3D based on where the car has driven and projecting labels from this plane into\nall images of the sequence. The average labelling time per image is reduced to\n5 seconds and only an inexpensive dash-cam is required for data capture. We are\nreleasing a dataset of 24,000 images and additionally show experimental\nsemantic segmentation and instance segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 19:04:35 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 11:03:01 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Roberts", "Brook", ""], ["Kaltwang", "Sebastian", ""], ["Samangooei", "Sina", ""], ["Pender-Bare", "Mark", ""], ["Tertikas", "Konstantinos", ""], ["Redford", "John", ""]]}, {"id": "1807.01394", "submitter": "Hadi Kiapour", "authors": "Shuai Zheng and Fan Yang and M. Hadi Kiapour and Robinson Piramuthu", "title": "ModaNet: A Large-Scale Street Fashion Dataset with Polygon Annotations", "comments": "Accepted as a full paper for an oral presentation at ACM Multimedia\n  2018, Seoul, South Korea. ModaNet is only for non-commercial research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding clothes from a single image has strong commercial and cultural\nimpacts on modern societies. However, this task remains a challenging computer\nvision problem due to wide variations in the appearance, style, brand and\nlayering of clothing items. We present a new database called ModaNet, a\nlarge-scale collection of images based on Paperdoll dataset. Our dataset\nprovides 55,176 street images, fully annotated with polygons on top of the 1\nmillion weakly annotated street images in Paperdoll. ModaNet aims to provide a\ntechnical benchmark to fairly evaluate the progress of applying the latest\ncomputer vision techniques that rely on large data for fashion understanding.\nThe rich annotation of the dataset allows to measure the performance of\nstate-of-the-art algorithms for object detection, semantic segmentation and\npolygon prediction on street fashion images in detail. The polygon-based\nannotation dataset has been released https://github.com/eBay/modanet, we also\nhost the leaderboard at EvalAI:\nhttps://evalai.cloudcv.org/featured-challenges/136/overview.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 23:20:36 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 05:23:53 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 09:18:52 GMT"}, {"version": "v4", "created": "Wed, 10 Apr 2019 16:57:55 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zheng", "Shuai", ""], ["Yang", "Fan", ""], ["Kiapour", "M. Hadi", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1807.01401", "submitter": "Henry Kvinge", "authors": "Elin Farnell, Henry Kvinge, Michael Kirby, Chris Peterson", "title": "Endmember Extraction on the Grassmannian", "comments": "To appear in Proceedings of the 2018 IEEE Data Science Workshop,\n  Lausanne, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Endmember extraction plays a prominent role in a variety of data analysis\nproblems as endmembers often correspond to data representing the purest or best\nrepresentative of some feature. Identifying endmembers then can be useful for\nfurther identification and classification tasks. In settings with\nhigh-dimensional data, such as hyperspectral imagery, it can be useful to\nconsider endmembers that are subspaces as they are capable of capturing a wider\nrange of variations of a signature. The endmember extraction problem in this\nsetting thus translates to finding the vertices of the convex hull of a set of\npoints on a Grassmannian. In the presence of noise, it can be less clear\nwhether a point should be considered a vertex. In this paper, we propose an\nalgorithm to extract endmembers on a Grassmannian, identify subspaces of\ninterest that lie near the boundary of a convex hull, and demonstrate the use\nof the algorithm on a synthetic example and on the 220 spectral band AVIRIS\nIndian Pines hyperspectral image.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 23:35:47 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Farnell", "Elin", ""], ["Kvinge", "Henry", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "1807.01418", "submitter": "Minho Ha", "authors": "Minho Ha, Younghoon Byeon, Youngjoo Lee, and Sunggu Lee", "title": "Selective Deep Convolutional Neural Network for Low Cost Distorted Image\n  Classification", "comments": "The authors think that the results of this paper are insufficient.\n  Therefore, we will improve the experiments and analyses, and then rewrite the\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks have proven to be well suited for image\nclassification applications. However, if there is distortion in the image, the\nclassification accuracy can be significantly degraded, even with\nstate-of-the-art neural networks. The accuracy cannot be significantly improved\nby simply training with distorted images. Instead, this paper proposes a\nmultiple neural network topology referred to as a selective deep convolutional\nneural network. By modifying existing state-of-the-art neural networks in the\nproposed manner, it is shown that a similar level of classification accuracy\ncan be achieved, but at a significantly lower cost. The cost reduction is\nobtained primarily through the use of fewer weight parameters. Using fewer\nweights reduces the number of multiply-accumulate operations and also reduces\nthe energy required for data accesses. Finally, it is shown that the\neffectiveness of the proposed selective deep convolutional neural network can\nbe further improved by combining it with previously proposed network cost\nreduction methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 01:06:45 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 00:06:21 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Ha", "Minho", ""], ["Byeon", "Younghoon", ""], ["Lee", "Youngjoo", ""], ["Lee", "Sunggu", ""]]}, {"id": "1807.01424", "submitter": "Hyun-Chul Choi", "authors": "Hyun-Chul Choi and Minseong Kim", "title": "Unbiased Image Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent fast image style transferring methods use feed-forward neural networks\nto generate an output image of desired style strength from the input pair of a\ncontent and a target style image. In the existing methods, the image of\nintermediate style between the content and the target style is obtained by\ndecoding a linearly interpolated feature in encoded feature space. However,\nthere has been no work on analyzing the effectiveness of this kind of style\nstrength interpolation so far. In this paper, we tackle the missing work on the\nin-depth analysis of style interpolation and propose a method that is more\neffective in controlling style strength. We interpret the training task of a\nstyle transfer network as a regression learning between the control parameter\nand output style strength. In this understanding, the existing methods are\nbiased due to the fact that training is performed with one-sided data of full\nstyle strength (alpha = 1.0). Thus, this biased learning does not guarantee the\ngeneration of a desired intermediate style corresponding to the style control\nparameter between 0.0 and 1.0. To solve this problem of the biased network, we\npropose an unbiased learning technique which uses unbiased training data and\ncorresponding unbiased loss for alpha = 0.0 to make the feed-forward networks\nto generate a zero-style image, i.e., content image when alpha = 0.0. Our\nexperimental results verified that our unbiased learning method achieved the\nreconstruction of a content image with zero style strength, better regression\nspecification between style control parameter and output style, and more stable\nstyle transfer that is insensitive to the weight of style loss without additive\ncomplexity in image generating process.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 01:47:03 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 09:09:06 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Choi", "Hyun-Chul", ""], ["Kim", "Minseong", ""]]}, {"id": "1807.01430", "submitter": "Zhisheng Wang", "authors": "Zhisheng Wang, Fangxuan Sun, Jun Lin, Zhongfeng Wang and Bo Yuan", "title": "SGAD: Soft-Guided Adaptively-Dropped Neural Network", "comments": "9 pages, 4 figures; the first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been proven to have many redundancies.\nHence, many efforts have been made to compress DNNs. However, the existing\nmodel compression methods treat all the input samples equally while ignoring\nthe fact that the difficulties of various input samples being correctly\nclassified are different. To address this problem, DNNs with adaptive dropping\nmechanism are well explored in this work. To inform the DNNs how difficult the\ninput samples can be classified, a guideline that contains the information of\ninput samples is introduced to improve the performance. Based on the developed\nguideline and adaptive dropping mechanism, an innovative soft-guided\nadaptively-dropped (SGAD) neural network is proposed in this paper. Compared\nwith the 32 layers residual neural networks, the presented SGAD can reduce the\nFLOPs by 77% with less than 1% drop in accuracy on CIFAR-10.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 02:23:10 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Wang", "Zhisheng", ""], ["Sun", "Fangxuan", ""], ["Lin", "Jun", ""], ["Wang", "Zhongfeng", ""], ["Yuan", "Bo", ""]]}, {"id": "1807.01438", "submitter": "Di Xie", "authors": "Tao Song and Leiyu Sun and Di Xie and Haiming Sun and Shiliang Pu", "title": "Small-scale Pedestrian Detection Based on Somatic Topology Localization\n  and Temporal Feature Aggregation", "comments": "Accepted by ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A critical issue in pedestrian detection is to detect small-scale objects\nthat will introduce feeble contrast and motion blur in images and videos, which\nin our opinion should partially resort to deep-rooted annotation bias.\nMotivated by this, we propose a novel method integrated with somatic\ntopological line localization (TLL) and temporal feature aggregation for\ndetecting multi-scale pedestrians, which works particularly well with\nsmall-scale pedestrians that are relatively far from the camera. Moreover, a\npost-processing scheme based on Markov Random Field (MRF) is introduced to\neliminate ambiguities in occlusion cases. Applying with these methodologies\ncomprehensively, we achieve best detection performance on Caltech benchmark and\nimprove performance of small-scale objects significantly (miss rate decreases\nfrom 74.53% to 60.79%). Beyond this, we also achieve competitive performance on\nCityPersons dataset and show the existence of annotation bias in KITTI dataset.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 03:33:29 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Song", "Tao", ""], ["Sun", "Leiyu", ""], ["Xie", "Di", ""], ["Sun", "Haiming", ""], ["Pu", "Shiliang", ""]]}, {"id": "1807.01440", "submitter": "Shan Lin", "authors": "Shan Lin, Haoliang Li, Chang-Tsun Li, Alex Chichung Kot", "title": "Multi-task Mid-level Feature Alignment Network for Unsupervised\n  Cross-Dataset Person Re-Identification", "comments": "Accepted by BMVC 2018 as Spotlight", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing person re-identification (Re-ID) approaches follow a supervised\nlearning framework, in which a large number of labelled matching pairs are\nrequired for training. Such a setting severely limits their scalability in\nreal-world applications where no labelled samples are available during the\ntraining phase. To overcome this limitation, we develop a novel unsupervised\nMulti-task Mid-level Feature Alignment (MMFA) network for the unsupervised\ncross-dataset person re-identification task. Under the assumption that the\nsource and target datasets share the same set of mid-level semantic attributes,\nour proposed model can be jointly optimised under the person's identity\nclassification and the attribute learning task with a cross-dataset mid-level\nfeature alignment regularisation term. In this way, the learned feature\nrepresentation can be better generalised from one dataset to another which\nfurther improve the person re-identification accuracy. Experimental results on\nfour benchmark datasets demonstrate that our proposed method outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 03:43:20 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:19:14 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Lin", "Shan", ""], ["Li", "Haoliang", ""], ["Li", "Chang-Tsun", ""], ["Kot", "Alex Chichung", ""]]}, {"id": "1807.01448", "submitter": "Karan Sikka", "authors": "Karuna Ahuja, Karan Sikka, Anirban Roy, Ajay Divakaran", "title": "Understanding Visual Ads by Aligning Symbols and Objects using\n  Co-Attention", "comments": "Accepted at CVPR 2018 workshop- Towards Automatic Understanding of\n  Visual Advertisements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We tackle the problem of understanding visual ads where given an ad image,\nour goal is to rank appropriate human generated statements describing the\npurpose of the ad. This problem is generally addressed by jointly embedding\nimages and candidate statements to establish correspondence. Decoding a visual\nad requires inference of both semantic and symbolic nuances referenced in an\nimage and prior methods may fail to capture such associations especially with\nweakly annotated symbols. In order to create better embeddings, we leverage an\nattention mechanism to associate image proposals with symbols and thus\neffectively aggregate information from aligned multimodal representations. We\npropose a multihop co-attention mechanism that iteratively refines the\nattention map to ensure accurate attention estimation. Our attention based\nembedding model is learned end-to-end guided by a max-margin loss function. We\nshow that our model outperforms other baselines on the benchmark Ad dataset and\nalso show qualitative results to highlight the advantages of using multihop\nco-attention.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 04:50:09 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Ahuja", "Karuna", ""], ["Sikka", "Karan", ""], ["Roy", "Anirban", ""], ["Divakaran", "Ajay", ""]]}, {"id": "1807.01452", "submitter": "Trung-Nghia Le", "authors": "Trung-Nghia Le and Akihiro Sugimoto", "title": "Semantic Instance Meets Salient Object: Study on Video Semantic Salient\n  Instance Segmentation", "comments": "accepted in WACV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on only semantic instances that only salient in a scene gains more\nbenefits for robot navigation and self-driving cars than looking at all objects\nin the whole scene. This paper pushes the envelope on salient regions in a\nvideo to decompose them into semantically meaningful components, namely,\nsemantic salient instances. We provide the baseline for the new task of video\nsemantic salient instance segmentation (VSSIS), that is, Semantic Instance -\nSalient Object (SISO) framework. The SISO framework is simple yet efficient,\nleveraging advantages of two different segmentation tasks, i.e. semantic\ninstance segmentation and salient object segmentation to eventually fuse them\nfor the final result. In SISO, we introduce a sequential fusion by looking at\noverlapping pixels between semantic instances and salient regions to have\nnon-overlapping instances one by one. We also introduce a recurrent instance\npropagation to refine the shapes and semantic meanings of instances, and an\nidentity tracking to maintain both the identity and the semantic meaning of\ninstances over the entire video. Experimental results demonstrated the\neffectiveness of our SISO baseline, which can handle occlusions in videos. In\naddition, to tackle the task of VSSIS, we augment the DAVIS-2017 benchmark\ndataset by assigning semantic ground-truth for salient instance labels,\nobtaining SEmantic Salient Instance Video (SESIV) dataset. Our SESIV dataset\nconsists of 84 high-quality video sequences with pixel-wisely per-frame\nground-truth labels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 05:30:52 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:59:21 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 06:11:28 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Le", "Trung-Nghia", ""], ["Sugimoto", "Akihiro", ""]]}, {"id": "1807.01455", "submitter": "Sanping Zhou", "authors": "Sanping Zhou, Jinjun Wang, Deyu Meng, Yudong Liang, Yihong Gong,\n  Nanning Zheng", "title": "Discriminative Feature Learning with Foreground Attention for Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of person re-identification (Re-ID) has been seriously\neffected by the large cross-view appearance variations caused by mutual\nocclusions and background clutters. Hence learning a feature representation\nthat can adaptively emphasize the foreground persons becomes very critical to\nsolve the person Re-ID problem. In this paper, we propose a simple yet\neffective foreground attentive neural network (FANN) to learn a discriminative\nfeature representation for person Re-ID, which can adaptively enhance the\npositive side of foreground and weaken the negative side of background.\nSpecifically, a novel foreground attentive subnetwork is designed to drive the\nnetwork's attention, in which a decoder network is used to reconstruct the\nbinary mask by using a novel local regression loss function, and an encoder\nnetwork is regularized by the decoder network to focus its attention on the\nforeground persons. The resulting feature maps of encoder network are further\nfed into the body part subnetwork and feature fusion subnetwork to learn\ndiscriminative features. Besides, a novel symmetric triplet loss function is\nintroduced to supervise feature learning, in which the intra-class distance is\nminimized and the inter-class distance is maximized in each triplet unit,\nsimultaneously. Training our FANN in a multi-task learning framework, a\ndiscriminative feature representation can be learned to find out the matched\nreference to each probe among various candidates in the gallery. Extensive\nexperimental results on several public benchmark datasets are evaluated, which\nhave shown clear improvements of our method over the state-of-the-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 06:18:17 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 21:34:46 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Zhou", "Sanping", ""], ["Wang", "Jinjun", ""], ["Meng", "Deyu", ""], ["Liang", "Yudong", ""], ["Gong", "Yihong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1807.01459", "submitter": "Sheng Jin", "authors": "Sheng Jin, Hongxun Yao, Xiaoshuai Sun, Shangchen Zhou, Lei Zhang,\n  Xiansheng Hua", "title": "Deep Saliency Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, hashing methods have been proved to be effective and\nefficient for the large-scale Web media search. However, the existing general\nhashing methods have limited discriminative power for describing fine-grained\nobjects that share similar overall appearance but have subtle difference. To\nsolve this problem, we for the first time introduce the attention mechanism to\nthe learning of fine-grained hashing codes. Specifically, we propose a novel\ndeep hashing model, named deep saliency hashing (DSaH), which automatically\nmines salient regions and learns semantic-preserving hashing codes\nsimultaneously. DSaH is a two-step end-to-end model consisting of an attention\nnetwork and a hashing network. Our loss function contains three basic\ncomponents, including the semantic loss, the saliency loss, and the\nquantization loss. As the core of DSaH, the saliency loss guides the attention\nnetwork to mine discriminative regions from pairs of images. We conduct\nextensive experiments on both fine-grained and general retrieval datasets for\nperformance evaluation. Experimental results on fine-grained datasets,\nincluding Oxford Flowers-17, Stanford Dogs-120, and CUB Bird demonstrate that\nour DSaH performs the best for fine-grained retrieval task and beats the\nstrongest competitor (DTQ) by approximately 10% on both Stanford Dogs-120 and\nCUB Bird. DSaH is also comparable to several state-of-the-art hashing methods\non general datasets, including CIFAR-10 and NUS-WIDE.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 06:31:13 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 05:30:49 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Jin", "Sheng", ""], ["Yao", "Hongxun", ""], ["Sun", "Xiaoshuai", ""], ["Zhou", "Shangchen", ""], ["Zhang", "Lei", ""], ["Hua", "Xiansheng", ""]]}, {"id": "1807.01462", "submitter": "Duc Nguyen", "authors": "Anh-Duc Nguyen, Woojae Kim, Jongyoo Kim, and Sanghoon Lee", "title": "Video Frame Interpolation by Plug-and-Play Deep Locally Linear Embedding", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative framework which takes on the video frame\ninterpolation problem. Our framework, which we call Deep Locally Linear\nEmbedding (DeepLLE), is powered by a deep convolutional neural network (CNN)\nwhile it can be used instantly like conventional models. DeepLLE fits an\nauto-encoding CNN to a set of several consecutive frames and embeds a linearity\nconstraint on the latent codes so that new frames can be generated by\ninterpolating new latent codes. Different from the current deep learning\nparadigm which requires training on large datasets, DeepLLE works in a\nplug-and-play and unsupervised manner, and is able to generate an arbitrary\nnumber of frames. Thorough experiments demonstrate that without bells and\nwhistles, our method is highly competitive among current state-of-the-art\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 06:49:03 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Nguyen", "Anh-Duc", ""], ["Kim", "Woojae", ""], ["Kim", "Jongyoo", ""], ["Lee", "Sanghoon", ""]]}, {"id": "1807.01477", "submitter": "Zhiqiang Gong", "authors": "Zhiqiang Gong, Ping Zhong, Weidong Hu", "title": "Diversity in Machine Learning", "comments": "Accepted by IEEE Access", "journal-ref": "IEEE Access,2019", "doi": "10.1109/ACCESS.2019.2917620", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning methods have achieved good performance and been widely\napplied in various real-world applications. They can learn the model adaptively\nand be better fit for special requirements of different tasks. Generally, a\ngood machine learning system is composed of plentiful training data, a good\nmodel training process, and an accurate inference. Many factors can affect the\nperformance of the machine learning process, among which the diversity of the\nmachine learning process is an important one. The diversity can help each\nprocedure to guarantee a total good machine learning: diversity of the training\ndata ensures that the training data can provide more discriminative information\nfor the model, diversity of the learned model (diversity in parameters of each\nmodel or diversity among different base models) makes each parameter/model\ncapture unique or complement information and the diversity in inference can\nprovide multiple choices each of which corresponds to a specific plausible\nlocal optimal result. Even though the diversity plays an important role in\nmachine learning process, there is no systematical analysis of the\ndiversification in machine learning system. In this paper, we systematically\nsummarize the methods to make data diversification, model diversification, and\ninference diversification in the machine learning process, respectively. In\naddition, the typical applications where the diversity technology improved the\nmachine learning performance have been surveyed, including the remote sensing\nimaging tasks, machine translation, camera relocalization, image segmentation,\nobject detection, topic modeling, and others. Finally, we discuss some\nchallenges of the diversity technology in machine learning and point out some\ndirections in future work.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 08:25:17 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 13:41:11 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Gong", "Zhiqiang", ""], ["Zhong", "Ping", ""], ["Hu", "Weidong", ""]]}, {"id": "1807.01493", "submitter": "Minseong Kim", "authors": "Minseong Kim, Jongju Shin, Myung-Cheol Roh, Hyun-Chul Choi", "title": "Uncorrelated Feature Encoding for Faster Image Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent fast style transfer methods use a pre-trained convolutional neural\nnetwork as a feature encoder and a perceptual loss network. Although the\npre-trained network is used to generate responses of receptive fields effective\nfor representing style and content of image, it is not optimized for image\nstyle transfer but rather for image classification. Furthermore, it also\nrequires a time-consuming and correlation-considering feature alignment process\nfor image style transfer because of its inter-channel correlation. In this\npaper, we propose an end-to-end learning method which optimizes an\nencoder/decoder network for the purpose of style transfer as well as relieves\nthe feature alignment complexity from considering inter-channel correlation. We\nused uncorrelation loss, i.e., the total correlation coefficient between the\nresponses of different encoder channels, with style and content losses for\ntraining style transfer network. This makes the encoder network to be trained\nto generate inter-channel uncorrelated features and to be optimized for the\ntask of image style transfer which maintained the quality of image style only\nwith a light-weighted and correlation-unaware feature alignment process.\nMoreover, our method drastically reduced redundant channels of the encoded\nfeature and this resulted in the efficient size of structure of network and\nfaster forward processing speed. Our method can also be applied to cascade\nnetwork scheme for multiple scaled style transferring and allows user-control\nof style strength by using a content-style trade-off parameter.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 09:21:19 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Kim", "Minseong", ""], ["Shin", "Jongju", ""], ["Roh", "Myung-Cheol", ""], ["Choi", "Hyun-Chul", ""]]}, {"id": "1807.01511", "submitter": "Andrew Gilbert", "authors": "Matthew Trumble, Andrew Gilbert, Adrian Hilton, John Collomosse", "title": "Deep Autoencoder for Combined Human Pose Estimation and body Model\n  Upscaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for simultaneously estimating 3D human pose and body\nshape from a sparse set of wide-baseline camera views. We train a symmetric\nconvolutional autoencoder with a dual loss that enforces learning of a latent\nrepresentation that encodes skeletal joint positions, and at the same time\nlearns a deep representation of volumetric body shape. We harness the latter to\nup-scale input volumetric data by a factor of $4 \\times$, whilst recovering a\n3D estimate of joint positions with equal or greater accuracy than the state of\nthe art. Inference runs in real-time (25 fps) and has the potential for passive\nhuman behaviour monitoring where there is a requirement for high fidelity\nestimation of human body shape and pose.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 10:41:24 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Trumble", "Matthew", ""], ["Gilbert", "Andrew", ""], ["Hilton", "Adrian", ""], ["Collomosse", "John", ""]]}, {"id": "1807.01532", "submitter": "Nevrez Imamoglu", "authors": "Nevrez Imamoglu and Wataru Shimoda and Chi Zhang and Yuming Fang and\n  Asako Kanezaki and Keiji Yanai and Yoshifumi Nishida", "title": "An Integration of Bottom-up and Top-Down Salient Cues on RGB-D Data:\n  Saliency from Objectness vs. Non-Objectness", "comments": "9 pages, 3 figures, 3 tables, This work includes the accepted version\n  content of the paper published in journal of Signal Image and Video\n  Processing (SIViP, Springer), Vol. 12, Issue 2, pp 307-314, Feb 2018 (DOI:\n  https://doi.org/10.1007/s11760-017-1159-7)", "journal-ref": "Nevrez Imamoglu and Wataru Shimoda and Chi Zhang and Yuming Fang\n  and Asako Kanezaki and Keiji Yanai and Yoshifumi Nishida, Signal Image and\n  Video Processing (SIViP), Springer, Vol. 12, Issue 2, pp 307-314, Feb 2018", "doi": "10.1007/s11760-017-1159-7", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up and top-down visual cues are two types of information that helps\nthe visual saliency models. These salient cues can be from spatial\ndistributions of the features (space-based saliency) or contextual /\ntask-dependent features (object based saliency). Saliency models generally\nincorporate salient cues either in bottom-up or top-down norm separately. In\nthis work, we combine bottom-up and top-down cues from both space and object\nbased salient features on RGB-D data. In addition, we also investigated the\nability of various pre-trained convolutional neural networks for extracting\ntop-down saliency on color images based on the object dependent feature\nactivation. We demonstrate that combining salient features from color and dept\nthrough bottom-up and top-down methods gives significant improvement on the\nsalient object detection with space based and object based salient cues. RGB-D\nsaliency integration framework yields promising results compared with the\nseveral state-of-the-art-models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 12:10:02 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Imamoglu", "Nevrez", ""], ["Shimoda", "Wataru", ""], ["Zhang", "Chi", ""], ["Fang", "Yuming", ""], ["Kanezaki", "Asako", ""], ["Yanai", "Keiji", ""], ["Nishida", "Yoshifumi", ""]]}, {"id": "1807.01544", "submitter": "Shangbang Long", "authors": "Shangbang Long, Jiaqiang Ruan, Wenjie Zhang, Xin He, Wenhao Wu, Cong\n  Yao", "title": "TextSnake: A Flexible Representation for Detecting Text of Arbitrary\n  Shapes", "comments": "17 pages, accepted to ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by deep neural networks and large scale datasets, scene text detection\nmethods have progressed substantially over the past years, continuously\nrefreshing the performance records on various standard benchmarks. However,\nlimited by the representations (axis-aligned rectangles, rotated rectangles or\nquadrangles) adopted to describe text, existing methods may fall short when\ndealing with much more free-form text instances, such as curved text, which are\nactually very common in real-world scenarios. To tackle this problem, we\npropose a more flexible representation for scene text, termed as TextSnake,\nwhich is able to effectively represent text instances in horizontal, oriented\nand curved forms. In TextSnake, a text instance is described as a sequence of\nordered, overlapping disks centered at symmetric axes, each of which is\nassociated with potentially variable radius and orientation. Such geometry\nattributes are estimated via a Fully Convolutional Network (FCN) model. In\nexperiments, the text detector based on TextSnake achieves state-of-the-art or\ncomparable performance on Total-Text and SCUT-CTW1500, the two newly published\nbenchmarks with special emphasis on curved text in natural images, as well as\nthe widely-used datasets ICDAR 2015 and MSRA-TD500. Specifically, TextSnake\noutperforms the baseline on Total-Text by more than 40% in F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 12:37:07 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 00:54:35 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Long", "Shangbang", ""], ["Ruan", "Jiaqiang", ""], ["Zhang", "Wenjie", ""], ["He", "Xin", ""], ["Wu", "Wenhao", ""], ["Yao", "Cong", ""]]}, {"id": "1807.01569", "submitter": "Michael Schmitt", "authors": "Michael Schmitt and Lloyd Haydn Hughes and Xiao Xiang Zhu", "title": "The SEN1-2 Dataset for Deep Learning in SAR-Optical Data Fusion", "comments": "accepted for publication in the ISPRS Annals of the Photogrammetry,\n  Remote Sensing and Spatial Information Sciences (online from October 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning techniques have an increasing impact on many technical\nfields, gathering sufficient amounts of training data is a challenging problem\nin remote sensing. In particular, this holds for applications involving data\nfrom multiple sensors with heterogeneous characteristics. One example for that\nis the fusion of synthetic aperture radar (SAR) data and optical imagery. With\nthis paper, we publish the SEN1-2 dataset to foster deep learning research in\nSAR-optical data fusion. SEN1-2 comprises 282,384 pairs of corresponding image\npatches, collected from across the globe and throughout all meteorological\nseasons. Besides a detailed description of the dataset, we show exemplary\nresults for several possible applications, such as SAR image colorization,\nSAR-optical image matching, and creation of artificial optical images from SAR\ninput data. Since SEN1-2 is the first large open dataset of this kind, we\nbelieve it will support further developments in the field of deep learning for\nremote sensing as well as multi-sensor data fusion.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 13:29:14 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Schmitt", "Michael", ""], ["Hughes", "Lloyd Haydn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1807.01577", "submitter": "Mario Corsolini", "authors": "Mario Corsolini and Andrea Carta", "title": "VideoKifu, or the automatic transcription of a Go game", "comments": "14 pages, 6 figures. Accepted for the \"International Conference on\n  Research in Mind Games\" (August 7-8, 2018) at the EGC in Pisa, Italy.\n  Datasets available from http://www.oipaz.net/VideoKifu.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two previous papers [arXiv:1508.03269, arXiv:1701.05419] we described the\ntechniques we employed for reconstructing the whole move sequence of a Go game.\nThat task was at first accomplished by means of a series of photographs,\nmanually shot, as explained during the scientific conference held within the\nLIX European Go Congress (Liberec, CZ). The photographs were subsequently\nreplaced by a possibly unattended video live stream (provided by webcams,\nvideocameras, smartphones and so on) or, were the live stream not available, by\nmeans of a pre-recorded video of the game itself, on condition that the goban\nand the stones were clearly visible more often than not. As we hinted in the\nlatter paper, in the last two years we have improved both the algorithms\nemployed for reconstructing the grid and detecting the stones, making extensive\nusage of the multicore capabilities offered by modern CPUs. Those capabilities\nprompted us to develop some asynchronous routines, capable of double-checking\nthe position of the grid and the number and colour of any stone previously\ndetected, in order to get rid of minor errors possibly occurred during the main\nanalysis, and that may pass undetected especially in the course of an\nunattended live streaming. Those routines will be described in details, as they\naddress some problems that are of general interest when reconstructing the move\nsequence, for example what to do when large movements of the whole goban occur\n(deliberate or not) and how to deal with captures of dead stones $-$ that could\nbe wrongly detected and recorded as \"fresh\" moves if not promptly removed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 13:52:10 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Corsolini", "Mario", ""], ["Carta", "Andrea", ""]]}, {"id": "1807.01605", "submitter": "Mubariz Zaffar", "authors": "Mubariz Zaffar, Shoaib Ehsan, Rustam Stolkin and Klaus McDonald Maier", "title": "Sensors, SLAM and Long-term Autonomy: A Review", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": "10.1109/AHS.2018.8541483", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simultaneous Localization and Mapping, commonly known as SLAM, has been an\nactive research area in the field of Robotics over the past three decades. For\nsolving the SLAM problem, every robot is equipped with either a single sensor\nor a combination of similar/different sensors. This paper attempts to review,\ndiscuss, evaluate and compare these sensors. Keeping an eye on future, this\npaper also assesses the characteristics of these sensors against factors\ncritical to the long-term autonomy challenge.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 14:16:23 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Zaffar", "Mubariz", ""], ["Ehsan", "Shoaib", ""], ["Stolkin", "Rustam", ""], ["Maier", "Klaus McDonald", ""]]}, {"id": "1807.01631", "submitter": "Ghada Zamzmi", "authors": "Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun", "title": "Neonatal Pain Expression Recognition Using Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning using pre-trained Convolutional Neural Networks (CNNs) has\nbeen successfully applied to images for different classification tasks. In this\npaper, we propose a new pipeline for pain expression recognition in neonates\nusing transfer learning. Specifically, we propose to exploit a pre-trained CNN\nthat was originally trained on a relatively similar dataset for face\nrecognition (VGG Face) as well as CNNs that were pre-trained on a relatively\ndifferent dataset for image classification (iVGG F,M, and S) to extract deep\nfeatures from neonates' faces. In the final stage, several supervised machine\nlearning classifiers are trained to classify neonates' facial expression into\npain or no pain expression. The proposed pipeline achieved, on a testing\ndataset, 0.841 AUC and 90.34 accuracy, which is approx. 7 higher than the\naccuracy of handcrafted traditional features. We also propose to combine deep\nfeatures with traditional features and hypothesize that the mixed features\nwould improve pain classification performance. Combining deep features with\ntraditional features achieved 92.71 accuracy and 0.948 AUC. These results show\nthat transfer learning, which is a faster and more practical option than\ntraining CNN from the scratch, can be used to extract useful features for pain\nexpression recognition in neonates. It also shows that combining deep features\nwith traditional handcrafted features is a good practice to improve the\nperformance of pain expression recognition and possibly the performance of\nsimilar applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 15:15:05 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Zamzmi", "Ghada", ""], ["Goldgof", "Dmitry", ""], ["Kasturi", "Rangachar", ""], ["Sun", "Yu", ""]]}, {"id": "1807.01653", "submitter": "Davide Abati", "authors": "Davide Abati, Angelo Porrello, Simone Calderara, Rita Cucchiara", "title": "Latent Space Autoregression for Novelty Detection", "comments": "Accepted by CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is commonly referred to as the discrimination of\nobservations that do not conform to a learned model of regularity. Despite its\nimportance in different application settings, designing a novelty detector is\nutterly complex due to the unpredictable nature of novelties and its\ninaccessibility during the training procedure, factors which expose the\nunsupervised nature of the problem. In our proposal, we design a general\nframework where we equip a deep autoencoder with a parametric density estimator\nthat learns the probability distribution underlying its latent representations\nthrough an autoregressive procedure. We show that a maximum likelihood\nobjective, optimized in conjunction with the reconstruction of normal samples,\neffectively acts as a regularizer for the task at hand, by minimizing the\ndifferential entropy of the distribution spanned by latent vectors. In addition\nto providing a very general formulation, extensive experiments of our model on\npublicly available datasets deliver on-par or superior performances if compared\nto state-of-the-art methods in one-class and video anomaly detection settings.\nDifferently from prior works, our proposal does not make any assumption about\nthe nature of the novelties, making our work readily applicable to diverse\ncontexts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 16:06:39 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 09:53:59 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Abati", "Davide", ""], ["Porrello", "Angelo", ""], ["Calderara", "Simone", ""], ["Cucchiara", "Rita", ""]]}, {"id": "1807.01670", "submitter": "Karl Moritz Hermann", "authors": "Tiago Ramalho, Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Frederic Besse, S. M. Ali\n  Eslami, G\\'abor Melis, Fabio Viola, Phil Blunsom, Karl Moritz Hermann", "title": "Encoding Spatial Relations from Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing has made significant inroads into learning the\nsemantics of words through distributional approaches, however representations\nlearnt via these methods fail to capture certain kinds of information implicit\nin the real world. In particular, spatial relations are encoded in a way that\nis inconsistent with human spatial reasoning and lacking invariance to\nviewpoint changes. We present a system capable of capturing the semantics of\nspatial relations such as behind, left of, etc from natural language. Our key\ncontributions are a novel multi-modal objective based on generating images of\nscenes from their textual descriptions, and a new dataset on which to train it.\nWe demonstrate that internal representations are robust to meaning preserving\ntransformations of descriptions (paraphrase invariance), while viewpoint\ninvariance is an emergent property of the system.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 16:38:49 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:03:23 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ramalho", "Tiago", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Besse", "Frederic", ""], ["Eslami", "S. M. Ali", ""], ["Melis", "G\u00e1bor", ""], ["Viola", "Fabio", ""], ["Blunsom", "Phil", ""], ["Hermann", "Karl Moritz", ""]]}, {"id": "1807.01688", "submitter": "Quoc Dung Cao", "authors": "Quoc Dung Cao and Youngjun Choe", "title": "Building Damage Annotation on Post-Hurricane Satellite Imagery Based on\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/s11069-020-04133-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After a hurricane, damage assessment is critical to emergency managers for\nefficient response and resource allocation. One way to gauge the damage extent\nis to quantify the number of flooded/damaged buildings, which is traditionally\ndone by ground survey. This process can be labor-intensive and time-consuming.\nIn this paper, we propose to improve the efficiency of building damage\nassessment by applying image classification algorithms to post-hurricane\nsatellite imagery. At the known building coordinates (available from public\ndata), we extract square-sized images from the satellite imagery to create\ntraining, validation, and test datasets. Each square-sized image contains a\nbuilding to be classified as either 'Flooded/Damaged' (labeled by volunteers in\na crowd-sourcing project) or 'Undamaged'. We design and train a convolutional\nneural network from scratch and compare it with an existing neural network used\nwidely for common object classification. We demonstrate the promise of our\ndamage annotation model (over 97% accuracy) in the case study of building\ndamage assessment in the Greater Houston area affected by 2017 Hurricane\nHarvey.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 17:11:16 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 06:37:56 GMT"}, {"version": "v3", "created": "Mon, 20 May 2019 23:00:25 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 20:37:55 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Cao", "Quoc Dung", ""], ["Choe", "Youngjun", ""]]}, {"id": "1807.01696", "submitter": "Kemal Oksuz", "authors": "Kemal Oksuz, Baris Can Cam, Emre Akbas and Sinan Kalkan", "title": "Localization Recall Precision (LRP): A New Performance Metric for Object\n  Detection", "comments": "to appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Average precision (AP), the area under the recall-precision (RP) curve, is\nthe standard performance measure for object detection. Despite its wide\nacceptance, it has a number of shortcomings, the most important of which are\n(i) the inability to distinguish very different RP curves, and (ii) the lack of\ndirectly measuring bounding box localization accuracy. In this paper, we\npropose 'Localization Recall Precision (LRP) Error', a new metric which we\nspecifically designed for object detection. LRP Error is composed of three\ncomponents related to localization, false negative (FN) rate and false positive\n(FP) rate. Based on LRP, we introduce the 'Optimal LRP', the minimum achievable\nLRP error representing the best achievable configuration of the detector in\nterms of recall-precision and the tightness of the boxes. In contrast to AP,\nwhich considers precisions over the entire recall domain, Optimal LRP\ndetermines the 'best' confidence score threshold for a class, which balances\nthe trade-off between localization and recall-precision. In our experiments, we\nshow that, for state-of-the-art object (SOTA) detectors, Optimal LRP provides\nricher and more discriminative information than AP. We also demonstrate that\nthe best confidence score thresholds vary significantly among classes and\ndetectors. Moreover, we present LRP results of a simple online video object\ndetector which uses a SOTA still image object detector and show that the\nclass-specific optimized thresholds increase the accuracy against the common\napproach of using a general threshold for all classes. At\nhttps://github.com/cancam/LRP we provide the source code that can compute LRP\nfor the PASCAL VOC and MSCOCO datasets. Our source code can easily be adapted\nto other datasets as well.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 17:47:53 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 14:32:06 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Oksuz", "Kemal", ""], ["Cam", "Baris Can", ""], ["Akbas", "Emre", ""], ["Kalkan", "Sinan", ""]]}, {"id": "1807.01697", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Thomas G. Dietterich", "title": "Benchmarking Neural Network Robustness to Common Corruptions and Surface\n  Variations", "comments": "Superseded by _Benchmarking Neural Network Robustness to Common\n  Corruptions and Perturbations_ arXiv:1903.12261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish rigorous benchmarks for image classifier\nrobustness. Our first benchmark, ImageNet-C, standardizes and expands the\ncorruption robustness topic, while showing which classifiers are preferable in\nsafety-critical applications. Unlike recent robustness research, this benchmark\nevaluates performance on commonplace corruptions not worst-case adversarial\ncorruptions. We find that there are negligible changes in relative corruption\nrobustness from AlexNet to ResNet classifiers, and we discover ways to enhance\ncorruption robustness. Then we propose a new dataset called Icons-50 which\nopens research on a new kind of robustness, surface variation robustness. With\nthis dataset we evaluate the frailty of classifiers on new styles of known\nobjects and unexpected instances of known classes. We also demonstrate two\nmethods that improve surface variation robustness. Together our benchmarks may\naid future work toward networks that learn fundamental class structure and also\nrobustly generalize.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 17:57:11 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:57:31 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 20:30:27 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 21:36:39 GMT"}, {"version": "v5", "created": "Sat, 27 Apr 2019 18:19:39 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Hendrycks", "Dan", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1807.01702", "submitter": "Jung Ho Ahn", "authors": "Wonkyung Jung and Daejin Jung and and Byeongho Kim and Sunjung Lee and\n  Wonjong Rhee and Jung Ho Ahn", "title": "Restructuring Batch Normalization to Accelerate CNN Training", "comments": "13 pages, 8 figures, to appear in SysML 2019, added ResNet-50 results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch Normalization (BN) has become a core design block of modern\nConvolutional Neural Networks (CNNs). A typical modern CNN has a large number\nof BN layers in its lean and deep architecture. BN requires mean and variance\ncalculations over each mini-batch during training. Therefore, the existing\nmemory access reduction techniques, such as fusing multiple CONV layers, are\nnot effective for accelerating BN due to their inability to optimize mini-batch\nrelated calculations during training. To address this increasingly important\nproblem, we propose to restructure BN layers by first splitting a BN layer into\ntwo sub-layers (fission) and then combining the first sub-layer with its\npreceding CONV layer and the second sub-layer with the following activation and\nCONV layers (fusion). The proposed solution can significantly reduce\nmain-memory accesses while training the latest CNN models, and the experiments\non a chip multiprocessor show that the proposed BN restructuring can improve\nthe performance of DenseNet-121 by 25.7%.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 02:00:19 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 08:27:20 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Jung", "Wonkyung", ""], ["Jung", "Daejin", ""], ["Kim", "and Byeongho", ""], ["Lee", "Sunjung", ""], ["Rhee", "Wonjong", ""], ["Ahn", "Jung Ho", ""]]}, {"id": "1807.01726", "submitter": "Ze Wang", "authors": "Ze Wang, Weiqiang Ren, Qiang Qiu", "title": "LaneNet: Real-Time Lane Detection Networks for Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane detection is to detect lanes on the road and provide the accurate\nlocation and shape of each lane. It severs as one of the key techniques to\nenable modern assisted and autonomous driving systems. However, several unique\nproperties of lanes challenge the detection methods. The lack of distinctive\nfeatures makes lane detection algorithms tend to be confused by other objects\nwith similar local appearance. Moreover, the inconsistent number of lanes on a\nroad as well as diverse lane line patterns, e.g. solid, broken, single, double,\nmerging, and splitting lines further hamper the performance. In this paper, we\npropose a deep neural network based method, named LaneNet, to break down the\nlane detection into two stages: lane edge proposal and lane line localization.\nStage one uses a lane edge proposal network for pixel-wise lane edge\nclassification, and the lane line localization network in stage two then\ndetects lane lines based on lane edge proposals. Please note that the goal of\nour LaneNet is built to detect lane line only, which introduces more\ndifficulties on suppressing the false detections on the similar lane marks on\nthe road like arrows and characters. Despite all the difficulties, our lane\ndetection is shown to be robust to both highway and urban road scenarios method\nwithout relying on any assumptions on the lane number or the lane line\npatterns. The high running speed and low computational cost endow our LaneNet\nthe capability of being deployed on vehicle-based systems. Experiments validate\nthat our LaneNet consistently delivers outstanding performances on real world\ntraffic scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 18:05:04 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Wang", "Ze", ""], ["Ren", "Weiqiang", ""], ["Qiu", "Qiang", ""]]}, {"id": "1807.01759", "submitter": "Kuang Gong", "authors": "Kuang Gong, Kyungsang Kim, Jianan Cui, Ning Guo, Ciprian Catana, Jinyi\n  Qi, Quanzheng Li", "title": "Learning Personalized Representation for Inverse Problems in Medical\n  Imaging Using Deep Neural Network", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently deep neural networks have been widely and successfully applied in\ncomputer vision tasks and attracted growing interests in medical imaging. One\nbarrier for the application of deep neural networks to medical imaging is the\nneed of large amounts of prior training pairs, which is not always feasible in\nclinical practice. In this work we propose a personalized representation\nlearning framework where no prior training pairs are needed, but only the\npatient's own prior images. The representation is expressed using a deep neural\nnetwork with the patient's prior images as network input. We then applied this\nnovel image representation to inverse problems in medical imaging in which the\noriginal inverse problem was formulated as a constraint optimization problem\nand solved using the alternating direction method of multipliers (ADMM)\nalgorithm. Anatomically guided brain positron emission tomography (PET) image\nreconstruction and image denoising were employed as examples to demonstrate the\neffectiveness of the proposed framework. Quantification results based on\nsimulation and real datasets show that the proposed personalized representation\nframework outperform other widely adopted methods.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 20:00:00 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Gong", "Kuang", ""], ["Kim", "Kyungsang", ""], ["Cui", "Jianan", ""], ["Guo", "Ning", ""], ["Catana", "Ciprian", ""], ["Qi", "Jinyi", ""], ["Li", "Quanzheng", ""]]}, {"id": "1807.01779", "submitter": "Daniele Della Latta", "authors": "Gianmarco Santini, Lorena M. Zumbo, Nicola Martini, Gabriele Valvano,\n  Andrea Leo, Andrea Ripoli, Francesco Avogliero, Dante Chiappino and Daniele\n  Della Latta", "title": "Synthetic contrast enhancement in cardiac CT with Deep Learning", "comments": "8 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Europe the 20% of the CT scans cover the thoracic region. The acquired\nimages contain information about the cardiovascular system that often remains\nlatent due to the lack of contrast in the cardiac area. On the other hand, the\ncontrast enhanced computed tomography (CECT) represents an imaging technique\nthat allows to easily assess the cardiac chambers volumes and the contrast\ndynamics. With this work we aim to face the problem of extraction and\npresentation of these latent information, using a deep learning approach with\nconvolutional neural networks. Starting from the extraction of relevant\nfeatures from the image without contrast medium, we try to re-map them on\nfeatures typical of CECT, to synthesize an image characterized by an\nattenuation in the cardiac chambers as if a virtually iodine contrast medium\nwas injected. The purposes are to guarantee an estimation of the left cardiac\nchambers volume and to perform an evaluation of the contrast dynamics. Our\napproach is based on a deconvolutional network trained on a set of 120 patients\nwho underwent both CT acquisitions in the same contrastographic arterial phase\nand the same cardiac phase. To ensure a reliable predicted CECT image, in terms\nof values and morphology, a custom loss function is defined by combining an\nerror function to find a pixel-wise correspondence, which takes into account\nthe similarity in term of Hounsfield units between the input and output images\nand by a cross-entropy computed on the binarized versions of the synthesized\nand of the real CECT image. The proposed method is finally tested on 20\nsubjects.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 15:26:14 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Santini", "Gianmarco", ""], ["Zumbo", "Lorena M.", ""], ["Martini", "Nicola", ""], ["Valvano", "Gabriele", ""], ["Leo", "Andrea", ""], ["Ripoli", "Andrea", ""], ["Avogliero", "Francesco", ""], ["Chiappino", "Dante", ""], ["Della Latta", "Daniele", ""]]}, {"id": "1807.01788", "submitter": "Siddhant Rao", "authors": "Siddhant Rao", "title": "MITOS-RCNN: A Novel Approach to Mitotic Figure Detection in Breast\n  Cancer Histopathology Images using Region Based Convolutional Neural Networks", "comments": "Submitted to Elsevier Medical Image Analysis journal. 17 pages. 3\n  tables. 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies estimate that there will be 266,120 new cases of invasive breast\ncancer and 40,920 breast cancer induced deaths in the year of 2018 alone.\nDespite the pervasiveness of this affliction, the current process to obtain an\naccurate breast cancer prognosis is tedious and time consuming, requiring a\ntrained pathologist to manually examine histopathological images in order to\nidentify the features that characterize various cancer severity levels. We\npropose MITOS-RCNN: a novel region based convolutional neural network (RCNN)\ngeared for small object detection to accurately grade one of the three factors\nthat characterize tumor belligerence described by the Nottingham Grading\nSystem: mitotic count. Other computational approaches to mitotic figure\ncounting and detection do not demonstrate ample recall or precision to be\nclinically viable. Our models outperformed all previous participants in the\nICPR 2012 challenge, the AMIDA 2013 challenge and the MITOS-ATYPIA-14 challenge\nalong with recently published works. Our model achieved an F-measure score of\n0.955, a 6.11% improvement in accuracy from the most accurate of the previously\nproposed models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 21:29:53 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Rao", "Siddhant", ""]]}, {"id": "1807.01806", "submitter": "Jiaxin Chen", "authors": "Jiaxin Chen and Yi Fang", "title": "Deep Cross-modality Adaptation via Semantics Preserving Adversarial\n  Learning for Sketch-based 3D Shape Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the large cross-modality discrepancy between 2D sketches and 3D\nshapes, retrieving 3D shapes by sketches is a significantly challenging task.\nTo address this problem, we propose a novel framework to learn a discriminative\ndeep cross-modality adaptation model in this paper. Specifically, we first\nseparately adopt two metric networks, following two deep convolutional neural\nnetworks (CNNs), to learn modality-specific discriminative features based on an\nimportance-aware metric learning method. Subsequently, we explicitly introduce\na cross-modality transformation network to compensate for the divergence\nbetween two modalities, which can transfer features of 2D sketches to the\nfeature space of 3D shapes. We develop an adversarial learning based method to\ntrain the transformation model, by simultaneously enhancing the holistic\ncorrelations between data distributions of two modalities, and mitigating the\nlocal semantic divergences through minimizing a cross-modality mean discrepancy\nterm. Experimental results on the SHREC 2013 and SHREC 2014 datasets clearly\nshow the superior retrieval performance of our proposed model, compared to the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 22:51:44 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Chen", "Jiaxin", ""], ["Fang", "Yi", ""]]}, {"id": "1807.01826", "submitter": "Jiali Duan", "authors": "Jiali Duan, Xiaoyuan Guo, Yuhang Song, Chao Yang, C.-C. Jay Kuo", "title": "PortraitGAN for Flexible Portrait Manipulation", "comments": null, "journal-ref": "APSIPA Transactions on Signal and Information Processing 9 (2020)\n  e22", "doi": "10.1017/ATSIP.2020.20", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous methods have dealt with discrete manipulation of facial attributes\nsuch as smile, sad, angry, surprise etc, out of canonical expressions and they\nare not scalable, operating in single modality. In this paper, we propose a\nnovel framework that supports continuous edits and multi-modality portrait\nmanipulation using adversarial learning. Specifically, we adapt\ncycle-consistency into the conditional setting by leveraging additional facial\nlandmarks information. This has two effects: first cycle mapping induces\nbidirectional manipulation and identity preserving; second pairing samples from\ndifferent modalities can thus be utilized. To ensure high-quality synthesis, we\nadopt texture-loss that enforces texture consistency and multi-level\nadversarial supervision that facilitates gradient flow. Quantitative and\nqualitative experiments show the effectiveness of our framework in performing\nflexible and multi-modality portrait manipulation with photo-realistic effects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 01:52:15 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 19:22:43 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Duan", "Jiali", ""], ["Guo", "Xiaoyuan", ""], ["Song", "Yuhang", ""], ["Yang", "Chao", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1807.01864", "submitter": "Wei Ao", "authors": "Wei Ao, Yanwei Fu and Feng Xu", "title": "Detecting Tiny Moving Vehicles in Satellite Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the satellite videos have been captured by a moving\nsatellite platform. In contrast to consumer, movie, and common surveillance\nvideos, satellite video can record the snapshot of the city-scale scene. In a\nbroad field-of-view of satellite videos, each moving target would be very tiny\nand usually composed of several pixels in frames. Even worse, the noise signals\nalso existed in the video frames, since the background of the video frame has\nthe subpixel-level and uneven moving thanks to the motion of satellites. We\nargue that this is a new type of computer vision task since previous\ntechnologies are unable to detect such tiny vehicles efficiently. This paper\nproposes a novel framework that can identify the small moving vehicles in\nsatellite videos. In particular, we offer a novel detecting algorithm based on\nthe local noise modeling. We differentiate the potential vehicle targets from\nnoise patterns by an exponential probability distribution. Subsequently, a\nmulti-morphological-cue based discrimination strategy is designed to\ndistinguish correct vehicle targets from a few existing noises further. Another\nsignificant contribution is to introduce a series of evaluation protocols to\nmeasure the performance of tiny moving vehicle detection systematically. We\nannotate a satellite video manually and use it to test our algorithms under\ndifferent evaluation criterion. The proposed algorithm is also compared with\nthe state-of-the-art baselines, and demonstrates the advantages of our\nframework over the benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 06:46:31 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Ao", "Wei", ""], ["Fu", "Yanwei", ""], ["Xu", "Feng", ""]]}, {"id": "1807.01874", "submitter": "Rui Fan", "authors": "Rui Fan, Xiao Ai, Naim Dahnoun", "title": "Road surface 3d reconstruction based on dense subpixel disparity map\n  estimation", "comments": "11 pages, 16 figures, IEEE Transactions on Image Processing", "journal-ref": "Fan R, Ai X, Dahnoun N. Road surface 3d reconstruction based on\n  dense subpixel disparity map estimation[J]. IEEE Transactions on Image\n  Processing, 2018, 27(6): 3025-3035", "doi": "10.1109/TIP.2018.2808770", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various 3D reconstruction methods have enabled civil engineers to detect\ndamage on a road surface. To achieve the millimetre accuracy required for road\ncondition assessment, a disparity map with subpixel resolution needs to be\nused. However, none of the existing stereo matching algorithms are specially\nsuitable for the reconstruction of the road surface. Hence in this paper, we\npropose a novel dense subpixel disparity estimation algorithm with high\ncomputational efficiency and robustness. This is achieved by first transforming\nthe perspective view of the target frame into the reference view, which not\nonly increases the accuracy of the block matching for the road surface but also\nimproves the processing speed. The disparities are then estimated iteratively\nusing our previously published algorithm where the search range is propagated\nfrom three estimated neighbouring disparities. Since the search range is\nobtained from the previous iteration, errors may occur when the propagated\nsearch range is not sufficient. Therefore, a correlation maxima verification is\nperformed to rectify this issue, and the subpixel resolution is achieved by\nconducting a parabola interpolation enhancement. Furthermore, a novel disparity\nglobal refinement approach developed from the Markov Random Fields and Fast\nBilateral Stereo is introduced to further improve the accuracy of the estimated\ndisparity map, where disparities are updated iteratively by minimising the\nenergy function that is related to their interpolated correlation polynomials.\nThe algorithm is implemented in C language with a near real-time performance.\nThe experimental results illustrate that the absolute error of the\nreconstruction varies from 0.1 mm to 3 mm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 07:13:42 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Fan", "Rui", ""], ["Ai", "Xiao", ""], ["Dahnoun", "Naim", ""]]}, {"id": "1807.01884", "submitter": "Bingwang Zhang", "authors": "Qi Yuan and Bingwang Zhang and Haojie Li and Zhihui Wang and Zhongxuan\n  Luo", "title": "A Single Shot Text Detector with Scale-adaptive Anchors", "comments": "8 pages, 6figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, most top-performing text detection networks tend to employ\nfixed-size anchor boxes to guide the search for text instances. They usually\nrely on a large amount of anchors with different scales to discover texts in\nscene images, thus leading to high computational cost. In this paper, we\npropose an end-to-end box-based text detector with scale-adaptive anchors,\nwhich can dynamically adjust the scales of anchors according to the sizes of\nunderlying texts by introducing an additional scale regression layer. The\nproposed scale-adaptive anchors allow us to use a few number of anchors to\nhandle multi-scale texts and therefore significantly improve the computational\nefficiency. Moreover, compared to discrete scales used in previous methods, the\nlearned continuous scales are more reliable, especially for small texts\ndetection. Additionally, we propose Anchor convolution to better exploit\nnecessary feature information by dynamically adjusting the sizes of receptive\nfields according to the learned scales. Extensive experiments demonstrate that\nthe proposed detector is fast, taking only $0.28$ second per image, while\noutperforming most state-of-the-art methods in accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 07:48:18 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Yuan", "Qi", ""], ["Zhang", "Bingwang", ""], ["Li", "Haojie", ""], ["Wang", "Zhihui", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1807.01950", "submitter": "Andrew Gilbert", "authors": "Andrew Gilbert, Marco Volino, John Collomosse, Adrian Hilton", "title": "Volumetric performance capture from minimal camera viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a convolutional autoencoder that enables high fidelity volumetric\nreconstructions of human performance to be captured from multi-view video\ncomprising only a small set of camera views. Our method yields similar\nend-to-end reconstruction error to that of a probabilistic visual hull computed\nusing significantly more (double or more) viewpoints. We use a deep prior\nimplicitly learned by the autoencoder trained over a dataset of view-ablated\nmulti-view video footage of a wide range of subjects and actions. This opens up\nthe possibility of high-end volumetric performance capture in on-set and\nprosumer scenarios where time or cost prohibit a high witness camera count.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 11:51:50 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 11:25:21 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Gilbert", "Andrew", ""], ["Volino", "Marco", ""], ["Collomosse", "John", ""], ["Hilton", "Adrian", ""]]}, {"id": "1807.01952", "submitter": "Tobias B\\\"ottger", "authors": "Tobias B\\\"ottger and Markus Ulrich and Carsten Steger", "title": "Subpixel-Precise Tracking of Rigid Objects in Real-time", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-59126-1_5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel object tracking scheme that can track rigid objects in\nreal time. The approach uses subpixel-precise image edges to track objects with\nhigh accuracy. It can determine the object position, scale, and rotation with\nsubpixel-precision at around 80fps. The tracker returns a reliable score for\neach frame and is capable of self diagnosing a tracking failure. Furthermore,\nthe choice of the similarity measure makes the approach inherently robust\nagainst occlusion, clutter, and nonlinear illumination changes. We evaluate the\nmethod on sequences from rigid objects from the OTB-2015 and VOT2016 dataset\nand discuss its performance. The evaluation shows that the tracker is more\naccurate than state-of-the-art real-time trackers while being equally robust.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 11:54:41 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["B\u00f6ttger", "Tobias", ""], ["Ulrich", "Markus", ""], ["Steger", "Carsten", ""]]}, {"id": "1807.01963", "submitter": "Thomas Probst", "authors": "Thomas Probst, Ajad Chhatkuli, Danda Pani Paudel, Luc Van Gool", "title": "Model-free Consensus Maximization for Non-Rigid Shapes", "comments": "ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision methods use consensus maximization to relate\nmeasurements containing outliers with the correct transformation model. In the\ncontext of rigid shapes, this is typically done using Random Sampling and\nConsensus (RANSAC) by estimating an analytical model that agrees with the\nlargest number of measurements (inliers). However, small parameter models may\nnot be always available. In this paper, we formulate the model-free consensus\nmaximization as an Integer Program in a graph using `rules' on measurements. We\nthen provide a method to solve it optimally using the Branch and Bound (BnB)\nparadigm. We focus its application on non-rigid shapes, where we apply the\nmethod to remove outlier 3D correspondences and achieve performance superior to\nthe state of the art. Our method works with outlier ratio as high as 80\\%. We\nfurther derive a similar formulation for 3D template to image matching,\nachieving similar or better performance compared to the state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 12:34:40 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 13:10:07 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Probst", "Thomas", ""], ["Chhatkuli", "Ajad", ""], ["Paudel", "Danda Pani", ""], ["Van Gool", "Luc", ""]]}, {"id": "1807.01964", "submitter": "Hang Su", "authors": "Hang Su, Xiatian Zhu, Shaogang Gong", "title": "Open Logo Detection Challenge", "comments": "Accepted by BMVC 2018. The QMUL-OpenLogo benchmark is publicly\n  available at: qmul-openlogo.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing logo detection benchmarks consider artificial deployment scenarios\nby assuming that large training data with fine-grained bounding box annotations\nfor each class are available for model training. Such assumptions are often\ninvalid in realistic logo detection scenarios where new logo classes come\nprogressively and require to be detected with little or none budget for\nexhaustively labelling fine-grained training data for every new class. Existing\nbenchmarks are thus unable to evaluate the true performance of a logo detection\nmethod in realistic and open deployments. In this work, we introduce a more\nrealistic and challenging logo detection setting, called Open Logo Detection.\nSpecifically, this new setting assumes fine-grained labelling only on a small\nproportion of logo classes whilst the remaining classes have no labelled\ntraining data to simulate the open deployment. We further create an open logo\ndetection benchmark, called OpenLogo,to promote the investigation of this new\nchallenge. OpenLogo contains 27,083 images from 352 logo classes, built by\naggregating/refining 7 existing datasets and establishing an open logo\ndetection evaluation protocol. To address this challenge, we propose a Context\nAdversarial Learning (CAL) approach to synthesising training data with coherent\nlogo instance appearance against diverse background context for enabling more\neffective optimisation of contemporary deep learning detection models.\nExperiments show the performance advantage of CAL over existing\nstate-of-the-art alternative methods on the more realistic and challenging\nOpenLogo benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 12:40:39 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 17:55:03 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 10:58:01 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Su", "Hang", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1807.01972", "submitter": "Alex Ter-Sarkisov", "authors": "Aram Ter-Sarkisov, Robert Ross, John Kelleher, Bernadette Earley,\n  Michael Keane", "title": "Beef Cattle Instance Segmentation Using Fully Convolutional Neural\n  Network", "comments": "accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an instance segmentation algorithm trained and applied to a CCTV\nrecording of beef cattle during a winter finishing period. A fully\nconvolutional network was transformed into an instance segmentation network\nthat learns to label each instance of an animal separately. We introduce a\nconceptually simple framework that the network uses to output a single\nprediction for every animal. These results are a contribution towards behaviour\nanalysis in winter finishing beef cattle for early detection of animal\nwelfare-related problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 12:52:07 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 15:18:34 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Ter-Sarkisov", "Aram", ""], ["Ross", "Robert", ""], ["Kelleher", "John", ""], ["Earley", "Bernadette", ""], ["Keane", "Michael", ""]]}, {"id": "1807.01989", "submitter": "Miaojing Shi", "authors": "Miaojing Shi, Zhaohui Yang, Chao Xu, Qijun Chen", "title": "Revisiting Perspective Information for Efficient Crowd Counting", "comments": "CVPR2019 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd counting is the task of estimating people numbers in crowd images.\nModern crowd counting methods employ deep neural networks to estimate crowd\ncounts via crowd density regressions. A major challenge of this task lies in\nthe perspective distortion, which results in drastic person scale change in an\nimage. Density regression on the small person area is in general very hard. In\nthis work, we propose a perspective-aware convolutional neural network (PACNN)\nfor efficient crowd counting, which integrates the perspective information into\ndensity regression to provide additional knowledge of the person scale change\nin an image. Ground truth perspective maps are firstly generated for training;\nPACNN is then specifically designed to predict multi-scale perspective maps,\nand encode them as perspective-aware weighting layers in the network to\nadaptively combine the outputs of multi-scale density maps. The weights are\nlearned at every pixel of the maps such that the final density combination is\nrobust to the perspective distortion. We conduct extensive experiments on the\nShanghaiTech, WorldExpo'10, UCF_CC_50, and UCSD datasets, and demonstrate the\neffectiveness and efficiency of PACNN over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 13:33:28 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 19:26:07 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 15:29:29 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Shi", "Miaojing", ""], ["Yang", "Zhaohui", ""], ["Xu", "Chao", ""], ["Chen", "Qijun", ""]]}, {"id": "1807.01990", "submitter": "Tadanobu Inoue", "authors": "Tadanobu Inoue, Subhajit Chaudhury, Giovanni De Magistris, Sakyasingha\n  Dasgupta", "title": "Transfer Learning From Synthetic To Real Images Using Variational\n  Autoencoders For Precise Position Detection", "comments": "Copyright 2018 IEEE - Accepted at ICIP 2018, Athens, Greece, October\n  7-10, 2018. Video: https://youtu.be/30vji7nJibA. arXiv admin note: text\n  overlap with arXiv:1709.06762", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing and labeling camera images in the real world is an expensive task,\nwhereas synthesizing labeled images in a simulation environment is easy for\ncollecting large-scale image data. However, learning from only synthetic images\nmay not achieve the desired performance in the real world due to a gap between\nsynthetic and real images. We propose a method that transfers learned detection\nof an object position from a simulation environment to the real world. This\nmethod uses only a significantly limited dataset of real images while\nleveraging a large dataset of synthetic images using variational autoencoders.\nAdditionally, the proposed method consistently performed well in different\nlighting conditions, in the presence of other distractor objects, and on\ndifferent backgrounds. Experimental results showed that it achieved accuracy of\n1.5mm to 3.5mm on average. Furthermore, we showed how the method can be used in\na real-world scenario like a \"pick-and-place\" robotic task.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 05:45:48 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Inoue", "Tadanobu", ""], ["Chaudhury", "Subhajit", ""], ["De Magistris", "Giovanni", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1807.02001", "submitter": "Tobias B\\\"ottger", "authors": "Patrick Follmann, Bertram Drost, and Tobias B\\\"ottger", "title": "Acquire, Augment, Segment & Enjoy: Weakly Supervised Instance\n  Segmentation of Supermarket Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grocery stores have thousands of products that are usually identified using\nbarcodes with a human in the loop. For automated checkout systems, it is\nnecessary to count and classify the groceries efficiently and robustly. One\npossibility is to use a deep learning algorithm for instance-aware semantic\nsegmentation. Such methods achieve high accuracies but require a large amount\nof annotated training data.\n  We propose a system to generate the training annotations in a weakly\nsupervised manner, drastically reducing the labeling effort. We assume that for\neach training image, only the object class is known. The system automatically\nsegments the corresponding object from the background. The obtained training\ndata is augmented to simulate variations similar to those seen in real-world\nsetups.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 13:38:55 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 07:22:22 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Follmann", "Patrick", ""], ["Drost", "Bertram", ""], ["B\u00f6ttger", "Tobias", ""]]}, {"id": "1807.02004", "submitter": "Christoph Wick", "authors": "Christoph Wick, Christian Reul, Frank Puppe", "title": "Calamari - A High-Performance Tensorflow-based Deep Learning Package for\n  Optical Character Recognition", "comments": "11 pages, 3 figures", "journal-ref": "Digital Humanities Quarterly 14 (2), 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Character Recognition (OCR) on contemporary and historical data is\nstill in the focus of many researchers. Especially historical prints require\nbook specific trained OCR models to achieve applicable results (Springmann and\nL\\\"udeling, 2016, Reul et al., 2017a). To reduce the human effort for manually\nannotating ground truth (GT) various techniques such as voting and pretraining\nhave shown to be very efficient (Reul et al., 2018a, Reul et al., 2018b).\nCalamari is a new open source OCR line recognition software that both uses\nstate-of-the art Deep Neural Networks (DNNs) implemented in Tensorflow and\ngiving native support for techniques such as pretraining and voting. The\ncustomizable network architectures constructed of Convolutional Neural Networks\n(CNNS) and Long-ShortTerm-Memory (LSTM) layers are trained by the so-called\nConnectionist Temporal Classification (CTC) algorithm of Graves et al. (2006).\nOptional usage of a GPU drastically reduces the computation times for both\ntraining and prediction. We use two different datasets to compare the\nperformance of Calamari to OCRopy, OCRopus3, and Tesseract 4. Calamari reaches\na Character Error Rate (CER) of 0.11% on the UW3 dataset written in modern\nEnglish and 0.18% on the DTA19 dataset written in German Fraktur, which\nconsiderably outperforms the results of the existing softwares.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 13:46:37 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:18:26 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 07:52:56 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wick", "Christoph", ""], ["Reul", "Christian", ""], ["Puppe", "Frank", ""]]}, {"id": "1807.02011", "submitter": "Paul Bergmann", "authors": "Paul Bergmann, Sindy L\\\"owe, Michael Fauser, David Sattlegger, Carsten\n  Steger", "title": "Improving Unsupervised Defect Segmentation by Applying Structural\n  Similarity to Autoencoders", "comments": null, "journal-ref": null, "doi": "10.5220/0007364503720380", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional autoencoders have emerged as popular methods for unsupervised\ndefect segmentation on image data. Most commonly, this task is performed by\nthresholding a pixel-wise reconstruction error based on an $\\ell^p$ distance.\nThis procedure, however, leads to large residuals whenever the reconstruction\nencompasses slight localization inaccuracies around edges. It also fails to\nreveal defective regions that have been visually altered when intensity values\nstay roughly consistent. We show that these problems prevent these approaches\nfrom being applied to complex real-world scenarios and that it cannot be easily\navoided by employing more elaborate architectures such as variational or\nfeature matching autoencoders. We propose to use a perceptual loss function\nbased on structural similarity which examines inter-dependencies between local\nimage regions, taking into account luminance, contrast and structural\ninformation, instead of simply comparing single pixel values. It achieves\nsignificant performance gains on a challenging real-world dataset of\nnanofibrous materials and a novel dataset of two woven fabrics over the state\nof the art approaches for unsupervised defect segmentation that use pixel-wise\nreconstruction error metrics.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:07:23 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 11:58:22 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 16:16:28 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bergmann", "Paul", ""], ["L\u00f6we", "Sindy", ""], ["Fauser", "Michael", ""], ["Sattlegger", "David", ""], ["Steger", "Carsten", ""]]}, {"id": "1807.02020", "submitter": "Christian Koch", "authors": "Moritz Lode, Michael \\\"Ortl, Christian Koch, Amr Rizk, Ralf Steinmetz", "title": "Detection and Analysis of Content Creator Collaborations in YouTube\n  Videos using Face- and Speaker-Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work discusses and implements the application of speaker recognition for\nthe detection of collaborations in YouTube videos. CATANA, an existing\nframework for detection and analysis of YouTube collaborations, is utilizing\nface recognition for the detection of collaborators, which naturally performs\npoor on video-content without appearing faces. This work proposes an extension\nof CATANA using active speaker detection and speaker recognition to improve the\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:27:02 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Lode", "Moritz", ""], ["\u00d6rtl", "Michael", ""], ["Koch", "Christian", ""], ["Rizk", "Amr", ""], ["Steinmetz", "Ralf", ""]]}, {"id": "1807.02030", "submitter": "Clemens Seibold", "authors": "Clemens Seibold, Anna Hilsmann, Peter Eisert", "title": "Reflection Analysis for Face Morphing Attack Detection", "comments": "5 pages, 6 figures, accepted at EUSIPCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A facial morph is a synthetically created image of a face that looks similar\nto two different individuals and can even trick biometric facial recognition\nsystems into recognizing both individuals. This attack is known as face\nmorphing attack. The process of creating such a facial morph is well documented\nand a lot of tutorials and software to create them are freely available.\nTherefore, it is mandatory to be able to detect this kind of fraud to ensure\nthe integrity of the face as reliable biometric feature. In this work, we study\nthe effects of face morphing on the physically correctness of the illumination.\nWe estimate the direction to the light sources based on specular highlights in\nthe eyes and use them to generate a synthetic map for highlights on the skin.\nThis map is compared with the highlights in the image that is suspected to be a\nfraud. Morphing faces with different geometries, a bad alignment of the source\nimages or using images with different illuminations, can lead to\ninconsistencies in reflections that indicate the existence of a morphing\nattack.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:46:19 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Seibold", "Clemens", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "1807.02033", "submitter": "Ananya Kumar", "authors": "Ananya Kumar, S. M. Ali Eslami, Danilo J. Rezende, Marta Garnelo,\n  Fabio Viola, Edward Lockhart, Murray Shanahan", "title": "Consistent Generative Query Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic video prediction models take in a sequence of image frames, and\ngenerate a sequence of consecutive future image frames. These models typically\ngenerate future frames in an autoregressive fashion, which is slow and requires\nthe input and output frames to be consecutive. We introduce a model that\novercomes these drawbacks by generating a latent representation from an\narbitrary set of frames that can then be used to simultaneously and efficiently\nsample temporally consistent frames at arbitrary time-points. For example, our\nmodel can \"jump\" and directly sample frames at the end of the video, without\nsampling intermediate frames. Synthetic video evaluations confirm substantial\ngains in speed and functionality without loss in fidelity. We also apply our\nframework to a 3D scene reconstruction dataset. Here, our model is conditioned\non camera location and can sample consistent sets of images for what an\noccluded region of a 3D scene might look like, even if there are multiple\npossibilities for what that region might contain. Reconstructions and videos\nare available at https://bit.ly/2O4Pc4R.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:51:51 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 17:59:14 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 00:51:13 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Kumar", "Ananya", ""], ["Eslami", "S. M. Ali", ""], ["Rezende", "Danilo J.", ""], ["Garnelo", "Marta", ""], ["Viola", "Fabio", ""], ["Lockhart", "Edward", ""], ["Shanahan", "Murray", ""]]}, {"id": "1807.02044", "submitter": "Rui Fan", "authors": "Rui Fan, Yanan Liu, Mohammud Junaid Bocus, Lujia Wang, Ming Liu", "title": "Real-Time Subpixel Fast Bilateral Stereo", "comments": "8 pages, 7 figures, International Conference on Information and\n  automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo vision technique has been widely used in robotic systems to acquire\n3-D information. In recent years, many researchers have applied bilateral\nfiltering in stereo vision to adaptively aggregate the matching costs. This has\ngreatly improved the accuracy of the estimated disparity maps. However, the\nprocess of filtering the whole cost volume is very time consuming and therefore\nthe researchers have to resort to some powerful hardware for the real-time\npurpose. This paper presents the implementation of fast bilateral stereo on a\nstate-of-the-art GPU. By highly exploiting the parallel computing architecture\nof the GPU, the fast bilateral stereo performs in real time when processing the\nMiddlebury stereo datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 15:06:35 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 05:57:02 GMT"}, {"version": "v3", "created": "Wed, 15 Aug 2018 13:49:41 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Fan", "Rui", ""], ["Liu", "Yanan", ""], ["Bocus", "Mohammud Junaid", ""], ["Wang", "Lujia", ""], ["Liu", "Ming", ""]]}, {"id": "1807.02062", "submitter": "Peiliang Li", "authors": "Peiliang Li, Tong Qin, and Shaojie Shen", "title": "Stereo Vision-based Semantic 3D Object and Ego-motion Tracking for\n  Autonomous Driving", "comments": "14 pages, 9 figures, eccv2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a stereo vision-based approach for tracking the camera ego-motion\nand 3D semantic objects in dynamic autonomous driving scenarios. Instead of\ndirectly regressing the 3D bounding box using end-to-end approaches, we propose\nto use the easy-to-labeled 2D detection and discrete viewpoint classification\ntogether with a light-weight semantic inference method to obtain rough 3D\nobject measurements. Based on the object-aware-aided camera pose tracking which\nis robust in dynamic environments, in combination with our novel dynamic object\nbundle adjustment (BA) approach to fuse temporal sparse feature correspondences\nand the semantic 3D measurement model, we obtain 3D object pose, velocity and\nanchored dynamic point cloud estimation with instance accuracy and temporal\nconsistency. The performance of our proposed method is demonstrated in diverse\nscenarios. Both the ego-motion estimation and object localization are compared\nwith the state-of-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 15:52:09 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 06:51:31 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 11:10:42 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Li", "Peiliang", ""], ["Qin", "Tong", ""], ["Shen", "Shaojie", ""]]}, {"id": "1807.02080", "submitter": "Dongdong Zeng", "authors": "Dongdong Zeng, Ming Zhu and Arjan Kuijper", "title": "Combining Background Subtraction Algorithms with Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": "10.1117/1.JEI.28.1.013011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and fast extraction of foreground object is a key prerequisite for a\nwide range of computer vision applications such as object tracking and\nrecognition. Thus, enormous background subtraction methods for foreground\nobject detection have been proposed in recent decades. However, it is still\nregarded as a tough problem due to a variety of challenges such as illumination\nvariations, camera jitter, dynamic backgrounds, shadows, and so on. Currently,\nthere is no single method that can handle all the challenges in a robust way.\nIn this letter, we try to solve this problem from a new perspective by\ncombining different state-of-the-art background subtraction algorithms to\ncreate a more robust and more advanced foreground detection algorithm. More\nspecifically, an encoder-decoder fully convolutional neural network\narchitecture is trained to automatically learn how to leverage the\ncharacteristics of different algorithms to fuse the results produced by\ndifferent background subtraction algorithms and output a more precise result.\nComprehensive experiments evaluated on the CDnet 2014 dataset demonstrate that\nthe proposed method outperforms all the considered single background\nsubtraction algorithm. And we show that our solution is more efficient than\nother combination strategies.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 16:39:50 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 19:30:56 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zeng", "Dongdong", ""], ["Zhu", "Ming", ""], ["Kuijper", "Arjan", ""]]}, {"id": "1807.02087", "submitter": "Henning Tjaden", "authors": "Henning Tjaden, Ulrich Schwanecke, Elmar Sch\\\"omer and Daniel Cremers", "title": "A Region-based Gauss-Newton Approach to Real-Time Monocular Multiple\n  Object Tracking", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2018.2884990", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for real-time 6DOF pose tracking of rigid 3D objects\nusing a monocular RGB camera. The key idea is to derive a region-based cost\nfunction using temporally consistent local color histograms. While such\nregion-based cost functions are commonly optimized using first-order gradient\ndescent techniques, we systematically derive a Gauss-Newton optimization scheme\nwhich gives rise to drastically faster convergence and highly accurate and\nrobust tracking performance. We furthermore propose a novel complex dataset\ndedicated for the task of monocular object pose tracking and make it publicly\navailable to the community. To our knowledge, it is the first to address the\ncommon and important scenario in which both the camera as well as the objects\nare moving simultaneously in cluttered scenes. In numerous experiments -\nincluding our own proposed dataset - we demonstrate that the proposed\nGauss-Newton approach outperforms existing approaches, in particular in the\npresence of cluttered backgrounds, heterogeneous objects and partial\nocclusions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 16:56:54 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 07:51:46 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Tjaden", "Henning", ""], ["Schwanecke", "Ulrich", ""], ["Sch\u00f6mer", "Elmar", ""], ["Cremers", "Daniel", ""]]}, {"id": "1807.02098", "submitter": "Somdip Dey Mr.", "authors": "Somdip Dey, Grigorios Kalliatakis, Sangeet Saha, Amit Kumar Singh,\n  Shoaib Ehsan, Klaus McDonald-Maier", "title": "MAT-CNN-SOPC: Motionless Analysis of Traffic Using Convolutional Neural\n  Networks on System-On-a-Programmable-Chip", "comments": "6 pages, 3 figures, 2 tables", "journal-ref": "2018 NASA/ESA Conference on Adaptive Hardware and Systems (AHS\n  2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent Transportation Systems (ITS) have become an important pillar in\nmodern \"smart city\" framework which demands intelligent involvement of\nmachines. Traffic load recognition can be categorized as an important and\nchallenging issue for such systems. Recently, Convolutional Neural Network\n(CNN) models have drawn considerable amount of interest in many areas such as\nweather classification, human rights violation detection through images, due to\nits accurate prediction capabilities. This work tackles real-life traffic load\nrecognition problem on System-On-a-Programmable-Chip (SOPC) platform and coin\nit as MAT-CNN- SOPC, which uses an intelligent re-training mechanism of the CNN\nwith known environments. The proposed methodology is capable of enhancing the\nefficacy of the approach by 2.44x in comparison to the state-of-art and proven\nthrough experimental analysis. We have also introduced a mathematical equation,\nwhich is capable of quantifying the suitability of using different CNN models\nover the other for a particular application based implementation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 17:35:33 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 23:31:16 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Dey", "Somdip", ""], ["Kalliatakis", "Grigorios", ""], ["Saha", "Sangeet", ""], ["Singh", "Amit Kumar", ""], ["Ehsan", "Shoaib", ""], ["McDonald-Maier", "Klaus", ""]]}, {"id": "1807.02110", "submitter": "Yash Patel", "authors": "Yash Patel, Lluis Gomez, Raul Gomez, Mar\\c{c}al Rusi\\~nol, Dimosthenis\n  Karatzas, C.V. Jawahar", "title": "TextTopicNet - Self-Supervised Learning of Visual Features Through\n  Embedding Images on Semantic Text Spaces", "comments": "arXiv admin note: text overlap with arXiv:1705.08631", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immense success of deep learning based methods in computer vision heavily\nrelies on large scale training datasets. These richly annotated datasets help\nthe network learn discriminative visual features. Collecting and annotating\nsuch datasets requires a tremendous amount of human effort and annotations are\nlimited to popular set of classes. As an alternative, learning visual features\nby designing auxiliary tasks which make use of freely available\nself-supervision has become increasingly popular in the computer vision\ncommunity.\n  In this paper, we put forward an idea to take advantage of multi-modal\ncontext to provide self-supervision for the training of computer vision\nalgorithms. We show that adequate visual features can be learned efficiently by\ntraining a CNN to predict the semantic textual context in which a particular\nimage is more probable to appear as an illustration. More specifically we use\npopular text embedding techniques to provide the self-supervision for the\ntraining of deep CNN.\n  Our experiments demonstrate state-of-the-art performance in image\nclassification, object detection, and multi-modal retrieval compared to recent\nself-supervised or naturally-supervised approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 21:44:09 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Patel", "Yash", ""], ["Gomez", "Lluis", ""], ["Gomez", "Raul", ""], ["Rusi\u00f1ol", "Mar\u00e7al", ""], ["Karatzas", "Dimosthenis", ""], ["Jawahar", "C. V.", ""]]}, {"id": "1807.02131", "submitter": "\\c{S}eyma Y\\\"ucer", "authors": "Seyma Yucer and Yusuf Sinan Akgul", "title": "3D Human Action Recognition with Siamese-LSTM Based Deep Metric Learning", "comments": null, "journal-ref": "Journal of Image and Graphics,2018", "doi": "10.18178/joig.6.1.21-26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new 3D Human Action Recognition system as a two-phase\nsystem: (1) Deep Metric Learning Module which learns a similarity metric\nbetween two 3D joint sequences using Siamese-LSTM networks; (2) A Multiclass\nClassification Module that uses the output of the first module to produce the\nfinal recognition output. This model has several advantages: the first module\nis trained with a larger set of data because it uses many combinations of\nsequence pairs.Our deep metric learning module can also be trained\nindependently of the datasets, which makes our system modular and\ngeneralizable. We tested the proposed system on standard and newly introduced\ndatasets that showed us that initial results are promising. We will continue\ndeveloping this system by adding more sophisticated LSTM blocks and by\ncross-training between different datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:13:07 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Yucer", "Seyma", ""], ["Akgul", "Yusuf Sinan", ""]]}, {"id": "1807.02135", "submitter": "I Gede Pasek Suta Wijaya", "authors": "I Gede Pasek Suta Wijaya", "title": "Face Recognition Using Map Discriminant on YCbCr Color Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper presents face recognition using maximum a posteriori (MAP)\ndiscriminant on YCbCr color space. The YCbCr color space is considered in order\nto cover the skin information of face image on the recognition process. The\nproposed method is employed to improve the recognition rate and equal error\nrate (EER) of the gray scale based face recognition. In this case, the face\nfeatures vector consisting of small part of dominant frequency elements which\nis extracted by non-blocking DCT is implemented as dimensional reduction of the\nraw face images. The matching process between the query face features and the\ntrained face features is performed using maximum a posteriori (MAP)\ndiscriminant. From the experimental results on data from four face databases\ncontaining 2268 images with 196 classes show that the face recognition YCbCr\ncolor space provide better recognition rate and lesser EER than those of gray\nscale based face recognition which improve the first rank of grayscale based\nmethod result by about 4%. However, it requires three times more computation\ntime than that of grayscale based method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:19:30 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Wijaya", "I Gede Pasek Suta", ""]]}, {"id": "1807.02136", "submitter": "Alexander Kolesnikov", "authors": "Alexander Kolesnikov, Alina Kuznetsova, Christoph H. Lampert and\n  Vittorio Ferrari", "title": "Detecting Visual Relationships Using Box Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for detecting visual relationships, such as \"person\nriding motorcycle\" or \"bottle on table\". This task is an important step towards\ncomprehensive structured image understanding, going beyond detecting individual\nobjects. Our main novelty is a Box Attention mechanism that allows to model\npairwise interactions between objects using standard object detection\npipelines. The resulting model is conceptually clean, expressive and relies on\nwell-justified training and prediction procedures. Moreover, unlike previously\nproposed approaches, our model does not introduce any additional complex\ncomponents or hyperparameters on top of those already required by the\nunderlying detection model. We conduct an experimental evaluation on three\nchallenging datasets, V-COCO, Visual Relationships and Open Images,\ndemonstrating strong quantitative and qualitative results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:24:56 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 15:23:12 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Kolesnikov", "Alexander", ""], ["Kuznetsova", "Alina", ""], ["Lampert", "Christoph H.", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1807.02143", "submitter": "Huynh Manh", "authors": "Huynh Manh, Gita Alaghband", "title": "Spatiotemporal KSVD Dictionary Learning for Online Multi-target Tracking", "comments": "To appear in Proceedings of 15th Conference on Computer and Robot\n  Vision 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a new spatial discriminative KSVD dictionary\nalgorithm (STKSVD) for learning target appearance in online multi-target\ntracking. Different from other classification/recognition tasks (e.g. face,\nimage recognition), learning target's appearance in online multi-target\ntracking is impacted by factors such as posture/articulation changes, partial\nocclusion by background scene or other targets, background changes (human\ndetection bounding box covers human parts and part of the scene), etc. However,\nwe observe that these variations occur gradually relative to spatial and\ntemporal dynamics. We characterize the spatial and temporal information between\ntarget's samples through a new STKSVD appearance learning algorithm to better\ndiscriminate sparse code, linear classifier parameters and minimize\nreconstruction error in a single optimization system. Our appearance learning\nalgorithm and tracking framework employ two different methods of calculating\nappearance similarity score in each stage of a two-stage association: a linear\nclassifier in the first stage, and minimum residual errors in the second stage.\nThe results tested using 2DMOT2015 dataset and its public Aggregated Channel\nfeatures (ACF) human detection for all comparisons show that our method\noutperforms the existing related learning methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:36:24 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Manh", "Huynh", ""], ["Alaghband", "Gita", ""]]}, {"id": "1807.02152", "submitter": "Jun Zhang", "authors": "Jun Zhang, Ashirbani Saha, Brian J. Soher, Maciej A. Mazurowski", "title": "Automatic deep learning-based normalization of breast dynamic\n  contrast-enhanced magnetic resonance images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: To develop an automatic image normalization algorithm for\nintensity correction of images from breast dynamic contrast-enhanced magnetic\nresonance imaging (DCE-MRI) acquired by different MRI scanners with various\nimaging parameters, using only image information. Methods: DCE-MR images of 460\nsubjects with breast cancer acquired by different scanners were used in this\nstudy. Each subject had one T1-weighted pre-contrast image and three\nT1-weighted post-contrast images available. Our normalization algorithm\noperated under the assumption that the same type of tissue in different\npatients should be represented by the same voxel value. We used four\ntissue/material types as the anchors for the normalization: 1) air, 2) fat\ntissue, 3) dense tissue, and 4) heart. The algorithm proceeded in the following\ntwo steps: First, a state-of-the-art deep learning-based algorithm was applied\nto perform tissue segmentation accurately and efficiently. Then, based on the\nsegmentation results, a subject-specific piecewise linear mapping function was\napplied between the anchor points to normalize the same type of tissue in\ndifferent patients into the same intensity ranges. We evaluated the algorithm\nwith 300 subjects used for training and the rest used for testing. Results: The\napplication of our algorithm to images with different scanning parameters\nresulted in highly improved consistency in pixel values and extracted radiomics\nfeatures. Conclusion: The proposed image normalization strategy based on tissue\nsegmentation can perform intensity correction fully automatically, without the\nknowledge of the scanner parameters. Significance: We have thoroughly tested\nour algorithm and showed that it successfully normalizes the intensity of\nDCE-MR images. We made our software publicly available for others to apply in\ntheir analyses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:56:14 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Zhang", "Jun", ""], ["Saha", "Ashirbani", ""], ["Soher", "Brian J.", ""], ["Mazurowski", "Maciej A.", ""]]}, {"id": "1807.02188", "submitter": "Priyadarshini Panda", "authors": "Priyadarshini Panda, and Kaushik Roy", "title": "Implicit Generative Modeling of Random Noise during Training for\n  Adversarial Robustness", "comments": "Preliminary version of this work accepted at ICML 2019 (Workshop on\n  Uncertainty and Robustness in Deep Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Noise-based prior Learning (NoL) approach for training neural\nnetworks that are intrinsically robust to adversarial attacks. We find that the\nimplicit generative modeling of random noise with the same loss function used\nduring posterior maximization, improves a model's understanding of the data\nmanifold furthering adversarial robustness. We evaluate our approach's efficacy\nand provide a simplistic visualization tool for understanding adversarial data,\nusing Principal Component Analysis. Our analysis reveals that adversarial\nrobustness, in general, manifests in models with higher variance along the\nhigh-ranked principal components. We show that models learnt with our approach\nperform remarkably well against a wide-range of attacks. Furthermore, combining\nNoL with state-of-the-art adversarial training extends the robustness of a\nmodel, even beyond what it is adversarially trained for, in both white-box and\nblack-box attack scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 21:52:36 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 16:54:50 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 12:19:08 GMT"}, {"version": "v4", "created": "Fri, 31 May 2019 19:09:09 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Panda", "Priyadarshini", ""], ["Roy", "Kaushik", ""]]}, {"id": "1807.02232", "submitter": "Yueyu Hu", "authors": "Yueyu Hu, Wenhan Yang, Mading Li, Jiaying Liu", "title": "Progressive Spatial Recurrent Neural Network for Intra Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra prediction is an important component of modern video codecs, which is\nable to efficiently squeeze out the spatial redundancy in video frames. With\npreceding pixels as the context, traditional intra prediction schemes generate\nlinear predictions based on several predefined directions (i.e. modes) for\nblocks to be encoded. However, these modes are relatively simple and their\npredictions may fail when facing blocks with complex textures, which leads to\nadditional bits encoding the residue. In this paper, we design a Progressive\nSpatial Recurrent Neural Network (PS-RNN) that learns to conduct intra\nprediction. Specifically, our PS-RNN consists of three spatial recurrent units\nand progressively generates predictions by passing information along from\npreceding contents to blocks to be encoded. To make our network generate\npredictions considering both distortion and bit-rate, we propose to use Sum of\nAbsolute Transformed Difference (SATD) as the loss function to train PS-RNN\nsince SATD is able to measure rate-distortion cost of encoding a residue block.\nMoreover, our method supports variable-block-size for intra prediction, which\nis more practical in real coding conditions. The proposed intra prediction\nscheme achieves on average 2.5% bit-rate reduction on variable-block-size\nsettings under the same reconstruction quality compared with HEVC.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 03:13:55 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 14:59:34 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hu", "Yueyu", ""], ["Yang", "Wenhan", ""], ["Li", "Mading", ""], ["Liu", "Jiaying", ""]]}, {"id": "1807.02242", "submitter": "Minghui Liao", "authors": "Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, Xiang Bai", "title": "Mask TextSpotter: An End-to-End Trainable Neural Network for Spotting\n  Text with Arbitrary Shapes", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, models based on deep neural networks have dominated the fields of\nscene text detection and recognition. In this paper, we investigate the problem\nof scene text spotting, which aims at simultaneous text detection and\nrecognition in natural images. An end-to-end trainable neural network model for\nscene text spotting is proposed. The proposed model, named as Mask TextSpotter,\nis inspired by the newly published work Mask R-CNN. Different from previous\nmethods that also accomplish text spotting with end-to-end trainable deep\nneural networks, Mask TextSpotter takes advantage of simple and smooth\nend-to-end learning procedure, in which precise text detection and recognition\nare acquired via semantic segmentation. Moreover, it is superior to previous\nmethods in handling text instances of irregular shapes, for example, curved\ntext. Experiments on ICDAR2013, ICDAR2015 and Total-Text demonstrate that the\nproposed method achieves state-of-the-art results in both scene text detection\nand end-to-end text recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 03:40:11 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 06:49:14 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lyu", "Pengyuan", ""], ["Liao", "Minghui", ""], ["Yao", "Cong", ""], ["Wu", "Wenhao", ""], ["Bai", "Xiang", ""]]}, {"id": "1807.02247", "submitter": "Fan Yang", "authors": "Kevin Lin, Fan Yang, Qiaosong Wang, Robinson Piramuthu", "title": "Adversarial Learning for Fine-grained Image Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained image search is still a challenging problem due to the\ndifficulty in capturing subtle differences regardless of pose variations of\nobjects from fine-grained categories. In practice, a dynamic inventory with new\nfine-grained categories adds another dimension to this challenge. In this work,\nwe propose an end-to-end network, called FGGAN, that learns discriminative\nrepresentations by implicitly learning a geometric transformation from\nmulti-view images for fine-grained image search. We integrate a generative\nadversarial network (GAN) that can automatically handle complex view and pose\nvariations by converting them to a canonical view without any predefined\ntransformations. Moreover, in an open-set scenario, our network is able to\nbetter match images from unseen and unknown fine-grained categories. Extensive\nexperiments on two public datasets and a newly collected dataset have\ndemonstrated the outstanding robust performance of the proposed FGGAN in both\nclosed-set and open-set scenarios, providing as much as 10% relative\nimprovement compared to baselines.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 04:03:11 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Lin", "Kevin", ""], ["Yang", "Fan", ""], ["Wang", "Qiaosong", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1807.02250", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Peter Anderson, and Len Hamey", "title": "Face-Cap: Image Captioning using Facial Expression Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning is the process of generating a natural language description\nof an image. Most current image captioning models, however, do not take into\naccount the emotional aspect of an image, which is very relevant to activities\nand interpersonal relationships represented therein. Towards developing a model\nthat can produce human-like captions incorporating these, we use facial\nexpression features extracted from images including human faces, with the aim\nof improving the descriptive ability of the model. In this work, we present two\nvariants of our Face-Cap model, which embed facial expression features in\ndifferent ways, to generate image captions. Using all standard evaluation\nmetrics, our Face-Cap models outperform a state-of-the-art baseline model for\ngenerating image captions when applied to an image caption dataset extracted\nfrom the standard Flickr 30K dataset, consisting of around 11K images\ncontaining faces. An analysis of the captions finds that, perhaps surprisingly,\nthe improvement in caption quality appears to come not from the addition of\nadjectives linked to emotional aspects of the images, but from more variety in\nthe actions described in the captions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 04:12:20 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:30:42 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Anderson", "Peter", ""], ["Hamey", "Len", ""]]}, {"id": "1807.02251", "submitter": "WajihUllah Baig", "authors": "Wajih Ullah Baig, Umar Munir, Waqas Ellahi, Adeel Ejaz, Kashif Sardar\n  (National Database and Registration Authority)", "title": "Minutia Texture Cylinder Codes for fingerprint matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minutia Cylinder Codes (MCC) are minutiae based fingerprint descriptors that\ntake into account minutiae information in a fingerprint image for fingerprint\nmatching. In this paper, we present a modification to the underlying\ninformation of the MCC descriptor and show that using different features, the\naccuracy of matching is highly affected by such changes. MCC originally being a\nminutia only descriptor is transformed into a texture descriptor. The\ntransformation is from minutiae angular information to orientation, frequency\nand energy information using Short Time Fourier Transform (STFT) analysis. The\nminutia cylinder codes are converted to minutiae texture cylinder codes (MTCC).\nBased on a fixed set of parameters, the proposed changes to MCC show improved\nperformance on FVC 2002 and 2004 data sets and surpass the traditional MCC\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 04:25:18 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Baig", "Wajih Ullah", "", "National Database and Registration Authority"], ["Munir", "Umar", "", "National Database and Registration Authority"], ["Ellahi", "Waqas", "", "National Database and Registration Authority"], ["Ejaz", "Adeel", "", "National Database and Registration Authority"], ["Sardar", "Kashif", "", "National Database and Registration Authority"]]}, {"id": "1807.02257", "submitter": "Edgar Margffoy-Tuay", "authors": "Edgar Margffoy-Tuay, Juan C. P\\'erez, Emilio Botero, Pablo Arbel\\'aez", "title": "Dynamic Multimodal Instance Segmentation guided by natural language\n  queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of segmenting an object given a natural language\nexpression that describes it. Current techniques tackle this task by either\n(\\textit{i}) directly or recursively merging linguistic and visual information\nin the channel dimension and then performing convolutions; or by (\\textit{ii})\nmapping the expression to a space in which it can be thought of as a filter,\nwhose response is directly related to the presence of the object at a given\nspatial coordinate in the image, so that a convolution can be applied to look\nfor the object. We propose a novel method that integrates these two insights in\norder to fully exploit the recursive nature of language. Additionally, during\nthe upsampling process, we take advantage of the intermediate information\ngenerated when downsampling the image, so that detailed segmentations can be\nobtained. We compare our method against the state-of-the-art approaches in four\nstandard datasets, in which it surpasses all previous methods in six of eight\nof the splits for this task.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 05:21:06 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 22:31:18 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Margffoy-Tuay", "Edgar", ""], ["P\u00e9rez", "Juan C.", ""], ["Botero", "Emilio", ""], ["Arbel\u00e1ez", "Pablo", ""]]}, {"id": "1807.02265", "submitter": "Shiqi Yang", "authors": "Shiqi Yang and Gang Peng", "title": "Parallel Convolutional Networks for Image Recognition via a\n  Discriminator", "comments": "Accepted by ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a simple but quite effective recognition\nframework dubbed D-PCN, aiming at enhancing feature extracting ability of CNN.\nThe framework consists of two parallel CNNs, a discriminator and an extra\nclassifier which takes integrated features from parallel networks and gives\nfinal prediction. The discriminator is core which drives parallel networks to\nfocus on different regions and learn different representations. The\ncorresponding training strategy is introduced to ensures utilization of\ndiscriminator. We validate D-PCN with several CNN models on benchmark datasets:\nCIFAR-100, and ImageNet, D-PCN enhances all models. In particular it yields\nstate of the art performance on CIFAR-100 compared with related works. We also\nconduct visualization experiment on fine-grained Stanford Dogs dataset to\nverify our motivation. Additionally, we apply D-PCN for segmentation on PASCAL\nVOC 2012 and also find promotion.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 06:08:22 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 13:47:43 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 05:33:08 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Yang", "Shiqi", ""], ["Peng", "Gang", ""]]}, {"id": "1807.02294", "submitter": "Yuanhong Xu", "authors": "Yuanhong Xu, Pei Dong, Junyu Dong, Lin Qi", "title": "Combining SLAM with muti-spectral photometric stereo for real-time dense\n  3D reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining dense 3D reconstrution with low computational cost is one of the\nimportant goals in the field of SLAM. In this paper we propose a dense 3D\nreconstruction framework from monocular multispectral video sequences using\njointly semi-dense SLAM and Multispectral Photometric Stereo approaches.\nStarting from multispectral video, SALM (a) reconstructs a semi-dense 3D shape\nthat will be densified;(b) recovers relative sparse depth map that is then fed\nas prioris into optimization-based multispectral photometric stereo for a more\naccurate dense surface normal recovery;(c)obtains camera pose that is\nsubsequently used for conversion of view in the process of fusion where we\ncombine the relative sparse point cloud with the dense surface normal using the\nautomated cross-scale fusion method proposed in this paper to get a dense point\ncloud with subtle texture information. Experiments show that our method can\neffectively obtain denser 3D reconstructions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 07:40:17 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Xu", "Yuanhong", ""], ["Dong", "Pei", ""], ["Dong", "Junyu", ""], ["Qi", "Lin", ""]]}, {"id": "1807.02323", "submitter": "Andreas Pfeuffer", "authors": "Andreas Pfeuffer, and Klaus Dietmayer", "title": "Optimal Sensor Data Fusion Architecture for Object Detection in Adverse\n  Weather Conditions", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": "21st International Conference on Information Fusion (FUSION)\n  (2008) 2592-2599", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good and robust sensor data fusion in diverse weather conditions is a quite\nchallenging task. There are several fusion architectures in the literature,\ne.g. the sensor data can be fused right at the beginning (Early Fusion), or\nthey can be first processed separately and then concatenated later (Late\nFusion). In this work, different fusion architectures are compared and\nevaluated by means of object detection tasks, in which the goal is to recognize\nand localize predefined objects in a stream of data. Usually, state-of-the-art\nobject detectors based on neural networks are highly optimized for good weather\nconditions, since the well-known benchmarks only consist of sensor data\nrecorded in optimal weather conditions. Therefore, the performance of these\napproaches decreases enormously or even fails in adverse weather conditions. In\nthis work, different sensor fusion architectures are compared for good and\nadverse weather conditions for finding the optimal fusion architecture for\ndiverse weather situations. A new training strategy is also introduced such\nthat the performance of the object detector is greatly enhanced in adverse\nweather scenarios or if a sensor fails. Furthermore, the paper responds to the\nquestion if the detection accuracy can be increased further by providing the\nneural network with a-priori knowledge such as the spatial calibration of the\nsensors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 09:21:58 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Pfeuffer", "Andreas", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1807.02370", "submitter": "Dong Hye Ye", "authors": "Dong Hye Ye, Gregery T. Buzzard, Max Ruby, Charles A. Bouman", "title": "Deep Back Projection for Sparse-View CT Reconstruction", "comments": "GlobalSIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtered back projection (FBP) is a classical method for image reconstruction\nfrom sinogram CT data. FBP is computationally efficient but produces lower\nquality reconstructions than more sophisticated iterative methods, particularly\nwhen the number of views is lower than the number required by the Nyquist rate.\nIn this paper, we use a deep convolutional neural network (CNN) to produce\nhigh-quality reconstructions directly from sinogram data. A primary novelty of\nour approach is that we first back project each view separately to form a stack\nof back projections and then feed this stack as input into the convolutional\nneural network. These single-view back projections convert the encoding of\nsinogram data into the appropriate spatial location, which can then be\nleveraged by the spatial invariance of the CNN to learn the reconstruction\neffectively. We demonstrate the benefit of our CNN based back projection on\nsimulated sparse-view CT data over classical FBP.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 12:06:19 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Ye", "Dong Hye", ""], ["Buzzard", "Gregery T.", ""], ["Ruby", "Max", ""], ["Bouman", "Charles A.", ""]]}, {"id": "1807.02371", "submitter": "Maximilian Jaritz", "authors": "Maximilian Jaritz, Raoul de Charette, Marin Toromanoff, Etienne Perot,\n  Fawzi Nashashibi", "title": "End-to-End Race Driving with Deep Reinforcement Learning", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present research using the latest reinforcement learning algorithm for\nend-to-end driving without any mediated perception (object recognition, scene\nunderstanding). The newly proposed reward and learning strategies lead together\nto faster convergence and more robust driving using only RGB image from a\nforward facing camera. An Asynchronous Actor Critic (A3C) framework is used to\nlearn the car control in a physically and graphically realistic rally game,\nwith the agents evolving simultaneously on tracks with a variety of road\nstructures (turns, hills), graphics (seasons, location) and physics (road\nadherence). A thorough evaluation is conducted and generalization is proven on\nunseen tracks and using legal speed limits. Open loop tests on real sequences\nof images show some domain adaption capability of our method.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 12:08:53 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 13:28:49 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Jaritz", "Maximilian", ""], ["de Charette", "Raoul", ""], ["Toromanoff", "Marin", ""], ["Perot", "Etienne", ""], ["Nashashibi", "Fawzi", ""]]}, {"id": "1807.02401", "submitter": "Kaixin Hu", "authors": "Kaixin Hu, Peter O'Connor", "title": "Learning a Representation Map for Robot Navigation using Deep\n  Variational Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to use Variational Autoencoder (VAE) to learn a\nrepresentation of an indoor environment that can be used for robot navigation.\nWe use images extracted from a video, in which a camera takes a tour around a\nhouse, for training the VAE model with a 4 dimensional latent space. After the\nmodel is trained, each real frame has a corresponding representation point on\nmanifold in the latent space, and each representation point has corresponding\nreconstructed image. For the navigation problem, we map the starting image and\ndestination image to the latent space, then optimize a path on the learned\nmanifold connecting the two points, and finally map the path back through\ndecoder to a sequence of images. The ideal sequence of images should correspond\nto a route that is spatially continuous - i.e. neighbor images in the route\nshould correspond to neighbor locations in physical space. Such a route could\nbe used for navigation with computer vision techniques, i.e. a robot could\nfollow the image sequence from starting location to destination in the\nenvironment step by step. We implement this algorithm, but find in our\nexperimental results that the resulting route is not satisfactory. The route\nconsist of several discontinuous image frames along the ideal routes, so that\nthe route could not be followed by a robot with computer vision techniques in\npractice. In our evaluation, we propose two reasons for our failure to\nautomatically find continuous routes: (1) The VAE tends to capture global\nstructures, but discard the details; (2) the Euclidean similarity metric used\nfor measuring continuity between house images is sub-optimal. For further work,\nwe propose: trying other generative models like VAE-GANs which may be better at\nreconstructing the details to learn the representation map, and adjusting the\nsimilarity metric in the path selecting algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:46:32 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:22:04 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Hu", "Kaixin", ""], ["O'Connor", "Peter", ""]]}, {"id": "1807.02420", "submitter": "Yuexiang Li", "authors": "Yuexiang Li, Xinpeng Xie, Linlin Shen and Shaoxiong Liu", "title": "Reversed Active Learning based Atrous DenseNet for Pathological Image\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Witnessed the development of deep learning in recent years, increasing number\nof researches try to adopt deep learning model for medical image analysis.\nHowever, the usage of deep learning networks for the pathological image\nanalysis encounters several challenges, e.g. high resolution (gigapixel) of\npathological images and lack of annotations of cancer areas. To address the\nchallenges, we proposed a complete framework for the pathological image\nclassification, which consists of a novel training strategy, namely reversed\nactive learning (RAL), and an advanced network, namely atrous DenseNet (ADN).\nThe proposed RAL can remove the mislabel patches in the training set. The\nrefined training set can then be used to train widely used deep learning\nnetworks, e.g. VGG-16, ResNets, etc. A novel deep learning network, i.e. atrous\nDenseNet (ADN), is also proposed for the classification of pathological images.\nThe proposed ADN achieves multi-scale feature extraction by integrating the\natrous convolutions to the Dense Block. The proposed RAL and ADN have been\nevaluated on two pathological datasets, i.e. BACH and CCG. The experimental\nresults demonstrate the excellent performance of the proposed ADN + RAL\nframework, i.e. the average patch-level ACAs of 94.10% and 92.05% on BACH and\nCCG validation sets were achieved.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 13:57:48 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Li", "Yuexiang", ""], ["Xie", "Xinpeng", ""], ["Shen", "Linlin", ""], ["Liu", "Shaoxiong", ""]]}, {"id": "1807.02437", "submitter": "Alexey Novikov", "authors": "Alexey Novikov, David Major, Maria Wimmer, Dimitrios Lenis, Katja\n  B\\\"uhler", "title": "Deep Sequential Segmentation of Organs in Volumetric Medical Scans", "comments": null, "journal-ref": "Published in IEEE Transactions on Medical Imaging on 16 November\n  2018, URL:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8537944&isnumber=4359023", "doi": "10.1109/TMI.2018.2881678", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation in 3D scans is playing an increasingly important role in current\nclinical practice supporting diagnosis, tissue quantification, or treatment\nplanning. The current 3D approaches based on convolutional neural networks\nusually suffer from at least three main issues caused predominantly by\nimplementation constraints - first, they require resizing the volume to the\nlower-resolutional reference dimensions, second, the capacity of such\napproaches is very limited due to memory restrictions, and third, all slices of\nvolumes have to be available at any given training or testing time. We address\nthese problems by a U-Net-like architecture consisting of bidirectional\nconvolutional LSTM and convolutional, pooling, upsampling and concatenation\nlayers enclosed into time-distributed wrappers. Our network can either process\nthe full volumes in a sequential manner, or segment slabs of slices on demand.\nWe demonstrate performance of our architecture on vertebrae and liver\nsegmentation tasks in 3D CT scans.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 14:48:04 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 10:13:15 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Novikov", "Alexey", ""], ["Major", "David", ""], ["Wimmer", "Maria", ""], ["Lenis", "Dimitrios", ""], ["B\u00fchler", "Katja", ""]]}, {"id": "1807.02443", "submitter": "Maxim Tatarchenko", "authors": "Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, Qian-Yi Zhou", "title": "Tangent Convolutions for Dense Prediction in 3D", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to semantic scene analysis using deep convolutional\nnetworks. Our approach is based on tangent convolutions - a new construction\nfor convolutional networks on 3D data. In contrast to volumetric approaches,\nour method operates directly on surface geometry. Crucially, the construction\nis applicable to unstructured point clouds and other noisy real-world data. We\nshow that tangent convolutions can be evaluated efficiently on large-scale\npoint clouds with millions of points. Using tangent convolutions, we design a\ndeep fully-convolutional network for semantic segmentation of 3D point clouds,\nand apply it to challenging real-world datasets of indoor and outdoor 3D\nenvironments. Experimental results show that the presented approach outperforms\nother recent deep network constructions in detailed analysis of large 3D\nscenes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 15:14:51 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Tatarchenko", "Maxim", ""], ["Park", "Jaesik", ""], ["Koltun", "Vladlen", ""], ["Zhou", "Qian-Yi", ""]]}, {"id": "1807.02444", "submitter": "Andre Beckus", "authors": "Andre Beckus, Alexandru Tamasan, George K. Atia", "title": "Multi-modal Non-line-of-sight Passive Imaging", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2896517", "report-no": null, "categories": "cs.CV physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-line-of-sight (NLOS) imaging of an object using the light\nreflected off a diffusive wall. The wall scatters incident light such that a\nlens is no longer useful to form an image. Instead, we exploit the 4D spatial\ncoherence function to reconstruct a 2D projection of the obscured object. The\napproach is completely passive in the sense that no control over the light\nilluminating the object is assumed and is compatible with the partially\ncoherent fields ubiquitous in both the indoor and outdoor environments. We\nformulate a multi-criteria convex optimization problem for reconstruction,\nwhich fuses the reflected field's intensity and spatial coherence information\nat different scales. Our formulation leverages established optics models of\nlight propagation and scattering and exploits the sparsity common to many\nimages in different bases. We also develop an algorithm based on the\nalternating direction method of multipliers to efficiently solve the convex\nprogram proposed. A means for analyzing the null space of the measurement\nmatrices is provided as well as a means for weighting the contribution of\nindividual measurements to the reconstruction. This paper holds promise to\nadvance passive imaging in the challenging NLOS regimes in which the intensity\ndoes not necessarily retain distinguishable features and provides a framework\nfor multi-modal information fusion for efficient scene reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 15:15:03 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 04:23:29 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Beckus", "Andre", ""], ["Tamasan", "Alexandru", ""], ["Atia", "George K.", ""]]}, {"id": "1807.02480", "submitter": "Yang Hu", "authors": "Yang Hu, Andrea Soltoggio, Russell Lock, Steve Carter", "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel fully convolutional two-stream fusion\nnetwork (FCTSFN) for interactive image segmentation. The proposed network\nincludes two sub-networks: a two-stream late fusion network (TSLFN) that\npredicts the foreground at a reduced resolution, and a multi-scale refining\nnetwork (MSRN) that refines the foreground at full resolution. The TSLFN\nincludes two distinct deep streams followed by a fusion network. The intuition\nis that, since user interactions are more direct information on\nforeground/background than the image itself, the two-stream structure of the\nTSLFN reduces the number of layers between the pure user interaction features\nand the network output, allowing the user interactions to have a more direct\nimpact on the segmentation result. The MSRN fuses the features from different\nlayers of TSLFN with different scales, in order to seek the local to global\ninformation on the foreground to refine the segmentation result at full\nresolution. We conduct comprehensive experiments on four benchmark datasets.\nThe results show that the proposed network achieves competitive performance\ncompared to current state-of-the-art interactive image segmentation methods\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 16:48:31 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 19:43:11 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Hu", "Yang", ""], ["Soltoggio", "Andrea", ""], ["Lock", "Russell", ""], ["Carter", "Steve", ""]]}, {"id": "1807.02504", "submitter": "Zhiyuan Zha", "authors": "Zhiyuan Zha, Xin Yuan, Bihan Wen, Jiantao Zhou, Jiachao Zhang, Ce Zhu", "title": "From Rank Estimation to Rank Approximation: Rank Residual Constraint for\n  Image Restoration", "comments": null, "journal-ref": "IEEE Transaction on Image Processing 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel approach to the rank minimization problem,\ntermed rank residual constraint (RRC) model. Different from existing low-rank\nbased approaches, such as the well-known nuclear norm minimization (NNM) and\nthe weighted nuclear norm minimization (WNNM), which estimate the underlying\nlow-rank matrix directly from the corrupted observations, we progressively\napproximate the underlying low-rank matrix via minimizing the rank residual.\nThrough integrating the image nonlocal self-similarity (NSS) prior with the\nproposed RRC model, we apply it to image restoration tasks, including image\ndenoising and image compression artifacts reduction. Towards this end, we first\nobtain a good reference of the original image groups by using the image NSS\nprior, and then the rank residual of the image groups between this reference\nand the degraded image is minimized to achieve a better estimate to the desired\nimage. In this manner, both the reference and the estimated image are updated\ngradually and jointly in each iteration. Based on the group-based sparse\nrepresentation model, we further provide a theoretical analysis on the\nfeasibility of the proposed RRC model. Experimental results demonstrate that\nthe proposed RRC model outperforms many state-of-the-art schemes in both the\nobjective and perceptual quality.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 17:43:20 GMT"}, {"version": "v10", "created": "Thu, 2 Jan 2020 04:04:55 GMT"}, {"version": "v11", "created": "Tue, 4 Feb 2020 02:49:01 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 11:35:30 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 03:21:55 GMT"}, {"version": "v4", "created": "Tue, 7 Aug 2018 18:46:35 GMT"}, {"version": "v5", "created": "Sat, 11 Aug 2018 13:04:22 GMT"}, {"version": "v6", "created": "Tue, 14 Aug 2018 02:08:11 GMT"}, {"version": "v7", "created": "Wed, 30 Oct 2019 02:13:46 GMT"}, {"version": "v8", "created": "Fri, 29 Nov 2019 14:06:19 GMT"}, {"version": "v9", "created": "Fri, 6 Dec 2019 03:01:52 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Zha", "Zhiyuan", ""], ["Yuan", "Xin", ""], ["Wen", "Bihan", ""], ["Zhou", "Jiantao", ""], ["Zhang", "Jiachao", ""], ["Zhu", "Ce", ""]]}, {"id": "1807.02536", "submitter": "Xin Yu", "authors": "Xin Yu, Sagar Chaturvedi, Chen Feng, Yuichi Taguchi, Teng-Yok Lee,\n  Clinton Fernandes, Srikumar Ramalingam", "title": "VLASE: Vehicle Localization by Aggregating Semantic Edges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose VLASE, a framework to use semantic edge features\nfrom images to achieve on-road localization. Semantic edge features denote edge\ncontours that separate pairs of distinct objects such as building-sky, road-\nsidewalk, and building-ground. While prior work has shown promising results by\nutilizing the boundary between prominent classes such as sky and building using\nskylines, we generalize this approach to consider semantic edge features that\narise from 19 different classes. Our localization algorithm is simple, yet very\npowerful. We extract semantic edge features using a recently introduced CASENet\narchitecture and utilize VLAD framework to perform image retrieval. Our\nexperiments show that we achieve improvement over some of the state-of-the-art\nlocalization algorithms such as SIFT-VLAD and its deep variant NetVLAD. We use\nablation study to study the importance of different semantic classes and show\nthat our unified approach achieves better performance compared to individual\nprominent features such as skylines.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 18:15:06 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yu", "Xin", ""], ["Chaturvedi", "Sagar", ""], ["Feng", "Chen", ""], ["Taguchi", "Yuichi", ""], ["Lee", "Teng-Yok", ""], ["Fernandes", "Clinton", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1807.02569", "submitter": "Rahul Deo", "authors": "Geoffrey H. Tison, Jeffrey Zhang, Francesca N. Delling, Rahul C. Deo", "title": "Automated and Interpretable Patient ECG Profiles for Disease Detection,\n  Tracking, and Discovery", "comments": "13 pages, 6 figures, 1 Table + Supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The electrocardiogram or ECG has been in use for over 100 years and remains\nthe most widely performed diagnostic test to characterize cardiac structure and\nelectrical activity. We hypothesized that parallel advances in computing power,\ninnovations in machine learning algorithms, and availability of large-scale\ndigitized ECG data would enable extending the utility of the ECG beyond its\ncurrent limitations, while at the same time preserving interpretability, which\nis fundamental to medical decision-making. We identified 36,186 ECGs from the\nUCSF database that were 1) in normal sinus rhythm and 2) would enable training\nof specific models for estimation of cardiac structure or function or detection\nof disease. We derived a novel model for ECG segmentation using convolutional\nneural networks (CNN) and Hidden Markov Models (HMM) and evaluated its output\nby comparing electrical interval estimates to 141,864 measurements from the\nclinical workflow. We built a 725-element patient-level ECG profile using\ndownsampled segmentation data and trained machine learning models to estimate\nleft ventricular mass, left atrial volume, mitral annulus e' and to detect and\ntrack four diseases: pulmonary arterial hypertension (PAH), hypertrophic\ncardiomyopathy (HCM), cardiac amyloid (CA), and mitral valve prolapse (MVP).\nCNN-HMM derived ECG segmentation agreed with clinical estimates, with median\nabsolute deviations (MAD) as a fraction of observed value of 0.6% for heart\nrate and 4% for QT interval. Patient-level ECG profiles enabled quantitative\nestimates of left ventricular and mitral annulus e' velocity with good\ndiscrimination in binary classification models of left ventricular hypertrophy\nand diastolic function. Models for disease detection ranged from AUROC of 0.94\nto 0.77 for MVP. Top-ranked variables for all models included known ECG\ncharacteristics along with novel predictors of these traits/diseases.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 21:12:12 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Tison", "Geoffrey H.", ""], ["Zhang", "Jeffrey", ""], ["Delling", "Francesca N.", ""], ["Deo", "Rahul C.", ""]]}, {"id": "1807.02570", "submitter": "Nan Yang", "authors": "Nan Yang, Rui Wang, J\\\"org St\\\"uckler, Daniel Cremers", "title": "Deep Virtual Stereo Odometry: Leveraging Deep Depth Prediction for\n  Monocular Direct Sparse Odometry", "comments": "To appear in ECCV 2018, Munich. 17 pages including references, 7\n  figures, 4 tables. Supplementary material:\n  https://vision.in.tum.de/members/yangn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monocular visual odometry approaches that purely rely on geometric cues are\nprone to scale drift and require sufficient motion parallax in successive\nframes for motion estimation and 3D reconstruction. In this paper, we propose\nto leverage deep monocular depth prediction to overcome limitations of\ngeometry-based monocular visual odometry. To this end, we incorporate deep\ndepth predictions into Direct Sparse Odometry (DSO) as direct virtual stereo\nmeasurements. For depth prediction, we design a novel deep network that refines\npredicted depth from a single image in a two-stage process. We train our\nnetwork in a semi-supervised way on photoconsistency in stereo images and on\nconsistency with accurate sparse depth reconstructions from Stereo DSO. Our\ndeep predictions excel state-of-the-art approaches for monocular depth on the\nKITTI benchmark. Moreover, our Deep Virtual Stereo Odometry clearly exceeds\nprevious monocular and deep learning based methods in accuracy. It even\nachieves comparable performance to the state-of-the-art stereo methods, while\nonly relying on a single camera.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 21:14:31 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 16:24:02 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Yang", "Nan", ""], ["Wang", "Rui", ""], ["St\u00fcckler", "J\u00f6rg", ""], ["Cremers", "Daniel", ""]]}, {"id": "1807.02578", "submitter": "Ilke Demir", "authors": "Ilke Demir and Daniel G. Aliaga", "title": "Guided Proceduralization: Optimizing Geometry Processing and Grammar\n  Extraction for Architectural Models", "comments": null, "journal-ref": "Computers & Graphics, Volume 74, 2018, Pages 257-267, ISSN\n  0097-8493", "doi": "10.1016/j.cag.2018.05.013", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a guided proceduralization framework that optimizes geometry\nprocessing on architectural input models to extract target grammars. We aim to\nprovide efficient artistic workflows by creating procedural representations\nfrom existing 3D models, where the procedural expressiveness is controlled by\nthe user. Architectural reconstruction and modeling tasks have been handled as\neither time consuming manual processes or procedural generation with difficult\ncontrol and artistic influence. We bridge the gap between creation and\ngeneration by converting existing manually modeled architecture to procedurally\neditable parametrized models, and carrying the guidance to procedural domain by\nletting the user define the target procedural representation. Additionally, we\npropose various applications of such procedural representations, including\nguided completion of point cloud models, controllable 3D city modeling, and\nother benefits of procedural modeling.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 22:22:53 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Demir", "Ilke", ""], ["Aliaga", "Daniel G.", ""]]}, {"id": "1807.02587", "submitter": "Kihwan Kim", "authors": "Ben Eckart and Kihwan Kim and Jan Kautz", "title": "Fast and Accurate Point Cloud Registration using Trees of Gaussian\n  Mixtures", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud registration sits at the core of many important and challenging\n3D perception problems including autonomous navigation, SLAM, object/scene\nrecognition, and augmented reality. In this paper, we present a new\nregistration algorithm that is able to achieve state-of-the-art speed and\naccuracy through its use of a hierarchical Gaussian Mixture Model (GMM)\nrepresentation. Our method constructs a top-down multi-scale representation of\npoint cloud data by recursively running many small-scale data likelihood\nsegmentations in parallel on a GPU. We leverage the resulting representation\nusing a novel PCA-based optimization criterion that adaptively finds the best\nscale to perform data association between spatial subsets of point cloud data.\nCompared to previous Iterative Closest Point and GMM-based techniques, our\ntree-based point association algorithm performs data association in\nlogarithmic-time while dynamically adjusting the level of detail to best match\nthe complexity and spatial distribution characteristics of local scene\ngeometry. In addition, unlike other GMM methods that restrict covariances to be\nisotropic, our new PCA-based optimization criterion well-approximates the true\nMLE solution even when fully anisotropic Gaussian covariances are used.\nEfficient data association, multi-scale adaptability, and a robust MLE\napproximation produce an algorithm that is up to an order of magnitude both\nfaster and more accurate than current state-of-the-art on a wide variety of 3D\ndatasets captured from LiDAR to structured light.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 23:44:51 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Eckart", "Ben", ""], ["Kim", "Kihwan", ""], ["Kautz", "Jan", ""]]}, {"id": "1807.02588", "submitter": "Stanislav Pidhorskyi", "authors": "Stanislav Pidhorskyi, Ranya Almohsen, Donald A Adjeroh, Gianfranco\n  Doretto", "title": "Generative Probabilistic Novelty Detection with Adversarial Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novelty detection is the problem of identifying whether a new data point is\nconsidered to be an inlier or an outlier. We assume that training data is\navailable to describe only the inlier distribution. Recent approaches primarily\nleverage deep encoder-decoder network architectures to compute a reconstruction\nerror that is used to either compute a novelty score or to train a one-class\nclassifier. While we too leverage a novel network of that kind, we take a\nprobabilistic approach and effectively compute how likely is that a sample was\ngenerated by the inlier distribution. We achieve this with two main\ncontributions. First, we make the computation of the novelty probability\nfeasible because we linearize the parameterized manifold capturing the\nunderlying structure of the inlier distribution, and show how the probability\nfactorizes and can be computed with respect to local coordinates of the\nmanifold tangent space. Second, we improved the training of the autoencoder\nnetwork. An extensive set of results show that the approach achieves\nstate-of-the-art results on several benchmark datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 23:46:30 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 01:39:29 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Pidhorskyi", "Stanislav", ""], ["Almohsen", "Ranya", ""], ["Adjeroh", "Donald A", ""], ["Doretto", "Gianfranco", ""]]}, {"id": "1807.02608", "submitter": "Matthew Yung", "authors": "Matthew Yung, Eli T. Brown, Alexander Rasin, Jacob D. Furst, Daniela\n  S. Raicu", "title": "Synthetic Sampling for Multi-Class Malignancy Prediction", "comments": "5 pages, 3 figures, 4 Tables, KDD MLMH'18 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore several oversampling techniques for an imbalanced multi-label\nclassification problem, a setting often encountered when developing models for\nComputer-Aided Diagnosis (CADx) systems. While most CADx systems aim to\noptimize classifiers for overall accuracy without considering the relative\ndistribution of each class, we look into using synthetic sampling to increase\nper-class performance when predicting the degree of malignancy. Using low-level\nimage features and a random forest classifier, we show that using synthetic\noversampling techniques increases the sensitivity of the minority classes by an\naverage of 7.22% points, with as much as a 19.88% point increase in sensitivity\nfor a particular minority class. Furthermore, the analysis of low-level image\nfeature distributions for the synthetic nodules reveals that these nodules can\nprovide insights on how to preprocess image data for better classification\nperformance or how to supplement the original datasets when more data\nacquisition is feasible.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 03:23:08 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yung", "Matthew", ""], ["Brown", "Eli T.", ""], ["Rasin", "Alexander", ""], ["Furst", "Jacob D.", ""], ["Raicu", "Daniela S.", ""]]}, {"id": "1807.02632", "submitter": "Akihiko Sayo", "authors": "Ryosuke Kimura, Akihiko Sayo, Fabian Lorenzo Dayrit, Yuta Nakashima,\n  Hiroshi Kawasaki, Ambrosio Blanco and Katsushi Ikeuchi", "title": "Representing a Partially Observed Non-Rigid 3D Human Using Eigen-Texture\n  and Eigen-Deformation", "comments": "6pages, accepted to ICPR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconstruction of the shape and motion of humans from RGB-D is a challenging\nproblem, receiving much attention in recent years. Recent approaches for\nfull-body reconstruction use a statistic shape model, which is built upon\naccurate full-body scans of people in skin-tight clothes, to complete invisible\nparts due to occlusion. Such a statistic model may still be fit to an RGB-D\nmeasurement with loose clothes but cannot describe its deformations, such as\nclothing wrinkles. Observed surfaces may be reconstructed precisely from actual\nmeasurements, while we have no cues for unobserved surfaces. For full-body\nreconstruction with loose clothes, we propose to use lower dimensional\nembeddings of texture and deformation referred to as eigen-texturing and\neigen-deformation, to reproduce views of even unobserved surfaces. Provided a\nfull-body reconstruction from a sequence of partial measurements as 3D meshes,\nthe texture and deformation of each triangle are then embedded using\neigen-decomposition. Combined with neural-network-based coefficient regression,\nour method synthesizes the texture and deformation from arbitrary viewpoints.\nWe evaluate our method using simulated data and visually demonstrate how our\nmethod works on real data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 08:18:31 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kimura", "Ryosuke", ""], ["Sayo", "Akihiko", ""], ["Dayrit", "Fabian Lorenzo", ""], ["Nakashima", "Yuta", ""], ["Kawasaki", "Hiroshi", ""], ["Blanco", "Ambrosio", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "1807.02635", "submitter": "Yunseok Jang", "authors": "Yunseok Jang, Gunhee Kim, Yale Song", "title": "Video Prediction with Appearance and Motion Conditions", "comments": "Accepted paper at ICML 2018. Project page:\n  http://vision.snu.ac.kr/projects/amc-gan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video prediction aims to generate realistic future frames by learning dynamic\nvisual patterns. One fundamental challenge is to deal with future uncertainty:\nHow should a model behave when there are multiple correct, equally probable\nfuture? We propose an Appearance-Motion Conditional GAN to address this\nchallenge. We provide appearance and motion information as conditions that\nspecify how the future may look like, reducing the level of uncertainty. Our\nmodel consists of a generator, two discriminators taking charge of appearance\nand motion pathways, and a perceptual ranking module that encourages videos of\nsimilar conditions to look similar. To train our model, we develop a novel\nconditioning scheme that consists of different combinations of appearance and\nmotion conditions. We evaluate our model using facial expression and human\naction datasets and report favorable results compared to existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 08:55:10 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Jang", "Yunseok", ""], ["Kim", "Gunhee", ""], ["Song", "Yale", ""]]}, {"id": "1807.02654", "submitter": "Ivan Ustyuzhaninov", "authors": "Ivan Ustyuzhaninov, Claudio Michaelis, Wieland Brendel, Matthias\n  Bethge", "title": "One-shot Texture Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce one-shot texture segmentation: the task of segmenting an input\nimage containing multiple textures given a patch of a reference texture. This\ntask is designed to turn the problem of texture-based perceptual grouping into\nan objective benchmark. We show that it is straight-forward to generate large\nsynthetic data sets for this task from a relatively small number of natural\ntextures. In particular, this task can be cast as a self-supervised problem\nthereby alleviating the need for massive amounts of manually annotated data\nnecessary for traditional segmentation tasks. In this paper we introduce and\nstudy two concrete data sets: a dense collage of textures (CollTex) and a\ncluttered texturized Omniglot data set. We show that a baseline model trained\non these synthesized data is able to generalize to natural images and videos\nwithout further fine-tuning, suggesting that the learned image representations\nare useful for higher-level vision tasks.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 12:23:38 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ustyuzhaninov", "Ivan", ""], ["Michaelis", "Claudio", ""], ["Brendel", "Wieland", ""], ["Bethge", "Matthias", ""]]}, {"id": "1807.02657", "submitter": "Tae Joon Jun", "authors": "Dohyeun Kim, Tae Joon Jun, Daeyoung Kim, Youngsub Eom", "title": "Tournament Based Ranking CNN for the Cataract grading", "comments": "Submitted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving the classification problem, unbalanced number of dataset among the\nclasses often causes performance degradation. Especially when some classes\ndominate the other classes with its large number of datasets, trained model\nshows low performance in identifying the dominated classes. This is common case\nwhen it comes to medical dataset. Because the case with a serious degree is not\nquite usual, there are imbalance in number of dataset between severe case and\nnormal cases of diseases. Also, there is difficulty in precisely identifying\ngrade of medical data because of vagueness between them. To solve these\nproblems, we propose new architecture of convolutional neural network named\nTournament based Ranking CNN which shows remarkable performance gain in\nidentifying dominated classes while trading off very small accuracy loss in\ndominating classes. Our Approach complemented problems that occur when method\nof Ranking CNN that aggregates outputs of multiple binary neural network models\nis applied to medical data. By having tournament structure in aggregating\nmethod and using very deep pretrained binary models, our proposed model\nrecorded 68.36% of exact match accuracy, while Ranking CNN recorded 53.40%,\npretrained Resnet recorded 56.12% and CNN with linear regression recorded\n57.48%. As a result, our proposed method is applied efficiently to cataract\ngrading which have ordinal labels with imbalanced number of data among classes,\nalso can be applied further to medical problems which have similar features to\ncataract and similar dataset configuration.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 12:40:32 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kim", "Dohyeun", ""], ["Jun", "Tae Joon", ""], ["Kim", "Daeyoung", ""], ["Eom", "Youngsub", ""]]}, {"id": "1807.02700", "submitter": "SeyedMajid Azimi", "authors": "Seyed Majid Azimi, Eleonora Vig, Reza Bahmanyar, Marco K\\\"orner, Peter\n  Reinartz", "title": "Towards Multi-class Object Detection in Unconstrained Remote Sensing\n  Imagery", "comments": "ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic multi-class object detection in remote sensing images in\nunconstrained scenarios is of high interest for several applications including\ntraffic monitoring and disaster management. The huge variation in object scale,\norientation, category, and complex backgrounds, as well as the different camera\nsensors pose great challenges for current algorithms. In this work, we propose\na new method consisting of a novel joint image cascade and feature pyramid\nnetwork with multi-size convolution kernels to extract multi-scale strong and\nweak semantic features. These features are fed into rotation-based region\nproposal and region of interest networks to produce object detections. Finally,\nrotational non-maximum suppression is applied to remove redundant detections.\nDuring training, we minimize joint horizontal and oriented bounding box loss\nfunctions, as well as a novel loss that enforces oriented boxes to be\nrectangular. Our method achieves 68.16% mAP on horizontal and 72.45% mAP on\noriented bounding box detection tasks on the challenging DOTA dataset,\noutperforming all published methods by a large margin (+6% and +12% absolute\nimprovement, respectively). Furthermore, it generalizes to two other datasets,\nNWPU VHR-10 and UCAS-AOD, and achieves competitive results with the baselines\neven when trained on DOTA. Our method can be deployed in multi-class object\ndetection applications, regardless of the image and object scales and\norientations, making it a great choice for unconstrained aerial and satellite\nimagery.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 17:48:57 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 04:57:59 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 12:52:41 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Azimi", "Seyed Majid", ""], ["Vig", "Eleonora", ""], ["Bahmanyar", "Reza", ""], ["K\u00f6rner", "Marco", ""], ["Reinartz", "Peter", ""]]}, {"id": "1807.02701", "submitter": "Alireza Vafaei Sadr", "authors": "A. Vafaei Sadr, Etienne. E. Vos, Bruce A. Bassett, Zafiirah Hosenie,\n  N. Oozeer, Michelle Lochner", "title": "DeepSource: Point Source Detection using Deep Learning", "comments": "15 pages, 13 figures, submitted to MNRAS", "journal-ref": "MNRAS, Volume 484, Issue 2, April 2019, Pages 2793-2806", "doi": "10.1093/mnras/stz131", "report-no": null, "categories": "astro-ph.IM cs.CV cs.LG hep-ph stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Point source detection at low signal-to-noise is challenging for astronomical\nsurveys, particularly in radio interferometry images where the noise is\ncorrelated. Machine learning is a promising solution, allowing the development\nof algorithms tailored to specific telescope arrays and science cases. We\npresent DeepSource - a deep learning solution - that uses convolutional neural\nnetworks to achieve these goals. DeepSource enhances the Signal-to-Noise Ratio\n(SNR) of the original map and then uses dynamic blob detection to detect\nsources. Trained and tested on two sets of 500 simulated 1 deg x 1 deg MeerKAT\nimages with a total of 300,000 sources, DeepSource is essentially perfect in\nboth purity and completeness down to SNR = 4 and outperforms PyBDSF in all\nmetrics. For uniformly-weighted images it achieves a Purity x Completeness (PC)\nscore at SNR = 3 of 0.73, compared to 0.31 for the best PyBDSF model. For\nnatural-weighting we find a smaller improvement of ~40% in the PC score at SNR\n= 3. If instead we ask where either of the purity or completeness first drop to\n90%, we find that DeepSource reaches this value at SNR = 3.6 compared to the\n4.3 of PyBDSF (natural-weighting). A key advantage of DeepSource is that it can\nlearn to optimally trade off purity and completeness for any science case under\nconsideration. Our results show that deep learning is a promising approach to\npoint source detection in astronomical images.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 18:00:07 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Sadr", "A. Vafaei", ""], ["Vos", "Etienne. E.", ""], ["Bassett", "Bruce A.", ""], ["Hosenie", "Zafiirah", ""], ["Oozeer", "N.", ""], ["Lochner", "Michelle", ""]]}, {"id": "1807.02716", "submitter": "Yimin Liu", "authors": "Yimin Liu, Wenyue Sun, Louis J. Durlofsky", "title": "A Deep-Learning-Based Geological Parameterization for History Matching\n  Complex Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new low-dimensional parameterization based on principal component analysis\n(PCA) and convolutional neural networks (CNN) is developed to represent complex\ngeological models. The CNN-PCA method is inspired by recent developments in\ncomputer vision using deep learning. CNN-PCA can be viewed as a generalization\nof an existing optimization-based PCA (O-PCA) method. Both CNN-PCA and O-PCA\nentail post-processing a PCA model to better honor complex geological features.\nIn CNN-PCA, rather than use a histogram-based regularization as in O-PCA, a new\nregularization involving a set of metrics for multipoint statistics is\nintroduced. The metrics are based on summary statistics of the nonlinear filter\nresponses of geological models to a pre-trained deep CNN. In addition, in the\nCNN-PCA formulation presented here, a convolutional neural network is trained\nas an explicit transform function that can post-process PCA models quickly.\nCNN-PCA is shown to provide both unconditional and conditional realizations\nthat honor the geological features present in reference SGeMS geostatistical\nrealizations for a binary channelized system. Flow statistics obtained through\nsimulation of random CNN-PCA models closely match results for random SGeMS\nmodels for a demanding case in which O-PCA models lead to significant\ndiscrepancies. Results for history matching are also presented. In this\nassessment CNN-PCA is applied with derivative-free optimization, and a subspace\nrandomized maximum likelihood method is used to provide multiple posterior\nmodels. Data assimilation and significant uncertainty reduction are achieved\nfor existing wells, and physically reasonable predictions are also obtained for\nnew wells. Finally, the CNN-PCA method is extended to a more complex\nnon-stationary bimodal deltaic fan system, and is shown to provide high-quality\nrealizations for this challenging example.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 20:34:04 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Liu", "Yimin", ""], ["Sun", "Wenyue", ""], ["Durlofsky", "Louis J.", ""]]}, {"id": "1807.02739", "submitter": "Toufiq Parag", "authors": "Toufiq Parag, Daniel Berger, Lee Kamentsky, Benedikt Staffler, Donglai\n  Wei, Moritz Helmstaedter, Jeff W. Lichtman, Hanspeter Pfister", "title": "Detecting Synapse Location and Connectivity by Signed Proximity\n  Estimation and Pruning with Deep Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic connectivity detection is a critical task for neural reconstruction\nfrom Electron Microscopy (EM) data. Most of the existing algorithms for synapse\ndetection do not identify the cleft location and direction of connectivity\nsimultaneously. The few methods that computes direction along with contact\nlocation have only been demonstrated to work on either dyadic (most common in\nvertebrate brain) or polyadic (found in fruit fly brain) synapses, but not on\nboth types. In this paper, we present an algorithm to automatically predict the\nlocation as well as the direction of both dyadic and polyadic synapses. The\nproposed algorithm first generates candidate synaptic connections from\nvoxelwise predictions of signed proximity generated by a 3D U-net. A second 3D\nCNN then prunes the set of candidates to produce the final detection of cleft\nand connectivity orientation. Experimental results demonstrate that the\nproposed method outperforms the existing methods for determining synapses in\nboth rodent and fruit fly brain.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 02:00:14 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 02:12:27 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Parag", "Toufiq", ""], ["Berger", "Daniel", ""], ["Kamentsky", "Lee", ""], ["Staffler", "Benedikt", ""], ["Wei", "Donglai", ""], ["Helmstaedter", "Moritz", ""], ["Lichtman", "Jeff W.", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "1807.02740", "submitter": "Wentai Zhang", "authors": "Wentai Zhang, Haoliang Jiang, Zhangsihao Yang, Soji Yamakawa, Kenji\n  Shimada, Levent Burak Kara", "title": "Data-driven Upsampling of Point Clouds", "comments": "Preprint submitted to CAD", "journal-ref": "Computer-Aided Design, Volume 112, Pages 1-13, 2019", "doi": "10.1016/j.cad.2019.02.006.", "report-no": null, "categories": "cs.CV cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High quality upsampling of sparse 3D point clouds is critically useful for a\nwide range of geometric operations such as reconstruction, rendering, meshing,\nand analysis. In this paper, we propose a data-driven algorithm that enables an\nupsampling of 3D point clouds without the need for hard-coded rules. Our\napproach uses a deep network with Chamfer distance as the loss function,\ncapable of learning the latent features in point clouds belonging to different\nobject categories. We evaluate our algorithm across different amplification\nfactors, with upsampling learned and performed on objects belonging to the same\ncategory as well as different categories. We also explore the desirable\ncharacteristics of input point clouds as a function of the distribution of the\npoint samples. Finally, we demonstrate the performance of our algorithm in\nsingle-category training versus multi-category training scenarios. The final\nproposed model is compared against a baseline, optimization-based upsampling\nmethod. Results indicate that our algorithm is capable of generating more\nuniform and accurate upsamplings.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 02:19:09 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 03:49:16 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Zhang", "Wentai", ""], ["Jiang", "Haoliang", ""], ["Yang", "Zhangsihao", ""], ["Yamakawa", "Soji", ""], ["Shimada", "Kenji", ""], ["Kara", "Levent Burak", ""]]}, {"id": "1807.02752", "submitter": "Rui Fan", "authors": "Rui Fan, Naim Dahnoun", "title": "Real-time stereo vision-based lane detection system", "comments": "24 pages, 10 figures", "journal-ref": "Measurement Science and Technology 29.7 (2018): 074005", "doi": "10.1088/1361-6501/aac163", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of multiple curved lane markings on a non-flat road surface is\nstill a challenging task for automotive applications. To make an improvement,\nthe depth information can be used to greatly enhance the robustness of the lane\ndetection systems. The proposed system in this paper is developed from our\nprevious work where the dense vanishing point Vp is estimated globally to\nassist the detection of multiple curved lane markings. However, the outliers in\nthe optimal solution may severely affect the accuracy of the least squares\nfitting when estimating Vp. Therefore, in this paper we use Random Sample\nConsensus to update the inliers and outliers iteratively until the fraction of\nthe number of inliers versus the total number exceeds our pre-set threshold.\nThis significantly helps the system to overcome some suddenly changing\nconditions. Furthermore, we propose a novel lane position validation approach\nwhich provides a piecewise weight based on Vp and the gradient to reduce the\ngradient magnitude of the non-lane candidates. Then, we compute the energy of\neach possible solution and select all satisfying lane positions for\nvisualisation. The proposed system is implemented on a heterogeneous system\nwhich consists of an Intel Core i7-4720HQ CPU and a NVIDIA GTX 970M GPU. A\nprocessing speed of 143 fps has been achieved, which is over 38 times faster\nthan our previous work. Also, in order to evaluate the detection precision, we\ntested 2495 frames with 5361 lanes from the KITTI database (1637 lanes more\nthan our previous experiment). It is shown that the overall successful\ndetection rate is improved from 98.7% to 99.5%.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 04:21:28 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Fan", "Rui", ""], ["Dahnoun", "Naim", ""]]}, {"id": "1807.02758", "submitter": "Yulun Zhang", "authors": "Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu", "title": "Image Super-Resolution Using Very Deep Residual Channel Attention\n  Networks", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural network (CNN) depth is of crucial importance for image\nsuper-resolution (SR). However, we observe that deeper networks for image SR\nare more difficult to train. The low-resolution inputs and features contain\nabundant low-frequency information, which is treated equally across channels,\nhence hindering the representational ability of CNNs. To solve these problems,\nwe propose the very deep residual channel attention networks (RCAN).\nSpecifically, we propose a residual in residual (RIR) structure to form very\ndeep network, which consists of several residual groups with long skip\nconnections. Each residual group contains some residual blocks with short skip\nconnections. Meanwhile, RIR allows abundant low-frequency information to be\nbypassed through multiple skip connections, making the main network focus on\nlearning high-frequency information. Furthermore, we propose a channel\nattention mechanism to adaptively rescale channel-wise features by considering\ninterdependencies among channels. Extensive experiments show that our RCAN\nachieves better accuracy and visual improvements against state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 05:45:45 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 21:57:37 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Zhang", "Yulun", ""], ["Li", "Kunpeng", ""], ["Li", "Kai", ""], ["Wang", "Lichen", ""], ["Zhong", "Bineng", ""], ["Fu", "Yun", ""]]}, {"id": "1807.02799", "submitter": "Haseeb Shah", "authors": "Haseeb Shah, Khurram Javed and Faisal Shafait", "title": "Distillation Techniques for Pseudo-rehearsal Based Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to learn from incrementally arriving data is essential for any\nlife-long learning system. However, standard deep neural networks forget the\nknowledge about the old tasks, a phenomenon called catastrophic forgetting,\nwhen trained on incrementally arriving data. We discuss the biases in current\nGenerative Adversarial Networks (GAN) based approaches that learn the\nclassifier by knowledge distillation from previously trained classifiers. These\nbiases cause the trained classifier to perform poorly. We propose an approach\nto remove these biases by distilling knowledge from the classifier of AC-GAN.\nExperiments on MNIST and CIFAR10 show that this method is comparable to current\nstate of the art rehearsal based approaches. The code for this paper is\navailable at https://bit.ly/incremental-learning\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 11:01:00 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 09:46:50 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 08:05:39 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Shah", "Haseeb", ""], ["Javed", "Khurram", ""], ["Shafait", "Faisal", ""]]}, {"id": "1807.02800", "submitter": "Pascal Mettes", "authors": "Pascal Mettes and Cees G. M. Snoek", "title": "Spatio-Temporal Instance Learning: Action Tubes from Class Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is spatio-temporal action localization in videos, using\nonly the supervision from video-level class labels. The state-of-the-art casts\nthis weakly-supervised action localization regime as a Multiple Instance\nLearning problem, where instances are a priori computed spatio-temporal\nproposals. Rather than disconnecting the spatio-temporal learning from the\ntraining, we propose Spatio-Temporal Instance Learning, which enables action\nlocalization directly from box proposals in video frames. We outline the\nassumptions of our model and propose a max-margin objective and optimization\nwith latent variables that enable spatio-temporal learning of actions from\nvideo labels. We also provide an efficient linking algorithm and two reranking\nstrategies to facilitate and further improve the action localization.\nExperimental evaluation on four action datasets demonstrate the effectiveness\nof our approach for localization from weak supervision. Moreover, we show how\nto incorporate other supervision levels and mixtures, as a step towards\ndetermining optimal supervision strategies for action localization.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 11:12:51 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 21:13:28 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Mettes", "Pascal", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1807.02802", "submitter": "Khurram Javed Mr", "authors": "Khurram Javed and Faisal Shafait", "title": "Revisiting Distillation and Incremental Classifier Learning", "comments": "16 pages, 5 figures, open-source, pytorch, ACCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key differences between the learning mechanism of humans and\nArtificial Neural Networks (ANNs) is the ability of humans to learn one task at\na time. ANNs, on the other hand, can only learn multiple tasks simultaneously.\nAny attempts at learning new tasks incrementally cause them to completely\nforget about previous tasks. This lack of ability to learn incrementally,\ncalled Catastrophic Forgetting, is considered a major hurdle in building a true\nAI system. In this paper, our goal is to isolate the truly effective existing\nideas for incremental learning from those that only work under certain\nconditions. To this end, we first thoroughly analyze the current state of the\nart (iCaRL) method for incremental learning and demonstrate that the good\nperformance of the system is not because of the reasons presented in the\nexisting literature. We conclude that the success of iCaRL is primarily due to\nknowledge distillation and recognize a key limitation of knowledge\ndistillation, i.e, it often leads to bias in classifiers. Finally, we propose a\ndynamic threshold moving algorithm that is able to successfully remove this\nbias. We demonstrate the effectiveness of our algorithm on CIFAR100 and MNIST\ndatasets showing near-optimal results. Our implementation is available at\nhttps://github.com/Khurramjaved96/incremental-learning.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 11:42:31 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 16:24:50 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Javed", "Khurram", ""], ["Shafait", "Faisal", ""]]}, {"id": "1807.02804", "submitter": "Xiaomeng Li", "authors": "Xiaomeng Li, Lequan Yu, Chi-Wing Fu, Pheng-Ann Heng", "title": "Deeply Supervised Rotation Equivariant Network for Lesion Segmentation\n  in Dermoscopy Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic lesion segmentation in dermoscopy images is an essential step for\ncomputer-aided diagnosis of melanoma. The dermoscopy images exhibits rotational\nand reflectional symmetry, however, this geometric property has not been\nencoded in the state-of-the-art convolutional neural networks based skin lesion\nsegmentation methods. In this paper, we present a deeply supervised rotation\nequivariant network for skin lesion segmentation by extending the recent group\nrotation equivariant network~\\cite{cohen2016group}. Specifically, we propose\nthe G-upsampling and G-projection operations to adapt the rotation equivariant\nclassification network for our skin lesion segmentation problem. To further\nincrease the performance, we integrate the deep supervision scheme into our\nproposed rotation equivariant segmentation architecture. The whole framework is\nequivariant to input transformations, including rotation and reflection, which\nimproves the network efficiency and thus contributes to the segmentation\nperformance. We extensively evaluate our method on the ISIC 2017 skin lesion\nchallenge dataset. The experimental results show that our rotation equivariant\nnetworks consistently excel the regular counterparts with the same model\ncomplexity under different experimental settings. Our best model achieves\n77.23\\%(JA) on the test dataset, outperforming the state-of-the-art challenging\nmethods and further demonstrating the effectiveness of our proposed deeply\nsupervised rotation equivariant segmentation network. Our best model also\noutperforms the state-of-the-art challenging methods, which further demonstrate\nthe effectiveness of our proposed deeply supervised rotation equivariant\nsegmentation network.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 11:49:49 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Li", "Xiaomeng", ""], ["Yu", "Lequan", ""], ["Fu", "Chi-Wing", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1807.02839", "submitter": "Anjan Dutta", "authors": "Anjan Dutta, Pau Riba, Josep Llad\\'os, Alicia Forn\\'es", "title": "Hierarchical stochastic graphlet embedding for graph-based pattern\n  recognition", "comments": "In Neural Computing and Applications (17 pages, 5 figures, 6 tables)", "journal-ref": null, "doi": "10.1007/s00521-019-04642-7", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being very successful within the pattern recognition and machine\nlearning community, graph-based methods are often unusable because of the lack\nof mathematical operations defined in graph domain. Graph embedding, which maps\ngraphs to a vectorial space, has been proposed as a way to tackle these\ndifficulties enabling the use of standard machine learning techniques. However,\nit is well known that graph embedding functions usually suffer from the loss of\nstructural information. In this paper, we consider the hierarchical structure\nof a graph as a way to mitigate this loss of information. The hierarchical\nstructure is constructed by topologically clustering the graph nodes, and\nconsidering each cluster as a node in the upper hierarchical level. Once this\nhierarchical structure is constructed, we consider several configurations to\ndefine the mapping into a vector space given a classical graph embedding, in\nparticular, we propose to make use of the Stochastic Graphlet Embedding (SGE).\nBroadly speaking, SGE produces a distribution of uniformly sampled low to high\norder graphlets as a way to embed graphs into the vector space. In what\nfollows, the coarse-to-fine structure of a graph hierarchy and the statistics\nfetched by the SGE complements each other and includes important structural\ninformation with varied contexts. Altogether, these two techniques\nsubstantially cope with the usual information loss involved in graph embedding\ntechniques, obtaining a more robust graph representation. This fact has been\ncorroborated through a detailed experimental evaluation on various benchmark\ngraph datasets, where we outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 15:33:22 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 17:37:12 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Dutta", "Anjan", ""], ["Riba", "Pau", ""], ["Llad\u00f3s", "Josep", ""], ["Forn\u00e9s", "Alicia", ""]]}, {"id": "1807.02842", "submitter": "Tianfu Wu", "authors": "Bo Li, Tianfu Wu, Lun Zhang and Rufeng Chu", "title": "Auto-Context R-CNN", "comments": "Rejected by ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Region-based convolutional neural networks\n(R-CNN)~\\cite{fast_rcnn,faster_rcnn,mask_rcnn} have largely dominated object\ndetection. Operators defined on RoIs (Region of Interests) play an important\nrole in R-CNNs such as RoIPooling~\\cite{fast_rcnn} and\nRoIAlign~\\cite{mask_rcnn}. They all only utilize information inside RoIs for\nRoI prediction, even with their recent deformable\nextensions~\\cite{deformable_cnn}. Although surrounding context is well-known\nfor its importance in object detection, it has yet been integrated in R-CNNs in\na flexible and effective way. Inspired by the auto-context\nwork~\\cite{auto_context} and the multi-class object layout\nwork~\\cite{nms_context}, this paper presents a generic context-mining RoI\noperator (i.e., \\textit{RoICtxMining}) seamlessly integrated in R-CNNs, and the\nresulting object detection system is termed \\textbf{Auto-Context R-CNN} which\nis trained end-to-end. The proposed RoICtxMining operator is a simple yet\neffective two-layer extension of the RoIPooling or RoIAlign operator. Centered\nat an object-RoI, it creates a $3\\times 3$ layout to mine contextual\ninformation adaptively in the $8$ surrounding context regions on-the-fly.\nWithin each of the $8$ context regions, a context-RoI is mined in term of\ndiscriminative power and its RoIPooling / RoIAlign features are concatenated\nwith the object-RoI for final prediction. \\textit{The proposed Auto-Context\nR-CNN is robust to occlusion and small objects, and shows promising\nvulnerability for adversarial attacks without being adversarially-trained.} In\nexperiments, it is evaluated using RoIPooling as the backbone and shows\ncompetitive results on Pascal VOC, Microsoft COCO, and KITTI datasets\n(including $6.9\\%$ mAP improvements over the R-FCN~\\cite{rfcn} method on COCO\n\\textit{test-dev} dataset and the first place on both KITTI pedestrian and\ncyclist detection as of this submission).\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 15:45:28 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Li", "Bo", ""], ["Wu", "Tianfu", ""], ["Zhang", "Lun", ""], ["Chu", "Rufeng", ""]]}, {"id": "1807.02851", "submitter": "Francisco Barranco", "authors": "Francisco Barranco, Cornelia Fermuller, Eduardo Ros", "title": "Real-time clustering and multi-target tracking using event-based sensors", "comments": "Conference paper. Accepted for IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is crucial for many computer vision applications such as robust\ntracking, object detection and segmentation. This work presents a real-time\nclustering technique that takes advantage of the unique properties of\nevent-based vision sensors. Since event-based sensors trigger events only when\nthe intensity changes, the data is sparse, with low redundancy. Thus, our\napproach redefines the well-known mean-shift clustering method using\nasynchronous events instead of conventional frames. The potential of our\napproach is demonstrated in a multi-target tracking application using Kalman\nfilters to smooth the trajectories. We evaluated our method on an existing\ndataset with patterns of different shapes and speeds, and a new dataset that we\ncollected. The sensor was attached to the Baxter robot in an eye-in-hand setup\nmonitoring real-world objects in an action manipulation task. Clustering\naccuracy achieved an F-measure of 0.95, reducing the computational cost by 88%\ncompared to the frame-based method. The average error for tracking was 2.5\npixels and the clustering achieved a consistent number of clusters along time.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 16:43:32 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Barranco", "Francisco", ""], ["Fermuller", "Cornelia", ""], ["Ros", "Eduardo", ""]]}, {"id": "1807.02855", "submitter": "Karim Iskakov", "authors": "Karim Iskakov", "title": "Semi-parametric Image Inpainting", "comments": "Quick Draw Irregular Mask Dataset (QD-IMD):\n  http://github.com/karfly/qd-imd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a semi-parametric approach to image inpainting for\nirregular holes. The nonparametric part consists of an external image database.\nDuring test time database is used to retrieve a supplementary image, similar to\nthe input masked picture, and utilize it as auxiliary information for the deep\nneural network. Further, we propose a novel method of generating masks with\nirregular holes and present public dataset with such masks. Experiments on\nCelebA-HQ dataset show that our semi-parametric method yields more realistic\nresults than previous approaches, which is confirmed by the user study.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 17:34:01 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 17:42:46 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Iskakov", "Karim", ""]]}, {"id": "1807.02857", "submitter": "Pushparaja Murugan", "authors": "Pushparaja Murugan", "title": "Learning The Sequential Temporal Information with Recurrent Neural\n  Networks", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recurrent Networks are one of the most powerful and promising artificial\nneural network algorithms to processing the sequential data such as natural\nlanguages, sound, time series data. Unlike traditional feed-forward network,\nRecurrent Network has a inherent feed back loop that allows to store the\ntemporal context information and pass the state of information to the entire\nsequences of the events. This helps to achieve the state of art performance in\nmany important tasks such as language modeling, stock market prediction, image\ncaptioning, speech recognition, machine translation and object tracking etc.,\nHowever, training the fully connected RNN and managing the gradient flow are\nthe complicated process. Many studies are carried out to address the mentioned\nlimitation. This article is intent to provide the brief details about recurrent\nneurons, its variances and trips & tricks to train the fully recurrent neural\nnetwork. This review work is carried out as a part of our IPO studio software\nmodule 'Multiple Object Tracking'.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 17:57:27 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Murugan", "Pushparaja", ""]]}, {"id": "1807.02894", "submitter": "Sergiu Deitsch", "authors": "Sergiu Deitsch, Vincent Christlein, Stephan Berger, Claudia\n  Buerhop-Lutz, Andreas Maier, Florian Gallwitz, Christian Riess", "title": "Automatic Classification of Defective Photovoltaic Module Cells in\n  Electroluminescence Images", "comments": null, "journal-ref": null, "doi": "10.1016/j.solener.2019.02.067", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electroluminescence (EL) imaging is a useful modality for the inspection of\nphotovoltaic (PV) modules. EL images provide high spatial resolution, which\nmakes it possible to detect even finest defects on the surface of PV modules.\nHowever, the analysis of EL images is typically a manual process that is\nexpensive, time-consuming, and requires expert knowledge of many different\ntypes of defects. In this work, we investigate two approaches for automatic\ndetection of such defects in a single image of a PV cell. The approaches differ\nin their hardware requirements, which are dictated by their respective\napplication scenarios. The more hardware-efficient approach is based on\nhand-crafted features that are classified in a Support Vector Machine (SVM). To\nobtain a strong performance, we investigate and compare various processing\nvariants. The more hardware-demanding approach uses an end-to-end deep\nConvolutional Neural Network (CNN) that runs on a Graphics Processing Unit\n(GPU). Both approaches are trained on 1,968 cells extracted from high\nresolution EL intensity images of mono- and polycrystalline PV modules. The CNN\nis more accurate, and reaches an average accuracy of 88.42%. The SVM achieves a\nslightly lower average accuracy of 82.44%, but can run on arbitrary hardware.\nBoth automated approaches make continuous, highly accurate monitoring of PV\ncells feasible.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 23:24:00 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 21:16:47 GMT"}, {"version": "v3", "created": "Sat, 16 Mar 2019 21:17:39 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Deitsch", "Sergiu", ""], ["Christlein", "Vincent", ""], ["Berger", "Stephan", ""], ["Buerhop-Lutz", "Claudia", ""], ["Maier", "Andreas", ""], ["Gallwitz", "Florian", ""], ["Riess", "Christian", ""]]}, {"id": "1807.02905", "submitter": "Saeid Asgari Taghanaki", "authors": "Saeid Asgari Taghanaki, Arkadeep Das, Ghassan Hamarneh", "title": "Vulnerability Analysis of Chest X-Ray Image Classification Against\n  Adversarial Attacks", "comments": "Accepted in MICCAI, DLF, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been several successful deep learning approaches for\nautomatically classifying chest X-ray images into different disease categories.\nHowever, there is not yet a comprehensive vulnerability analysis of these\nmodels against the so-called adversarial perturbations/attacks, which makes\ndeep models more trustful in clinical practices. In this paper, we extensively\nanalyzed the performance of two state-of-the-art classification deep networks\non chest X-ray images. These two networks were attacked by three different\ncategories (ten methods in total) of adversarial methods (both white- and\nblack-box), namely gradient-based, score-based, and decision-based attacks.\nFurthermore, we modified the pooling operations in the two classification\nnetworks to measure their sensitivities against different attacks, on the\nspecific task of chest X-ray classification.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 00:58:04 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 22:01:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Taghanaki", "Saeid Asgari", ""], ["Das", "Arkadeep", ""], ["Hamarneh", "Ghassan", ""]]}, {"id": "1807.02908", "submitter": "Walid Abdullah Al", "authors": "Walid Abdullah Al, Il Dong Yun", "title": "Partial Policy-based Reinforcement Learning for Anatomical Landmark\n  Localization in 3D Medical Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deploying the idea of long-term cumulative return, reinforcement learning has\nshown remarkable performance in various fields. We propose a formulation of the\nlandmark localization in 3D medical images as a reinforcement learning problem.\nWhereas value-based methods have been widely used to solve similar problems, we\nadopt an actor-critic based direct policy search method framed in a temporal\ndifference learning approach. Successful behavior learning is challenging in\nlarge state and/or action spaces, requiring many trials. We introduce a partial\npolicy-based reinforcement learning to enable solving the large problem of\nlocalization by learning the optimal policy on smaller partial domains.\nIndependent actors efficiently learn the corresponding partial policies, each\nutilizing their own independent critic. The proposed policy reconstruction from\nthe partial policies ensures a robust and efficient localization utilizing the\nsub-agents solving simple binary decision problems in their corresponding\npartial action spaces. The proposed reinforcement learning requires a small\nnumber of trials to learn the optimal behavior compared with the original\nbehavior learning scheme.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 01:34:14 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 06:22:17 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Al", "Walid Abdullah", ""], ["Yun", "Il Dong", ""]]}, {"id": "1807.02917", "submitter": "Shiqi Yang", "authors": "Shiqi Yang and Gang Peng", "title": "Attention to Refine through Multi-Scales for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel attention model for semantic segmentation, which\naggregates multi-scale and context features to refine prediction. Specifically,\nthe skeleton convolutional neural network framework takes in multiple different\nscales inputs, by which means the CNN can get representations in different\nscales. The proposed attention model will handle the features from different\nscale streams respectively and integrate them. Then location attention branch\nof the model learns to softly weight the multi-scale features at each pixel\nlocation. Moreover, we add an recalibrating branch, parallel to where location\nattention comes out, to recalibrate the score map per class. We achieve quite\ncompetitive results on PASCAL VOC 2012 and ADE20K datasets, which surpass\nbaseline and related works.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 02:31:44 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yang", "Shiqi", ""], ["Peng", "Gang", ""]]}, {"id": "1807.02925", "submitter": "Jeesoo Kim", "authors": "Jeesoo Kim, Jangho Kim, Jaeyoung Yoo, Daesik Kim, Nojun Kwak", "title": "Vehicle Image Generation Going Well with The Surroundings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the generative neural networks have made a breakthrough in the image\ngeneration problem, lots of researches on their applications have been studied\nsuch as image restoration, style transfer and image completion. However, there\nhas been few research generating objects in uncontrolled real-world\nenvironments. In this paper, we propose a novel approach for vehicle image\ngeneration in real-world scenes. Using a subnetwork based on a precedent work\nof image completion, our model makes the shape of an object. Details of objects\nare trained by an additional colorization and refinement subnetwork, resulting\nin a better quality of generated objects. Unlike many other works, our method\ndoes not require any segmentation layout but still makes a plausible vehicle in\nthe image. We evaluate our method by using images from Berkeley Deep Drive\n(BDD) and Cityscape datasets, which are widely used for object detection and\nimage segmentation problems. The adequacy of the generated images by the\nproposed method has also been evaluated using a widely utilized object\ndetection algorithm and the FID score.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 03:26:10 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 08:34:54 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 09:08:33 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Kim", "Jeesoo", ""], ["Kim", "Jangho", ""], ["Yoo", "Jaeyoung", ""], ["Kim", "Daesik", ""], ["Kwak", "Nojun", ""]]}, {"id": "1807.02929", "submitter": "Jia-Xing Zhong", "authors": "Jia-Xing Zhong, Nannan Li, Weijie Kong, Tao Zhang, Thomas H. Li, Ge Li", "title": "Step-by-step Erasion, One-by-one Collection: A Weakly Supervised\n  Temporal Action Detector", "comments": "To Appear in ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised temporal action detection is a Herculean task in\nunderstanding untrimmed videos, since no supervisory signal except the\nvideo-level category label is available on training data. Under the supervision\nof category labels, weakly supervised detectors are usually built upon\nclassifiers. However, there is an inherent contradiction between classifier and\ndetector; i.e., a classifier in pursuit of high classification performance\nprefers top-level discriminative video clips that are extremely fragmentary,\nwhereas a detector is obliged to discover the whole action instance without\nmissing any relevant snippet. To reconcile this contradiction, we train a\ndetector by driving a series of classifiers to find new actionness clips\nprogressively, via step-by-step erasion from a complete video. During the test\nphase, all we need to do is to collect detection results from the one-by-one\ntrained classifiers at various erasing steps. To assist in the collection\nprocess, a fully connected conditional random field is established to refine\nthe temporal localization outputs. We evaluate our approach on two prevailing\ndatasets, THUMOS'14 and ActivityNet. The experiments show that our detector\nadvances state-of-the-art weakly supervised temporal action detection results,\nand even compares with quite a few strongly supervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 03:33:54 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 08:12:36 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhong", "Jia-Xing", ""], ["Li", "Nannan", ""], ["Kong", "Weijie", ""], ["Zhang", "Tao", ""], ["Li", "Thomas H.", ""], ["Li", "Ge", ""]]}, {"id": "1807.02939", "submitter": "Sangryul Jeon", "authors": "Sangryul Jeon, Seungryong Kim, Dongbo Min and Kwanghoon Sohn", "title": "PARN: Pyramidal Affine Regression Networks for Dense Semantic\n  Correspondence", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep architecture for dense semantic correspondence,\ncalled pyramidal affine regression networks (PARN), that estimates\nlocally-varying affine transformation fields across images. To deal with\nintra-class appearance and shape variations that commonly exist among different\ninstances within the same object category, we leverage a pyramidal model where\naffine transformation fields are progressively estimated in a coarse-to-fine\nmanner so that the smoothness constraint is naturally imposed within deep\nnetworks. PARN estimates residual affine transformations at each level and\ncomposes them to estimate final affine transformations. Furthermore, to\novercome the limitations of insufficient training data for semantic\ncorrespondence, we propose a novel weakly-supervised training scheme that\ngenerates progressive supervisions by leveraging a correspondence consistency\nacross image pairs. Our method is fully learnable in an end-to-end manner and\ndoes not require quantizing infinite continuous affine transformation fields.\nTo the best of our knowledge, it is the first work that attempts to estimate\ndense affine transformation fields in a coarse-to-fine manner within deep\nnetworks. Experimental results demonstrate that PARN outperforms the\nstate-of-the-art methods for dense semantic correspondence on various\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 04:55:09 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 06:10:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Jeon", "Sangryul", ""], ["Kim", "Seungryong", ""], ["Min", "Dongbo", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1807.02941", "submitter": "Zhuotun Zhu", "authors": "Zhuotun Zhu, Yingda Xia, Lingxi Xie, Elliot K. Fishman, Alan L. Yuille", "title": "Multi-Scale Coarse-to-Fine Segmentation for Screening Pancreatic Ductal\n  Adenocarcinoma", "comments": "Accepted by MICCAI 2019, 4 figures, 2 tables, 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an intuitive approach of detecting pancreatic ductal\nadenocarcinoma (PDAC), the most common type of pancreatic cancer, by checking\nabdominal CT scans. Our idea is named multi-scale\nsegmentation-for-classification, which classifies volumes by checking if at\nleast a sufficient number of voxels is segmented as tumors, by which we can\nprovide radiologists with tumor locations. In order to deal with tumors with\ndifferent scales, we train and test our volumetric segmentation networks with\nmulti-scale inputs in a coarse-to-fine flowchart. A post-processing module is\nused to filter out outliers and reduce false alarms. We collect a new dataset\ncontaining 439 CT scans, in which 136 cases were diagnosed with PDAC and 303\ncases are normal, which is the largest set for PDAC tumors to the best of our\nknowledge. To offer the best trade-off between sensitivity and specificity, our\nproposed framework reports a sensitivity of 94.1% at a specificity of 98.5%,\nwhich demonstrates the potential to make a clinical impact.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 05:01:19 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 02:28:41 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Zhu", "Zhuotun", ""], ["Xia", "Yingda", ""], ["Xie", "Lingxi", ""], ["Fishman", "Elliot K.", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1807.02947", "submitter": "Snehasis Mukherjee", "authors": "Snehasis Mukherjee, Leburu Anvitha and T. Mohana Lahari", "title": "Human Activity Recognition in RGB-D Videos by Dynamic Images", "comments": "Submitted in ICARCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition in RGB-D videos has been an active research topic\nduring the last decade. However, no efforts have been found in the literature,\nfor recognizing human activity in RGB-D videos where several performers are\nperforming simultaneously. In this paper we introduce such a challenging\ndataset with several performers performing the activities. We present a novel\nmethod for recognizing human activities in such videos. The proposed method\naims in capturing the motion information of the whole video by producing a\ndynamic image corresponding to the input video. We use two parallel ResNext-101\nto produce the dynamic images for the RGB video and depth video separately. The\ndynamic images contain only the motion information and hence, the unnecessary\nbackground information are eliminated. We send the two dynamic images extracted\nfrom the RGB and Depth videos respectively, through a fully connected layer of\nneural networks. The proposed dynamic image reduces the complexity of the\nrecognition process by extracting a sparse matrix from a video. However, the\nproposed system maintains the required motion information for recognizing the\nactivity. The proposed method has been tested on the MSR Action 3D dataset and\nhas shown comparable performances with respect to the state-of-the-art. We also\napply the proposed method on our own dataset, where the proposed method\noutperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 05:28:19 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Mukherjee", "Snehasis", ""], ["Anvitha", "Leburu", ""], ["Lahari", "T. Mohana", ""]]}, {"id": "1807.02951", "submitter": "Nripesh Parajuli", "authors": "Nripesh Parajuli, Allen Lu, Kevinminh Ta, John C. Stendahl, Nabil\n  Boutagy, Imran Alkhalil, Melissa Eberle, Geng-Shi Jeng, Maria Zontak, Matthew\n  ODonnell, Albert J. Sinusas, James S. Duncan", "title": "Flow Network Tracking for Spatiotemporal and Periodic Point Matching:\n  Applied to Cardiac Motion Analysis", "comments": "Submitted manuscript to Medical Image Analysis Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate quantification of left ventricular (LV) deformation/strain shows\nsignificant promise for quantitatively assessing cardiac function for use in\ndiagnosis and therapy planning (Jasaityte et al., 2013). However, accurate\nestimation of the displacement of myocardial tissue and hence LV strain has\nbeen challenging due to a variety of issues, including those related to\nderiving tracking tokens from images and following tissue locations over the\nentire cardiac cycle. In this work, we propose a point matching scheme where\ncorrespondences are modeled as flow through a graphical network. Myocardial\nsurface points are set up as nodes in the network and edges define neighborhood\nrelationships temporally. The novelty lies in the constraints that are imposed\non the matching scheme, which render the correspondences one-to-one through the\nentire cardiac cycle, and not just two consecutive frames. The constraints also\nencourage motion to be cyclic, which is an important characteristic of LV\nmotion. We validate our method by applying it to the estimation of quantitative\nLV displacement and strain estimation using 8 synthetic and 8 open-chested\ncanine 4D echocardiographic image sequences, the latter with sonomicrometric\ncrystals implanted on the LV wall. We were able to achieve excellent tracking\naccuracy on the synthetic dataset and observed a good correlation with\ncrystal-based strains on the in-vivo data.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 05:54:05 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Parajuli", "Nripesh", ""], ["Lu", "Allen", ""], ["Ta", "Kevinminh", ""], ["Stendahl", "John C.", ""], ["Boutagy", "Nabil", ""], ["Alkhalil", "Imran", ""], ["Eberle", "Melissa", ""], ["Jeng", "Geng-Shi", ""], ["Zontak", "Maria", ""], ["ODonnell", "Matthew", ""], ["Sinusas", "Albert J.", ""], ["Duncan", "James S.", ""]]}, {"id": "1807.02962", "submitter": "Chih-Yi Chiu", "authors": "Chih-Yi Chiu, Amorntip Prayoonwong, and Yin-Chih Liao", "title": "Learning to Index for Nearest Neighbor Search", "comments": "This paper was accepted by IEEE Transcations on Pattern Analysis and\n  Machine Intelligence in March 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a novel ranking model based on learning\nneighborhood relationships embedded in the index space. Given a query point,\nconventional approximate nearest neighbor search calculates the distances to\nthe cluster centroids, before ranking the clusters from near to far based on\nthe distances. The data indexed in the top-ranked clusters are retrieved and\ntreated as the nearest neighbor candidates for the query. However, the loss of\nquantization between the data and cluster centroids will inevitably harm the\nsearch accuracy. To address this problem, the proposed model ranks clusters\nbased on their nearest neighbor probabilities rather than the query-centroid\ndistances. The nearest neighbor probabilities are estimated by employing neural\nnetworks to characterize the neighborhood relationships, i.e., the density\nfunction of nearest neighbors with respect to the query. The proposed\nprobability-based ranking can replace the conventional distance-based ranking\nfor finding candidate clusters, and the predicted probability can be used to\ndetermine the data quantity to be retrieved from the candidate cluster. Our\nexperimental results demonstrated that the proposed ranking model could boost\nthe search performance effectively in billion-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 06:55:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 16:52:27 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 12:21:18 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Chiu", "Chih-Yi", ""], ["Prayoonwong", "Amorntip", ""], ["Liao", "Yin-Chih", ""]]}, {"id": "1807.02975", "submitter": "Xu Liu", "authors": "Xu Liu, Licheng Jiao, Xu Tang, Qigong Sun, Dan Zhang", "title": "Polarimetric Convolutional Network for PolSAR Image Classification", "comments": "15 pages", "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no.\n  5, pp. 3040-3054, May 2019", "doi": "10.1109/TGRS.2018.2879984", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approaches for analyzing the polarimetric scattering matrix of\npolarimetric synthetic aperture radar (PolSAR) data have always been the focus\nof PolSAR image classification. Generally, the polarization coherent matrix and\nthe covariance matrix obtained by the polarimetric scattering matrix only show\na limited number of polarimetric information. In order to solve this problem,\nwe propose a sparse scattering coding way to deal with polarimetric scattering\nmatrix and obtain a close complete feature. This encoding mode can also\nmaintain polarimetric information of scattering matrix completely. At the same\ntime, in view of this encoding way, we design a corresponding classification\nalgorithm based on convolution network to combine this feature. Based on sparse\nscattering coding and convolution neural network, the polarimetric\nconvolutional network is proposed to classify PolSAR images by making full use\nof polarimetric information. We perform the experiments on the PolSAR images\nacquired by AIRSAR and RADARSAT-2 to verify the proposed method. The\nexperimental results demonstrate that the proposed method get better results\nand has huge potential for PolSAR data classification. Source code for sparse\nscattering coding is available at\nhttps://github.com/liuxuvip/Polarimetric-Scattering-Coding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 07:52:13 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 02:23:41 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Liu", "Xu", ""], ["Jiao", "Licheng", ""], ["Tang", "Xu", ""], ["Sun", "Qigong", ""], ["Zhang", "Dan", ""]]}, {"id": "1807.02996", "submitter": "Mark Pfeiffer", "authors": "Guoxiang Zhou, Berta Bescos, Marcin Dymczyk, Mark Pfeiffer, Jos\\'e\n  Neira, Roland Siegwart", "title": "Dynamic Objects Segmentation for Visual Localization in Urban\n  Environments", "comments": "4 pages, submitted to the IROS 2018 Workshop \"From Freezing to\n  Jostling Robots: Current Challenges and New Paradigms for Safe Robot\n  Navigation in Dense Crowds\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual localization and mapping is a crucial capability to address many\nchallenges in mobile robotics. It constitutes a robust, accurate and\ncost-effective approach for local and global pose estimation within prior maps.\nYet, in highly dynamic environments, like crowded city streets, problems arise\nas major parts of the image can be covered by dynamic objects. Consequently,\nvisual odometry pipelines often diverge and the localization systems\nmalfunction as detected features are not consistent with the precomputed 3D\nmodel. In this work, we present an approach to automatically detect dynamic\nobject instances to improve the robustness of vision-based localization and\nmapping in crowded environments. By training a convolutional neural network\nmodel with a combination of synthetic and real-world data, dynamic object\ninstance masks are learned in a semi-supervised way. The real-world data can be\ncollected with a standard camera and requires minimal further post-processing.\nOur experiments show that a wide range of dynamic objects can be reliably\ndetected using the presented method. Promising performance is demonstrated on\nour own and also publicly available datasets, which also shows the\ngeneralization capabilities of this approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:17:59 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zhou", "Guoxiang", ""], ["Bescos", "Berta", ""], ["Dymczyk", "Marcin", ""], ["Pfeiffer", "Mark", ""], ["Neira", "Jos\u00e9", ""], ["Siegwart", "Roland", ""]]}, {"id": "1807.03018", "submitter": "Milad Niknejad", "authors": "Milad Niknejad, Jose M. Bioucas-Dias, Mario A.T. Figueiredo", "title": "External Patch-Based Image Restoration Using Importance Sampling", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2912122", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to patch-based image restoration based\non external datasets and importance sampling. The Minimum Mean Squared Error\n(MMSE) estimate of the image patches, the computation of which requires solving\na multidimensional (typically intractable) integral, is approximated using\nsamples from an external dataset. The new method, which can be interpreted as a\ngeneralization of the external non-local means (NLM), uses self-normalized\nimportance sampling to efficiently approximate the MMSE estimates. The use of\nself-normalized importance sampling endows the proposed method with great\nflexibility, namely regarding the statistical properties of the measurement\nnoise. The effectiveness of the proposed method is shown in a series of\nexperiments using both generic large-scale and class-specific external\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:52:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Niknejad", "Milad", ""], ["Bioucas-Dias", "Jose M.", ""], ["Figueiredo", "Mario A. T.", ""]]}, {"id": "1807.03021", "submitter": "Fangneng Zhan", "authors": "Fangneng Zhan, Shijian Lu, Chuhui Xue", "title": "Verisimilar Image Synthesis for Accurate Detection and Recognition of\n  Texts in Scenes", "comments": "14 pages, ECCV2018, datasets:\n  https://github.com/fnzhan/Verisimilar-Image-Synthesis-for-Accurate-Detection-and-Recognition-of-Texts-in-Scenes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The requirement of large amounts of annotated images has become one grand\nchallenge while training deep neural network models for various visual\ndetection and recognition tasks. This paper presents a novel image synthesis\ntechnique that aims to generate a large amount of annotated scene text images\nfor training accurate and robust scene text detection and recognition models.\nThe proposed technique consists of three innovative designs. First, it realizes\n\"semantic coherent\" synthesis by embedding texts at semantically sensible\nregions within the background image, where the semantic coherence is achieved\nby leveraging the semantic annotations of objects and image regions that have\nbeen created in the prior semantic segmentation research. Second, it exploits\nvisual saliency to determine the embedding locations within each semantic\nsensible region, which coincides with the fact that texts are often placed\naround homogeneous regions for better visibility in scenes. Third, it designs\nan adaptive text appearance model that determines the color and brightness of\nembedded texts by learning from the feature of real scene text images\nadaptively. The proposed technique has been evaluated over five public datasets\nand the experiments show its superior performance in training accurate and\nrobust scene text detection and recognition models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:58:06 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 07:55:02 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Zhan", "Fangneng", ""], ["Lu", "Shijian", ""], ["Xue", "Chuhui", ""]]}, {"id": "1807.03026", "submitter": "Ari Heljakka", "authors": "Ari Heljakka, Arno Solin, Juho Kannala", "title": "Pioneer Networks: Progressively Growing Generative Autoencoder", "comments": "To appear in ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel generative autoencoder network model that learns to\nencode and reconstruct images with high quality and resolution, and supports\nsmooth random sampling from the latent space of the encoder. Generative\nadversarial networks (GANs) are known for their ability to simulate random\nhigh-quality images, but they cannot reconstruct existing images. Previous\nworks have attempted to extend GANs to support such inference but, so far, have\nnot delivered satisfactory high-quality results. Instead, we propose the\nProgressively Growing Generative Autoencoder (PIONEER) network which achieves\nhigh-quality reconstruction with $128{\\times}128$ images without requiring a\nGAN discriminator. We merge recent techniques for progressively building up the\nparts of the network with the recently introduced adversarial encoder-generator\nnetwork. The ability to reconstruct input images is crucial in many real-world\napplications, and allows for precise intelligent manipulation of existing\nimages. We show promising results in image synthesis and inference, with\nstate-of-the-art results in CelebA inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 10:19:51 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 15:26:41 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Heljakka", "Ari", ""], ["Solin", "Arno", ""], ["Kannala", "Juho", ""]]}, {"id": "1807.03027", "submitter": "Milad Niknejad", "authors": "Milad Niknejad, Jose M. Bioucas-Dias, Mario A.T. Figueiredo", "title": "Image Restoration Using Conditional Random Fields and Scale Mixtures of\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework for internal patch-based image\nrestoration based on Conditional Random Fields (CRF). Unlike related models\nbased on Markov Random Fields (MRF), our approach explicitly formulates the\nposterior distribution for the entire image. The potential functions are taken\nas proportional to the product of a likelihood and prior for each patch. By\nassuming identical parameters for similar patches, our approach can be\nclassified as a model-based non-local method. For the prior term in the\npotential function of the CRF model, multivariate Gaussians and multivariate\nscale-mixture of Gaussians are considered, with the latter being a novel prior\nfor image patches. Our results show that the proposed approach outperforms\nmethods based on Gaussian mixture models for image denoising and\nstate-of-the-art methods for image interpolation/inpainting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 10:24:54 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Niknejad", "Milad", ""], ["Bioucas-Dias", "Jose M.", ""], ["Figueiredo", "Mario A. T.", ""]]}, {"id": "1807.03043", "submitter": "Kezhi Li", "authors": "Kezhi Li, John Daniels, Chengyuan Liu, Pau Herrero, Pantelis Georgiou", "title": "Convolutional Recurrent Neural Networks for Glucose Prediction", "comments": "10 pages, 7 figures", "journal-ref": "IEEE journal of biomedical and health informatics 2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control of blood glucose is essential for diabetes management. Current\ndigital therapeutic approaches for subjects with Type 1 diabetes mellitus\n(T1DM) such as the artificial pancreas and insulin bolus calculators leverage\nmachine learning techniques for predicting subcutaneous glucose for improved\ncontrol. Deep learning has recently been applied in healthcare and medical\nresearch to achieve state-of-the-art results in a range of tasks including\ndisease diagnosis, and patient state prediction among others. In this work, we\npresent a deep learning model that is capable of forecasting glucose levels\nwith leading accuracy for simulated patient cases (RMSE = 9.38$\\pm$0.71 [mg/dL]\nover a 30-minute horizon, RMSE = 18.87$\\pm$2.25 [mg/dL] over a 60-minute\nhorizon) and real patient cases (RMSE = 21.07$\\pm$2.35 [mg/dL] for 30-minute,\nRMSE = 33.27$\\pm$4.79\\% for 60-minute). In addition, the model provides\ncompetitive performance in providing effective prediction horizon ($PH_{eff}$)\nwith minimal time lag both in a simulated patient dataset ($PH_{eff}$ =\n29.0$\\pm$0.7 for 30-min and $PH_{eff}$ = 49.8$\\pm$2.9 for 60-min) and in a real\npatient dataset ($PH_{eff}$ = 19.3$\\pm$3.1 for 30-min and $PH_{eff}$ =\n29.3$\\pm$9.4 for 60-min). This approach is evaluated on a dataset of 10\nsimulated cases generated from the UVa/Padova simulator and a clinical dataset\nof 10 real cases each containing glucose readings, insulin bolus, and meal\n(carbohydrate) data. Performance of the recurrent convolutional neural network\nis benchmarked against four algorithms. The proposed algorithm is implemented\non an Android mobile phone, with an execution time of $6$ms on a phone compared\nto an execution time of $780$ms on a laptop.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 11:12:16 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 19:37:31 GMT"}, {"version": "v3", "created": "Mon, 6 Aug 2018 18:05:54 GMT"}, {"version": "v4", "created": "Thu, 16 Aug 2018 15:02:02 GMT"}, {"version": "v5", "created": "Mon, 25 Feb 2019 21:06:08 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Li", "Kezhi", ""], ["Daniels", "John", ""], ["Liu", "Chengyuan", ""], ["Herrero", "Pau", ""], ["Georgiou", "Pantelis", ""]]}, {"id": "1807.03057", "submitter": "Christopher Syben", "authors": "Christopher Syben, Bernhard Stimpel, Jonathan Lommen, Tobias W\\\"urfl,\n  Arnd D\\\"orfler and Andreas Maier", "title": "Deriving Neural Network Architectures using Precision Learning:\n  Parallel-to-fan beam Conversion", "comments": "Inproceedings GCPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a neural network architecture based on an analytical\nformulation of the parallel-to-fan beam conversion problem following the\nconcept of precision learning. The network allows to learn the unknown\noperators in this conversion in a data-driven manner avoiding interpolation and\npotential loss of resolution. Integration of known operators results in a small\nnumber of trainable parameters that can be estimated from synthetic data only.\nThe concept is evaluated in the context of Hybrid MRI/X-ray imaging where\ntransformation of the parallel-beam MRI projections to fan-beam X-ray\nprojections is required. The proposed method is compared to a traditional\nrebinning method. The results demonstrate that the proposed method is superior\nto ray-by-ray interpolation and is able to deliver sharper images using the\nsame amount of parallel-beam input projections which is crucial for\ninterventional applications. We believe that this approach forms a basis for\nfurther work uniting deep learning, signal processing, physics, and traditional\npattern recognition.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 11:43:07 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 12:37:46 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Syben", "Christopher", ""], ["Stimpel", "Bernhard", ""], ["Lommen", "Jonathan", ""], ["W\u00fcrfl", "Tobias", ""], ["D\u00f6rfler", "Arnd", ""], ["Maier", "Andreas", ""]]}, {"id": "1807.03058", "submitter": "Yong Xia", "authors": "Hongyu Wang, Yong Xia", "title": "ChestNet: A Deep Neural Network for Classification of Thoracic Diseases\n  on Chest Radiography", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided techniques may lead to more accurate and more acces-sible\ndiagnosis of thorax diseases on chest radiography. Despite the success of deep\nlearning-based solutions, this task remains a major challenge in smart\nhealthcare, since it is intrinsically a weakly supervised learning problem. In\nthis paper, we incorporate the attention mechanism into a deep convolutional\nneural network, and thus propose the ChestNet model to address effective\ndiagnosis of thorax diseases on chest radiography. This model consists of two\nbranches: a classification branch serves as a uniform feature\nextraction-classification network to free users from troublesome handcrafted\nfeature extraction, and an attention branch exploits the correlation between\nclass labels and the locations of patholog-ical abnormalities and allows the\nmodel to concentrate adaptively on the patholog-ically abnormal regions. We\nevaluated our model against three state-of-the-art deep learning models on the\nChest X-ray 14 dataset using the official patient-wise split. The results\nindicate that our model outperforms other methods, which use no extra training\ndata, in diagnosing 14 thorax diseases on chest radiography.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 11:48:42 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Wang", "Hongyu", ""], ["Xia", "Yong", ""]]}, {"id": "1807.03089", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Tao Xiang, Andrea Cavallaro", "title": "Video Summarisation by Classification with Deep Reinforcement Learning", "comments": "In Proc. of BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing video summarisation methods are based on either supervised or\nunsupervised learning. In this paper, we propose a reinforcement learning-based\nweakly supervised method that exploits easy-to-obtain, video-level category\nlabels and encourages summaries to contain category-related information and\nmaintain category recognisability. Specifically, We formulate video\nsummarisation as a sequential decision-making process and train a summarisation\nnetwork with deep Q-learning (DQSN). A companion classification network is also\ntrained to provide rewards for training the DQSN. With the classification\nnetwork, we develop a global recognisability reward based on the classification\nresult. Critically, a novel dense ranking-based reward is also proposed in\norder to cope with the temporally delayed and sparse reward problems for long\nsequence reinforcement learning. Extensive experiments on two benchmark\ndatasets show that the proposed approach achieves state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:05:36 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 09:40:36 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 23:56:18 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Xiang", "Tao", ""], ["Cavallaro", "Andrea", ""]]}, {"id": "1807.03094", "submitter": "Di Hu", "authors": "Di Hu, Feiping Nie, Xuelong Li", "title": "Deep Multimodal Clustering for Unsupervised Audiovisual Learning", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seen birds twitter, the running cars accompany with noise, etc. These\nnaturally audiovisual correspondences provide the possibilities to explore and\nunderstand the outside world. However, the mixed multiple objects and sounds\nmake it intractable to perform efficient matching in the unconstrained\nenvironment. To settle this problem, we propose to adequately excavate audio\nand visual components and perform elaborate correspondence learning among them.\nConcretely, a novel unsupervised audiovisual learning model is proposed, named\nas \\Deep Multimodal Clustering (DMC), that synchronously performs sets of\nclustering with multimodal vectors of convolutional maps in different shared\nspaces for capturing multiple audiovisual correspondences. And such integrated\nmultimodal clustering network can be effectively trained with max-margin loss\nin the end-to-end fashion. Amounts of experiments in feature evaluation and\naudiovisual tasks are performed. The results demonstrate that DMC can learn\neffective unimodal representation, with which the classifier can even\noutperform human performance. Further, DMC shows noticeable performance in\nsound localization, multisource detection, and audiovisual understanding.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:13:10 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 13:16:52 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 07:36:17 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hu", "Di", ""], ["Nie", "Feiping", ""], ["Li", "Xuelong", ""]]}, {"id": "1807.03095", "submitter": "Ulzee An", "authors": "Ulzee An, Khader Shameer, Lakshmi Subramanian", "title": "Mammography Assessment using Multi-Scale Deep Classifiers", "comments": "Prepared for MLMH at KDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying deep learning methods to mammography assessment has remained a\nchallenging topic. Dense noise with sparse expressions, mega-pixel raw data\nresolution, lack of diverse examples have all been factors affecting\nperformance. The lack of pixel-level ground truths have especially limited\nsegmentation methods in pushing beyond approximately bounding regions. We\npropose a classification approach grounded in high performance tissue\nassessment as an alternative to all-in-one localization and assessment models\nthat is also capable of pinpointing the causal pixels. First, the objective of\nthe mammography assessment task is formalized in the context of local tissue\nclassifiers. Then, the accuracy of a convolutional neural net is evaluated on\nclassifying patches of tissue with suspicious findings at varying scales, where\nhighest obtained AUC is above $0.9$. The local evaluations of one such expert\ntissue classifier is used to augment the results of a heatmap regression model\nand additionally recover the exact causal regions at high resolution as a\nsaliency image suitable for clinical settings.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 01:26:51 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["An", "Ulzee", ""], ["Shameer", "Khader", ""], ["Subramanian", "Lakshmi", ""]]}, {"id": "1807.03116", "submitter": "Zhi Chen", "authors": "Zhi Chen, Pin-han Ho", "title": "Deep Global-Connected Net With The Generalized Multi-Piecewise ReLU\n  Activation in Deep Learning", "comments": "9 pages, 3 figures and 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent Progress has shown that exploitation of hidden layer neurons in\nconvolution neural networks incorporating with a carefully designed activation\nfunction can yield better classification results in the field of computer\nvision. The paper firstly introduces a novel deep learning architecture aiming\nto mitigate the gradient-vanishing problem, in which the earlier hidden layer\nneurons could be directly connected with the last hidden layer and feed into\nthe last layer for classification. We then design a generalized linear\nrectifier function as the activation function that can approximate arbitrary\ncomplex functions via training of the parameters. We will show that our design\ncan achieve similar performance in a number of object recognition and video\naction benchmark tasks, under significantly less number of parameters and\nshallower network infrastructure, which is not only promising in training in\nterms of computation burden and memory usage, but is also applicable to\nlow-computation, low-memory mobile scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 22:53:48 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chen", "Zhi", ""], ["Ho", "Pin-han", ""]]}, {"id": "1807.03117", "submitter": "Miguel Martin-Abadal", "authors": "Miguel Martin-Abadal, Eric Guerrero-Font, Francisco Bonin-Font and\n  Yolanda Gonzalez-Cid", "title": "Deep Semantic Segmentation in an AUV for Online Posidonia Oceanica\n  Meadows identification", "comments": "11 pages, 16 figures", "journal-ref": null, "doi": "10.1109/ACCESS.2018.2875412", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown evidence of a significant decline of the Posidonia\noceanica (P.O.) meadows on a global scale. The monitoring and mapping of these\nmeadows are fundamental tools for measuring their status. We present an\napproach based on a deep neural network to automatically perform a\nhigh-precision semantic segmentation of P.O. meadows in sea-floor images,\noffering several improvements over the state of the art techniques. Our network\ndemonstrates outstanding performance over diverse test sets, reaching a\nprecision of 96.57% and an accuracy of 96.81%, surpassing the reliability of\nlabelling the images manually. Also, the network is implemented in an\nAutonomous Underwater Vehicle (AUV), performing an online P.O. segmentation,\nwhich will be used to generate real-time semantic coverage maps.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 22:21:51 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 19:46:42 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Martin-Abadal", "Miguel", ""], ["Guerrero-Font", "Eric", ""], ["Bonin-Font", "Francisco", ""], ["Gonzalez-Cid", "Yolanda", ""]]}, {"id": "1807.03119", "submitter": "Nicholas Tan Jerome", "authors": "N. Tan Jerome, Z. Ateyev, S. Schmelzle, S. Chilingaryan, A. Kopmann", "title": "Real-time Local Noise Filter in 3D Visualization of CT Data", "comments": "5 pages, 5 figures, 21st IEEE Real Time Conference", "journal-ref": null, "doi": "10.1109/TNS.2019.2893444", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Removing noise in computer tomography (CT) data for real-time 3D\nvisualization is vital to improving the quality of the final display. However,\nthe CT noise cannot be removed by straight averaging because the noise has a\nbroadband spatial frequency that is overlapping with the interesting signal\nfrequencies. To improve the display of structures and features contained in the\ndata, we present spatially variant filtering that performs averaging of\nsub-regions around a central region. We compare our filter with four other\nsimilar spatially variant filters regarding entropy and processing time. The\nresults demonstrate significant improvement of the visual quality with\nprocessing time still within the millisecond range.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 10:27:04 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Jerome", "N. Tan", ""], ["Ateyev", "Z.", ""], ["Schmelzle", "S.", ""], ["Chilingaryan", "S.", ""], ["Kopmann", "A.", ""]]}, {"id": "1807.03120", "submitter": "Mrinal Haloi", "authors": "Mrinal Haloi, K. Raja Rajalakshmi, Pradeep Walia", "title": "Towards Radiologist-Level Accurate Deep Learning System for Pulmonary\n  Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose advanced pneumonia and Tuberculosis grading system\nfor X-ray images. The proposed system is a very deep fully convolutional\nclassification network with online augmentation that outputs confidence values\nfor diseases prevalence. Its a fully automated system capable of disease\nfeature understanding without any offline preprocessing step or manual feature\nextraction. We have achieved state- of-the- art performance on the public\ndatabases such as ChestXray-14, Mendeley, Shenzhen Hospital X-ray and Belarus\nX-ray set.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 07:07:25 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Haloi", "Mrinal", ""], ["Rajalakshmi", "K. Raja", ""], ["Walia", "Pradeep", ""]]}, {"id": "1807.03122", "submitter": "Taro Langner", "authors": "Taro Langner, Anders Hedstr\\\"om, Katharina M\\\"orwald, Daniel Weghuber,\n  Anders Forslund, Peter Bergsten, H{\\aa}kan Ahlstr\\\"om, Joel Kullberg", "title": "Fully Convolutional Networks for Automated Segmentation of Abdominal\n  Adipose Tissue Depots in Multicenter Water-Fat MRI", "comments": "Key words: deep learning, fully convolutional networks, segmentation,\n  water-fat MRI, adipose tissue, abdominal", "journal-ref": "Magn Reson Med. 2018;00:1-10", "doi": "10.1002/mrm.27550", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Purpose: An approach for the automated segmentation of visceral adipose\ntissue (VAT) and subcutaneous adipose tissue (SAT) in multicenter water-fat MRI\nscans of the abdomen was investigated, using two different neural network\narchitectures.\n  Methods: The two fully convolutional network architectures U-Net and V-Net\nwere trained, evaluated and compared on the water-fat MRI data. Data of the\nstudy Tellus with 90 scans from a single center was used for a 10-fold\ncross-validation in which the most successful configuration for both networks\nwas determined. These configurations were then tested on 20 scans of the\nmulticenter study beta-cell function in JUvenile Diabetes and Obesity\n(BetaJudo), which involved a different study population and scanning device.\n  Results: The U-Net outperformed the used implementation of the V-Net in both\ncross-validation and testing. In cross-validation, the U-Net reached average\ndice scores of 0.988 (VAT) and 0.992 (SAT). The average of the absolute\nquantification errors amount to 0.67% (VAT) and 0.39% (SAT). On the\nmulti-center test data, the U-Net performs only slightly worse, with average\ndice scores of 0.970 (VAT) and 0.987 (SAT) and quantification errors of 2.80%\n(VAT) and 1.65% (SAT).\n  Conclusion: The segmentations generated by the U-Net allow for reliable\nquantification and could therefore be viable for high-quality automated\nmeasurements of VAT and SAT in large-scale studies with minimal need for human\nintervention. The high performance on the multicenter test data furthermore\nshows the robustness of this approach for data of different patient\ndemographics and imaging centers, as long as a consistent imaging protocol is\nused.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:28:21 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 12:41:54 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 09:51:57 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 12:08:30 GMT"}, {"version": "v5", "created": "Thu, 1 Nov 2018 10:09:09 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Langner", "Taro", ""], ["Hedstr\u00f6m", "Anders", ""], ["M\u00f6rwald", "Katharina", ""], ["Weghuber", "Daniel", ""], ["Forslund", "Anders", ""], ["Bergsten", "Peter", ""], ["Ahlstr\u00f6m", "H\u00e5kan", ""], ["Kullberg", "Joel", ""]]}, {"id": "1807.03123", "submitter": "Michaela Blott", "authors": "Michaela Blott, Thomas B. Preusser, Nicholas Fraser, Giulio\n  Gambardella, Kenneth OBrien, Yaman Umuroglu and Miriam Leeser", "title": "Scaling Neural Network Performance through Customized Hardware\n  Architectures on Reconfigurable Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks have dramatically improved in recent years,\nsurpassing human accuracy on certain problems and performance exceeding that of\ntraditional computer vision algorithms. While the compute pattern in itself is\nrelatively simple, significant compute and memory challenges remain as CNNs may\ncontain millions of floating-point parameters and require billions of\nfloating-point operations to process a single image. These computational\nrequirements, combined with storage footprints that exceed typical cache sizes,\npose a significant performance and power challenge for modern compute\narchitectures. One of the promising opportunities to scale performance and\npower efficiency is leveraging reduced precision representations for all\nactivations and weights as this allows to scale compute capabilities, reduce\nweight and feature map buffering requirements as well as energy consumption.\nWhile a small reduction in accuracy is encountered, these Quantized Neural\nNetworks have been shown to achieve state-of-the-art accuracy on standard\nbenchmark datasets, such as MNIST, CIFAR-10, SVHN and even ImageNet, and thus\nprovide highly attractive design trade-offs. Current research has focused\nmainly on the implementation of extreme variants with full binarization of\nweights and or activations, as well typically smaller input images. Within this\npaper, we investigate the scalability of dataflow architectures with respect to\nsupporting various precisions for both weights and activations, larger image\ndimensions, and increasing numbers of feature map channels. Key contributions\nare a formalized approach to understanding the scalability of the existing\nhardware architecture with cost models and a performance prediction as a\nfunction of the target device size. We provide validating experimental results\nfor an ImageNet classification on a server-class platform, namely the AWS F1\nnode.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:39:44 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Blott", "Michaela", ""], ["Preusser", "Thomas B.", ""], ["Fraser", "Nicholas", ""], ["Gambardella", "Giulio", ""], ["OBrien", "Kenneth", ""], ["Umuroglu", "Yaman", ""], ["Leeser", "Miriam", ""]]}, {"id": "1807.03124", "submitter": "Inkyu Sa", "authors": "Ho Seok Ahn, Feras Dayoub, Marija Popovic, Bruce MacDonald, Roland\n  Siegwart, Inkyu Sa", "title": "An Overview of Perception Methods for Horticultural Robots: From\n  Pollination to Harvest", "comments": "6 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Horticultural enterprises are becoming more sophisticated as the range of the\ncrops they target expands. Requirements for enhanced efficiency and\nproductivity have driven the demand for automating on-field operations.\nHowever, various problems remain yet to be solved for their reliable, safe\ndeployment in real-world scenarios. This paper examines major research trends\nand current challenges in horticultural robotics. Specifically, our work\nfocuses on sensing and perception in the three main horticultural procedures:\npollination, yield estimation, and harvesting. For each task, we expose major\nissues arising from the unstructured, cluttered, and rugged nature of field\nenvironments, including variable lighting conditions and difficulties in\nfruit-specific detection, and highlight promising contemporary studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 07:44:48 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Ahn", "Ho Seok", ""], ["Dayoub", "Feras", ""], ["Popovic", "Marija", ""], ["MacDonald", "Bruce", ""], ["Siegwart", "Roland", ""], ["Sa", "Inkyu", ""]]}, {"id": "1807.03125", "submitter": "Moneish Kumar", "authors": "Kranthi Kumar, Moneish Kumar, Vineet Gandhi, Ramanathan Subramanian", "title": "Watch to Edit: Video Retargeting using Gaze", "comments": null, "journal-ref": "Computer Graphics Forum, Volume37, Issue2(2018)205-215", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to optimally retarget videos for varied displays\nwith differing aspect ratios by preserving salient scene content discovered via\neye tracking. Our algorithm performs editing with cut, pan and zoom operations\nby optimizing the path of a cropping window within the original video while\nseeking to (i) preserve salient regions, and (ii) adhere to the principles of\ncinematography. Our approach is (a) content agnostic as the same methodology is\nemployed to re-edit a wide-angle video recording or a close-up movie sequence\ncaptured with a static or moving camera, and (b) independent of video length\nand can in principle re-edit an entire movie in one shot. Our algorithm\nconsists of two steps. The first step employs gaze transition cues to detect\ntime stamps where new cuts are to be introduced in the original video via\ndynamic programming. A subsequent step optimizes the cropping window path (to\ncreate pan and zoom effects), while accounting for the original and new cuts.\nThe cropping window path is designed to include maximum gaze information, and\nis composed of piecewise constant, linear and parabolic segments. It is\nobtained via L(1) regularized convex optimization which ensures a smooth\nviewing experience. We test our approach on a wide variety of videos and\ndemonstrate significant improvement over the state-of-the-art, both in terms of\ncomputational complexity and qualitative aspects. A study performed with 16\nusers confirms that our approach results in a superior viewing experience as\ncompared to gaze driven re-editing and letterboxing methods, especially for\nwide-angle static camera recordings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 16:21:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kumar", "Kranthi", ""], ["Kumar", "Moneish", ""], ["Gandhi", "Vineet", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1807.03126", "submitter": "V\\'it R\\r{u}\\v{z}i\\v{c}ka", "authors": "V\\'it R\\r{u}\\v{z}i\\v{c}ka", "title": "Estimating Bicycle Route Attractivity from Image Data", "comments": "86 pages. (Master's thesis, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This master thesis focuses on practical application of Convolutional Neural\nNetwork models on the task of road labeling with bike attractivity score. We\nstart with an abstraction of real world locations into nodes and scored edges\nin partially annotated dataset. We enhance information available about each\nedge with photographic data from Google Street View service and with additional\nneighborhood information from Open Street Map database. We teach a model on\nthis enhanced dataset and experiment with ImageNet Large Scale Visual\nRecognition Competition. We try different dataset enhancing techniques as well\nas various model architectures to improve road scoring. We also make use of\ntransfer learning to use features from a task with rich dataset of ImageNet\ninto our task with smaller number of images, to prevent model overfitting.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:29:05 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["R\u016f\u017ei\u010dka", "V\u00edt", ""]]}, {"id": "1807.03128", "submitter": "Diederik Paul Moeys", "authors": "Diederik Paul Moeys, Daniel Neil, Federico Corradi, Emmett Kerr,\n  Philip Vance, Gautham Das, Sonya A. Coleman, Thomas M. McGinnity, Dermot\n  Kerr, Tobi Delbruck", "title": "PRED18: Dataset and Further Experiments with DAVIS Event Camera in\n  Predator-Prey Robot Chasing", "comments": "8 pages", "journal-ref": "IEEE EBCCSP 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine vision systems using convolutional neural networks (CNNs) for robotic\napplications are increasingly being developed. Conventional vision CNNs are\ndriven by camera frames at constant sample rate, thus achieving a fixed latency\nand power consumption tradeoff. This paper describes further work on the first\nexperiments of a closed-loop robotic system integrating a CNN together with a\nDynamic and Active Pixel Vision Sensor (DAVIS) in a predator/prey scenario. The\nDAVIS, mounted on the predator Summit XL robot, produces frames at a fixed 15\nHz frame-rate and Dynamic Vision Sensor (DVS) histograms containing 5k ON and\nOFF events at a variable frame-rate ranging from 15-500 Hz depending on the\nrobot speeds. In contrast to conventional frame-based systems, the latency and\nprocessing cost depends on the rate of change of the image. The CNN is trained\noffline on the 1.25h labeled dataset to recognize the position and size of the\nprey robot, in the field of view of the predator. During inference, combining\nthe ten output classes of the CNN allows extracting the analog position vector\nof the prey relative to the predator with a mean 8.7% error in angular\nestimation. The system is compatible with conventional deep learning\ntechnology, but achieves a variable latency-power tradeoff that adapts\nautomatically to the dynamics. Finally, investigations on the robustness of the\nalgorithm, a human performance comparison and a deconvolution analysis are also\nexplored.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 18:07:18 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Moeys", "Diederik Paul", ""], ["Neil", "Daniel", ""], ["Corradi", "Federico", ""], ["Kerr", "Emmett", ""], ["Vance", "Philip", ""], ["Das", "Gautham", ""], ["Coleman", "Sonya A.", ""], ["McGinnity", "Thomas M.", ""], ["Kerr", "Dermot", ""], ["Delbruck", "Tobi", ""]]}, {"id": "1807.03130", "submitter": "Dov Danon", "authors": "Dov Danon, Hadar Averbuch-Elor, Ohad Fried, Daniel Cohen-Or", "title": "Unsupervised Natural Image Patch Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a metric of natural image patches is an important tool for analyzing\nimages. An efficient means is to train a deep network to map an image patch to\na vector space, in which the Euclidean distance reflects patch similarity.\nPrevious attempts learned such an embedding in a supervised manner, requiring\nthe availability of many annotated images. In this paper, we present an\nunsupervised embedding of natural image patches, avoiding the need for\nannotated images. The key idea is that the similarity of two patches can be\nlearned from the prevalence of their spatial proximity in natural images.\nClearly, relying on this simple principle, many spatially nearby pairs are\noutliers, however, as we show, the outliers do not harm the convergence of the\nmetric learning. We show that our unsupervised embedding approach is more\neffective than a supervised one or one that uses deep patch representations.\nMoreover, we show that it naturally leads itself to an efficient\nself-supervised domain adaptation technique onto a target domain that contains\na common foreground object.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 18:21:43 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Danon", "Dov", ""], ["Averbuch-Elor", "Hadar", ""], ["Fried", "Ohad", ""], ["Cohen-Or", "Daniel", ""]]}, {"id": "1807.03132", "submitter": "Zhiyan Cui", "authors": "Zhiyan Cui, Na Lu", "title": "Fast Dynamic Convolutional Neural Networks for Visual Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing tracking methods based on CNN(convolutional neural\nnetworks) are too slow for real-time application despite the excellent tracking\nprecision compared with the traditional ones. In this paper, a fast dynamic\nvisual tracking algorithm combining CNN based MDNet(Multi-Domain Network) and\nRoIAlign was developed. The major problem of MDNet also lies in the time\nefficiency. Considering the computational complexity of MDNet is mainly caused\nby the large amount of convolution operations and fine-tuning of the network\nduring tracking, a RoIPool layer which could conduct the convolution over the\nwhole image instead of each RoI is added to accelerate the convolution and a\nnew strategy of fine-tuning the fully-connected layers is used to accelerate\nthe update. With RoIPool employed, the computation speed has been increased but\nthe tracking precision has dropped simultaneously. RoIPool could lose some\npositioning precision because it can not handle locations represented by\nfloating numbers. So RoIAlign, instead of RoIPool, which can process floating\nnumbers of locations by bilinear interpolation has been added to the network.\nThe results show the target localization precision has been improved and it\nhardly increases the computational cost. These strategies can accelerate the\nprocessing and make it 7x faster than MDNet with very low impact on precision\nand it can run at around 7 fps. The proposed algorithm has been evaluated on\ntwo benchmarks: OTB100 and VOT2016, on which high precision and speed have been\nobtained. The influence of the network structure and training data are also\ndiscussed with experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 06:30:18 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 08:06:56 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Cui", "Zhiyan", ""], ["Lu", "Na", ""]]}, {"id": "1807.03133", "submitter": "Ryosuke Goto", "authors": "Takuma Nakamura and Ryosuke Goto", "title": "Outfit Generation and Style Extraction via Bidirectional LSTM and\n  Autoencoder", "comments": "9 pages, 5 figures, KDD Workshop AI for fashion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When creating an outfit, style is a criterion in selecting each fashion item.\nThis means that style can be regarded as a feature of the overall outfit.\nHowever, in various previous studies on outfit generation, there have been few\nmethods focusing on global information obtained from an outfit. To address this\ndeficiency, we have incorporated an unsupervised style extraction module into a\nmodel to learn outfits. Using the style information of an outfit as a whole,\nthe proposed model succeeded in generating outfits more flexibly without\nrequiring additional information. Moreover, the style information extracted by\nthe proposed model is easy to interpret. The proposed model was evaluated on\ntwo human-generated outfit datasets. In a fashion item prediction task (missing\nprediction task), the proposed model outperformed a baseline method. In a style\nextraction task, the proposed model extracted some easily distinguishable\nstyles. In an outfit generation task, the proposed model generated an outfit\nwhile controlling its styles. This capability allows us to generate fashionable\noutfits according to various preferences.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 18:00:03 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 04:24:35 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 10:28:23 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Nakamura", "Takuma", ""], ["Goto", "Ryosuke", ""]]}, {"id": "1807.03135", "submitter": "Mohammad Tofighi", "authors": "Mohammad Tofighi, Tiantong Guo, Jairam K.P. Vanamala, and Vishal Monga", "title": "Deep Networks with Shape Priors for Nucleus Detection", "comments": "Accepted paper to 2018 IEEE International Conference on Image\n  Processing (ICIP 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of cell nuclei in microscopic images is a challenging research\ntopic, because of limitations in cellular image quality and diversity of\nnuclear morphology, i.e. varying nuclei shapes, sizes, and overlaps between\nmultiple cell nuclei. This has been a topic of enduring interest with promising\nrecent success shown by deep learning methods. These methods train for example\nconvolutional neural networks (CNNs) with a training set of input images and\nknown, labeled nuclei locations. Many of these methods are supplemented by\nspatial or morphological processing. We develop a new approach that we call\nShape Priors with Convolutional Neural Networks (SP-CNN) to perform\nsignificantly enhanced nuclei detection. A set of canonical shapes is prepared\nwith the help of a domain expert. Subsequently, we present a new network\nstructure that can incorporate `expected behavior' of nucleus shapes via two\ncomponents: {\\em learnable} layers that perform the nucleus detection and a\n{\\em fixed} processing part that guides the learning with prior information.\nAnalytically, we formulate a new regularization term that is targeted at\npenalizing false positives while simultaneously encouraging detection inside\ncell nucleus boundary. Experimental results on a challenging dataset reveal\nthat SP-CNN is competitive with or outperforms several state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:54:41 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Tofighi", "Mohammad", ""], ["Guo", "Tiantong", ""], ["Vanamala", "Jairam K. P.", ""], ["Monga", "Vishal", ""]]}, {"id": "1807.03136", "submitter": "Shiwan Zhao Mr", "authors": "Bingzhe Wu and Xiaolu Zhang and Shiwan Zhao and Lingxi Xie and Caihong\n  Zeng and Zhihong Liu and Guangyu Sun", "title": "G2C: A Generator-to-Classifier Framework Integrating Multi-Stained\n  Visual Cues for Pathological Glomerulus Classification", "comments": "Accepted by AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathological glomerulus classification plays a key role in the diagnosis of\nnephropathy. As the difference between different subcategories is subtle,\ndoctors often refer to slides from different staining methods to make\ndecisions. However, creating correspondence across various stains is\nlabor-intensive, bringing major difficulties in collecting data and training a\nvision-based algorithm to assist nephropathy diagnosis. This paper provides an\nalternative solution for integrating multi-stained visual cues for glomerulus\nclassification. Our approach, named generator-to-classifier (G2C), is a\ntwo-stage framework. Given an input image from a specified stain, several\ngenerators are first applied to estimate its appearances in other staining\nmethods, and a classifier follows to combine visual cues from different stains\nfor prediction (whether it is pathological, or which type of pathology it has).\nWe optimize these two stages in a joint manner. To provide a reasonable\ninitialization, we pre-train the generators in an unlabeled reference set under\nan unpaired image-to-image translation task, and then fine-tune them together\nwith the classifier. We conduct experiments on a glomerulus type classification\ndataset collected by ourselves (there are no publicly available datasets for\nthis purpose). Although joint optimization slightly harms the authenticity of\nthe generated patches, it boosts classification performance, suggesting more\neffective visual cues are extracted in an automatic way. We also transfer our\nmodel to a public dataset for breast cancer classification, and outperform the\nstate-of-the-arts significantly.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 01:09:32 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 02:59:42 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 07:06:53 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Wu", "Bingzhe", ""], ["Zhang", "Xiaolu", ""], ["Zhao", "Shiwan", ""], ["Xie", "Lingxi", ""], ["Zeng", "Caihong", ""], ["Liu", "Zhihong", ""], ["Sun", "Guangyu", ""]]}, {"id": "1807.03138", "submitter": "Will Nash", "authors": "Will Nash, Tom Drummond and Nick Birbilis", "title": "Quantity beats quality for semantic segmentation of corrosion in images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataset creation is typically one of the first steps when applying Artificial\nIntelligence methods to a new task; and the real world performance of models\nhinges on the quality and quantity of data available. Producing an image\ndataset for semantic segmentation is resource intensive, particularly for\nspecialist subjects where class segmentation is not able to be effectively\nfarmed out. The benefit of producing a large, but poorly labelled, dataset\nversus a small, expertly segmented dataset for semantic segmentation is an open\nquestion. Here we show that a large, noisy dataset outperforms a small,\nexpertly segmented dataset for training a Fully Convolutional Network model for\nsemantic segmentation of corrosion in images. A large dataset of 250 images\nwith segmentations labelled by undergraduates and a second dataset of just 10\nimages, with segmentations labelled by subject matter experts were produced.\nThe mean Intersection over Union and micro F-score metrics were compared after\ntraining for 50,000 epochs. This work is illustrative for researchers setting\nout to develop deep learning models for detection and location of specialist\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 04:53:57 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nash", "Will", ""], ["Drummond", "Tom", ""], ["Birbilis", "Nick", ""]]}, {"id": "1807.03139", "submitter": "Vikram Garg", "authors": "Vikram Garg, Girish Sathyanarayana, Sumit Borar, Aruna Rajan", "title": "Utility in Fashion with implicit feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fashion preference is a fuzzy concept that depends on customer taste,\nprevailing norms in fashion product/style, henceforth used interchangeably, and\na customer's perception of utility or fashionability, yet fashion e-retail\nrelies on algorithmically generated search and recommendation systems that\nprocess structured data and images to best match customer preference. Retailers\nstudy tastes solely as a function of what sold vs what did not, and take it to\nrepresent customer preference. Such explicit modeling, however, belies the\nunderlying user preference, which is a complicated interplay of preference and\ncommercials such as brand, price point, promotions, other sale events, and\ncompetitor push/marketing. It is hard to infer a notion of utility or even\ncustomer preference by looking at sales data.\n  In search and recommendation systems for fashion e-retail, customer\npreference is implicitly derived by user-user similarity or item-item\nsimilarity. In this work, we aim to derive a metric that separates the buying\npreferences of users from the commercials of the merchandise (price,\npromotions, etc). We extend our earlier work on explicit signals to gauge\nsellability or preference with implicit signals from user behaviour.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 12:27:17 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Garg", "Vikram", ""], ["Sathyanarayana", "Girish", ""], ["Borar", "Sumit", ""], ["Rajan", "Aruna", ""]]}, {"id": "1807.03142", "submitter": "Bishwo Adhikari Mr.", "authors": "Bishwo Adhikari, Jukka Peltom\\\"aki, Jussi Puura and Heikki Huttunen", "title": "Faster Bounding Box Annotation for Object Detection in Indoor Scenes", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": "7th European Workshop on Visual Information Processing (EUVIP),\n  2018", "doi": "10.1109/EUVIP.2018.8611732", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach for rapid bounding box annotation for object\ndetection datasets. The procedure consists of two stages: The first step is to\nannotate a part of the dataset manually, and the second step proposes\nannotations for the remaining samples using a model trained with the first\nstage annotations. We experimentally study which first/second stage split\nminimizes to total workload. In addition, we introduce a new fully labeled\nobject detection dataset collected from indoor scenes. Compared to other indoor\ndatasets, our collection has more class categories, different backgrounds,\nlighting conditions, occlusion and high intra-class differences. We train deep\nlearning based object detectors with a number of state-of-the-art models and\ncompare them in terms of speed and accuracy. The fully annotated dataset is\nreleased freely available for the research community.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 08:46:57 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Adhikari", "Bishwo", ""], ["Peltom\u00e4ki", "Jukka", ""], ["Puura", "Jussi", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1807.03146", "submitter": "Supasorn Suwajanakorn", "authors": "Supasorn Suwajanakorn, Noah Snavely, Jonathan Tompson, Mohammad\n  Norouzi", "title": "Discovery of Latent 3D Keypoints via End-to-end Geometric Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents KeypointNet, an end-to-end geometric reasoning framework\nto learn an optimal set of category-specific 3D keypoints, along with their\ndetectors. Given a single image, KeypointNet extracts 3D keypoints that are\noptimized for a downstream task. We demonstrate this framework on 3D pose\nestimation by proposing a differentiable objective that seeks the optimal set\nof keypoints for recovering the relative pose between two views of an object.\nOur model discovers geometrically and semantically consistent keypoints across\nviewing angles and instances of an object category. Importantly, we find that\nour end-to-end framework using no ground-truth keypoint annotations outperforms\na fully supervised baseline using the same neural network architecture on the\ntask of pose estimation. The discovered 3D keypoints on the car, chair, and\nplane categories of ShapeNet are visualized at http://keypointnet.github.io/.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 17:41:49 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 10:21:09 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Suwajanakorn", "Supasorn", ""], ["Snavely", "Noah", ""], ["Tompson", "Jonathan", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "1807.03147", "submitter": "Theerawit Wilaiprasitporn", "authors": "Theerawit Wilaiprasitporn, Apiwat Ditthapron, Karis Matchaparn,\n  Tanaboon Tongbuasirilai, Nannapas Banluesombatkul and Ekapol Chuangsuwanich", "title": "Affective EEG-Based Person Identification Using the Deep Learning\n  Approach", "comments": "10 pages", "journal-ref": "IEEE Transactions on Cognitive and Developmental System (2019)", "doi": "10.1109/TCDS.2019.2924648", "report-no": null, "categories": "eess.SP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) is another mode for performing Person\nIdentification (PI). Due to the nature of the EEG signals, EEG-based PI is\ntypically done while the person is performing some kind of mental task, such as\nmotor control. However, few works have considered EEG-based PI while the person\nis in different mental states (affective EEG). The aim of this paper is to\nimprove the performance of affective EEG-based PI using a deep learning\napproach. \\textcolor{red}{We proposed a cascade of deep learning using a\ncombination of Convolutional Neural Networks (CNNs) and Recurrent Neural\nNetworks (RNNs)}. CNNs are used to handle the spatial information from the EEG\nwhile RNNs extract the temporal information. \\textcolor{red}{We evaluated two\ntypes of RNNs, namely, Long Short-Term Memory (CNN-LSTM) and Gated Recurrent\nUnit (CNN-GRU). } The proposed method is evaluated on the state-of-the-art\naffective dataset DEAP. The results indicate that CNN-GRU and CNN-LSTM can\nperform PI from different affective states and reach up to 99.90--100\\% mean\nCorrect Recognition Rate (CRR), significantly outperforming a support vector\nmachine (SVM) baseline system that uses power spectral density (PSD) features.\nNotably, the 100\\% mean \\emph{CRR} comes from only 40 subjects in DEAP dataset.\nTo reduce the number of EEG electrodes from thirty-two to five for more\npractical applications, the frontal region gives the best results reaching up\nto 99.17\\% CRR (from CNN-GRU). Amongst the two deep learning models, we find\nCNN-GRU to slightly outperform CNN-LSTM, while having faster training time.\n\\textcolor{red}{Furthermore, CNN-GRU overcomes the influence of affective\nstates in EEG-Based PI reported in the previous works.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 22:43:01 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 09:43:12 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 11:22:02 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Wilaiprasitporn", "Theerawit", ""], ["Ditthapron", "Apiwat", ""], ["Matchaparn", "Karis", ""], ["Tongbuasirilai", "Tanaboon", ""], ["Banluesombatkul", "Nannapas", ""], ["Chuangsuwanich", "Ekapol", ""]]}, {"id": "1807.03148", "submitter": "Siddhartha Chandra", "authors": "Siddhartha Chandra and Camille Couprie and Iasonas Kokkinos", "title": "Deep Spatio-Temporal Random Fields for Efficient Video Segmentation", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a time- and memory-efficient method for structured\nprediction that couples neuron decisions across both space at time. We show\nthat we are able to perform exact and efficient inference on a densely\nconnected spatio-temporal graph by capitalizing on recent advances on deep\nGaussian Conditional Random Fields (GCRFs). Our method, called VideoGCRF is (a)\nefficient, (b) has a unique global minimum, and (c) can be trained end-to-end\nalongside contemporary deep networks for video understanding. We experiment\nwith multiple connectivity patterns in the temporal domain, and present\nempirical improvements over strong baselines on the tasks of both semantic and\ninstance segmentation of videos.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 10:46:07 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chandra", "Siddhartha", ""], ["Couprie", "Camille", ""], ["Kokkinos", "Iasonas", ""]]}, {"id": "1807.03149", "submitter": "Dan Rosenbaum", "authors": "Dan Rosenbaum, Frederic Besse, Fabio Viola, Danilo J. Rezende, S. M.\n  Ali Eslami", "title": "Learning models for visual 3D localization with implicit mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning based methods for visual localization that do not\nrequire the construction of explicit maps in the form of point clouds or\nvoxels. The goal is to learn an implicit representation of the environment at a\nhigher, more abstract level. We propose to use a generative approach based on\nGenerative Query Networks (GQNs, Eslami et al. 2018), asking the following\nquestions: 1) Can GQN capture more complex scenes than those it was originally\ndemonstrated on? 2) Can GQN be used for localization in those scenes? To study\nthis approach we consider procedurally generated Minecraft worlds, for which we\ncan generate images of complex 3D scenes along with camera pose coordinates. We\nfirst show that GQNs, enhanced with a novel attention mechanism can capture the\nstructure of 3D scenes in Minecraft, as evidenced by their samples. We then\napply the models to the localization problem, comparing the results to a\ndiscriminative baseline, and comparing the ways each approach captures the task\nuncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 15:50:58 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 11:26:23 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Rosenbaum", "Dan", ""], ["Besse", "Frederic", ""], ["Viola", "Fabio", ""], ["Rezende", "Danilo J.", ""], ["Eslami", "S. M. Ali", ""]]}, {"id": "1807.03155", "submitter": "Marie-Morgane Paumard", "authors": "Marie-Morgane Paumard, David Picard, Hedi Tabia", "title": "Jigsaw Puzzle Solving Using Local Feature Co-Occurrences in Deep Neural\n  Networks", "comments": "ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archaeologists are in dire need of automated object reconstruction methods.\nFragments reassembly is close to puzzle problems, which may be solved by\ncomputer vision algorithms. As they are often beaten on most image related\ntasks by deep learning algorithms, we study a classification method that can\nsolve jigsaw puzzles. In this paper, we focus on classifying the relative\nposition: given a couple of fragments, we compute their local relation (e.g. on\ntop). We propose several enhancements over the state of the art in this domain,\nwhich is outperformed by our method by 25\\%. We propose an original dataset\ncomposed of pictures from the Metropolitan Museum of Art. We propose a greedy\nreconstruction method based on the predicted relative positions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 12:19:09 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Paumard", "Marie-Morgane", ""], ["Picard", "David", ""], ["Tabia", "Hedi", ""]]}, {"id": "1807.03160", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "A new ultrasound despeckling method through adaptive threshold", "comments": "7 pages, 2 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient despeckling method using a quantum-inspired adaptive threshold\nfunction is presented for reducing noise of ultrasound images. In the first\nstep, the ultrasound image is decorrelated by an spectrum equalization\nprocedure due to the fact that speckle noise is neither Gaussian nor white. In\nfact, a linear filter is exploited to flatten the power spectral density (PSD)\nof the ultrasound image. Then, the proposed method shrinks complex wavelet\ncoefficients based on the quantum-inspired adaptive threshold function. The\nproposed approach has been used to denoise both real and simulated data sets\nand compare with other widely adopted techniques. Experimental results\ndemonstrate that the proposed method has a competitive performance to remove\nspeckle noise and can preserve details and textures of medical ultrasound\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 20:39:14 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1807.03165", "submitter": "Jeremy Kepner", "authors": "Jeremy Kepner, Vijay Gadepally, Hayden Jananthan, Lauren Milechin, Sid\n  Samsi", "title": "Sparse Deep Neural Network Exact Solutions", "comments": "8 pages, 10 figures, accepted to IEEE HPEC 2018. arXiv admin note:\n  text overlap with arXiv:1708.02937", "journal-ref": null, "doi": "10.1109/HPEC.2018.8547742", "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have emerged as key enablers of machine learning.\nApplying larger DNNs to more diverse applications is an important challenge.\nThe computations performed during DNN training and inference are dominated by\noperations on the weight matrices describing the DNN. As DNNs incorporate more\nlayers and more neurons per layers, these weight matrices may be required to be\nsparse because of memory limitations. Sparse DNNs are one possible approach,\nbut the underlying theory is in the early stages of development and presents a\nnumber of challenges, including determining the accuracy of inference and\nselecting nonzero weights for training. Associative array algebra has been\ndeveloped by the big data community to combine and extend database, matrix, and\ngraph/network concepts for use in large, sparse data problems. Applying this\nmathematics to DNNs simplifies the formulation of DNN mathematics and reveals\nthat DNNs are linear over oscillating semirings. This work uses associative\narray DNNs to construct exact solutions and corresponding perturbation models\nto the rectified linear unit (ReLU) DNN equations that can be used to construct\ntest vectors for sparse DNN implementations over various precisions. These\nsolutions can be used for DNN verification, theoretical explorations of DNN\nproperties, and a starting point for the challenge of sparse training.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 00:47:12 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kepner", "Jeremy", ""], ["Gadepally", "Vijay", ""], ["Jananthan", "Hayden", ""], ["Milechin", "Lauren", ""], ["Samsi", "Sid", ""]]}, {"id": "1807.03167", "submitter": "Helder Oliveira", "authors": "Arthur C. Costa, Helder C. R. Oliveira, Juliana H. Catani, Nestor de\n  Barros, Carlos F. E. Melo, and Marcelo A. C. Vieira", "title": "Data Augmentation for Detection of Architectural Distortion in Digital\n  Mammography using Deep Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Early detection of breast cancer can increase treatment efficiency.\nArchitectural Distortion (AD) is a very subtle contraction of the breast tissue\nand may represent the earliest sign of cancer. Since it is very likely to be\nunnoticed by radiologists, several approaches have been proposed over the years\nbut none using deep learning techniques. To train a Convolutional Neural\nNetwork (CNN), which is a deep neural architecture, is necessary a huge amount\nof data. To overcome this problem, this paper proposes a data augmentation\napproach applied to clinical image dataset to properly train a CNN. Results\nusing receiver operating characteristic analysis showed that with a very\nlimited dataset we could train a CNN to detect AD in digital mammography with\narea under the curve (AUC = 0.74).\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 02:12:49 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Costa", "Arthur C.", ""], ["Oliveira", "Helder C. R.", ""], ["Catani", "Juliana H.", ""], ["de Barros", "Nestor", ""], ["Melo", "Carlos F. E.", ""], ["Vieira", "Marcelo A. C.", ""]]}, {"id": "1807.03173", "submitter": "Kilian Hett", "authors": "Kilian Hett (LaBRI, CNRS), Vinh-Thong Ta (Bordeaux INP), Jose Vicente\n  Manjon, Pierrick Coup\\'e (LaBRI, CNRS)", "title": "Graph of brain structures grading for early detection of Alzheimer's\n  disease", "comments": null, "journal-ref": "Medical Image Computing and Computer-Assisted Intervention, Sep\n  2018, GRANADA, Spain", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease is the most common dementia leading to an irreversible\nneurodegenerative process. To date, subject revealed advanced brain structural\nalterations when the diagnosis is established. Therefore, an earlier diagnosis\nof this dementia is crucial although it is a challenging task. Recently, many\nstudies have proposed biomarkers to perform early detection of Alzheimer's\ndisease. Some of them have proposed methods based on inter-subject similarity\nwhile other approaches have investigated framework using intra-subject\nvariability. In this work, we propose a novel framework combining both\napproaches within an efficient graph of brain structures grading. Subsequently,\nwe demonstrate the competitive performance of the proposed method compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 08:43:31 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Hett", "Kilian", "", "LaBRI, CNRS"], ["Ta", "Vinh-Thong", "", "Bordeaux INP"], ["Manjon", "Jose Vicente", "", "LaBRI, CNRS"], ["Coup\u00e9", "Pierrick", "", "LaBRI, CNRS"]]}, {"id": "1807.03179", "submitter": "Xiao Liu", "authors": "Xiao Liu, Bin Zhang, Anjana Susarla, Rema Padman", "title": "YouTube for Patient Education: A Deep Learning Approach for\n  Understanding Medical Knowledge from User-Generated Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  YouTube presents an unprecedented opportunity to explore how machine learning\nmethods can improve healthcare information dissemination. We propose an\ninterdisciplinary lens that synthesizes machine learning methods with\nhealthcare informatics themes to address the critical issue of developing a\nscalable algorithmic solution to evaluate videos from a health literacy and\npatient education perspective. We develop a deep learning method to understand\nthe level of medical knowledge encoded in YouTube videos. Preliminary results\nsuggest that we can extract medical knowledge from YouTube videos and classify\nvideos according to the embedded knowledge with satisfying performance. Deep\nlearning methods show great promise in knowledge extraction, natural language\nunderstanding, and image classification, especially in an era of\npatient-centric care and precision medicine.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 17:19:26 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Liu", "Xiao", ""], ["Zhang", "Bin", ""], ["Susarla", "Anjana", ""], ["Padman", "Rema", ""]]}, {"id": "1807.03191", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, Ben Cox, Felix Lucka, Nam Huynh, Marta Betcke, Paul\n  Beard, Simon Arridge", "title": "Approximate k-space models and Deep Learning for fast photoacoustic\n  reconstruction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-00129-2_12", "report-no": null, "categories": "cs.CV cs.LG cs.SD eess.AS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for accelerated iterative reconstructions using a fast\nand approximate forward model that is based on k-space methods for\nphotoacoustic tomography. The approximate model introduces aliasing artefacts\nin the gradient information for the iterative reconstruction, but these\nartefacts are highly structured and we can train a CNN that can use the\napproximate information to perform an iterative reconstruction. We show\nfeasibility of the method for human in-vivo measurements in a limited-view\ngeometry. The proposed method is able to produce superior results to total\nvariation reconstructions with a speed-up of 32 times.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 14:32:18 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Cox", "Ben", ""], ["Lucka", "Felix", ""], ["Huynh", "Nam", ""], ["Betcke", "Marta", ""], ["Beard", "Paul", ""], ["Arridge", "Simon", ""]]}, {"id": "1807.03215", "submitter": "Fenglei Fan", "authors": "Fenglei Fan, Ge Wang", "title": "Fuzzy Logic Interpretation of Quadratic Networks", "comments": "10 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over past several years, deep learning has achieved huge successes in various\napplications. However, such a data-driven approach is often criticized for lack\nof interpretability. Recently, we proposed artificial quadratic neural networks\nconsisting of second-order neurons in potentially many layers. In each\nsecond-order neuron, a quadratic function is used in the place of the inner\nproduct in a traditional neuron, and then undergoes a nonlinear activation.\nWith a single second-order neuron, any fuzzy logic operation, such as XOR, can\nbe implemented. In this sense, any deep network constructed with quadratic\nneurons can be interpreted as a deep fuzzy logic system. Since traditional\nneural networks and second-order counterparts can represent each other and\nfuzzy logic operations are naturally implemented in second-order neural\nnetworks, it is plausible to explain how a deep neural network works with a\nsecond-order network as the system model. In this paper, we generalize and\ncategorize fuzzy logic operations implementable with individual second-order\nneurons, and then perform statistical/information theoretic analyses of\nexemplary quadratic neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 12:45:25 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 19:57:58 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 00:21:40 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Fan", "Fenglei", ""], ["Wang", "Ge", ""]]}, {"id": "1807.03216", "submitter": "Krishna Kumar Venkatasubramanian", "authors": "Joshua Hebert, Brittany Lewis, Hang Cai, Krishna K.\n  Venkatasubramanian, Matthew Provost, Kelly Charlebois", "title": "Ballistocardiogram-based Authentication using Convolutional Neural\n  Networks", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this work is to demonstrate the use of the ballistocardiogram\n(BCG) signal, derived using head-mounted wearable devices, as a viable\nbiometric for authentication. The BCG signal is the measure of an person's body\nacceleration as a result of the heart's ejection of blood. It is a\ncharacterization of the cardiac cycle and can be derived non-invasively from\nthe measurement of subtle movements of a person's extremities. In this paper,\nwe use several versions of the BCG signal, derived from accelerometer and\ngyroscope sensors on a Smart Eyewear (SEW) device, for authentication. The\nderived BCG signals are used to train a convolutional neural network (CNN) as\nan authentication model, which is personalized for each subject. We evaluate\nour authentication models using data from 12 subjects and show that our\napproach has an equal error rate (EER) of 3.5% immediately after training and\n13\\% after about 2 months, in the worst case. We also explore the use of our\nauthentication approach for people with motor disabilities. Our analysis using\na separate dataset of 6 subjects with non-spastic cerebral palsy shows an EER\nof 11.2% immediately after training and 21.6% after about 2 months, in the\nworst-case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 19:11:19 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Hebert", "Joshua", ""], ["Lewis", "Brittany", ""], ["Cai", "Hang", ""], ["Venkatasubramanian", "Krishna K.", ""], ["Provost", "Matthew", ""], ["Charlebois", "Kelly", ""]]}, {"id": "1807.03232", "submitter": "Sandeep Chandra Bollepalli", "authors": "B S Chandra, C S Sastry and S Jana", "title": "Robust Heartbeat Detection from Multimodal Data via CNN-based\n  Generalizable Information Fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Heartbeat detection remains central to cardiac disease diagnosis\nand management, and is traditionally performed based on electrocardiogram\n(ECG). To improve robustness and accuracy of detection, especially, in certain\ncritical-care scenarios, the use of additional physiological signals such as\narterial blood pressure (BP) has recently been suggested. There, estimation of\nheartbeat location requires information fusion from multiple signals. However,\nreported efforts in this direction often obtain multimodal estimates somewhat\nindirectly, by voting among separately obtained signal-specific intermediate\nestimates. In contrast, we propose to directly fuse information from multiple\nsignals without requiring intermediate estimates, and thence estimate heartbeat\nlocation in a robust manner. Method: We propose as a heartbeat detector, a\nconvolutional neural network (CNN) that learns fused features from multiple\nphysiological signals. This method eliminates the need for hand-picked\nsignal-specific features and ad hoc fusion schemes. Further, being data-driven,\nthe same algorithm learns suitable features from arbitrary set of signals.\nResults: Using ECG and BP signals of PhysioNet 2014 Challenge database, we\nobtained a score of 94%. Further, using two ECG channels of MIT-BIH arrhythmia\ndatabase, we scored 99.92\\%. Both those scores compare favourably with\npreviously reported database-specific results. Also, our detector achieved high\naccuracy in a variety of clinical conditions. Conclusion: The proposed\nCNN-based information fusion (CIF) algorithm is generalizable, robust and\nefficient in detecting heartbeat location from multiple signals. Significance:\nIn medical signal monitoring systems, our technique would accurately estimate\nheartbeat locations even when only a subset of channels are reliable.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 06:47:16 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chandra", "B S", ""], ["Sastry", "C S", ""], ["Jana", "S", ""]]}, {"id": "1807.03235", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar, Gerard Pons-Moll, Mario Fritz", "title": "Fashion is Taking Shape: Understanding Clothing Preference Based on Body\n  Shape From Online Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the correlation between clothing garments and body shape, we\ncollected a new dataset (Fashion Takes Shape), which includes images of users\nwith clothing category annotations. We employ our multi-photo approach to\nestimate body shapes of each user and build a conditional model of clothing\ncategories given body-shape. We demonstrate that in real-world data, clothing\ncategories and body-shapes are correlated and show that our multi-photo\napproach leads to a better predictive model for clothing categories compared to\nmodels based on single-view shape estimates or manually annotated body types.\nWe see our method as the first step towards the large-scale understanding of\nclothing preferences from body shape.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:35:13 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 11:27:46 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Pons-Moll", "Gerard", ""], ["Fritz", "Mario", ""]]}, {"id": "1807.03236", "submitter": "Zhiguo Zhou", "authors": "Zhiguo Zhou, Shulong Li, Genggeng Qin, Michael Folkert, Steve Jiang,\n  and Jing Wang", "title": "Automatic multi-objective based feature selection for classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Accurately classifying the malignancy of lesions detected in a\nscreening scan is critical for reducing false positives. Radiomics holds great\npotential to differentiate malignant from benign tumors by extracting and\nanalyzing a large number of quantitative image features. Since not all radiomic\nfeatures contribute to an effective classifying model, selecting an optimal\nfeature subset is critical. Methods: This work proposes a new multi-objective\nbased feature selection (MO-FS) algorithm that considers sensitivity and\nspecificity simultaneously as the objective functions during feature selection.\nFor MO-FS, we developed a modified entropy based termination criterion (METC)\nthat stops the algorithm automatically rather than relying on a preset number\nof generations. We also designed a solution selection methodology for\nmulti-objective learning that uses the evidential reasoning approach (SMOLER)\nto automatically select the optimal solution from the Pareto-optimal set.\nFurthermore, we developed an adaptive mutation operation to generate the\nmutation probability in MO-FS automatically. Results: We evaluated the MO-FS\nfor classifying lung nodule malignancy in low-dose CT and breast lesion\nmalignancy in digital breast tomosynthesis. Conclusion: The experimental\nresults demonstrated that the feature set selected by MO-FS achieved better\nclassification performance than features selected by other commonly used\nmethods. Significance: The proposed method is general and more effective\nradiomic feature selection strategy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:37:10 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 16:18:00 GMT"}, {"version": "v3", "created": "Mon, 11 Feb 2019 20:08:36 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 01:31:02 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Zhou", "Zhiguo", ""], ["Li", "Shulong", ""], ["Qin", "Genggeng", ""], ["Folkert", "Michael", ""], ["Jiang", "Steve", ""], ["Wang", "Jing", ""]]}, {"id": "1807.03238", "submitter": "Asim Iqbal", "authors": "Asim Iqbal, Asfandyar Sheikh, Theofanis Karayannis", "title": "Exploring Brain-wide Development of Inhibition through Deep Learning", "comments": "33 pages, 21 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce here a fully automated convolutional neural network-based method\nfor brain image processing to Detect Neurons in different brain Regions during\nDevelopment (DeNeRD). Our method takes a developing mouse brain as input and i)\nregisters the brain sections against a developing mouse reference atlas, ii)\ndetects various types of neurons, and iii) quantifies the neural density in\nmany unique brain regions at different postnatal (P) time points. Our method is\ninvariant to the shape, size and expression of neurons and by using DeNeRD, we\ncompare the brain-wide neural density of all GABAergic neurons in developing\nbrains of ages P4, P14 and P56. We discover and report 6 different clusters of\nregions in the mouse brain in which GABAergic neurons develop in a differential\nmanner from early age (P4) to adulthood (P56). These clusters reveal key steps\nof GABAergic cell development that seem to track with the functional\ndevelopment of diverse brain regions as the mouse transitions from a passive\nreceiver of sensory information (<P14) to an active seeker (>P14).\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:39:09 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Iqbal", "Asim", ""], ["Sheikh", "Asfandyar", ""], ["Karayannis", "Theofanis", ""]]}, {"id": "1807.03247", "submitter": "Rosanne Liu", "authors": "Rosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric\n  Frank, Alex Sergeev, Jason Yosinski", "title": "An Intriguing Failing of Convolutional Neural Networks and the CoordConv\n  Solution", "comments": "Published in NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few ideas have enjoyed as large an impact on deep learning as convolution.\nFor any problem involving pixels or spatial representations, common intuition\nholds that convolutional neural networks may be appropriate. In this paper we\nshow a striking counterexample to this intuition via the seemingly trivial\ncoordinate transform problem, which simply requires learning a mapping between\ncoordinates in (x,y) Cartesian space and one-hot pixel space. Although\nconvolutional networks would seem appropriate for this task, we show that they\nfail spectacularly. We demonstrate and carefully analyze the failure first on a\ntoy problem, at which point a simple fix becomes obvious. We call this solution\nCoordConv, which works by giving convolution access to its own input\ncoordinates through the use of extra coordinate channels. Without sacrificing\nthe computational and parametric efficiency of ordinary convolution, CoordConv\nallows networks to learn either complete translation invariance or varying\ndegrees of translation dependence, as required by the end task. CoordConv\nsolves the coordinate transform problem with perfect generalization and 150\ntimes faster with 10--100 times fewer parameters than convolution. This stark\ncontrast raises the question: to what extent has this inability of convolution\npersisted insidiously inside other tasks, subtly hampering performance from\nwithin? A complete answer to this question will require further investigation,\nbut we show preliminary evidence that swapping convolution for CoordConv can\nimprove models on a diverse set of tasks. Using CoordConv in a GAN produced\nless mode collapse as the transform between high-level spatial latents and\npixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST\nshowed 24% better IOU when using CoordConv, and in the RL domain agents playing\nAtari games benefit significantly from the use of CoordConv layers.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:48:08 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 16:31:58 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Liu", "Rosanne", ""], ["Lehman", "Joel", ""], ["Molino", "Piero", ""], ["Such", "Felipe Petroski", ""], ["Frank", "Eric", ""], ["Sergeev", "Alex", ""], ["Yosinski", "Jason", ""]]}, {"id": "1807.03284", "submitter": "Pengchong Jin", "authors": "Pengchong Jin and Vivek Rathod and Xiangxin Zhu", "title": "Pooling Pyramid Network for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We'd like to share a simple tweak of Single Shot Multibox Detector (SSD)\nfamily of detectors, which is effective in reducing model size while\nmaintaining the same quality. We share box predictors across all scales, and\nreplace convolution between scales with max pooling. This has two advantages\nover vanilla SSD: (1) it avoids score miscalibration across scales; (2) the\nshared predictor sees the training data over all scales. Since we reduce the\nnumber of predictors to one, and trim all convolutions between them, model size\nis significantly smaller. We empirically show that these changes do not hurt\nmodel quality compared to vanilla SSD.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 17:40:09 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Jin", "Pengchong", ""], ["Rathod", "Vivek", ""], ["Zhu", "Xiangxin", ""]]}, {"id": "1807.03326", "submitter": "Xiaoyong Yuan", "authors": "Xiaoyong Yuan, Pan He, Xiaolin Andy Li, Dapeng Oliver Wu", "title": "Adaptive Adversarial Attack on Scene Text Recognition", "comments": "To be appear in INFOCOM 2020, The Eighth International Workshop on\n  Security and Privacy in Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that state-of-the-art deep learning models are\nvulnerable to the inputs with small perturbations (adversarial examples). We\nobserve two critical obstacles in adversarial examples: (i) Strong adversarial\nattacks (e.g., C&W attack) require manually tuning hyper-parameters and take a\nlong time to construct an adversarial example, making it impractical to attack\nreal-time systems; (ii) Most of the studies focus on non-sequential tasks, such\nas image classification, yet only a few consider sequential tasks. In this\nwork, we speed up adversarial attacks, especially on sequential learning tasks.\nBy leveraging the uncertainty of each task, we directly learn the adaptive\nmulti-task weightings, without manually searching hyper-parameters. A unified\narchitecture is developed and evaluated for both non-sequential tasks and\nsequential ones. To validate the effectiveness, we take the scene text\nrecognition task as a case study. To our best knowledge, our proposed method is\nthe first attempt to adversarial attack for scene text recognition. Adaptive\nAttack achieves over 99.9\\% success rate with 3-6X speedup compared to\nstate-of-the-art adversarial attacks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 18:12:27 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 18:57:08 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 15:40:36 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Yuan", "Xiaoyong", ""], ["He", "Pan", ""], ["Li", "Xiaolin Andy", ""], ["Wu", "Dapeng Oliver", ""]]}, {"id": "1807.03342", "submitter": "Peng Tang", "authors": "Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu,\n  Alan Yuille", "title": "PCL: Proposal Cluster Learning for Weakly Supervised Object Detection", "comments": "Accepted by TPAMI. Codes are available at\n  https://github.com/ppengtang/oicr/tree/pcl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly Supervised Object Detection (WSOD), using only image-level annotations\nto train object detectors, is of growing importance in object recognition. In\nthis paper, we propose a novel deep network for WSOD. Unlike previous networks\nthat transfer the object detection problem to an image classification problem\nusing Multiple Instance Learning (MIL), our strategy generates proposal\nclusters to learn refined instance classifiers by an iterative process. The\nproposals in the same cluster are spatially adjacent and associated with the\nsame object. This prevents the network from concentrating too much on parts of\nobjects instead of whole objects. We first show that instances can be assigned\nobject or background labels directly based on proposal clusters for instance\nclassifier refinement, and then show that treating each cluster as a small new\nbag yields fewer ambiguities than the directly assigning label method. The\niterative instance classifier refinement is implemented online using multiple\nstreams in convolutional neural networks, where the first is an MIL network and\nthe others are for instance classifier refinement supervised by the preceding\none. Experiments are conducted on the PASCAL VOC, ImageNet detection, and\nMS-COCO benchmarks for WSOD. Results show that our method outperforms the\nprevious state of the art significantly.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 18:59:29 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 18:28:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Tang", "Peng", ""], ["Wang", "Xinggang", ""], ["Bai", "Song", ""], ["Shen", "Wei", ""], ["Bai", "Xiang", ""], ["Liu", "Wenyu", ""], ["Yuille", "Alan", ""]]}, {"id": "1807.03343", "submitter": "Muneer Ahmad", "authors": "Muneer Ahmad Dedmari and Sailesh Conjeti and Santiago Estrada and\n  Phillip Ehses and Tony St\\\"ocker and Martin Reuter", "title": "Complex Fully Convolutional Neural Networks for MR Image Reconstruction", "comments": "9 pages, accepted in MICCAI-MLMIR 2018 Worshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undersampling the k-space data is widely adopted for acceleration of Magnetic\nResonance Imaging (MRI). Current deep learning based approaches for supervised\nlearning of MRI image reconstruction employ real-valued operations and\nrepresentations by treating complex valued k-space/spatial-space as real\nvalues. In this paper, we propose complex dense fully convolutional neural\nnetwork ($\\mathbb{C}$DFNet) for learning to de-alias the reconstruction\nartifacts within undersampled MRI images. We fashioned a densely-connected\nfully convolutional block tailored for complex-valued inputs by introducing\ndedicated layers such as complex convolution, batch normalization,\nnon-linearities etc. $\\mathbb{C}$DFNet leverages the inherently complex-valued\nnature of input k-space and learns richer representations. We demonstrate\nimproved perceptual quality and recovery of anatomical structures through\n$\\mathbb{C}$DFNet in contrast to its real-valued counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 19:03:25 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Dedmari", "Muneer Ahmad", ""], ["Conjeti", "Sailesh", ""], ["Estrada", "Santiago", ""], ["Ehses", "Phillip", ""], ["St\u00f6cker", "Tony", ""], ["Reuter", "Martin", ""]]}, {"id": "1807.03354", "submitter": "Gary Tam", "authors": "Gareth Andrews, Sam Endean, Roberto Dyke, Yukun Lai, Gwenno Ffrancon,\n  Gary KL Tam", "title": "HDFD --- A High Deformation Facial Dynamics Benchmark for Evaluation of\n  Non-Rigid Surface Registration and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects that undergo non-rigid deformation are common in the real world. A\ntypical and challenging example is the human faces. While various techniques\nhave been developed for deformable shape registration and classification,\nbenchmarks with detailed labels and landmarks suitable for evaluating such\ntechniques are still limited. In this paper, we present a novel facial dynamic\ndataset HDFD which addresses the gap of existing datasets, including 4D funny\nfaces with substantial non-isometric deformation, and 4D visual-audio faces of\nspoken phrases in a minority language (Welsh). Both datasets are captured from\n21 participants. The sequences are manually landmarked, with the spoken phrases\nfurther rated by a Welsh expert for level of fluency. These are useful for\nquantitative evaluation of both registration and classification tasks. We\nfurther develop a methodology to evaluate several recent non-rigid surface\nregistration techniques, using our dynamic sequences as test cases. The study\ndemonstrates the significance and usefulness of our new dataset --- a\nchallenging benchmark dataset for future techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 19:36:42 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Andrews", "Gareth", ""], ["Endean", "Sam", ""], ["Dyke", "Roberto", ""], ["Lai", "Yukun", ""], ["Ffrancon", "Gwenno", ""], ["Tam", "Gary KL", ""]]}, {"id": "1807.03361", "submitter": "Yipeng Hu", "authors": "Yipeng Hu, Marc Modat, Eli Gibson, Wenqi Li, Nooshin Ghavami, Ester\n  Bonmati, Guotai Wang, Steven Bandula, Caroline M. Moore, Mark Emberton,\n  S\\'ebastien Ourselin, J. Alison Noble, Dean C. Barratt, Tom Vercauteren", "title": "Weakly-Supervised Convolutional Neural Networks for Multimodal Image\n  Registration", "comments": "Accepted manuscript in Medical Image Analysis", "journal-ref": null, "doi": "10.1016/j.media.2018.07.002", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the fundamental challenges in supervised learning for multimodal image\nregistration is the lack of ground-truth for voxel-level spatial\ncorrespondence. This work describes a method to infer voxel-level\ntransformation from higher-level correspondence information contained in\nanatomical labels. We argue that such labels are more reliable and practical to\nobtain for reference sets of image pairs than voxel-level correspondence.\nTypical anatomical labels of interest may include solid organs, vessels, ducts,\nstructure boundaries and other subject-specific ad hoc landmarks. The proposed\nend-to-end convolutional neural network approach aims to predict displacement\nfields to align multiple labelled corresponding structures for individual image\npairs during the training, while only unlabelled image pairs are used as the\nnetwork input for inference. We highlight the versatility of the proposed\nstrategy, for training, utilising diverse types of anatomical labels, which\nneed not to be identifiable over all training image pairs. At inference, the\nresulting 3D deformable image registration algorithm runs in real-time and is\nfully-automated without requiring any anatomical labels or initialisation.\nSeveral network architecture variants are compared for registering T2-weighted\nmagnetic resonance images and 3D transrectal ultrasound images from prostate\ncancer patients. A median target registration error of 3.6 mm on landmark\ncentroids and a median Dice of 0.87 on prostate glands are achieved from\ncross-validation experiments, in which 108 pairs of multimodal images from 76\npatients were tested with high-quality anatomical labels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 19:53:16 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Hu", "Yipeng", ""], ["Modat", "Marc", ""], ["Gibson", "Eli", ""], ["Li", "Wenqi", ""], ["Ghavami", "Nooshin", ""], ["Bonmati", "Ester", ""], ["Wang", "Guotai", ""], ["Bandula", "Steven", ""], ["Moore", "Caroline M.", ""], ["Emberton", "Mark", ""], ["Ourselin", "S\u00e9bastien", ""], ["Noble", "J. Alison", ""], ["Barratt", "Dean C.", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1807.03367", "submitter": "Harm de Vries", "authors": "Harm de Vries, Kurt Shuster, Dhruv Batra, Devi Parikh, Jason Weston,\n  Douwe Kiela", "title": "Talk the Walk: Navigating New York City through Grounded Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce \"Talk The Walk\", the first large-scale dialogue dataset grounded\nin action and perception. The task involves two agents (a \"guide\" and a\n\"tourist\") that communicate via natural language in order to achieve a common\ngoal: having the tourist navigate to a given target location. The task and\ndataset, which are described in detail, are challenging and their full solution\nis an open problem that we pose to the community. We (i) focus on the task of\ntourist localization and develop the novel Masked Attention for Spatial\nConvolutions (MASC) mechanism that allows for grounding tourist utterances into\nthe guide's map, (ii) show it yields significant improvements for both emergent\nand natural language communication, and (iii) using this method, we establish\nnon-trivial baselines on the full task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 20:05:24 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 16:07:08 GMT"}, {"version": "v3", "created": "Sun, 23 Dec 2018 22:42:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["de Vries", "Harm", ""], ["Shuster", "Kurt", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""], ["Weston", "Jason", ""], ["Kiela", "Douwe", ""]]}, {"id": "1807.03376", "submitter": "Aparna Bharati", "authors": "Aparna Bharati, Daniel Moreira, Joel Brogan, Patricia Hale, Kevin W.\n  Bowyer, Patrick J. Flynn, Anderson Rocha, Walter J. Scheirer", "title": "Beyond Pixels: Image Provenance Analysis Leveraging Metadata", "comments": "Supplemental material for this paper can be found at\n  https://drive.google.com/file/d/1Tbs2CQg_VQAc2PdztW5twVaiXD0G12-H/view?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative works, whether paintings or memes, follow unique journeys that\nresult in their final form. Understanding these journeys, a process known as\n\"provenance analysis\", provides rich insights into the use, motivation, and\nauthenticity underlying any given work. The application of this type of study\nto the expanse of unregulated content on the Internet is what we consider in\nthis paper. Provenance analysis provides a snapshot of the chronology and\nvalidity of content as it is uploaded, re-uploaded, and modified over time.\nAlthough still in its infancy, automated provenance analysis for online\nmultimedia is already being applied to different types of content. Most current\nworks seek to build provenance graphs based on the shared content between\nimages or videos. This can be a computationally expensive task, especially when\nconsidering the vast influx of content that the Internet sees every day.\nUtilizing non-content-based information, such as timestamps, geotags, and\ncamera IDs can help provide important insights into the path a particular image\nor video has traveled during its time on the Internet without large\ncomputational overhead. This paper tests the scope and applicability of\nmetadata-based inferences for provenance graph construction in two different\nscenarios: digital image forensics and cultural analytics.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 20:34:30 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 17:31:06 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 19:03:26 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Bharati", "Aparna", ""], ["Moreira", "Daniel", ""], ["Brogan", "Joel", ""], ["Hale", "Patricia", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""], ["Rocha", "Anderson", ""], ["Scheirer", "Walter J.", ""]]}, {"id": "1807.03380", "submitter": "Aarush Gupta", "authors": "Aarush Gupta (1), Dakshit Agrawal (1), Hardik Chauhan (1), Jose Dolz\n  (2) and Marco Pedersoli (2) ((1) Indian Institute of Technology Roorkee,\n  India, (2) \\'Ecole de Technologie Sup\\'erieure, Montreal, Canada)", "title": "An Attention Model for group-level emotion recognition", "comments": "5 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new approach for classifying the global emotion of\nimages containing groups of people. To achieve this task, we consider two\ndifferent and complementary sources of information: i) a global representation\nof the entire image (ii) a local representation where only faces are\nconsidered. While the global representation of the image is learned with a\nconvolutional neural network (CNN), the local representation is obtained by\nmerging face features through an attention mechanism. The two representations\nare first learned independently with two separate CNN branches and then fused\nthrough concatenation in order to obtain the final group-emotion classifier.\nFor our submission to the EmotiW 2018 group-level emotion recognition\nchallenge, we combine several variations of the proposed model into an\nensemble, obtaining a final accuracy of 64.83% on the test set and ranking 4th\namong all challenge participants.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 20:40:50 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Gupta", "Aarush", ""], ["Agrawal", "Dakshit", ""], ["Chauhan", "Hardik", ""], ["Dolz", "Jose", ""], ["Pedersoli", "Marco", ""]]}, {"id": "1807.03401", "submitter": "Dimitrios Korkinof", "authors": "Dimitrios Korkinof, Tobias Rijken, Michael O'Neill, Joseph Yearsley,\n  Hugh Harvey and Ben Glocker", "title": "High-Resolution Mammogram Synthesis using Progressive Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to generate synthetic medical images is useful for data\naugmentation, domain transfer, and out-of-distribution detection. However,\ngenerating realistic, high-resolution medical images is challenging,\nparticularly for Full Field Digital Mammograms (FFDM), due to the textural\nheterogeneity, fine structural details and specific tissue properties. In this\npaper, we explore the use of progressively trained generative adversarial\nnetworks (GANs) to synthesize mammograms, overcoming the underlying\ninstabilities when training such adversarial models. This work is the first to\nshow that generation of realistic synthetic medical images is feasible at up to\n1280x1024 pixels, the highest resolution achieved for medical image synthesis,\nenabling visualizations within standard mammographic hanging protocols. We hope\nthis work can serve as a useful guide and facilitate further research on GANs\nin the medical imaging domain.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 21:53:54 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 13:27:04 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Korkinof", "Dimitrios", ""], ["Rijken", "Tobias", ""], ["O'Neill", "Michael", ""], ["Yearsley", "Joseph", ""], ["Harvey", "Hugh", ""], ["Glocker", "Ben", ""]]}, {"id": "1807.03407", "submitter": "Swaminathan Gurumurthy", "authors": "Swaminathan Gurumurthy and Shubham Agrawal", "title": "High Fidelity Semantic Shape Completion for Point Clouds using Latent\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic shape completion is a challenging problem in 3D computer vision\nwhere the task is to generate a complete 3D shape using a partial 3D shape as\ninput. We propose a learning-based approach to complete incomplete 3D shapes\nthrough generative modeling and latent manifold optimization. Our algorithm\nworks directly on point clouds. We use an autoencoder and a GAN to learn a\ndistribution of embeddings for point clouds of object classes. An input point\ncloud with missing regions is first encoded to a feature vector. The\nrepresentations learnt by the GAN are then used to find the best latent vector\non the manifold using a combined optimization that finds a vector in the\nmanifold of plausible vectors that is close to the original input (both in the\nfeature space and the output space of the decoder). Experiments show that our\nalgorithm is capable of successfully reconstructing point clouds with large\nmissing regions with very high fidelity without having to rely on exemplar\nbased database retrieval.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 22:24:17 GMT"}, {"version": "v2", "created": "Sun, 30 Sep 2018 01:06:28 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Gurumurthy", "Swaminathan", ""], ["Agrawal", "Shubham", ""]]}, {"id": "1807.03425", "submitter": "Elin Farnell", "authors": "Henry Kvinge, Elin Farnell, Michael Kirby, and Chris Peterson", "title": "A GPU-Oriented Algorithm Design for Secant-Based Dimensionality\n  Reduction", "comments": "To appear in the 17th IEEE International Symposium on Parallel and\n  Distributed Computing, Geneva, Switzerland 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality-reduction techniques are a fundamental tool for extracting\nuseful information from high-dimensional data sets. Because secant sets encode\nmanifold geometry, they are a useful tool for designing meaningful\ndata-reduction algorithms. In one such approach, the goal is to construct a\nprojection that maximally avoids secant directions and hence ensures that\ndistinct data points are not mapped too close together in the reduced space.\nThis type of algorithm is based on a mathematical framework inspired by the\nconstructive proof of Whitney's embedding theorem from differential topology.\nComputing all (unit) secants for a set of points is by nature computationally\nexpensive, thus opening the door for exploitation of GPU architecture for\nachieving fast versions of these algorithms. We present a polynomial-time\ndata-reduction algorithm that produces a meaningful low-dimensional\nrepresentation of a data set by iteratively constructing improved projections\nwithin the framework described above. Key to our algorithm design and\nimplementation is the use of GPUs which, among other things, minimizes the\ncomputational time required for the calculation of all secant lines. One goal\nof this report is to share ideas with GPU experts and to discuss a class of\nmathematical algorithms that may be of interest to the broader GPU community.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 00:02:16 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Kvinge", "Henry", ""], ["Farnell", "Elin", ""], ["Kirby", "Michael", ""], ["Peterson", "Chris", ""]]}, {"id": "1807.03434", "submitter": "Nanqing Dong", "authors": "Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai\n  and Eric P. Xing", "title": "Unsupervised Domain Adaptation for Automatic Estimation of\n  Cardiothoracic Ratio", "comments": "Accepted by MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cardiothoracic ratio (CTR), a clinical metric of heart size in chest\nX-rays (CXRs), is a key indicator of cardiomegaly. Manual measurement of CTR is\ntime-consuming and can be affected by human subjectivity, making it desirable\nto design computer-aided systems that assist clinicians in the diagnosis\nprocess. Automatic CTR estimation through chest organ segmentation, however,\nrequires large amounts of pixel-level annotated data, which is often\nunavailable. To alleviate this problem, we propose an unsupervised domain\nadaptation framework based on adversarial networks. The framework learns domain\ninvariant feature representations from openly available data sources to produce\naccurate chest organ segmentation for unlabeled datasets. Specifically, we\npropose a model that enforces our intuition that prediction masks should be\ndomain independent. Hence, we introduce a discriminator that distinguishes\nsegmentation predictions from ground truth masks. We evaluate our system's\nprediction based on the assessment of radiologists and demonstrate the clinical\npracticability for the diagnosis of cardiomegaly. We finally illustrate on the\nJSRT dataset that the semi-supervised performance of our model is also very\npromising.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 01:18:40 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Dong", "Nanqing", ""], ["Kampffmeyer", "Michael", ""], ["Liang", "Xiaodan", ""], ["Wang", "Zeya", ""], ["Dai", "Wei", ""], ["Xing", "Eric P.", ""]]}, {"id": "1807.03440", "submitter": "Asim Iqbal", "authors": "Asim Iqbal, Romesa Khan, Theofanis Karayannis", "title": "Developing Brain Atlas through Deep Learning", "comments": "31 pages, 17 figures, 1 Table", "journal-ref": null, "doi": "10.1038/s42256-019-0058-8", "report-no": null, "categories": "cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroscientists have devoted significant effort into the creation of standard\nbrain reference atlases for high-throughput registration of anatomical regions\nof interest. However, variability in brain size and form across individuals\nposes a significant challenge for such reference atlases. To overcome these\nlimitations, we introduce a fully automated deep neural network-based method\n(SeBRe) for registration through Segmenting Brain Regions of interest with\nminimal human supervision. We demonstrate the validity of our method on brain\nimages from different mouse developmental time points, across a range of\nneuronal markers and imaging modalities. We further assess the performance of\nour method on images from MR-scanned human brains. Our registration method can\naccelerate brain-wide exploration of region-specific changes in brain\ndevelopment and, by simply segmenting brain regions of interest for\nhigh-throughput brain-wide analysis, provides an alternative to existing\ncomplex brain registration techniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 01:28:44 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 15:01:56 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Iqbal", "Asim", ""], ["Khan", "Romesa", ""], ["Karayannis", "Theofanis", ""]]}, {"id": "1807.03464", "submitter": "Ravi Kumar Thakur", "authors": "Ravi Kumar Thakur and Snehasis Mukherjee", "title": "SceneEDNet: A Deep Learning Approach for Scene Flow Estimation", "comments": null, "journal-ref": "ICARCV (2018) 394-399", "doi": "10.1109/ICARCV.2018.8581172", "report-no": null, "categories": "cs.CV cs.CG cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating scene flow in RGB-D videos is attracting much interest of the\ncomputer vision researchers, due to its potential applications in robotics. The\nstate-of-the-art techniques for scene flow estimation, typically rely on the\nknowledge of scene structure of the frame and the correspondence between\nframes. However, with the increasing amount of RGB-D data captured from\nsophisticated sensors like Microsoft Kinect, and the recent advances in the\narea of sophisticated deep learning techniques, introduction of an efficient\ndeep learning technique for scene flow estimation, is becoming important. This\npaper introduces a first effort to apply a deep learning method for direct\nestimation of scene flow by presenting a fully convolutional neural network\nwith an encoder-decoder (ED) architecture. The proposed network SceneEDNet\ninvolves estimation of three dimensional motion vectors of all the scene points\nfrom sequence of stereo images. The training for direct estimation of scene\nflow is done using consecutive pairs of stereo images and corresponding scene\nflow ground truth. The proposed architecture is applied on a huge dataset and\nprovides meaningful results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:26:55 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Thakur", "Ravi Kumar", ""], ["Mukherjee", "Snehasis", ""]]}, {"id": "1807.03470", "submitter": "Jianrui Cai", "authors": "Jianrui Cai, Zisheng Cao, and Lei Zhang", "title": "Learning a Single Tucker Decomposition Network for Lossy Image\n  Compression with Multiple Bits-Per-Pixel Rates", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression (LIC), which aims to utilize inexact approximations\nto represent an image more compactly, is a classical problem in image\nprocessing. Recently, deep convolutional neural networks (CNNs) have achieved\ninteresting results in LIC by learning an encoder-quantizer-decoder network\nfrom a large amount of data. However, existing CNN-based LIC methods usually\ncan only train a network for a specific bits-per-pixel (bpp). Such a \"one\nnetwork per bpp\" problem limits the generality and flexibility of CNNs to\npractical LIC applications. In this paper, we propose to learn a single CNN\nwhich can perform LIC at multiple bpp rates. A simple yet effective Tucker\nDecomposition Network (TDNet) is developed, where there is a novel tucker\ndecomposition layer (TDL) to decompose a latent image representation into a set\nof projection matrices and a core tensor. By changing the rank of the core\ntensor and its quantization, we can easily adjust the bpp rate of latent image\nrepresentation within a single CNN. Furthermore, an iterative non-uniform\nquantization scheme is presented to optimize the quantizer, and a\ncoarse-to-fine training strategy is introduced to reconstruct the decompressed\nimages. Extensive experiments demonstrate the state-of-the-art compression\nperformance of TDNet in terms of both PSNR and MS-SSIM indices.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:40:36 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Cai", "Jianrui", ""], ["Cao", "Zisheng", ""], ["Zhang", "Lei", ""]]}, {"id": "1807.03477", "submitter": "Tom Needham", "authors": "Tom Needham", "title": "Shape analysis of framed space curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the elastic shape analysis approach to shape matching and object\nclassification, plane curves are represented as points in an\ninfinite-dimensional Riemannian manifold, wherein shape dissimilarity is\nmeasured by geodesic distance. A remarkable result of Younes, Michor, Shah and\nMumford says that the space of closed planar shapes, endowed with a natural\nmetric, is isometric to an infinite-dimensional Grassmann manifold via the\nso-called square root transform. This result facilitates efficient shape\ncomparison by virtue of explicit descriptions of Grassmannian geodesics. In\nthis paper, we extend this shape analysis framework to treat shapes of framed\nspace curves. By considering framed curves, we are able to generalize the\nsquare root transform by using quaternionic arithmetic and properties of the\nHopf fibration. Under our coordinate transformation, the space of closed framed\ncurves corresponds to an infinite-dimensional complex Grassmannian. This allows\nus to describe geodesics in framed curve space explicitly. We are also able to\nproduce explicit geodesics between closed, unframed space curves by studying\nthe action of the loop group of the circle on the Grassmann manifold. Averages\nof collections of plane and space curves are computed via a novel algorithm\nutilizing flag means.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:33:42 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Needham", "Tom", ""]]}, {"id": "1807.03478", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "An Adaptive Learning Method of Restricted Boltzmann Machine by Neuron\n  Generation and Annihilation Algorithm", "comments": "6 pages, 6 figures", "journal-ref": "Proc. of 2016 IEEE International Conference on Systems, Man, and\n  Cybernetics (IEEE SMC 2016)", "doi": "10.1109/SMC.2016.7844417", "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann Machine (RBM) is a generative stochastic energy-based\nmodel of artificial neural network for unsupervised learning. Recently, RBM is\nwell known to be a pre-training method of Deep Learning. In addition to visible\nand hidden neurons, the structure of RBM has a number of parameters such as the\nweights between neurons and the coefficients for them. Therefore, we may meet\nsome difficulties to determine an optimal network structure to analyze big\ndata. In order to evade the problem, we investigated the variance of parameters\nto find an optimal structure during learning. For the reason, we should check\nthe variance of parameters to cause the fluctuation for energy function in RBM\nmodel. In this paper, we propose the adaptive learning method of RBM that can\ndiscover an optimal number of hidden neurons according to the training\nsituation by applying the neuron generation and annihilation algorithm. In this\nmethod, a new hidden neuron is generated if the energy function is not still\nconverged and the variance of the parameters is large. Moreover, the\ninactivated hidden neuron will be annihilated if the neuron does not affect the\nlearning situation. The experimental results for some benchmark data sets were\ndiscussed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:39:18 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:06:52 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03480", "submitter": "De-An Huang", "authors": "De-An Huang, Suraj Nair, Danfei Xu, Yuke Zhu, Animesh Garg, Li\n  Fei-Fei, Silvio Savarese, Juan Carlos Niebles", "title": "Neural Task Graphs: Generalizing to Unseen Tasks from a Single Video\n  Demonstration", "comments": "CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to generate a policy to complete an unseen task given just a\nsingle video demonstration of the task in a given domain. We hypothesize that\nto successfully generalize to unseen complex tasks from a single video\ndemonstration, it is necessary to explicitly incorporate the compositional\nstructure of the tasks into the model. To this end, we propose Neural Task\nGraph (NTG) Networks, which use conjugate task graph as the intermediate\nrepresentation to modularize both the video demonstration and the derived\npolicy. We empirically show NTG achieves inter-task generalization on two\ncomplex tasks: Block Stacking in BulletPhysics and Object Collection in\nAI2-THOR. NTG improves data efficiency with visual input as well as achieve\nstrong generalization without the need for dense hierarchical supervision. We\nfurther show that similar performance trends hold when applied to real-world\ndata. We show that NTG can effectively predict task structure on the JIGSAWS\nsurgical dataset and generalize to unseen tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 04:55:45 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 21:56:52 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Huang", "De-An", ""], ["Nair", "Suraj", ""], ["Xu", "Danfei", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""], ["Niebles", "Juan Carlos", ""]]}, {"id": "1807.03486", "submitter": "Takumi Ichimura", "authors": "Shin Kamada, Takumi Ichimura", "title": "An Adaptive Learning Method of Deep Belief Network by Layer Generation\n  Algorithm", "comments": "4 pages, 2 figures, Proc. of 2016 IEEE Region 10 Conference\n  (TENCON2016)", "journal-ref": null, "doi": "10.1109/TENCON.2016.7848589", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Belief Network (DBN) has a deep architecture that represents multiple\nfeatures of input patterns hierarchically with the pre-trained Restricted\nBoltzmann Machines (RBM). A traditional RBM or DBN model cannot change its\nnetwork structure during the learning phase. Our proposed adaptive learning\nmethod can discover the optimal number of hidden neurons and weights and/or\nlayers according to the input space. The model is an important method to take\naccount of the computational cost and the model stability. The regularities to\nhold the sparse structure of network is considerable problem, since the\nextraction of explicit knowledge from the trained network should be required.\nIn our previous research, we have developed the hybrid method of adaptive\nstructural learning method of RBM and Learning Forgetting method to the trained\nRBM. In this paper, we propose the adaptive learning method of DBN that can\ndetermine the optimal number of layers during the learning. We evaluated our\nproposed model on some benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 05:55:26 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:04:14 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kamada", "Shin", ""], ["Ichimura", "Takumi", ""]]}, {"id": "1807.03503", "submitter": "Daniel Barath", "authors": "Daniel Barath", "title": "Recovering affine features from orientation- and scale-invariant ones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach is proposed for recovering affine correspondences (ACs) from\norientation- and scale-invariant, e.g. SIFT, features. The method calculates\nthe affine parameters consistent with a pre-estimated epipolar geometry from\nthe point coordinates and the scales and rotations which the feature detector\nobtains. The closed-form solution is given as the roots of a quadratic\npolynomial equation, thus having two possible real candidates and fast\nprocedure, i.e. <1 millisecond. It is shown, as a possible application, that\nusing the proposed algorithm allows us to estimate a homography for every\nsingle correspondence independently. It is validated both in our synthetic\nenvironment and on publicly available real world datasets, that the proposed\ntechnique leads to accurate ACs. Also, the estimated homographies have similar\naccuracy to what the state-of-the-art methods obtain, but due to requiring only\na single correspondence, the robust estimation, e.g. by locally optimized\nRANSAC, is an order of magnitude faster.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 07:34:05 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Barath", "Daniel", ""]]}, {"id": "1807.03514", "submitter": "Zhihao Zhu", "authors": "Zhihao Zhu, Zhan Xue, Zejian Yuan", "title": "Topic-Guided Attention for Image Captioning", "comments": "Accepted by ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention mechanisms have attracted considerable interest in image captioning\nbecause of its powerful performance. Existing attention-based models use\nfeedback information from the caption generator as guidance to determine which\nof the image features should be attended to. A common defect of these attention\ngeneration methods is that they lack a higher-level guiding information from\nthe image itself, which sets a limit on selecting the most informative image\nfeatures. Therefore, in this paper, we propose a novel attention mechanism,\ncalled topic-guided attention, which integrates image topics in the attention\nmodel as a guiding information to help select the most important image\nfeatures. Moreover, we extract image features and image topics with separate\nnetworks, which can be fine-tuned jointly in an end-to-end manner during\ntraining. The experimental results on the benchmark Microsoft COCO dataset show\nthat our method yields state-of-art performance on various quantitative\nmetrics.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 07:59:42 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Zhu", "Zhihao", ""], ["Xue", "Zhan", ""], ["Yuan", "Zejian", ""]]}, {"id": "1807.03520", "submitter": "Matheus Gadelha", "authors": "Matheus Gadelha, Rui Wang and Subhransu Maji", "title": "Multiresolution Tree Networks for 3D Point Cloud Processing", "comments": "Accepted to ECCV 2018. 23 pages, including supplemental material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present multiresolution tree-structured networks to process point clouds\nfor 3D shape understanding and generation tasks. Our network represents a 3D\nshape as a set of locality-preserving 1D ordered list of points at multiple\nresolutions. This allows efficient feed-forward processing through 1D\nconvolutions, coarse-to-fine analysis through a multi-grid architecture, and it\nleads to faster convergence and small memory footprint during training. The\nproposed tree-structured encoders can be used to classify shapes and outperform\nexisting point-based architectures on shape classification benchmarks, while\ntree-structured decoders can be used for generating point clouds directly and\nthey outperform existing approaches for image-to-shape inference tasks learned\nusing the ShapeNet dataset. Our model also allows unsupervised learning of\npoint-cloud based shapes by using a variational autoencoder, leading to\nhigher-quality generated shapes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 08:28:01 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 20:19:30 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Gadelha", "Matheus", ""], ["Wang", "Rui", ""], ["Maji", "Subhransu", ""]]}, {"id": "1807.03528", "submitter": "Chongyi Li", "authors": "Saeed Anwar and Chongyi Li and Fatih Porikli", "title": "Deep Underwater Image Enhancement", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an underwater scene, wavelength-dependent light absorption and scattering\ndegrade the visibility of images, causing low contrast and distorted color\ncasts. To address this problem, we propose a convolutional neural network based\nimage enhancement model, i.e., UWCNN, which is trained efficiently using a\nsynthetic underwater image database. Unlike the existing works that require the\nparameters of underwater imaging model estimation or impose inflexible\nframeworks applicable only for specific scenes, our model directly reconstructs\nthe clear latent underwater image by leveraging on an automatic end-to-end and\ndata-driven training mechanism. Compliant with underwater imaging models and\noptical properties of underwater scenes, we first synthesize ten different\nmarine image databases. Then, we separately train multiple UWCNN models for\neach underwater image formation type. Experimental results on real-world and\nsynthetic underwater images demonstrate that the presented method generalizes\nwell on different underwater scenes and outperforms the existing methods both\nqualitatively and quantitatively. Besides, we conduct an ablation study to\ndemonstrate the effect of each component in our network.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 08:44:04 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Anwar", "Saeed", ""], ["Li", "Chongyi", ""], ["Porikli", "Fatih", ""]]}, {"id": "1807.03547", "submitter": "Chuhui Xue", "authors": "Chuhui Xue, Shijian Lu, Fangneng Zhan", "title": "Accurate Scene Text Detection through Border Semantics Awareness and\n  Bootstrapping", "comments": "14 pages, 8 figures, accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a scene text detection technique that exploits\nbootstrapping and text border semantics for accurate localization of texts in\nscenes. A novel bootstrapping technique is designed which samples multiple\n'subsections' of a word or text line and accordingly relieves the constraint of\nlimited training data effectively. At the same time, the repeated sampling of\ntext 'subsections' improves the consistency of the predicted text feature maps\nwhich is critical in predicting a single complete instead of multiple broken\nboxes for long words or text lines. In addition, a semantics-aware text border\ndetection technique is designed which produces four types of text border\nsegments for each scene text. With semantics-aware text borders, scene texts\ncan be localized more accurately by regressing text pixels around the ends of\nwords or text lines instead of all text pixels which often leads to inaccurate\nlocalization while dealing with long words or text lines. Extensive experiments\ndemonstrate the effectiveness of the proposed techniques, and superior\nperformance is obtained over several public datasets, e. g. 80.1 f-score for\nthe MSRA-TD500, 67.1 f-score for the ICDAR2017-RCTW, etc.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 09:26:07 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 05:21:48 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 05:39:57 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Xue", "Chuhui", ""], ["Lu", "Shijian", ""], ["Zhan", "Fangneng", ""]]}, {"id": "1807.03587", "submitter": "Richeng Tan", "authors": "Richeng Tan and Jing Li", "title": "Two-stage iterative Procrustes match algorithm and its application for\n  VQ-based speaker verification", "comments": "Submitted in ICMV 2018, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, Vector Quantization (VQ) model has been very popular\nacross different pattern recognition areas, especially for feature-based tasks.\nHowever, the classification or regression performance of VQ-based systems\nalways confronts the feature mismatch problem, which will heavily affect the\nperformance of them. In this paper, we propose a two-stage iterative Procrustes\nalgorithm (TIPM) to address the feature mismatch problem for VQ-based\napplications. At the first stage, the algorithm will remove mismatched feature\nvector pairs for a pair of input feature sets. Then, the second stage will\ncollect those correct matched feature pairs that were discarded during the\nfirst stage. To evaluate the effectiveness of the proposed TIPM algorithm,\nspeaker verification is used as the case study in this paper. The experiments\nwere conducted on the TIMIT database and the results show that TIPM can improve\nVQ-based speaker verification performance clean condition and all noisy\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 12:15:54 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Tan", "Richeng", ""], ["Li", "Jing", ""]]}, {"id": "1807.03594", "submitter": "Emanuel Aldea", "authors": "Sylvie Le H\\'egarat-Mascle, Emanuel Aldea, Jennifer Vandoni", "title": "Efficient Evaluation of the Number of False Alarm Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for computing efficiently the significance of a\nparametric pattern inside a binary image. On the one hand, a-contrario\nstrategies avoid the user involvement for tuning detection thresholds, and\nallow one to account fairly for different pattern sizes. On the other hand,\na-contrario criteria become intractable when the pattern complexity in terms of\nparametrization increases. In this work, we introduce a strategy which relies\non the use of a cumulative space of reduced dimensionality, derived from the\ncoupling of a classic (Hough) cumulative space with an integral histogram\ntrick. This space allows us to store partial computations which are required by\nthe a-contrario criterion, and to evaluate the significance with a lower\ncomputational cost than by following a straightforward approach. The method is\nillustrated on synthetic examples on patterns with various parametrizations up\nto five dimensions. In order to demonstrate how to apply this generic concept\nin a real scenario, we consider a difficult crack detection task in still\nimages, which has been addressed in the literature with various local and\nglobal detection strategies. We model cracks as bounded segments, detected by\nthe proposed a-contrario criterion, which allow us to introduce additional\nspatial constraints based on their relative alignment. On this application, the\nproposed strategy yields state-of the-art results, and underlines its potential\nfor handling complex pattern detection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 12:37:14 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["H\u00e9garat-Mascle", "Sylvie Le", ""], ["Aldea", "Emanuel", ""], ["Vandoni", "Jennifer", ""]]}, {"id": "1807.03602", "submitter": "Jianlong Wu", "authors": "Jianlong Wu, Zhouchen Lin, Hongbin Zha", "title": "Essential Tensor Learning for Multi-view Spectral Clustering", "comments": "Accepted by IEEE Transactions on Image Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2916740", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view clustering attracts much attention recently, which aims to take\nadvantage of multi-view information to improve the performance of clustering.\nHowever, most recent work mainly focus on self-representation based subspace\nclustering, which is of high computation complexity. In this paper, we focus on\nthe Markov chain based spectral clustering method and propose a novel essential\ntensor learning method to explore the high order correlations for multi-view\nrepresentation. We first construct a tensor based on multi-view transition\nprobability matrices of the Markov chain. By incorporating the idea from robust\nprinciple component analysis, tensor singular value decomposition (t-SVD) based\ntensor nuclear norm is imposed to preserve the low-rank property of the\nessential tensor, which can well capture the principle information from\nmultiple views. We also employ the tensor rotation operator for this task to\nbetter investigate the relationship among views as well as reduce the\ncomputation complexity. The proposed method can be efficiently optimized by the\nalternating direction method of multipliers~(ADMM). Extensive experiments on\nsix real world datasets corresponding to five different applications show that\nour method achieves superior performance over other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:01:48 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 14:10:05 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Wu", "Jianlong", ""], ["Lin", "Zhouchen", ""], ["Zha", "Hongbin", ""]]}, {"id": "1807.03613", "submitter": "Shenghua He", "authors": "Shenghua He, Jie Zheng, Akiko Maehara, Gary Mintz, Dalin Tang, Mark\n  Anastasio, Hua Li", "title": "Convolutional neural network based automatic plaque characterization\n  from intracoronary optical coherence tomography images", "comments": "SPIE 2018", "journal-ref": null, "doi": "10.1117/12.2293957", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) can provide high-resolution\ncross-sectional images for analyzing superficial plaques in coronary arteries.\nCommonly, plaque characterization using intra-coronary OCT images is performed\nmanually by expert observers. This manual analysis is time consuming and its\naccuracy heavily relies on the experience of human observers. Traditional\nmachine learning based methods, such as the least squares support vector\nmachine and random forest methods, have been recently employed to automatically\ncharacterize plaque regions in OCT images. Several processing steps, including\nfeature extraction, informative feature selection, and final pixel\nclassification, are commonly used in these traditional methods. Therefore, the\nfinal classification accuracy can be jeopardized by error or inaccuracy within\neach of these steps. In this study, we proposed a convolutional neural network\n(CNN) based method to automatically characterize plaques in OCT images. Unlike\ntraditional methods, our method uses the image as a direct input and performs\nclassification as a single-step process. The experiments on 269 OCT images\nshowed that the average prediction accuracy of CNN-based method was 0.866,\nwhich indicated a great promise for clinical translation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:18:21 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["He", "Shenghua", ""], ["Zheng", "Jie", ""], ["Maehara", "Akiko", ""], ["Mintz", "Gary", ""], ["Tang", "Dalin", ""], ["Anastasio", "Mark", ""], ["Li", "Hua", ""]]}, {"id": "1807.03651", "submitter": "Nils Gessert", "authors": "Omer Rajput, Nils Gessert, Martin Gromniak, Lars Matth\\\"aus, Alexander\n  Schlaefer", "title": "Towards Head Motion Compensation Using Multi-Scale Convolutional Neural\n  Networks", "comments": "Presented at CURAC 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head pose estimation and tracking is useful in variety of medical\napplications. With the advent of RGBD cameras like Kinect, it has become\nfeasible to do markerless tracking by estimating the head pose directly from\nthe point clouds. One specific medical application is robot assisted\ntranscranial magnetic stimulation (TMS) where any patient motion is compensated\nwith the help of a robot. For increased patient comfort, it is important to\ntrack the head without markers. In this regard, we address the head pose\nestimation problem using two different approaches. In the first approach, we\nbuild upon the more traditional approach of model based head tracking, where a\nhead model is morphed according to the particular head to be tracked and the\nmorphed model is used to track the head in the point cloud streams. In the\nsecond approach, we propose a new multi-scale convolutional neural network\narchitecture for more accurate pose regression. Additionally, we outline a\nsystematic data set acquisition strategy using a head phantom mounted on the\nrobot and ground-truth labels generated using a highly accurate tracking\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:57:58 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Rajput", "Omer", ""], ["Gessert", "Nils", ""], ["Gromniak", "Martin", ""], ["Matth\u00e4us", "Lars", ""], ["Schlaefer", "Alexander", ""]]}, {"id": "1807.03658", "submitter": "Xiangxi Shi", "authors": "Xiangxi Shi, Jianfei Cai, Jiuxiang Gu, Shafiq Joty", "title": "Video Captioning with Boundary-aware Hierarchical Language Decoding and\n  Joint Video Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of video data on the internet requires effective and efficient\ntechnology to generate captions automatically for people who are not able to\nwatch the videos. Despite the great progress of video captioning research,\nparticularly on video feature encoding, the language decoder is still largely\nbased on the prevailing RNN decoder such as LSTM, which tends to prefer the\nfrequent word that aligns with the video. In this paper, we propose a\nboundary-aware hierarchical language decoder for video captioning, which\nconsists of a high-level GRU based language decoder, working as a global\n(caption-level) language model, and a low-level GRU based language decoder,\nworking as a local (phrase-level) language model. Most importantly, we\nintroduce a binary gate into the low-level GRU language decoder to detect the\nlanguage boundaries. Together with other advanced components including joint\nvideo prediction, shared soft attention, and boundary-aware video encoding, our\nintegrated video captioning framework can discover hierarchical language\ninformation and distinguish the subject and the object in a sentence, which are\nusually confusing during the language generation. Extensive experiments on two\nwidely-used video captioning datasets, MSR-Video-to-Text (MSR-VTT)\n\\cite{xu2016msr} and YouTube-to-Text (MSVD) \\cite{chen2011collecting} show that\nour method is highly competitive, compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 08:49:34 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Shi", "Xiangxi", ""], ["Cai", "Jianfei", ""], ["Gu", "Jiuxiang", ""], ["Joty", "Shafiq", ""]]}, {"id": "1807.03694", "submitter": "Aamir Adam A. M. Adam", "authors": "R. M. Farouk, M. E. Abd El-aziz, A. M. Adam", "title": "Sparse Representation and Non-Negative Matrix Factorization for image\n  denoise", "comments": null, "journal-ref": "Journal of computer science approaches; Vol.4, Issue 2 Pages\n  20-27;2017", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the problem of blind image separation has been widely investigated,\nespecially the medical image denoise which is the main step in medical\ndiag-nosis. Removing the noise without affecting relevant features of the image\nis the main goal. Sparse decomposition over redundant dictionaries become of\nthe most used approaches to solve this problem. NMF codes naturally favor\nsparse, parts-based representations. In sparse representation, signals\nrepresented as a linear combination of a redundant dictionary atoms. In this\npaper, we propose an algorithm based on sparse representation over the\nredundant dictionary and Non-Negative Matrix Factorization (N-NMF). The\nalgorithm initializes a dic-tionary based on training samples constructed from\nnoised image, then it searches for the best representation for the source by\nusing the approximate matching pursuit (AMP). The proposed N-NMF gives a better\nreconstruction of an image from denoised one. We have compared our numerical\nresults with different image denoising techniques and we have found the\nperformance of the proposed technique is promising. Keywords: Image denoising,\nsparse representation, dictionary learning, matching pursuit, non-negative\nmatrix factorization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 13:02:46 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Farouk", "R. M.", ""], ["El-aziz", "M. E. Abd", ""], ["Adam", "A. M.", ""]]}, {"id": "1807.03770", "submitter": "Robert Rudolph", "authors": "Robert Rudolph, Katja Herzog, Reinhard T\\\"opfer, Volker Steinhage", "title": "Efficient identification, localization and quantification of grapevine\n  inflorescences in unprepared field images using Fully Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yield and its prediction is one of the most important tasks in grapevine\nbreeding purposes and vineyard management. Commonly, this trait is estimated\nmanually right before harvest by extrapolation, which mostly is\nlabor-intensive, destructive and inaccurate. In the present study an automated\nimage-based workflow was developed quantifying inflorescences and single\nflowers in unprepared field images of grapevines, i.e. no artificial background\nor light was applied. It is a novel approach for non-invasive, inexpensive and\nobjective phenotyping with high-throughput.\n  First, image regions depicting inflorescences were identified and localized.\nThis was done by segmenting the images into the classes \"inflorescence\" and\n\"non-inflorescence\" using a Fully Convolutional Network (FCN). Efficient image\nsegmentation hereby is the most challenging step regarding the small geometry\nand dense distribution of flowers (several hundred flowers per inflorescence),\nsimilar color of all plant organs in the fore- and background as well as the\ncircumstance that only approximately 5% of an image show inflorescences. The\ntrained FCN achieved a mean Intersection Over Union (IOU) of 87.6% on the test\ndata set. Finally, individual flowers were extracted from the\n\"inflorescence\"-areas using Circular Hough Transform. The flower extraction\nachieved a recall of 80.3% and a precision of 70.7% using the segmentation\nderived by the trained FCN model.\n  Summarized, the presented approach is a promising strategy in order to\npredict yield potential automatically in the earliest stage of grapevine\ndevelopment which is applicable for objective monitoring and evaluations of\nbreeding material, genetic repositories or commercial vineyards.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 17:46:40 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Rudolph", "Robert", ""], ["Herzog", "Katja", ""], ["T\u00f6pfer", "Reinhard", ""], ["Steinhage", "Volker", ""]]}, {"id": "1807.03776", "submitter": "Tairui Wang", "authors": "Xiaodan Liang, Tairui Wang, Luona Yang, Eric Xing", "title": "CIRL: Controllable Imitative Reinforcement Learning for Vision-based\n  Self-driving", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous urban driving navigation with complex multi-agent dynamics is\nunder-explored due to the difficulty of learning an optimal driving policy. The\ntraditional modular pipeline heavily relies on hand-designed rules and the\npre-processing perception system while the supervised learning-based models are\nlimited by the accessibility of extensive human experience. We present a\ngeneral and principled Controllable Imitative Reinforcement Learning (CIRL)\napproach which successfully makes the driving agent achieve higher success\nrates based on only vision inputs in a high-fidelity car simulator. To\nalleviate the low exploration efficiency for large continuous action space that\noften prohibits the use of classical RL on challenging real tasks, our CIRL\nexplores over a reasonably constrained action space guided by encoded\nexperiences that imitate human demonstrations, building upon Deep Deterministic\nPolicy Gradient (DDPG). Moreover, we propose to specialize adaptive policies\nand steering-angle reward designs for different control signals (i.e. follow,\nstraight, turn right, turn left) based on the shared representations to improve\nthe model capability in tackling with diverse cases. Extensive experiments on\nCARLA driving benchmark demonstrate that CIRL substantially outperforms all\nprevious methods in terms of the percentage of successfully completed episodes\non a variety of goal-directed driving tasks. We also show its superior\ngeneralization capability in unseen environments. To our knowledge, this is the\nfirst successful case of the learned driving policy through reinforcement\nlearning in the high-fidelity simulator, which performs better-than supervised\nimitation learning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 17:56:22 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Liang", "Xiaodan", ""], ["Wang", "Tairui", ""], ["Yang", "Luona", ""], ["Xing", "Eric", ""]]}, {"id": "1807.03845", "submitter": "Sampurna Biswas", "authors": "Sampurna Biswas, Hemant K. Aggarwal, Sunrita Poddar, and Mathews Jacob", "title": "Model-based free-breathing cardiac MRI reconstruction using deep learned\n  \\& STORM priors: MoDL-STORM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a model-based reconstruction framework with deep learned (DL)\nand smoothness regularization on manifolds (STORM) priors to recover free\nbreathing and ungated (FBU) cardiac MRI from highly undersampled measurements.\nThe DL priors enable us to exploit the local correlations, while the STORM\nprior enables us to make use of the extensive non-local similarities that are\nsubject dependent. We introduce a novel model-based formulation that allows the\nseamless integration of deep learning methods with available prior information,\nwhich current deep learning algorithms are not capable of. The experimental\nresults demonstrate the preliminary potential of this work in accelerating FBU\ncardiac MRI.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:04:14 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Biswas", "Sampurna", ""], ["Aggarwal", "Hemant K.", ""], ["Poddar", "Sunrita", ""], ["Jacob", "Mathews", ""]]}, {"id": "1807.03848", "submitter": "Chun-Fu (Richard) Chen", "authors": "Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, Rogerio Feris", "title": "Big-Little Net: An Efficient Multi-Scale Feature Representation for\n  Visual and Speech Recognition", "comments": "git repo: https://github.com/IBM/BigLittleNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel Convolutional Neural Network (CNN)\narchitecture for learning multi-scale feature representations with good\ntradeoffs between speed and accuracy. This is achieved by using a multi-branch\nnetwork, which has different computational complexity at different branches.\nThrough frequent merging of features from branches at distinct scales, our\nmodel obtains multi-scale features while using less computation. The proposed\napproach demonstrates improvement of model efficiency and performance on both\nobject recognition and speech recognition tasks,using popular architectures\nincluding ResNet and ResNeXt. For object recognition, our approach reduces\ncomputation by 33% on object recognition while improving accuracy with 0.9%.\nFurthermore, our model surpasses state-of-the-art CNN acceleration approaches\nby a large margin in accuracy and FLOPs reduction. On the task of speech\nrecognition, our proposed multi-scale CNNs save 30% FLOPs with slightly better\nword error rates, showing good generalization across domains. The codes are\navailable at https://github.com/IBM/BigLittleNet\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 20:19:27 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 17:59:06 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 02:06:37 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Chen", "Chun-Fu", ""], ["Fan", "Quanfu", ""], ["Mallinar", "Neil", ""], ["Sercu", "Tom", ""], ["Feris", "Rogerio", ""]]}, {"id": "1807.03871", "submitter": "Tianlang Chen", "authors": "Tianlang Chen, Zhongping Zhang, Quanzeng You, Chen Fang, Zhaowen Wang,\n  Hailin Jin, Jiebo Luo", "title": "\"Factual\" or \"Emotional\": Stylized Image Captioning with Adaptive\n  Learning and Attention", "comments": "17 pages, 7 figures, ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating stylized captions for an image is an emerging topic in image\ncaptioning. Given an image as input, it requires the system to generate a\ncaption that has a specific style (e.g., humorous, romantic, positive, and\nnegative) while describing the image content semantically accurately. In this\npaper, we propose a novel stylized image captioning model that effectively\ntakes both requirements into consideration. To this end, we first devise a new\nvariant of LSTM, named style-factual LSTM, as the building block of our model.\nIt uses two groups of matrices to capture the factual and stylized knowledge,\nrespectively, and automatically learns the word-level weights of the two groups\nbased on previous context. In addition, when we train the model to capture\nstylized elements, we propose an adaptive learning approach based on a\nreference factual model, it provides factual knowledge to the model as the\nmodel learns from stylized caption labels, and can adaptively compute how much\ninformation to supply at each time step. We evaluate our model on two stylized\nimage captioning datasets, which contain humorous/romantic captions and\npositive/negative captions, respectively. Experiments shows that our proposed\nmodel outperforms the state-of-the-art approaches, without using extra ground\ntruth supervision.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 21:33:22 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 22:29:21 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2018 22:01:26 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Tianlang", ""], ["Zhang", "Zhongping", ""], ["You", "Quanzeng", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["Jin", "Hailin", ""], ["Luo", "Jiebo", ""]]}, {"id": "1807.03887", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Sergey Rodionov, Maxim Peterson, Oleg Shcherbakov,\n  Innokentii Zhdanov, Nikolai Skorobogatko", "title": "Vision System for AGI: Problems and Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What frameworks and architectures are necessary to create a vision system for\nAGI? In this paper, we propose a formal model that states the task of\nperception within AGI. We show the role of discriminative and generative models\nin achieving efficient and general solution of this task, thus specifying the\ntask in more detail. We discuss some existing generative and discriminative\nmodels and demonstrate their insufficiency for our purposes. Finally, we\ndiscuss some architectural dilemmas and open questions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 22:12:52 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Potapov", "Alexey", ""], ["Rodionov", "Sergey", ""], ["Peterson", "Maxim", ""], ["Shcherbakov", "Oleg", ""], ["Zhdanov", "Innokentii", ""], ["Skorobogatko", "Nikolai", ""]]}, {"id": "1807.03903", "submitter": "Nikolaos Sarafianos", "authors": "Nikolaos Sarafianos and Xiang Xu and Ioannis A. Kakadiaris", "title": "Deep Imbalanced Attribute Classification using Visual Attention\n  Aggregation", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many computer vision applications, such as image description and human\nidentification, recognizing the visual attributes of humans is an essential yet\nchallenging problem. Its challenges originate from its multi-label nature, the\nlarge underlying class imbalance and the lack of spatial annotations. Existing\nmethods follow either a computer vision approach while failing to account for\nclass imbalance, or explore machine learning solutions, which disregard the\nspatial and semantic relations that exist in the images. With that in mind, we\npropose an effective method that extracts and aggregates visual attention masks\nat different scales. We introduce a loss function to handle class imbalance\nboth at class and at an instance level and further demonstrate that penalizing\nattention masks with high prediction variance accounts for the weak supervision\nof the attention mechanism. By identifying and addressing these challenges, we\nachieve state-of-the-art results with a simple attention mechanism in both PETA\nand WIDER-Attribute datasets without additional context or side information.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 23:49:03 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 02:12:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Sarafianos", "Nikolaos", ""], ["Xu", "Xiang", ""], ["Kakadiaris", "Ioannis A.", ""]]}, {"id": "1807.03923", "submitter": "Guoqiang Zhong", "authors": "Guoqiang Zhong, Wei Gao, Yongbin Liu, Youzhao Yang", "title": "Generative Adversarial Networks with Decoder-Encoder Output Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research on image generation methods has been developing\nfast. The auto-encoding variational Bayes method (VAEs) was proposed in 2013,\nwhich uses variational inference to learn a latent space from the image\ndatabase and then generates images using the decoder. The generative\nadversarial networks (GANs) came out as a promising framework, which uses\nadversarial training to improve the generative ability of the generator.\nHowever, the generated pictures by GANs are generally blurry. The deep\nconvolutional generative adversarial networks (DCGANs) were then proposed to\nleverage the quality of generated images. Since the input noise vectors are\nrandomly sampled from a Gaussian distribution, the generator has to map from a\nwhole normal distribution to the images. This makes DCGANs unable to reflect\nthe inherent structure of the training data. In this paper, we propose a novel\ndeep model, called generative adversarial networks with decoder-encoder output\nnoise (DE-GANs), which takes advantage of both the adversarial training and the\nvariational Bayesain inference to improve the performance of image generation.\nDE-GANs use a pre-trained decoder-encoder architecture to map the random\nGaussian noise vectors to informative ones and pass them to the generator of\nthe adversarial networks. Since the decoder-encoder architecture is trained by\nthe same images as the generators, the output vectors could carry the intrinsic\ndistribution information of the original images. Moreover, the loss function of\nDE-GANs is different from GANs and DCGANs. A hidden-space loss function is\nadded to the adversarial loss function to enhance the robustness of the model.\nExtensive empirical results show that DE-GANs can accelerate the convergence of\nthe adversarial training process and improve the quality of the generated\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 01:50:17 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Zhong", "Guoqiang", ""], ["Gao", "Wei", ""], ["Liu", "Yongbin", ""], ["Yang", "Youzhao", ""]]}, {"id": "1807.03959", "submitter": "Chunhua Shen", "authors": "Ruibo Li, Ke Xian, Chunhua Shen, Zhiguo Cao, Hao Lu, Lingxiao Hang", "title": "Deep attention-based classification network for robust depth prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our deep attention-based classification (DABC)\nnetwork for robust single image depth prediction, in the context of the Robust\nVision Challenge 2018 (ROB 2018). Unlike conventional depth prediction, our\ngoal is to design a model that can perform well in both indoor and outdoor\nscenes with a single parameter set. However, robust depth prediction suffers\nfrom two challenging problems: a) How to extract more discriminative features\nfor different scenes (compared to a single scene)? b) How to handle the large\ndifferences of depth ranges between indoor and outdoor datasets? To address\nthese two problems, we first formulate depth prediction as a multi-class\nclassification task and apply a softmax classifier to classify the depth label\nof each pixel. We then introduce a global pooling layer and a channel-wise\nattention mechanism to adaptively select the discriminative channels of\nfeatures and to update the original features by assigning important channels\nwith higher weights. Further, to reduce the influence of quantization errors,\nwe employ a soft-weighted sum inference strategy for the final prediction.\nExperimental results on both indoor and outdoor datasets demonstrate the\neffectiveness of our method. It is worth mentioning that we won the 2-nd place\nin single image depth prediction entry of ROB 2018, in conjunction with IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) 2018.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 06:19:22 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Li", "Ruibo", ""], ["Xian", "Ke", ""], ["Shen", "Chunhua", ""], ["Cao", "Zhiguo", ""], ["Lu", "Hao", ""], ["Hang", "Lingxiao", ""]]}, {"id": "1807.04001", "submitter": "Ismail Elezi", "authors": "Benjamin Bruno Meier, Ismail Elezi, Mohammadreza Amirian, Oliver Durr\n  and Thilo Stadelmann", "title": "Learning Neural Models for End-to-End Clustering", "comments": "Accepted for publication on ANNPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel end-to-end neural network architecture that, once trained,\ndirectly outputs a probabilistic clustering of a batch of input examples in one\npass. It estimates a distribution over the number of clusters $k$, and for each\n$1 \\leq k \\leq k_\\mathrm{max}$, a distribution over the individual cluster\nassignment for each data point. The network is trained in advance in a\nsupervised fashion on separate data to learn grouping by any perceptual\nsimilarity criterion based on pairwise labels (same/different group). It can\nthen be applied to different data containing different groups. We demonstrate\npromising performance on high-dimensional data like images (COIL-100) and\nspeech (TIMIT). We call this ``learning to cluster'' and show its conceptual\ndifference to deep metric learning, semi-supervise clustering and other related\napproaches while having the advantage of performing learnable clustering fully\nend-to-end.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 08:45:45 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Meier", "Benjamin Bruno", ""], ["Elezi", "Ismail", ""], ["Amirian", "Mohammadreza", ""], ["Durr", "Oliver", ""], ["Stadelmann", "Thilo", ""]]}, {"id": "1807.04047", "submitter": "Hongyu Li", "authors": "Hongyu Li, Fan Zhu, Junhua Qiu", "title": "CG-DIQA: No-reference Document Image Quality Assessment Based on\n  Character Gradient", "comments": "To be published in Proc. of ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document image quality assessment (DIQA) is an important and challenging\nproblem in real applications. In order to predict the quality scores of\ndocument images, this paper proposes a novel no-reference DIQA method based on\ncharacter gradient, where the OCR accuracy is used as a ground-truth quality\nmetric. Character gradient is computed on character patches detected with the\nmaximally stable extremal regions (MSER) based method. Character patches are\nessentially significant to character recognition and therefore suitable for use\nin estimating document image quality. Experiments on a benchmark dataset show\nthat the proposed method outperforms the state-of-the-art methods in estimating\nthe quality score of document images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:00:03 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Li", "Hongyu", ""], ["Zhu", "Fan", ""], ["Qiu", "Junhua", ""]]}, {"id": "1807.04049", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Perception of Image Features in Post-Mortem Iris Recognition: Humans vs\n  Machines", "comments": "Accepted for the 10th IEEE International COnference on Biometrics:\n  Theory, Applications and Systems (BTAS2019), 23-26 Sept 2019, Tampa, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-mortem iris recognition can offer an additional forensic method of\npersonal identification. However, in contrary to already well-established human\nexamination of fingerprints, making iris recognition human-interpretable is\nharder, and therefore it has never been applied in forensic proceedings. There\nis no strong consensus among biometric experts which iris features, especially\nthose in iris images acquired post-mortem, are the most important for human\nexperts solving an iris recognition task. This paper explores two ways of\nbroadening this knowledge: (a) with an eye tracker, the salient features used\nby humans comparing iris images on a screen are extracted, and (b)\nclass-activation maps produced by the convolutional neural network solving the\niris recognition task are analyzed. Both humans and deep learning-based\nsolutions were examined with the same set of iris image pairs. This made it\npossible to compare the attention maps and conclude that (a) deep\nlearning-based method can offer human-interpretable decisions backed by visual\nexplanations pointing a human examiner to salient regions, and (b) in many\ncases humans and a machine used different features, what means that a deep\nlearning-based method can offer a complementary support to human experts. This\npaper offers the first known to us human-interpretable comparison of\nmachine-based and human-based post-mortem iris recognition, and the trained\nmodels annotating salient iris image regions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:01:07 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:02:09 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 09:06:54 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1807.04050", "submitter": "Roberto Annunziata", "authors": "Roberto Annunziata, Christos Sagonas, Jacques Cal\\`i", "title": "DeSTNet: Densely Fused Spatial Transformer Networks", "comments": "Accepted for publication at the 29th British Machine Vision\n  Conference (BMVC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Convolutional Neural Networks (CNN) are extremely powerful on a range\nof computer vision tasks. However, their performance may degrade when the data\nis characterised by large intra-class variability caused by spatial\ntransformations. The Spatial Transformer Network (STN) is currently the method\nof choice for providing CNNs the ability to remove those transformations and\nimprove performance in an end-to-end learning framework. In this paper, we\npropose Densely Fused Spatial Transformer Network (DeSTNet), which, to our best\nknowledge, is the first dense fusion pattern for combining multiple STNs.\nSpecifically, we show how changing the connectivity pattern of multiple STNs\nfrom sequential to dense leads to more powerful alignment modules. Extensive\nexperiments on three benchmarks namely, MNIST, GTSRB, and IDocDB show that the\nproposed technique outperforms related state-of-the-art methods (i.e., STNs and\nCSTNs) both in terms of accuracy and robustness.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:06:32 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 10:27:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Annunziata", "Roberto", ""], ["Sagonas", "Christos", ""], ["Cal\u00ec", "Jacques", ""]]}, {"id": "1807.04056", "submitter": "Nicolo' Savioli", "authors": "Nicolo' Savioli, Silvia Visentin, Erich Cosmi, Enrico Grisan, Pablo\n  Lamata, Giovanni Montana", "title": "Temporal Convolution Networks for Real-Time Abdominal Fetal Aorta\n  Analysis with Ultrasound", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The automatic analysis of ultrasound sequences can substantially improve the\nefficiency of clinical diagnosis. In this work we present our attempt to\nautomate the challenging task of measuring the vascular diameter of the fetal\nabdominal aorta from ultrasound images. We propose a neural network\narchitecture consisting of three blocks: a convolutional layer for the\nextraction of imaging features, a Convolution Gated Recurrent Unit (C-GRU) for\nenforcing the temporal coherence across video frames and exploiting the\ntemporal redundancy of a signal, and a regularized loss function, called\n\\textit{CyclicLoss}, to impose our prior knowledge about the periodicity of the\nobserved signal. We present experimental evidence suggesting that the proposed\narchitecture can reach an accuracy substantially superior to previously\nproposed methods, providing an average reduction of the mean squared error from\n$0.31 mm^2$ (state-of-art) to $0.09 mm^2$, and a relative error reduction from\n$8.1\\%$ to $5.3\\%$. The mean execution speed of the proposed approach of 289\nframes per second makes it suitable for real time clinical use.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:22:38 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Savioli", "Nicolo'", ""], ["Visentin", "Silvia", ""], ["Cosmi", "Erich", ""], ["Grisan", "Enrico", ""], ["Lamata", "Pablo", ""], ["Montana", "Giovanni", ""]]}, {"id": "1807.04058", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka and Piotr Maciejewicz", "title": "Presentation Attack Detection for Cadaver Iris", "comments": "Accepted for publication at the 9th IEEE International Conference on\n  Biometrics: Theory, Applications, and Systems (BTAS 2018), Los Angeles, USA,\n  October 22-25, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep-learning-based method for iris presentation attack\ndetection (PAD) when iris images are obtained from deceased people. Our\napproach is based on the VGG-16 architecture fine-tuned with a database of 574\npost-mortem, near-infrared iris images from the\nWarsaw-BioBase-PostMortem-Iris-v1 database, complemented by a dataset of 256\nimages of live irises, collected within the scope of this study. Experiments\ndescribed in this paper show that our approach is able to correctly classify\niris images as either representing a live or a dead eye in almost 99% of the\ntrials, averaged over 20 subject-disjoint, train/test splits. We also show that\nthe post-mortem iris detection accuracy increases as time since death elapses,\nand that we are able to construct a classification system with\nAPCER=0%@BPCER=1% (Attack Presentation and Bona Fide Presentation\nClassification Error Rates, respectively) when only post-mortem samples\ncollected at least 16 hours post-mortem are considered. Since acquisitions of\nante- and post-mortem samples differ significantly, we applied countermeasures\nto minimize bias in our classification methodology caused by image properties\nthat are not related to the PAD. This included using the same iris sensor in\ncollection of ante- and post-mortem samples, and analysis of class activation\nmaps to ensure that discriminant iris regions utilized by our classifier are\nrelated to properties of the eye, and not to those of the acquisition protocol.\nThis paper offers the first known to us PAD method in a post-mortem setting,\ntogether with an explanation of the decisions made by the convolutional neural\nnetwork. Along with the paper we offer source codes, weights of the trained\nnetwork, and a dataset of live iris images to facilitate reproducibility and\nfurther research.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:35:22 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:46:59 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Maciejewicz", "Piotr", ""]]}, {"id": "1807.04061", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Ewelina Bartuzi", "title": "Cross-spectral Iris Recognition for Mobile Applications using\n  High-quality Color Images", "comments": null, "journal-ref": "Journal of Telecommunications and Information Technology, vol.\n  3/2016, pages 91-97", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent shift towards mobile computing, new challenges for biometric\nauthentication appear on the horizon. This paper provides a comprehensive study\nof cross-spectral iris recognition in a scenario, in which high quality color\nimages obtained with a mobile phone are used against enrollment images\ncollected in typical, near-infrared setups. Grayscale conversion of the color\nimages that employs selective RGB channel choice depending on the iris\ncoloration is shown to improve the recognition accuracy for some combinations\nof eye colors and matching software, when compared to using the red channel\nonly, with equal error rates driven down to as low as 2%. The authors are not\naware of any other paper focusing on cross-spectral iris recognition is a\nscenario with near-infrared enrollment using a professional iris recognition\nsetup and then a mobile-based verification employing color images.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:43:06 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Bartuzi", "Ewelina", ""]]}, {"id": "1807.04067", "submitter": "Muhammed Kocabas", "authors": "Muhammed Kocabas, Salih Karagoz, Emre Akbas", "title": "MultiPoseNet: Fast Multi-Person Pose Estimation using Pose Residual\n  Network", "comments": "to appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present MultiPoseNet, a novel bottom-up multi-person pose\nestimation architecture that combines a multi-task model with a novel\nassignment method. MultiPoseNet can jointly handle person detection, keypoint\ndetection, person segmentation and pose estimation problems. The novel\nassignment method is implemented by the Pose Residual Network (PRN) which\nreceives keypoint and person detections, and produces accurate poses by\nassigning keypoints to person instances. On the COCO keypoints dataset, our\npose estimation method outperforms all previous bottom-up methods both in\naccuracy (+4-point mAP over previous best result) and speed; it also performs\non par with the best top-down methods while being at least 4x faster. Our\nmethod is the fastest real time system with 23 frames/sec. Source code is\navailable at: https://github.com/mkocabas/pose-residual-network\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:56:49 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Kocabas", "Muhammed", ""], ["Karagoz", "Salih", ""], ["Akbas", "Emre", ""]]}, {"id": "1807.04093", "submitter": "Vladimir Rybalkin", "authors": "Vladimir Rybalkin, Alessandro Pappalardo, Muhammad Mohsin Ghaffar,\n  Giulio Gambardella, Norbert Wehn, Michaela Blott", "title": "FINN-L: Library Extensions and Design Trade-off Analysis for Variable\n  Precision LSTM Networks on FPGAs", "comments": "Accepted for publication, 28th International Conference on Field\n  Programmable Logic and Applications (FPL), August, 2018, Dublin, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many types of artificial neural networks, including\nrecurrent networks, can achieve a high classification accuracy even with\nlow-precision weights and activations. The reduction in precision generally\nyields much more efficient hardware implementations in regards to hardware\ncost, memory requirements, energy, and achievable throughput. In this paper, we\npresent the first systematic exploration of this design space as a function of\nprecision for Bidirectional Long Short-Term Memory (BiLSTM) neural network.\nSpecifically, we include an in-depth investigation of precision vs. accuracy\nusing a fully hardware-aware training flow, where during training quantization\nof all aspects of the network including weights, input, output and in-memory\ncell activations are taken into consideration. In addition, hardware resource\ncost, power consumption and throughput scalability are explored as a function\nof precision for FPGA-based implementations of BiLSTM, and multiple approaches\nof parallelizing the hardware. We provide the first open source HLS library\nextension of FINN for parameterizable hardware architectures of LSTM layers on\nFPGAs which offers full precision flexibility and allows for parameterizable\nperformance scaling offering different levels of parallelism within the\narchitecture. Based on this library, we present an FPGA-based accelerator for\nBiLSTM neural network designed for optical character recognition, along with\nnumerous other experimental proof points for a Zynq UltraScale+ XCZU7EV MPSoC\nwithin the given design space.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 11:52:59 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Rybalkin", "Vladimir", ""], ["Pappalardo", "Alessandro", ""], ["Ghaffar", "Muhammad Mohsin", ""], ["Gambardella", "Giulio", ""], ["Wehn", "Norbert", ""], ["Blott", "Michaela", ""]]}, {"id": "1807.04099", "submitter": "Huaibo Huang", "authors": "Huaibo Huang, Lingxiao Song, Ran He, Zhenan Sun, Tieniu Tan", "title": "Variational Capsules for Image Analysis and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A capsule is a group of neurons whose activity vector models different\nproperties of the same entity. This paper extends the capsule to a generative\nversion, named variational capsules (VCs). Each VC produces a latent variable\nfor a specific entity, making it possible to integrate image analysis and image\nsynthesis into a unified framework. Variational capsules model an image as a\ncomposition of entities in a probabilistic model. Different capsules'\ndivergence with a specific prior distribution represents the presence of\ndifferent entities, which can be applied in image analysis tasks such as\nclassification. In addition, variational capsules encode multiple entities in a\nsemantically-disentangling way. Diverse instantiations of capsules are related\nto various properties of the same entity, making it easy to generate diverse\nsamples with fine-grained semantic attributes. Extensive experiments\ndemonstrate that deep networks designed with variational capsules can not only\nachieve promising performance on image analysis tasks (including image\nclassification and attribute prediction) but can also improve the diversity and\ncontrollability of image synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 12:13:58 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Huang", "Huaibo", ""], ["Song", "Lingxiao", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1807.04154", "submitter": "Mateusz Trokielewicz", "authors": "Mateusz Trokielewicz and Adam Czajka", "title": "Data-Driven Segmentation of Post-mortem Iris Images", "comments": null, "journal-ref": "2018 International Workshop on Biometrics and Forensics (IWBF),\n  IEEE Xplore, 2018", "doi": "10.1109/IWBF.2018.8401558", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for segmenting iris images obtained from the\ndeceased subjects, by training a deep convolutional neural network (DCNN)\ndesigned for the purpose of semantic segmentation. Post-mortem iris recognition\nhas recently emerged as an alternative, or additional, method useful in\nforensic analysis. At the same time it poses many new challenges from the\ntechnological standpoint, one of them being the image segmentation stage, which\nhas proven difficult to be reliably executed by conventional iris recognition\nmethods. Our approach is based on the SegNet architecture, fine-tuned with\n1,300 manually segmented post-mortem iris images taken from the\nWarsaw-BioBase-Post-Mortem-Iris v1.0 database. The experiments presented in\nthis paper show that this data-driven solution is able to learn specific\ndeformations present in post-mortem samples, which are missing from alive\nirises, and offers a considerable improvement over the state-of-the-art,\nconventional segmentation algorithm (OSIRIS): the Intersection over Union (IoU)\nmetric was improved from 73.6% (for OSIRIS) to 83% (for DCNN-based presented in\nthis paper) averaged over subject-disjoint, multiple splits of the data into\ntrain and test subsets. This paper offers the first known to us method of\nautomatic processing of post-mortem iris images. We offer source codes with the\ntrained DCNN that perform end-to-end segmentation of post-mortem iris images,\nas described in this paper. Also, we offer binary masks corresponding to manual\nsegmentation of samples from Warsaw-BioBase-Post-Mortem-Iris v1.0 database to\nfacilitate development of alternative methods for post-mortem iris\nsegmentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:21:59 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""]]}, {"id": "1807.04169", "submitter": "Tomasz {\\L}uczy\\'nski", "authors": "Tomasz {\\L}uczy\\'nski and Andreas Birk", "title": "Underwater Image Haze Removal and Color Correction with an\n  Underwater-ready Dark Channel Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Underwater images suffer from extremely unfavourable conditions. Light is\nheavily attenuated and scattered. Attenuation creates change in hue, scattering\ncauses so called veiling light. General state of the art methods for enhancing\nimage quality are either unreliable or cannot be easily used in underwater\noperations. On the other hand there is a well known method for haze removal in\nair, called Dark Channel Prior. Even though there are known adaptations of this\nmethod to underwater applications, they do not always work correctly. This work\nelaborates and improves upon the initial concept presented in [1]. A\nmodification to the Dark Channel Prior is proposed that allows for an easy\napplication to underwater images. It is also shown that our method outperforms\ncompeting solutions based on the Dark Channel Prior. Experiments on real-life\ndata collected within the DexROV project are also presented, showing the\nrobustness and high performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:43:57 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["\u0141uczy\u0144ski", "Tomasz", ""], ["Birk", "Andreas", ""]]}, {"id": "1807.04170", "submitter": "Stephane Perrin", "authors": "St\\'ephane Perrin (LISTIC), Eric Benoit (LISTIC), Didier Coquin\n  (LISTIC)", "title": "Decision method choice in a human posture recognition context", "comments": null, "journal-ref": "Human-Computer Systems Interaction. Backgrounds and Applications\n  4, 4, 2018", "doi": "10.1007/978-3-319-62120-3_11", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human posture recognition provides a dynamic field that has produced many\nmethods. Using fuzzy subsets based data fusion methods to aggregate the results\ngiven by different types of recognition processes is a convenient way to\nimprove recognition methods. Nevertheless, choosing a defuzzification method to\nimple-ment the decision is a crucial point of this approach. The goal of this\npaper is to present an approach where the choice of the defuzzification method\nis driven by the constraints of the final data user, which are expressed as\nlimitations on indica-tors like confidence or accuracy. A practical\nexperimentation illustrating this ap-proach is presented: from a depth camera\nsensor, human posture is interpreted and the defuzzification method is selected\nin accordance with the constraints of the final information consumer. The paper\nillustrates the interest of the approach in a context of postures based human\nrobot communication.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:45:07 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Perrin", "St\u00e9phane", "", "LISTIC"], ["Benoit", "Eric", "", "LISTIC"], ["Coquin", "Didier", "", "LISTIC"]]}, {"id": "1807.04191", "submitter": "Bardia Doosti", "authors": "Bardia Doosti, Tao Dong, Biplab Deka, Jeffrey Nichols", "title": "A Computational Method for Evaluating UI Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UI design languages, such as Google's Material Design, make applications both\neasier to develop and easier to learn by providing a set of standard UI\ncomponents. Nonetheless, it is hard to assess the impact of design languages in\nthe wild. Moreover, designers often get stranded by strong-opinionated debates\naround the merit of certain UI components, such as the Floating Action Button\nand the Navigation Drawer. To address these challenges, this short paper\nintroduces a method for measuring the impact of design languages and informing\ndesign debates through analyzing a dataset consisting of view hierarchies,\nscreenshots, and app metadata for more than 9,000 mobile apps. Our data\nanalysis shows that use of Material Design is positively correlated to app\nratings, and to some extent, also the number of installs. Furthermore, we show\nthat use of UI components vary by app category, suggesting a more nuanced view\nneeded in design debates.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:21:59 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Doosti", "Bardia", ""], ["Dong", "Tao", ""], ["Deka", "Biplab", ""], ["Nichols", "Jeffrey", ""]]}, {"id": "1807.04200", "submitter": "Nicholas Lord", "authors": "Saumya Jetley, Nicholas A. Lord, Philip H.S. Torr", "title": "With Friends Like These, Who Needs Adversaries?", "comments": "Published in this form at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vulnerability of deep image classification networks to adversarial attack\nis now well known, but less well understood. Via a novel experimental analysis,\nwe illustrate some facts about deep convolutional networks for image\nclassification that shed new light on their behaviour and how it connects to\nthe problem of adversaries. In short, the celebrated performance of these\nnetworks and their vulnerability to adversarial attack are simply two sides of\nthe same coin: the input image-space directions along which the networks are\nmost vulnerable to attack are the same directions which they use to achieve\ntheir classification performance in the first place. We develop this result in\ntwo main steps. The first uncovers the fact that classes tend to be associated\nwith specific image-space directions. This is shown by an examination of the\nclass-score outputs of nets as functions of 1D movements along these\ndirections. This provides a novel perspective on the existence of universal\nadversarial perturbations. The second is a clear demonstration of the tight\ncoupling between classification performance and vulnerability to adversarial\nattack within the spaces spanned by these directions. Thus, our analysis\nresolves the apparent contradiction between accuracy and vulnerability. It\nprovides a new perspective on much of the prior art and reveals profound\nimplications for efforts to construct neural nets that are both accurate and\nrobust to adversarial attack.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:38:33 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 15:09:35 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 10:23:53 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 19:24:23 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Jetley", "Saumya", ""], ["Lord", "Nicholas A.", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1807.04219", "submitter": "Yandong Li", "authors": "Yandong Li, Liqiang Wang, Tianbao Yang, Boqing Gong", "title": "How Local is the Local Diversity? Reinforcing Sequential Determinantal\n  Point Processes with Dynamic Ground Sets for Supervised Video Summarization", "comments": null, "journal-ref": "European Conference on Computer Vision (ECCV 2018)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large volume of video content and high viewing frequency demand automatic\nvideo summarization algorithms, of which a key property is the capability of\nmodeling diversity. If videos are lengthy like hours-long egocentric videos, it\nis necessary to track the temporal structures of the videos and enforce local\ndiversity. The local diversity refers to that the shots selected from a short\ntime duration are diverse but visually similar shots are allowed to co-exist in\nthe summary if they appear far apart in the video. In this paper, we propose a\nnovel probabilistic model, built upon SeqDPP, to dynamically control the time\nspan of a video segment upon which the local diversity is imposed. In\nparticular, we enable SeqDPP to learn to automatically infer how local the\nlocal diversity is supposed to be from the input video. The resulting model is\nextremely involved to train by the hallmark maximum likelihood estimation\n(MLE), which further suffers from the exposure bias and non-differentiable\nevaluation metrics. To tackle these problems, we instead devise a reinforcement\nlearning algorithm for training the proposed model. Extensive experiments\nverify the advantages of our model and the new learning algorithm over\nMLE-based methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:59:49 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 15:51:36 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 14:49:31 GMT"}, {"version": "v4", "created": "Fri, 24 Aug 2018 02:06:38 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Li", "Yandong", ""], ["Wang", "Liqiang", ""], ["Yang", "Tianbao", ""], ["Gong", "Boqing", ""]]}, {"id": "1807.04339", "submitter": "Awais Mansoor", "authors": "Awais Mansoor, Juan J. Cerrolaza, Geovanny Perez, Elijah Biggs,\n  Kazunori Okada, Gustavo Nino, Marius George Linguraru", "title": "A Generic Approach to Lung Field Segmentation from Chest Radiographs\n  using Deep Space and Shape Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-aided diagnosis (CAD) techniques for lung field segmentation from\nchest radiographs (CXR) have been proposed for adult cohorts, but rarely for\npediatric subjects. Statistical shape models (SSMs), the workhorse of most\nstate-of-the-art CXR-based lung field segmentation methods, do not efficiently\naccommodate shape variation of the lung field during the pediatric\ndevelopmental stages. The main contributions of our work are: (1) a generic\nlung field segmentation framework from CXR accommodating large shape variation\nfor adult and pediatric cohorts; (2) a deep representation learning detection\nmechanism, \\emph{ensemble space learning}, for robust object localization; and\n(3) \\emph{marginal shape deep learning} for the shape deformation parameter\nestimation. Unlike the iterative approach of conventional SSMs, the proposed\nshape learning mechanism transforms the parameter space into marginal subspaces\nthat are solvable efficiently using the recursive representation learning\nmechanism. Furthermore, our method is the first to include the challenging\nretro-cardiac region in the CXR-based lung segmentation for accurate lung\ncapacity estimation. The framework is evaluated on 668 CXRs of patients between\n3 month to 89 year of age. We obtain a mean Dice similarity coefficient of\n$0.96\\pm0.03$ (including the retro-cardiac region). For a given accuracy, the\nproposed approach is also found to be faster than conventional SSM-based\niterative segmentation methods. The computational simplicity of the proposed\ngeneric framework could be similarly applied to the fast segmentation of other\ndeformable objects.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 20:17:25 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Mansoor", "Awais", ""], ["Cerrolaza", "Juan J.", ""], ["Perez", "Geovanny", ""], ["Biggs", "Elijah", ""], ["Okada", "Kazunori", ""], ["Nino", "Gustavo", ""], ["Linguraru", "Marius George", ""]]}, {"id": "1807.04352", "submitter": "Sri Kalyan Yarlagadda", "authors": "Sri Kalyan Yarlagadda, Fengqing Zhu", "title": "A Reflectance Based Method For Shadow Detection and Removal", "comments": "Presented at the 2018 IEEE Southwest Symposium on Image Analysis and\n  Interpretation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shadows are common aspect of images and when left undetected can hinder scene\nunderstanding and visual processing. We propose a simple yet effective approach\nbased on reflectance to detect shadows from single image. An image is first\nsegmented and based on the reflectance, illumination and texture\ncharacteristics, segments pairs are identified as shadow and non-shadow pairs.\nThe proposed method is tested on two publicly available and widely used\ndatasets. Our method achieves higher accuracy in detecting shadows compared to\nprevious reported methods despite requiring fewer parameters. We also show\nresults of shadow-free images by relighting the pixels in the detected shadow\nregions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 21:11:25 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Yarlagadda", "Sri Kalyan", ""], ["Zhu", "Fengqing", ""]]}, {"id": "1807.04355", "submitter": "Oliver Aalami", "authors": "Varun Shenoy, Elizabeth Foster, Lauren Aalami, Bakar Majeed and Oliver\n  Aalami", "title": "Deepwound: Automated Postoperative Wound Assessment and Surgical Site\n  Surveillance through Convolutional Neural Networks", "comments": "7 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Postoperative wound complications are a significant cause of expense for\nhospitals, doctors, and patients. Hence, an effective method to diagnose the\nonset of wound complications is strongly desired. Algorithmically classifying\nwound images is a difficult task due to the variability in the appearance of\nwound sites. Convolutional neural networks (CNNs), a subgroup of artificial\nneural networks that have shown great promise in analyzing visual imagery, can\nbe leveraged to categorize surgical wounds. We present a multi-label CNN\nensemble, Deepwound, trained to classify wound images using only image pixels\nand corresponding labels as inputs. Our final computational model can\naccurately identify the presence of nine labels: drainage, fibrinous exudate,\ngranulation tissue, surgical site infection, open wound, staples, steri strips,\nand sutures. Our model achieves receiver operating curve (ROC) area under curve\n(AUC) scores, sensitivity, specificity, and F1 scores superior to prior work in\nthis area. Smartphones provide a means to deliver accessible wound care due to\ntheir increasing ubiquity. Paired with deep neural networks, they offer the\ncapability to provide clinical insight to assist surgeons during postoperative\ncare. We also present a mobile application frontend to Deepwound that assists\npatients in tracking their wound and surgical recovery from the comfort of\ntheir home.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 21:17:49 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Shenoy", "Varun", ""], ["Foster", "Elizabeth", ""], ["Aalami", "Lauren", ""], ["Majeed", "Bakar", ""], ["Aalami", "Oliver", ""]]}, {"id": "1807.04364", "submitter": "Jun Xu", "authors": "Jun Xu, Lei Zhang, David Zhang", "title": "A Trilateral Weighted Sparse Coding Scheme for Real-World Image\n  Denoising", "comments": "Accepted to ECCV 2018. 17 pages, not including supplemental material.\n  Code will be published on https://github.com/csjunxu/TWSC-ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing image denoising methods assume the corrupted noise to be\nadditive white Gaussian noise (AWGN). However, the realistic noise in\nreal-world noisy images is much more complex than AWGN, and is hard to be\nmodelled by simple analytical distributions. As a result, many state-of-the-art\ndenoising methods in literature become much less effective when applied to\nreal-world noisy images captured by CCD or CMOS cameras. In this paper, we\ndevelop a trilateral weighted sparse coding (TWSC) scheme for robust real-world\nimage denoising. Specifically, we introduce three weight matrices into the data\nand regularisation terms of the sparse coding framework to characterise the\nstatistics of realistic noise and image priors. TWSC can be reformulated as a\nlinear equality-constrained problem and can be solved by the alternating\ndirection method of multipliers. The existence and uniqueness of the solution\nand convergence of the proposed algorithm are analysed. Extensive experiments\ndemonstrate that the proposed TWSC scheme outperforms state-of-the-art\ndenoising methods on removing realistic noise.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 22:00:51 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Xu", "Jun", ""], ["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1807.04409", "submitter": "Anoop Cherian", "authors": "Anoop Cherian and Alan Sullivan", "title": "Sem-GAN: Semantically-Consistent Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unpaired image-to-image translation is the problem of mapping an image in the\nsource domain to one in the target domain, without requiring corresponding\nimage pairs. To ensure the translated images are realistically plausible,\nrecent works, such as Cycle-GAN, demands this mapping to be invertible. While,\nthis requirement demonstrates promising results when the domains are unimodal,\nits performance is unpredictable in a multi-modal scenario such as in an image\nsegmentation task. This is because, invertibility does not necessarily enforce\nsemantic correctness. To this end, we present a semantically-consistent GAN\nframework, dubbed Sem-GAN, in which the semantics are defined by the class\nidentities of image segments in the source domain as produced by a semantic\nsegmentation algorithm. Our proposed framework includes consistency constraints\non the translation task that, together with the GAN loss and the\ncycle-constraints, enforces that the images when translated will inherit the\nappearances of the target domain, while (approximately) maintaining their\nidentities from the source domain. We present experiments on several\nimage-to-image translation tasks and demonstrate that Sem-GAN improves the\nquality of the translated images significantly, sometimes by more than 20% on\nthe FCN score. Further, we show that semantic segmentation models, trained with\nsynthetic images translated via Sem-GAN, leads to significantly better\nsegmentation results than other variants.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:55:19 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Cherian", "Anoop", ""], ["Sullivan", "Alan", ""]]}, {"id": "1807.04418", "submitter": "Wai Ho Chak", "authors": "Wai Ho Chak, Chun Pong Lau, Lok Ming Lui", "title": "Subsampled Turbulence Removal Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep-learning approach to restore a sequence of\nturbulence-distorted video frames from turbulent deformations and space-time\nvarying blurs. Instead of requiring a massive training sample size in deep\nnetworks, we purpose a training strategy that is based on a new data\naugmentation method to model turbulence from a relatively small dataset. Then\nwe introduce a subsampled method to enhance the restoration performance of the\npresented GAN model. The contributions of the paper is threefold: first, we\nintroduce a simple but effective data augmentation algorithm to model the\nturbulence in real life for training in the deep network; Second, we firstly\npurpose the Wasserstein GAN combined with $\\ell_1$ cost for successful\nrestoration of turbulence-corrupted video sequence; Third, we combine the\nsubsampling algorithm to filter out strongly corrupted frames to generate a\nvideo sequence with better quality.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 04:21:06 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 05:56:51 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Chak", "Wai Ho", ""], ["Lau", "Chun Pong", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1807.04445", "submitter": "Pengfei Zhang", "authors": "Pengfei Zhang, Jianru Xue, Cuiling Lan, Wenjun Zeng, Zhanning Gao,\n  Nanning Zheng", "title": "Adding Attentiveness to the Neurons in Recurrent Neural Networks", "comments": "ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are capable of modeling the temporal\ndynamics of complex sequential information. However, the structures of existing\nRNN neurons mainly focus on controlling the contributions of current and\nhistorical information but do not explore the different importance levels of\ndifferent elements in an input vector of a time slot. We propose adding a\nsimple yet effective Element-wiseAttention Gate (EleAttG) to an RNN block\n(e.g., all RNN neurons in a network layer) that empowers the RNN neurons to\nhave the attentiveness capability. For an RNN block, an EleAttG is added to\nadaptively modulate the input by assigning different levels of importance,\ni.e., attention, to each element/dimension of the input. We refer to an RNN\nblock equipped with an EleAttG as an EleAtt-RNN block. Specifically, the\nmodulation of the input is content adaptive and is performed at fine\ngranularity, being element-wise rather than input-wise. The proposed EleAttG,\nas an additional fundamental unit, is general and can be applied to any RNN\nstructures, e.g., standard RNN, Long Short-Term Memory (LSTM), or Gated\nRecurrent Unit (GRU). We demonstrate the effectiveness of the proposed\nEleAtt-RNN by applying it to the action recognition tasks on both 3D human\nskeleton data and RGB videos. Experiments show that adding attentiveness\nthrough EleAttGs to RNN blocks significantly boosts the power of RNNs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 06:59:36 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Zhang", "Pengfei", ""], ["Xue", "Jianru", ""], ["Lan", "Cuiling", ""], ["Zeng", "Wenjun", ""], ["Gao", "Zhanning", ""], ["Zheng", "Nanning", ""]]}, {"id": "1807.04459", "submitter": "Yue Zhang", "authors": "Wanli Chen, Yue Zhang, Junjun He, Yu Qiao, Yifan Chen, Hongjian Shi\n  and Xiaoying Tang", "title": "Prostate Segmentation using 2D Bridged U-net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on three problems in deep learning based medical\nimage segmentation. Firstly, U-net, as a popular model for medical image\nsegmentation, is difficult to train when convolutional layers increase even\nthough a deeper network usually has a better generalization ability because of\nmore learnable parameters. Secondly, the exponential ReLU (ELU), as an\nalternative of ReLU, is not much different from ReLU when the network of\ninterest gets deep. Thirdly, the Dice loss, as one of the pervasive loss\nfunctions for medical image segmentation, is not effective when the prediction\nis close to ground truth and will cause oscillation during training. To address\nthe aforementioned three problems, we propose and validate a deeper network\nthat can fit medical image datasets that are usually small in the sample size.\nMeanwhile, we propose a new loss function to accelerate the learning process\nand a combination of different activation functions to improve the network\nperformance. Our experimental results suggest that our network is comparable or\nsuperior to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 08:08:31 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:50:42 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Chen", "Wanli", ""], ["Zhang", "Yue", ""], ["He", "Junjun", ""], ["Qiao", "Yu", ""], ["Chen", "Yifan", ""], ["Shi", "Hongjian", ""], ["Tang", "Xiaoying", ""]]}, {"id": "1807.04465", "submitter": "Cheng Kang Hsieh", "authors": "Miguel Campo, Cheng-Kang Hsieh, Matt Nickens, JJ Espinoza, Abhinav\n  Taliyan, Julie Rieger, Jean Ho, Bettina Sherick", "title": "Competitive Analysis System for Theatrical Movie Releases Based on Movie\n  Trailer Deep Video Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Audience discovery is an important activity at major movie studios. Deep\nmodels that use convolutional networks to extract frame-by-frame features of a\nmovie trailer and represent it in a form that is suitable for prediction are\nnow possible thanks to the availability of pre-built feature extractors trained\non large image datasets. Using these pre-built feature extractors, we are able\nto process hundreds of publicly available movie trailers, extract\nframe-by-frame low level features (e.g., a face, an object, etc) and create\nvideo-level representations. We use the video-level representations to train a\nhybrid Collaborative Filtering model that combines video features with\nhistorical movie attendance records. The trained model not only makes accurate\nattendance and audience prediction for existing movies, but also successfully\nprofiles new movies six to eight months prior to their release.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 08:24:56 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Campo", "Miguel", ""], ["Hsieh", "Cheng-Kang", ""], ["Nickens", "Matt", ""], ["Espinoza", "JJ", ""], ["Taliyan", "Abhinav", ""], ["Rieger", "Julie", ""], ["Ho", "Jean", ""], ["Sherick", "Bettina", ""]]}, {"id": "1807.04514", "submitter": "Guanqun Ding", "authors": "Guanqun Ding, and Yuming Fang", "title": "Video Saliency Detection by 3D Convolutional Neural Networks", "comments": null, "journal-ref": "International Forum on Digital TV and Wireless Multimedia\n  Communications (IFTC 2017)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from salient object detection methods for still images, a key\nchallenging for video saliency detection is how to extract and combine spatial\nand temporal features. In this paper, we present a novel and effective approach\nfor salient object detection for video sequences based on 3D convolutional\nneural networks. First, we design a 3D convolutional network (Conv3DNet) with\nthe input as three video frame to learn the spatiotemporal features for video\nsequences. Then, we design a 3D deconvolutional network (Deconv3DNet) to\ncombine the spatiotemporal features to predict the final saliency map for video\nsequences. Experimental results show that the proposed saliency detection model\nperforms better in video saliency prediction compared with the state-of-the-art\nvideo saliency detection methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 10:18:12 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Ding", "Guanqun", ""], ["Fang", "Yuming", ""]]}, {"id": "1807.04562", "submitter": "Guanqun Ding", "authors": "Yuming Fang, Guanqun Ding, Yuan Yuan, Weisi Lin, and Haiwen Liu", "title": "Robustness Analysis of Pedestrian Detectors for Surveillance", "comments": null, "journal-ref": "IEEE Access 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To obtain effective pedestrian detection results in surveillance video, there\nhave been many methods proposed to handle the problems from severe occlusion,\npose variation, clutter background, \\emph{etc}. Besides detection accuracy, a\nrobust surveillance video system should be stable to video quality degradation\nby network transmission, environment variation, etc. In this study, we conduct\nthe research on the robustness of pedestrian detection algorithms to video\nquality degradation. The main contribution of this work includes the following\nthree aspects. First, a large-scale Distorted Surveillance Video Data Set\n(DSurVD) is constructed from high-quality video sequences and their\ncorresponding distorted versions. Second, we design a method to evaluate\ndetection stability and a robustness measure called Robustness Quadrangle,\nwhich can be adopted to visualize detection accuracy of pedestrian detection\nalgorithms on high-quality video sequences and stability with video quality\ndegradation. Third, the robustness of seven existing pedestrian detection\nalgorithms is evaluated by the built DSurVD. Experimental results show that the\nrobustness can be further improved for existing pedestrian detection\nalgorithms. Additionally, we provide much in-depth discussion on how different\ndistortion types influence the performance of pedestrian detection algorithms,\nwhich is important to design effective pedestrian detection algorithms for\nsurveillance. The DSurVD data set can be download from BaiduYunDisk,\nhttps://pan.baidu.com/s/1I9Kqj8rmubOYu7bkBfkUpA, Password: lqmc\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 12:14:38 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 13:54:02 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Fang", "Yuming", ""], ["Ding", "Guanqun", ""], ["Yuan", "Yuan", ""], ["Lin", "Weisi", ""], ["Liu", "Haiwen", ""]]}, {"id": "1807.04585", "submitter": "Tjeng Wawan Cenggoro Mr.", "authors": "Fanny, Tjeng Wawan Cenggoro", "title": "Deep Learning for Imbalance Data Classification using Class Expert\n  Generative Adversarial Network", "comments": "Accepted in 3rd International Conference on Computer Science and\n  Computational Intelligence, 7-8 September 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without any specific way for imbalance data classification, artificial\nintelligence algorithm cannot recognize data from minority classes easily. In\ngeneral, modifying the existing algorithm by assuming that the training data is\nimbalanced, is the only way to handle imbalance data. However, for a normal\ndata handling, this way mostly produces a deficient result. In this research,\nwe propose a class expert generative adversarial network (CE-GAN) as the\nsolution for imbalance data classification. CE-GAN is a modification in deep\nlearning algorithm architecture that does not have an assumption that the\ntraining data is imbalance data. Moreover, CE-GAN is designed to identify more\ndetail about the character of each class before classification step. CE-GAN has\nbeen proved in this research to give a good performance for imbalance data\nclassification.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 12:51:24 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 03:11:44 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Fanny", "", ""], ["Cenggoro", "Tjeng Wawan", ""]]}, {"id": "1807.04629", "submitter": "Hanwei Wu", "authors": "Hanwei Wu and Markus Flierl", "title": "Learning Product Codebooks using Vector Quantized Autoencoders for Image\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector-Quantized Variational Autoencoders (VQ-VAE)[1] provide an unsupervised\nmodel for learning discrete representations by combining vector quantization\nand autoencoders. In this paper, we study the use of VQ-VAE for representation\nlearning for downstream tasks, such as image retrieval. We first describe the\nVQ-VAE in the context of an information-theoretic framework. We show that the\nregularization term on the learned representation is determined by the size of\nthe embedded codebook before the training and it affects the generalization\nability of the model. As a result, we introduce a hyperparameter to balance the\nstrength of the vector quantizer and the reconstruction error. By tuning the\nhyperparameter, the embedded bottleneck quantizer is used as a regularizer that\nforces the output of the encoder to share a constrained coding space such that\nlearned latent features preserve the similarity relations of the data space. In\naddition, we provide a search range for finding the best hyperparameter.\nFinally, we incorporate the product quantization into the bottleneck stage of\nVQ-VAE and propose an end-to-end unsupervised learning model for the image\nretrieval task. The product quantizer has the advantage of generating\nlarge-size codebooks. Fast retrieval can be achieved by using the lookup tables\nthat store the distance between any pair of sub-codewords. State-of-the-art\nretrieval results are achieved by the learned codebooks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:08:31 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 15:42:37 GMT"}, {"version": "v3", "created": "Sun, 11 Nov 2018 18:35:35 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 11:50:21 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wu", "Hanwei", ""], ["Flierl", "Markus", ""]]}, {"id": "1807.04638", "submitter": "Monica Hernandez", "authors": "Monica Hernandez", "title": "PDE-constrained LDDMM via geodesic shooting and inexact\n  Gauss-Newton-Krylov optimization using the incremental adjoint Jacobi\n  equations", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6560/aaf598", "report-no": null, "categories": "cs.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of non-rigid registration methods proposed in the framework of\nPDE-constrained Large Deformation Diffeomorphic Metric Mapping is a\nparticularly interesting family of physically meaningful diffeomorphic\nregistration methods. Inexact Newton-Krylov optimization has shown an excellent\nnumerical accuracy and an extraordinarily fast convergence rate in this\nframework. However, the Galerkin representation of the non-stationary velocity\nfields does not provide proper geodesic paths. In this work, we propose a\nmethod for PDE-constrained LDDMM parameterized in the space of initial velocity\nfields under the EPDiff equation. The derivation of the gradient and the\nHessian-vector products are performed on the final velocity field and\ntransported backward using the adjoint and the incremental adjoint Jacobi\nequations. This way, we avoid the complex dependence on the initial velocity\nfield in the derivations and the computation of the adjoint equation and its\nincremental counterpart. The proposed method provides geodesics in the\nframework of PDE-constrained LDDMM, and it shows performance competitive to\nbenchmark PDE-constrained LDDMM and EPDiff-LDDMM methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:08:16 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hernandez", "Monica", ""]]}, {"id": "1807.04657", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone, Julien Cohen-Adad", "title": "Deep semi-supervised segmentation with weight-averaged consistency\n  targets", "comments": "8 pages, 1 figure, accepted for DLMIA/MICCAI", "journal-ref": null, "doi": "10.1007/978-3-030-00889-5_2", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently proposed techniques for semi-supervised learning such as Temporal\nEnsembling and Mean Teacher have achieved state-of-the-art results in many\nimportant classification benchmarks. In this work, we expand the Mean Teacher\napproach to segmentation tasks and show that it can bring important\nimprovements in a realistic small data regime using a publicly available\nmulti-center dataset from the Magnetic Resonance Imaging (MRI) domain. We also\ndevise a method to solve the problems that arise when using traditional data\naugmentation strategies for segmentation tasks on our new training scheme.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:55:26 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 15:06:15 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Perone", "Christian S.", ""], ["Cohen-Adad", "Julien", ""]]}, {"id": "1807.04668", "submitter": "Christian Baumgartner", "authors": "Yigit B. Can, Krishna Chaitanya, Basil Mustafa, Lisa M. Koch, Ender\n  Konukoglu, Christian F. Baumgartner", "title": "Learning to Segment Medical Images with Scribble-Supervision Alone", "comments": "Accepted for presentation at DLMIA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of medical images is a crucial step for the\nquantification of healthy anatomy and diseases alike. The majority of the\ncurrent state-of-the-art segmentation algorithms are based on deep neural\nnetworks and rely on large datasets with full pixel-wise annotations. Producing\nsuch annotations can often only be done by medical professionals and requires\nlarge amounts of valuable time. Training a medical image segmentation network\nwith weak annotations remains a relatively unexplored topic. In this work we\ninvestigate training strategies to learn the parameters of a pixel-wise\nsegmentation network from scribble annotations alone. We evaluate the\ntechniques on public cardiac (ACDC) and prostate (NCI-ISBI) segmentation\ndatasets. We find that the networks trained on scribbles suffer from a\nremarkably small degradation in Dice of only 2.9% (cardiac) and 4.5% (prostate)\nwith respect to a network trained on full annotations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:24:48 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Can", "Yigit B.", ""], ["Chaitanya", "Krishna", ""], ["Mustafa", "Basil", ""], ["Koch", "Lisa M.", ""], ["Konukoglu", "Ender", ""], ["Baumgartner", "Christian F.", ""]]}, {"id": "1807.04686", "submitter": "Shi Guo", "authors": "Shi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo and Lei Zhang", "title": "Toward Convolutional Blind Denoising of Real Photographs", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n  2019", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep convolutional neural networks (CNNs) have achieved impressive\nsuccess in image denoising with additive white Gaussian noise (AWGN), their\nperformance remains limited on real-world noisy photographs. The main reason is\nthat their learned models are easy to overfit on the simplified AWGN model\nwhich deviates severely from the complicated real-world noise model. In order\nto improve the generalization ability of deep CNN denoisers, we suggest\ntraining a convolutional blind denoising network (CBDNet) with more realistic\nnoise model and real-world noisy-clean image pairs. On the one hand, both\nsignal-dependent noise and in-camera signal processing pipeline is considered\nto synthesize realistic noisy images. On the other hand, real-world noisy\nphotographs and their nearly noise-free counterparts are also included to train\nour CBDNet. To further provide an interactive strategy to rectify denoising\nresult conveniently, a noise estimation subnetwork with asymmetric learning to\nsuppress under-estimation of noise level is embedded into CBDNet. Extensive\nexperimental results on three datasets of real-world noisy photographs clearly\ndemonstrate the superior performance of CBDNet over state-of-the-arts in terms\nof quantitative metrics and visual quality. The code has been made available at\nhttps://github.com/GuoShi28/CBDNet.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:52:17 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 07:05:40 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Guo", "Shi", ""], ["Yan", "Zifei", ""], ["Zhang", "Kai", ""], ["Zuo", "Wangmeng", ""], ["Zhang", "Lei", ""]]}, {"id": "1807.04702", "submitter": "Marcin Dymczyk", "authors": "Marcin Dymczyk, Igor Gilitschenski, Juan Nieto, Simon Lynen, Bernhard\n  Zeisl, Roland Siegwart", "title": "LandmarkBoost: Efficient Visual Context Classifiers for Robust\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity of autonomous systems creates a need for reliable and\nefficient metric pose retrieval algorithms. Currently used approaches tend to\nrely on nearest neighbor search of binary descriptors to perform the 2D-3D\nmatching and guarantee realtime capabilities on mobile platforms. These methods\nstruggle, however, with the growing size of the map, changes in viewpoint or\nappearance, and visual aliasing present in the environment. The rigidly defined\ndescriptor patterns only capture a limited neighborhood of the keypoint and\ncompletely ignore the overall visual context.\n  We propose LandmarkBoost - an approach that, in contrast to the conventional\n2D-3D matching methods, casts the search problem as a landmark classification\ntask. We use a boosted classifier to classify landmark observations and\ndirectly obtain correspondences as classifier scores. We also introduce a\nformulation of visual context that is flexible, efficient to compute, and can\ncapture relationships in the entire image plane. The original binary\ndescriptors are augmented with contextual information and informative features\nare selected by the boosting framework. Through detailed experiments, we\nevaluate the retrieval quality and performance of LandmarkBoost, demonstrating\nthat it outperforms common state-of-the-art descriptor matching methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 16:14:37 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 11:19:50 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Dymczyk", "Marcin", ""], ["Gilitschenski", "Igor", ""], ["Nieto", "Juan", ""], ["Lynen", "Simon", ""], ["Zeisl", "Bernhard", ""], ["Siegwart", "Roland", ""]]}, {"id": "1807.04742", "submitter": "Ashvin Nair", "authors": "Ashvin Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin,\n  Sergey Levine", "title": "Visual Reinforcement Learning with Imagined Goals", "comments": "15 pages, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an autonomous agent to fulfill a wide range of user-specified goals at\ntest time, it must be able to learn broadly applicable and general-purpose\nskill repertoires. Furthermore, to provide the requisite level of generality,\nthese skills must handle raw sensory input such as images. In this paper, we\npropose an algorithm that acquires such general-purpose skills by combining\nunsupervised representation learning and reinforcement learning of\ngoal-conditioned policies. Since the particular goals that might be required at\ntest-time are not known in advance, the agent performs a self-supervised\n\"practice\" phase where it imagines goals and attempts to achieve them. We learn\na visual representation with three distinct purposes: sampling goals for\nself-supervised practice, providing a structured transformation of raw sensory\ninputs, and computing a reward signal for goal reaching. We also propose a\nretroactive goal relabeling scheme to further improve the sample-efficiency of\nour method. Our off-policy algorithm is efficient enough to learn policies that\noperate on raw image observations and goals for a real-world robotic system,\nand substantially outperforms prior techniques.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 17:51:16 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 08:44:08 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Nair", "Ashvin", ""], ["Pong", "Vitchyr", ""], ["Dalal", "Murtaza", ""], ["Bahl", "Shikhar", ""], ["Lin", "Steven", ""], ["Levine", "Sergey", ""]]}, {"id": "1807.04798", "submitter": "Florian Dubost", "authors": "Florian Dubost, Gerda Bortsova, Hieab Adams, M. Arfan Ikram, Wiro\n  Niessen, Meike Vernooij, Marleen de Bruijne", "title": "Hydranet: Data Augmentation for Regression Neural Networks", "comments": "accepted in MICCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques are often criticized to heavily depend on a large\nquantity of labeled data. This problem is even more challenging in medical\nimage analysis where the annotator expertise is often scarce. We propose a\nnovel data-augmentation method to regularize neural network regressors that\nlearn from a single global label per image. The principle of the method is to\ncreate new samples by recombining existing ones. We demonstrate the performance\nof our algorithm on two tasks: estimation of the number of enlarged\nperivascular spaces in the basal ganglia, and estimation of white matter\nhyperintensities volume. We show that the proposed method improves the\nperformance over more basic data augmentation. The proposed method reached an\nintraclass correlation coefficient between ground truth and network predictions\nof 0.73 on the first task and 0.84 on the second task, only using between 25\nand 30 scans with a single global label per scan for training. With the same\nnumber of training scans, more conventional data augmentation methods could\nonly reach intraclass correlation coefficients of 0.68 on the first task, and\n0.79 on the second task.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 19:30:21 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 18:41:25 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 12:57:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dubost", "Florian", ""], ["Bortsova", "Gerda", ""], ["Adams", "Hieab", ""], ["Ikram", "M. Arfan", ""], ["Niessen", "Wiro", ""], ["Vernooij", "Meike", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1807.04807", "submitter": "Allen Lu Lu", "authors": "Allen Lu, Nripesh Parajuli, Maria Zontak, John Stendahl, Kevinminh Ta,\n  Zhao Liu, Nabil Boutagy, Geng-Shi Jeng, Imran Alkhalil, Lawrence H. Staib,\n  Matthew O'Donnell, Albert J. Sinusas, James S. Duncan", "title": "Learning-based Regularization for Cardiac Strain Analysis with Ability\n  for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable motion estimation and strain analysis using 3D+time echocardiography\n(4DE) for localization and characterization of myocardial injury is valuable\nfor early detection and targeted interventions. However, motion estimation is\ndifficult due to the low-SNR that stems from the inherent image properties of\n4DE, and intelligent regularization is critical for producing reliable motion\nestimates. In this work, we incorporated the notion of domain adaptation into a\nsupervised neural network regularization framework. We first propose an\nunsupervised autoencoder network with biomechanical constraints for learning a\nlatent representation that is shown to have more physiologically plausible\ndisplacements. We extended this framework to include a supervised loss term on\nsynthetic data and showed the effects of biomechanical constraints on the\nnetwork's ability for domain adaptation. We validated both the autoencoder and\nsemi-supervised regularization method on in vivo data with implanted\nsonomicrometers. Finally, we showed the ability of our semi-supervised learning\nregularization approach to identify infarcted regions using estimated regional\nstrain maps with good agreement to manually traced infarct regions from\npostmortem excised hearts.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 20:19:48 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Lu", "Allen", ""], ["Parajuli", "Nripesh", ""], ["Zontak", "Maria", ""], ["Stendahl", "John", ""], ["Ta", "Kevinminh", ""], ["Liu", "Zhao", ""], ["Boutagy", "Nabil", ""], ["Jeng", "Geng-Shi", ""], ["Alkhalil", "Imran", ""], ["Staib", "Lawrence H.", ""], ["O'Donnell", "Matthew", ""], ["Sinusas", "Albert J.", ""], ["Duncan", "James S.", ""]]}, {"id": "1807.04812", "submitter": "Sangpil Kim", "authors": "Sangpil Kim, Nick Winovich, Guang Lin, Karthik Ramani", "title": "Latent Transformations for Object View Points Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a fully-convolutional conditional generative model, the latent\ntransformation neural network (LTNN), capable of view synthesis using a\nlight-weight neural network suited for real-time applications. In contrast to\nexisting conditional generative models which incorporate conditioning\ninformation via concatenation, we introduce a dedicated network component, the\nconditional transformation unit (CTU), designed to learn the latent space\ntransformations corresponding to specified target views. In addition, a\nconsistency loss term is defined to guide the network toward learning the\ndesired latent space mappings, a task-divided decoder is constructed to refine\nthe quality of generated views, and an adaptive discriminator is introduced to\nimprove the adversarial training process. The generality of the proposed\nmethodology is demonstrated on a collection of three diverse tasks: multi-view\nreconstruction on real hand depth images, view synthesis of real and synthetic\nfaces, and the rotation of rigid objects. The proposed model is shown to exceed\nstate-of-the-art results in each category while simultaneously achieving a\nreduction in the computational demand required for inference by 30% on average.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 20:46:43 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 00:47:23 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 14:27:40 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 17:52:39 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kim", "Sangpil", ""], ["Winovich", "Nick", ""], ["Lin", "Guang", ""], ["Ramani", "Karthik", ""]]}, {"id": "1807.04821", "submitter": "Kan Chen", "authors": "Jiyang Gao, Kan Chen, Ram Nevatia", "title": "CTAP: Complementary Temporal Action Proposal Generation", "comments": "ECCV 2018 main conference paper (camera ready version). Code is\n  available in http://www.github.com/jiyanggao/CTAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal action proposal generation is an important task, akin to object\nproposals, temporal action proposals are intended to capture \"clips\" or\ntemporal intervals in videos that are likely to contain an action. Previous\nmethods can be divided to two groups: sliding window ranking and actionness\nscore grouping. Sliding windows uniformly cover all segments in videos, but the\ntemporal boundaries are imprecise; grouping based method may have more precise\nboundaries but it may omit some proposals when the quality of actionness score\nis low. Based on the complementary characteristics of these two methods, we\npropose a novel Complementary Temporal Action Proposal (CTAP) generator.\nSpecifically, we apply a Proposal-level Actionness Trustworthiness Estimator\n(PATE) on the sliding windows proposals to generate the probabilities\nindicating whether the actions can be correctly detected by actionness scores,\nthe windows with high scores are collected. The collected sliding windows and\nactionness proposals are then processed by a temporal convolutional neural\nnetwork for proposal ranking and boundary adjustment. CTAP outperforms\nstate-of-the-art methods on average recall (AR) by a large margin on THUMOS-14\nand ActivityNet 1.3 datasets. We further apply CTAP as a proposal generation\nmethod in an existing action detector, and show consistent significant\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:07:01 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 20:25:26 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Gao", "Jiyang", ""], ["Chen", "Kan", ""], ["Nevatia", "Ram", ""]]}, {"id": "1807.04834", "submitter": "Yandong Wen", "authors": "Yandong Wen, Mahmoud Al Ismail, Bhiksha Raj, Rita Singh", "title": "Optimal Strategies for Matching and Retrieval Problems by Comparing\n  Covariates", "comments": "support material for \"Disjoint Mapping Network for Cross-modal\n  Matching of Voices and Faces\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many retrieval problems, where we must retrieve one or more entries from a\ngallery in response to a probe, it is common practice to learn to do by\ndirectly comparing the probe and gallery entries to one another. In many\nsituations the gallery and probe have common covariates -- external variables\nthat are common to both. In principle it is possible to perform the retrieval\nbased merely on these covariates. The process, however, becomes gated by our\nability to recognize the covariates for the probe and gallery entries\ncorrectly.\n  In this paper we analyze optimal strategies for retrieval based only on\nmatching covariates, when the recognition of the covariates is itself\ninaccurate. We investigate multiple problems: recovering one item from a\ngallery of $N$ entries, matching pairs of instances, and retrieval from large\ncollections. We verify our analytical formulae through experiments to verify\ntheir correctness in practical settings.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:36:07 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 00:53:37 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wen", "Yandong", ""], ["Ismail", "Mahmoud Al", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1807.04836", "submitter": "Yandong Wen", "authors": "Yandong Wen, Mahmoud Al Ismail, Weiyang Liu, Bhiksha Raj, Rita Singh", "title": "Disjoint Mapping Network for Cross-modal Matching of Voices and Faces", "comments": "Tech report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework, called Disjoint Mapping Network (DIMNet), for\ncross-modal biometric matching, in particular of voices and faces. Different\nfrom the existing methods, DIMNet does not explicitly learn the joint\nrelationship between the modalities. Instead, DIMNet learns a shared\nrepresentation for different modalities by mapping them individually to their\ncommon covariates. These shared representations can then be used to find the\ncorrespondences between the modalities. We show empirically that DIMNet is able\nto achieve better performance than other current methods, with the additional\nbenefits of being conceptually simpler and less data-intensive.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 21:37:34 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 01:00:13 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wen", "Yandong", ""], ["Ismail", "Mahmoud Al", ""], ["Liu", "Weiyang", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1807.04855", "submitter": "Stefan Maetschke", "authors": "Stefan Maetschke, Bhavna Antony, Hiroshi Ishikawa, Gadi Wollstein,\n  Joel S. Schuman, Rahil Garnavi", "title": "A feature agnostic approach for glaucoma detection in OCT volumes", "comments": "13 pages,3 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0219126", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical coherence tomography (OCT) based measurements of retinal layer\nthickness, such as the retinal nerve fibre layer (RNFL) and the ganglion cell\nwith inner plexiform layer (GCIPL) are commonly used for the diagnosis and\nmonitoring of glaucoma. Previously, machine learning techniques have utilized\nsegmentation-based imaging features such as the peripapillary RNFL thickness\nand the cup-to-disc ratio. Here, we propose a deep learning technique that\nclassifies eyes as healthy or glaucomatous directly from raw, unsegmented OCT\nvolumes of the optic nerve head (ONH) using a 3D Convolutional Neural Network\n(CNN). We compared the accuracy of this technique with various feature-based\nmachine learning algorithms and demonstrated the superiority of the proposed\ndeep learning based method.\n  Logistic regression was found to be the best performing classical machine\nlearning technique with an AUC of 0.89. In direct comparison, the deep learning\napproach achieved a substantially higher AUC of 0.94 with the additional\nadvantage of providing insight into which regions of an OCT volume are\nimportant for glaucoma detection.\n  Computing Class Activation Maps (CAM), we found that the CNN identified\nneuroretinal rim and optic disc cupping as well as the lamina cribrosa (LC) and\nits surrounding areas as the regions significantly associated with the glaucoma\nclassification. These regions anatomically correspond to the well established\nand commonly used clinical markers for glaucoma diagnosis such as increased cup\nvolume, cup diameter, and neuroretinal rim thinning at the superior and\ninferior segments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 22:57:19 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 01:15:27 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 02:29:18 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2019 03:14:22 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Maetschke", "Stefan", ""], ["Antony", "Bhavna", ""], ["Ishikawa", "Hiroshi", ""], ["Wollstein", "Gadi", ""], ["Schuman", "Joel S.", ""], ["Garnavi", "Rahil", ""]]}, {"id": "1807.04856", "submitter": "Arturo Gomez Chavez", "authors": "Arturo Gomez Chavez, Andrea Ranieri, Davide Chiarella, Enrica Zereik,\n  Anja Babi\\'c, Andreas Birk", "title": "CADDY Underwater Stereo-Vision Dataset for Human-Robot Interaction (HRI)\n  in the Context of Diver Activities", "comments": "submitted to IJRR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present a novel underwater dataset collected from several\nfield trials within the EU FP7 project \"Cognitive autonomous diving buddy\n(CADDY)\", where an Autonomous Underwater Vehicle (AUV) was used to interact\nwith divers and monitor their activities. To our knowledge, this is one of the\nfirst efforts to collect a large dataset in underwater environments targeting\nobject classification, segmentation and human pose estimation tasks. The first\npart of the dataset contains stereo camera recordings (~10K) of divers\nperforming hand gestures to communicate and interact with an AUV in different\nenvironmental conditions. These gestures samples serve to test the robustness\nof object detection and classification algorithms against underwater image\ndistortions i.e., color attenuation and light backscatter. The second part\nincludes stereo footage (~12.7K) of divers free-swimming in front of the AUV,\nalong with synchronized IMUs measurements located throughout the diver's suit\n(DiverNet) which serve as ground-truth for human pose and tracking methods. In\nboth cases, these rectified images allow investigation of 3D representation and\nreasoning pipelines from low-texture targets commonly present in underwater\nscenarios. In this paper we describe our recording platform, sensor calibration\nprocedure plus the data format and the utilities provided to use the dataset.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 23:00:06 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Chavez", "Arturo Gomez", ""], ["Ranieri", "Andrea", ""], ["Chiarella", "Davide", ""], ["Zereik", "Enrica", ""], ["Babi\u0107", "Anja", ""], ["Birk", "Andreas", ""]]}, {"id": "1807.04870", "submitter": "Konstantinos Zampogiannis", "authors": "Konstantinos Zampogiannis, Kanishka Ganguly, Cornelia Fermuller,\n  Yiannis Aloimonos", "title": "Extracting Contact and Motion from Manipulation Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we physically interact with our environment using our hands, we touch\nobjects and force them to move: contact and motion are defining properties of\nmanipulation. In this paper, we present an active, bottom-up method for the\ndetection of actor-object contacts and the extraction of moved objects and\ntheir motions in RGBD videos of manipulation actions. At the core of our\napproach lies non-rigid registration: we continuously warp a point cloud model\nof the observed scene to the current video frame, generating a set of dense 3D\npoint trajectories. Under loose assumptions, we employ simple point cloud\nsegmentation techniques to extract the actor and subsequently detect\nactor-environment contacts based on the estimated trajectories. For each such\ninteraction, using the detected contact as an attention mechanism, we obtain an\ninitial motion segment for the manipulated object by clustering trajectories in\nthe contact area vicinity and then we jointly refine the object segment and\nestimate its 6DOF pose in all observed frames. Because of its generality and\nthe fundamental, yet highly informative, nature of its outputs, our approach is\napplicable to a wide range of perception and planning tasks. We qualitatively\nevaluate our method on a number of input sequences and present a comprehensive\nrobot imitation learning example, in which we demonstrate the crucial role of\nour outputs in developing action representations/plans from observation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 00:15:39 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 17:06:16 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 01:40:26 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zampogiannis", "Konstantinos", ""], ["Ganguly", "Kanishka", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1807.04880", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, T.T. Wong", "title": "Effective Occlusion Handling for Fast Correlation Filter-based Trackers", "comments": "www.eecjournal.com/ (Vol-5,Issue-6,November - December 2020)", "journal-ref": null, "doi": "10.22161/eec.562", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter-based trackers heavily suffer from the problem of multiple\npeaks in their response maps incurred by occlusions. Moreover, the whole\ntracking pipeline may break down due to the uncertainties brought by shifting\namong peaks, which will further lead to the degraded correlation filter model.\nTo alleviate the drift problem caused by occlusions, we propose a novel scheme\nto choose the specific filter model according to different scenarios.\nSpecifically, an effective measurement function is designed to evaluate the\nquality of filter response. A sophisticated strategy is employed to judge\nwhether occlusions occur, and then decide how to update the filter models. In\naddition, we take advantage of both log-polar method and pyramid-like approach\nto estimate the best scale of the target. We evaluate our proposed approach on\nVOT2018 challenge and OTB100 dataset, whose experimental result shows that the\nproposed tracker achieves the promising performance compared against the\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 01:23:24 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 08:03:43 GMT"}, {"version": "v3", "created": "Fri, 1 Jan 2021 13:19:18 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Zheng", ""], ["Wong", "T. T.", ""]]}, {"id": "1807.04888", "submitter": "Oliver Aalami", "authors": "Varun N. Shenoy and Oliver O. Aalami", "title": "Utilizing Smartphone-Based Machine Learning in Medical Monitor Data\n  Collection: Seven Segment Digit Recognition", "comments": "Accepted for publication in AMIA 2017 Annual Symposium", "journal-ref": "Shenoy VN, Aalami OO. Utilizing Smartphone-Based Machine Learning\n  in Medical Monitor Data Collection: Seven Segment Digit Recognition. AMIA\n  Annual Symposium Proceedings. 2017;2017:1564-1570", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric measurements captured from medical devices, such as blood pressure\ngauges, glucose monitors, and weighing scales, are essential to tracking a\npatient's health. Trends in these measurements can accurately track diabetes,\ncardiovascular issues, and assist medication management for patients.\nCurrently, patients record their results and data of measurement in a physical\nnotebook. It may be weeks before a doctor sees a patient's records and can\nassess the health of the patient. With a predicted 6.8 billion smartphones in\nthe world by 2022, health monitoring platforms, such as Apple's HealthKit, can\nbe leveraged to provide the right care at the right time. This research\npresents a mobile application that enables users to capture medical monitor\ndata and send it to their doctor swiftly. A key contribution of this paper is a\nrobust engine that can recognize digits from medical monitors with an accuracy\nof 98.2%.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 02:12:21 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Shenoy", "Varun N.", ""], ["Aalami", "Oliver O.", ""]]}, {"id": "1807.04890", "submitter": "Junjie Huang", "authors": "Junjie Huang, Wei Zou, Jiagang Zhu, Zheng Zhu", "title": "Optical Flow Based Real-time Moving Object Detection in Unconstrained\n  Scenes", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time moving object detection in unconstrained scenes is a difficult task\ndue to dynamic background, changing foreground appearance and limited\ncomputational resource. In this paper, an optical flow based moving object\ndetection framework is proposed to address this problem. We utilize homography\nmatrixes to online construct a background model in the form of optical flow.\nWhen judging out moving foregrounds from scenes, a dual-mode judge mechanism is\ndesigned to heighten the system's adaptation to challenging situations. In\nexperiment part, two evaluation metrics are redefined for more properly\nreflecting the performance of methods. We quantitatively and qualitatively\nvalidate the effectiveness and feasibility of our method with videos in various\nscene conditions. The experimental results show that our method adapts itself\nto different situations and outperforms the state-of-the-art methods,\nindicating the advantages of optical flow based methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 02:15:39 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Huang", "Junjie", ""], ["Zou", "Wei", ""], ["Zhu", "Jiagang", ""], ["Zhu", "Zheng", ""]]}, {"id": "1807.04892", "submitter": "Lior Shamir", "authors": "Fan Wei, Yuan Li, Lior Shamir", "title": "Computer Analysis of Architecture Using Automatic Image Understanding", "comments": null, "journal-ref": "Journal of Data Mining & Digital Humanities, 2018 (January 22,\n  2019) jdmdh:5030", "doi": "10.46298/jdmdh.4683", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past few years, computer vision and pattern recognition systems have\nbeen becoming increasingly more powerful, expanding the range of automatic\ntasks enabled by machine vision. Here we show that computer analysis of\nbuilding images can perform quantitative analysis of architecture, and quantify\nsimilarities between city architectural styles in a quantitative fashion.\nImages of buildings from 18 cities and three countries were acquired using\nGoogle StreetView, and were used to train a machine vision system to\nautomatically identify the location of the imaged building based on the image\nvisual content. Experimental results show that the automatic computer analysis\ncan automatically identify the geographical location of the StreetView image.\nMore importantly, the algorithm was able to group the cities and countries and\nprovide a phylogeny of the similarities between architectural styles as\ncaptured by StreetView images. These results demonstrate that computer vision\nand pattern recognition algorithms can perform the complex cognitive task of\nanalyzing images of buildings, and can be used to measure and quantify visual\nsimilarities and differences between different styles of architectures. This\nexperiment provides a new paradigm for studying architecture, based on a\nquantitative approach that can enhance the traditional manual observation and\nanalysis. The source code used for the analysis is open and publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 02:25:28 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 03:44:18 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 01:39:25 GMT"}, {"version": "v4", "created": "Thu, 9 Jul 2020 16:31:32 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wei", "Fan", ""], ["Li", "Yuan", ""], ["Shamir", "Lior", ""]]}, {"id": "1807.04893", "submitter": "Joshua Ebenezer", "authors": "Joshua Peter Ebenezer and Jagath C. Rajapakse", "title": "Automatic segmentation of skin lesions using deep learning", "comments": "4 pages, 1 figure, extended abstract of submission to ISIC 2018 skin\n  analysis challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper summarizes the method used in our submission to Task 1 of the\nInternational Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis Towards\nMelanoma Detection challenge held in 2018. We used a fully automated method to\naccurately segment lesion boundaries from dermoscopic images. A U-net deep\nlearning network is trained on publicly available data from ISIC. We introduce\nthe use of intensity, color, and texture enhancement operations as\npre-processing steps and morphological operations and contour identification as\npost-processing steps.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 02:34:24 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Ebenezer", "Joshua Peter", ""], ["Rajapakse", "Jagath C.", ""]]}, {"id": "1807.04897", "submitter": "Yunchao Wei", "authors": "Yunchao Wei and Zhiqiang Shen and Bowen Cheng and Honghui Shi and\n  Jinjun Xiong and Jiashi Feng and Thomas Huang", "title": "TS2C: Tight Box Mining with Surrounding Segmentation Context for Weakly\n  Supervised Object Detection", "comments": "ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a simple approach to discover tight object bounding boxes\nwith only image-level supervision, called Tight box mining with Surrounding\nSegmentation Context (TS2C). We observe that object candidates mined through\ncurrent multiple instance learning methods are usually trapped to\ndiscriminative object parts, rather than the entire object. TS2C leverages\nsurrounding segmentation context derived from weakly-supervised segmentation to\nsuppress such low-quality distracting candidates and boost the high-quality\nones. Specifically, TS2C is developed based on two key properties of desirable\nbounding boxes: 1) high purity, meaning most pixels in the box are with high\nobject response, and 2) high completeness, meaning the box covers high object\nresponse pixels comprehensively. With such novel and computable criteria, more\ntight candidates can be discovered for learning a better object detector. With\nTS2C, we obtain 48.0% and 44.4% mAP scores on VOC 2007 and 2012 benchmarks,\nwhich are the new state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 03:38:49 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Wei", "Yunchao", ""], ["Shen", "Zhiqiang", ""], ["Cheng", "Bowen", ""], ["Shi", "Honghui", ""], ["Xiong", "Jinjun", ""], ["Feng", "Jiashi", ""], ["Huang", "Thomas", ""]]}, {"id": "1807.04899", "submitter": "Wen Tang", "authors": "Wen Tang, Ashkan Panahi, Hamid Krim, and Liyi Dai", "title": "Analysis Dictionary Learning based Classification: Structure for\n  Robustness", "comments": "This manuscript has been accepted and published to IEEE Transactions\n  on Image Processing on June 2019", "journal-ref": null, "doi": "10.1109/TIP.2019.2919409", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discriminative structured analysis dictionary is proposed for the\nclassification task. A structure of the union of subspaces (UoS) is integrated\ninto the conventional analysis dictionary learning to enhance the capability of\ndiscrimination. A simple classifier is also simultaneously included into the\nformulated functional to ensure a more complete consistent classification. The\nsolution of the algorithm is efficiently obtained by the linearized alternating\ndirection method of multipliers. Moreover, a distributed structured analysis\ndictionary learning is also presented to address large scale datasets. It can\ngroup-(class-) independently train the structured analysis dictionaries by\ndifferent machines/cores/threads, and therefore avoid a high computational\ncost. A consensus structured analysis dictionary and a global classifier are\njointly learned in the distributed approach to safeguard the discriminative\npower and the efficiency of classification. Experiments demonstrate that our\nmethod achieves a comparable or better performance than the state-of-the-art\nalgorithms in a variety of visual classification tasks. In addition, the\ntraining and testing computational complexity are also greatly reduced.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 03:47:38 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 02:01:40 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Tang", "Wen", ""], ["Panahi", "Ashkan", ""], ["Krim", "Hamid", ""], ["Dai", "Liyi", ""]]}, {"id": "1807.04917", "submitter": "Abdullah Nazib", "authors": "Abdullah Nazib, James Galloway, Clinton Fookes, Dimitri Perrin", "title": "Performance of Image Registration Tools on High-Resolution 3D Brain\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in tissue clearing has allowed for the imaging of entire\norgans at single-cell resolution. These methods produce very large 3D images\n(several gigabytes for a whole mouse brain). A necessary step in analysing\nthese images is registration across samples. Existing methods of registration\nwere developed for lower resolution image modalities (e.g. MRI) and it is\nunclear whether their performance and accuracy is satisfactory at this larger\nscale. In this study, we used data from different mouse brains cleared with the\nCUBIC protocol to evaluate five freely available image registration tools. We\nused several performance metrics to assess accuracy, and completion time as a\nmeasure of efficiency. The results of this evaluation suggest that the ANTS\nregistration tool provides the best registration accuracy while Elastix has the\nhighest computational efficiency among the methods with an acceptable accuracy.\nThe results also highlight the need to develop new registration methods\noptimised for these high-resolution 3D images.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 05:13:50 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Nazib", "Abdullah", ""], ["Galloway", "James", ""], ["Fookes", "Clinton", ""], ["Perrin", "Dimitri", ""]]}, {"id": "1807.04919", "submitter": "Rafael Valle", "authors": "Rafael Valle and Wilson Cai and Anish Doshi", "title": "TequilaGAN: How to easily identify GAN samples", "comments": "10 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show strategies to easily identify fake samples generated\nwith the Generative Adversarial Network framework. One strategy is based on the\nstatistical analysis and comparison of raw pixel values and features extracted\nfrom them. The other strategy learns formal specifications from the real data\nand shows that fake samples violate the specifications of the real data. We\nshow that fake samples produced with GANs have a universal signature that can\nbe used to identify fake samples. We provide results on MNIST, CIFAR10, music\nand speech data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 05:25:54 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Valle", "Rafael", ""], ["Cai", "Wilson", ""], ["Doshi", "Anish", ""]]}, {"id": "1807.04950", "submitter": "Ismail Elezi", "authors": "Thilo Stadelmann, Mohammadreza Amirian and Ismail Arabaci, Marek\n  Arnold, Gilbert Fran\\c{c}ois Duivesteijn, Ismail Elezi, Melanie Geiger and\n  Stefan L\\\"orwald and Benjamin Bruno Meier, Katharina Rombach and Lukas\n  Tuggener", "title": "Deep Learning in the Wild", "comments": "Invited paper on ANNPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning with neural networks is applied by an increasing number of\npeople outside of classic research environments, due to the vast success of the\nmethodology on a wide range of machine perception tasks. While this interest is\nfueled by beautiful success stories, practical work in deep learning on novel\ntasks without existing baselines remains challenging. This paper explores the\nspecific challenges arising in the realm of real world tasks, based on case\nstudies from research \\& development in conjunction with industry, and extracts\nlessons learned from them. It thus fills a gap between the publication of\nlatest algorithmic and methodical developments, and the usually omitted\nnitty-gritty of how to make them work. Specifically, we give insight into deep\nlearning projects on face matching, print media monitoring, industrial quality\ncontrol, music scanning, strategy game playing, and automated machine learning,\nthereby providing best practices for deep learning in practice.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 07:22:45 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Stadelmann", "Thilo", ""], ["Amirian", "Mohammadreza", ""], ["Arabaci", "Ismail", ""], ["Arnold", "Marek", ""], ["Duivesteijn", "Gilbert Fran\u00e7ois", ""], ["Elezi", "Ismail", ""], ["Geiger", "Melanie", ""], ["L\u00f6rwald", "Stefan", ""], ["Meier", "Benjamin Bruno", ""], ["Rombach", "Katharina", ""], ["Tuggener", "Lukas", ""]]}, {"id": "1807.04960", "submitter": "Lige Zhang", "authors": "Lige Zhang, Xiaolin Qin, Qing Li, Haoyue Peng, Yu Hou", "title": "Single Bitmap Block Truncation Coding of Color Images Using Hill\n  Climbing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the use of digital images in various fields is increasing rapidly.\nTo increase the number of images stored and get faster transmission of them, it\nis necessary to reduce the size of these images. Single bitmap block truncation\ncoding (SBBTC) schemes are compression techniques, which are used to generate a\ncommon bitmap to quantize the R, G and B planes in color image. As one of the\ntraditional SBBTC schemes, weighted plane (W-plane) method is famous for its\nsimplicity and low time consumption. However, the W-plane method also has poor\nperformance in visual quality. This paper proposes an improved SBBTC scheme\nbased on W-plane method using parallel computing and hill climbing algorithm.\nCompared with various schemes, the simulation results of the proposed scheme\nare better than that of the reference schemes in visual quality and time\nconsumption.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 07:57:22 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Zhang", "Lige", ""], ["Qin", "Xiaolin", ""], ["Li", "Qing", ""], ["Peng", "Haoyue", ""], ["Hou", "Yu", ""]]}, {"id": "1807.04975", "submitter": "Sara Beery", "authors": "Sara Beery, Grant van Horn, Pietro Perona", "title": "Recognition in Terra Incognita", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is desirable for detection and classification algorithms to generalize to\nunfamiliar environments, but suitable benchmarks for quantitatively studying\nthis phenomenon are not yet available. We present a dataset designed to measure\nrecognition generalization to novel environments. The images in our dataset are\nharvested from twenty camera traps deployed to monitor animal populations.\nCamera traps are fixed at one location, hence the background changes little\nacross images; capture is triggered automatically, hence there is no human\nbias. The challenge is learning recognition in a handful of locations, and\ngeneralizing animal detection and classification to new locations where no\ntraining data is available. In our experiments state-of-the-art algorithms show\nexcellent performance when tested at the same location where they were trained.\nHowever, we find that generalization to new locations is poor, especially for\nclassification systems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 08:47:59 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 01:59:00 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Beery", "Sara", ""], ["van Horn", "Grant", ""], ["Perona", "Pietro", ""]]}, {"id": "1807.04979", "submitter": "Guojun Yin", "authors": "Guojun Yin, Lu Sheng, Bin Liu, Nenghai Yu, Xiaogang Wang, Jing Shao,\n  Chen Change Loy", "title": "Zoom-Net: Mining Deep Feature Interactions for Visual Relationship\n  Recognition", "comments": "22 pages, 9 figures, accepted by ECCV 2018, the source code will be\n  released on https://github.com/gjyin91/ZoomNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing visual relationships <subject-predicate-object> among any pair of\nlocalized objects is pivotal for image understanding. Previous studies have\nshown remarkable progress in exploiting linguistic priors or external textual\ninformation to improve the performance. In this work, we investigate an\northogonal perspective based on feature interactions. We show that by\nencouraging deep message propagation and interactions between local object\nfeatures and global predicate features, one can achieve compelling performance\nin recognizing complex relationships without using any linguistic priors. To\nthis end, we present two new pooling cells to encourage feature interactions:\n(i) Contrastive ROI Pooling Cell, which has a unique deROI pooling that\ninversely pools local object features to the corresponding area of global\npredicate features. (ii) Pyramid ROI Pooling Cell, which broadcasts global\npredicate features to reinforce local object features.The two cells constitute\na Spatiality-Context-Appearance Module (SCA-M), which can be further stacked\nconsecutively to form our final Zoom-Net.We further shed light on how one could\nresolve ambiguous and noisy object and predicate annotations by\nIntra-Hierarchical trees (IH-tree). Extensive experiments conducted on Visual\nGenome dataset demonstrate the effectiveness of our feature-oriented approach\ncompared to state-of-the-art methods (Acc@1 11.42% from 8.16%) that depend on\nexplicit modeling of linguistic interactions. We further show that SCA-M can be\nincorporated seamlessly into existing approaches to improve the performance by\na large margin. The source code will be released on\nhttps://github.com/gjyin91/ZoomNet.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 09:20:39 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Yin", "Guojun", ""], ["Sheng", "Lu", ""], ["Liu", "Bin", ""], ["Yu", "Nenghai", ""], ["Wang", "Xiaogang", ""], ["Shao", "Jing", ""], ["Loy", "Chen Change", ""]]}, {"id": "1807.05039", "submitter": "Dimitrios Tsourounis", "authors": "Elias N. Zois, Dimitrios Tsourounis, Ilias Theodorakopoulos,\n  Anastasios Kesidis and George Economou", "title": "A comprehensive study of sparse representation techniques for offline\n  signature verification", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a feature extraction method for offline signature verification\nis presented that harnesses the power of sparse representation in order to\ndeliver state-of-the-art verification performance in several signature datasets\nlike CEDAR, MCYT-75, GPDS and UTSIG. Beyond the accuracy improvements, several\nmajor parameters associated with sparse representation; such as selected\nconfiguration, dictionary size, sparsity level and positivity priors are\ninvestigated. Besides, it is evinced that 2nd order statistics of the sparse\ncodes is a powerful pooling function for the formation of the global signature\ndescriptor. Also, a thorough evaluation of the effects of preprocessing is\nintroduced by an automated algorithm in order to select the optimum thinning\nlevel. Finally, a segmentation strategy which employs a special form of spatial\npyramid tailored to the problem of sparse representation is presented along\nwith the enhancing of the produced descriptor on meaningful areas of the\nsignature as emerged from the BRISK key-point detection mechanism. The obtained\nstate-of-the-art results on the most challenging signature datasets provide a\nstrong indication towards the benefits of learned features, even in writer\ndependent (WD) scenarios with a unique model for each writer and only a few\navailable reference samples of him/her.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 12:38:54 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 09:51:37 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2018 08:21:52 GMT"}, {"version": "v4", "created": "Wed, 23 Jan 2019 14:28:43 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Zois", "Elias N.", ""], ["Tsourounis", "Dimitrios", ""], ["Theodorakopoulos", "Ilias", ""], ["Kesidis", "Anastasios", ""], ["Economou", "George", ""]]}, {"id": "1807.05053", "submitter": "Alexandros Kouris", "authors": "Alexandros Kouris, Stylianos I. Venieris and Christos-Savvas Bouganis", "title": "CascadeCNN: Pushing the Performance Limits of Quantisation in\n  Convolutional Neural Networks", "comments": "Accepted for publication at the 28th International Conference on\n  Field Programmable Logic & Applications (FPL), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents CascadeCNN, an automated toolflow that pushes the\nquantisation limits of any given CNN model, aiming to perform high-throughput\ninference. A two-stage architecture tailored for any given CNN-FPGA pair is\ngenerated, consisting of a low- and high-precision unit in a cascade. A\nconfidence evaluation unit is employed to identify misclassified cases from the\nexcessively low-precision unit and forward them to the high-precision unit for\nre-processing. Experiments demonstrate that the proposed toolflow can achieve a\nperformance boost up to 55% for VGG-16 and 48% for AlexNet over the baseline\ndesign for the same resource budget and accuracy, without the need of\nretraining the model or accessing the training data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 13:21:18 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Kouris", "Alexandros", ""], ["Venieris", "Stylianos I.", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1807.05073", "submitter": "Xingyu Liao", "authors": "Xingyu Liao, Lingxiao He, Zhouwang Yang, Chi Zhang", "title": "Video-based Person Re-identification via 3D Convolutional Networks and\n  Non-local Attention", "comments": "arXiv admin note: text overlap with arXiv:1805.02104,\n  arXiv:1711.07971, arXiv:1803.09882 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (ReID) is a challenging problem, where\nsome video tracks of people across non-overlapping cameras are available for\nmatching. Feature aggregation from a video track is a key step for video-based\nperson ReID. Many existing methods tackle this problem by average/maximum\ntemporal pooling or RNNs with attention. However, these methods cannot deal\nwith temporal dependency and spatial misalignment problems at the same time. We\nare inspired by video action recognition that involves the identification of\ndifferent actions from video tracks. Firstly, we use 3D convolutions on video\nvolume, instead of using 2D convolutions across frames, to extract spatial and\ntemporal features simultaneously. Secondly, we use a non-local block to tackle\nthe misalignment problem and capture spatial-temporal long-range dependencies.\nAs a result, the network can learn useful spatial-temporal information as a\nweighted sum of the features in all space and temporal positions in the input\nfeature map. Experimental results on three datasets show that our framework\noutperforms state-of-the-art approaches by a large margin on multiple metrics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 05:30:26 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 05:51:52 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 07:02:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Liao", "Xingyu", ""], ["He", "Lingxiao", ""], ["Yang", "Zhouwang", ""], ["Zhang", "Chi", ""]]}, {"id": "1807.05117", "submitter": "Monica Hernandez", "authors": "Monica Hernandez", "title": "Newton-Krylov PDE-constrained LDDMM in the space of band-limited vector\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PDE-constrained Large Deformation Diffeomorphic Metric Mapping is a\nparticularly interesting framework of physically meaningful diffeomorphic\nregistration methods. Newton-Krylov optimization has shown an excellent\nnumerical accuracy and an extraordinarily fast convergence rate in this\nframework. However, the most significant limitation of PDE-constrained LDDMM is\nthe huge computational complexity, that hinders the extensive use in\nComputational Anatomy applications. In this work, we propose two\nPDE-constrained LDDMM methods parameterized in the space of band-limited vector\nfields and we evaluate their performance with respect to the most related state\nof the art methods. The parameterization in the space of band-limited vector\nfields dramatically alleviates the computational burden avoiding the\ncomputation of the high-frequency components of the velocity fields that would\nbe suppressed by the action of the low-pass filters involved in the computation\nof the gradient and the Hessian-vector products. Besides, the proposed methods\nhave shown an improved accuracy with respect to the benchmark methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 14:59:01 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Hernandez", "Monica", ""]]}, {"id": "1807.05119", "submitter": "Yifang Xu", "authors": "Yifang Xu, Tianli Liao, and Jing Chen", "title": "Learning-based Natural Geometric Matching with Homography Prior", "comments": "13 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric matching is a key step in computer vision tasks. Previous\nlearning-based methods for geometric matching concentrate more on improving\nalignment quality, while we argue the importance of naturalness issue\nsimultaneously. To deal with this, firstly, Pearson correlation is applied to\nhandle large intra-class variations of features in feature matching stage.\nThen, we parametrize homography transformation with 9 parameters in full\nconnected layer of our network, to better characterize large viewpoint\nvariations compared with affine transformation. Furthermore, a novel loss\nfunction with Gaussian weights guarantees the model accuracy and efficiency in\ntraining procedure. Finally, we provide two choices for different purposes in\ngeometric matching. When compositing homography with affine transformation, the\nalignment accuracy improves and all lines are preserved, which results in a\nmore natural transformed image. When compositing homography with non-rigid\nthin-plate-spline transformation, the alignment accuracy further improves.\nExperimental results on Proposal Flow dataset show that our method outperforms\nstate-of-the-art methods, both in terms of alignment accuracy and naturalness.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:01:02 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Xu", "Yifang", ""], ["Liao", "Tianli", ""], ["Chen", "Jing", ""]]}, {"id": "1807.05153", "submitter": "Hongwei Li", "authors": "Hongwei Li, Jianguo Zhang, Mark Muehlau, Jan Kirschke and Bjoern Menze", "title": "Multi-Scale Convolutional-Stack Aggregation for Robust White Matter\n  Hyperintensities Segmentation", "comments": "accepted by MICCAI brain lesion workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of both large and small white matter hyperintensities/lesions in\nbrain MR images is a challenging task which has drawn much attention in recent\nyears. We propose a multi-scale aggregation model framework to deal with\nvolume-varied lesions. Firstly, we present a specifically-designed network for\nsmall lesion segmentation called Stack-Net, in which multiple convolutional\nlayers are connected, aiming to preserve rich local spatial information of\nsmall lesions before the sub-sampling layer. Secondly, we aggregate multi-scale\nStack-Nets with different receptive fields to learn multi-scale contextual\ninformation of both large and small lesions. Our model is evaluated on recent\nMICCAI WMH Challenge Dataset and outperforms the state-of-the-art on lesion\nrecall and lesion F1-score under 5-fold cross validation. In addition, we\nfurther test our pre-trained models on a Multiple Sclerosis lesion dataset with\n30 subjects under cross-center evaluation. Results show that the aggregation\nmodel is effective in learning multi-scale spatial information.It claimed the\nfirst place on the hidden test set after independent evaluation by the\nchallenge organizer. In addition, we further test our pre-trained models on a\nMultiple Sclerosis lesion dataset with 30 subjects under cross-center\nevaluation. Results show that the aggregation model is effective in learning\nmulti-scale spatial information.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 15:56:20 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 21:55:37 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 14:57:19 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Li", "Hongwei", ""], ["Zhang", "Jianguo", ""], ["Muehlau", "Mark", ""], ["Kirschke", "Jan", ""], ["Menze", "Bjoern", ""]]}, {"id": "1807.05162", "submitter": "Yannis Assael", "authors": "Brendan Shillingford, Yannis Assael, Matthew W. Hoffman, Thomas Paine,\n  C\\'ian Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne\n  Bennett, Marie Mulville, Ben Coppin, Ben Laurie, Andrew Senior, Nando de\n  Freitas", "title": "Large-Scale Visual Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a scalable solution to open-vocabulary visual speech\nrecognition. To achieve this, we constructed the largest existing visual speech\nrecognition dataset, consisting of pairs of text and video clips of faces\nspeaking (3,886 hours of video). In tandem, we designed and trained an\nintegrated lipreading system, consisting of a video processing pipeline that\nmaps raw video to stable videos of lips and sequences of phonemes, a scalable\ndeep neural network that maps the lip videos to sequences of phoneme\ndistributions, and a production-level speech decoder that outputs sequences of\nwords. The proposed system achieves a word error rate (WER) of 40.9% as\nmeasured on a held-out set. In comparison, professional lipreaders achieve\neither 86.4% or 92.9% WER on the same dataset when having access to additional\ntypes of contextual information. Our approach significantly improves on other\nlipreading approaches, including variants of LipNet and of Watch, Attend, and\nSpell (WAS), which are only capable of 89.8% and 76.8% WER respectively.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 16:21:34 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 16:44:01 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 11:23:03 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Shillingford", "Brendan", ""], ["Assael", "Yannis", ""], ["Hoffman", "Matthew W.", ""], ["Paine", "Thomas", ""], ["Hughes", "C\u00edan", ""], ["Prabhu", "Utsav", ""], ["Liao", "Hank", ""], ["Sak", "Hasim", ""], ["Rao", "Kanishka", ""], ["Bennett", "Lorrayne", ""], ["Mulville", "Marie", ""], ["Coppin", "Ben", ""], ["Laurie", "Ben", ""], ["Senior", "Andrew", ""], ["de Freitas", "Nando", ""]]}, {"id": "1807.05206", "submitter": "Abdulkareem Alsudais", "authors": "Abdulkareem Alsudais", "title": "Image Classification for Arabic: Assessing the Accuracy of Direct\n  English to Arabic Translations", "comments": null, "journal-ref": "IEEE Access 2019", "doi": "10.1109/ACCESS.2019.2926924", "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image classification is an ongoing research challenge. Most of the available\nresearch focuses on image classification for the English language, however\nthere is very little research on image classification for the Arabic language.\nExpanding image classification to Arabic has several applications. The present\nstudy investigated a method for generating Arabic labels for images of objects.\nThe method used in this study involved a direct English to Arabic translation\nof the labels that are currently available on ImageNet, a database commonly\nused in image classification research. The purpose of this study was to test\nthe accuracy of this method. In this study, 2,887 labeled images were randomly\nselected from ImageNet. All of the labels were translated from English to\nArabic using Google Translate. The accuracy of the translations was evaluated.\nResults indicated that that 65.6% of the Arabic labels were accurate. This\nstudy makes three important contributions to the image classification\nliterature: (1) it determined the baseline level of accuracy for algorithms\nthat provide Arabic labels for images, (2) it provided 1,895 images that are\ntagged with accurate Arabic labels, and (3) provided the accuracy of\ntranslations of image labels from English to Arabic.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 17:44:20 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 11:38:21 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Alsudais", "Abdulkareem", ""]]}, {"id": "1807.05245", "submitter": "Adam Czajka", "authors": "Daniel Moreira, Mateusz Trokielewicz, Adam Czajka, Kevin W. Bowyer,\n  Patrick J. Flynn", "title": "Performance of Humans in Iris Recognition: The Impact of Iris Condition\n  and Annotation-driven Verification", "comments": "Paper accepted for WACV 2019, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper advances the state of the art in human examination of iris images\nby (1) assessing the impact of different iris conditions in identity\nverification, and (2) introducing an annotation step that improves the accuracy\nof people's decisions. In a first experimental session, 114 subjects were asked\nto decide if pairs of iris images depict the same eye (genuine pairs) or two\ndistinct eyes (impostor pairs). The image pairs sampled six conditions: (1)\neasy for algorithms to classify, (2) difficult for algorithms to classify, (3)\nlarge difference in pupil dilation, (4) disease-affected eyes, (5) identical\ntwins, and (6) post-mortem samples. In a second session, 85 of the 114 subjects\nwere asked to annotate matching and non-matching regions that supported their\ndecisions. Subjects were allowed to change their initial classification as a\nresult of the annotation process. Results suggest that: (a) people improve\ntheir identity verification accuracy when asked to annotate matching and\nnon-matching regions between the pair of images, (b) images depicting the same\neye with large difference in pupil dilation were the most challenging to\nsubjects, but benefited well from the annotation-driven classification, (c)\nhumans performed better than iris recognition algorithms when verifying genuine\npairs of post-mortem and disease-affected eyes (i.e., samples showing\ndeformations that go beyond the distortions of a healthy iris due to pupil\ndilation), and (d) annotation does not improve accuracy of analyzing images\nfrom identical twins, which remain confusing for people.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 18:36:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:50:34 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Moreira", "Daniel", ""], ["Trokielewicz", "Mateusz", ""], ["Czajka", "Adam", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1807.05248", "submitter": "Adam Czajka", "authors": "Adam Czajka, Daniel Moreira, Kevin W. Bowyer, Patrick J. Flynn", "title": "Domain-Specific Human-Inspired Binarized Statistical Image Features for\n  Iris Recognition", "comments": "Paper accepted for WACV 2019, Hawaii, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binarized statistical image features (BSIF) have been successfully used for\ntexture analysis in many computer vision tasks, including iris recognition and\nbiometric presentation attack detection. One important point is that all\napplications of BSIF in iris recognition have used the original BSIF filters,\nwhich were trained on image patches extracted from natural images. This paper\ntests the question of whether domain-specific BSIF can give better performance\nthan the default BSIF. The second important point is in the selection of image\npatches to use in training for BSIF. Can image patches derived from\neye-tracking experiments, in which humans perform an iris recognition task,\ngive better performance than random patches? Our results say that (1)\ndomain-specific BSIF features can out-perform the default BSIF features, and\n(2) selecting image patches in a task-specific manner guided by human\nperformance can out-perform selecting random patches. These results are\nimportant because BSIF is often regarded as a generic texture tool that does\nnot need any domain adaptation, and human-task-guided selection of patches for\ntraining has never (to our knowledge) been done. This paper follows the\nreproducible research requirements, and the new iris-domain-specific BSIF\nfilters, the patches used in filter training, the database used in testing and\nthe source codes of the designed iris recognition method are made available\nalong with this paper to facilitate applications of this concept.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 18:57:18 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 03:48:18 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Czajka", "Adam", ""], ["Moreira", "Daniel", ""], ["Bowyer", "Kevin W.", ""], ["Flynn", "Patrick J.", ""]]}, {"id": "1807.05284", "submitter": "Bahram Lavi", "authors": "Bahram Lavi, Mehdi Fatan Serj, and Ihsan Ullah", "title": "Survey on Deep Learning Techniques for Person Re-Identification Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent video-surveillance is currently an active research field in\ncomputer vision and machine learning techniques. It provides useful tools for\nsurveillance operators and forensic video investigators. Person\nre-identification (PReID) is one among these tools. It consists of recognizing\nwhether an individual has already been observed over a camera in a network or\nnot. This tool can also be employed in various possible applications such as\noff-line retrieval of all the video-sequences showing an individual of interest\nwhose image is given a query, and online pedestrian tracking over multiple\ncamera views. To this aim, many techniques have been proposed to increase the\nperformance of PReID. Among the systems, many researchers utilized deep neural\nnetworks (DNNs) because of their better performance and fast execution at test\ntime. Our objective is to provide for future researchers the work being done on\nPReID to date. Therefore, we summarized state-of-the-art DNN models being used\nfor this task. A brief description of each model along with their evaluation on\na set of benchmark datasets is given. Finally, a detailed comparison is\nprovided among these models followed by some limitations that can work as\nguidelines for future research.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 21:18:54 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 14:27:22 GMT"}, {"version": "v3", "created": "Thu, 19 Jul 2018 20:58:50 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Lavi", "Bahram", ""], ["Serj", "Mehdi Fatan", ""], ["Ullah", "Ihsan", ""]]}, {"id": "1807.05333", "submitter": "Hyungjoon Kim", "authors": "Hyungjoon Kim, Hyeonwoo Kim, and Eenjun Hwang", "title": "Real-Time Shape Tracking of Facial Landmarks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of facial landmarks and accurate tracking of their shape are\nessential in real-time virtual makeup applications, where users can see the\nmakeups effect by moving their face in different directions. Typical face\ntracking techniques detect diverse facial landmarks and track them using a\npoint tracker such as the Kanade-Lucas-Tomasi (KLT) point tracker. Typically, 5\nor 64 points are used for tracking a face. Even though these points are\nsufficient to track the approximate locations of facial landmarks, they are not\nsufficient to track the exact shape of facial landmarks. In this paper, we\npropose a method that can track the exact shape of facial landmarks in\nreal-time by combining a deep learning technique and a point tracker. We detect\nfacial landmarks accurately using SegNet, which performs semantic segmentation\nbased on deep learning. Edge points of detected landmarks are tracked using the\nKLT point tracker. In spite of its popularity, the KLT point tracker suffers\nfrom the point loss problem. We solve this problem by executing SegNet\nperiodically to calculate the shape of facial landmarks. That is, by combining\nthe two techniques, we can avoid the computational overhead of SegNet for\nreal-time shape tracking and the point loss problem of the KLT point tracker.\nWe performed several experiments to evaluate the performance of our method and\nreport some of the results herein.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 04:57:16 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Kim", "Hyungjoon", ""], ["Kim", "Hyeonwoo", ""], ["Hwang", "Eenjun", ""]]}, {"id": "1807.05361", "submitter": "Hwann-Tzong Chen", "authors": "Shou-Yao Roy Tseng, Hwann-Tzong Chen, Shao-Heng Tai, and Tyng-Luh Liu", "title": "Non-local RoIs for Instance Segmentation", "comments": "Robust Vision Challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of Non-Local RoI (NL-RoI) Block as a generic and\nflexible module that can be seamlessly adapted into different Mask R-CNN heads\nfor various tasks. Mask R-CNN treats RoIs (Regions of Interest) independently\nand performs the prediction based on individual object bounding boxes. However,\nthe correlation between objects may provide useful information for detection\nand segmentation. The proposed NL-RoI Block enables each RoI to refer to all\nother RoIs' information, and results in a simple, low-cost but effective\nmodule. Our experimental results show that generalizations with NL-RoI Blocks\ncan improve the performance of Mask R-CNN for instance segmentation on the\nRobust Vision Challenge benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 08:58:01 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tseng", "Shou-Yao Roy", ""], ["Chen", "Hwann-Tzong", ""], ["Tai", "Shao-Heng", ""], ["Liu", "Tyng-Luh", ""]]}, {"id": "1807.05380", "submitter": "Masoud Abdi", "authors": "Masoud Abdi, Ehsan Abbasnejad, Chee Peng Lim, Saeid Nahavandi", "title": "3D Hand Pose Estimation using Simulation and Partial-Supervision with a\n  Shared Latent Space", "comments": "Oral presentation at British Machine Vision Conference (BMVC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous amounts of expensive annotated data are a vital ingredient for\nstate-of-the-art 3d hand pose estimation. Therefore, synthetic data has been\npopularized as annotations are automatically available. However, models trained\nonly with synthetic samples do not generalize to real data, mainly due to the\ngap between the distribution of synthetic and real data. In this paper, we\npropose a novel method that seeks to predict the 3d position of the hand using\nboth synthetic and partially-labeled real data. Accordingly, we form a shared\nlatent space between three modalities: synthetic depth image, real depth image,\nand pose. We demonstrate that by carefully learning the shared latent space, we\ncan find a regression model that is able to generalize to real data. As such,\nwe show that our method produces accurate predictions in both semi-supervised\nand unsupervised settings. Additionally, the proposed model is capable of\ngenerating novel, meaningful, and consistent samples from all of the three\ndomains. We evaluate our method qualitatively and quantitively on two highly\ncompetitive benchmarks (i.e., NYU and ICVL) and demonstrate its superiority\nover the state-of-the-art methods. The source code will be made available at\nhttps://github.com/masabdi/LSPS.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 11:26:19 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Abdi", "Masoud", ""], ["Abbasnejad", "Ehsan", ""], ["Lim", "Chee Peng", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1807.05389", "submitter": "Manuel Marin-Jimenez", "authors": "Manuel J. Marin-Jimenez and Francisco J. Romero-Ramirez and Rafael\n  Mu\\~noz-Salinas and Rafael Medina-Carnicer", "title": "3D human pose estimation from depth maps using a deep combination of\n  poses", "comments": "Accepted for publication at \"Journal of Visual Communication and\n  Image Representation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications require the estimation of human body joints for\nhigher-level tasks as, for example, human behaviour understanding. In recent\nyears, depth sensors have become a popular approach to obtain three-dimensional\ninformation. The depth maps generated by these sensors provide information that\ncan be employed to disambiguate the poses observed in two-dimensional images.\nThis work addresses the problem of 3D human pose estimation from depth maps\nemploying a Deep Learning approach. We propose a model, named Deep Depth Pose\n(DDP), which receives a depth map containing a person and a set of predefined\n3D prototype poses and returns the 3D position of the body joints of the\nperson. In particular, DDP is defined as a ConvNet that computes the specific\nweights needed to linearly combine the prototypes for the given input. We have\nthoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which\nrespectively depict realistic and synthetic samples, defining a new\nstate-of-the-art on them.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 12:31:38 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Marin-Jimenez", "Manuel J.", ""], ["Romero-Ramirez", "Francisco J.", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "1807.05439", "submitter": "Shihao Wu", "authors": "Shihao Wu, Hui Huang, Tiziano Portenier, Matan Sela, Danny Cohen-Or,\n  Ron Kimmel, Matthias Zwicker", "title": "Specular-to-Diffuse Translation for Multi-View Reconstruction", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most multi-view 3D reconstruction algorithms, especially when\nshape-from-shading cues are used, assume that object appearance is\npredominantly diffuse. To alleviate this restriction, we introduce S2Dnet, a\ngenerative adversarial network for transferring multiple views of objects with\nspecular reflection into diffuse ones, so that multi-view reconstruction\nmethods can be applied more effectively. Our network extends unsupervised\nimage-to-image translation to multi-view \"specular to diffuse\" translation. To\npreserve object appearance across multiple views, we introduce a Multi-View\nCoherence loss (MVC) that evaluates the similarity and faithfulness of local\npatches after the view-transformation. Our MVC loss ensures that the similarity\nof local correspondences among multi-view images is preserved under the\nimage-to-image translation. As a result, our network yields significantly\nbetter results than several single-view baseline techniques. In addition, we\ncarefully design and generate a large synthetic training data set using\nphysically-based rendering. During testing, our network takes only the raw\nglossy images as input, without extra information such as segmentation masks or\nlighting estimation. Results demonstrate that multi-view reconstruction can be\nsignificantly improved using the images filtered by our network. We also show\npromising performance on real world training and testing data.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 20:51:30 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 13:53:02 GMT"}, {"version": "v3", "created": "Mon, 30 Jul 2018 16:13:07 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wu", "Shihao", ""], ["Huang", "Hui", ""], ["Portenier", "Tiziano", ""], ["Sela", "Matan", ""], ["Cohen-Or", "Danny", ""], ["Kimmel", "Ron", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1807.05478", "submitter": "Bo Fu", "authors": "Bo Fu, Xiao-Yang Zhao, Yong-Gong Ren, Xi-Ming Li, Xiang-Hai Wang", "title": "A salt and pepper noise image denoising method based on the generative\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an image denoising algorithm is proposed for salt and pepper\nnoise. First, a generative model is built on a patch as a basic unit and then\nthe algorithm locates the image noise within that patch in order to better\ndescribe the patch and obtain better subsequent clustering. Second, the\nalgorithm classifies patches using a generative clustering method, thus\nproviding additional similarity information for noise repair and suppressing\nthe interference of noise, abandoning those categories that consist of a\nsmaller number of patches. Finally, the algorithm builds a non-local switching\nfilter to remove the salt and pepper noise. Simulation results show that the\nproposed algorithm effectively denoises salt and pepper noise of various\ndensities. It obtains a better visual quality and higher peak signal-to-noise\nratio score than several state-of-the-art algorithms. In short, our algorithm\nuses a noisy patch as the basic unit, a patch clustering method to optimize the\nrepair data set as well as obtain a better denoising effect, and provides a\nguideline for future denoising and repair methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 02:28:46 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Fu", "Bo", ""], ["Zhao", "Xiao-Yang", ""], ["Ren", "Yong-Gong", ""], ["Li", "Xi-Ming", ""], ["Wang", "Xiang-Hai", ""]]}, {"id": "1807.05482", "submitter": "Zhongliu Xie", "authors": "Zhongliu Xie and Duncan Gillies", "title": "Near Real-time Hippocampus Segmentation Using Patch-based Canonical\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decades, state-of-the-art medical image segmentation has\nheavily rested on signal processing paradigms, most notably registration-based\nlabel propagation and pair-wise patch comparison, which are generally slow\ndespite a high segmentation accuracy. In recent years, deep learning has\nrevolutionalized computer vision with many practices outperforming prior art,\nin particular the convolutional neural network (CNN) studies on image\nclassification. Deep CNN has also started being applied to medical image\nsegmentation lately, but generally involves long training and demanding memory\nrequirements, achieving limited success. We propose a patch-based deep learning\nframework based on a revisit to the classic neural network model with\nsubstantial modernization, including the use of Rectified Linear Unit (ReLU)\nactivation, dropout layers, 2.5D tri-planar patch multi-pathway settings. In a\ntest application to hippocampus segmentation using 100 brain MR images from the\nADNI database, our approach significantly outperformed prior art in terms of\nboth segmentation accuracy and speed: scoring a median Dice score up to 90.98%\non a near real-time performance (<1s).\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 03:23:28 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Xie", "Zhongliu", ""], ["Gillies", "Duncan", ""]]}, {"id": "1807.05485", "submitter": "Thomas Mitchel", "authors": "Thomas Mitchel, Sipu Ruan, Yixin Gao, Gregory S. Chirikjian", "title": "The Globally Optimal Reparameterization Algorithm: an Alternative to\n  Fast Dynamic Time Warping for Action Recognition in Video Sequences", "comments": "ICARCV 2018, initial submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Signal alignment has become a popular problem in robotics due in part to its\nfundamental role in action recognition. Currently, the most successful\nalgorithms for signal alignment are Dynamic Time Warping (DTW) and its variant\n'Fast' Dynamic Time Warping (FastDTW). Here we introduce a new framework for\nsignal alignment, namely the Globally Optimal Reparameterization Algorithm\n(GORA). We review the algorithm's mathematical foundation and provide a\nnumerical verification of its theoretical basis. We compare the performance of\nGORA with that of the DTW and FastDTW algorithms, in terms of computational\nefficiency and accuracy in matching signals. Our results show a significant\nimprovement in both speed and accuracy over the DTW and FastDTW algorithms and\nsuggest that GORA has the potential to provide a highly effective framework for\nsignal alignment and action recognition.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 04:18:48 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mitchel", "Thomas", ""], ["Ruan", "Sipu", ""], ["Gao", "Yixin", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1807.05496", "submitter": "Manik Goyal", "authors": "Manik Goyal, Jagath C. Rajapakse", "title": "Deep neural network ensemble by data augmentation and bagging for skin\n  lesion classification", "comments": "4 pages, 1 figure. ISIC 2018 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work summarizes our submission for the Task 3: Disease Classification of\nISIC 2018 challenge in Skin Lesion Analysis Towards Melanoma Detection. We use\na novel deep neural network (DNN) ensemble architecture introduced by us that\ncan effectively classify skin lesions by using data-augmentation and bagging to\naddress paucity of data and prevent over-fitting. The ensemble is composed of\ntwo DNN architectures: Inception-v4 and Inception-Resnet-v2. The DNN\narchitectures are combined in to an ensemble by using a $1\\times1$ convolution\nfor fusion in a meta-learning layer.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 06:31:41 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:08:20 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Goyal", "Manik", ""], ["Rajapakse", "Jagath C.", ""]]}, {"id": "1807.05511", "submitter": "Zhongqiu Zhao", "authors": "Zhong-Qiu Zhao and Peng Zheng and Shou-tao Xu and Xindong Wu", "title": "Object Detection with Deep Learning: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to object detection's close relationship with video analysis and image\nunderstanding, it has attracted much research attention in recent years.\nTraditional object detection methods are built on handcrafted features and\nshallow trainable architectures. Their performance easily stagnates by\nconstructing complex ensembles which combine multiple low-level image features\nwith high-level context from object detectors and scene classifiers. With the\nrapid development in deep learning, more powerful tools, which are able to\nlearn semantic, high-level, deeper features, are introduced to address the\nproblems existing in traditional architectures. These models behave differently\nin network architecture, training strategy and optimization function, etc. In\nthis paper, we provide a review on deep learning based object detection\nframeworks. Our review begins with a brief introduction on the history of deep\nlearning and its representative tool, namely Convolutional Neural Network\n(CNN). Then we focus on typical generic object detection architectures along\nwith some modifications and useful tricks to improve detection performance\nfurther. As distinct specific detection tasks exhibit different\ncharacteristics, we also briefly survey several specific tasks, including\nsalient object detection, face detection and pedestrian detection. Experimental\nanalyses are also provided to compare various methods and draw some meaningful\nconclusions. Finally, several promising directions and tasks are provided to\nserve as guidelines for future work in both object detection and relevant\nneural network based learning systems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 08:16:03 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 09:24:59 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Zhao", "Zhong-Qiu", ""], ["Zheng", "Peng", ""], ["Xu", "Shou-tao", ""], ["Wu", "Xindong", ""]]}, {"id": "1807.05520", "submitter": "Mathilde Caron", "authors": "Mathilde Caron, Piotr Bojanowski, Armand Joulin and Matthijs Douze", "title": "Deep Clustering for Unsupervised Learning of Visual Features", "comments": "Accepted at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a class of unsupervised learning methods that has been\nextensively applied and studied in computer vision. Little work has been done\nto adapt it to the end-to-end training of visual features on large scale\ndatasets. In this work, we present DeepCluster, a clustering method that\njointly learns the parameters of a neural network and the cluster assignments\nof the resulting features. DeepCluster iteratively groups the features with a\nstandard clustering algorithm, k-means, and uses the subsequent assignments as\nsupervision to update the weights of the network. We apply DeepCluster to the\nunsupervised training of convolutional neural networks on large datasets like\nImageNet and YFCC100M. The resulting model outperforms the current state of the\nart by a significant margin on all the standard benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 09:41:39 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 16:20:30 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Caron", "Mathilde", ""], ["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Douze", "Matthijs", ""]]}, {"id": "1807.05597", "submitter": "Marcus Scheunemann", "authors": "Sander G. van Dijk, Marcus M. Scheunemann", "title": "Deep Learning for Semantic Segmentation on Minimal Hardware", "comments": "12 pages, 5 figures, RoboCup International Symposium 2018", "journal-ref": null, "doi": "10.1007/978-3-030-27544-0_29", "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has revolutionised many fields, but it is still challenging to\ntransfer its success to small mobile robots with minimal hardware.\nSpecifically, some work has been done to this effect in the RoboCup humanoid\nfootball domain, but results that are performant and efficient and still\ngenerally applicable outside of this domain are lacking. We propose an approach\nconceptually different from those taken previously. It is based on semantic\nsegmentation and does achieve these desired properties. In detail, it is being\nable to process full VGA images in real-time on a low-power mobile processor.\nIt can further handle multiple image dimensions without retraining, it does not\nrequire specific domain knowledge for achieving a high frame rate and it is\napplicable on a minimal mobile hardware.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 19:15:41 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["van Dijk", "Sander G.", ""], ["Scheunemann", "Marcus M.", ""]]}, {"id": "1807.05618", "submitter": "Rodolfo Quispe", "authors": "Rodolfo Quispe and Helio Pedrini", "title": "Improved Person Re-Identification Based on Saliency and Semantic Parsing\n  with Deep Neural Network Models", "comments": "person re-identification", "journal-ref": "Image and Vision Computing 2019", "doi": "10.1016/j.imavis.2019.07.009", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a video or an image of a person acquired from a camera, person\nre-identification is the process of retrieving all instances of the same person\nfrom videos or images taken from a different camera with non-overlapping view.\nThis task has applications in various fields, such as surveillance, forensics,\nrobotics, multimedia. In this paper, we present a novel framework, named\nSaliency-Semantic Parsing Re-Identification (SSP-ReID), for taking advantage of\nthe capabilities of both clues: saliency and semantic parsing maps, to guide a\nbackbone convolutional neural network (CNN) to learn complementary\nrepresentations that improves the results over the original backbones. The\ninsight of fusing multiple clues is based on specific scenarios in which one\nresponse is better than another, thus favoring the combination of them to\nincrease performance. Due to its definition, our framework can be easily\napplied to a wide variety of networks and, in contrast to other competitive\nmethods, our training process follows simple and standard protocols. We present\nextensive evaluation of our approach through five backbones and three\nbenchmarks. Experimental results demonstrate the effectiveness of our person\nre-identification framework. In addition, we combine our framework with\nre-ranking techniques to achieve state-of-the-art results on three benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 21:40:00 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Quispe", "Rodolfo", ""], ["Pedrini", "Helio", ""]]}, {"id": "1807.05636", "submitter": "Aravindh Mahendran", "authors": "Aravindh Mahendran, James Thewlis, Andrea Vedaldi", "title": "Cross Pixel Optical Flow Similarity for Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for learning convolutional neural image\nrepresentations without manual supervision. We use motion cues in the form of\noptical flow, to supervise representations of static images. The obvious\napproach of training a network to predict flow from a single image can be\nneedlessly difficult due to intrinsic ambiguities in this prediction task. We\ninstead propose a much simpler learning goal: embed pixels such that the\nsimilarity between their embeddings matches that between their optical flow\nvectors. At test time, the learned deep network can be used without access to\nvideo or flow information and transferred to tasks such as image\nclassification, detection, and segmentation. Our method, which significantly\nsimplifies previous attempts at using motion for self-supervision, achieves\nstate-of-the-art results in self-supervision using motion cues, competitive\nresults for self-supervision in general, and is overall state of the art in\nself-supervised pretraining for semantic image segmentation, as demonstrated on\nstandard benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 23:48:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mahendran", "Aravindh", ""], ["Thewlis", "James", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1807.05648", "submitter": "Euijoon Ahn", "authors": "Euijoon Ahn, Jinman Kim, Ashnil Kumar, Michael Fulham, Dagan Feng", "title": "Convolutional Sparse Kernel Network for Unsupervised Medical Image\n  Analysis", "comments": "Accepted by Medical Image Analysis (with a new title 'Convolutional\n  Sparse Kernel Network for Unsupervised Medical Image Analysis'). The\n  manuscript is available from following link\n  (https://doi.org/10.1016/j.media.2019.06.005)", "journal-ref": null, "doi": "10.1016/j.media.2019.06.005", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large-scale annotated image datasets and recent advances\nin supervised deep learning methods enable the end-to-end derivation of\nrepresentative image features that can impact a variety of image analysis\nproblems. Such supervised approaches, however, are difficult to implement in\nthe medical domain where large volumes of labelled data are difficult to obtain\ndue to the complexity of manual annotation and inter- and intra-observer\nvariability in label assignment. We propose a new convolutional sparse kernel\nnetwork (CSKN), which is a hierarchical unsupervised feature learning framework\nthat addresses the challenge of learning representative visual features in\nmedical image analysis domains where there is a lack of annotated training\ndata. Our framework has three contributions: (i) We extend kernel learning to\nidentify and represent invariant features across image sub-patches in an\nunsupervised manner. (ii) We initialise our kernel learning with a layer-wise\npre-training scheme that leverages the sparsity inherent in medical images to\nextract initial discriminative features. (iii) We adapt a multi-scale spatial\npyramid pooling (SPP) framework to capture subtle geometric differences between\nlearned visual features. We evaluated our framework in medical image retrieval\nand classification on three public datasets. Our results show that our CSKN had\nbetter accuracy when compared to other conventional unsupervised methods and\ncomparable accuracy to methods that used state-of-the-art supervised\nconvolutional neural networks (CNNs). Our findings indicate that our\nunsupervised CSKN provides an opportunity to leverage unannotated big data in\nmedical imaging repositories.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 01:33:00 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 05:05:36 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 02:23:13 GMT"}, {"version": "v4", "created": "Sat, 20 Jun 2020 05:02:28 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ahn", "Euijoon", ""], ["Kim", "Jinman", ""], ["Kumar", "Ashnil", ""], ["Fulham", "Michael", ""], ["Feng", "Dagan", ""]]}, {"id": "1807.05653", "submitter": "Lei Zhou", "authors": "Lei Zhou, Siyu Zhu, Zixin Luo, Tianwei Shen, Runze Zhang, Mingmin\n  Zhen, Tian Fang, Long Quan", "title": "Learning and Matching Multi-View Descriptors for Registration of Point\n  Clouds", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01267-0_31", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Critical to the registration of point clouds is the establishment of a set of\naccurate correspondences between points in 3D space. The correspondence problem\nis generally addressed by the design of discriminative 3D local descriptors on\nthe one hand, and the development of robust matching strategies on the other\nhand. In this work, we first propose a multi-view local descriptor, which is\nlearned from the images of multiple views, for the description of 3D keypoints.\nThen, we develop a robust matching approach, aiming at rejecting outlier\nmatches based on the efficient inference via belief propagation on the defined\ngraphical model. We have demonstrated the boost of our approaches to\nregistration on the public scanning and multi-view stereo datasets. The\nsuperior performance has been verified by the intensive comparisons against a\nvariety of descriptors and matching methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 01:58:27 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Zhou", "Lei", ""], ["Zhu", "Siyu", ""], ["Luo", "Zixin", ""], ["Shen", "Tianwei", ""], ["Zhang", "Runze", ""], ["Zhen", "Mingmin", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1807.05688", "submitter": "Ruimao Zhang", "authors": "Ruimao Zhang, Hongbin Sun, Jingyu Li, Yuying Ge, Liang Lin, Ping Luo,\n  Xiaogang Wang", "title": "SCAN: Self-and-Collaborative Attention Network for Video Person\n  Re-identification", "comments": "10 pages, 5 figures", "journal-ref": "IEEE Transactions on Image Processing 2019", "doi": "10.1109/TIP.2019.2911488", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video person re-identification attracts much attention in recent years. It\naims to match image sequences of pedestrians from different camera views.\nPrevious approaches usually improve this task from three aspects, including a)\nselecting more discriminative frames, b) generating more informative temporal\nrepresentations, and c) developing more effective distance metrics. To address\nthe above issues, we present a novel and practical deep architecture for video\nperson re-identification termed Self-and-Collaborative Attention Network\n(SCAN). It has several appealing properties. First, SCAN adopts non-parametric\nattention mechanism to refine the intra-sequence and inter-sequence feature\nrepresentation of videos, and outputs self-and-collaborative feature\nrepresentation for each video, making the discriminative frames aligned between\nthe probe and gallery sequences.Second, beyond existing models, a generalized\npairwise similarity measurement is proposed to calculate the similarity feature\nrepresentations of video pairs, enabling computing the matching scores by the\nbinary classifier. Third, a dense clip segmentation strategy is also introduced\nto generate rich probe-gallery pairs to optimize the model. Extensive\nexperiments demonstrate the effectiveness of SCAN, which outperforms the\nbest-performing baselines on iLIDS-VID, PRID2011 and MARS dataset,\nrespectively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 06:09:24 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 08:29:56 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2018 07:38:28 GMT"}, {"version": "v4", "created": "Tue, 6 Aug 2019 13:51:34 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhang", "Ruimao", ""], ["Sun", "Hongbin", ""], ["Li", "Jingyu", ""], ["Ge", "Yuying", ""], ["Lin", "Liang", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1807.05696", "submitter": "Dun Liang", "authors": "Dun Liang, Yuanchen Guo, Shaokui Zhang, Song-Hai Zhang, Peter Hall,\n  Min Zhang, Shimin Hu", "title": "LineNet: a Zoomable CNN for Crowdsourced High Definition Maps Modeling\n  in Urban Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Definition (HD) maps play an important role in modern traffic scenes.\nHowever, the development of HD maps coverage grows slowly because of the cost\nlimitation. To efficiently model HD maps, we proposed a convolutional neural\nnetwork with a novel prediction layer and a zoom module, called LineNet. It is\ndesigned for state-of-the-art lane detection in an unordered crowdsourced image\ndataset. And we introduced TTLane, a dataset for efficient lane detection in\nurban road modeling applications. Combining LineNet and TTLane, we proposed a\npipeline to model HD maps with crowdsourced data for the first time. And the\nmaps can be constructed precisely even with inaccurate crowdsourced data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 06:35:48 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Liang", "Dun", ""], ["Guo", "Yuanchen", ""], ["Zhang", "Shaokui", ""], ["Zhang", "Song-Hai", ""], ["Hall", "Peter", ""], ["Zhang", "Min", ""], ["Hu", "Shimin", ""]]}, {"id": "1807.05698", "submitter": "Jianlong Wu", "authors": "Xia Li, Jianlong Wu, Zhouchen Lin, Hong Liu, Hongbin Zha", "title": "Recurrent Squeeze-and-Excitation Context Aggregation Net for Single\n  Image Deraining", "comments": "Accepted by ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rain streaks can severely degrade the visibility, which causes many current\ncomputer vision algorithms fail to work. So it is necessary to remove the rain\nfrom images. We propose a novel deep network architecture based on deep\nconvolutional and recurrent neural networks for single image deraining. As\ncontextual information is very important for rain removal, we first adopt the\ndilated convolutional neural network to acquire large receptive field. To\nbetter fit the rain removal task, we also modify the network. In heavy rain,\nrain streaks have various directions and shapes, which can be regarded as the\naccumulation of multiple rain streak layers. We assign different alpha-values\nto various rain streak layers according to the intensity and transparency by\nincorporating the squeeze-and-excitation block. Since rain streak layers\noverlap with each other, it is not easy to remove the rain in one stage. So we\nfurther decompose the rain removal into multiple stages. Recurrent neural\nnetwork is incorporated to preserve the useful information in previous stages\nand benefit the rain removal in later stages. We conduct extensive experiments\non both synthetic and real-world datasets. Our proposed method outperforms the\nstate-of-the-art approaches under all evaluation metrics. Codes and\nsupplementary material are available at our project webpage:\nhttps://xialipku.github.io/RESCAN .\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 06:49:22 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 14:31:53 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Li", "Xia", ""], ["Wu", "Jianlong", ""], ["Lin", "Zhouchen", ""], ["Liu", "Hong", ""], ["Zha", "Hongbin", ""]]}, {"id": "1807.05705", "submitter": "Thanuja Dharmasiri", "authors": "Thanuja Dharmasiri, Andrew Spek, Tom Drummond", "title": "ENG: End-to-end Neural Geometry for Robust Depth and Pose Estimation\n  using CNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recovering structure and motion parameters given a image pair or a sequence\nof images is a well studied problem in computer vision. This is often achieved\nby employing Structure from Motion (SfM) or Simultaneous Localization and\nMapping (SLAM) algorithms based on the real-time requirements. Recently, with\nthe advent of Convolutional Neural Networks (CNNs) researchers have explored\nthe possibility of using machine learning techniques to reconstruct the 3D\nstructure of a scene and jointly predict the camera pose. In this work, we\npresent a framework that achieves state-of-the-art performance on single image\ndepth prediction for both indoor and outdoor scenes. The depth prediction\nsystem is then extended to predict optical flow and ultimately the camera pose\nand trained end-to-end. Our motion estimation framework outperforms the\nprevious motion prediction systems and we also demonstrate that the\nstate-of-the-art metric depths can be further improved using the knowledge of\npose.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 07:23:56 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 06:02:51 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Dharmasiri", "Thanuja", ""], ["Spek", "Andrew", ""], ["Drummond", "Tom", ""]]}, {"id": "1807.05711", "submitter": "Sounak Ray", "authors": "Suhita Ray", "title": "Disease Classification within Dermascopic Images Using features\n  extracted by ResNet50 and classification through Deep Forest", "comments": "I have decided to not participate in the competition due to poor\n  results in the used methodology and hence would like to withdraw this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we propose a classification technique for skin lesion images\nas a part of our submission for ISIC 2018 Challenge in Skin Lesion Analysis\nTowards Melanoma Detection. Our data was extracted from the ISIC 2018: Skin\nLesion Analysis Towards Melanoma Detection grand challenge datasets. The\nfeatures are extracted through a Convolutional Neural Network, in our case\nResNet50 and then using these features we train a DeepForest, having cascading\nlayers, to classify our skin lesion images. We know that Convolutional Neural\nNetworks are a state-of-the-art technique in representation learning for\nimages, with the convolutional filters learning to detect features from images\nthrough backpropagation. These features are then usually fed to a classifier\nlike a softmax layer or other such classifiers for classification tasks. In our\ncase we do not use the traditional backpropagation method and train a softmax\nlayer for classification. Instead, we use Deep Forest, a novel decision tree\nensemble approach with performance highly competitive to deep neural networks\nin a broad range of tasks. Thus we use a ResNet50 to extract the features from\nskin lesion images and then use the Deep Forest to classify these images. This\nmethod has been used because Deep Forest has been found to be hugely efficient\nin areas where there are only small-scale training data available. Also as the\nDeep Forest network decides its complexity by itself, it also caters to the\nproblem of dataset imbalance we faced in this problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 07:57:31 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 13:06:12 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 13:30:50 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ray", "Suhita", ""]]}, {"id": "1807.05713", "submitter": "Xinyi Tong", "authors": "Xin-Yi Tong, Gui-Song Xia, Qikai Lu, Huanfeng Shen, Shengyang Li,\n  Shucheng You, Liangpei Zhang", "title": "Land-Cover Classification with High-Resolution Remote Sensing Images\n  Using Transferable Deep Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large amount of high spatial-resolution remote sensing\n(HRRS) images are available for land-cover mapping. However, due to the complex\ninformation brought by the increased spatial resolution and the data\ndisturbances caused by different conditions of image acquisition, it is often\ndifficult to find an efficient method for achieving accurate land-cover\nclassification with high-resolution and heterogeneous remote sensing images. In\nthis paper, we propose a scheme to apply deep model obtained from labeled\nland-cover dataset to classify unlabeled HRRS images. The main idea is to rely\non deep neural networks for presenting the contextual information contained in\ndifferent types of land-covers and propose a pseudo-labeling and sample\nselection scheme for improving the transferability of deep models. More\nprecisely, a deep Convolutional Neural Networks is first pre-trained with a\nwell-annotated land-cover dataset, referred to as the source data. Then, given\na target image with no labels, the pre-trained CNN model is utilized to\nclassify the image in a patch-wise manner. The patches with high confidence are\nassigned with pseudo-labels and employed as the queries to retrieve related\nsamples from the source data. The pseudo-labels confirmed with the retrieved\nresults are regarded as supervised information for fine-tuning the pre-trained\ndeep model. To obtain a pixel-wise land-cover classification with the target\nimage, we rely on the fine-tuned CNN and develop a hybrid classification by\ncombining patch-wise classification and hierarchical segmentation. In addition,\nwe create a large-scale land-cover dataset containing 150 Gaofen-2 satellite\nimages for CNN pre-training. Experiments on multi-source HRRS images show\nencouraging results and demonstrate the applicability of the proposed scheme to\nland-cover classification.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:02:10 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 21:04:34 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Tong", "Xin-Yi", ""], ["Xia", "Gui-Song", ""], ["Lu", "Qikai", ""], ["Shen", "Huanfeng", ""], ["Li", "Shengyang", ""], ["You", "Shucheng", ""], ["Zhang", "Liangpei", ""]]}, {"id": "1807.05726", "submitter": "Yu-Hsun Lin", "authors": "Yu-Hsun Lin, Chun-Nan Chou and Edward Y. Chang", "title": "BRIEF: Backward Reduction of CNNs with Information Flow Analysis", "comments": "IEEE Artificial Intelligence and Virtual Reality (IEEE AIVR) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes BRIEF, a backward reduction algorithm that explores\ncompact CNN-model designs from the information flow perspective. This algorithm\ncan remove substantial non-zero weighting parameters (redundant neural\nchannels) of a network by considering its dynamic behavior, which traditional\nmodel-compaction techniques cannot achieve. With the aid of our proposed\nalgorithm, we achieve significant model reduction on ResNet-34 in the ImageNet\nscale (32.3% reduction), which is 3X better than the previous result (10.8%).\nEven for highly optimized models such as SqueezeNet and MobileNet, we can\nachieve additional 10.81% and 37.56% reduction, respectively, with negligible\nperformance degradation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:32:54 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 02:57:28 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 02:35:47 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Lin", "Yu-Hsun", ""], ["Chou", "Chun-Nan", ""], ["Chang", "Edward Y.", ""]]}, {"id": "1807.05771", "submitter": "Rizwan Ahmed Khan", "authors": "Rumaisah Munir, Rizwan Ahmed Khan", "title": "An Extensive Review on Spectral Imaging in Biometric Systems: Challenges\n  and Advancements", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation, 2019", "doi": "10.1016/j.jvcir.2019.102660", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectral imaging has recently gained traction for face recognition in\nbiometric systems. We investigate the merits of spectral imaging for face\nrecognition and the current challenges that hamper the widespread deployment of\nspectral sensors for face recognition. The reliability of conventional face\nrecognition systems operating in the visible range is compromised by\nillumination changes, pose variations and spoof attacks. Recent works have\nreaped the benefits of spectral imaging to counter these limitations in\nsurveillance activities (defence, airport security checks, etc.). However, the\nimplementation of this technology for biometrics, is still in its infancy due\nto multiple reasons. We present an overview of the existing work in the domain\nof spectral imaging for face recognition, different types of modalities and\ntheir assessment, availability of public databases for sake of reproducible\nresearch as well as evaluation of algorithms, and recent advancements in the\nfield, such as, the use of deep learning-based methods for recognizing faces\nfrom spectral images.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 10:24:28 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 07:16:04 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Munir", "Rumaisah", ""], ["Khan", "Rizwan Ahmed", ""]]}, {"id": "1807.05786", "submitter": "Konstantin Bulatov", "authors": "Vladimir V. Arlazarov, Konstantin Bulatov, Timofey Chernov, Vladimir\n  L. Arlazarov", "title": "MIDV-500: A Dataset for Identity Documents Analysis and Recognition on\n  Mobile Devices in Video Stream", "comments": "7 pages, 6 figures, 5 tables", "journal-ref": "Computer optics 43 N5 (2019) 818-824", "doi": "10.18287/2412-6179-2019-43-5-818-824", "report-no": null, "categories": "cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of research has been devoted to identity documents analysis and\nrecognition on mobile devices. However, no publicly available datasets designed\nfor this particular problem currently exist. There are a few datasets which are\nuseful for associated subtasks but in order to facilitate a more comprehensive\nscientific and technical approach to identity document recognition more\nspecialized datasets are required. In this paper we present a Mobile Identity\nDocument Video dataset (MIDV-500) consisting of 500 video clips for 50\ndifferent identity document types with ground truth which allows to perform\nresearch in a wide scope of document analysis problems. The paper presents\ncharacteristics of the dataset and evaluation results for existing methods of\nface detection, text line recognition, and document fields data extraction.\nSince an important feature of identity documents is their sensitiveness as they\ncontain personal data, all source document images used in MIDV-500 are either\nin public domain or distributed under public copyright licenses.\n  The main goal of this paper is to present a dataset. However, in addition and\nas a baseline, we present evaluation results for existing methods for face\ndetection, text line recognition, and document data extraction, using the\npresented dataset.\n  (The dataset is available for download at ftp://smartengines.com/midv-500/.)\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 10:51:10 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 12:17:23 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 08:30:01 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 07:28:48 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Arlazarov", "Vladimir V.", ""], ["Bulatov", "Konstantin", ""], ["Chernov", "Timofey", ""], ["Arlazarov", "Vladimir L.", ""]]}, {"id": "1807.05799", "submitter": "Xinxing Su", "authors": "Xinxing Su, Yingtian Zou, Yu Cheng, Shuangjie Xu, Mo Yu, Pan Zhou", "title": "Spatial-Temporal Synergic Residual Learning for Video Person\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of person re-identification in video setting in this\npaper, which has been viewed as a crucial task in many applications. Meanwhile,\nit is very challenging since the task requires learning effective\nrepresentations from video sequences with heterogeneous spatial-temporal\ninformation. We present a novel method - Spatial-Temporal Synergic Residual\nNetwork (STSRN) for this problem. STSRN contains a spatial residual extractor,\na temporal residual processor and a spatial-temporal smooth module. The\nsmoother can alleviate sample noises along the spatial-temporal dimensions thus\nenable STSRN extracts more robust spatial-temporal features of consecutive\nframes. Extensive experiments are conducted on several challenging datasets\nincluding iLIDS-VID, PRID2011 and MARS. The results demonstrate that the\nproposed method achieves consistently superior performance over most of\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 11:39:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Su", "Xinxing", ""], ["Zou", "Yingtian", ""], ["Cheng", "Yu", ""], ["Xu", "Shuangjie", ""], ["Yu", "Mo", ""], ["Zhou", "Pan", ""]]}, {"id": "1807.05800", "submitter": "Takashi Matsubara", "authors": "Takashi Matsubara, Kenta Hama, Ryosuke Tachibana, Kuniaki Uehara", "title": "Deep Generative Model using Unregularized Score for Anomaly Detection\n  with Heterogeneous Complexity", "comments": "An extended version of a manuscript in Proc. of The 2018\n  International Joint Conference on Neural Networks (IJCNN2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and automated detection of anomalous samples in a natural image\ndataset can be accomplished with a probabilistic model for end-to-end modeling\nof images. Such images have heterogeneous complexity, however, and a\nprobabilistic model overlooks simply shaped objects with small anomalies. This\nis because the probabilistic model assigns undesirably lower likelihoods to\ncomplexly shaped objects that are nevertheless consistent with set standards.\nTo overcome this difficulty, we propose an unregularized score for deep\ngenerative models (DGMs), which are generative models leveraging deep neural\nnetworks. We found that the regularization terms of the DGMs considerably\ninfluence the anomaly score depending on the complexity of the samples. By\nremoving these terms, we obtain an unregularized score, which we evaluated on a\ntoy dataset and real-world manufacturing datasets. Empirical results\ndemonstrate that the unregularized score is robust to the inherent complexity\nof samples and can be used to better detect anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 11:41:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 13:14:38 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Matsubara", "Takashi", ""], ["Hama", "Kenta", ""], ["Tachibana", "Ryosuke", ""], ["Uehara", "Kuniaki", ""]]}, {"id": "1807.05838", "submitter": "Ranju Mandal Dr.", "authors": "Ranju Mandal, Rod M. Connolly, Thomas A. Schlacherz, Bela Stantic", "title": "Assessing fish abundance from underwater video using deep neural\n  networks", "comments": "IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uses of underwater videos to assess diversity and abundance of fish are being\nrapidly adopted by marine biologists. Manual processing of videos for\nquantification by human analysts is time and labour intensive. Automatic\nprocessing of videos can be employed to achieve the objectives in a cost and\ntime-efficient way. The aim is to build an accurate and reliable fish detection\nand recognition system, which is important for an autonomous robotic platform.\nHowever, there are many challenges involved in this task (e.g. complex\nbackground, deformation, low resolution and light propagation). Recent\nadvancement in the deep neural network has led to the development of object\ndetection and recognition in real time scenarios. An end-to-end deep\nlearning-based architecture is introduced which outperformed the state of the\nart methods and first of its kind on fish assessment task. A Region Proposal\nNetwork (RPN) introduced by an object detector termed as Faster R-CNN was\ncombined with three classification networks for detection and recognition of\nfish species obtained from Remote Underwater Video Stations (RUVS). An accuracy\nof 82.4% (mAP) obtained from the experiments are much higher than previously\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:13:37 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mandal", "Ranju", ""], ["Connolly", "Rod M.", ""], ["Schlacherz", "Thomas A.", ""], ["Stantic", "Bela", ""]]}, {"id": "1807.05854", "submitter": "Jacob Shermeyer", "authors": "Jacob Shermeyer", "title": "Assessment of electrical and infrastructure recovery in Puerto Rico\n  following hurricane Maria using a multisource time series of satellite\n  imagery", "comments": "15 pages, 5 figures, 5 videos", "journal-ref": null, "doi": "10.1117/12.2325585", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Puerto Rico suffered severe damage from the category 5 hurricane (Maria) in\nSeptember 2017. Total monetary damages are estimated to be ~92 billion USD, the\nthird most costly tropical cyclone in US history. The response to this damage\nhas been tempered and slow moving, with recent estimates placing 45% of the\npopulation without power three months after the storm. Consequently, we\ndeveloped a unique data-fusion mapping approach called the Urban Development\nIndex (UDI) and new open source tool, Comet Time Series (CometTS), to analyze\nthe recovery of electricity and infrastructure in Puerto Rico. Our approach\nincorporates a combination of time series visualizations and change detection\nmapping to create depictions of power or infrastructure loss. It also provides\na unique independent assessment of areas that are still struggling to recover.\nFor this workflow, our time series approach combines nighttime imagery from the\nSuomi National Polar-orbiting Partnership Visible Infrared Imaging Radiometer\nSuite (NPP VIIRS), multispectral imagery from two Landsat satellites, US Census\ndata, and crowd-sourced building footprint labels. Based upon our approach we\ncan identify and evaluate: 1) the recovery of electrical power compared to\npre-storm levels, 2) the location of potentially damaged infrastructure that\nhas yet to recover from the storm, and 3) the number of persons without power\nover time. As of May 31, 2018, declined levels of observed brightness across\nthe island indicate that 13.9% +/- ~5.6% of persons still lack power and/or\nthat 13.2% +/- ~5.3% of infrastructure has been lost. In comparison, the Puerto\nRico Electric Power Authority states that less than 1% of their customers still\nare without power.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:38:57 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Shermeyer", "Jacob", ""]]}, {"id": "1807.05857", "submitter": "Li Zhou", "authors": "Li Zhou, Jian Zhao, Jianshu Li, Li Yuan, Jiashi Feng", "title": "Object Relation Detection Based on One-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting the relations among objects, such as \"cat on sofa\" and \"person ride\nhorse\", is a crucial task in image understanding, and beneficial to bridging\nthe semantic gap between images and natural language. Despite the remarkable\nprogress of deep learning in detection and recognition of individual objects,\nit is still a challenging task to localize and recognize the relations between\nobjects due to the complex combinatorial nature of various kinds of object\nrelations. Inspired by the recent advances in one-shot learning, we propose a\nsimple yet effective Semantics Induced Learner (SIL) model for solving this\nchallenging task. Learning in one-shot manner can enable a detection model to\nadapt to a huge number of object relations with diverse appearance effectively\nand robustly. In addition, the SIL combines bottom-up and top-down attention\nmech- anisms, therefore enabling attention at the level of vision and semantics\nfavorably. Within our proposed model, the bottom-up mechanism, which is based\non Faster R-CNN, proposes objects regions, and the top-down mechanism selects\nand integrates visual features according to semantic information. Experiments\ndemonstrate the effectiveness of our framework over other state-of-the-art\nmethods on two large-scale data sets for object relation detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:42:28 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zhou", "Li", ""], ["Zhao", "Jian", ""], ["Li", "Jianshu", ""], ["Yuan", "Li", ""], ["Feng", "Jiashi", ""]]}, {"id": "1807.05927", "submitter": "Ram Krishna Pandey", "authors": "Ram Krishna Pandey, Samarjit Karmakar and A G Ramakrishnan", "title": "Computationally Efficient Approaches for Image Style Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we have investigated various style transfer approaches and (i)\nexamined how the stylized reconstruction changes with the change of loss\nfunction and (ii) provided a computationally efficient solution for the same.\nWe have used elegant techniques like depth-wise separable convolution in place\nof convolution and nearest neighbor interpolation in place of transposed\nconvolution. Further, we have also added multiple interpolations in place of\ntransposed convolution. The results obtained are perceptually similar in\nquality, while being computationally very efficient. The decrease in the\ncomputational complexity of our architecture is validated by the decrease in\nthe testing time by 26.1%, 39.1%, and 57.1%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:43:12 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Pandey", "Ram Krishna", ""], ["Karmakar", "Samarjit", ""], ["Ramakrishnan", "A G", ""]]}, {"id": "1807.05933", "submitter": "Paul Gay", "authors": "Paul Gay, Stuart James, Alessio Del Bue", "title": "Visual Graphs from Motion (VGfM): Scene understanding with object\n  geometry reasoning", "comments": "Accepted to ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches on visual scene understanding attempt to build a scene\ngraph -- a computational representation of objects and their pairwise\nrelationships. Such rich semantic representation is very appealing, yet\ndifficult to obtain from a single image, especially when considering complex\nspatial arrangements in the scene. Differently, an image sequence conveys\nuseful information using the multi-view geometric relations arising from camera\nmotion. Indeed, in such cases, object relationships are naturally related to\nthe 3D scene structure. To this end, this paper proposes a system that first\ncomputes the geometrical location of objects in a generic scene and then\nefficiently constructs scene graphs from video by embedding such geometrical\nreasoning. Such compelling representation is obtained using a new model where\ngeometric and visual features are merged using an RNN framework. We report\nresults on a dataset we created for the task of 3D scene graph generation in\nmultiple views.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:49:43 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2018 17:32:06 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Gay", "Paul", ""], ["James", "Stuart", ""], ["Del Bue", "Alessio", ""]]}, {"id": "1807.05959", "submitter": "Debanjan Mahata", "authors": "Mayank Meghawat, Satyendra Yadav, Debanjan Mahata, Yifang Yin, Rajiv\n  Ratn Shah, Roger Zimmermann", "title": "A Multimodal Approach to Predict Social Media Popularity", "comments": "Preprint version for paper accepted in Proceedings of 1st IEEE\n  International Conference on Multimedia Information Processing and Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple modalities represent different aspects by which information is\nconveyed by a data source. Modern day social media platforms are one of the\nprimary sources of multimodal data, where users use different modes of\nexpression by posting textual as well as multimedia content such as images and\nvideos for sharing information. Multimodal information embedded in such posts\ncould be useful in predicting their popularity. To the best of our knowledge,\nno such multimodal dataset exists for the prediction of social media photos. In\nthis work, we propose a multimodal dataset consisiting of content, context, and\nsocial information for popularity prediction. Specifically, we augment the\nSMPT1 dataset for social media prediction in ACM Multimedia grand challenge\n2017 with image content, titles, descriptions, and tags. Next, in this paper,\nwe propose a multimodal approach which exploits visual features (i.e., content\ninformation), textual features (i.e., contextual information), and social\nfeatures (e.g., average views and group counts) to predict popularity of social\nmedia photos in terms of view counts. Experimental results confirm that despite\nour multimodal approach uses the half of the training dataset from SMP-T1, it\nachieves comparable performance with that of state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:35:23 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Meghawat", "Mayank", ""], ["Yadav", "Satyendra", ""], ["Mahata", "Debanjan", ""], ["Yin", "Yifang", ""], ["Shah", "Rajiv Ratn", ""], ["Zimmermann", "Roger", ""]]}, {"id": "1807.05960", "submitter": "Dushyant Rao", "authors": "Andrei A. Rusu, Dushyant Rao, Jakub Sygnowski, Oriol Vinyals, Razvan\n  Pascanu, Simon Osindero, and Raia Hadsell", "title": "Meta-Learning with Latent Embedding Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based meta-learning techniques are both widely applicable and\nproficient at solving challenging few-shot learning and fast adaptation\nproblems. However, they have practical difficulties when operating on\nhigh-dimensional parameter spaces in extreme low-data regimes. We show that it\nis possible to bypass these limitations by learning a data-dependent latent\ngenerative representation of model parameters, and performing gradient-based\nmeta-learning in this low-dimensional latent space. The resulting approach,\nlatent embedding optimization (LEO), decouples the gradient-based adaptation\nprocedure from the underlying high-dimensional space of model parameters. Our\nevaluation shows that LEO can achieve state-of-the-art performance on the\ncompetitive miniImageNet and tieredImageNet few-shot classification tasks.\nFurther analysis indicates LEO is able to capture uncertainty in the data, and\ncan perform adaptation more effectively by optimizing in latent space.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:35:29 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 16:38:43 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 13:36:45 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rusu", "Andrei A.", ""], ["Rao", "Dushyant", ""], ["Sygnowski", "Jakub", ""], ["Vinyals", "Oriol", ""], ["Pascanu", "Razvan", ""], ["Osindero", "Simon", ""], ["Hadsell", "Raia", ""]]}, {"id": "1807.05972", "submitter": "Zhongliu Xie", "authors": "Zhongliu Xie", "title": "Towards Single-phase Single-stage Detection of Pulmonary Nodules in\n  Chest CT Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of pulmonary nodules in chest CT imaging plays a crucial role in\nearly diagnosis of lung cancer. Manual examination is highly time-consuming and\nerror prone, calling for computer-aided detection, both to improve efficiency\nand reduce misdiagnosis. Over the years, a range of systems have been proposed,\nmostly following a two-phase paradigm with: 1) candidate detection, 2) false\npositive reduction. Recently, deep learning has become a dominant force in\nalgorithm development. As for candidate detection, prior art was mainly based\non the two-stage Faster R-CNN framework, which starts with an initial sub-net\nto generate a set of class-agnostic region proposals, followed by a second\nsub-net to perform classification and bounding-box regression. In contrast, we\nabandon the conventional two-phase paradigm and two-stage framework altogether\nand propose to train a single network for end-to-end nodule detection instead,\nwithout transfer learning or further post-processing. Our feature learning\nmodel is a modification of the ResNet and feature pyramid network combined,\npowered by RReLU activation. The major challenge is the condition of extreme\ninter-class and intra-class sample imbalance, where the positives are\noverwhelmed by a large negative pool, which is mostly composed of easy and a\nhandful of hard negatives. Direct training on all samples can seriously\nundermine training efficacy. We propose a patch-based sampling strategy over a\nset of regularly updating anchors, which narrows sampling scope to all\npositives and only hard negatives, effectively addressing this issue. As a\nresult, our approach substantially outperforms prior art in terms of both\naccuracy and speed. Finally, the prevailing FROC evaluation over [1/8, 1/4,\n1/2, 1, 2, 4, 8] false positives per scan, is far from ideal in real clinical\nenvironments. We suggest FROC over [1, 2, 4] false positives as a better\nmetric.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 17:10:11 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Xie", "Zhongliu", ""]]}, {"id": "1807.05983", "submitter": "Amir Soleimani", "authors": "Amir Soleimani, Nasser M. Nasrabadi", "title": "Convolutional Neural Networks for Aerial Multi-Label Pedestrian\n  Detection", "comments": "This paper has been accepted in the 21st International Conference on\n  Information Fusion and would be indexed in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The low resolution of objects of interest in aerial images makes pedestrian\ndetection and action detection extremely challenging tasks. Furthermore, using\ndeep convolutional neural networks to process large images can be demanding in\nterms of computational requirements. In order to alleviate these challenges, we\npropose a two-step, yes and no question answering framework to find specific\nindividuals doing one or multiple specific actions in aerial images. First, a\ndeep object detector, Single Shot Multibox Detector (SSD), is used to generate\nobject proposals from small aerial images. Second, another deep network, is\nused to learn a latent common sub-space which associates the high resolution\naerial imagery and the pedestrian action labels that are provided by the\nhuman-based sources\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 17:25:54 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Soleimani", "Amir", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1807.06009", "submitter": "Yinda Zhang", "authors": "Yinda Zhang, Sameh Khamis, Christoph Rhemann, Julien Valentin, Adarsh\n  Kowdle, Vladimir Tankovich, Michael Schoenberg, Shahram Izadi, Thomas\n  Funkhouser, Sean Fanello", "title": "ActiveStereoNet: End-to-End Self-Supervised Learning for Active Stereo\n  Systems", "comments": "Accepted by ECCV2018, Oral Presentation, Main paper + Supplementary\n  Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present ActiveStereoNet, the first deep learning solution\nfor active stereo systems. Due to the lack of ground truth, our method is fully\nself-supervised, yet it produces precise depth with a subpixel precision of\n$1/30th$ of a pixel; it does not suffer from the common over-smoothing issues;\nit preserves the edges; and it explicitly handles occlusions. We introduce a\nnovel reconstruction loss that is more robust to noise and texture-less\npatches, and is invariant to illumination changes. The proposed loss is\noptimized using a window-based cost aggregation with an adaptive support weight\nscheme. This cost aggregation is edge-preserving and smooths the loss function,\nwhich is key to allow the network to reach compelling results. Finally we show\nhow the task of predicting invalid regions, such as occlusions, can be trained\nend-to-end without ground-truth. This component is crucial to reduce blur and\nparticularly improves predictions along depth discontinuities. Extensive\nquantitatively and qualitatively evaluations on real and synthetic data\ndemonstrate state of the art results in many challenging scenes.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 16:55:19 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Zhang", "Yinda", ""], ["Khamis", "Sameh", ""], ["Rhemann", "Christoph", ""], ["Valentin", "Julien", ""], ["Kowdle", "Adarsh", ""], ["Tankovich", "Vladimir", ""], ["Schoenberg", "Michael", ""], ["Izadi", "Shahram", ""], ["Funkhouser", "Thomas", ""], ["Fanello", "Sean", ""]]}, {"id": "1807.06010", "submitter": "Lequan Yu", "authors": "Lequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, Pheng-Ann Heng", "title": "EC-Net: an Edge-aware Point set Consolidation Network", "comments": "accepted by ECCV2018; project in https://yulequan.github.io/ec-net/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point clouds obtained from 3D scans are typically sparse, irregular, and\nnoisy, and required to be consolidated. In this paper, we present the first\ndeep learning based edge-aware technique to facilitate the consolidation of\npoint clouds. We design our network to process points grouped in local patches,\nand train it to learn and help consolidate points, deliberately for edges. To\nachieve this, we formulate a regression component to simultaneously recover 3D\npoint coordinates and point-to-edge distances from upsampled features, and an\nedge-aware joint loss function to directly minimize distances from output\npoints to 3D meshes and to edges. Compared with previous neural network based\nworks, our consolidation is edge-aware. During the synthesis, our network can\nattend to the detected sharp edges and enable more accurate 3D reconstructions.\nAlso, we trained our network on virtual scanned point clouds, demonstrated the\nperformance of our method on both synthetic and real point clouds, presented\nvarious surface reconstruction results, and showed how our method outperforms\nthe state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 17:44:18 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Yu", "Lequan", ""], ["Li", "Xianzhi", ""], ["Fu", "Chi-Wing", ""], ["Cohen-Or", "Daniel", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "1807.06056", "submitter": "Matt Angus", "authors": "Matt Angus, Mohamed ElBalkini, Samin Khan, Ali Harakeh, Oles\n  Andrienko, Cody Reading, Steven Waslander, Krzysztof Czarnecki", "title": "Unlimited Road-scene Synthetic Annotation (URSA) Dataset", "comments": "Accepted in The 21st IEEE International Conference on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In training deep neural networks for semantic segmentation, the main limiting\nfactor is the low amount of ground truth annotation data that is available in\ncurrently existing datasets. The limited availability of such data is due to\nthe time cost and human effort required to accurately and consistently label\nreal images on a pixel level. Modern sandbox video game engines provide open\nworld environments where traffic and pedestrians behave in a pseudo-realistic\nmanner. This caters well to the collection of a believable road-scene dataset.\nUtilizing open-source tools and resources found in single-player modding\ncommunities, we provide a method for persistent, ground truth, asset annotation\nof a game world. By collecting a synthetic dataset containing upwards of\n$1,000,000$ images, we demonstrate real-time, on-demand, ground truth data\nannotation capability of our method. Supplementing this synthetic data to\nCityscapes dataset, we show that our data generation method provides\nqualitative as well as quantitative improvements---for training networks---over\nprevious methods that use video games as surrogate.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 18:45:49 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Angus", "Matt", ""], ["ElBalkini", "Mohamed", ""], ["Khan", "Samin", ""], ["Harakeh", "Ali", ""], ["Andrienko", "Oles", ""], ["Reading", "Cody", ""], ["Waslander", "Steven", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1807.06067", "submitter": "Chaochao Yan", "authors": "Chaochao Yan and Jiawen Yao and Ruoyu Li and Zheng Xu and Junzhou\n  Huang", "title": "Weakly Supervised Deep Learning for Thoracic Disease Classification and\n  Localization on Chest X-rays", "comments": "10 pages. Accepted by the ACM BCB 2018", "journal-ref": null, "doi": "10.1145/3233547.3233573", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chest X-rays is one of the most commonly available and affordable\nradiological examinations in clinical practice. While detecting thoracic\ndiseases on chest X-rays is still a challenging task for machine intelligence,\ndue to 1) the highly varied appearance of lesion areas on X-rays from patients\nof different thoracic disease and 2) the shortage of accurate pixel-level\nannotations by radiologists for model training. Existing machine learning\nmethods are unable to deal with the challenge that thoracic diseases usually\nhappen in localized disease-specific areas. In this article, we propose a\nweakly supervised deep learning framework equipped with squeeze-and-excitation\nblocks, multi-map transfer, and max-min pooling for classifying thoracic\ndiseases as well as localizing suspicious lesion regions. The comprehensive\nexperiments and discussions are performed on the ChestX-ray14 dataset. Both\nnumerical and visual results have demonstrated the effectiveness of the\nproposed model and its better performance against the state-of-the-art\npipelines.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:19:38 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Yan", "Chaochao", ""], ["Yao", "Jiawen", ""], ["Li", "Ruoyu", ""], ["Xu", "Zheng", ""], ["Huang", "Junzhou", ""]]}, {"id": "1807.06081", "submitter": "Max-Heinrich Laves M. Sc.", "authors": "Max-Heinrich Laves, Jens Bicker, L\\\"uder A. Kahrs, Tobias Ortmaier", "title": "A Dataset of Laryngeal Endoscopic Images with Comparative Study on\n  Convolution Neural Network Based Semantic Segmentation", "comments": "Accepted for publication in International Journal of Computer\n  Assisted Radiology and Surgery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose Automated segmentation of anatomical structures in medical image\nanalysis is a prerequisite for autonomous diagnosis as well as various computer\nand robot aided interventions. Recent methods based on deep convolutional\nneural networks (CNN) have outperformed former heuristic methods. However,\nthose methods were primarily evaluated on rigid, real-world environments. In\nthis study, existing segmentation methods were evaluated for their use on a new\ndataset of transoral endoscopic exploration. Methods Four machine learning\nbased methods SegNet, UNet, ENet and ErfNet were trained with supervision on a\nnovel 7-class dataset of the human larynx. The dataset contains 536 manually\nsegmented images from two patients during laser incisions. The\nIntersection-over-Union (IoU) evaluation metric was used to measure the\naccuracy of each method. Data augmentation and network ensembling were employed\nto increase segmentation accuracy. Stochastic inference was used to show\nuncertainties of the individual models. Patient-to-patient transfer was\ninvestigated using patient-specific fine-tuning. Results In this study, a\nweighted average ensemble network of UNet and ErfNet was best suited for the\nsegmentation of laryngeal soft tissue with a mean IoU of 84.7 %. The highest\nefficiency was achieved by ENet with a mean inference time of 9.22 ms per\nimage. It is shown that 10 additional images from a new patient are sufficient\nfor patient-specific fine-tuning. Conclusion CNN-based methods for semantic\nsegmentation are applicable to endoscopic images of laryngeal soft tissue. The\nsegmentation can be used for active constraints or to monitor morphological\nchanges and autonomously detect pathologies. Further improvements could be\nachieved by using a larger dataset or training the models in a self-supervised\nmanner on additional unlabeled data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 19:56:13 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 18:17:04 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 09:26:26 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 13:42:59 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Laves", "Max-Heinrich", ""], ["Bicker", "Jens", ""], ["Kahrs", "L\u00fcder A.", ""], ["Ortmaier", "Tobias", ""]]}, {"id": "1807.06089", "submitter": "Michael Schwier", "authors": "Michael Schwier, Joost van Griethuysen, Mark G Vangel, Steve Pieper,\n  Sharon Peled, Clare M Tempany, Hugo JWL Aerts, Ron Kikinis, Fiona M Fennessy,\n  Andrey Fedorov", "title": "Repeatability of Multiparametric Prostate MRI Radiomics Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we assessed the repeatability of the values of radiomics\nfeatures for small prostate tumors using test-retest Multiparametric Magnetic\nResonance Imaging (mpMRI) images. The premise of radiomics is that quantitative\nimage features can serve as biomarkers characterizing disease. For such\nbiomarkers to be useful, repeatability is a basic requirement, meaning its\nvalue must remain stable between two scans, if the conditions remain stable. We\ninvestigated repeatability of radiomics features under various preprocessing\nand extraction configurations including various image normalization schemes,\ndifferent image pre-filtering, 2D vs 3D texture computation, and different bin\nwidths for image discretization. Image registration as means to re-identify\nregions of interest across time points was evaluated against human-expert\nsegmented regions in both time points. Even though we found many radiomics\nfeatures and preprocessing combinations with a high repeatability (Intraclass\nCorrelation Coefficient (ICC) > 0.85), our results indicate that overall the\nrepeatability is highly sensitive to the processing parameters (under certain\nconfigurations, it can be below 0.0). Image normalization, using a variety of\napproaches considered, did not result in consistent improvements in\nrepeatability. There was also no consistent improvement of repeatability\nthrough the use of pre-filtering options, or by using image registration\nbetween timepoints to improve consistency of the region of interest\nlocalization. Based on these results we urge caution when interpreting\nradiomics features and advise paying close attention to the processing\nconfiguration details of reported results. Furthermore, we advocate reporting\nall processing details in radiomics studies and strongly recommend making the\nimplementation available.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 20:17:18 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 05:31:01 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Schwier", "Michael", ""], ["van Griethuysen", "Joost", ""], ["Vangel", "Mark G", ""], ["Pieper", "Steve", ""], ["Peled", "Sharon", ""], ["Tempany", "Clare M", ""], ["Aerts", "Hugo JWL", ""], ["Kikinis", "Ron", ""], ["Fennessy", "Fiona M", ""], ["Fedorov", "Andrey", ""]]}, {"id": "1807.06110", "submitter": "James Pritts", "authors": "James Pritts, Zuzana Kukelova, Viktor Larsson, Ondrej Chum", "title": "Rectification from Radially-Distorted Scales", "comments": "pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the first minimal solvers that jointly estimate lens\ndistortion and affine rectification from repetitions of rigidly transformed\ncoplanar local features. The proposed solvers incorporate lens distortion into\nthe camera model and extend accurate rectification to wide-angle images that\ncontain nearly any type of coplanar repeated content. We demonstrate a\nprincipled approach to generating stable minimal solvers by the Grobner basis\nmethod, which is accomplished by sampling feasible monomial bases to maximize\nnumerical stability. Synthetic and real-image experiments confirm that the\nsolvers give accurate rectifications from noisy measurements when used in a\nRANSAC-based estimator. The proposed solvers demonstrate superior robustness to\nnoise compared to the state-of-the-art. The solvers work on scenes without\nstraight lines and, in general, relax the strong assumptions on scene content\nmade by the state-of-the-art. Accurate rectifications on imagery that was taken\nwith narrow focal length to near fish-eye lenses demonstrate the wide\napplicability of the proposed method. The method is fully automated, and the\ncode is publicly available at https://github.com/prittjam/repeats.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 21:03:36 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 11:43:26 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 16:38:52 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 12:02:51 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Pritts", "James", ""], ["Kukelova", "Zuzana", ""], ["Larsson", "Viktor", ""], ["Chum", "Ondrej", ""]]}, {"id": "1807.06132", "submitter": "Fatemeh Sadat Saleh", "authors": "Fatemeh Sadat Saleh, Mohammad Sadegh Aliakbarian, Mathieu Salzmann,\n  Lars Petersson, Jose M. Alvarez", "title": "Effective Use of Synthetic Data for Urban Scene Semantic Segmentation", "comments": "Accepted in European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a deep network to perform semantic segmentation requires large\namounts of labeled data. To alleviate the manual effort of annotating real\nimages, researchers have investigated the use of synthetic data, which can be\nlabeled automatically. Unfortunately, a network trained on synthetic data\nperforms relatively poorly on real images. While this can be addressed by\ndomain adaptation, existing methods all require having access to real images\nduring training. In this paper, we introduce a drastically different way to\nhandle synthetic images that does not require seeing any real images at\ntraining time. Our approach builds on the observation that foreground and\nbackground classes are not affected in the same manner by the domain shift, and\nthus should be treated differently. In particular, the former should be handled\nin a detection-based manner to better account for the fact that, while their\ntexture in synthetic images is not photo-realistic, their shape looks natural.\nOur experiments evidence the effectiveness of our approach on Cityscapes and\nCamVid with models trained on synthetic data only.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 22:10:09 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Saleh", "Fatemeh Sadat", ""], ["Aliakbarian", "Mohammad Sadegh", ""], ["Salzmann", "Mathieu", ""], ["Petersson", "Lars", ""], ["Alvarez", "Jose M.", ""]]}, {"id": "1807.06144", "submitter": "Ruggiero Santeramo", "authors": "Ruggiero Santeramo, Samuel Withey, Giovanni Montana", "title": "Longitudinal detection of radiological abnormalities with time-modulated\n  LSTM", "comments": "Submitted to 4th MICCAI Workshop on Deep Learning in Medical Imaging\n  Analysis", "journal-ref": "DLMIA/ML-CDS@MICCAI 2018", "doi": "10.1007/978-3-030-00889-5_37", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have been successfully employed in\nrecent years for the detection of radiological abnormalities in medical images\nsuch as plain x-rays. To date, most studies use CNNs on individual examinations\nin isolation and discard previously available clinical information. In this\nstudy we set out to explore whether Long-Short-Term-Memory networks (LSTMs) can\nbe used to improve classification performance when modelling the entire\nsequence of radiographs that may be available for a given patient, including\ntheir reports. A limitation of traditional LSTMs, though, is that they\nimplicitly assume equally-spaced observations, whereas the radiological exams\nare event-based, and therefore irregularly sampled. Using both a simulated\ndataset and a large-scale chest x-ray dataset, we demonstrate that a simple\nmodification of the LSTM architecture, which explicitly takes into account the\ntime lag between consecutive observations, can boost classification\nperformance. Our empirical results demonstrate improved detection of commonly\nreported abnormalities on chest x-rays such as cardiomegaly, consolidation,\npleural effusion and hiatus hernia.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 22:53:46 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Santeramo", "Ruggiero", ""], ["Withey", "Samuel", ""], ["Montana", "Giovanni", ""]]}, {"id": "1807.06160", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj", "title": "Layer-wise Relevance Propagation for Explainable Recommendations", "comments": "Accepted in Proceedings of the EARS Workshop at SIGIR 2018", "journal-ref": "Homanga Bharadhwaj. 2018. Layer-wise Relevance Propagation for\n  Explainable Recommendations. In Proceedings of SIGIR 2018 Workshop on\n  ExplainAble Recommendation and Search (EARS'18). ACM, New York, NY, USA", "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of explanations in a deep-learning based\nmodel for recommendations by leveraging the technique of layer-wise relevance\npropagation. We use a Deep Convolutional Neural Network to extract relevant\nfeatures from the input images before identifying similarity between the images\nin feature space. Relationships between the images are identified by the model\nand layer-wise relevance propagation is used to infer pixel-level details of\nthe images that may have significantly informed the model's choice. We evaluate\nour method on an Amazon products dataset and demonstrate the efficacy of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 00:38:31 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bharadhwaj", "Homanga", ""]]}, {"id": "1807.06196", "submitter": "Mireille Boutin", "authors": "Christian Tendyck and Andrew Haddad and Mireille Boutin", "title": "Photo-unrealistic Image Enhancement for Subject Placement in Outdoor\n  Photography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Camera display reflections are an issue in bright light situations, as they\nmay prevent users from correctly positioning the subject in the picture. We\npropose a software solution to this problem, which consists in modifying the\nimage in the viewer, in real time. In our solution, the user is seeing a\nposterized image which roughly represents the contour of the objects. Five\nenhancement methods are compared in a user study. Our results indicate that the\nproblem considered is a valid one, as users had problems locating landmarks\nnearly 37% of the time under sunny conditions, and that our proposed\nenhancement method using contrasting colors is a practical solution to that\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 03:24:14 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Tendyck", "Christian", ""], ["Haddad", "Andrew", ""], ["Boutin", "Mireille", ""]]}, {"id": "1807.06216", "submitter": "Peng Qiao", "authors": "Peng Qiao, Yong Dou, Yunjin Chen, Wensen Feng", "title": "Learning Generic Diffusion Processes for Image Restoration", "comments": "12 pages, 3 figures, 3 tables", "journal-ref": "British Machine Vision Conference 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration problems are typical ill-posed problems where the\nregularization term plays an important role. The regularization term learned\nvia generative approaches is easy to transfer to various image restoration, but\noffers inferior restoration quality compared with that learned via\ndiscriminative approaches. On the contrary, the regularization term learned via\ndiscriminative approaches are usually trained for a specific image restoration\nproblem, and fail in the problem for which it is not trained. To address this\nissue, we propose a generic diffusion process (genericDP) to handle multiple\nGaussian denoising problems based on the Trainable Non-linear Reaction\nDiffusion (TNRD) models. Instead of one model, which consists of a diffusion\nand a reaction term, for one Gaussian denoising problem in TNRD, we enforce\nmultiple TNRD models to share one diffusion term. The trained genericDP model\ncan provide both promising denoising performance and high training efficiency\ncompared with the original TNRD models. We also transfer the trained diffusion\nterm to non-blind deconvolution which is unseen in the training phase.\nExperiment results show that the trained diffusion term for multiple Gaussian\ndenoising can be transferred to image non-blind deconvolution as an image prior\nand provide competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 04:21:29 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Qiao", "Peng", ""], ["Dou", "Yong", ""], ["Chen", "Yunjin", ""], ["Feng", "Wensen", ""]]}, {"id": "1807.06233", "submitter": "Jaekyum Kim", "authors": "Jaekyum Kim, Junho Koh, Yecheol Kim, Jaehyung Choi, Youngbae Hwang,\n  Jun Won Choi", "title": "Robust Deep Multi-modal Learning Based on Gated Information Fusion\n  Network", "comments": "2018 Asian Conference on Computer Vision (ACCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of multi-modal learning is to use complimentary information on the\nrelevant task provided by the multiple modalities to achieve reliable and\nrobust performance. Recently, deep learning has led significant improvement in\nmulti-modal learning by allowing for the information fusion in the intermediate\nfeature levels. This paper addresses a problem of designing robust deep\nmulti-modal learning architecture in the presence of imperfect modalities. We\nintroduce deep fusion architecture for object detection which processes each\nmodality using the separate convolutional neural network (CNN) and constructs\nthe joint feature map by combining the intermediate features from the CNNs. In\norder to facilitate the robustness to the degraded modalities, we employ the\ngated information fusion (GIF) network which weights the contribution from each\nmodality according to the input feature maps to be fused. The weights are\ndetermined through the convolutional layers followed by a sigmoid function and\ntrained along with the information fusion network in an end-to-end fashion. Our\nexperiments show that the proposed GIF network offers the additional\narchitectural flexibility to achieve robust performance in handling some\ndegraded modalities, and show a significant performance improvement based on\nSingle Shot Detector (SSD) for KITTI dataset using the proposed fusion network\nand data augmentation schemes.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 05:42:32 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 11:02:01 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kim", "Jaekyum", ""], ["Koh", "Junho", ""], ["Kim", "Yecheol", ""], ["Choi", "Jaehyung", ""], ["Hwang", "Youngbae", ""], ["Choi", "Jun Won", ""]]}, {"id": "1807.06270", "submitter": "Animesh Prasad", "authors": "Animesh Prasad, Herv\\'e D\\'ejean, Jean-Luc Meunier, Max Weidemann,\n  Johannes Michael, Gundram Leifert", "title": "Bench-Marking Information Extraction in Semi-Structured Historical\n  Handwritten Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present our findings from benchmarking experiments for\ninformation extraction on historical handwritten marriage records Esposalles\nfrom IEHHR - ICDAR 2017 robust reading competition. The information extraction\nis modeled as semantic labeling of the sequence across 2 set of labels. This\ncan be achieved by sequentially or jointly applying handwritten text\nrecognition (HTR) and named entity recognition (NER). We deploy a pipeline\napproach where first we use state-of-the-art HTR and use its output as input\nfor NER. We show that given low resource setup and simple structure of the\nrecords, high performance of HTR ensures overall high performance. We explore\nthe various configurations of conditional random fields and neural networks to\nbenchmark NER on given certain noisy input. The best model on 10-fold\ncross-validation as well as blind test data uses n-gram features with\nbidirectional long short-term memory.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:13:19 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Prasad", "Animesh", ""], ["D\u00e9jean", "Herv\u00e9", ""], ["Meunier", "Jean-Luc", ""], ["Weidemann", "Max", ""], ["Michael", "Johannes", ""], ["Leifert", "Gundram", ""]]}, {"id": "1807.06271", "submitter": "Boitumelo Ruf", "authors": "Boitumelo Ruf, Sebastian Monka, Matthias Kollmann, Michael Grinberg", "title": "Real-time on-board obstacle avoidance for UAVs based on embedded stereo\n  vision", "comments": "Accepted in the International Archives of the Photogrammetry, Remote\n  Sensing and Spatial Information Science", "journal-ref": "Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., XLII-1,\n  363-370, 2018", "doi": "10.5194/isprs-archives-XLII-1-363-2018", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to improve usability and safety, modern unmanned aerial vehicles\n(UAVs) are equipped with sensors to monitor the environment, such as\nlaser-scanners and cameras. One important aspect in this monitoring process is\nto detect obstacles in the flight path in order to avoid collisions. Since a\nlarge number of consumer UAVs suffer from tight weight and power constraints,\nour work focuses on obstacle avoidance based on a lightweight stereo camera\nsetup. We use disparity maps, which are computed from the camera images, to\nlocate obstacles and to automatically steer the UAV around them. For disparity\nmap computation we optimize the well-known semi-global matching (SGM) approach\nfor the deployment on an embedded FPGA. The disparity maps are then converted\ninto simpler representations, the so called U-/V-Maps, which are used for\nobstacle detection. Obstacle avoidance is based on a reactive approach which\nfinds the shortest path around the obstacles as soon as they have a critical\ndistance to the UAV. One of the fundamental goals of our work was the reduction\nof development costs by closing the gap between application development and\nhardware optimization. Hence, we aimed at using high-level synthesis (HLS) for\nporting our algorithms, which are written in C/C++, to the embedded FPGA. We\nevaluated our implementation of the disparity estimation on the KITTI Stereo\n2015 benchmark. The integrity of the overall realtime reactive obstacle\navoidance algorithm has been evaluated by using Hardware-in-the-Loop testing in\nconjunction with two flight simulators.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:17:10 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 20:22:38 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ruf", "Boitumelo", ""], ["Monka", "Sebastian", ""], ["Kollmann", "Matthias", ""], ["Grinberg", "Michael", ""]]}, {"id": "1807.06277", "submitter": "Paul Jaeger", "authors": "Jennifer Kamphenkel, Paul F. Jaeger, Sebastian Bickelhaupt, Frederik\n  Bernd Laun, Wolfgang Lederer, Heidi Daniel, Tristan Anselm Kuder, Stefan\n  Delorme, Heinz-Peter Schlemmer, Franziska Koenig and Klaus H. Maier-Hein", "title": "Domain Adaptation for Deviating Acquisition Protocols in CNN-based\n  Lesion Classification on Diffusion-Weighted MR Images", "comments": "accepted at the MICCAI workshop on Breast Image Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end deep learning improves breast cancer classification on\ndiffusion-weighted MR images (DWI) using a convolutional neural network (CNN)\narchitecture. A limitation of CNN as opposed to previous model-based approaches\nis the dependence on specific DWI input channels used during training. However,\nin the context of large-scale application, methods agnostic towards\nheterogeneous inputs are desirable, due to the high deviation of scanning\nprotocols between clinical sites. We propose model-based domain adaptation to\novercome input dependencies and avoid re-training of networks at clinical sites\nby restoring training inputs from altered input channels given during\ndeployment. We demonstrate the method's significant increase in classification\nperformance and superiority over implicit domain adaptation provided by\ntraining-schemes operating on model-parameters instead of raw DWI images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:33:43 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Kamphenkel", "Jennifer", ""], ["Jaeger", "Paul F.", ""], ["Bickelhaupt", "Sebastian", ""], ["Laun", "Frederik Bernd", ""], ["Lederer", "Wolfgang", ""], ["Daniel", "Heidi", ""], ["Kuder", "Tristan Anselm", ""], ["Delorme", "Stefan", ""], ["Schlemmer", "Heinz-Peter", ""], ["Koenig", "Franziska", ""], ["Maier-Hein", "Klaus H.", ""]]}, {"id": "1807.06288", "submitter": "Yuan Wang", "authors": "Yuan Wang, Tianyue Shi, Peng Yun, Lei Tai, Ming Liu", "title": "PointSeg: Real-Time Semantic Segmentation Based on 3D LiDAR Point Cloud", "comments": "Video link: https://youtu.be/b1BAbcjJ10s code link:\n  https://github.com/ywangeq/PointSeg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose PointSeg, a real-time end-to-end semantic\nsegmentation method for road-objects based on spherical images. We take the\nspherical image, which is transformed from the 3D LiDAR point clouds, as input\nof the convolutional neural networks (CNNs) to predict the point-wise semantic\nmap. To make PointSeg applicable on a mobile system, we build the model based\non the light-weight network, SqueezeNet, with several improvements. It\nmaintains a good balance between memory cost and prediction performance. Our\nmodel is trained on spherical images and label masks projected from the KITTI\n3D object detection dataset. Experiments show that PointSeg can achieve\ncompetitive accuracy with 90fps on a single GPU 1080ti. which makes it quite\ncompatible for autonomous driving applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 09:06:30 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 12:13:54 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 09:59:49 GMT"}, {"version": "v4", "created": "Sun, 9 Sep 2018 07:52:11 GMT"}, {"version": "v5", "created": "Fri, 14 Sep 2018 13:35:00 GMT"}, {"version": "v6", "created": "Mon, 17 Sep 2018 11:53:28 GMT"}, {"version": "v7", "created": "Tue, 18 Sep 2018 13:21:11 GMT"}, {"version": "v8", "created": "Tue, 25 Sep 2018 07:41:47 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Wang", "Yuan", ""], ["Shi", "Tianyue", ""], ["Yun", "Peng", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""]]}, {"id": "1807.06294", "submitter": "Zixin Luo", "authors": "Zixin Luo, Tianwei Shen, Lei Zhou, Siyu Zhu, Runze Zhang, Yao Yao,\n  Tian Fang and Long Quan", "title": "GeoDesc: Learning Local Descriptors by Integrating Geometry Constraints", "comments": "Accepted to ECCV'18", "journal-ref": null, "doi": "10.1007/978-3-030-01240-3_11", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learned local descriptors based on Convolutional Neural Networks (CNNs) have\nachieved significant improvements on patch-based benchmarks, whereas not having\ndemonstrated strong generalization ability on recent benchmarks of image-based\n3D reconstruction. In this paper, we mitigate this limitation by proposing a\nnovel local descriptor learning approach that integrates geometry constraints\nfrom multi-view reconstructions, which benefits the learning process in terms\nof data generation, data sampling and loss computation. We refer to the\nproposed descriptor as GeoDesc, and demonstrate its superior performance on\nvarious large-scale benchmarks, and in particular show its great success on\nchallenging reconstruction tasks. Moreover, we provide guidelines towards\npractical integration of learned descriptors in Structure-from-Motion (SfM)\npipelines, showing the good trade-off that GeoDesc delivers to 3D\nreconstruction tasks between accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 09:31:34 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 12:46:10 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Luo", "Zixin", ""], ["Shen", "Tianwei", ""], ["Zhou", "Lei", ""], ["Zhu", "Siyu", ""], ["Zhang", "Runze", ""], ["Yao", "Yao", ""], ["Fang", "Tian", ""], ["Quan", "Long", ""]]}, {"id": "1807.06329", "submitter": "Takao Yamanaka", "authors": "Tatsuya Suzuki and Takao Yamanaka", "title": "Saliency Map Estimation for Omni-Directional Image Considering Prior\n  Distributions", "comments": "SMC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the deep learning techniques have been applied to the\nestimation of saliency maps, which represent probability density functions of\nfixations when people look at the images. Although the methods of saliency-map\nestimation have been actively studied for 2-dimensional planer images, the\nmethods for omni-directional images to be utilized in virtual environments had\nnot been studied, until a competition of saliency-map estimation for the\nomni-directional images was held in ICME2017. In this paper, novel methods for\nestimating saliency maps for the omni-directional images are proposed\nconsidering the properties of prior distributions for fixations in the planar\nimages and the omni-directional images.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 10:43:35 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Suzuki", "Tatsuya", ""], ["Yamanaka", "Takao", ""]]}, {"id": "1807.06356", "submitter": "Fabian Balsiger", "authors": "Fabian Balsiger, Amaresha Shridhar Konar, Shivaprasad Chikop, Vimal\n  Chandran, Olivier Scheidegger, Sairam Geethanath, Mauricio Reyes", "title": "Magnetic Resonance Fingerprinting Reconstruction via Spatiotemporal\n  Convolutional Neural Networks", "comments": "Accepted for Machine Learning for Medical Image Reconstruction\n  (MLMIR) workshop at MICCAI 2018. The revision corrects Amaresha's last name\n  and Section 2.1 (scanner type and flip angles)", "journal-ref": null, "doi": "10.1007/978-3-030-00129-2_5", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance fingerprinting (MRF) quantifies multiple nuclear magnetic\nresonance parameters in a single and fast acquisition. Standard MRF\nreconstructs parametric maps using dictionary matching, which lacks scalability\ndue to computational inefficiency. We propose to perform MRF map reconstruction\nusing a spatiotemporal convolutional neural network, which exploits the\nrelationship between neighboring MRF signal evolutions to replace the\ndictionary matching. We evaluate our method on multiparametric brain scans and\ncompare it to three recent MRF reconstruction approaches. Our method achieves\nstate-of-the-art reconstruction accuracy and yields qualitatively more\nappealing maps compared to other reconstruction methods. In addition, the\nreconstruction time is significantly reduced compared to a dictionary-based\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:33:51 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 10:28:45 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Balsiger", "Fabian", ""], ["Konar", "Amaresha Shridhar", ""], ["Chikop", "Shivaprasad", ""], ["Chandran", "Vimal", ""], ["Scheidegger", "Olivier", ""], ["Geethanath", "Sairam", ""], ["Reyes", "Mauricio", ""]]}, {"id": "1807.06358", "submitter": "Huaibo Huang", "authors": "Huaibo Huang, Zhihang Li, Ran He, Zhenan Sun, Tieniu Tan", "title": "IntroVAE: Introspective Variational Autoencoders for Photographic Image\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel introspective variational autoencoder (IntroVAE) model for\nsynthesizing high-resolution photographic images. IntroVAE is capable of\nself-evaluating the quality of its generated samples and improving itself\naccordingly. Its inference and generator models are jointly trained in an\nintrospective way. On one hand, the generator is required to reconstruct the\ninput images from the noisy outputs of the inference model as normal VAEs. On\nthe other hand, the inference model is encouraged to classify between the\ngenerated and real samples while the generator tries to fool it as GANs. These\ntwo famous generative frameworks are integrated in a simple yet efficient\nsingle-stream architecture that can be trained in a single stage. IntroVAE\npreserves the advantages of VAEs, such as stable training and nice latent\nmanifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires\nno extra discriminators, because the inference model itself serves as a\ndiscriminator to distinguish between the generated and real samples.\nExperiments demonstrate that our method produces high-resolution\nphoto-realistic images (e.g., CELEBA images at \\(1024^{2}\\)), which are\ncomparable to or better than the state-of-the-art GANs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:37:31 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 13:46:18 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Huang", "Huaibo", ""], ["Li", "Zhihang", ""], ["He", "Ran", ""], ["Sun", "Zhenan", ""], ["Tan", "Tieniu", ""]]}, {"id": "1807.06403", "submitter": "Filippos Kokkinos", "authors": "Filippos Kokkinos, Stamatios Lefkimmiatis", "title": "Iterative Joint Image Demosaicking and Denoising using a Residual\n  Denoising Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.05215", "journal-ref": null, "doi": "10.1109/TIP.2019.2905991", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern digital cameras rely on the sequential execution of separate image\nprocessing steps to produce realistic images. The first two steps are usually\nrelated to denoising and demosaicking where the former aims to reduce noise\nfrom the sensor and the latter converts a series of light intensity readings to\ncolor images. Modern approaches try to jointly solve these problems, i.e. joint\ndenoising-demosaicking which is an inherently ill-posed problem given that\ntwo-thirds of the intensity information is missing and the rest are perturbed\nby noise. While there are several machine learning systems that have been\nrecently introduced to solve this problem, the majority of them relies on\ngeneric network architectures which do not explicitly take into account the\nphysical image model. In this work we propose a novel algorithm which is\ninspired by powerful classical image regularization methods, large-scale\noptimization, and deep learning techniques. Consequently, our derived iterative\noptimization algorithm, which involves a trainable denoising network, has a\ntransparent and clear interpretation compared to other black-box data driven\napproaches. Our extensive experimentation line demonstrates that our proposed\nmethod outperforms any previous approaches for both noisy and noise-free data\nacross many different datasets. This improvement in reconstruction quality is\nattributed to the rigorous derivation of an iterative solution and the\nprincipled way we design our denoising network architecture, which as a result\nrequires fewer trainable parameters than the current state-of-the-art solution\nand furthermore can be efficiently trained by using a significantly smaller\nnumber of training data than existing deep demosaicking networks. Code and\nresults can be found at https://github.com/cig-skoltech/deep_demosaick\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:17:46 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 10:08:43 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 12:34:51 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kokkinos", "Filippos", ""], ["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1807.06416", "submitter": "Pierluigi Carcagni", "authors": "Pierluigi Carcagn\\`i, Andrea Cuna and Cosimo Distante", "title": "A Dense CNN approach for skin lesion classification", "comments": "ISIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a Deep CNN, based on the DenseNet architecture jointly\nwith a highly discriminating learning methodology, in order to classify seven\nkinds of skin lesions: Melanoma, Melanocytic nevus, Basal cell carcinoma,\nActinic keratosis / Bowen's disease, Benign keratosis, Dermatofibroma, Vascular\nlesion. In particular a 61 layers DenseNet, pre-trained on IMAGENET dataset,\nhas been fine-tuned on ISIC 2018 Task 3 Challenge Dataset exploiting a Center\nLoss function.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 13:33:41 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 10:42:50 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Carcagn\u00ec", "Pierluigi", ""], ["Cuna", "Andrea", ""], ["Distante", "Cosimo", ""]]}, {"id": "1807.06450", "submitter": "Alessandro Betti", "authors": "Alessandro Betti, Marco Gori, Stefano Melacci", "title": "Motion Invariance in Visual Environments", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.07110", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The puzzle of computer vision might find new challenging solutions when we\nrealize that most successful methods are working at image level, which is\nremarkably more difficult than processing directly visual streams, just as\nhappens in nature. In this paper, we claim that their processing naturally\nleads to formulate the motion invariance principle, which enables the\nconstruction of a new theory of visual learning based on convolutional\nfeatures. The theory addresses a number of intriguing questions that arise in\nnatural vision, and offers a well-posed computational scheme for the discovery\nof convolutional filters over the retina. They are driven by the Euler-Lagrange\ndifferential equations derived from the principle of least cognitive action,\nthat parallels laws of mechanics. Unlike traditional convolutional networks,\nwhich need massive supervision, the proposed theory offers a truly new scenario\nin which feature learning takes place by unsupervised processing of video\nsignals. An experimental report of the theory is presented where we show that\nfeatures extracted under motion invariance yield an improvement that can be\nassessed by measuring information-based indexes.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 07:16:42 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Betti", "Alessandro", ""], ["Gori", "Marco", ""], ["Melacci", "Stefano", ""]]}, {"id": "1807.06466", "submitter": "Hongming Xu", "authors": "Hongming Xu and Tae Hyun Hwang", "title": "Automatic Skin Lesion Segmentation Using Deep Fully Convolutional\n  Networks", "comments": "4 pages, ISIC Challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our method and validation results for the ISIC\nChallenge 2018 - Skin Lesion Analysis Towards Melanoma Detection - Task 1:\nLesion Segmentation\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:24:37 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Xu", "Hongming", ""], ["Hwang", "Tae Hyun", ""]]}, {"id": "1807.06514", "submitter": "Jongchan Park", "authors": "Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So Kweon", "title": "BAM: Bottleneck Attention Module", "comments": "Accepted to BMVC 2018 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep neural networks have been developed via architecture\nsearch for stronger representational power. In this work, we focus on the\neffect of attention in general deep neural networks. We propose a simple and\neffective attention module, named Bottleneck Attention Module (BAM), that can\nbe integrated with any feed-forward convolutional neural networks. Our module\ninfers an attention map along two separate pathways, channel and spatial. We\nplace our module at each bottleneck of models where the downsampling of feature\nmaps occurs. Our module constructs a hierarchical attention at bottlenecks with\na number of parameters and it is trainable in an end-to-end manner jointly with\nany feed-forward models. We validate our BAM through extensive experiments on\nCIFAR-100, ImageNet-1K, VOC 2007 and MS COCO benchmarks. Our experiments show\nconsistent improvement in classification and detection performances with\nvarious models, demonstrating the wide applicability of BAM. The code and\nmodels will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 15:55:31 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 11:17:12 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Park", "Jongchan", ""], ["Woo", "Sanghyun", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1807.06521", "submitter": "Jongchan Park", "authors": "Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon", "title": "CBAM: Convolutional Block Attention Module", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Convolutional Block Attention Module (CBAM), a simple yet\neffective attention module for feed-forward convolutional neural networks.\nGiven an intermediate feature map, our module sequentially infers attention\nmaps along two separate dimensions, channel and spatial, then the attention\nmaps are multiplied to the input feature map for adaptive feature refinement.\nBecause CBAM is a lightweight and general module, it can be integrated into any\nCNN architectures seamlessly with negligible overheads and is end-to-end\ntrainable along with base CNNs. We validate our CBAM through extensive\nexperiments on ImageNet-1K, MS~COCO detection, and VOC~2007 detection datasets.\nOur experiments show consistent improvements in classification and detection\nperformances with various models, demonstrating the wide applicability of CBAM.\nThe code and models will be publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:05:59 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 11:20:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Woo", "Sanghyun", ""], ["Park", "Jongchan", ""], ["Lee", "Joon-Young", ""], ["Kweon", "In So", ""]]}, {"id": "1807.06530", "submitter": "Salaheddin Alakkari", "authors": "Salaheddin Alakkari, John Dingliana", "title": "An Acceleration Scheme for Memory Limited, Streaming PCA", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an acceleration scheme for online memory-limited\nPCA methods. Our scheme converges to the first $k>1$ eigenvectors in a single\ndata pass. We provide empirical convergence results of our scheme based on the\nspiked covariance model. Our scheme does not require any predefined parameters\nsuch as the eigengap and hence is well facilitated for streaming data\nscenarios. Furthermore, we apply our scheme to challenging time-varying systems\nwhere online PCA methods fail to converge. Specifically, we discuss a family of\ntime-varying systems that are based on Molecular Dynamics simulations where\nbatch PCA converges to the actual analytic solution of such systems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:25:01 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Alakkari", "Salaheddin", ""], ["Dingliana", "John", ""]]}, {"id": "1807.06535", "submitter": "R\\'emi Cresson", "authors": "R\\'emi Cresson", "title": "A framework for remote sensing images processing using deep learning\n  technique", "comments": null, "journal-ref": null, "doi": "10.1109/LGRS.2018.2867949", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Deep learning techniques are becoming increasingly important to solve a\nnumber of image processing tasks. Among common algorithms, Convolutional Neural\nNetworks and Recurrent Neural Networks based systems achieve state of the art\nresults on satellite and aerial imagery in many applications. While these\napproaches are subject to scientific interest, there is currently no\noperational and generic implementation available at user-level for the remote\nsensing community. In this paper, we presents a framework enabling the use of\ndeep learning techniques with remote sensing images and geospatial data. Our\nsolution takes roots in two extensively used open-source libraries, the remote\nsensing image processing library Orfeo ToolBox, and the high performance\nnumerical computation library TensorFlow. It can apply deep nets without\nrestriction on images size and is computationally efficient, regardless\nhardware configuration.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:32:08 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 11:13:47 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Cresson", "R\u00e9mi", ""]]}, {"id": "1807.06537", "submitter": "Thomas Varsavsky", "authors": "Thomas Varsavsky, Zach Eaton-Rosen, Carole H. Sudre, Parashkev Nachev\n  and M. Jorge Cardoso", "title": "PIMMS: Permutation Invariant Multi-Modal Segmentation", "comments": "Accepted at the 4th Workshop on Deep Learning in Medical Image\n  Analysis held at MICCAI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a research context, image acquisition will often involve a pre-defined\nstatic protocol and the data will be of high quality. If we are to build\napplications that work in hospitals without significant operational changes in\ncare delivery, algorithms should be designed to cope with the available data in\nthe best possible way. In a clinical environment, imaging protocols are highly\nflexible, with MRI sequences commonly missing appropriate sequence labeling\n(e.g. T1, T2, FLAIR). To this end we introduce PIMMS, a Permutation Invariant\nMulti-Modal Segmentation technique that is able to perform inference over sets\nof MRI scans without using modality labels. We present results which show that\nour convolutional neural network can, in some settings, outperform a baseline\nmodel which utilizes modality labels, and achieve comparable performance\notherwise.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:32:38 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Varsavsky", "Thomas", ""], ["Eaton-Rosen", "Zach", ""], ["Sudre", "Carole H.", ""], ["Nachev", "Parashkev", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1807.06538", "submitter": "Konno Tomohiko", "authors": "Tomohiko Konno and Michiaki Iwazume", "title": "Cavity Filling: Pseudo-Feature Generation for Multi-Class Imbalanced\n  Data Problems in Deep Learning", "comments": "The slides are available at https://goo.gl/SPsSDh in English and at\n  https://goo.gl/RFHYAa in Japanese. 9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein, we generate pseudo-features based on the multivariate probability\ndistributions obtained from the feature maps in layers of trained deep neural\nnetworks. Further, we augment the minor-class data based on these generated\npseudo-features to overcome the imbalanced data problems. The proposed method,\ni.e., cavity filling, improves the deep learning capabilities in several\nproblems because all the real-world data are observed to be imbalanced.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:34:47 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 02:37:09 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 01:22:27 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 02:45:11 GMT"}, {"version": "v5", "created": "Tue, 1 Oct 2019 08:50:34 GMT"}, {"version": "v6", "created": "Sun, 13 Oct 2019 14:47:42 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Konno", "Tomohiko", ""], ["Iwazume", "Michiaki", ""]]}, {"id": "1807.06540", "submitter": "Konno Tomohiko", "authors": "Tomohiko Konno and Michiaki Iwazume", "title": "Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try\n  After Deep Learning", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We found an easy and quick post-learning method named \"Icing on the Cake\" to\nenhance a classification performance in deep learning. The method is that we\ntrain only the final classifier again after an ordinary training is done.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:35:51 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Konno", "Tomohiko", ""], ["Iwazume", "Michiaki", ""]]}, {"id": "1807.06551", "submitter": "David George", "authors": "David George, Xianguha Xie, Yu-Kun Lai, Gary KL Tam", "title": "A Deep Learning Driven Active Framework for Segmentation of Large 3D\n  Shape Collections", "comments": "16 pages, 17 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level shape understanding and technique evaluation on large repositories\nof 3D shapes often benefit from additional information known about the shapes.\nOne example of such information is the semantic segmentation of a shape into\nfunctional or meaningful parts. Generating accurate segmentations with\nmeaningful segment boundaries is, however, a costly process, typically\nrequiring large amounts of user time to achieve high quality results. In this\npaper we present an active learning framework for large dataset segmentation,\nwhich iteratively provides the user with new predictions by training new models\nbased on already segmented shapes. Our proposed pipeline consists of three\nnovel components. First, we a propose a fast and relatively accurate\nfeature-based deep learning model to provide dataset-wide segmentation\npredictions. Second, we propose an information theory measure to estimate the\nprediction quality and for ordering subsequent fast and meaningful shape\nselection. Our experiments show that such suggestive ordering helps reduce\nusers time and effort, produce high quality predictions, and construct a model\nthat generalizes well. Finally, we provide effective segmentation refinement\nfeatures to help the user quickly correct any incorrect predictions. We show\nthat our framework is more accurate and in general more efficient than\nstate-of-the-art, for massive dataset segmentation with while also providing\nconsistent segment boundaries.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:58:06 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["George", "David", ""], ["Xie", "Xianguha", ""], ["Lai", "Yu-Kun", ""], ["Tam", "Gary KL", ""]]}, {"id": "1807.06583", "submitter": "Yordan Hristov", "authors": "Yordan Hristov, Alex Lascarides and Subramanian Ramamoorthy", "title": "Interpretable Latent Spaces for Learning from Demonstration", "comments": "12 pages, 6 figures, accepted at the Conference on Robot Learning\n  (CoRL) 2018, Zurich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective human-robot interaction, such as in robot learning from human\ndemonstration, requires the learning agent to be able to ground abstract\nconcepts (such as those contained within instructions) in a corresponding\nhigh-dimensional sensory input stream from the world. Models such as deep\nneural networks, with high capacity through their large parameter spaces, can\nbe used to compress the high-dimensional sensory data to lower dimensional\nrepresentations. These low-dimensional representations facilitate symbol\ngrounding, but may not guarantee that the representation would be\nhuman-interpretable. We propose a method which utilises the grouping of\nuser-defined symbols and their corresponding sensory observations in order to\nalign the learnt compressed latent representation with the semantic notions\ncontained in the abstract labels. We demonstrate this through experiments with\nboth simulated and real-world object data, showing that such alignment can be\nachieved in a process of physical symbol grounding.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 17:56:09 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 15:27:23 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Hristov", "Yordan", ""], ["Lascarides", "Alex", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1807.06587", "submitter": "Dongdong Chen", "authors": "Mingming He, Dongdong Chen, Jing Liao, Pedro V. Sander, Lu Yuan", "title": "Deep Exemplar-based Colorization", "comments": "To Appear in Siggraph 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the first deep learning approach for exemplar-based local\ncolorization. Given a reference color image, our convolutional neural network\ndirectly maps a grayscale image to an output colorized image. Rather than using\nhand-crafted rules as in traditional exemplar-based methods, our end-to-end\ncolorization network learns how to select, propagate, and predict colors from\nthe large-scale data. The approach performs robustly and generalizes well even\nwhen using reference images that are unrelated to the input grayscale image.\nMore importantly, as opposed to other learning-based colorization methods, our\nnetwork allows the user to achieve customizable results by simply feeding\ndifferent references. In order to further reduce manual effort in selecting the\nreferences, the system automatically recommends references with our proposed\nimage retrieval algorithm, which considers both semantic and luminance\ninformation. The colorization can be performed fully automatically by simply\npicking the top reference suggestion. Our approach is validated through a user\nstudy and favorable quantitative comparisons to the-state-of-the-art methods.\nFurthermore, our approach can be naturally extended to video colorization. Our\ncode and models will be freely available for public use.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 17:59:01 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 15:39:03 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["He", "Mingming", ""], ["Chen", "Dongdong", ""], ["Liao", "Jing", ""], ["Sander", "Pedro V.", ""], ["Yuan", "Lu", ""]]}, {"id": "1807.06604", "submitter": "Subhayan Mukherjee", "authors": "Subhayan Mukherjee, Irene Cheng, Steven Miller, Jessie Guo, Vann Chau,\n  Anup Basu", "title": "A Fast Segmentation-free Fully Automated Approach to White Matter Injury\n  Detection in Preterm Infants", "comments": null, "journal-ref": "Medical and Biological Engineering and Computing (Springer), 2018", "doi": "10.1007/s11517-018-1829-9", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White Matter Injury (WMI) is the most prevalent brain injury in the preterm\nneonate leading to developmental deficits. However, detecting WMI in Magnetic\nResonance (MR) images of preterm neonate brains using traditional WM\nsegmentation-based methods is difficult mainly due to lack of reliable preterm\nneonate brain atlases to guide segmentation. Hence, we propose a\nsegmentation-free, fast, unsupervised, atlas-free WMI detection method. We\ndetect the ventricles as blobs using a fast linear Maximally Stable Extremal\nRegions algorithm. A reference contour equidistant from the blobs and the\nbrain-background boundary is used to identify tissue adjacent to the blobs.\nAssuming normal distribution of the gray-value intensity of this tissue, the\noutlier intensities in the entire brain region are identified as potential WMI\ncandidates. Thereafter, false positives are discriminated using appropriate\nheuristics. Experiments using an expert-annotated dataset show that the\nproposed method runs 20 times faster than our earlier work which relied on\ntime-consuming segmentation of the WM region, without compromising WMI\ndetection accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 18:00:26 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mukherjee", "Subhayan", ""], ["Cheng", "Irene", ""], ["Miller", "Steven", ""], ["Guo", "Jessie", ""], ["Chau", "Vann", ""], ["Basu", "Anup", ""]]}, {"id": "1807.06621", "submitter": "Wei Ke", "authors": "Wei Ke and Jie Chen and Jianbin Jiao and Guoying Zhao and Qixiang Ye", "title": "SRN: Side-output Residual Network for Object Reflection Symmetry\n  Detection and Beyond", "comments": "reject by PAMI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a baseline for object reflection symmetry\ndetection in complex backgrounds by presenting a new benchmark and an\nend-to-end deep learning approach, opening up a promising direction for\nsymmetry detection in the wild. The new benchmark, Sym-PASCAL, spans challenges\nincluding object diversity, multi-objects, part-invisibility, and various\ncomplex backgrounds that are far beyond those in existing datasets. The\nend-to-end deep learning approach, referred to as a side-output residual\nnetwork (SRN), leverages the output residual units (RUs) to fit the errors\nbetween the object ground-truth symmetry and the side-outputs of multiple\nstages. By cascading RUs in a deep-to-shallow manner, SRN exploits the 'flow'\nof errors among multiple stages to address the challenges of fitting complex\noutput with limited convolutional layers, suppressing the complex backgrounds,\nand effectively matching object symmetry at different scales. SRN is further\nupgraded to a multi-task side-output residual network (MT-SRN) for joint\nsymmetry and edge detection, demonstrating its generality to image-to-mask\nlearning tasks. Experimental results validate both the challenging aspects of\nSym-PASCAL benchmark related to real-world images and the state-of-the-art\nperformance of the proposed SRN approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 18:51:19 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 15:47:16 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Ke", "Wei", ""], ["Chen", "Jie", ""], ["Jiao", "Jianbin", ""], ["Zhao", "Guoying", ""], ["Ye", "Qixiang", ""]]}, {"id": "1807.06644", "submitter": "Omar Tahri", "authors": "Omar Tahri", "title": "A Framework for Moment Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than half a century, moments have attracted lot ot interest in the\npattern recognition community.The moments of a distribution (an object) provide\nseveral of its characteristics as center of gravity, orientation, disparity,\nvolume. Moments can be used to define invariant characteristics to some\ntransformations that an object can undergo, commonly called moment invariants.\nThis work provides a simple and systematic formalism to compute geometric\nmoment invariants in n-dimensional space.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 20:03:19 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Tahri", "Omar", ""]]}, {"id": "1807.06653", "submitter": "Xu Ji", "authors": "Xu Ji, Jo\\~ao F. Henriques, Andrea Vedaldi", "title": "Invariant Information Clustering for Unsupervised Image Classification\n  and Segmentation", "comments": "International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel clustering objective that learns a neural network\nclassifier from scratch, given only unlabelled data samples. The model\ndiscovers clusters that accurately match semantic classes, achieving\nstate-of-the-art results in eight unsupervised clustering benchmarks spanning\nimage classification and segmentation. These include STL10, an unsupervised\nvariant of ImageNet, and CIFAR10, where we significantly beat the accuracy of\nour closest competitors by 6.6 and 9.5 absolute percentage points respectively.\nThe method is not specialised to computer vision and operates on any paired\ndataset samples; in our experiments we use random transforms to obtain a pair\nfrom each image. The trained network directly outputs semantic labels, rather\nthan high dimensional representations that need external processing to be\nusable for semantic clustering. The objective is simply to maximise mutual\ninformation between the class assignments of each pair. It is easy to implement\nand rigorously grounded in information theory, meaning we effortlessly avoid\ndegenerate solutions that other clustering methods are susceptible to. In\naddition to the fully unsupervised mode, we also test two semi-supervised\nsettings. The first achieves 88.8% accuracy on STL10 classification, setting a\nnew global state-of-the-art over all existing methods (whether supervised,\nsemi-supervised or unsupervised). The second shows robustness to 90% reductions\nin label coverage, of relevance to applications that wish to make use of small\namounts of labels. github.com/xu-ji/IIC\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 20:17:29 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 23:06:58 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 00:44:00 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2019 14:32:16 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Ji", "Xu", ""], ["Henriques", "Jo\u00e3o F.", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1807.06667", "submitter": "Samvit Jain", "authors": "Samvit Jain, Xin Wang, Joseph Gonzalez", "title": "Accel: A Corrective Fusion Network for Efficient Semantic Segmentation\n  on Video", "comments": "CVPR 2019 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Accel, a novel semantic video segmentation system that achieves\nhigh accuracy at low inference cost by combining the predictions of two network\nbranches: (1) a reference branch that extracts high-detail features on a\nreference keyframe, and warps these features forward using frame-to-frame\noptical flow estimates, and (2) an update branch that computes features of\nadjustable quality on the current frame, performing a temporal update at each\nvideo frame. The modularity of the update branch, where feature subnetworks of\nvarying layer depth can be inserted (e.g. ResNet-18 to ResNet-101), enables\noperation over a new, state-of-the-art accuracy-throughput trade-off spectrum.\nOver this curve, Accel models achieve both higher accuracy and faster inference\ntimes than the closest comparable single-frame segmentation networks. In\ngeneral, Accel significantly outperforms previous work on efficient semantic\nvideo segmentation, correcting warping-related error that compounds on datasets\nwith complex dynamics. Accel is end-to-end trainable and highly modular: the\nreference network, the optical flow network, and the update network can each be\nselected independently, depending on application requirements, and then jointly\nfine-tuned. The result is a robust, general system for fast, high-accuracy\nsemantic segmentation on video.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 20:45:23 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 03:28:11 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 23:47:24 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 20:36:08 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Jain", "Samvit", ""], ["Wang", "Xin", ""], ["Gonzalez", "Joseph", ""]]}, {"id": "1807.06677", "submitter": "Yujia Zhang", "authors": "Yujia Zhang, Michael Kampffmeyer, Xiaodan Liang, Min Tan and Eric P.\n  Xing", "title": "Query-Conditioned Three-Player Adversarial Network for Video\n  Summarization", "comments": "13 pages, 3 figures, BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summarization plays an important role in video understanding by\nselecting key frames/shots. Traditionally, it aims to find the most\nrepresentative and diverse contents in a video as short summaries. Recently, a\nmore generalized task, query-conditioned video summarization, has been\nintroduced, which takes user queries into consideration to learn more\nuser-oriented summaries. In this paper, we propose a query-conditioned\nthree-player generative adversarial network to tackle this challenge. The\ngenerator learns the joint representation of the user query and the video\ncontent, and the discriminator takes three pairs of query-conditioned summaries\nas the input to discriminate the real summary from a generated and a random\none. A three-player loss is introduced for joint training of the generator and\nthe discriminator, which forces the generator to learn better summary results,\nand avoids the generation of random trivial summaries. Experiments on a\nrecently proposed query-conditioned video summarization benchmark dataset show\nthe efficiency and efficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 21:21:32 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhang", "Yujia", ""], ["Kampffmeyer", "Michael", ""], ["Liang", "Xiaodan", ""], ["Tan", "Min", ""], ["Xing", "Eric P.", ""]]}, {"id": "1807.06699", "submitter": "Kai Arulkumaran", "authors": "Ryutaro Tanno, Kai Arulkumaran, Daniel C. Alexander, Antonio\n  Criminisi, Aditya Nori", "title": "Adaptive Neural Trees", "comments": "International Conference on Machine Learning 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks and decision trees operate on largely separate\nparadigms; typically, the former performs representation learning with\npre-specified architectures, while the latter is characterised by learning\nhierarchies over pre-specified features with data-driven architectures. We\nunite the two via adaptive neural trees (ANTs) that incorporates representation\nlearning into edges, routing functions and leaf nodes of a decision tree, along\nwith a backpropagation-based training algorithm that adaptively grows the\narchitecture from primitive modules (e.g., convolutional layers). We\ndemonstrate that, whilst achieving competitive performance on classification\nand regression datasets, ANTs benefit from (i) lightweight inference via\nconditional computation, (ii) hierarchical separation of features useful to the\ntask e.g. learning meaningful class associations, such as separating natural\nvs. man-made objects, and (iii) a mechanism to adapt the architecture to the\nsize and complexity of the training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:01:35 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 20:36:03 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 12:24:20 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2019 16:00:57 GMT"}, {"version": "v5", "created": "Sun, 9 Jun 2019 19:32:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Tanno", "Ryutaro", ""], ["Arulkumaran", "Kai", ""], ["Alexander", "Daniel C.", ""], ["Criminisi", "Antonio", ""], ["Nori", "Aditya", ""]]}, {"id": "1807.06708", "submitter": "Xiangyun Zhao", "authors": "Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, Ying Wu", "title": "A Modulation Module for Multi-task Learning with Applications in Image\n  Retrieval", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning has been widely adopted in many computer vision tasks to\nimprove overall computation efficiency or boost the performance of individual\ntasks, under the assumption that those tasks are correlated and complementary\nto each other. However, the relationships between the tasks are complicated in\npractice, especially when the number of involved tasks scales up. When two\ntasks are of weak relevance, they may compete or even distract each other\nduring joint training of shared parameters, and as a consequence undermine the\nlearning of all the tasks. This will raise destructive interference which\ndecreases learning efficiency of shared parameters and lead to low quality loss\nlocal optimum w.r.t. shared parameters. To address the this problem, we propose\na general modulation module, which can be inserted into any convolutional\nneural network architecture, to encourage the coupling and feature sharing of\nrelevant tasks while disentangling the learning of irrelevant tasks with minor\nparameters addition. Equipped with this module, gradient directions from\ndifferent tasks can be enforced to be consistent for those shared parameters,\nwhich benefits multi-task joint training. The module is end-to-end learnable\nwithout ad-hoc design for specific tasks, and can naturally handle many tasks\nat the same time. We apply our approach on two retrieval tasks, face retrieval\non the CelebA dataset [1] and product retrieval on the UT-Zappos50K dataset [2,\n3], and demonstrate its advantage over other multi-task learning methods in\nboth accuracy and storage efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:33:24 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 21:13:03 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Zhao", "Xiangyun", ""], ["Li", "Haoxiang", ""], ["Shen", "Xiaohui", ""], ["Liang", "Xiaodan", ""], ["Wu", "Ying", ""]]}, {"id": "1807.06714", "submitter": "Adnan Siraj Rakin", "authors": "Adnan Siraj Rakin, Jinfeng Yi, Boqing Gong and Deliang Fan", "title": "Defend Deep Neural Networks Against Adversarial Examples via Fixed and\n  Dynamic Quantized Activation Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that deep neural networks (DNNs) are vulnerable to\nadversarial attacks. To this end, many defense approaches that attempt to\nimprove the robustness of DNNs have been proposed. In a separate and yet\nrelated area, recent works have explored to quantize neural network weights and\nactivation functions into low bit-width to compress model size and reduce\ncomputational complexity. In this work, we find that these two different\ntracks, namely the pursuit of network compactness and robustness, can be merged\ninto one and give rise to networks of both advantages. To the best of our\nknowledge, this is the first work that uses quantization of activation\nfunctions to defend against adversarial examples. We also propose to train\nrobust neural networks by using adaptive quantization techniques for the\nactivation functions. Our proposed Dynamic Quantized Activation (DQA) is\nverified through a wide range of experiments with the MNIST and CIFAR-10\ndatasets under different white-box attack methods, including FGSM, PGD, and C &\nW attacks. Furthermore, Zeroth Order Optimization and substitute model-based\nblack-box attacks are also considered in this work. The experimental results\nclearly show that the robustness of DNNs could be greatly improved using the\nproposed DQA.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 00:21:12 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 23:54:10 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Rakin", "Adnan Siraj", ""], ["Yi", "Jinfeng", ""], ["Gong", "Boqing", ""], ["Fan", "Deliang", ""]]}, {"id": "1807.06742", "submitter": "Yong Xia", "authors": "Haozhe Jia, Yang Song, Donghao Zhang, Heng Huang, Dagan Feng, Michael\n  Fulham, Yong Xia, Weidong Cai", "title": "3D Global Convolutional Adversarial Network\\\\ for Prostate MR Volume\n  Segmentation", "comments": "9 pages, 3 figures, 2 tables, 16 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced deep learning methods have been developed to conduct prostate MR\nvolume segmentation in either a 2D or 3D fully convolutional manner. However,\n2D methods tend to have limited segmentation performance, since large amounts\nof spatial information of prostate volumes are discarded during the\nslice-by-slice segmentation process; and 3D methods also have room for\nimprovement, since they use isotropic kernels to perform 3D convolutions\nwhereas most prostate MR volumes have anisotropic spatial resolution. Besides,\nthe fully convolutional structural methods achieve good performance for\nlocalization issues but neglect the per-voxel classification for segmentation\ntasks. In this paper, we propose a 3D Global Convolutional Adversarial Network\n(3D GCA-Net) to address efficient prostate MR volume segmentation. We first\ndesign a 3D ResNet encoder to extract 3D features from prostate scans, and then\ndevelop the decoder, which is composed of a multi-scale 3D global convolutional\nblock and a 3D boundary refinement block, to address the classification and\nlocalization issues simultaneously for volumetric segmentation. Additionally,\nwe combine the encoder-decoder segmentation network with an adversarial network\nin the training phrase to enforce the contiguity of long-range spatial\npredictions. Throughout the proposed model, we use anisotropic convolutional\nprocessing for better feature learning on prostate MR scans. We evaluated our\n3D GCA-Net model on two public prostate MR datasets and achieved\nstate-of-the-art performances.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 02:27:53 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Jia", "Haozhe", ""], ["Song", "Yang", ""], ["Zhang", "Donghao", ""], ["Huang", "Heng", ""], ["Feng", "Dagan", ""], ["Fulham", "Michael", ""], ["Xia", "Yong", ""], ["Cai", "Weidong", ""]]}, {"id": "1807.06757", "submitter": "Vladlen Koltun", "authors": "Peter Anderson, Angel Chang, Devendra Singh Chaplot, Alexey\n  Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana Kosecka, Jitendra Malik,\n  Roozbeh Mottaghi, Manolis Savva, and Amir R. Zamir", "title": "On Evaluation of Embodied Navigation Agents", "comments": "Report of a working group on empirical methodology in navigation\n  research. Authors are listed in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skillful mobile operation in three-dimensional environments is a primary\ntopic of study in Artificial Intelligence. The past two years have seen a surge\nof creative work on navigation. This creative output has produced a plethora of\nsometimes incompatible task definitions and evaluation protocols. To coordinate\nongoing and future research in this area, we have convened a working group to\nstudy empirical methodology in navigation research. The present document\nsummarizes the consensus recommendations of this working group. We discuss\ndifferent problem statements and the role of generalization, present evaluation\nmeasures, and provide standard scenarios that can be used for benchmarking.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 03:28:02 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Anderson", "Peter", ""], ["Chang", "Angel", ""], ["Chaplot", "Devendra Singh", ""], ["Dosovitskiy", "Alexey", ""], ["Gupta", "Saurabh", ""], ["Koltun", "Vladlen", ""], ["Kosecka", "Jana", ""], ["Malik", "Jitendra", ""], ["Mottaghi", "Roozbeh", ""], ["Savva", "Manolis", ""], ["Zamir", "Amir R.", ""]]}, {"id": "1807.06772", "submitter": "Ranju Mandal Dr.", "authors": "Ranju Mandal and Partha Pratim Roy and Umapada Pal and Michael\n  Blumenstein", "title": "Bag-of-Visual-Words for Signature-Based Multi-Script Document Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An end-to-end architecture for multi-script document retrieval using\nhandwritten signatures is proposed in this paper. The user supplies a query\nsignature sample and the system exclusively returns a set of documents that\ncontain the query signature. In the first stage, a component-wise\nclassification technique separates the potential signature components from all\nother components. A bag-of-visual-words powered by SIFT descriptors in a\npatch-based framework is proposed to compute the features and a Support Vector\nMachine (SVM)-based classifier was used to separate signatures from the\ndocuments. In the second stage, features from the foreground (i.e. signature\nstrokes) and the background spatial information (i.e. background loops,\nreservoirs etc.) were combined to characterize the signature object to match\nwith the query signature. Finally, three distance measures were used to match a\nquery signature with the signature present in target documents for retrieval.\nThe `Tobacco' document database and an Indian script database containing 560\ndocuments of Devanagari (Hindi) and Bangla scripts were used for the\nperformance evaluation. The proposed system was also tested on noisy documents\nand promising results were obtained. A comparative study shows that the\nproposed method outperforms the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 04:29:20 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mandal", "Ranju", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""], ["Blumenstein", "Michael", ""]]}, {"id": "1807.06775", "submitter": "Mohammed Hassanin", "authors": "Mohammed Hassanin, Salman Khan, Murat Tahtali", "title": "Visual Affordance and Function Understanding: A Survey", "comments": "26 pages, 22 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, robots are dominating the manufacturing, entertainment and\nhealthcare industries. Robot vision aims to equip robots with the ability to\ndiscover information, understand it and interact with the environment. These\ncapabilities require an agent to effectively understand object affordances and\nfunctionalities in complex visual domains. In this literature survey, we first\nfocus on Visual affordances and summarize the state of the art as well as open\nproblems and research gaps. Specifically, we discuss sub-problems such as\naffordance detection, categorization, segmentation and high-level reasoning.\nFurthermore, we cover functional scene understanding and the prevalent\nfunctional descriptors used in the literature. The survey also provides\nnecessary background to the problem, sheds light on its significance and\nhighlights the existing challenges for affordance and functionality learning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 05:21:30 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Hassanin", "Mohammed", ""], ["Khan", "Salman", ""], ["Tahtali", "Murat", ""]]}, {"id": "1807.06779", "submitter": "Yuan Liu", "authors": "Yuan Liu, Yuancheng Wang, Nan Li, Xu Cheng, Yifeng Zhang, Yongming\n  Huang, Guojun Lu", "title": "An Attention-Based Approach for Single Image Super Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main challenge of single image super resolution (SISR) is the recovery of\nhigh frequency details such as tiny textures. However, most of the\nstate-of-the-art methods lack specific modules to identify high frequency\nareas, causing the output image to be blurred. We propose an attention-based\napproach to give a discrimination between texture areas and smooth areas. After\nthe positions of high frequency details are located, high frequency\ncompensation is carried out. This approach can incorporate with previously\nproposed SISR networks. By providing high frequency enhancement, better\nperformance and visual effect are achieved. We also propose our own SISR\nnetwork composed of DenseRes blocks. The block provides an effective way to\ncombine the low level features and high level features. Extensive benchmark\nevaluation shows that our proposed method achieves significant improvement over\nthe state-of-the-art works in SISR.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 05:34:49 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Liu", "Yuan", ""], ["Wang", "Yuancheng", ""], ["Li", "Nan", ""], ["Cheng", "Xu", ""], ["Zhang", "Yifeng", ""], ["Huang", "Yongming", ""], ["Lu", "Guojun", ""]]}, {"id": "1807.06789", "submitter": "Christos Kyrkou", "authors": "Christos Kyrkou, George Plastiras, Stylianos Venieris, Theocharis\n  Theocharides, Christos-Savvas Bouganis", "title": "DroNet: Efficient convolutional neural network detector for real-time\n  UAV applications", "comments": "C. Kyrkou, G. Plastiras, T. Theocharides, S. I. Venieris and C. S.\n  Bouganis, \"DroNet: Efficient convolutional neural network detector for\n  real-time UAV applications,\" 2018 Design, Automation & Test in Europe\n  Conference & Exhibition (DATE), Dresden, 2018, pp. 967-972. Keywords:\n  Convolutional neural networks, Machine learning, autonomous aerial vehicles,\n  computer vision, embedded systems", "journal-ref": "2018 Design, Automation & Test in Europe Conference & Exhibition\n  (DATE)", "doi": "10.23919/DATE.2018.8342149", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned Aerial Vehicles (drones) are emerging as a promising technology for\nboth environmental and infrastructure monitoring, with broad use in a plethora\nof applications. Many such applications require the use of computer vision\nalgorithms in order to analyse the information captured from an on-board\ncamera. Such applications include detecting vehicles for emergency response and\ntraffic monitoring. This paper therefore, explores the trade-offs involved in\nthe development of a single-shot object detector based on deep convolutional\nneural networks (CNNs) that can enable UAVs to perform vehicle detection under\na resource constrained environment such as in a UAV. The paper presents a\nholistic approach for designing such systems; the data collection and training\nstages, the CNN architecture, and the optimizations necessary to efficiently\nmap such a CNN on a lightweight embedded processing platform suitable for\ndeployment on UAVs. Through the analysis we propose a CNN architecture that is\ncapable of detecting vehicles from aerial UAV images and can operate between\n5-18 frames-per-second for a variety of platforms with an overall accuracy of\n~95%. Overall, the proposed architecture is suitable for UAV applications,\nutilizing low-power embedded processors that can be deployed on commercial\nUAVs.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 06:30:54 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Kyrkou", "Christos", ""], ["Plastiras", "George", ""], ["Venieris", "Stylianos", ""], ["Theocharides", "Theocharis", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1807.06814", "submitter": "Wojciech Chojnacki", "authors": "Wojciech Chojnacki and Zygmunt L. Szpak", "title": "Determining ellipses from low-resolution images with a comprehensive\n  image formation model", "comments": "20 pages, 22 figures", "journal-ref": "Journal of the Optical Society of America A, vol. 36, 2019, pp.\n  212 - 233", "doi": "10.1364/JOSAA.36.000212", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When determining the parameters of a parametric planar shape based on a\nsingle low-resolution image, common estimation paradigms lead to inaccurate\nparameter estimates. The reason behind poor estimation results is that standard\nestimation frameworks fail to model the image formation process at a\nsufficiently detailed level of analysis. We propose a new method for estimating\nthe parameters of a planar elliptic shape based on a single photon-limited,\nlow-resolution image. Our technique incorporates the effects of several\nelements - point-spread function, discretisation step, quantisation step, and\nphoton noise - into a single cohesive and manageable statistical model. While\nwe concentrate on the particular task of estimating the parameters of elliptic\nshapes, our ideas and methods have a much broader scope and can be used to\naddress the problem of estimating the parameters of an arbitrary parametrically\nrepresentable planar shape. Comprehensive experimental results on simulated and\nreal imagery demonstrate that our approach yields parameter estimates with\nunprecedented accuracy. Furthermore, our method supplies a parameter covariance\nmatrix as a measure of uncertainty for the estimated parameters, as well as a\nplanar confidence region as a means for visualising the parameter uncertainty.\nThe mathematical model developed in this paper may prove useful in a variety of\ndisciplines which operate with imagery at the limits of resolution.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 08:23:34 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 05:58:49 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 03:57:24 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chojnacki", "Wojciech", ""], ["Szpak", "Zygmunt L.", ""]]}, {"id": "1807.06819", "submitter": "Byung Cheol Song", "authors": "Seung Hyun Lee, Dae Ha Kim, Byung Cheol Song", "title": "Self-supervised Knowledge Distillation Using Singular Value\n  Decomposition", "comments": "accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve deep neural network (DNN)'s huge training dataset and its high\ncomputation issue, so-called teacher-student (T-S) DNN which transfers the\nknowledge of T-DNN to S-DNN has been proposed. However, the existing T-S-DNN\nhas limited range of use, and the knowledge of T-DNN is insufficiently\ntransferred to S-DNN. To improve the quality of the transferred knowledge from\nT-DNN, we propose a new knowledge distillation using singular value\ndecomposition (SVD). In addition, we define a knowledge transfer as a\nself-supervised task and suggest a way to continuously receive information from\nT-DNN. Simulation results show that a S-DNN with a computational cost of 1/5 of\nthe T-DNN can be up to 1.1\\% better than the T-DNN in terms of classification\naccuracy. Also assuming the same computational cost, our S-DNN outperforms the\nS-DNN driven by the state-of-the-art distillation with a performance advantage\nof 1.79\\%. code is available on https://github.com/sseung0703/SSKD\\_SVD.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 08:52:05 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Lee", "Seung Hyun", ""], ["Kim", "Dae Ha", ""], ["Song", "Byung Cheol", ""]]}, {"id": "1807.06821", "submitter": "Meng Li", "authors": "Meng Li, Shiwen Shen, Wen Gao, William Hsu and Jason Cong", "title": "Computed Tomography Image Enhancement using 3D Convolutional Neural\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) is increasingly being used for cancer screening,\nsuch as early detection of lung cancer. However, CT studies have varying pixel\nspacing due to differences in acquisition parameters. Thick slice CTs have\nlower resolution, hindering tasks such as nodule characterization during\ncomputer-aided detection due to partial volume effect. In this study, we\npropose a novel 3D enhancement convolutional neural network (3DECNN) to improve\nthe spatial resolution of CT studies that were acquired using lower\nresolution/slice thicknesses to higher resolutions. Using a subset of the LIDC\ndataset consisting of 20,672 CT slices from 100 scans, we simulated lower\nresolution/thick section scans then attempted to reconstruct the original\nimages using our 3DECNN network. A significant improvement in PSNR (29.3087dB\nvs. 28.8769dB, p-value < 2.2e-16) and SSIM (0.8529dB vs. 0.8449dB, p-value <\n2.2e-16) compared to other state-of-art deep learning methods is observed.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 08:52:27 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Li", "Meng", ""], ["Shen", "Shiwen", ""], ["Gao", "Wen", ""], ["Hsu", "William", ""], ["Cong", "Jason", ""]]}, {"id": "1807.06843", "submitter": "Carlo Biffi", "authors": "Carlo Biffi, Ozan Oktay, Giacomo Tarroni, Wenjia Bai, Antonio De\n  Marvao, Georgia Doumou, Martin Rajchl, Reem Bedair, Sanjay Prasad, Stuart\n  Cook, Declan O'Regan, Daniel Rueckert", "title": "Learning Interpretable Anatomical Features Through Deep Generative\n  Models: Application to Cardiac Remodeling", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alterations in the geometry and function of the heart define well-established\ncauses of cardiovascular disease. However, current approaches to the diagnosis\nof cardiovascular diseases often rely on subjective human assessment as well as\nmanual analysis of medical images. Both factors limit the sensitivity in\nquantifying complex structural and functional phenotypes. Deep learning\napproaches have recently achieved success for tasks such as classification or\nsegmentation of medical images, but lack interpretability in the feature\nextraction and decision processes, limiting their value in clinical diagnosis.\nIn this work, we propose a 3D convolutional generative model for automatic\nclassification of images from patients with cardiac diseases associated with\nstructural remodeling. The model leverages interpretable task-specific anatomic\npatterns learned from 3D segmentations. It further allows to visualise and\nquantify the learned pathology-specific remodeling patterns in the original\ninput space of the images. This approach yields high accuracy in the\ncategorization of healthy and hypertrophic cardiomyopathy subjects when tested\non unseen MR images from our own multi-centre dataset (100%) as well on the\nACDC MICCAI 2017 dataset (90%). We believe that the proposed deep learning\napproach is a promising step towards the development of interpretable\nclassifiers for the medical imaging domain, which may help clinicians to\nimprove diagnostic accuracy and enhance patient risk-stratification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 10:15:23 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Biffi", "Carlo", ""], ["Oktay", "Ozan", ""], ["Tarroni", "Giacomo", ""], ["Bai", "Wenjia", ""], ["De Marvao", "Antonio", ""], ["Doumou", "Georgia", ""], ["Rajchl", "Martin", ""], ["Bedair", "Reem", ""], ["Prasad", "Sanjay", ""], ["Cook", "Stuart", ""], ["O'Regan", "Declan", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1807.06905", "submitter": "Christoph Rasche", "authors": "Christoph Rasche", "title": "Melanoma Recognition with an Ensemble of Techniques for Segmentation and\n  a Structural Analysis for Classification", "comments": "Participation in ISIC 2018 competition. 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to lesion recognition is described that for lesion localization\nuses an ensemble of segmentation techniques and for lesion classification an\nexhaustive structural analysis. For localization, candidate regions are\nobtained from global thresholding of the chromatic maps and from applying the\nK-Means algorithm to the RGB image; the candidate regions are then integrated.\nFor classification, a relatively exhaustive structural analysis of contours and\nregions is carried out.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:10:56 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Rasche", "Christoph", ""]]}, {"id": "1807.06906", "submitter": "Arber Zela", "authors": "Arber Zela, Aaron Klein, Stefan Falkner and Frank Hutter", "title": "Towards Automated Deep Learning: Efficient Joint Neural Architecture and\n  Hyperparameter Search", "comments": "11 pages, 3 figures, 3 tables, ICML 2018 AutoML Workshop", "journal-ref": "ICML 2018 AutoML Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While existing work on neural architecture search (NAS) tunes hyperparameters\nin a separate post-processing step, we demonstrate that architectural choices\nand other hyperparameter settings interact in a way that can render this\nseparation suboptimal. Likewise, we demonstrate that the common practice of\nusing very few epochs during the main NAS and much larger numbers of epochs\nduring a post-processing step is inefficient due to little correlation in the\nrelative rankings for these two training regimes. To combat both of these\nproblems, we propose to use a recent combination of Bayesian optimization and\nHyperband for efficient joint neural architecture and hyperparameter search.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:11:08 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zela", "Arber", ""], ["Klein", "Aaron", ""], ["Falkner", "Stefan", ""], ["Hutter", "Frank", ""]]}, {"id": "1807.06920", "submitter": "Weisheng Dong", "authors": "Fangfang Wu, Weisheng Dong, Guangming Shi and Xin Li", "title": "Learning Hybrid Sparsity Prior for Image Restoration: Where Deep\n  Learning Meets Sparse Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art approaches toward image restoration can be classified into\nmodel-based and learning-based. The former - best represented by sparse coding\ntechniques - strive to exploit intrinsic prior knowledge about the unknown\nhigh-resolution images; while the latter - popularized by recently developed\ndeep learning techniques - leverage external image prior from some training\ndataset. It is natural to explore their middle ground and pursue a hybrid image\nprior capable of achieving the best in both worlds. In this paper, we propose a\nsystematic approach of achieving this goal called Structured Analysis Sparse\nCoding (SASC). Specifically, a structured sparse prior is learned from\nextrinsic training data via a deep convolutional neural network (in a similar\nway to previous learning-based approaches); meantime another structured sparse\nprior is internally estimated from the input observation image (similar to\nprevious model-based approaches). Two structured sparse priors will then be\ncombined to produce a hybrid prior incorporating the knowledge from both\ndomains. To manage the computational complexity, we have developed a novel\nframework of implementing hybrid structured sparse coding processes by deep\nconvolutional neural networks. Experimental results show that the proposed\nhybrid image restoration method performs comparably with and often better than\nthe current state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:33:02 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 12:36:01 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Wu", "Fangfang", ""], ["Dong", "Weisheng", ""], ["Shi", "Guangming", ""], ["Li", "Xin", ""]]}, {"id": "1807.06956", "submitter": "Daiki Tamada", "authors": "Daiki Tamada, Marie-Luise Kromrey, Hiroshi Onishi, Utaroh Motosugi", "title": "Method for motion artifact reduction using a convolutional neural\n  network for dynamic contrast enhanced MRI of the liver", "comments": "11 pages, 6 figures", "journal-ref": "Magnetic Resonance in Medical Sciences 19.1 (2020): 64-76", "doi": "10.2463/mrms.mp.2018-0156", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To improve the quality of images obtained via dynamic\ncontrast-enhanced MRI (DCE-MRI) that include motion artifacts and blurring\nusing a deep learning approach. Methods: A multi-channel convolutional neural\nnetwork (MARC) based method is proposed for reducing the motion artifacts and\nblurring caused by respiratory motion in images obtained via DCE-MRI of the\nliver. The training datasets for the neural network included images with and\nwithout respiration-induced motion artifacts or blurring, and the distortions\nwere generated by simulating the phase error in k-space. Patient studies were\nconducted using a multi-phase T1-weighted spoiled gradient echo sequence for\nthe liver containing breath-hold failures during data acquisition. The trained\nnetwork was applied to the acquired images to analyze the filtering\nperformance, and the intensities and contrast ratios before and after denoising\nwere compared via Bland-Altman plots. Results: The proposed network was found\nto significantly reduce the magnitude of the artifacts and blurring induced by\nrespiratory motion, and the contrast ratios of the images after processing via\nthe network were consistent with those of the unprocessed images. Conclusion: A\ndeep learning based method for removing motion artifacts in images obtained via\nDCE-MRI in the liver was demonstrated and validated.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 14:16:31 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 06:09:25 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Tamada", "Daiki", ""], ["Kromrey", "Marie-Luise", ""], ["Onishi", "Hiroshi", ""], ["Motosugi", "Utaroh", ""]]}, {"id": "1807.06962", "submitter": "Firat Ozdemir", "authors": "Firat Ozdemir, Zixuan Peng, Christine Tanner, Philipp Fuernstahl,\n  Orcun Goksel", "title": "Active Learning for Segmentation by Optimizing Content Information for\n  Maximal Entropy", "comments": "8 pages, 4 figures, Accepted to MICCAI 2018 Workshop: Deep Learning\n  in Medical Image Analysis (DLMIA)", "journal-ref": null, "doi": "10.1007/978-3-030-00889-5_21", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is essential for medical image analysis tasks such as\nintervention planning, therapy guidance, diagnosis, treatment decisions. Deep\nlearning is becoming increasingly prominent for segmentation, where the lack of\nannotations, however, often becomes the main limitation. Due to privacy\nconcerns and ethical considerations, most medical datasets are created,\ncurated, and allow access only locally. Furthermore, current deep learning\nmethods are often suboptimal in translating anatomical knowledge between\ndifferent medical imaging modalities. Active learning can be used to select an\ninformed set of image samples to request for manual annotation, in order to\nbest utilize the limited annotation time of clinical experts for optimal\noutcomes, which we focus on in this work. Our contributions herein are two\nfold: (1) we enforce domain-representativeness of selected samples using a\nproposed penalization scheme to maximize information at the network abstraction\nlayer, and (2) we propose a Borda-count based sample querying scheme for\nselecting samples for segmentation. Comparative experiments with baseline\napproaches show that the samples queried with our proposed method, where both\nabove contributions are combined, result in significantly improved segmentation\nperformance for this active learning task.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 14:24:39 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Ozdemir", "Firat", ""], ["Peng", "Zixuan", ""], ["Tanner", "Christine", ""], ["Fuernstahl", "Philipp", ""], ["Goksel", "Orcun", ""]]}, {"id": "1807.06964", "submitter": "Jungwook Choi", "authors": "Jungwook Choi, Pierce I-Jen Chuang, Zhuo Wang, Swagath Venkataramani,\n  Vijayalakshmi Srinivasan, Kailash Gopalakrishnan", "title": "Bridging the Accuracy Gap for 2-bit Quantized Neural Networks (QNN)", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.06085", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms achieve high classification accuracy at the expense\nof significant computation cost. In order to reduce this cost, several\nquantization schemes have gained attention recently with some focusing on\nweight quantization, and others focusing on quantizing activations. This paper\nproposes novel techniques that target weight and activation quantizations\nseparately resulting in an overall quantized neural network (QNN). The\nactivation quantization technique, PArameterized Clipping acTivation (PACT),\nuses an activation clipping parameter $\\alpha$ that is optimized during\ntraining to find the right quantization scale. The weight quantization scheme,\nstatistics-aware weight binning (SAWB), finds the optimal scaling factor that\nminimizes the quantization error based on the statistical characteristics of\nthe distribution of weights without the need for an exhaustive search. The\ncombination of PACT and SAWB results in a 2-bit QNN that achieves\nstate-of-the-art classification accuracy (comparable to full precision\nnetworks) across a range of popular models and datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 13:36:57 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Choi", "Jungwook", ""], ["Chuang", "Pierce I-Jen", ""], ["Wang", "Zhuo", ""], ["Venkataramani", "Swagath", ""], ["Srinivasan", "Vijayalakshmi", ""], ["Gopalakrishnan", "Kailash", ""]]}, {"id": "1807.06980", "submitter": "Amir Ghodrati", "authors": "Amir Ghodrati, Efstratios Gavves, Cees G. M. Snoek", "title": "Video Time: Properties, Encoders and Evaluation", "comments": "14 pages, BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-aware encoding of frame sequences in a video is a fundamental problem in\nvideo understanding. While many attempted to model time in videos, an explicit\nstudy on quantifying video time is missing. To fill this lacuna, we aim to\nevaluate video time explicitly. We describe three properties of video time,\nnamely a) temporal asymmetry, b)temporal continuity and c) temporal causality.\nBased on each we formulate a task able to quantify the associated property.\nThis allows assessing the effectiveness of modern video encoders, like C3D and\nLSTM, in their ability to model time. Our analysis provides insights about\nexisting encoders while also leading us to propose a new video time encoder,\nwhich is better suited for the video time recognition tasks than C3D and LSTM.\nWe believe the proposed meta-analysis can provide a reasonable baseline to\nassess video time encoders on equal grounds on a set of temporal-aware tasks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 14:47:36 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Ghodrati", "Amir", ""], ["Gavves", "Efstratios", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1807.07001", "submitter": "Russell Hardie", "authors": "Russell C. Hardie, Redha Ali, Manawaduge Supun De Silva, and Temesguen\n  Messay Kebede", "title": "Skin Lesion Segmentation and Classification for ISIC 2018 Using\n  Traditional Classifiers with Hand-Crafted Features", "comments": "ISIC 2018 https://challenge2018.isic-archive.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides the required description of the methods used to obtain\nsubmitted results for Task1 and Task 3 of ISIC 2018: Skin Lesion Analysis\nTowards Melanoma Detection. The results have been created by a team of\nresearchers at the University of Dayton Signal and Image Processing Lab. In\nthis submission, traditional classifiers with hand-crafted features are\nutilized for Task 1 and Task 3. Our team is providing additional separate\nsubmissions using deep learning methods for comparison.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 15:42:29 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Hardie", "Russell C.", ""], ["Ali", "Redha", ""], ["De Silva", "Manawaduge Supun", ""], ["Kebede", "Temesguen Messay", ""]]}, {"id": "1807.07033", "submitter": "Huy-Hieu Pham", "authors": "Huy Hieu Pham, Louahdi Khoudour, Alain Crouzil, Pablo Zegers, Sergio\n  A. Velastin", "title": "Skeletal Movement to Color Map: A Novel Representation for 3D Action\n  Recognition with Inception Residual Networks", "comments": "This article corresponds to our accepted version at the 2018 IEEE\n  International Conference on Image Processing (ICIP). We will link the Digital\n  Object Identifier (DOI) as soon as it is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel skeleton-based representation for 3D action recognition in\nvideos using Deep Convolutional Neural Networks (D-CNNs). Two key issues have\nbeen addressed: First, how to construct a robust representation that easily\ncaptures the spatial-temporal evolutions of motions from skeleton sequences.\nSecond, how to design D-CNNs capable of learning discriminative features from\nthe new representation in a effective manner. To address these tasks, a\nskeletonbased representation, namely, SPMF (Skeleton Pose-Motion Feature) is\nproposed. The SPMFs are built from two of the most important properties of a\nhuman action: postures and their motions. Therefore, they are able to\neffectively represent complex actions. For learning and recognition tasks, we\ndesign and optimize new D-CNNs based on the idea of Inception Residual networks\nto predict actions from SPMFs. Our method is evaluated on two challenging\ndatasets including MSR Action3D and NTU-RGB+D. Experimental results indicated\nthat the proposed method surpasses state-of-the-art methods whilst requiring\nless computation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 16:36:39 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Pham", "Huy Hieu", ""], ["Khoudour", "Louahdi", ""], ["Crouzil", "Alain", ""], ["Zegers", "Pablo", ""], ["Velastin", "Sergio A.", ""]]}, {"id": "1807.07044", "submitter": "Olga Veksler", "authors": "Zhenyi Wang, Olga Veksler", "title": "Location Augmentation for CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CNNs have made a tremendous impact on the field of computer vision in the\nlast several years. The main component of any CNN architecture is the\nconvolution operation, which is translation invariant by design. However,\nlocation in itself can be an important cue. For example, a salient object is\nmore likely to be closer to the center of the image, the sky in the top part of\nan image, etc. To include the location cue for feature learning, we propose to\naugment the color image, the usual input to CNNs, with one or more channels\nthat carry location information. We test two approaches for adding location\ninformation. In the first approach, we incorporate location directly, by\nincluding the row and column indexes as two additional channels to the input\nimage. In the second approach, we add location less directly by adding distance\ntransform from the center pixel as an additional channel to the input image. We\nperform experiments with both direct and indirect ways to encode location. We\nshow the advantage of augmenting the standard color input with location related\nchannels on the tasks of salient object segmentation, semantic segmentation,\nand scene parsing.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 17:16:42 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 18:16:07 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 23:57:51 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Wang", "Zhenyi", ""], ["Veksler", "Olga", ""]]}, {"id": "1807.07049", "submitter": "Lerrel Pinto Mr", "authors": "Abhinav Gupta, Adithyavairavan Murali, Dhiraj Gandhi, Lerrel Pinto", "title": "Robot Learning in Homes: Improving Generalization and Reducing Dataset\n  Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven approaches to solving robotic tasks have gained a lot of traction\nin recent years. However, most existing policies are trained on large-scale\ndatasets collected in curated lab settings. If we aim to deploy these models in\nunstructured visual environments like people's homes, they will be unable to\ncope with the mismatch in data distribution. In such light, we present the\nfirst systematic effort in collecting a large dataset for robotic grasping in\nhomes. First, to scale and parallelize data collection, we built a low cost\nmobile manipulator assembled for under 3K USD. Second, data collected using low\ncost robots suffer from noisy labels due to imperfect execution and calibration\nerrors. To handle this, we develop a framework which factors out the noise as a\nlatent variable. Our model is trained on 28K grasps collected in several houses\nunder an array of different environmental conditions. We evaluate our models by\nphysically executing grasps on a collection of novel objects in multiple unseen\nhomes. The models trained with our home dataset showed a marked improvement of\n43.7% over a baseline model trained with data collected in lab. Our\narchitecture which explicitly models the latent noise in the dataset also\nperformed 10% better than one that did not factor out the noise. We hope this\neffort inspires the robotics community to look outside the lab and embrace\nlearning based approaches to handle inaccurate cheap robots.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 17:25:28 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Gupta", "Abhinav", ""], ["Murali", "Adithyavairavan", ""], ["Gandhi", "Dhiraj", ""], ["Pinto", "Lerrel", ""]]}, {"id": "1807.07144", "submitter": "Youbao Tang", "authors": "Youbao Tang, Jinzheng Cai, Le Lu, Adam P. Harrison, Ke Yan, Jing Xiao,\n  Lin Yang, Ronald M. Summers", "title": "CT Image Enhancement Using Stacked Generative Adversarial Networks and\n  Transfer Learning for Lesion Segmentation Improvement", "comments": "Accepted by MLMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated lesion segmentation from computed tomography (CT) is an important\nand challenging task in medical image analysis. While many advancements have\nbeen made, there is room for continued improvements. One hurdle is that CT\nimages can exhibit high noise and low contrast, particularly in lower dosages.\nTo address this, we focus on a preprocessing method for CT images that uses\nstacked generative adversarial networks (SGAN) approach. The first GAN reduces\nthe noise in the CT image and the second GAN generates a higher resolution\nimage with enhanced boundaries and high contrast. To make up for the absence of\nhigh quality CT images, we detail how to synthesize a large number of low- and\nhigh-quality natural images and use transfer learning with progressively larger\namounts of CT images. We apply both the classic GrabCut method and the modern\nholistically nested network (HNN) to lesion segmentation, testing whether SGAN\ncan yield improved lesion segmentation. Experimental results on the DeepLesion\ndataset demonstrate that the SGAN enhancements alone can push GrabCut\nperformance over HNN trained on original images. We also demonstrate that HNN +\nSGAN performs best compared against four other enhancement methods, including\nwhen using only a single GAN.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 21:01:37 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tang", "Youbao", ""], ["Cai", "Jinzheng", ""], ["Lu", "Le", ""], ["Harrison", "Adam P.", ""], ["Yan", "Ke", ""], ["Xiao", "Jing", ""], ["Yang", "Lin", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1807.07155", "submitter": "Stephen Law Dr", "authors": "Stephen Law and Brooks Paige and Chris Russell", "title": "Take a Look Around: Using Street View and Satellite Images to Estimate\n  House Prices", "comments": "published in ACM Transactions on Intelligent Systems and Technology\n  (TIST) Volume 10 Issue 5, October 2019 Article No. 54", "journal-ref": null, "doi": "10.1145/3342240", "report-no": null, "categories": "econ.EM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an individual purchases a home, they simultaneously purchase its\nstructural features, its accessibility to work, and the neighborhood amenities.\nSome amenities, such as air quality, are measurable while others, such as the\nprestige or the visual impression of a neighborhood, are difficult to quantify.\nDespite the well-known impacts intangible housing features have on house\nprices, limited attention has been given to systematically quantifying these\ndifficult to measure amenities. Two issues have led to this neglect. Not only\ndo few quantitative methods exist that can measure the urban environment, but\nthat the collection of such data is both costly and subjective.\n  We show that street image and satellite image data can capture these urban\nqualities and improve the estimation of house prices. We propose a pipeline\nthat uses a deep neural network model to automatically extract visual features\nfrom images to estimate house prices in London, UK. We make use of traditional\nhousing features such as age, size, and accessibility as well as visual\nfeatures from Google Street View images and Bing aerial images in estimating\nthe house price model. We find encouraging results where learning to\ncharacterize the urban quality of a neighborhood improves house price\nprediction, even when generalizing to previously unseen London boroughs.\n  We explore the use of non-linear vs. linear methods to fuse these cues with\nconventional models of house pricing, and show how the interpretability of\nlinear models allows us to directly extract proxy variables for visual\ndesirability of neighborhoods that are both of interest in their own right, and\ncould be used as inputs to other econometric methods. This is particularly\nvaluable as once the network has been trained with the training data, it can be\napplied elsewhere, allowing us to generate vivid dense maps of the visual\nappeal of London streets.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 21:10:08 GMT"}, {"version": "v2", "created": "Mon, 21 Oct 2019 09:49:59 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Law", "Stephen", ""], ["Paige", "Brooks", ""], ["Russell", "Chris", ""]]}, {"id": "1807.07203", "submitter": "Nakamasa Inoue", "authors": "Nakamasa Inoue, Koichi Shinoda", "title": "Few-Shot Adaptation for Multimedia Semantic Indexing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a few-shot adaptation framework, which bridges zero-shot learning\nand supervised many-shot learning, for semantic indexing of image and video\ndata. Few-shot adaptation provides robust parameter estimation with few\ntraining examples, by optimizing the parameters of zero-shot learning and\nsupervised many-shot learning simultaneously. In this method, first we build a\nzero-shot detector, and then update it by using the few examples. Our\nexperiments show the effectiveness of the proposed framework on three datasets:\nTRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we\nshow that our method outperforms recent few-shot learning methods. On the\nTRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision\nunder the zero-shot condition and the supervised condition, respectively. To\nthe best of our knowledge, these are the best results on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 00:58:33 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Inoue", "Nakamasa", ""], ["Shinoda", "Koichi", ""]]}, {"id": "1807.07226", "submitter": "Siddharth Mahendran", "authors": "Siddharth Mahendran, Ming Yang Lu, Haider Ali, Ren\\'e Vidal", "title": "Monocular Object Orientation Estimation using Riemannian Regression and\n  Classification Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating the 3D orientation of an object of known\ncategory given an image of the object and a bounding box around it. Recently,\nCNN-based regression and classification methods have shown significant\nperformance improvements for this task. This paper proposes a new CNN-based\napproach to monocular orientation estimation that advances the state of the art\nin four different directions. First, we take into account the Riemannian\nstructure of the orientation space when designing regression losses and\nnonlinear activation functions. Second, we propose a mixed Riemannian\nregression and classification framework that better handles the challenging\ncase of nearly symmetric objects. Third, we propose a data augmentation\nstrategy that is specifically designed to capture changes in 3D orientation.\nFourth, our approach leads to state-of-the-art results on the PASCAL3D+\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 03:28:30 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Mahendran", "Siddharth", ""], ["Lu", "Ming Yang", ""], ["Ali", "Haider", ""], ["Vidal", "Ren\u00e9", ""]]}, {"id": "1807.07243", "submitter": "Chao Li", "authors": "Chao Li and Zheheng Zhao and Xiaohu Guo", "title": "ArticulatedFusion: Real-time Reconstruction of Motion, Geometry and\n  Segmentation Using a Single Depth Camera", "comments": "European Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a real-time dynamic scene reconstruction method capable\nof reproducing the motion, geometry, and segmentation simultaneously given live\ndepth stream from a single RGB-D camera. Our approach fuses geometry frame by\nframe and uses a segmentation-enhanced node graph structure to drive the\ndeformation of geometry in registration step. A two-level node motion\noptimization is proposed. The optimization space of node motions and the range\nof physically-plausible deformations are largely reduced by taking advantage of\nthe articulated motion prior, which is solved by an efficient node graph\nsegmentation method. Compared to previous fusion-based dynamic scene\nreconstruction methods, our experiments show robust and improved reconstruction\nresults for tangential and occluded motions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 05:38:40 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Li", "Chao", ""], ["Zhao", "Zheheng", ""], ["Guo", "Xiaohu", ""]]}, {"id": "1807.07247", "submitter": "Dwarikanath Mahapatra", "authors": "Zongyuan Ge, Dwarikanath Mahapatra, Suman Sedai, Rahil Garnavi, Rajib\n  Chakravorty", "title": "Chest X-rays Classification: A Multi-Label and Fine-Grained Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely used ChestX-ray14 dataset addresses an important medical image\nclassification problem and has the following caveats: 1) many lung pathologies\nare visually similar, 2) a variant of diseases including lung cancer,\ntuberculosis, and pneumonia are present in a single scan, i.e. multiple labels\nand 3) The incidence of healthy images is much larger than diseased samples,\ncreating imbalanced data. These properties are common in medical domain.\nExisting literature uses stateof- the-art DensetNet/Resnet models being\ntransfer learned where output neurons of the networks are trained for\nindividual diseases to cater for multiple diseases labels in each image.\nHowever, most of them don't consider relationship between multiple classes. In\nthis work we have proposed a novel error function, Multi-label Softmax Loss\n(MSML), to specifically address the properties of multiple labels and\nimbalanced data. Moreover, we have designed deep network architecture based on\nfine-grained classification concept that incorporates MSML. We have evaluated\nour proposed method on various network backbones and showed consistent\nperformance improvements of AUC-ROC scores on the ChestX-ray14 dataset. The\nproposed error function provides a new method to gain improved performance\nacross wider medical datasets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 06:02:54 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 12:47:43 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 22:15:49 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ge", "Zongyuan", ""], ["Mahapatra", "Dwarikanath", ""], ["Sedai", "Suman", ""], ["Garnavi", "Rahil", ""], ["Chakravorty", "Rajib", ""]]}, {"id": "1807.07258", "submitter": "Jindong Wang", "authors": "Jindong Wang and Wenjie Feng and Yiqiang Chen and Han Yu and Meiyu\n  Huang and Philip S. Yu", "title": "Visual Domain Adaptation with Manifold Embedded Distribution Alignment", "comments": "ACM Multimedia conference 2018 (ACM MM) ORAL paper; top 10 papers; 9\n  pages; code available at http://transferlearning.xyz", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Visual domain adaptation aims to learn robust classifiers for the target\ndomain by leveraging knowledge from a source domain. Existing methods either\nattempt to align the cross-domain distributions, or perform manifold subspace\nlearning. However, there are two significant challenges: (1) degenerated\nfeature transformation, which means that distribution alignment is often\nperformed in the original feature space, where feature distortions are hard to\novercome. On the other hand, subspace learning is not sufficient to reduce the\ndistribution divergence. (2) unevaluated distribution alignment, which means\nthat existing distribution alignment methods only align the marginal and\nconditional distributions with equal importance, while they fail to evaluate\nthe different importance of these two distributions in real applications. In\nthis paper, we propose a Manifold Embedded Distribution Alignment (MEDA)\napproach to address these challenges. MEDA learns a domain-invariant classifier\nin Grassmann manifold with structural risk minimization, while performing\ndynamic distribution alignment to quantitatively account for the relative\nimportance of marginal and conditional distributions. To the best of our\nknowledge, MEDA is the first attempt to perform dynamic distribution alignment\nfor manifold domain adaptation. Extensive experiments demonstrate that MEDA\nshows significant improvements in classification accuracy compared to\nstate-of-the-art traditional and deep methods.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 06:45:42 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 08:15:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wang", "Jindong", ""], ["Feng", "Wenjie", ""], ["Chen", "Yiqiang", ""], ["Yu", "Han", ""], ["Huang", "Meiyu", ""], ["Yu", "Philip S.", ""]]}, {"id": "1807.07284", "submitter": "Roberto J. L\\'opez-Sastre", "authors": "Carlos Herranz-Perdiguero, Carolina Redondo-Cabrera and Roberto J.\n  L\\'opez-Sastre", "title": "In pixels we trust: From Pixel Labeling to Object Localization and Scene\n  Categorization", "comments": "IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there has been significant progress in solving the problems of image\npixel labeling, object detection and scene classification, existing approaches\nnormally address them separately. In this paper, we propose to tackle these\nproblems from a bottom-up perspective, where we simply need a semantic\nsegmentation of the scene as input. We employ the DeepLab architecture, based\non the ResNet deep network, which leverages multi-scale inputs to later fuse\ntheir responses to perform a precise pixel labeling of the scene. This semantic\nsegmentation mask is used to localize the objects and to recognize the scene,\nfollowing two simple yet effective strategies. We evaluate the benefits of our\nsolutions, performing a thorough experimental evaluation on the NYU Depth V2\ndataset. Our approach achieves a performance that beats the leading results by\na significant margin, defining the new state of the art in this benchmark for\nthe three tasks comprising the scene understanding: semantic segmentation,\nobject detection and scene categorization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 08:28:32 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Herranz-Perdiguero", "Carlos", ""], ["Redondo-Cabrera", "Carolina", ""], ["L\u00f3pez-Sastre", "Roberto J.", ""]]}, {"id": "1807.07295", "submitter": "K L Navaneet", "authors": "K L Navaneet, Ravi Kiran Sarvadevabhatla, Shashank Shekhar, R.\n  Venkatesh Babu and Anirban Chakraborty", "title": "Operator-in-the-Loop Deep Sequential Multi-camera Feature Fusion for\n  Person Re-identification", "comments": "Accepted at IEEE Transactions on Information Forensics & Security", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a target image as query, person re-identification systems retrieve a\nranked list of candidate matches on a per-camera basis. In deployed systems, a\nhuman operator scans these lists and labels sighted targets by touch or\nmouse-based selection. However, classical re-id approaches generate per-camera\nlists independently. Therefore, target identifications by operator in a subset\nof cameras cannot be utilized to improve ranking of the target in remaining set\nof network cameras. To address this shortcoming, we propose a novel sequential\nmulti-camera re-id approach. The proposed approach can accommodate human\noperator inputs and provides early gains via a monotonic improvement in target\nranking. At the heart of our approach is a fusion function which operates on\ndeep feature representations of query and candidate matches. We formulate an\noptimization procedure custom-designed to incrementally improve query\nrepresentation. Since existing evaluation methods cannot be directly adopted to\nour setting, we also propose two novel evaluation protocols. The results on two\nlarge-scale re-id datasets (Market-1501, DukeMTMC-reID) demonstrate that our\nmulti-camera method significantly outperforms baselines and other popular\nfeature fusion schemes. Additionally, we conduct a comparative subject-based\nstudy of human operator performance. The superior operator performance enabled\nby our approach makes a compelling case for its integration into deployable\nvideo-surveillance systems.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 08:52:19 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 08:07:40 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 10:54:56 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 16:35:00 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Navaneet", "K L", ""], ["Sarvadevabhatla", "Ravi Kiran", ""], ["Shekhar", "Shashank", ""], ["Babu", "R. Venkatesh", ""], ["Chakraborty", "Anirban", ""]]}, {"id": "1807.07320", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodr\\'iguez, Josep M. Gonfaus, Guillem Cucurull, F. Xavier Roca,\n  Jordi Gonz\\`alez", "title": "Attend and Rectify: a Gated Attention Mechanism for Fine-Grained\n  Recovery", "comments": "Published at ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel attention mechanism to enhance Convolutional Neural\nNetworks for fine-grained recognition. It learns to attend to lower-level\nfeature activations without requiring part annotations and uses these\nactivations to update and rectify the output likelihood distribution. In\ncontrast to other approaches, the proposed mechanism is modular,\narchitecture-independent and efficient both in terms of parameters and\ncomputation required. Experiments show that networks augmented with our\napproach systematically improve their classification accuracy and become more\nrobust to clutter. As a result, Wide Residual Networks augmented with our\nproposal surpasses the state of the art classification accuracies in CIFAR-10,\nthe Adience gender recognition task, Stanford dogs, and UEC Food-100.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 09:52:36 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 09:23:39 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Rodr\u00edguez", "Pau", ""], ["Gonfaus", "Josep M.", ""], ["Cucurull", "Guillem", ""], ["Roca", "F. Xavier", ""], ["Gonz\u00e0lez", "Jordi", ""]]}, {"id": "1807.07327", "submitter": "Xu Liu", "authors": "Lin Cheng, Xu Liu, Lingling Li, Licheng Jiao, Xu Tang", "title": "Deep Adaptive Proposal Network for Object Detection in Optical Remote\n  Sensing Images", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection is a fundamental and challenging problem in aerial and\nsatellite image analysis. More recently, a two-stage detector Faster R-CNN is\nproposed and demonstrated to be a promising tool for object detection in\noptical remote sensing images, while the sparse and dense characteristic of\nobjects in remote sensing images is complexity. It is unreasonable to treat all\nimages with the same region proposal strategy, and this treatment limits the\nperformance of two-stage detectors. In this paper, we propose a novel and\neffective approach, named deep adaptive proposal network (DAPNet), address this\ncomplexity characteristic of object by learning a new category prior network\n(CPN) on the basis of the existing Faster R-CNN architecture. Moreover, the\ncandidate regions produced by DAPNet model are different from the traditional\nregion proposal network (RPN), DAPNet predicts the detail category of each\ncandidate region. And these candidate regions combine the object number, which\ngenerated by the category prior network to achieve a suitable number of\ncandidate boxes for each image. These candidate boxes can satisfy detection\ntasks in sparse and dense scenes. The performance of the proposed framework has\nbeen evaluated on the challenging NWPU VHR-10 data set. Experimental results\ndemonstrate the superiority of the proposed framework to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 10:10:30 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Cheng", "Lin", ""], ["Liu", "Xu", ""], ["Li", "Lingling", ""], ["Jiao", "Licheng", ""], ["Tang", "Xu", ""]]}, {"id": "1807.07343", "submitter": "Pierre Barr\\'e", "authors": "Pierre Barr\\'e, Katja Herzog, Rebecca H\\\"ofle, Matthias B. Hullin,\n  Reinhard T\\\"opfer and Volker Steinhage", "title": "Automated Phenotyping of Epicuticular Waxes of Grapevine Berries Using\n  Light Separation and Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In viticulture the epicuticular wax as the outer layer of the berry skin is\nknown as trait which is correlated to resilience towards Botrytis bunch rot.\nTraditionally this trait is classified using the OIV descriptor 227 (berry\nbloom) in a time consuming way resulting in subjective and error-prone\nphenotypic data. In the present study an objective, fast and sensor-based\napproach was developed to monitor berry bloom. From the technical\npoint-of-view, it is known that the measurement of different illumination\ncomponents conveys important information about observed object surfaces. A\nMobile Light-Separation-Lab is proposed in order to capture\nillumination-separated images of grapevine berries for phenotyping the\ndistribution of epicuticular waxes (berry bloom). For image analysis, an\nefficient convolutional neural network approach is used to derive the\nuniformity and intactness of waxes on berries. Method validation over six\ngrapevine cultivars shows accuracies up to $97.3$%. In addition, electrical\nimpedance of the cuticle and its epicuticular waxes (described as an indicator\nfor the thickness of berry skin and its permeability) was correlated to the\ndetected proportion of waxes with $r=0.76$. This novel, fast and non-invasive\nphenotyping approach facilitates enlarged screenings within grapevine breeding\nmaterial and genetic repositories regarding berry bloom characteristics and its\nimpact on resilience towards Botrytis bunch rot.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 11:17:08 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 13:09:29 GMT"}, {"version": "v3", "created": "Thu, 6 Sep 2018 12:10:40 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Barr\u00e9", "Pierre", ""], ["Herzog", "Katja", ""], ["H\u00f6fle", "Rebecca", ""], ["Hullin", "Matthias B.", ""], ["T\u00f6pfer", "Reinhard", ""], ["Steinhage", "Volker", ""]]}, {"id": "1807.07349", "submitter": "Christine Tanner", "authors": "Christine Tanner, Firat Ozdemir, Romy Profanter, Valeriy Vishnevsky,\n  Ender Konukoglu, Orcun Goksel", "title": "Generative Adversarial Networks for MR-CT Deformable Image Registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable Image Registration (DIR) of MR and CT images is one of the most\nchallenging registration task, due to the inherent structural differences of\nthe modalities and the missing dense ground truth. Recently cycle Generative\nAdversarial Networks (cycle-GANs) have been used to learn the intensity\nrelationship between these 2 modalities for unpaired brain data. Yet its\nusefulness for DIR was not assessed.\n  In this study we evaluate the DIR performance for thoracic and abdominal\norgans after synthesis by cycle-GAN. We show that geometric changes, which\ndifferentiate the two populations (e.g. inhale vs. exhale), are readily\nsynthesized as well. This causes substantial problems for any application which\nrelies on spatial correspondences being preserved between the real and the\nsynthesized image (e.g. plan, segmentation, landmark propagation). To alleviate\nthis problem, we investigated reducing the spatial information provided to the\ndiscriminator by decreasing the size of its receptive fields.\n  Image synthesis was learned from 17 unpaired subjects per modality.\nRegistration performance was evaluated with respect to manual segmentations of\n11 structures for 3 subjects from the VISERAL challenge. State-of-the-art DIR\nmethods based on Normalized Mutual Information (NMI), Modality Independent\nNeighborhood Descriptor (MIND) and their novel combination achieved a mean\nsegmentation overlap ratio of 76.7, 67.7, 76.9%, respectively. This dropped to\n69.1% or less when registering images synthesized by cycle-GAN based on local\ncorrelation, due to the poor performance on the thoracic region, where large\nlung volume changes were synthesized. Performance for the abdominal region was\nsimilar to that of CT-MRI NMI registration (77.4 vs. 78.8%) when using 3D\nsynthesizing MRIs (12 slices) and medium sized receptive fields for the\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 11:41:07 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tanner", "Christine", ""], ["Ozdemir", "Firat", ""], ["Profanter", "Romy", ""], ["Vishnevsky", "Valeriy", ""], ["Konukoglu", "Ender", ""], ["Goksel", "Orcun", ""]]}, {"id": "1807.07356", "submitter": "Guotai Wang", "authors": "Guotai Wang, Wenqi Li, Michael Aertsen, Jan Deprest, Sebastien\n  Ourselin, Tom Vercauteren", "title": "Aleatoric uncertainty estimation with test-time augmentation for medical\n  image segmentation with convolutional neural networks", "comments": "13 pages, 8 figures, accepted by NeuroComputing", "journal-ref": null, "doi": "10.1016/j.neucom.2019.01.103", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the state-of-the-art performance for medical image segmentation, deep\nconvolutional neural networks (CNNs) have rarely provided uncertainty\nestimations regarding their segmentation outputs, e.g., model (epistemic) and\nimage-based (aleatoric) uncertainties. In this work, we analyze these different\ntypes of uncertainties for CNN-based 2D and 3D medical image segmentation\ntasks. We additionally propose a test-time augmentation-based aleatoric\nuncertainty to analyze the effect of different transformations of the input\nimage on the segmentation output. Test-time augmentation has been previously\nused to improve segmentation accuracy, yet not been formulated in a consistent\nmathematical framework. Hence, we also propose a theoretical formulation of\ntest-time augmentation, where a distribution of the prediction is estimated by\nMonte Carlo simulation with prior distributions of parameters in an image\nacquisition model that involves image transformations and noise. We compare and\ncombine our proposed aleatoric uncertainty with model uncertainty. Experiments\nwith segmentation of fetal brains and brain tumors from 2D and 3D Magnetic\nResonance Images (MRI) showed that 1) the test-time augmentation-based\naleatoric uncertainty provides a better uncertainty estimation than calculating\nthe test-time dropout-based model uncertainty alone and helps to reduce\noverconfident incorrect predictions, and 2) our test-time augmentation\noutperforms a single-prediction baseline and dropout-based multiple\npredictions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 11:58:14 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 08:26:59 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 06:07:11 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Wang", "Guotai", ""], ["Li", "Wenqi", ""], ["Aertsen", "Michael", ""], ["Deprest", "Jan", ""], ["Ourselin", "Sebastien", ""], ["Vercauteren", "Tom", ""]]}, {"id": "1807.07364", "submitter": "Muhammad Kamran Janjua", "authors": "Shah Nawaz, Muhammad Kamran Janjua, Alessandro Calefati, Ignazio Gallo", "title": "Revisiting Cross Modal Retrieval", "comments": "14 pages. Under review at ECCVW (MULA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a cross-modal retrieval system that leverages on image\nand text encoding. Most multimodal architectures employ separate networks for\neach modality to capture the semantic relationship between them. However, in\nour work image-text encoding can achieve comparable results in terms of\ncross-modal retrieval without having to use a separate network for each\nmodality. We show that text encodings can capture semantic relationships\nbetween multiple modalities. In our knowledge, this work is the first of its\nkind in terms of employing a single network and fused image-text embedding for\ncross-modal retrieval. We evaluate our approach on two famous multimodal\ndatasets: MS-COCO and Flickr30K.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 12:35:24 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Nawaz", "Shah", ""], ["Janjua", "Muhammad Kamran", ""], ["Calefati", "Alessandro", ""], ["Gallo", "Ignazio", ""]]}, {"id": "1807.07391", "submitter": "Rongjian Xu", "authors": "Hongdiao Wen, Rongjian Xu, Tie Zhang", "title": "ISIC 2018-A Method for Lesion Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our team participate in the challenge of Task 1: Lesion Boundary Segmentation\n, and use a combined network, one of which is designed by ourselves named\nupdcnn net and another is an improved VGG 16-layer net. Updcnn net uses reduced\nsize images for training, and VGG 16-layer net utilizes large size images.\nImage enhancement is used to get a richer data set. We use boxes in the VGG\n16-layer net network for local attention regularization to fine-tune the loss\nfunction, which can increase the number of training data, and also make the\nmodel more robust. In the test, the model is used for joint testing and\nachieves good results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:23:00 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 00:56:32 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Wen", "Hongdiao", ""], ["Xu", "Rongjian", ""], ["Zhang", "Tie", ""]]}, {"id": "1807.07416", "submitter": "Valery Vishnevskiy", "authors": "Valery Vishnevskiy, Sergio J Sanabria, and Orcun Goksel", "title": "Image Reconstruction via Variational Network for Real-Time Hand-Held\n  Sound-Speed Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speed-of-sound is a biomechanical property for quantitative tissue\ndifferentiation, with great potential as a new ultrasound-based image modality.\nA conventional ultrasound array transducer can be used together with an\nacoustic mirror, or so-called reflector, to reconstruct sound-speed images from\ntime-of-flight measurements to the reflector collected between transducer\nelement pairs, which constitutes a challenging problem of limited-angle\ncomputed tomography. For this problem, we herein present a variational network\nbased image reconstruction architecture that is based on optimization loop\nunrolling, and provide an efficient training protocol of this network\narchitecture on fully synthetic inclusion data. Our results indicate that the\nlearned model presents good generalization ability, being able to reconstruct\nimages with significantly different statistics compared to the training set.\nComplex inclusion geometries were shown to be successfully reconstructed, also\nimproving over the prior-art by 23% in reconstruction error and by 10% in\ncontrast on synthetic data. In a phantom study, we demonstrated the detection\nof multiple inclusions that were not distinguishable by prior-art\nreconstruction, meanwhile improving the contrast by 27% for a stiff inclusion\nand by 219% for a soft inclusion. Our reconstruction algorithm takes\napproximately 10ms, enabling its use as a real-time imaging method on an\nultrasound machine, for which we are demonstrating an example preliminary setup\nherein.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:39:45 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Vishnevskiy", "Valery", ""], ["Sanabria", "Sergio J", ""], ["Goksel", "Orcun", ""]]}, {"id": "1807.07428", "submitter": "Nikita Dvornik", "authors": "Nikita Dvornik (Thoth), Julien Mairal (Thoth), Cordelia Schmid (Thoth)", "title": "Modeling Visual Context is Key to Augmenting Object Detection Datasets", "comments": null, "journal-ref": "ECCV2018, Sep 2018, Munich, Germany. 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing data augmentation for learning deep neural networks is well known\nto be important for training visual recognition systems. By artificially\nincreasing the number of training examples, it helps reducing overfitting and\nimproves generalization. For object detection, classical approaches for data\naugmentation consist of generating images obtained by basic geometrical\ntransformations and color changes of original training images. In this work, we\ngo one step further and leverage segmentation annotations to increase the\nnumber of object instances present on training data. For this approach to be\nsuccessful, we show that modeling appropriately the visual context surrounding\nobjects is crucial to place them in the right environment. Otherwise, we show\nthat the previous strategy actually hurts. With our context model, we achieve\nsignificant mean average precision improvements when few labeled examples are\navailable on the VOC'12 benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:50:42 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Dvornik", "Nikita", "", "Thoth"], ["Mairal", "Julien", "", "Thoth"], ["Schmid", "Cordelia", "", "Thoth"]]}, {"id": "1807.07429", "submitter": "Guillermo Gallego", "authors": "Yi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip, Hongdong Li,\n  Davide Scaramuzza", "title": "Semi-Dense 3D Reconstruction with a Stereo Event Camera", "comments": "19 pages, 8 figures, Video: https://youtu.be/Qrnpj2FD1e4", "journal-ref": "European Conference on Computer Vision (ECCV), Munich, 2018", "doi": "10.1007/978-3-030-01246-5_15", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event cameras are bio-inspired sensors that offer several advantages, such as\nlow latency, high-speed and high dynamic range, to tackle challenging scenarios\nin computer vision. This paper presents a solution to the problem of 3D\nreconstruction from data captured by a stereo event-camera rig moving in a\nstatic scene, such as in the context of stereo Simultaneous Localization and\nMapping. The proposed method consists of the optimization of an energy function\ndesigned to exploit small-baseline spatio-temporal consistency of events\ntriggered across both stereo image planes. To improve the density of the\nreconstruction and to reduce the uncertainty of the estimation, a probabilistic\ndepth-fusion strategy is also developed. The resulting method has no special\nrequirements on either the motion of the stereo event-camera rig or on prior\nknowledge about the scene. Experiments demonstrate our method can deal with\nboth texture-rich scenes as well as sparse scenes, outperforming\nstate-of-the-art stereo methods based on event data image representations.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:54:08 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Zhou", "Yi", ""], ["Gallego", "Guillermo", ""], ["Rebecq", "Henri", ""], ["Kneip", "Laurent", ""], ["Li", "Hongdong", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1807.07432", "submitter": "Thomas Mitchel", "authors": "Thomas W. Mitchel, Sipu Ruan, Gregory S. Chirikjian", "title": "Signal Alignment for Humanoid Skeletons via the Globally Optimal\n  Reparameterization Algorithm", "comments": "Humanoids 2018 initial submission; companion paper to\n  arXiv:1807.05485", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general ability to analyze and classify the 3D kinematics of the human\nform is an essential step in the development of socially adept humanoid robots.\nA variety of different types of signals can be used by machines to represent\nand characterize actions such as RGB videos, infrared maps, and optical flow.\nIn particular, skeleton sequences provide a natural 3D kinematic description of\nhuman motions and can be acquired in real time using RGB+D cameras. Moreover,\nskeleton sequences are generalizable to characterize the motions of both humans\nand humanoid robots. The Globally Optimal Reparameterization Algorithm (GORA)\nis a novel, recently proposed algorithm for signal alignment in which signals\nare reparameterized to a globally optimal universal standard timescale (UST).\nHere, we introduce a variant of GORA for humanoid action recognition with\nskeleton sequences, which we call GORA-S. We briefly review the algorithm's\nmathematical foundations and contextualize them in the problem of action\nrecognition with skeleton sequences. Subsequently, we introduce GORA-S and\ndiscuss parameters and numerical techniques for its effective implementation.\nWe then compare its performance with that of the DTW and FastDTW algorithms, in\nterms of computational efficiency and accuracy in matching skeletons. Our\nresults show that GORA-S attains a complexity that is significantly less than\nthat of any tested DTW method. In addition, it displays a favorable balance\nbetween speed and accuracy that remains invariant under changes in skeleton\nsampling frequency, lending it a degree of versatility that could make it\nwell-suited for a variety of action recognition tasks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 03:16:39 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 15:14:55 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Mitchel", "Thomas W.", ""], ["Ruan", "Sipu", ""], ["Chirikjian", "Gregory S.", ""]]}, {"id": "1807.07433", "submitter": "Rui Fan", "authors": "Rui Fan, Yanan Liu, Xingrui Yang, Mohammud Junaid Bocus, Naim Dahnoun,\n  Scott Tancock", "title": "Real-Time Stereo Vision for Road Surface 3-D Reconstruction", "comments": "6 pages, 4 figures, IEEE International Conference on Imaging System\n  and Techniques (IST) 2018. arXiv admin note: substantial text overlap with\n  arXiv:1807.02044", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereo vision techniques have been widely used in civil engineering to\nacquire 3-D road data. The two important factors of stereo vision are accuracy\nand speed. However, it is very challenging to achieve both of them\nsimultaneously and therefore the main aim of developing a stereo vision system\nis to improve the trade-off between these two factors. In this paper, we\npresent a real-time stereo vision system used for road surface 3-D\nreconstruction. The proposed system is developed from our previously published\n3-D reconstruction algorithm where the perspective view of the target image is\nfirst transformed into the reference view, which not only increases the\ndisparity accuracy but also improves the processing speed. Then, the\ncorrelation cost between each pair of blocks is computed and stored in two 3-D\ncost volumes. To adaptively aggregate the matching costs from neighbourhood\nsystems, bilateral filtering is performed on the cost volumes. This greatly\nreduces the ambiguities during stereo matching and further improves the\nprecision of the estimated disparities. Finally, the subpixel resolution is\nachieved by conducting a parabola interpolation and the subpixel disparity map\nis used to reconstruct the 3-D road surface. The proposed algorithm is\nimplemented on an NVIDIA GTX 1080 GPU for the real-time purpose. The\nexperimental results illustrate that the reconstruction accuracy is around 3\nmm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 06:44:07 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 02:14:20 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Fan", "Rui", ""], ["Liu", "Yanan", ""], ["Yang", "Xingrui", ""], ["Bocus", "Mohammud Junaid", ""], ["Dahnoun", "Naim", ""], ["Tancock", "Scott", ""]]}, {"id": "1807.07437", "submitter": "Jie Song", "authors": "Jie Song, Chengchao Shen, Jie Lei, An-Xiang Zeng, Kairi Ou, Dacheng\n  Tao, Mingli Song", "title": "Selective Zero-Shot Classification with Augmented Attributes", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a selective zero-shot classification problem: how\ncan the classifier avoid making dubious predictions? Existing attribute-based\nzero-shot classification methods are shown to work poorly in the selective\nclassification scenario. We argue the under-complete human defined attribute\nvocabulary accounts for the poor performance. We propose a selective zero-shot\nclassifier based on both the human defined and the automatically discovered\nresidual attributes. The proposed classifier is constructed by firstly learning\nthe defined and the residual attributes jointly. Then the predictions are\nconducted within the subspace of the defined attributes. Finally, the\nprediction confidence is measured by both the defined and the residual\nattributes. Experiments conducted on several benchmarks demonstrate that our\nclassifier produces a superior performance to other methods under the\nrisk-coverage trade-off metric.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 13:58:36 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Song", "Jie", ""], ["Shen", "Chengchao", ""], ["Lei", "Jie", ""], ["Zeng", "An-Xiang", ""], ["Ou", "Kairi", ""], ["Tao", "Dacheng", ""], ["Song", "Mingli", ""]]}, {"id": "1807.07455", "submitter": "Preetham Putha", "authors": "Preetham Putha, Manoj Tadepalli, Bhargava Reddy, Tarun Raj, Justy\n  Antony Chiramal, Shalini Govil, Namita Sinha, Manjunath KS, Sundeep\n  Reddivari, Ammar Jagirdar, Pooja Rao, Prashant Warier", "title": "Can Artificial Intelligence Reliably Report Chest X-Rays?: Radiologist\n  Validation of an Algorithm trained on 2.3 Million X-Rays", "comments": "v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: Chest X-rays are the most commonly performed, cost-effective\ndiagnostic imaging tests ordered by physicians. A clinically validated AI\nsystem that can reliably separate normals from abnormals can be invaluble\nparticularly in low-resource settings. The aim of this study was to develop and\nvalidate a deep learning system to detect various abnormalities seen on a chest\nX-ray. Methods: A deep learning system was trained on 2.3 million chest X-rays\nand their corresponding radiology reports to identify various abnormalities\nseen on a Chest X-ray. The system was tested against - 1. A three-radiologist\nmajority on an independent, retrospectively collected set of 2000\nX-rays(CQ2000) 2. Radiologist reports on a separate validation set of 100,000\nscans(CQ100k). The primary accuracy measure was area under the ROC curve (AUC),\nestimated separately for each abnormality and for normal versus abnormal scans.\nResults: On the CQ2000 dataset, the deep learning system demonstrated an AUC of\n0.92(CI 0.91-0.94) for detection of abnormal scans, and AUC(CI) of\n0.96(0.94-0.98), 0.96(0.94-0.98), 0.95(0.87-1), 0.95(0.92-0.98),\n0.93(0.90-0.96), 0.89(0.83-0.94), 0.91(0.87-0.96), 0.94(0.93-0.96),\n0.98(0.97-1) for the detection of blunted costophrenic angle, cardiomegaly,\ncavity, consolidation, fibrosis, hilar enlargement, nodule, opacity and pleural\neffusion. The AUCs were similar on the larger CQ100k dataset except for\ndetecting normals where the AUC was 0.86(0.85-0.86). Interpretation: Our study\ndemonstrates that a deep learning algorithm trained on a large, well-labelled\ndataset can accurately detect multiple abnormalities on chest X-rays. As these\nsystems improve in accuracy, applying deep learning to widen the reach of chest\nX-ray interpretation and improve reporting efficiency will add tremendous value\nin radiology workflows and public health screenings globally.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:13:30 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 08:46:13 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Putha", "Preetham", ""], ["Tadepalli", "Manoj", ""], ["Reddy", "Bhargava", ""], ["Raj", "Tarun", ""], ["Chiramal", "Justy Antony", ""], ["Govil", "Shalini", ""], ["Sinha", "Namita", ""], ["KS", "Manjunath", ""], ["Reddivari", "Sundeep", ""], ["Jagirdar", "Ammar", ""], ["Rao", "Pooja", ""], ["Warier", "Prashant", ""]]}, {"id": "1807.07464", "submitter": "Miguel Monteiro", "authors": "Miguel Monteiro, M\\'ario A. T. Figueiredo and Arlindo L. Oliveira", "title": "Conditional Random Fields as Recurrent Neural Networks for 3D Medical\n  Imaging Segmentation", "comments": "11 pages, 3 figures (with 11 subfigures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Conditional Random Field as a Recurrent Neural Network layer is a\nrecently proposed algorithm meant to be placed on top of an existing\nFully-Convolutional Neural Network to improve the quality of semantic\nsegmentation. In this paper, we test whether this algorithm, which was shown to\nimprove semantic segmentation for 2D RGB images, is able to improve\nsegmentation quality for 3D multi-modal medical images. We developed an\nimplementation of the algorithm which works for any number of spatial\ndimensions, input/output image channels, and reference image channels. As far\nas we know this is the first publicly available implementation of this sort. We\ntested the algorithm with two distinct 3D medical imaging datasets, we\nconcluded that the performance differences observed were not statistically\nsignificant. Finally, in the discussion section of the paper, we go into the\nreasons as to why this technique transfers poorly from natural images to\nmedical images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:37:43 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Monteiro", "Miguel", ""], ["Figueiredo", "M\u00e1rio A. T.", ""], ["Oliveira", "Arlindo L.", ""]]}, {"id": "1807.07466", "submitter": "Davide Mazzini", "authors": "Davide Mazzini", "title": "Guided Upsampling Network for Real-Time Semantic Segmentation", "comments": "Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation architectures are mainly built upon an encoder-decoder\nstructure. These models perform subsequent downsampling operations in the\nencoder. Since operations on high-resolution activation maps are\ncomputationally expensive, usually the decoder produces output segmentation\nmaps by upsampling with parameters-free operators like bilinear or\nnearest-neighbor. We propose a Neural Network named Guided Upsampling Network\nwhich consists of a multiresolution architecture that jointly exploits\nhigh-resolution and large context information. Then we introduce a new module\nnamed Guided Upsampling Module (GUM) that enriches upsampling operators by\nintroducing a learnable transformation for semantic maps. It can be plugged\ninto any existing encoder-decoder architecture with little modifications and\nlow additional computation cost. We show with quantitative and qualitative\nexperiments how our network benefits from the use of GUM module. A\ncomprehensive set of experiments on the publicly available Cityscapes dataset\ndemonstrates that Guided Upsampling Network can efficiently process\nhigh-resolution images in real-time while attaining state-of-the art\nperformances.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:40:14 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Mazzini", "Davide", ""]]}, {"id": "1807.07473", "submitter": "Hoang-An Le", "authors": "Hoang-An Le, Anil S. Baslamisli, Thomas Mensink, Theo Gevers", "title": "Three for one and one for three: Flow, Segmentation, and Surface Normals", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical flow, semantic segmentation, and surface normals represent different\ninformation modalities, yet together they bring better cues for scene\nunderstanding problems. In this paper, we study the influence between the three\nmodalities: how one impacts on the others and their efficiency in combination.\nWe employ a modular approach using a convolutional refinement network which is\ntrained supervised but isolated from RGB images to enforce joint modality\nfeatures. To assist the training process, we create a large-scale synthetic\noutdoor dataset that supports dense annotation of semantic segmentation,\noptical flow, and surface normals. The experimental results show positive\ninfluence among the three modalities, especially for objects' boundaries,\nregion consistency, and scene structures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:54:21 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Le", "Hoang-An", ""], ["Baslamisli", "Anil S.", ""], ["Mensink", "Thomas", ""], ["Gevers", "Theo", ""]]}, {"id": "1807.07510", "submitter": "Yang Deng", "authors": "Yang Deng, Yao Sun, Yongpei Zhu, Mingwang Zhu, Wei Han, Kehong Yuan", "title": "A Strategy of MR Brain Tissue Images' Suggestive Annotation Based on\n  Modified U-Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of MR brain tissue is a crucial step for\ndiagnosis,surgical planning, and treatment of brain abnormalities. However,it\nis a time-consuming task to be performed by medical experts. So, automatic and\nreliable segmentation methods are required. How to choose appropriate training\ndataset from limited labeled dataset rather than the whole also has great\nsignificance in saving training time. In addition, medical data labeled is too\nrare and expensive to obtain extensively, so choosing appropriate unlabeled\ndataset instead of all the datasets to annotate, which can attain at least same\nperformance, is also very meaningful. To solve the problem above, we design an\nautomatic segmentation method based on U-shaped deep convolutional network and\nobtain excellent result with average DSC metric of 0.8610, 0.9131, 0.9003 for\nCerebrospinal Fluid (CSF), Gray Matter (GM) and White Matter (WM) respectively\non the well-known IBSR18 dataset. We use bootstrapping algorithm for selecting\nthe most effective training data and get more state-of-the-art segmentation\nperformance by using only 50% of training data. Moreover, we propose a strategy\nof MR brain tissue images' suggestive annotation for unlabeled medical data\nbased on the modified U-net. The proposed method performs fast and can be used\nin clinical.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:02:41 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 07:37:58 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 06:38:34 GMT"}, {"version": "v4", "created": "Sat, 28 Jul 2018 03:44:09 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Deng", "Yang", ""], ["Sun", "Yao", ""], ["Zhu", "Yongpei", ""], ["Zhu", "Mingwang", ""], ["Han", "Wei", ""], ["Yuan", "Kehong", ""]]}, {"id": "1807.07512", "submitter": "Torsten Sattler", "authors": "Federico Camposeco, Andrea Cohen, Marc Pollefeys and Torsten Sattler", "title": "Hybrid Scene Compression for Visual Localization", "comments": "Published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing an image wrt. a 3D scene model represents a core task for many\ncomputer vision applications. An increasing number of real-world applications\nof visual localization on mobile devices, e.g., Augmented Reality or autonomous\nrobots such as drones or self-driving cars, demand localization approaches to\nminimize storage and bandwidth requirements. Compressing the 3D models used for\nlocalization thus becomes a practical necessity. In this work, we introduce a\nnew hybrid compression algorithm that uses a given memory limit in a more\neffective way. Rather than treating all 3D points equally, it represents a\nsmall set of points with full appearance information and an additional, larger\nset of points with compressed information. This enables our approach to obtain\na more complete scene representation without increasing the memory\nrequirements, leading to a superior performance compared to previous\ncompression schemes. As part of our contribution, we show how to handle\nambiguous matches arising from point compression during RANSAC. Besides\noutperforming previous compression techniques in terms of pose accuracy under\nthe same memory constraints, our compression scheme itself is also more\nefficient. Furthermore, the localization rates and accuracy obtained with our\napproach are comparable to state-of-the-art feature-based methods, while using\na small fraction of the memory.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:04:58 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 09:20:05 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Camposeco", "Federico", ""], ["Cohen", "Andrea", ""], ["Pollefeys", "Marc", ""], ["Sattler", "Torsten", ""]]}, {"id": "1807.07521", "submitter": "Jo\\~ao Bernardino", "authors": "Jo\\~ao Bernardino (1), Lu\\'is Filipe Teixeira (1 and 2), Hugo Sereno\n  Ferreira (1 and 2) ((1) DEI - Faculty of Engineering - University of Porto,\n  (2) INESC TEC)", "title": "Bio-Measurements Estimation and Support in Knee Recovery through Machine\n  Learning", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knee injuries are frequent, varied and often require the patient to undergo\nintensive rehabilitation for several months. Treatment protocols usually\ncontemplate some recurrent measurements in order to assess progress, such as\ngoniometry. The need for specific equipment or the complexity and duration of\nthese tasks cause them to often be neglected. A novel deep learning based\nsolution is presented, supported by the generation of a synthetic image\ndataset. A 3D human-body model was used for this purpose, simulating a\nrecovering patient. For each image, the coordinates of three key points were\nregistered: the centers of the thigh, the knee and the lower leg. These values\nare sufficient to estimate the flexion angle. Convolutional neural networks\nwere then trained for predicting these six coordinates. Transfer learning was\nused with the VGG16 and InceptionV3 models pre-trained on the ImageNet dataset,\nbeing an additional custom model trained from scratch. All models were tested\nwith different combinations of data augmentation techniques applied on the\ntraining sets. InceptionV3 achieved the best overall results, producing\nconsiderably good predictions even on real unedited pictures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:24:22 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Bernardino", "Jo\u00e3o", "", "1 and 2"], ["Teixeira", "Lu\u00eds Filipe", "", "1 and 2"], ["Ferreira", "Hugo Sereno", "", "1 and 2"]]}, {"id": "1807.07532", "submitter": "Yuxing Tang", "authors": "Yuxing Tang, Xiaosong Wang, Adam P. Harrison, Le Lu, Jing Xiao, Ronald\n  M. Summers", "title": "Attention-Guided Curriculum Learning for Weakly Supervised\n  Classification and Localization of Thoracic Diseases on Chest Radiographs", "comments": "9th International Conference on Machine Learning in Medical Imaging\n  (MLMI 2018) In conjunction with MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we exploit the task of joint classification and weakly\nsupervised localization of thoracic diseases from chest radiographs, with only\nimage-level disease labels coupled with disease severity-level (DSL)\ninformation of a subset. A convolutional neural network (CNN) based\nattention-guided curriculum learning (AGCL) framework is presented, which\nleverages the severity-level attributes mined from radiology reports. Images in\norder of difficulty (grouped by different severity-levels) are fed to CNN to\nboost the learning gradually. In addition, highly confident samples (measured\nby classification probabilities) and their corresponding class-conditional\nheatmaps (generated by the CNN) are extracted and further fed into the AGCL\nframework to guide the learning of more distinctive convolutional features in\nthe next iteration. A two-path network architecture is designed to regress the\nheatmaps from selected seed samples in addition to the original classification\ntask. The joint learning scheme can improve the classification and localization\nperformance along with more seed samples for the next iteration. We demonstrate\nthe effectiveness of this iterative refinement framework via extensive\nexperimental evaluations on the publicly available ChestXray14 dataset. AGCL\nachieves over 5.7\\% (averaged over 14 diseases) increase in classification AUC\nand 7%/11% increases in Recall/Precision for the localization task compared to\nthe state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 16:54:16 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tang", "Yuxing", ""], ["Wang", "Xiaosong", ""], ["Harrison", "Adam P.", ""], ["Lu", "Le", ""], ["Xiao", "Jing", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1807.07556", "submitter": "Zukang Liao", "authors": "Yen Khye Lim, Zukang Liao, Stavros Petridis, Maja Pantic", "title": "Transfer Learning for Action Unit Recognition", "comments": "6 pages, Humanoids 2017 IEEE RAS International Conference workshop\n  Cooperative Autonomous Robot Experience (Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a classifier ensemble for Facial Expression Recognition\n(FER) based on models derived from transfer learning. The main experimentation\nwork is conducted for facial action unit detection using feature extraction and\nfine-tuning convolutional neural networks (CNNs). Several classifiers for\nextracted CNN codes such as Linear Discriminant Analysis (LDA), Support Vector\nMachines (SVMs) and Long Short-Term Memory (LSTM) are compared and evaluated.\nMulti-model ensembles are also used to further improve the performance. We have\nfound that VGG-Face and ResNet are the relatively optimal pre-trained models\nfor action unit recognition using feature extraction and the ensemble of\nVGG-Net variants and ResNet achieves the best result.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 17:54:08 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Lim", "Yen Khye", ""], ["Liao", "Zukang", ""], ["Petridis", "Stavros", ""], ["Pantic", "Maja", ""]]}, {"id": "1807.07559", "submitter": "Amelia Jim\\'enez-S\\'anchez", "authors": "Amelia Jim\\'enez-S\\'anchez, Shadi Albarqouni, Diana Mateus", "title": "Capsule Networks against Medical Imaging Data Challenges", "comments": "10 pages, 3 figures, accepted at MICCAI-LABELS 2018 Workshop", "journal-ref": "LABELS 2018, CVII 2018, STENT 2018: Intravascular Imaging and\n  Computer Assisted Stenting and Large-Scale Annotation of Biomedical Data and\n  Expert Label Synthesis pp 150-160", "doi": "10.1007/978-3-030-01364-6_17", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key component to the success of deep learning is the availability of\nmassive amounts of training data. Building and annotating large datasets for\nsolving medical image classification problems is today a bottleneck for many\napplications. Recently, capsule networks were proposed to deal with\nshortcomings of Convolutional Neural Networks (ConvNets). In this work, we\ncompare the behavior of capsule networks against ConvNets under typical\ndatasets constraints of medical image analysis, namely, small amounts of\nannotated data and class-imbalance. We evaluate our experiments on MNIST,\nFashion-MNIST and medical (histological and retina images) publicly available\ndatasets. Our results suggest that capsule networks can be trained with less\namount of data for the same or better performance and are more robust to an\nimbalanced class distribution, which makes our approach very promising for the\nmedical imaging community.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 17:56:37 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Jim\u00e9nez-S\u00e1nchez", "Amelia", ""], ["Albarqouni", "Shadi", ""], ["Mateus", "Diana", ""]]}, {"id": "1807.07560", "submitter": "Samaneh Azadi", "authors": "Samaneh Azadi, Deepak Pathak, Sayna Ebrahimi, Trevor Darrell", "title": "Compositional GAN: Learning Image-Conditional Binary Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) can produce images of remarkable\ncomplexity and realism but are generally structured to sample from a single\nlatent source ignoring the explicit spatial interaction between multiple\nentities that could be present in a scene. Capturing such complex interactions\nbetween different objects in the world, including their relative scaling,\nspatial layout, occlusion, or viewpoint transformation is a challenging\nproblem. In this work, we propose a novel self-consistent\nComposition-by-Decomposition (CoDe) network to compose a pair of objects. Given\nobject images from two distinct distributions, our model can generate a\nrealistic composite image from their joint distribution following the texture\nand shape of the input objects. We evaluate our approach through qualitative\nexperiments and user evaluations. Our results indicate that the learned model\ncaptures potential interactions between the two object domains, and generates\nrealistic composed scenes at test time.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 17:57:16 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 09:12:37 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 17:04:47 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Azadi", "Samaneh", ""], ["Pathak", "Deepak", ""], ["Ebrahimi", "Sayna", ""], ["Darrell", "Trevor", ""]]}, {"id": "1807.07569", "submitter": "Sungmin Cha", "authors": "Sungmin Cha, Taesup Moon", "title": "Fully Convolutional Pixel Adaptive Image Denoiser", "comments": "17 pages (including Supplementary Materials), ICCV 2019 camera ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new image denoising algorithm, dubbed as Fully Convolutional\nAdaptive Image DEnoiser (FC-AIDE), that can learn from an offline supervised\ntraining set with a fully convolutional neural network as well as adaptively\nfine-tune the supervised model for each given noisy image. We significantly\nextend the framework of the recently proposed Neural AIDE, which formulates the\ndenoiser to be context-based pixelwise mappings and utilizes the unbiased\nestimator of MSE for such denoisers. The two main contributions we make are; 1)\nimplementing a novel fully convolutional architecture that boosts the base\nsupervised model, and 2) introducing regularization methods for the adaptive\nfine-tuning such that a stronger and more robust adaptivity can be attained. As\na result, FC-AIDE is shown to possess many desirable features; it outperforms\nthe recent CNN-based state-of-the-art denoisers on all of the benchmark\ndatasets we tested, and gets particularly strong for various challenging\nscenarios, e.g., with mismatched image/noise characteristics or with scarce\nsupervised training data. The source code of our algorithm is available at\nhttps://github.com/csm9493/FC-AIDE-Keras.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 14:34:11 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 11:28:15 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 22:25:46 GMT"}, {"version": "v4", "created": "Sun, 27 Oct 2019 20:24:23 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Cha", "Sungmin", ""], ["Moon", "Taesup", ""]]}, {"id": "1807.07658", "submitter": "Jonas Bialopetravi\\v{c}ius", "authors": "J. Bialopetravi\\v{c}ius, D. Narbutis, V. Vansevi\\v{c}ius", "title": "Deriving star cluster parameters with convolutional neural networks. I.\n  Age, mass, and size", "comments": "11 pages, 12 figures", "journal-ref": "A&A 621, A103 (2019)", "doi": "10.1051/0004-6361/201833833", "report-no": null, "categories": "astro-ph.GA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. Convolutional neural networks (CNNs) have been proven to perform\nfast classification and detection on natural images and have potential to infer\nastrophysical parameters on the exponentially increasing amount of sky survey\nimaging data. The inference pipeline can be trained either from real\nhuman-annotated data or simulated mock observations. Until now star cluster\nanalysis was based on integral or individual resolved stellar photometry. This\nlimits the amount of information that can be extracted from cluster images.\n  Aims. Develop a CNN-based algorithm aimed to simultaneously derive ages,\nmasses, and sizes of star clusters directly from multi-band images. Demonstrate\nCNN capabilities on low mass semi-resolved star clusters in a low\nsignal-to-noise ratio regime.\n  Methods. A CNN was constructed based on the deep residual network (ResNet)\narchitecture and trained on simulated images of star clusters with various\nages, masses, and sizes. To provide realistic backgrounds, M31 star fields\ntaken from the PHAT survey were added to the mock cluster images.\n  Results. The proposed CNN was verified on mock images of artificial clusters\nand has demonstrated high precision and no significant bias for clusters of\nages $\\lesssim$3Gyr and masses between 250 and 4,000 ${\\rm M_\\odot}$. The\npipeline is end-to-end, starting from input images all the way to the inferred\nparameters; no hand-coded steps have to be performed: estimates of parameters\nare provided by the neural network in one inferential step from raw images.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 22:50:41 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 08:06:49 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Bialopetravi\u010dius", "J.", ""], ["Narbutis", "D.", ""], ["Vansevi\u010dius", "V.", ""]]}, {"id": "1807.07663", "submitter": "Aliasghar Mortazi", "authors": "Aliasghar Mortazi and Ulas Bagci", "title": "Automatically Designing CNN Architectures for Medical Image Segmentation", "comments": "Accepted to Machine Learning in Medical Imaging (MLMI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network architectures have traditionally been designed and\nexplored with human expertise in a long-lasting trial-and-error process. This\nprocess requires huge amount of time, expertise, and resources. To address this\ntedious problem, we propose a novel algorithm to optimally find hyperparameters\nof a deep network architecture automatically. We specifically focus on\ndesigning neural architectures for medical image segmentation task. Our\nproposed method is based on a policy gradient reinforcement learning for which\nthe reward function is assigned a segmentation evaluation utility (i.e., dice\nindex). We show the efficacy of the proposed method with its low computational\ncost in comparison with the state-of-the-art medical image segmentation\nnetworks. We also present a new architecture design, a densely connected\nencoder-decoder CNN, as a strong baseline architecture to apply the proposed\nhyperparameter search algorithm. We apply the proposed algorithm to each layer\nof the baseline architectures. As an application, we train the proposed system\non cine cardiac MR images from Automated Cardiac Diagnosis Challenge (ACDC)\nMICCAI 2017. Starting from a baseline segmentation architecture, the resulting\nnetwork architecture obtains the state-of-the-art results in accuracy without\nperforming any trial-and-error based architecture design approaches or close\nsupervision of the hyperparameters changes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 23:47:12 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Mortazi", "Aliasghar", ""], ["Bagci", "Ulas", ""]]}, {"id": "1807.07674", "submitter": "Jacob Richeimer", "authors": "Jacob Richeimer, Jonathan Mitchell", "title": "Bounding Box Embedding for Single Shot Person Instance Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a bottom-up approach for the task of object instance segmentation\nusing a single-shot model. The proposed model employs a fully convolutional\nnetwork which is trained to predict class-wise segmentation masks as well as\nthe bounding boxes of the object instances to which each pixel belongs. This\nallows us to group object pixels into individual instances. Our network\narchitecture is based on the DeepLabv3+ model, and requires only minimal extra\ncomputation to achieve pixel-wise instance assignments. We apply our method to\nthe task of person instance segmentation, a common task relevant to many\napplications. We train our model with COCO data and report competitive results\nfor the person class in the COCO instance segmentation task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 00:30:30 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Richeimer", "Jacob", ""], ["Mitchell", "Jonathan", ""]]}, {"id": "1807.07688", "submitter": "Bochao Wang", "authors": "Bochao Wang, Huabin Zheng, Xiaodan Liang, Yimin Chen, Liang Lin, Meng\n  Yang", "title": "Toward Characteristic-Preserving Image-based Virtual Try-On Network", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-based virtual try-on systems for fitting new in-shop clothes into a\nperson image have attracted increasing research attention, yet is still\nchallenging. A desirable pipeline should not only transform the target clothes\ninto the most fitting shape seamlessly but also preserve well the clothes\nidentity in the generated image, that is, the key characteristics (e.g.\ntexture, logo, embroidery) that depict the original clothes. However, previous\nimage-conditioned generation works fail to meet these critical requirements\ntowards the plausible virtual try-on performance since they fail to handle\nlarge spatial misalignment between the input image and target clothes. Prior\nwork explicitly tackled spatial deformation using shape context matching, but\nfailed to preserve clothing details due to its coarse-to-fine strategy. In this\nwork, we propose a new fully-learnable Characteristic-Preserving Virtual Try-On\nNetwork(CP-VTON) for addressing all real-world challenges in this task. First,\nCP-VTON learns a thin-plate spline transformation for transforming the in-shop\nclothes into fitting the body shape of the target person via a new Geometric\nMatching Module (GMM) rather than computing correspondences of interest points\nas prior works did. Second, to alleviate boundary artifacts of warped clothes\nand make the results more realistic, we employ a Try-On Module that learns a\ncomposition mask to integrate the warped clothes and the rendered image to\nensure smoothness. Extensive experiments on a fashion dataset demonstrate our\nCP-VTON achieves the state-of-the-art virtual try-on performance both\nqualitatively and quantitatively.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 01:42:58 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 11:35:01 GMT"}, {"version": "v3", "created": "Wed, 12 Sep 2018 08:19:42 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Wang", "Bochao", ""], ["Zheng", "Huabin", ""], ["Liang", "Xiaodan", ""], ["Chen", "Yimin", ""], ["Lin", "Liang", ""], ["Yang", "Meng", ""]]}, {"id": "1807.07696", "submitter": "Siyang Qin", "authors": "Siyang Qin, Jiahui Wei, Roberto Manduchi", "title": "Automatic Semantic Content Removal by Learning to Neglect", "comments": "Accepted to BMVC 2018 as an oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new system for automatic image content removal and inpainting.\nUnlike traditional inpainting algorithms, which require advance knowledge of\nthe region to be filled in, our system automatically detects the area to be\nremoved and infilled. Region segmentation and inpainting are performed jointly\nin a single pass. In this way, potential segmentation errors are more naturally\nalleviated by the inpainting module. The system is implemented as an\nencoder-decoder architecture, with two decoder branches, one tasked with\nsegmentation of the foreground region, the other with inpainting. The encoder\nand the two decoder branches are linked via neglect nodes, which guide the\ninpainting process in selecting which areas need reconstruction. The whole\nmodel is trained using a conditional GAN strategy. Comparative experiments show\nthat our algorithm outperforms state-of-the-art inpainting techniques (which,\nunlike our system, do not segment the input image and thus must be aided by an\nexternal segmentation module.)\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 02:17:33 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Qin", "Siyang", ""], ["Wei", "Jiahui", ""], ["Manduchi", "Roberto", ""]]}, {"id": "1807.07700", "submitter": "Hyunjung Shim Dr.", "authors": "Kyungjune Baek, Duhyeon Bang and Hyunjung Shim", "title": "Editable Generative Adversarial Networks: Generating and Editing Faces\n  Simultaneously", "comments": null, "journal-ref": null, "doi": null, "report-no": "Asian Conference on Computer Vision 2018 (Oral presentation)", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for simultaneously generating and manipulating\nthe face images with desired attributes. While the state-of-the-art attribute\nediting technique has achieved the impressive performance for creating\nrealistic attribute effects, they only address the image editing problem, using\nthe input image as the condition of model. Recently, several studies attempt to\ntackle both novel face generation and attribute editing problem using a single\nsolution. However, their image quality is still unsatisfactory. Our goal is to\ndevelop a single unified model that can simultaneously create and edit high\nquality face images with desired attributes. A key idea of our work is that we\ndecompose the image into the latent and attribute vector in low dimensional\nrepresentation, and then utilize the GAN framework for mapping the low\ndimensional representation to the image. In this way, we can address both the\ngeneration and editing problem by learning the generator. For qualitative and\nquantitative evaluations, the proposed algorithm outperforms recent algorithms\naddressing the same problem. Also, we show that our model can achieve the\ncompetitive performance with the state-of-the-art attribute editing technique\nin terms of attribute editing quality.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 03:13:16 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Baek", "Kyungjune", ""], ["Bang", "Duhyeon", ""], ["Shim", "Hyunjung", ""]]}, {"id": "1807.07701", "submitter": "Aydogan Ozcan", "authors": "Yair Rivenson, Tairan Liu, Zhensong Wei, Yibo Zhang, Aydogan Ozcan", "title": "PhaseStain: Digital staining of label-free quantitative phase microscopy\n  images using deep learning", "comments": null, "journal-ref": "Light: Science and Applications, 8 (2019)", "doi": "10.1038/s41377-019-0129-y", "report-no": null, "categories": "eess.IV cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a deep neural network, we demonstrate a digital staining technique,\nwhich we term PhaseStain, to transform quantitative phase images (QPI) of\nlabelfree tissue sections into images that are equivalent to brightfield\nmicroscopy images of the same samples that are histochemically stained. Through\npairs of image data (QPI and the corresponding brightfield images, acquired\nafter staining) we train a generative adversarial network (GAN) and demonstrate\nthe effectiveness of this virtual staining approach using sections of human\nskin, kidney and liver tissue, matching the brightfield microscopy images of\nthe same samples stained with Hematoxylin and Eosin, Jones' stain, and Masson's\ntrichrome stain, respectively. This digital staining framework might further\nstrengthen various uses of labelfree QPI techniques in pathology applications\nand biomedical research in general, by eliminating the need for chemical\nstaining, reducing sample preparation related costs and saving time. Our\nresults provide a powerful example of some of the unique opportunities created\nby data driven image transformations enabled by deep learning.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 03:17:13 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Rivenson", "Yair", ""], ["Liu", "Tairan", ""], ["Wei", "Zhensong", ""], ["Zhang", "Yibo", ""], ["Ozcan", "Aydogan", ""]]}, {"id": "1807.07716", "submitter": "Po-Yu Kao", "authors": "Po-Yu Kao and Thuyen Ngo and Angela Zhang and Jefferson W. Chen and\n  B.S. Manjunath", "title": "Brain Tumor Segmentation and Tractographic Feature Extraction from\n  Structural MR Images for Overall Survival Prediction", "comments": "14 pages, 5 figures, 4 tables, accepted by BrainLes 2018 MICCAI\n  workshop", "journal-ref": "4th International Workshop, BrainLes 2018, Held in Conjunction\n  with MICCAI 2018", "doi": "10.1007/978-3-030-11726-9_12", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a novel methodology to integrate human brain\nconnectomics and parcellation for brain tumor segmentation and survival\nprediction. For segmentation, we utilize an existing brain parcellation atlas\nin the MNI152 1mm space and map this parcellation to each individual subject\ndata. We use deep neural network architectures together with hard negative\nmining to achieve the final voxel level classification. For survival\nprediction, we present a new method for combining features from connectomics\ndata, brain parcellation information, and the brain tumor mask. We leverage the\naverage connectome information from the Human Connectome Project and map each\nsubject brain volume onto this common connectome space. From this, we compute\ntractographic features that describe potential neural disruptions due to the\nbrain tumor. These features are then used to predict the overall survival of\nthe subjects. The main novelty in the proposed methods is the use of normalized\nbrain parcellation data and tractography data from the human connectome project\nfor analyzing MR images for segmentation and survival prediction. Experimental\nresults are reported on the BraTS2018 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 07:10:24 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 04:09:24 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 23:20:28 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kao", "Po-Yu", ""], ["Ngo", "Thuyen", ""], ["Zhang", "Angela", ""], ["Chen", "Jefferson W.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1807.07718", "submitter": "Andrey Savchenko", "authors": "Andrey V. Savchenko", "title": "Efficient Facial Representations for Age, Gender and Identity\n  Recognition in Organizing Photo Albums using Multi-output CNN", "comments": "19 pages, 2 figures, 8 tables", "journal-ref": "PeerJ Computer Science 5:e197 (2019)", "doi": "10.7717/peerj-cs.197", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is focused on the automatic extraction of persons and their\nattributes (gender, year of born) from album of photos and videos. We propose\nthe two-stage approach, in which, firstly, the convolutional neural network\nsimultaneously predicts age/gender from all photos and additionally extracts\nfacial representations suitable for face identification. We modified the\nMobileNet, which is preliminarily trained to perform face recognition, in order\nto additionally recognize age and gender. In the second stage of our approach,\nextracted faces are grouped using hierarchical agglomerative clustering\ntechniques. The born year and gender of a person in each cluster are estimated\nusing aggregation of predictions for individual photos. We experimentally\ndemonstrated that our facial clustering quality is competitive with the\nstate-of-the-art neural networks, though our implementation is much\ncomputationally cheaper. Moreover, our approach is characterized by more\naccurate video-based age/gender recognition when compared to the publicly\navailable models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 07:12:36 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 13:28:20 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 07:59:42 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Savchenko", "Andrey V.", ""]]}, {"id": "1807.07760", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin and Byron Boots", "title": "Improving Image Clustering With Multiple Pretrained CNN Feature\n  Extractors", "comments": "13 pages, 3 figures, 4 tables. Poster presentation at BMVC 2018\n  (29.9% acceptance)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many image clustering problems, replacing raw image data with features\nextracted by a pretrained convolutional neural network (CNN), leads to better\nclustering performance. However, the specific features extracted, and, by\nextension, the selected CNN architecture, can have a major impact on the\nclustering results. In practice, this crucial design choice is often decided\narbitrarily due to the impossibility of using cross-validation with\nunsupervised learning problems. However, information contained in the different\npretrained CNN architectures may be complementary, even when pretrained on the\nsame data. To improve clustering performance, we rephrase the image clustering\nproblem as a multi-view clustering (MVC) problem that considers multiple\ndifferent pretrained feature extractors as different \"views\" of the same data.\nWe then propose a multi-input neural network architecture that is trained\nend-to-end to solve the MVC problem effectively. Our experimental results,\nconducted on three different natural image datasets, show that: 1. using\nmultiple pretrained CNNs jointly as feature extractors improves image\nclustering; 2. using an end-to-end approach improves MVC; and 3. combining both\nproduces state-of-the-art results for the problem of image clustering.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 09:46:55 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Boots", "Byron", ""]]}, {"id": "1807.07769", "submitter": "Kevin Eykholt", "authors": "Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,\n  Florian Tramer, Atul Prakash, Tadayoshi Kohno, Dawn Song", "title": "Physical Adversarial Examples for Object Detectors", "comments": "This paper is the extended version of the USENIX WOOT 2018 version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial\nexamples-maliciously crafted inputs that cause DNNs to make incorrect\npredictions. Recent work has shown that these attacks generalize to the\nphysical domain, to create perturbations on physical objects that fool image\nclassifiers under a variety of real-world conditions. Such attacks pose a risk\nto deep learning models used in safety-critical cyber-physical systems. In this\nwork, we extend physical attacks to more challenging object detection models, a\nbroader class of deep learning algorithms widely used to detect and label\nmultiple objects within a scene. Improving upon a previous physical attack on\nimage classifiers, we create perturbed physical objects that are either ignored\nor mislabeled by object detection models. We implement a Disappearance Attack,\nin which we cause a Stop sign to \"disappear\" according to the detector-either\nby covering thesign with an adversarial Stop sign poster, or by adding\nadversarial stickers onto the sign. In a video recorded in a controlled lab\nenvironment, the state-of-the-art YOLOv2 detector failed to recognize these\nadversarial Stop signs in over 85% of the video frames. In an outdoor\nexperiment, YOLO was fooled by the poster and sticker attacks in 72.5% and\n63.5% of the video frames respectively. We also use Faster R-CNN, a different\nobject detection model, to demonstrate the transferability of our adversarial\nperturbations. The created poster perturbation is able to fool Faster R-CNN in\n85.9% of the video frames in a controlled lab environment, and 40.2% of the\nvideo frames in an outdoor environment. Finally, we present preliminary results\nwith a new Creation Attack, where in innocuous physical stickers fool a model\ninto detecting nonexistent objects.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 10:14:27 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 18:07:23 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Eykholt", "Kevin", ""], ["Evtimov", "Ivan", ""], ["Fernandes", "Earlence", ""], ["Li", "Bo", ""], ["Rahmati", "Amir", ""], ["Tramer", "Florian", ""], ["Prakash", "Atul", ""], ["Kohno", "Tadayoshi", ""], ["Song", "Dawn", ""]]}, {"id": "1807.07778", "submitter": "Dongyang Ao", "authors": "Dongyang Ao, Corneliu Octavian Dumitru, Gottfried Schwarz and Mihai\n  Datcu", "title": "Dialectical GAN for SAR Image Translation: From Sentinel-1 to TerraSAR-X", "comments": "22 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrary to optical images, Synthetic Aperture Radar (SAR) images are in\ndifferent electromagnetic spectrum where the human visual system is not\naccustomed to. Thus, with more and more SAR applications, the demand for\nenhanced high-quality SAR images has increased considerably. However,\nhigh-quality SAR images entail high costs due to the limitations of current SAR\ndevices and their image processing resources. To improve the quality of SAR\nimages and to reduce the costs of their generation, we propose a Dialectical\nGenerative Adversarial Network (Dialectical GAN) to generate high-quality SAR\nimages. This method is based on the analysis of hierarchical SAR information\nand the \"dialectical\" structure of GAN frameworks. As a demonstration, a\ntypical example will be shown where a low-resolution SAR image (e.g., a\nSentinel-1 image) with large ground coverage is translated into a\nhigh-resolution SAR image (e.g., a TerraSAR-X image). Three traditional\nalgorithms are compared, and a new algorithm is proposed based on a network\nframework by combining conditional WGAN-GP (Wasserstein Generative Adversarial\nNetwork - Gradient Penalty) loss functions and Spatial Gram matrices under the\nrule of dialectics. Experimental results show that the SAR image translation\nworks very well when we compare the results of our proposed method with the\nselected traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 10:26:32 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Ao", "Dongyang", ""], ["Dumitru", "Corneliu Octavian", ""], ["Schwarz", "Gottfried", ""], ["Datcu", "Mihai", ""]]}, {"id": "1807.07784", "submitter": "Gabriel Maicas", "authors": "Gabriel Maicas, Gerard Snaauw, Andrew P. Bradley, Ian Reid, Gustavo\n  Carneiro", "title": "Model Agnostic Saliency for Weakly Supervised Lesion Detection from\n  Breast DCE-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a heated debate on how to interpret the decisions provided by deep\nlearning models (DLM), where the main approaches rely on the visualization of\nsalient regions to interpret the DLM classification process. However, these\napproaches generally fail to satisfy three conditions for the problem of lesion\ndetection from medical images: 1) for images with lesions, all salient regions\nshould represent lesions, 2) for images containing no lesions, no salient\nregion should be produced,and 3) lesions are generally small with relatively\nsmooth borders. We propose a new model-agnostic paradigm to interpret DLM\nclassification decisions supported by a novel definition of saliency that\nincorporates the conditions above. Our model-agnostic 1-class saliency detector\n(MASD) is tested on weakly supervised breast lesion detection from DCE-MRI,\nachieving state-of-the-art detection accuracy when compared to current\nvisualization methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 10:48:18 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 23:06:52 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 05:22:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Maicas", "Gabriel", ""], ["Snaauw", "Gerard", ""], ["Bradley", "Andrew P.", ""], ["Reid", "Ian", ""], ["Carneiro", "Gustavo", ""]]}, {"id": "1807.07796", "submitter": "K L Navaneet", "authors": "Priyanka Mandikal, K L Navaneet, Mayank Agarwal, R. Venkatesh Babu", "title": "3D-LMNet: Latent Embedding Matching for Accurate and Diverse 3D Point\n  Cloud Reconstruction from a Single Image", "comments": "Accepted at BMVC 2018; Codes are available at\n  https://github.com/val-iisc/3d-lmnet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction from single view images is an ill-posed problem. Inferring\nthe hidden regions from self-occluded images is both challenging and ambiguous.\nWe propose a two-pronged approach to address these issues. To better\nincorporate the data prior and generate meaningful reconstructions, we propose\n3D-LMNet, a latent embedding matching approach for 3D reconstruction. We first\ntrain a 3D point cloud auto-encoder and then learn a mapping from the 2D image\nto the corresponding learnt embedding. To tackle the issue of uncertainty in\nthe reconstruction, we predict multiple reconstructions that are consistent\nwith the input view. This is achieved by learning a probablistic latent space\nwith a novel view-specific diversity loss. Thorough quantitative and\nqualitative analysis is performed to highlight the significance of the proposed\napproach. We outperform state-of-the-art approaches on the task of single-view\n3D reconstruction on both real and synthetic datasets while generating multiple\nplausible reconstructions, demonstrating the generalizability and utility of\nour approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 11:32:02 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 06:49:01 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Mandikal", "Priyanka", ""], ["Navaneet", "K L", ""], ["Agarwal", "Mayank", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1807.07803", "submitter": "Santiago Estrada", "authors": "Santiago Estrada, Sailesh Conjeti, Muneer Ahmad, Nassir Navab and\n  Martin Reuter", "title": "Competition vs. Concatenation in Skip Connections of Fully Convolutional\n  Networks", "comments": "Paper accepted on MICCAI-MLMI 2018 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increased information sharing through short and long-range skip connections\nbetween layers in fully convolutional networks have demonstrated significant\nimprovement in performance for semantic segmentation. In this paper, we propose\nCompetitive Dense Fully Convolutional Networks (CDFNet) by introducing\ncompetitive maxout activations in place of naive feature concatenation for\ninducing competition amongst layers. Within CDFNet, we propose two\narchitectural contributions, namely competitive dense block (CDB) and\ncompetitive unpooling block (CUB) to induce competition at local and global\nscales for short and long-range skip connections respectively. This extension\nis demonstrated to boost learning of specialized sub-networks targeted at\nsegmenting specific anatomies, which in turn eases the training of complex\ntasks. We present the proof-of-concept on the challenging task of whole body\nsegmentation in the publicly available VISCERAL benchmark and demonstrate\nimproved performance over multiple learning and registration based\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 12:06:06 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Estrada", "Santiago", ""], ["Conjeti", "Sailesh", ""], ["Ahmad", "Muneer", ""], ["Navab", "Nassir", ""], ["Reuter", "Martin", ""]]}, {"id": "1807.07837", "submitter": "Yang Liu", "authors": "Yang Liu (1), Xin Yuan (1), Jinli Suo, David J. Brady, and Qionghai\n  Dai ((1) Equal contributions)", "title": "Rank Minimization for Snapshot Compressive Imaging", "comments": "18 pages, 21 figures, and 2 tables. Code available at\n  https://github.com/liuyang12/DeSCI", "journal-ref": null, "doi": "10.1109/TPAMI.2018.2873587", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snapshot compressive imaging (SCI) refers to compressive imaging systems\nwhere multiple frames are mapped into a single measurement, with video\ncompressive imaging and hyperspectral compressive imaging as two representative\napplications. Though exciting results of high-speed videos and hyperspectral\nimages have been demonstrated, the poor reconstruction quality precludes SCI\nfrom wide applications.This paper aims to boost the reconstruction quality of\nSCI via exploiting the high-dimensional structure in the desired signal. We\nbuild a joint model to integrate the nonlocal self-similarity of\nvideo/hyperspectral frames and the rank minimization approach with the SCI\nsensing process. Following this, an alternating minimization algorithm is\ndeveloped to solve this non-convex problem. We further investigate the special\nstructure of the sampling process in SCI to tackle the computational workload\nand memory issues in SCI reconstruction. Both simulation and real data\n(captured by four different SCI cameras) results demonstrate that our proposed\nalgorithm leads to significant improvements compared with current\nstate-of-the-art algorithms. We hope our results will encourage the researchers\nand engineers to pursue further in compressive imaging for real applications.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 13:44:37 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Liu", "Yang", "", "Equal contributions"], ["Yuan", "Xin", "", "Equal contributions"], ["Suo", "Jinli", ""], ["Brady", "David J.", ""], ["Dai", "Qionghai", ""]]}, {"id": "1807.07853", "submitter": "Constantinos Loukas", "authors": "Constantinos Loukas", "title": "Surgical Phase Recognition of Short Video Shots Based on Temporal\n  Modeling of Deep Features", "comments": "6 pages, 4 figures, 6 tables", "journal-ref": null, "doi": "10.5220/0007352000210029", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing the phases of a laparoscopic surgery (LS) operation form its\nvideo constitutes a fundamental step for efficient content representation,\nindexing and retrieval in surgical video databases. In the literature, most\ntechniques focus on phase segmentation of the entire LS video using\nhand-crafted visual features, instrument usage signals, and recently\nconvolutional neural networks (CNNs). In this paper we address the problem of\nphase recognition of short video shots (10s) of the operation, without\nutilizing information about the preceding/forthcoming video frames, their phase\nlabels or the instruments used. We investigate four state-of-the-art CNN\narchitectures (Alexnet, VGG19, GoogleNet, and ResNet101), for feature\nextraction via transfer learning. Visual saliency was employed for selecting\nthe most informative region of the image as input to the CNN. Video shot\nrepresentation was based on two temporal pooling mechanisms. Most importantly,\nwe investigate the role of 'elapsed time' (from the beginning of the\noperation), and we show that inclusion of this feature can increase performance\ndramatically (69% vs. 75% mean accuracy). Finally, a long short-term memory\n(LSTM) network was trained for video shot classification based on the fusion of\nCNN features with 'elapsed time', increasing the accuracy to 86%. Our results\nhighlight the prominent role of visual saliency, long-range temporal recursion\nand 'elapsed time' (a feature so far ignored), for surgical phase recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 14:10:32 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 11:28:15 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 15:22:49 GMT"}, {"version": "v4", "created": "Fri, 7 Dec 2018 08:00:17 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Loukas", "Constantinos", ""]]}, {"id": "1807.07860", "submitter": "Ziwei Liu", "authors": "Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, Xiaogang Wang", "title": "Talking Face Generation by Adversarially Disentangled Audio-Visual\n  Representation", "comments": "AAAI Conference on Artificial Intelligence (AAAI 2019) Oral\n  Presentation. Code, models, and video results are available on our webpage:\n  https://liuziwei7.github.io/projects/TalkingFace.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Talking face generation aims to synthesize a sequence of face images that\ncorrespond to a clip of speech. This is a challenging task because face\nappearance variation and semantics of speech are coupled together in the subtle\nmovements of the talking face regions. Existing works either construct specific\nface appearance model on specific subjects or model the transformation between\nlip motion and speech. In this work, we integrate both aspects and enable\narbitrary-subject talking face generation by learning disentangled audio-visual\nrepresentation. We find that the talking face sequence is actually a\ncomposition of both subject-related information and speech-related information.\nThese two spaces are then explicitly disentangled through a novel\nassociative-and-adversarial training process. This disentangled representation\nhas an advantage where both audio and video can serve as inputs for generation.\nExtensive experiments show that the proposed approach generates realistic\ntalking face sequences on arbitrary subjects with much clearer lip motion\npatterns than previous work. We also demonstrate the learned audio-visual\nrepresentation is extremely useful for the tasks of automatic lip reading and\naudio-video retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 14:26:32 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 16:40:06 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Zhou", "Hang", ""], ["Liu", "Yu", ""], ["Liu", "Ziwei", ""], ["Luo", "Ping", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1807.07872", "submitter": "Daniel C. Castro", "authors": "Daniel C. Castro, Sebastian Nowozin", "title": "From Face Recognition to Models of Identity: A Bayesian Approach to\n  Learning about Unknown Identities from Unsupervised Data", "comments": "Accepted for publication at ECCV 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01216-8_46", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current face recognition systems robustly recognize identities across a wide\nvariety of imaging conditions. In these systems recognition is performed via\nclassification into known identities obtained from supervised identity\nannotations. There are two problems with this current paradigm: (1) current\nsystems are unable to benefit from unlabelled data which may be available in\nlarge quantities; and (2) current systems equate successful recognition with\nlabelling a given input image. Humans, on the other hand, regularly perform\nidentification of individuals completely unsupervised, recognising the identity\nof someone they have seen before even without being able to name that\nindividual. How can we go beyond the current classification paradigm towards a\nmore human understanding of identities? We propose an integrated Bayesian model\nthat coherently reasons about the observed images, identities, partial\nknowledge about names, and the situational context of each observation. While\nour model achieves good recognition performance against known identities, it\ncan also discover new identities from unsupervised data and learns to associate\nidentities with different contexts depending on which identities tend to be\nobserved together. In addition, the proposed semi-supervised component is able\nto handle not only acquaintances, whose names are known, but also unlabelled\nfamiliar faces and complete strangers in a unified framework.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 14:40:10 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Castro", "Daniel C.", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1807.07928", "submitter": "Vivienne Sze", "authors": "Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, Vivienne Sze", "title": "Eyeriss v2: A Flexible Accelerator for Emerging Deep Neural Networks on\n  Mobile Devices", "comments": "accepted for publication in IEEE Journal on Emerging and Selected\n  Topics in Circuits and Systems. This extended version on arXiv also includes\n  Eyexam in the appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent trend in DNN development is to extend the reach of deep learning\napplications to platforms that are more resource and energy constrained, e.g.,\nmobile devices. These endeavors aim to reduce the DNN model size and improve\nthe hardware processing efficiency, and have resulted in DNNs that are much\nmore compact in their structures and/or have high data sparsity. These compact\nor sparse models are different from the traditional large ones in that there is\nmuch more variation in their layer shapes and sizes, and often require\nspecialized hardware to exploit sparsity for performance improvement. Thus,\nmany DNN accelerators designed for large DNNs do not perform well on these\nmodels. In this work, we present Eyeriss v2, a DNN accelerator architecture\ndesigned for running compact and sparse DNNs. To deal with the widely varying\nlayer shapes and sizes, it introduces a highly flexible on-chip network, called\nhierarchical mesh, that can adapt to the different amounts of data reuse and\nbandwidth requirements of different data types, which improves the utilization\nof the computation resources. Furthermore, Eyeriss v2 can process sparse data\ndirectly in the compressed domain for both weights and activations, and\ntherefore is able to improve both processing speed and energy efficiency with\nsparse models. Overall, with sparse MobileNet, Eyeriss v2 in a 65nm CMOS\nprocess achieves a throughput of 1470.6 inferences/sec and 2560.3 inferences/J\nat a batch size of 1, which is 12.6x faster and 2.5x more energy efficient than\nthe original Eyeriss running MobileNet. We also present an analysis methodology\ncalled Eyexam that provides a systematic way of understanding the performance\nlimits for DNN processors as a function of specific characteristics of the DNN\nmodel and accelerator design; it applies these characteristics as sequential\nsteps to increasingly tighten the bound on the performance limits.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 16:12:07 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 19:56:26 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chen", "Yu-Hsin", ""], ["Yang", "Tien-Ju", ""], ["Emer", "Joel", ""], ["Sze", "Vivienne", ""]]}, {"id": "1807.07930", "submitter": "Eduardo P\\'erez-Pellitero", "authors": "Eduardo P\\'erez-Pellitero and Mehdi S. M. Sajjadi and Michael Hirsch\n  and Bernhard Sch\\\"olkopf", "title": "Perceptual Video Super Resolution with Enhanced Temporal Consistency", "comments": "Major revision and improvement of the manuscript: New network\n  architecture, new loss function and extended experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of perceptual loss functions, new possibilities in\nsuper-resolution have emerged, and we currently have models that successfully\ngenerate near-photorealistic high-resolution images from their low-resolution\nobservations. Up to now, however, such approaches have been exclusively limited\nto single image super-resolution. The application of perceptual loss functions\non video processing still entails several challenges, mostly related to the\nlack of temporal consistency of the generated images, i.e., flickering\nartifacts. In this work, we present a novel adversarial recurrent network for\nvideo upscaling that is able to produce realistic textures in a temporally\nconsistent way. The proposed architecture naturally leverages information from\nprevious frames due to its recurrent architecture, i.e. the input to the\ngenerator is composed of the low-resolution image and, additionally, the warped\noutput of the network at the previous step. Together with a video\ndiscriminator, we also propose additional loss functions to further reinforce\ntemporal consistency in the generated sequences. The experimental validation of\nour algorithm shows the effectiveness of our approach which obtains images with\nhigh perceptual quality and improved temporal consistency.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 16:39:15 GMT"}, {"version": "v2", "created": "Thu, 2 May 2019 16:59:15 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["P\u00e9rez-Pellitero", "Eduardo", ""], ["Sajjadi", "Mehdi S. M.", ""], ["Hirsch", "Michael", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1807.07939", "submitter": "Karel Lenc", "authors": "Karel Lenc and Andrea Vedaldi", "title": "Large scale evaluation of local image feature detectors on homography\n  datasets", "comments": "Accepted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large scale benchmark for the evaluation of local feature\ndetectors. Our key innovation is the introduction of a new evaluation protocol\nwhich extends and improves the standard detection repeatability measure. The\nnew protocol is better for assessment on a large number of images and reduces\nthe dependency of the results on unwanted distractors such as the number of\ndetected features and the feature magnification factor. Additionally, our\nprotocol provides a comprehensive assessment of the expected performance of\ndetectors under several practical scenarios. Using images from the\nrecently-introduced HPatches dataset, we evaluate a range of state-of-the-art\nlocal feature detectors on two main tasks: viewpoint and illumination invariant\ndetection. Contrary to previous detector evaluations, our study contains an\norder of magnitude more image sequences, resulting in a quantitative evaluation\nsignificantly more robust to over-fitting. We also show that traditional\ndetectors are still very competitive when compared to recent deep-learning\nalternatives.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 16:54:27 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Lenc", "Karel", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1807.07946", "submitter": "Seyed Shahabeddin Nabavi", "authors": "Seyed shahabeddin Nabavi, Mrigank Rochan, Yang, Wang", "title": "Future Semantic Segmentation with Convolutional LSTM", "comments": "Accepted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting semantic segmentation of future frames\nin a video. Given several observed frames in a video, our goal is to predict\nthe semantic segmentation map of future frames that are not yet observed. A\nreliable solution to this problem is useful in many applications that require\nreal-time decision making, such as autonomous driving. We propose a novel model\nthat uses convolutional LSTM (ConvLSTM) to encode the spatiotemporal\ninformation of observed frames for future prediction. We also extend our model\nto use bidirectional ConvLSTM to capture temporal information in both\ndirections. Our proposed approach outperforms other state-of-the-art methods on\nthe benchmark dataset.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 17:31:06 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Nabavi", "Seyed shahabeddin", ""], ["Rochan", "Mrigank", ""], ["Yang", "", ""], ["Wang", "", ""]]}, {"id": "1807.07948", "submitter": "Zhezhi He", "authors": "Zhezhi He, Boqing Gong and Deliang Fan", "title": "Optimize Deep Convolutional Neural Network with Ternarized Weights and\n  High Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolution neural network has achieved great success in many artificial\nintelligence applications. However, its enormous model size and massive\ncomputation cost have become the main obstacle for deployment of such powerful\nalgorithm in the low power and resource-limited embedded systems. As the\ncountermeasure to this problem, in this work, we propose statistical weight\nscaling and residual expansion methods to reduce the bit-width of the whole\nnetwork weight parameters to ternary values (i.e. -1, 0, +1), with the\nobjectives to greatly reduce model size, computation cost and accuracy\ndegradation caused by the model compression. With about 16x model compression\nrate, our ternarized ResNet-32/44/56 could outperform full-precision\ncounterparts by 0.12%, 0.24% and 0.18% on CIFAR- 10 dataset. We also test our\nternarization method with AlexNet and ResNet-18 on ImageNet dataset, which both\nachieve the best top-1 accuracy compared to recent similar works, with the same\n16x compression rate. If further incorporating our residual expansion method,\ncompared to the full-precision counterpart, our ternarized ResNet-18 even\nimproves the top-5 accuracy by 0.61% and merely degrades the top-1 accuracy\nonly by 0.42% for the ImageNet dataset, with 8x model compression rate. It\noutperforms the recent ABC-Net by 1.03% in top-1 accuracy and 1.78% in top-5\naccuracy, with around 1.25x higher compression rate and more than 6x\ncomputation reduction due to the weight sparsity.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 17:36:05 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["He", "Zhezhi", ""], ["Gong", "Boqing", ""], ["Fan", "Deliang", ""]]}, {"id": "1807.07960", "submitter": "Aparna John", "authors": "Artyom M. Grigoryan, Aparna John, Sos S. Agaian", "title": "Alpha-rooting color image enhancement method by two-side 2-D quaternion\n  discrete Fourier transform followed by spatial transformation", "comments": "21 pages", "journal-ref": "International Journal of Applied Control, Electrical and\n  Electronics Engineering (IJACEEE) Vol 6, No. 1, February 2018", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a quaternion approach of enhancement method is proposed in\nwhich color in the image is considered as a single entity. This new method is\nreferred as the alpha-rooting method of color image enhancement by the\ntwo-dimensional quaternion discrete Fourier transform (2-D QDFT) followed by a\nspatial transformation. The results of the proposed color image enhancement\nmethod are compared with its counterpart channel-by-channel enhancement\nalgorithm by the 2-D DFT. The image enhancements are quantified to the\nenhancement measure that is based on visual perception referred as the color\nenhancement measure estimation (CEME). The preliminary experiment results show\nthat the quaternion approach of image enhancement is an effective color image\nenhancement technique.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 03:17:43 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Grigoryan", "Artyom M.", ""], ["John", "Aparna", ""], ["Agaian", "Sos S.", ""]]}, {"id": "1807.07962", "submitter": "Aparna John", "authors": "Artyom M Grigoryan, Aparna John, Sos S Agaian", "title": "A Novel Color Image Enhancement Method by the Transformation of Color\n  Images to 2-D Grayscale Images", "comments": "18 pages", "journal-ref": "Int J Signal Process Anal 2017, 2:002", "doi": null, "report-no": null, "categories": "eess.IV cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method of color image enhancement is proposed, in which three or four\ncolor channels of the image are transformed to one channel 2-D grayscale image.\nThis paper describes different models of such transformations in the RGB and\nother color models. Color image enhancement is achieved by enhancing first the\ntransformed grayscale image and, then, transforming back the grayscale image\ninto the colors. The color image enhancement is done on the transformed 2-D\ngrayscale image rather than on the color image. New algorithms of color image\nenhancement are described in both frequency and time domains. The enhancement\nby this novel method shows good results. The enhancement of the image is\nmeasured with respect to the metric referred to as the Color Enhancement\nMeasure Estimation (CEME).\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 04:18:21 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Grigoryan", "Artyom M", ""], ["John", "Aparna", ""], ["Agaian", "Sos S", ""]]}, {"id": "1807.07965", "submitter": "Arindam Chowdhury", "authors": "Arindam Chowdhury and Lovekesh Vig", "title": "An Efficient End-to-End Neural Model for Handwritten Text Recognition", "comments": "Accepted at British Machine Vision Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline handwritten text recognition from images is an important problem for\nenterprises attempting to digitize large volumes of handmarked scanned\ndocuments/reports. Deep recurrent models such as Multi-dimensional LSTMs have\nbeen shown to yield superior performance over traditional Hidden Markov Model\nbased approaches that suffer from the Markov assumption and therefore lack the\nrepresentational power of RNNs. In this paper we introduce a novel approach\nthat combines a deep convolutional network with a recurrent Encoder-Decoder\nnetwork to map an image to a sequence of characters corresponding to the text\npresent in the image. The entire model is trained end-to-end using Focal Loss,\nan improvement over the standard Cross-Entropy loss that addresses the class\nimbalance problem, inherent to text recognition. To enhance the decoding\ncapacity of the model, Beam Search algorithm is employed which searches for the\nbest sequence out of a set of hypotheses based on a joint distribution of\nindividual characters. Our model takes as input a downsampled version of the\noriginal image thereby making it both computationally and memory efficient. The\nexperimental results were benchmarked against two publicly available datasets,\nIAM and RIMES. We surpass the state-of-the-art word level accuracy on the\nevaluation set of both datasets by 3.5% & 1.1%, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 09:55:09 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 13:31:24 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Chowdhury", "Arindam", ""], ["Vig", "Lovekesh", ""]]}, {"id": "1807.08008", "submitter": "Loris Nanni", "authors": "Loris Nanni, Alessandra Lumini, Stefano Ghidoni", "title": "Ensemble of Deep Learned Features for Melanoma Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is to propose an ensemble of descriptors for Melanoma\nClassification, whose performance has been evaluated on validation and test\ndatasets of the melanoma challenge 2018. The system proposed here achieves a\nstrong discriminative power thanks to the combination of multiple descriptors.\nThe proposed system represents a very simple yet effective way of boosting the\nperformance of trained CNNs by composing multiple CNNs into an ensemble and\ncombining scores by sum rule. Several types of ensembles are considered, with\ndifferent CNN architectures along with different learning parameter sets.\nMoreover CNN are used as feature extractors: an input image is processed by a\ntrained CNN and the response of a particular layer (usually the classification\nlayer, but also internal layers can be employed) is treated as a descriptor for\nthe image and used for training a set of Support Vector Machines (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 19:25:22 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Nanni", "Loris", ""], ["Lumini", "Alessandra", ""], ["Ghidoni", "Stefano", ""]]}, {"id": "1807.08024", "submitter": "Chun-Hao Chang", "authors": "Chun-Hao Chang, Elliot Creager, Anna Goldenberg, David Duvenaud", "title": "Explaining Image Classifiers by Counterfactual Generation", "comments": "ICLR 2019 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an image classifier makes a prediction, which parts of the image are\nrelevant and why? We can rephrase this question to ask: which parts of the\nimage, if they were not seen by the classifier, would most change its decision?\nProducing an answer requires marginalizing over images that could have been\nseen but weren't. We can sample plausible image in-fills by conditioning a\ngenerative model on the rest of the image. We then optimize to find the image\nregions that most change the classifier's decision after in-fill. Our approach\ncontrasts with ad-hoc in-filling approaches, such as blurring or injecting\nnoise, which generate inputs far from the data distribution, and ignore\ninformative relationships between different parts of the image. Our method\nproduces more compact and relevant saliency maps, with fewer artifacts compared\nto previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 20:48:44 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 01:10:35 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 16:22:07 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Chang", "Chun-Hao", ""], ["Creager", "Elliot", ""], ["Goldenberg", "Anna", ""], ["Duvenaud", "David", ""]]}, {"id": "1807.08069", "submitter": "Da Zhang", "authors": "Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang", "title": "S3D: Single Shot multi-Span Detector via Fully 3D Convolutional Networks", "comments": "BMVC 2018 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel Single Shot multi-Span Detector for\ntemporal activity detection in long, untrimmed videos using a simple end-to-end\nfully three-dimensional convolutional (Conv3D) network. Our architecture, named\nS3D, encodes the entire video stream and discretizes the output space of\ntemporal activity spans into a set of default spans over different temporal\nlocations and scales. At prediction time, S3D predicts scores for the presence\nof activity categories in each default span and produces temporal adjustments\nrelative to the span location to predict the precise activity duration. Unlike\nmany state-of-the-art systems that require a separate proposal and\nclassification stage, our S3D is intrinsically simple and dedicatedly designed\nfor single-shot, end-to-end temporal activity detection. When evaluating on\nTHUMOS'14 detection benchmark, S3D achieves state-of-the-art performance and is\nvery efficient and can operate at 1271 FPS.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 02:34:57 GMT"}, {"version": "v2", "created": "Tue, 7 Aug 2018 18:33:06 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Zhang", "Da", ""], ["Dai", "Xiyang", ""], ["Wang", "Xin", ""], ["Wang", "Yuan-Fang", ""]]}, {"id": "1807.08093", "submitter": "Eric Wu", "authors": "Eric Wu, Kevin Wu, David Cox, William Lotter", "title": "Conditional Infilling GANs for Data Augmentation in Mammogram\n  Classification", "comments": "To appear in MICCAI 2018, Breast Image Analysis Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning approaches to breast cancer detection in mammograms have\nrecently shown promising results. However, such models are constrained by the\nlimited size of publicly available mammography datasets, in large part due to\nprivacy concerns and the high cost of generating expert annotations. Limited\ndataset size is further exacerbated by substantial class imbalance since\n\"normal\" images dramatically outnumber those with findings. Given the rapid\nprogress of generative models in synthesizing realistic images, and the known\neffectiveness of simple data augmentation techniques (e.g. horizontal\nflipping), we ask if it is possible to synthetically augment mammogram datasets\nusing generative adversarial networks (GANs). We train a class-conditional GAN\nto perform contextual in-filling, which we then use to synthesize lesions onto\nhealthy screening mammograms. First, we show that GANs are capable of\ngenerating high-resolution synthetic mammogram patches. Next, we experimentally\nevaluate using the augmented dataset to improve breast cancer classification\nperformance. We observe that a ResNet-50 classifier trained with GAN-augmented\ntraining data produces a higher AUROC compared to the same model trained only\non traditionally augmented data, demonstrating the potential of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 06:29:10 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 16:57:16 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Wu", "Eric", ""], ["Wu", "Kevin", ""], ["Cox", "David", ""], ["Lotter", "William", ""]]}, {"id": "1807.08107", "submitter": "Di Chen", "authors": "Di Chen, Shanshan Zhang, Wanli Ouyang, Jian Yang, Ying Tai", "title": "Person Search via A Mask-Guided Two-Stream CNN Model", "comments": "accepted as poster to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of person search, which is a challenging\ntask consisted of pedestrian detection and person re-identification~(re-ID).\nInstead of sharing representations in a single joint model, we find that\nseparating detector and re-ID feature extraction yields better performance. In\norder to extract more representative features for each identity, we segment out\nthe foreground person from the original image patch. We propose a simple yet\neffective re-ID method, which models foreground person and original image\npatches individually, and obtains enriched representations from two separate\nCNN streams. From the experiments on two standard person search benchmarks of\nCUHK-SYSU and PRW, we achieve mAP of $83.0\\%$ and $32.6\\%$ respectively,\nsurpassing the state of the art by a large margin (more than 5pp).\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 08:23:38 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Chen", "Di", ""], ["Zhang", "Shanshan", ""], ["Ouyang", "Wanli", ""], ["Yang", "Jian", ""], ["Tai", "Ying", ""]]}, {"id": "1807.08108", "submitter": "Zukang Liao", "authors": "Zukang Liao", "title": "Simultaneous Adversarial Training - Learn from Others Mistakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples are maliciously tweaked images that can easily fool\nmachine learning techniques, such as neural networks, but they are normally not\nvisually distinguishable for human beings. One of the main approaches to solve\nthis problem is to retrain the networks using those adversarial examples,\nnamely adversarial training. However, standard adversarial training might not\nactually change the decision boundaries but cause the problem of gradient\nmasking, resulting in a weaker ability to generate adversarial examples.\nTherefore, it cannot alleviate the problem of black-box attacks, where\nadversarial examples generated from other networks can transfer to the targeted\none. In order to reduce the problem of black-box attacks, we propose a novel\nmethod that allows two networks to learn from each others' adversarial examples\nand become resilient to black-box attacks. We also combine this method with a\nsimple domain adaptation to further improve the performance.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 08:28:21 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 04:53:14 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 02:13:28 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Liao", "Zukang", ""]]}, {"id": "1807.08114", "submitter": "Yanhui Guo", "authors": "Yanhui Guo and Amira S. Ashour", "title": "Multiple Convolutional Neural Network for Skin Dermoscopic Image\n  Classification", "comments": "9 pages, ISIC 2018: Skin Lesion Analysis Towards Melanoma Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Melanoma classification is a serious stage to identify the skin disease. It\nis considered a challenging process due to the intra-class discrepancy of\nmelanomas, skin lesions low contrast, and the artifacts in the dermoscopy\nimages, including noise, existence of hair, air bubbles, and the similarity\nbetween melanoma and non-melanoma cases. To solve these problems, we propose a\nnovel multiple convolution neural network model (MCNN) to classify different\nseven disease types in dermoscopic images, where several models were trained\nseparately using an additive sample learning strategy. The MCNN model is\ntrained and tested using the training and validation sets from the\nInternational Skin Imaging Collaboration (ISIC 2018), respectively. The\nreceiver operating characteristic (ROC) curve is used to evaluate the\nperformance of the proposed method. The values of AUC (the area under the ROC\ncurve) were used to evaluate the performance of the MCNN.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 09:42:12 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 03:17:47 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Guo", "Yanhui", ""], ["Ashour", "Amira S.", ""]]}, {"id": "1807.08118", "submitter": "Nicolas Dobigeon", "authors": "Vinicius Ferraris, Nicolas Dobigeon, Yanna Cavalcanti, Thomas Oberlin,\n  Marie Chabert", "title": "Coupled dictionary learning for unsupervised change detection between\n  multi-sensor remote sensing images", "comments": "Submitted manuscript under consideration at Computer Vision and Image\n  Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Archetypal scenarios for change detection generally consider two images\nacquired through sensors of the same modality. However, in some specific cases\nsuch as emergency situations, the only images available may be those acquired\nthrough sensors of different modalities. This paper addresses the problem of\nunsupervisedly detecting changes between two observed images acquired by\nsensors of different modalities with possibly different resolutions. These\nsensor dissimilarities introduce additional issues in the context of\noperational change detection that are not addressed by most of the classical\nmethods. This paper introduces a novel framework to effectively exploit the\navailable information by modelling the two observed images as a sparse linear\ncombination of atoms belonging to a pair of coupled overcomplete dictionaries\nlearnt from each observed image. As they cover the same geographical location,\ncodes are expected to be globally similar, except for possible changes in\nsparse spatial locations. Thus, the change detection task is envisioned through\na dual code estimation which enforces spatial sparsity in the difference\nbetween the estimated codes associated with each image. This problem is\nformulated as an inverse problem which is iteratively solved using an efficient\nproximal alternating minimization algorithm accounting for nonsmooth and\nnonconvex functions. The proposed method is applied to real images with\nsimulated yet realistic and real changes. A comparison with state-of-the-art\nchange detection methods evidences the accuracy of the proposed strategy.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 10:02:34 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 13:44:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ferraris", "Vinicius", ""], ["Dobigeon", "Nicolas", ""], ["Cavalcanti", "Yanna", ""], ["Oberlin", "Thomas", ""], ["Chabert", "Marie", ""]]}, {"id": "1807.08135", "submitter": "Benyuan Sun", "authors": "Benyuan Sun and Zhen Zhou and Fandong Zhang and Xiuli Li and Yizhou\n  Wang", "title": "Integrating Feature and Image Pyramid: A Lung Nodule Detector Learned in\n  Curriculum Fashion", "comments": "Nodule Detection, Deep Learning, Curriculum Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung nodules suffer large variation in size and appearance in CT images.\nNodules less than 10mm can easily lose information after down-sampling in\nconvolutional neural networks, which results in low sensitivity. In this paper,\na combination of 3D image and feature pyramid is exploited to integrate\nlower-level texture features with high-level semantic features, thus leading to\na higher recall. However, 3D operations are time and memory consuming, which\naggravates the situation with the explosive growth of medical images. To tackle\nthis problem, we propose a general curriculum training strategy to speed up\ntraining. An dynamic sampling method is designed to pick up partial samples\nwhich give the best contribution to network training, thus leading to much less\ntime consuming. In experiments, we demonstrate that the proposed network\noutperforms previous state-of-the-art methods. Meanwhile, our sampling strategy\nhalves the training time of the proposal network on LUNA16.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 12:02:19 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 07:22:38 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Sun", "Benyuan", ""], ["Zhou", "Zhen", ""], ["Zhang", "Fandong", ""], ["Li", "Xiuli", ""], ["Wang", "Yizhou", ""]]}, {"id": "1807.08179", "submitter": "Ankush Gupta", "authors": "Ankush Gupta, Andrea Vedaldi, Andrew Zisserman", "title": "Inductive Visual Localisation: Factorised Training for Superior\n  Generalisation", "comments": "In BMVC 2018 (spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end trained Recurrent Neural Networks (RNNs) have been successfully\napplied to numerous problems that require processing sequences, such as image\ncaptioning, machine translation, and text recognition. However, RNNs often\nstruggle to generalise to sequences longer than the ones encountered during\ntraining. In this work, we propose to optimise neural networks explicitly for\ninduction. The idea is to first decompose the problem in a sequence of\ninductive steps and then to explicitly train the RNN to reproduce such steps.\nGeneralisation is achieved as the RNN is not allowed to learn an arbitrary\ninternal state; instead, it is tasked with mimicking the evolution of a valid\nstate. In particular, the state is restricted to a spatial memory map that\ntracks parts of the input image which have been accounted for in previous\nsteps. The RNN is trained for single inductive steps, where it produces updates\nto the memory in addition to the desired output. We evaluate our method on two\ndifferent visual recognition problems involving visual sequences: (1) text\nspotting, i.e. joint localisation and reading of text in images containing\nmultiple lines (or a block) of text, and (2) sequential counting of objects in\naerial images. We show that inductive training of recurrent models enhances\ntheir generalisation ability on challenging image datasets.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 17:09:16 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Gupta", "Ankush", ""], ["Vedaldi", "Andrea", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1807.08186", "submitter": "Dongdong Chen", "authors": "Qingnan Fan, Dongdong Chen, Lu Yuan, Gang Hua, Nenghai Yu, Baoquan\n  Chen", "title": "Decouple Learning for Parameterized Image Operators", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different deep networks have been used to approximate, accelerate or\nimprove traditional image operators, such as image smoothing, super-resolution\nand denoising. Among these traditional operators, many contain parameters which\nneed to be tweaked to obtain the satisfactory results, which we refer to as\n\"parameterized image operators\". However, most existing deep networks trained\nfor these operators are only designed for one specific parameter configuration,\nwhich does not meet the needs of real scenarios that usually require flexible\nparameters settings. To overcome this limitation, we propose a new decouple\nlearning algorithm to learn from the operator parameters to dynamically adjust\nthe weights of a deep network for image operators, denoted as the base network.\nThe learned algorithm is formed as another network, namely the weight learning\nnetwork, which can be end-to-end jointly trained with the base network.\nExperiments demonstrate that the proposed framework can be successfully applied\nto many traditional parameterized image operators. We provide more analysis to\nbetter understand the proposed framework, which may inspire more promising\nresearch in this direction. Our codes and models have been released in\nhttps://github.com/fqnchina/DecoupleLearning\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 17:58:54 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 17:08:45 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Fan", "Qingnan", ""], ["Chen", "Dongdong", ""], ["Yuan", "Lu", ""], ["Hua", "Gang", ""], ["Yu", "Nenghai", ""], ["Chen", "Baoquan", ""]]}, {"id": "1807.08205", "submitter": "Mingda Zhang", "authors": "Mingda Zhang, Rebecca Hwa and Adriana Kovashka", "title": "Equal But Not The Same: Understanding the Implicit Relationship Between\n  Persuasive Images and Text", "comments": "To appear in BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Images and text in advertisements interact in complex, non-literal ways. The\ntwo channels are usually complementary, with each channel telling a different\npart of the story. Current approaches, such as image captioning methods, only\nexamine literal, redundant relationships, where image and text show exactly the\nsame content. To understand more complex relationships, we first collect a\ndataset of advertisement interpretations for whether the image and slogan in\nthe same visual advertisement form a parallel (conveying the same message\nwithout literally saying the same thing) or non-parallel relationship, with the\nhelp of workers recruited on Amazon Mechanical Turk. We develop a variety of\nfeatures that capture the creativity of images and the specificity or ambiguity\nof text, as well as methods that analyze the semantics within and across\nchannels. We show that our method outperforms standard image-text alignment\napproaches on predicting the parallel/non-parallel relationship between image\nand text.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 20:53:39 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhang", "Mingda", ""], ["Hwa", "Rebecca", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1807.08241", "submitter": "Malik Aqeel Anwar", "authors": "Malik Aqeel Anwar, Arijit Raychowdhury", "title": "NAVREN-RL: Learning to fly in real environment via end-to-end deep\n  reinforcement learning using monocular images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NAVREN-RL, an approach to NAVigate an unmanned aerial vehicle in\nan indoor Real ENvironment via end-to-end reinforcement learning RL. A suitable\nreward function is designed keeping in mind the cost and weight constraints for\nmicro drone with minimum number of sensing modalities. Collection of small\nnumber of expert data and knowledge based data aggregation is integrated into\nthe RL process to aid convergence. Experimentation is carried out on a Parrot\nAR drone in different indoor arenas and the results are compared with other\nbaseline technologies. We demonstrate how the drone successfully avoids\nobstacles and navigates across different arenas.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 06:10:04 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Anwar", "Malik Aqeel", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1807.08254", "submitter": "Minjie Cai", "authors": "Minjie Cai, Kris Kitani, Yoichi Sato", "title": "Understanding hand-object manipulation by modeling the contextual\n  relationship between actions, grasp types and object attributes", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for understanding daily hand-object\nmanipulation by developing computer vision-based techniques. Specifically, we\nfocus on recognizing hand grasp types, object attributes and manipulation\nactions within an unified framework by exploring their contextual\nrelationships. Our hypothesis is that it is necessary to jointly model hands,\nobjects and actions in order to accurately recognize multiple tasks that are\ncorrelated to each other in hand-object manipulation. In the proposed model, we\nexplore various semantic relationships between actions, grasp types and object\nattributes, and show how the context can be used to boost the recognition of\neach component. We also explore the spatial relationship between the hand and\nobject in order to detect the manipulated object from hand in cluttered\nenvironment. Experiment results on all three recognition tasks show that our\nproposed method outperforms traditional appearance-based methods which are not\ndesigned to take into account contextual relationships involved in hand-object\nmanipulation. The visualization and generalizability study of the learned\ncontext further supports our hypothesis.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 07:45:01 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Cai", "Minjie", ""], ["Kitani", "Kris", ""], ["Sato", "Yoichi", ""]]}, {"id": "1807.08259", "submitter": "Mohammad Tavakolian", "authors": "Mohammad Tavakolian and Abdenour Hadid", "title": "Deep Discriminative Model for Video Classification", "comments": "Accepted in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new deep learning approach for video-based scene\nclassification. We design a Heterogeneous Deep Discriminative Model (HDDM)\nwhose parameters are initialized by performing an unsupervised pre-training in\na layer-wise fashion using Gaussian Restricted Boltzmann Machines (GRBM). In\norder to avoid the redundancy of adjacent frames, we extract spatiotemporal\nvariation patterns within frames and represent them sparsely using Sparse Cubic\nSymmetrical Pattern (SCSP). Then, a pre-initialized HDDM is separately trained\nusing the videos of each class to learn class-specific models. According to the\nminimum reconstruction error from the learnt class-specific models, a weighted\nvoting strategy is employed for the classification. The performance of the\nproposed method is extensively evaluated on two action recognition datasets;\nUCF101 and Hollywood II, and three dynamic texture and dynamic scene datasets;\nDynTex, YUPENN, and Maryland. The experimental results and comparisons against\nstate-of-the-art methods demonstrate that the proposed method consistently\nachieves superior performance on all datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 08:46:02 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Tavakolian", "Mohammad", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1807.08260", "submitter": "Yawei Luo", "authors": "Yawei Luo, Zhedong Zheng, Liang Zheng, Tao Guan, Junqing Yu, Yi Yang", "title": "Macro-Micro Adversarial Network for Human Parsing", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In human parsing, the pixel-wise classification loss has drawbacks in its\nlow-level local inconsistency and high-level semantic inconsistency. The\nintroduction of the adversarial network tackles the two problems using a single\ndiscriminator. However, the two types of parsing inconsistency are generated by\ndistinct mechanisms, so it is difficult for a single discriminator to solve\nthem both. To address the two kinds of inconsistencies, this paper proposes the\nMacro-Micro Adversarial Net (MMAN). It has two discriminators. One\ndiscriminator, Macro D, acts on the low-resolution label map and penalizes\nsemantic inconsistency, e.g., misplaced body parts. The other discriminator,\nMicro D, focuses on multiple patches of the high-resolution label map to\naddress the local inconsistency, e.g., blur and hole. Compared with traditional\nadversarial networks, MMAN not only enforces local and semantic consistency\nexplicitly, but also avoids the poor convergence problem of adversarial\nnetworks when handling high resolution images. In our experiment, we validate\nthat the two discriminators are complementary to each other in improving the\nhuman parsing accuracy. The proposed framework is capable of producing\ncompetitive parsing performance compared with the state-of-the-art methods,\ni.e., mIoU=46.81% and 59.91% on LIP and PASCAL-Person-Part, respectively. On a\nrelatively small dataset PPSS, our pre-trained model demonstrates impressive\ngeneralization ability. The code is publicly available at\nhttps://github.com/RoyalVane/MMAN.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 08:49:49 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 07:47:46 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Luo", "Yawei", ""], ["Zheng", "Zhedong", ""], ["Zheng", "Liang", ""], ["Guan", "Tao", ""], ["Yu", "Junqing", ""], ["Yang", "Yi", ""]]}, {"id": "1807.08271", "submitter": "Daniel Gutierrez", "authors": "Daniel Gutierrez-Gomez, Jose J. Guerrero", "title": "RGBiD-SLAM for Accurate Real-time Localisation and 3D Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a complete SLAM system for RGB-D cameras, namely\nRGB-iD SLAM. The presented approach is a dense direct SLAM method with the main\ncharacteristic of working with the depth maps in inverse depth parametrisation\nfor the routines of dense alignment or keyframe fusion. The system consists in\n2 CPU threads working in parallel, which share the use of the GPU for dense\nalignment and keyframe fusion routines. The first thread is a front-end\noperating at frame rate, which processes every incoming frame from the RGB-D\nsensor to compute the incremental odometry and integrate it in a keyframe which\nis changed periodically following a covisibility-based strategy. The second\nthread is a back-end which receives keyframes from the front-end. This thread\nis in charge of segmenting the keyframes based on their structure, describing\nthem using Bags of Words, trying to find potential loop closures with previous\nkeyframes, and in such case perform pose-graph optimisation for trajectory\ncorrection. In addition, our system allows is able to compute the odometry both\nwith unregistered and registered depth maps, allowing to use customised\ncalibrations of the RGB-D sensor. As a consequence in the paper we also propose\na detailed calibration pipeline to compute customised calibrations for\nparticular RGB-D cameras. The experiments with our approach in the TUM RGB-D\nbenchmark datasets show results superior in accuracy to the state-of-the-art in\nmany of the sequences. The code has been made available on-line for research\npurposes https://github.com/dangut/RGBiD-SLAM.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 11:05:24 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Gutierrez-Gomez", "Daniel", ""], ["Guerrero", "Jose J.", ""]]}, {"id": "1807.08275", "submitter": "Ali Shafti", "authors": "Mireia Ruiz Maymo, Ali Shafti, A. Aldo Faisal", "title": "FastOrient: Lightweight Computer Vision for Wrist Control in Assistive\n  Robotic Grasping", "comments": "6 pages. Accepted for publication at IEEE BioRob 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable and Assistive robotics for human grasp support are broadly either\ntele-operated robotic arms or act through orthotic control of a paralyzed\nuser's hand. Such devices require correct orientation for successful and\nefficient grasping. In many human-robot assistive settings, the end-user is\nrequired to explicitly control the many degrees of freedom making effective or\nefficient control problematic. Here we are demonstrating the off-loading of\nlow-level control of assistive robotics and active orthotics, through automatic\nend-effector orientation control for grasping. This paper describes a compact\nalgorithm implementing fast computer vision techniques to obtain the\norientation of the target object to be grasped, by segmenting the images\nacquired with a camera positioned on top of the end-effector of the robotic\ndevice. The rotation needed that optimises grasping is directly computed from\nthe object's orientation. The algorithm has been evaluated in 6 different scene\nbackgrounds and end-effector approaches to 26 different objects. 94.8% of the\nobjects were detected in all backgrounds. Grasping of the object was achieved\nin 91.1% of the cases and has been evaluated with a robot simulator confirming\nthe performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 11:48:30 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Maymo", "Mireia Ruiz", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1807.08284", "submitter": "Mitko Veta", "authors": "Mitko Veta, Yujing J. Heng, Nikolas Stathonikos, Babak Ehteshami\n  Bejnordi, Francisco Beca, Thomas Wollmann, Karl Rohr, Manan A. Shah, Dayong\n  Wang, Mikael Rousson, Martin Hedlund, David Tellez, Francesco Ciompi, Erwan\n  Zerhouni, David Lanyi, Matheus Viana, Vassili Kovalev, Vitali Liauchuk, Hady\n  Ahmady Phoulady, Talha Qaiser, Simon Graham, Nasir Rajpoot, Erik Sj\\\"oblom,\n  Jesper Molin, Kyunghyun Paeng, Sangheum Hwang, Sunggyun Park, Zhipeng Jia,\n  Eric I-Chao Chang, Yan Xu, Andrew H. Beck, Paul J. van Diest and Josien P. W.\n  Pluim", "title": "Predicting breast tumor proliferation from whole-slide images: the\n  TUPAC16 challenge", "comments": "Overview paper of the TUPAC16 challenge: http://tupac.tue-image.nl/", "journal-ref": null, "doi": "10.1016/j.media.2019.02.012", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor proliferation is an important biomarker indicative of the prognosis of\nbreast cancer patients. Assessment of tumor proliferation in a clinical setting\nis highly subjective and labor-intensive task. Previous efforts to automate\ntumor proliferation assessment by image analysis only focused on mitosis\ndetection in predefined tumor regions. However, in a real-world scenario,\nautomatic mitosis detection should be performed in whole-slide images (WSIs)\nand an automatic method should be able to produce a tumor proliferation score\ngiven a WSI as input. To address this, we organized the TUmor Proliferation\nAssessment Challenge 2016 (TUPAC16) on prediction of tumor proliferation scores\nfrom WSIs. The challenge dataset consisted of 500 training and 321 testing\nbreast cancer histopathology WSIs. In order to ensure fair and independent\nevaluation, only the ground truth for the training dataset was provided to the\nchallenge participants. The first task of the challenge was to predict mitotic\nscores, i.e., to reproduce the manual method of assessing tumor proliferation\nby a pathologist. The second task was to predict the gene expression based\nPAM50 proliferation scores from the WSI. The best performing automatic method\nfor the first task achieved a quadratic-weighted Cohen's kappa score of\n$\\kappa$ = 0.567, 95% CI [0.464, 0.671] between the predicted scores and the\nground truth. For the second task, the predictions of the top method had a\nSpearman's correlation coefficient of r = 0.617, 95% CI [0.581 0.651] with the\nground truth. This was the first study that investigated tumor proliferation\nassessment from WSIs. The achieved results are promising given the difficulty\nof the tasks and weakly-labelled nature of the ground truth. However, further\nresearch is needed to improve the practical utility of image analysis methods\nfor this task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 13:46:03 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 15:39:03 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Veta", "Mitko", ""], ["Heng", "Yujing J.", ""], ["Stathonikos", "Nikolas", ""], ["Bejnordi", "Babak Ehteshami", ""], ["Beca", "Francisco", ""], ["Wollmann", "Thomas", ""], ["Rohr", "Karl", ""], ["Shah", "Manan A.", ""], ["Wang", "Dayong", ""], ["Rousson", "Mikael", ""], ["Hedlund", "Martin", ""], ["Tellez", "David", ""], ["Ciompi", "Francesco", ""], ["Zerhouni", "Erwan", ""], ["Lanyi", "David", ""], ["Viana", "Matheus", ""], ["Kovalev", "Vassili", ""], ["Liauchuk", "Vitali", ""], ["Phoulady", "Hady Ahmady", ""], ["Qaiser", "Talha", ""], ["Graham", "Simon", ""], ["Rajpoot", "Nasir", ""], ["Sj\u00f6blom", "Erik", ""], ["Molin", "Jesper", ""], ["Paeng", "Kyunghyun", ""], ["Hwang", "Sangheum", ""], ["Park", "Sunggyun", ""], ["Jia", "Zhipeng", ""], ["Chang", "Eric I-Chao", ""], ["Xu", "Yan", ""], ["Beck", "Andrew H.", ""], ["van Diest", "Paul J.", ""], ["Pluim", "Josien P. W.", ""]]}, {"id": "1807.08291", "submitter": "Novanto Yudistira", "authors": "Novanto Yudistira and Takio Kurita", "title": "Correlation Net: Spatiotemporal multimodal deep learning for action\n  recognition", "comments": null, "journal-ref": "Signal Processing: Image Communication, Volume 82, March 2020,\n  115731", "doi": "10.1016/j.image.2019.115731", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a network that captures multimodal correlations over\narbitrary timestamps. The proposed scheme operates as a complementary, extended\nnetwork over a multimodal convolutional neural network (CNN). Spatial and\ntemporal streams are required for action recognition by a deep CNN, but\noverfitting reduction and fusing these two streams remain open problems. The\nexisting fusion approach averages the two streams. Here we propose a\ncorrelation network with a Shannon fusion for learning a pre-trained CNN. A\nLong-range video may consist of spatiotemporal correlations over arbitrary\ntimes, which can be captured by forming the correlation network from simple\nfully connected layers. This approach was found to complement the existing\nnetwork fusion methods. The importance of multimodal correlation is validated\nin comparison experiments on the UCF-101 and HMDB-51 datasets. The multimodal\ncorrelation enhanced the accuracy of the video recognition results.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 14:48:32 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 06:59:46 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 05:01:31 GMT"}, {"version": "v4", "created": "Wed, 20 Mar 2019 10:28:43 GMT"}, {"version": "v5", "created": "Thu, 9 May 2019 01:40:45 GMT"}, {"version": "v6", "created": "Mon, 16 Dec 2019 06:57:10 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Yudistira", "Novanto", ""], ["Kurita", "Takio", ""]]}, {"id": "1807.08332", "submitter": "Hongzhi Li", "authors": "Katherine M. Li and Evelyn C. Li", "title": "Skin Lesion Analysis Towards Melanoma Detection via End-to-end Deep\n  Learning of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the design, experiments and results of our solution\nsubmitted to the 2018 ISIC challenge: Skin Lesion Analysis Towards Melanoma\nDetection. We design a pipeline using state-of-the-art Convolutional Neural\nNetwork (CNN) models for a Lesion Boundary Segmentation task and a Lesion\nDiagnosis task.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 18:07:50 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Li", "Katherine M.", ""], ["Li", "Evelyn C.", ""]]}, {"id": "1807.08333", "submitter": "Zheng Shou", "authors": "Zheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa, Shih-Fu Chang", "title": "AutoLoc: Weakly-supervised Temporal Action Localization", "comments": "Accepted by ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal Action Localization (TAL) in untrimmed video is important for many\napplications. But it is very expensive to annotate the segment-level ground\ntruth (action class and temporal boundary). This raises the interest of\naddressing TAL with weak supervision, namely only video-level annotations are\navailable during training). However, the state-of-the-art weakly-supervised TAL\nmethods only focus on generating good Class Activation Sequence (CAS) over time\nbut conduct simple thresholding on CAS to localize actions. In this paper, we\nfirst develop a novel weakly-supervised TAL framework called AutoLoc to\ndirectly predict the temporal boundary of each action instance. We propose a\nnovel Outer-Inner-Contrastive (OIC) loss to automatically discover the needed\nsegment-level supervision for training such a boundary predictor. Our method\nachieves dramatically improved performance: under the IoU threshold 0.5, our\nmethod improves mAP on THUMOS'14 from 13.7% to 21.2% and mAP on ActivityNet\nfrom 7.4% to 27.3%. It is also very encouraging to see that our\nweakly-supervised method achieves comparable results with some fully-supervised\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 18:14:45 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 19:37:02 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Shou", "Zheng", ""], ["Gao", "Hang", ""], ["Zhang", "Lei", ""], ["Miyazawa", "Kazuyuki", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1807.08335", "submitter": "Jans Glagolevs", "authors": "Jans Glagolevs, Karlis Freivalds", "title": "A Statistical Method for Object Counting", "comments": null, "journal-ref": "In Proceedings of the International Conference on Graphics and\n  Signal Processing (ICGSP '17). ACM, New York, NY, USA, 61-64 (2017)", "doi": "10.1145/3121360.3121364", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new object counting method that is intended for\ncounting similarly sized and mostly round objects. Unlike many other algorithms\nof the same purpose, the proposed method does not rely on identifying every\nobject, it uses statistical data obtained from the image instead. The method is\nevaluated on images with human bone cells, oranges and pills achieving good\naccuracy. Its strengths are ability to deal with touching and partly\noverlapping objects, ability to work with different kinds of objects without\nprior configuration and good performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 18:25:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Glagolevs", "Jans", ""], ["Freivalds", "Karlis", ""]]}, {"id": "1807.08368", "submitter": "Baran Bar{\\i}\\c{s} K{\\i}v{\\i}lc{\\i}m", "authors": "Baran Baris Kivilcim, Itir Onal Ertugrul, Fatos T. Yarman Vural", "title": "Modeling Brain Networks with Artificial Neural Networks", "comments": "Accepted to 2nd Workshop on GRaphs in biomedicAl Image anaLysis,\n  MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we propose a neural network approach to capture the functional\nconnectivities among anatomic brain regions. The suggested approach estimates a\nset of brain networks, each of which represents the connectivity patterns of a\ncognitive process. We employ two different architectures of neural networks to\nextract directed and undirected brain networks from functional Magnetic\nResonance Imaging (fMRI) data. Then, we use the edge weights of the estimated\nbrain networks to train a classifier, namely, Support Vector Machines(SVM) to\nlabel the underlying cognitive process. We compare our brain network models\nwith popular models, which generate similar functional brain networks. We\nobserve that both undirected and directed brain networks surpass the\nperformances of the network models used in the fMRI literature. We also observe\nthat directed brain networks offer more discriminative features compared to the\nundirected ones for recognizing the cognitive processes. The representation\npower of the suggested brain networks are tested in a task-fMRI dataset of\nHuman Connectome Project and a Complex Problem Solving dataset.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 21:04:46 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kivilcim", "Baran Baris", ""], ["Ertugrul", "Itir Onal", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1807.08370", "submitter": "Chia-Wen Lin", "authors": "Chih-Chung Hsu, Chia-Wen Lin, Weng-Tai Su, and Gene Cheung", "title": "SiGAN: Siamese Generative Adversarial Network for Identity-Preserving\n  Face Hallucination", "comments": "13 pages", "journal-ref": null, "doi": "10.1109/TIP.2019.2924554", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite generative adversarial networks (GANs) can hallucinate\nphoto-realistic high-resolution (HR) faces from low-resolution (LR) faces, they\ncannot guarantee preserving the identities of hallucinated HR faces, making the\nHR faces poorly recognizable. To address this problem, we propose a Siamese GAN\n(SiGAN) to reconstruct HR faces that visually resemble their corresponding\nidentities. On top of a Siamese network, the proposed SiGAN consists of a pair\nof two identical generators and one discriminator. We incorporate\nreconstruction error and identity label information in the loss function of\nSiGAN in a pairwise manner. By iteratively optimizing the loss functions of the\ngenerator pair and discriminator of SiGAN, we cannot only achieve\nphoto-realistic face reconstruction, but also ensures the reconstructed\ninformation is useful for identity recognition. Experimental results\ndemonstrate that SiGAN significantly outperforms existing face hallucination\nGANs in objective face verification performance, while achieving\nphoto-realistic reconstruction. Moreover, for input LR faces from unknown\nidentities who are not included in training, SiGAN can still do a good job.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 21:18:38 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Hsu", "Chih-Chung", ""], ["Lin", "Chia-Wen", ""], ["Su", "Weng-Tai", ""], ["Cheung", "Gene", ""]]}, {"id": "1807.08379", "submitter": "Zhenyu Wu", "authors": "Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, Hailin Jin", "title": "Towards Privacy-Preserving Visual Recognition via Adversarial Training:\n  A Pilot Study", "comments": "A significant extension of this paper is accepted by TPAMI-20. A\n  conference version of this paper is accepted by ECCV-18. A shorter version of\n  this paper is accepted by ICML-18 PiMLAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to improve privacy-preserving visual recognition, an\nincreasingly demanded feature in smart camera applications, by formulating a\nunique adversarial training framework. The proposed framework explicitly learns\na degradation transform for the original video inputs, in order to optimize the\ntrade-off between target task performance and the associated privacy budgets on\nthe degraded video. A notable challenge is that the privacy budget, often\ndefined and measured in task-driven contexts, cannot be reliably indicated\nusing any single model performance, because a strong protection of privacy has\nto sustain against any possible model that tries to hack privacy information.\nSuch an uncommon situation has motivated us to propose two strategies, i.e.,\nbudget model restarting and ensemble, to enhance the generalization of the\nlearned degradation on protecting privacy against unseen hacker models. Novel\ntraining strategies, evaluation protocols, and result visualization methods\nhave been designed accordingly. Two experiments on privacy-preserving action\nrecognition, with privacy budgets defined in various ways, manifest the\ncompelling effectiveness of the proposed framework in simultaneously\nmaintaining high target task (action recognition) performance while suppressing\nthe privacy breach risk.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 22:30:56 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 23:05:53 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Wu", "Zhenyu", ""], ["Wang", "Zhangyang", ""], ["Wang", "Zhaowen", ""], ["Jin", "Hailin", ""]]}, {"id": "1807.08381", "submitter": "Tharindu Fernando", "authors": "Tharindu Fernando, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Pedestrian Trajectory Prediction with Structured Memory Hierarchies", "comments": "To appear in ECML-PKDD 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for human trajectory prediction based\non multimodal data (video and radar). Motivated by recent neuroscience\ndiscoveries, we propose incorporating a structured memory component in the\nhuman trajectory prediction pipeline to capture historical information to\nimprove performance. We introduce structured LSTM cells for modelling the\nmemory content hierarchically, preserving the spatiotemporal structure of the\ninformation and enabling us to capture both short-term and long-term context.\nWe demonstrate how this architecture can be extended to integrate salient\ninformation from multiple modalities to automatically store and retrieve\nimportant information for decision making without any supervision. We evaluate\nthe effectiveness of the proposed models on a novel multimodal dataset that we\nintroduce, consisting of 40,000 pedestrian trajectories, acquired jointly from\na radar system and a CCTV camera system installed in a public place. The\nperformance is also evaluated on the publicly available New York Grand Central\npedestrian database. In both settings, the proposed models demonstrate their\ncapability to better anticipate future pedestrian motion compared to existing\nstate of the art.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 23:17:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Fernando", "Tharindu", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "1807.08388", "submitter": "Markus Foote", "authors": "Markus D. Foote (1), Blake E. Zimmerman (1), Amit Sawant (2), Sarang\n  Joshi (1) ((1) Scientific Computing and Imaging Institute, Department of\n  Bioengineering, University of Utah, (2) Department of Radiation Oncology, The\n  University of Maryland School of Medicine)", "title": "Real-Time 2D-3D Deformable Registration with Deep Learning and\n  Application to Lung Radiotherapy Targeting", "comments": null, "journal-ref": "IPMI 2019. Lecture Notes in Computer Science, vol 11492. Springer,\n  Cham (2019)", "doi": "10.1007/978-3-030-20351-1_20", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiation therapy presents a need for dynamic tracking of a target tumor\nvolume. Fiducial markers such as implanted gold seeds have been used to gate\nradiation delivery but the markers are invasive and gating significantly\nincreases treatment time. Pretreatment acquisition of a respiratory correlated\n4DCT allows for determination of accurate motion tracking which is useful in\ntreatment planning. We design a patient-specific motion subspace and a deep\nconvolutional neural network to recover anatomical positions from a single\nfluoroscopic projection in real-time. We use this deep network to approximate\nthe nonlinear inverse of a diffeomorphic deformation composed with radiographic\nprojection. This network recovers subspace coordinates to define the\npatient-specific deformation of the lungs from a baseline anatomic position.\nThe geometric accuracy of the subspace deformations on real patient data is\nsimilar to accuracy attained by original image registration between individual\nrespiratory-phase image volumes.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 23:45:34 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 01:49:30 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Foote", "Markus D.", ""], ["Zimmerman", "Blake E.", ""], ["Sawant", "Amit", ""], ["Joshi", "Sarang", ""]]}, {"id": "1807.08392", "submitter": "Lei Bi", "authors": "Lei Bi, Dagan Feng, Jinman Kim", "title": "Improving Automatic Skin Lesion Segmentation using Adversarial Learning\n  based Data Augmentation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of skin lesions is considered as an important step in computer\naided diagnosis (CAD) for automated melanoma diagnosis. In recent years,\nsegmentation methods based on fully convolutional networks (FCN) have achieved\ngreat success in general images. This success is primarily due to the\nleveraging of large labelled datasets to learn features that correspond to the\nshallow appearance as well as the deep semantics of the images. However, the\ndependence on large dataset does not translate well into medical images. To\nimprove the FCN performance for skin lesion segmentations, researchers\nattempted to use specific cost functions or add post-processing algorithms to\nrefine the coarse boundaries of the FCN results. However, the performance of\nthese methods is heavily reliant on the tuning of many parameters and\npost-processing techniques. In this paper, we leverage the state-of-the-art\nimage feature learning method of generative adversarial network (GAN) for its\ninherent ability to produce consistent and realistic image features by using\ndeep neural networks and adversarial learning concept. We improve upon GAN such\nthat skin lesion features can be learned at different level of complexities, in\na controlled manner. The outputs from our method is then augmented to the\nexisting FCN training data, thus increasing the overall feature diversity. We\nevaluated our method on the ISIC 2018 skin lesion segmentation challenge\ndataset and showed that it was more accurate and robust when compared to the\nexisting skin lesion segmentation methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 00:42:25 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 01:30:04 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Bi", "Lei", ""], ["Feng", "Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "1807.08405", "submitter": "Trent Houliston", "authors": "Trent Houliston and Stephan K. Chalup", "title": "Visual Mesh: Real-time Object Detection Using Constant Sample Density", "comments": "12 pages, 6 figures, RoboCup International Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an enhancement of convolutional neural networks for\nobject detection in resource-constrained robotics through a geometric input\ntransformation called Visual Mesh. It uses object geometry to create a graph in\nvision space, reducing computational complexity by normalizing the pixel and\nfeature density of objects. The experiments compare the Visual Mesh with\nseveral other fast convolutional neural networks. The results demonstrate\nexecution times sixteen times quicker than the fastest competitor tested, while\nachieving outstanding accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 02:21:31 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Houliston", "Trent", ""], ["Chalup", "Stephan K.", ""]]}, {"id": "1807.08407", "submitter": "Shifeng Zhang", "authors": "Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, Stan Z. Li", "title": "Occlusion-aware R-CNN: Detecting Pedestrians in a Crowd", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian detection in crowded scenes is a challenging problem since the\npedestrians often gather together and occlude each other. In this paper, we\npropose a new occlusion-aware R-CNN (OR-CNN) to improve the detection accuracy\nin the crowd. Specifically, we design a new aggregation loss to enforce\nproposals to be close and locate compactly to the corresponding objects.\nMeanwhile, we use a new part occlusion-aware region of interest (PORoI) pooling\nunit to replace the RoI pooling layer in order to integrate the prior structure\ninformation of human body with visibility prediction into the network to handle\nocclusion. Our detector is trained in an end-to-end fashion, which achieves\nstate-of-the-art results on three pedestrian detection datasets, i.e.,\nCityPersons, ETH, and INRIA, and performs on-pair with the state-of-the-arts on\nCaltech.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 02:36:03 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zhang", "Shifeng", ""], ["Wen", "Longyin", ""], ["Bian", "Xiao", ""], ["Lei", "Zhen", ""], ["Li", "Stan Z.", ""]]}, {"id": "1807.08430", "submitter": "Kang Dang Mr", "authors": "Kang Dang, Chunluan Zhou, Zhigang Tu, Michael Hoy, Justin Dauwels,\n  Junsong Yuan", "title": "Actor-Action Semantic Segmentation with Region Masks", "comments": "Accepted by BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the actor-action semantic segmentation problem, which\nrequires joint labeling of both actor and action categories in video frames.\nOne major challenge for this task is that when an actor performs an action,\ndifferent body parts of the actor provide different types of cues for the\naction category and may receive inconsistent action labeling when they are\nlabeled independently. To address this issue, we propose an end-to-end\nregion-based actor-action segmentation approach which relies on region masks\nfrom an instance segmentation algorithm. Our main novelty is to avoid labeling\npixels in a region mask independently - instead we assign a single action label\nto these pixels to achieve consistent action labeling. When a pixel belongs to\nmultiple region masks, max pooling is applied to resolve labeling conflicts.\nOur approach uses a two-stream network as the front-end (which learns features\ncapturing both appearance and motion information), and uses two region-based\nsegmentation networks as the back-end (which takes the fused features from the\ntwo-stream network as the input and predicts actor-action labeling).\nExperiments on the A2D dataset demonstrate that both the region-based\nsegmentation strategy and the fused features from the two-stream network\ncontribute to the performance improvements. The proposed approach outperforms\nthe state-of-the-art results by more than 8% in mean class accuracy, and more\nthan 5% in mean class IOU, which validates its effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 05:11:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Dang", "Kang", ""], ["Zhou", "Chunluan", ""], ["Tu", "Zhigang", ""], ["Hoy", "Michael", ""], ["Dauwels", "Justin", ""], ["Yuan", "Junsong", ""]]}, {"id": "1807.08435", "submitter": "Linghao Zhang", "authors": "Prakruthi Prabhakar, Nitish Kulkarni, Linghao Zhang", "title": "Question Relevance in Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free-form and open-ended Visual Question Answering systems solve the problem\nof providing an accurate natural language answer to a question pertaining to an\nimage. Current VQA systems do not evaluate if the posed question is relevant to\nthe input image and hence provide nonsensical answers when posed with\nirrelevant questions to an image. In this paper, we solve the problem of\nidentifying the relevance of the posed question to an image. We address the\nproblem as two sub-problems. We first identify if the question is visual or\nnot. If the question is visual, we then determine if it's relevant to the image\nor not. For the second problem, we generate a large dataset from existing\nvisual question answering datasets in order to enable the training of complex\narchitectures and model the relevance of a visual question to an image. We also\ncompare the results of our Long Short-Term Memory Recurrent Neural Network\nbased models to Logistic Regression, XGBoost and multi-layer perceptron based\napproaches to the problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 06:01:44 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Prabhakar", "Prakruthi", ""], ["Kulkarni", "Nitish", ""], ["Zhang", "Linghao", ""]]}, {"id": "1807.08469", "submitter": "Themos Stafylakis", "authors": "Themos Stafylakis and Georgios Tzimiropoulos", "title": "Zero-shot keyword spotting for visual speech recognition in-the-wild", "comments": "Accepted at ECCV-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual keyword spotting (KWS) is the problem of estimating whether a text\nquery occurs in a given recording using only video information. This paper\nfocuses on visual KWS for words unseen during training, a real-world, practical\nsetting which so far has received no attention by the community. To this end,\nwe devise an end-to-end architecture comprising (a) a state-of-the-art visual\nfeature extractor based on spatiotemporal Residual Networks, (b) a\ngrapheme-to-phoneme model based on sequence-to-sequence neural networks, and\n(c) a stack of recurrent neural networks which learn how to correlate visual\nfeatures with the keyword representation. Different to prior works on KWS,\nwhich try to learn word representations merely from sequences of graphemes\n(i.e. letters), we propose the use of a grapheme-to-phoneme encoder-decoder\nmodel which learns how to map words to their pronunciation. We demonstrate that\nour system obtains very promising visual-only KWS results on the challenging\nLRS2 database, for keywords unseen during training. We also show that our\nsystem outperforms a baseline which addresses KWS via automatic speech\nrecognition (ASR), while it drastically improves over other recently proposed\nASR-free KWS methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:06:08 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 03:41:31 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Stafylakis", "Themos", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1807.08471", "submitter": "Yangyang Hao", "authors": "Hengliang Zhu, Yangyang Hao, Lizhuang Ma, Ruixing Li, Hua Wang", "title": "Deep attention-guided fusion network for lesion segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We participated the Task 1: Lesion Segmentation. The paper describes our\nalgorithm and the final result of validation set for the ISIC Challenge 2018 -\nSkin Lesion Analysis Towards Melanoma Detection.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:14:36 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 09:31:46 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhu", "Hengliang", ""], ["Hao", "Yangyang", ""], ["Ma", "Lizhuang", ""], ["Li", "Ruixing", ""], ["Wang", "Hua", ""]]}, {"id": "1807.08473", "submitter": "Yang Deng", "authors": "Yang Deng, Yao Sun, Yongpei Zhu, Shuo Zhang, Mingwang Zhu and Kehong\n  Yuan", "title": "DASN:Data-Aware Skilled Network for Accurate MR Brain Tissue\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of MR brain tissue is a crucial step for diagnosis,\nsurgical planning, and treatment of brain abnormalities. Automatic and reliable\nsegmenta-tion methods are required to assist doctor. Over the last few years,\ndeep learning especially deep convolutional neural networks (CNNs) have emerged\nas one of the most prominent approaches for image recognition problems in\nvarious do-mains. But the improvement of deep networks always needs\ninspiration, which is rare for the ordinary. Until now,there have been\nreasonable MR brain tissue segmentation methods,all of which can achieve\npromising performance. These different methods have their own characteristic\nand are distinctive for data sets. In other words, different models performance\nvary widely on the same data sets and each model has what it is skilled in. It\nis on the basis of this, we propose a judgement to distinguish data sets that\ndifferent models are good at. With our method, the segmentation accuracy can be\nimproved easily based on the existing models, neither without increasing\ntraining data nor improving the network. We validate our method on the widely\nused IBSR 18 dataset and obtain average dice ratio of 88.06%,while it is 85.82%\nand 86.92% when only using separate one model respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:19:19 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 06:35:05 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Deng", "Yang", ""], ["Sun", "Yao", ""], ["Zhu", "Yongpei", ""], ["Zhang", "Shuo", ""], ["Zhu", "Mingwang", ""], ["Yuan", "Kehong", ""]]}, {"id": "1807.08476", "submitter": "R T Pramod", "authors": "R.T. Pramod, Harish Katti and S.P. Arun", "title": "Human peripheral blur is optimal for object recognition", "comments": "24 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our vision is sharpest at the center of our gaze and becomes progressively\nblurry into the periphery. It is widely believed that this high foveal\nresolution evolved at the expense of peripheral acuity. But what if this\nsampling scheme is actually optimal for object recognition? To test this\nhypothesis, we trained deep neural networks on 'foveated' images with high\nresolution near objects and increasingly sparse sampling into the periphery.\nNeural networks trained using a blur profile matching the human eye yielded the\nbest performance compared to shallower and steeper blur profiles. Even in\nhumans, categorization accuracy deteriorated only for steeper blur profiles.\nThus, our blurry peripheral vision may have evolved to optimize object\nrecognition rather than merely due to wiring constraints.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:25:35 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 02:55:22 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 21:06:08 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Pramod", "R. T.", ""], ["Katti", "Harish", ""], ["Arun", "S. P.", ""]]}, {"id": "1807.08479", "submitter": "Ya Li", "authors": "Ya Li, Mingming Gong, Xinmei Tian, Tongliang Liu, Dacheng Tao", "title": "Domain Generalization via Conditional Invariant Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain generalization aims to apply knowledge gained from multiple labeled\nsource domains to unseen target domains. The main difficulty comes from the\ndataset bias: training data and test data have different distributions, and the\ntraining set contains heterogeneous samples from different distributions. Let\n$X$ denote the features, and $Y$ be the class labels. Existing domain\ngeneralization methods address the dataset bias problem by learning a\ndomain-invariant representation $h(X)$ that has the same marginal distribution\n$\\mathbb{P}(h(X))$ across multiple source domains. The functional relationship\nencoded in $\\mathbb{P}(Y|X)$ is usually assumed to be stable across domains\nsuch that $\\mathbb{P}(Y|h(X))$ is also invariant. However, it is unclear\nwhether this assumption holds in practical problems. In this paper, we consider\nthe general situation where both $\\mathbb{P}(X)$ and $\\mathbb{P}(Y|X)$ can\nchange across all domains. We propose to learn a feature representation which\nhas domain-invariant class conditional distributions $\\mathbb{P}(h(X)|Y)$. With\nthe conditional invariant representation, the invariance of the joint\ndistribution $\\mathbb{P}(h(X),Y)$ can be guaranteed if the class prior\n$\\mathbb{P}(Y)$ does not change across training and test domains. Extensive\nexperiments on both synthetic and real data demonstrate the effectiveness of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:33:46 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Li", "Ya", ""], ["Gong", "Mingming", ""], ["Tian", "Xinmei", ""], ["Liu", "Tongliang", ""], ["Tao", "Dacheng", ""]]}, {"id": "1807.08485", "submitter": "Kripasindhu Sarkar", "authors": "Kripasindhu Sarkar and Basavaraj Hampiholi and Kiran Varanasi and\n  Didier Stricker", "title": "Learning 3D Shapes as Multi-Layered Height-maps using 2D Convolutional\n  Networks", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel global representation of 3D shapes, suitable for the\napplication of 2D CNNs. We represent 3D shapes as multi-layered height-maps\n(MLH) where at each grid location, we store multiple instances of height maps,\nthereby representing 3D shape detail that is hidden behind several layers of\nocclusion. We provide a novel view merging method for combining view dependent\ninformation (Eg. MLH descriptors) from multiple views. Because of the ability\nof using 2D CNNs, our method is highly memory efficient in terms of input\nresolution compared to the voxel based input. Together with MLH descriptors and\nour multi view merging, we achieve the state-of-the-art result in\nclassification on ModelNet dataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 08:58:39 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 11:40:12 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Sarkar", "Kripasindhu", ""], ["Hampiholi", "Basavaraj", ""], ["Varanasi", "Kiran", ""], ["Stricker", "Didier", ""]]}, {"id": "1807.08488", "submitter": "Yong Xia", "authors": "Yutong Xie, Jianpeng Zhang, Yong Xia", "title": "A Multi-Level Deep Ensemble Model for Skin Lesion Classification in\n  Dermoscopy Images", "comments": "4 pages, 2 figures, ISIC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-level deep ensemble (MLDE) model that can be trained in an 'end to\nend' manner is proposed for skin lesion classification in dermoscopy images. In\nthis model, four pre-trained ResNet-50 networks are used to characterize the\nmultiscale information of skin lesions and are combined by using an adaptive\nweighting scheme that can be learned during the error back propagation. The\nproposed MLDE model achieved an average AUC value of 86.5% on the ISIC-skin\n2018 official validation dataset, which is substantially higher than the\naverage AUC values achieved by each of four ResNet-50 networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 09:02:22 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Xie", "Yutong", ""], ["Zhang", "Jianpeng", ""], ["Xia", "Yong", ""]]}, {"id": "1807.08512", "submitter": "Muhammad Kamran Janjua", "authors": "Alessandro Calefati, Muhammad Kamran Janjua, Shah Nawaz, Ignazio Gallo", "title": "Git Loss for Deep Face Recognition", "comments": "12 pages. Accepted at BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) have been widely used in computer vision\ntasks, such as face recognition and verification, and have achieved\nstate-of-the-art results due to their ability to capture discriminative deep\nfeatures. Conventionally, CNNs have been trained with softmax as supervision\nsignal to penalize the classification loss. In order to further enhance the\ndiscriminative capability of deep features, we introduce a joint supervision\nsignal, Git loss, which leverages on softmax and center loss functions. The aim\nof our loss function is to minimize the intra-class variations as well as\nmaximize the inter-class distances. Such minimization and maximization of deep\nfeatures are considered ideal for face recognition task. We perform experiments\non two popular face recognition benchmarks datasets and show that our proposed\nloss function achieves maximum separability between deep face features of\ndifferent identities and achieves state-of-the-art accuracy on two major face\nrecognition benchmark datasets: Labeled Faces in the Wild (LFW) and YouTube\nFaces (YTF). However, it should be noted that the major objective of Git loss\nis to achieve maximum separability between deep features of divergent\nidentities.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 10:20:29 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 14:31:38 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 10:21:28 GMT"}, {"version": "v4", "created": "Sat, 28 Jul 2018 17:29:04 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Calefati", "Alessandro", ""], ["Janjua", "Muhammad Kamran", ""], ["Nawaz", "Shah", ""], ["Gallo", "Ignazio", ""]]}, {"id": "1807.08526", "submitter": "Sergey Rodionov", "authors": "Sergey Rodionov, Alexey Potapov, Hugo Latapie, Enzo Fenoglio, Maxim\n  Peterson", "title": "Improving Deep Models of Person Re-identification for Cross-Dataset\n  Usage", "comments": "AIAI 2018 (14th International Conference on Artificial Intelligence\n  Applications and Innovations) proceeding. The final publication is available\n  at link.springer.com", "journal-ref": null, "doi": "10.1007/978-3-319-92007-8_7", "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (Re-ID) is the task of matching humans across\ncameras with non-overlapping views that has important applications in visual\nsurveillance. Like other computer vision tasks, this task has gained much with\nthe utilization of deep learning methods. However, existing solutions based on\ndeep learning are usually trained and tested on samples taken from same\ndatasets, while in practice one need to deploy Re-ID systems for new sets of\ncameras for which labeled data is unavailable. Here, we mitigate this problem\nfor one state-of-the-art model, namely, metric embedding trained with the use\nof the triplet loss function, although our results can be extended to other\nmodels. The contribution of our work consists in developing a method of\ntraining the model on multiple datasets, and a method for its online\npractically unsupervised fine-tuning. These methods yield up to 19.1%\nimprovement in Rank-1 score in the cross-dataset evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 10:57:54 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Rodionov", "Sergey", ""], ["Potapov", "Alexey", ""], ["Latapie", "Hugo", ""], ["Fenoglio", "Enzo", ""], ["Peterson", "Maxim", ""]]}, {"id": "1807.08536", "submitter": "Minjun Li", "authors": "Minjun Li, Haozhi Huang, Lin Ma, Wei Liu, Tong Zhang, Yu-Gang Jiang", "title": "Unsupervised Image-to-Image Translation with Stacked Cycle-Consistent\n  Adversarial Networks", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on unsupervised image-to-image translation have made a\nremarkable progress by training a pair of generative adversarial networks with\na cycle-consistent loss. However, such unsupervised methods may generate\ninferior results when the image resolution is high or the two image domains are\nof significant appearance differences, such as the translations between\nsemantic layouts and natural images in the Cityscapes dataset. In this paper,\nwe propose novel Stacked Cycle-Consistent Adversarial Networks (SCANs) by\ndecomposing a single translation into multi-stage transformations, which not\nonly boost the image translation quality but also enable higher resolution\nimage-to-image translations in a coarse-to-fine manner. Moreover, to properly\nexploit the information from the previous stage, an adaptive fusion block is\ndevised to learn a dynamic integration of the current stage's output and the\nprevious stage's output. Experiments on multiple datasets demonstrate that our\nproposed approach can improve the translation quality compared with previous\nsingle-stage unsupervised methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 11:15:56 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 09:06:34 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Li", "Minjun", ""], ["Huang", "Haozhi", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""], ["Jiang", "Yu-Gang", ""]]}, {"id": "1807.08555", "submitter": "Gustav Bredell", "authors": "Gustav Bredell, Christine Tanner, Ender Konukoglu", "title": "Iterative Interaction Training for Segmentation Editing Networks", "comments": "8 pages, 4 figures, To appear in the Proceedings of the 21.\n  International Conference On Medical Image Computing & Computer Assisted\n  Intervention, Machine Learning in Medical Imaging workshop, 16-20 September\n  2018, Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation has great potential to facilitate morphological\nmeasurements while simultaneously increasing efficiency. Nevertheless often\nusers want to edit the segmentation to their own needs and will need different\ntools for this. There has been methods developed to edit segmentations of\nautomatic methods based on the user input, primarily for binary segmentations.\nHere however, we present an unique training strategy for convolutional neural\nnetworks (CNNs) trained on top of an automatic method to enable interactive\nsegmentation editing that is not limited to binary segmentation. By utilizing a\nrobot-user during training, we closely mimic realistic use cases to achieve\noptimal editing performance. In addition, we show that an increase of the\niterative interactions during the training process up to ten improves the\nsegmentation editing performance substantially. Furthermore, we compare our\nsegmentation editing CNN (interCNN) to state-of-the-art interactive\nsegmentation algorithms and show a superior or on par performance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:13:35 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bredell", "Gustav", ""], ["Tanner", "Christine", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1807.08556", "submitter": "Ronghang Hu", "authors": "Ronghang Hu, Jacob Andreas, Trevor Darrell, Kate Saenko", "title": "Explainable Neural Computation via Stack Neural Module Networks", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex inferential tasks like question answering, machine learning models\nmust confront two challenges: the need to implement a compositional reasoning\nprocess, and, in many applications, the need for this reasoning process to be\ninterpretable to assist users in both development and prediction. Existing\nmodels designed to produce interpretable traces of their decision-making\nprocess typically require these traces to be supervised at training time. In\nthis paper, we present a novel neural modular approach that performs\ncompositional reasoning by automatically inducing a desired sub-task\ndecomposition without relying on strong supervision. Our model allows linking\ndifferent reasoning tasks though shared modules that handle common routines\nacross tasks. Experiments show that the model is more interpretable to human\nevaluators compared to other state-of-the-art models: users can better\nunderstand the model's underlying reasoning procedure and predict when it will\nsucceed or fail based on observing its intermediate outputs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:18:18 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 21:05:28 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 03:38:51 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hu", "Ronghang", ""], ["Andreas", "Jacob", ""], ["Darrell", "Trevor", ""], ["Saenko", "Kate", ""]]}, {"id": "1807.08563", "submitter": "Kaixuan Wang", "authors": "Kaixuan Wang, Shaojie Shen", "title": "MVDepthNet: Real-time Multiview Depth Estimation Neural Network", "comments": "This paper is accepted by 3DV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep neural networks have been widely applied to computer vision\nproblems, extending them into multiview depth estimation is non-trivial. In\nthis paper, we present MVDepthNet, a convolutional network to solve the depth\nestimation problem given several image-pose pairs from a localized monocular\ncamera in neighbor viewpoints. Multiview observations are encoded in a cost\nvolume and then combined with the reference image to estimate the depth map\nusing an encoder-decoder network. By encoding the information from multiview\nobservations into the cost volume, our method achieves real-time performance\nand the flexibility of traditional methods that can be applied regardless of\nthe camera intrinsic parameters and the number of images. Geometric data\naugmentation is used to train MVDepthNet. We further apply MVDepthNet in a\nmonocular dense mapping system that continuously estimates depth maps using a\nsingle localized moving camera. Experiments show that our method can generate\ndepth maps efficiently and precisely.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:37:13 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Wang", "Kaixuan", ""], ["Shen", "Shaojie", ""]]}, {"id": "1807.08571", "submitter": "Shiqi Dong", "authors": "Ru Zhang, Shiqi Dong, Jianyi Liu", "title": "Invisible Steganography via Generative Adversarial Networks", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there are plenty of works introducing convolutional neural networks\n(CNNs) to the steganalysis and exceeding conventional steganalysis algorithms.\nThese works have shown the improving potential of deep learning in information\nhiding domain. There are also several works based on deep learning to do image\nsteganography, but these works still have problems in capacity, invisibility\nand security. In this paper, we propose a novel CNN architecture named as\n\\isgan to conceal a secret gray image into a color cover image on the sender\nside and exactly extract the secret image out on the receiver side. There are\nthree contributions in our work: (i) we improve the invisibility by hiding the\nsecret image only in the Y channel of the cover image; (ii) We introduce the\ngenerative adversarial networks to strengthen the security by minimizing the\ndivergence between the empirical probability distributions of stego images and\nnatural images. (iii) In order to associate with the human visual system\nbetter, we construct a mixed loss function which is more appropriate for\nsteganography to generate more realistic stego images and reveal out more\nbetter secret images. Experiment results show that ISGAN can achieve\nstart-of-art performances on LFW, Pascal VOC2012 and ImageNet datasets.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:53:29 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 15:52:21 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 15:02:45 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Zhang", "Ru", ""], ["Dong", "Shiqi", ""], ["Liu", "Jianyi", ""]]}, {"id": "1807.08582", "submitter": "Xu Lan", "authors": "Xu Lan, Xiatian Zhu and Shaogang Gong", "title": "Person Search by Multi-Scale Matching", "comments": "to Appear in European Conference on Computer Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of person search in unconstrained scene images.\nExisting methods usually focus on improving the person detection accuracy to\nmitigate negative effects imposed by misalignment, mis-detections, and false\nalarms resulted from noisy people auto-detection. In contrast to previous\nstudies, we show that sufficiently reliable person instance cropping is\nachievable by slightly improved state-of-the-art deep learning object detectors\n(e.g. Faster-RCNN), and the under-studied multi-scale matching problem in\nperson search is a more severe barrier. In this work, we address this\nmulti-scale person search challenge by proposing a Cross-Level Semantic\nAlignment (CLSA) deep learning approach capable of learning more discriminative\nidentity feature representations in a unified end-to-end model. This is\nrealised by exploiting the in-network feature pyramid structure of a deep\nneural network enhanced by a novel cross pyramid-level semantic alignment loss\nfunction. This favourably eliminates the need for constructing a\ncomputationally expensive image pyramid and a complex multi-branch network\narchitecture. Extensive experiments show the modelling advantages and\nperformance superiority of CLSA over the state-of-the-art person search and\nmulti-scale matching methods on two large person search benchmarking datasets:\nCUHK-SYSU and PRW.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:05:38 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lan", "Xu", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1807.08599", "submitter": "Pawel Mlynarski", "authors": "Pawel Mlynarski, Herv\\'e Delingette, Antonio Criminisi, Nicholas\n  Ayache", "title": "3D Convolutional Neural Networks for Tumor Segmentation using Long-range\n  2D Context", "comments": "Submitted to the journal Computerized Medical Imaging and Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient deep learning approach for the challenging task of\ntumor segmentation in multisequence MR images. In recent years, Convolutional\nNeural Networks (CNN) have achieved state-of-the-art performances in a large\nvariety of recognition tasks in medical imaging. Because of the considerable\ncomputational cost of CNNs, large volumes such as MRI are typically processed\nby subvolumes, for instance slices (axial, coronal, sagittal) or small 3D\npatches. In this paper we introduce a CNN-based model which efficiently\ncombines the advantages of the short-range 3D context and the long-range 2D\ncontext. To overcome the limitations of specific choices of neural network\narchitectures, we also propose to merge outputs of several cascaded 2D-3D\nmodels by a voxelwise voting strategy. Furthermore, we propose a network\narchitecture in which the different MR sequences are processed by separate\nsubnetworks in order to be more robust to the problem of missing MR sequences.\nFinally, a simple and efficient algorithm for training large CNN models is\nintroduced. We evaluate our method on the public benchmark of the BRATS 2017\nchallenge on the task of multiclass segmentation of malignant brain tumors. Our\nmethod achieves good performances and produces accurate segmentations with\nmedian Dice scores of 0.918 (whole tumor), 0.883 (tumor core) and 0.854\n(enhancing core). Our approach can be naturally applied to various tasks\ninvolving segmentation of lesions or organs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:31:51 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Mlynarski", "Pawel", ""], ["Delingette", "Herv\u00e9", ""], ["Criminisi", "Antonio", ""], ["Ayache", "Nicholas", ""]]}, {"id": "1807.08601", "submitter": "Gerda Bortsova", "authors": "Gerda Bortsova, Florian Dubost, Silas {\\O}rting, Ioannis Katramados,\n  Laurens Hogeweg, Laura Thomsen, Mathilde Wille, Marleen de Bruijne", "title": "Deep Learning from Label Proportions for Emphysema Quantification", "comments": "Accepted to MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end deep learning method that learns to estimate\nemphysema extent from proportions of the diseased tissue. These proportions\nwere visually estimated by experts using a standard grading system, in which\ngrades correspond to intervals (label example: 1-5% of diseased tissue). The\nproposed architecture encodes the knowledge that the labels represent a\nvolumetric proportion. A custom loss is designed to learn with intervals. Thus,\nduring training, our network learns to segment the diseased tissue such that\nits proportions fit the ground truth intervals. Our architecture and loss\ncombined improve the performance substantially (8% ICC) compared to a more\nconventional regression network. We outperform traditional lung densitometry\nand two recently published methods for emphysema quantification by a large\nmargin (at least 7% AUC and 15% ICC), and achieve near-human-level performance.\nMoreover, our method generates emphysema segmentations that predict the spatial\ndistribution of emphysema at human level.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:34:01 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Bortsova", "Gerda", ""], ["Dubost", "Florian", ""], ["\u00d8rting", "Silas", ""], ["Katramados", "Ioannis", ""], ["Hogeweg", "Laurens", ""], ["Thomsen", "Laura", ""], ["Wille", "Mathilde", ""], ["de Bruijne", "Marleen", ""]]}, {"id": "1807.08624", "submitter": "Zhengyu Zhao", "authors": "Zhengyu Zhao and Martha Larson", "title": "From Volcano to Toyshop: Adaptive Discriminative Region Discovery for\n  Scene Recognition", "comments": "To appear at the ACM International Conference on Multimedia (ACM MM\n  2018). Code available at https://github.com/ZhengyuZhao/Adi-Red-Scene", "journal-ref": null, "doi": "10.1145/3240508.3240698", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As deep learning approaches to scene recognition emerge, they have continued\nto leverage discriminative regions at multiple scales, building on practices\nestablished by conventional image classification research. However, approaches\nremain largely generic, and do not carefully consider the special properties of\nscenes. In this paper, inspired by the intuitive differences between scenes and\nobjects, we propose Adi-Red, an adaptive approach to discriminative region\ndiscovery for scene recognition. Adi-Red uses a CNN classifier, which was\npre-trained using only image-level scene labels, to discover discriminative\nimage regions directly. These regions are then used as a source of features to\nperform scene recognition. The use of the CNN classifier makes it possible to\nadapt the number of discriminative regions per image using a simple, yet\nelegant, threshold, at relatively low computational cost. Experimental results\non the scene recognition benchmark dataset SUN397 demonstrate the ability of\nAdi-Red to outperform the state of the art. Additional experimental analysis on\nthe Places dataset reveals the advantages of Adi-Red, and highlight how they\nare specific to scenes. We attribute the effectiveness of Adi-Red to the\nability of adaptive region discovery to avoid introducing noise, while also not\nmissing out on important information.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:57:40 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 14:35:23 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Zhao", "Zhengyu", ""], ["Larson", "Martha", ""]]}, {"id": "1807.08634", "submitter": "Weixun Zhou", "authors": "Weixun Zhou, Xueqing Deng, and Zhenfeng Shao", "title": "Region Convolutional Features for Multi-Label Remote Sensing Image\n  Retrieval", "comments": "8 pages", "journal-ref": "IEEE J-STARS, 13 (2020): 318-328", "doi": "10.1109/JSTARS.2019.2961634", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional remote sensing image retrieval (RSIR) systems usually perform\nsingle-label retrieval where each image is annotated by a single label\nrepresenting the most significant semantic content of the image. This\nassumption, however, ignores the complexity of remote sensing images, where an\nimage might have multiple classes (i.e., multiple labels), thus resulting in\nworse retrieval performance. We therefore propose a novel multi-label RSIR\napproach with fully convolutional networks (FCN). In our approach, we first\ntrain a FCN model using a pixel-wise labeled dataset,and the trained FCN is\nthen used to predict the segmentation maps of each image in the considered\narchive. We finally extract region convolutional features of each image based\non its segmentation map.The region features can be either used to perform\nregion-based retrieval or further post-processed to obtain a feature vector for\nsimilarity measure. The experimental results show that our approach achieves\nstate-of-the-art performance in contrast to conventional single-label and\nrecent multi-label RSIR approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 14:04:18 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhou", "Weixun", ""], ["Deng", "Xueqing", ""], ["Shao", "Zhenfeng", ""]]}, {"id": "1807.08638", "submitter": "Xingyu Chen", "authors": "Xingyu Chen, Junzhi Yu, Shihan Kong, Zhengxing Wu, and Li Wen", "title": "Joint Anchor-Feature Refinement for Real-Time Accurate Object Detection\n  in Images and Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has been vigorously investigated for years but fast accurate\ndetection for real-world scenes remains a very challenging problem. Overcoming\ndrawbacks of single-stage detectors, we take aim at precisely detecting objects\nfor static and temporal scenes in real time. Firstly, as a dual refinement\nmechanism, a novel anchor-offset detection is designed, which includes an\nanchor refinement, a feature location refinement, and a deformable detection\nhead. This new detection mode is able to simultaneously perform two-step\nregression and capture accurate object features. Based on the anchor-offset\ndetection, a dual refinement network (DRNet) is developed for high-performance\nstatic detection, where a multi-deformable head is further designed to leverage\ncontextual information for describing objects. As for temporal detection in\nvideos, temporal refinement networks (TRNet) and temporal dual refinement\nnetworks (TDRNet) are developed by propagating the refinement information\nacross time. We also propose a soft refinement strategy to temporally match\nobject motion with the previous refinement. Our proposed methods are evaluated\non PASCAL VOC, COCO, and ImageNet VID datasets. Extensive comparisons on static\nand temporal detection verify the superiority of DRNet, TRNet, and TDRNet.\nConsequently, our developed approaches run in a fairly fast speed, and in the\nmeantime achieve a significantly enhanced detection accuracy, i.e., 84.4% mAP\non VOC 2007, 83.6% mAP on VOC 2012, 69.4% mAP on VID 2017, and 42.4% AP on\nCOCO. Ultimately, producing encouraging results, our methods are applied to\nonline underwater object detection and grasping with an autonomous system.\nCodes are publicly available at https://github.com/SeanChenxy/TDRN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 14:29:27 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 01:06:52 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 10:02:44 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 13:55:18 GMT"}, {"version": "v5", "created": "Sun, 22 Dec 2019 09:50:38 GMT"}, {"version": "v6", "created": "Fri, 13 Mar 2020 15:41:01 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Chen", "Xingyu", ""], ["Yu", "Junzhi", ""], ["Kong", "Shihan", ""], ["Wu", "Zhengxing", ""], ["Wen", "Li", ""]]}, {"id": "1807.08692", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Yannis Avrithis, Giorgos Tolias, Teddy Furon, Ondrej Chum", "title": "Hybrid Diffusion: Spectral-Temporal Graph Filtering for Manifold Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art image retrieval performance is achieved with CNN features\nand manifold ranking using a k-NN similarity graph that is pre-computed\noff-line. The two most successful existing approaches are temporal filtering,\nwhere manifold ranking amounts to solving a sparse linear system online, and\nspectral filtering, where eigen-decomposition of the adjacency matrix is\nperformed off-line and then manifold ranking amounts to dot-product search\nonline. The former suffers from expensive queries and the latter from\nsignificant space overhead. Here we introduce a novel, theoretically\nwell-founded hybrid filtering approach allowing full control of the space-time\ntrade-off between these two extremes. Experimentally, we verify that our hybrid\nmethod delivers results on par with the state of the art, with lower memory\ndemands compared to spectral filtering approaches and faster compared to\ntemporal filtering.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:07:29 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 16:43:29 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Iscen", "Ahmet", ""], ["Avrithis", "Yannis", ""], ["Tolias", "Giorgos", ""], ["Furon", "Teddy", ""], ["Chum", "Ondrej", ""]]}, {"id": "1807.08696", "submitter": "Guanying Chen", "authors": "Guanying Chen, Kai Han, Kwan-Yee K. Wong", "title": "PS-FCN: A Flexible Learning Framework for Photometric Stereo", "comments": "ECCV 2018: https://guanyingc.github.io/PS-FCN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of photometric stereo for non-Lambertian\nsurfaces. Existing approaches often adopt simplified reflectance models to make\nthe problem more tractable, but this greatly hinders their applications on\nreal-world objects. In this paper, we propose a deep fully convolutional\nnetwork, called PS-FCN, that takes an arbitrary number of images of a static\nobject captured under different light directions with a fixed camera as input,\nand predicts a normal map of the object in a fast feed-forward pass. Unlike the\nrecently proposed learning based method, PS-FCN does not require a pre-defined\nset of light directions during training and testing, and can handle multiple\nimages and light directions in an order-agnostic manner. Although we train\nPS-FCN on synthetic data, it can generalize well on real datasets. We further\nshow that PS-FCN can be easily extended to handle the problem of uncalibrated\nphotometric stereo.Extensive experiments on public real datasets show that\nPS-FCN outperforms existing approaches in calibrated photometric stereo, and\npromising results are achieved in uncalibrated scenario, clearly demonstrating\nits effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:13:27 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Chen", "Guanying", ""], ["Han", "Kai", ""], ["Wong", "Kwan-Yee K.", ""]]}, {"id": "1807.08772", "submitter": "Yajie Zhao", "authors": "Yajie Zhao, Weikai Chen, Jun Xing, Xiaoming Li, Zach Bessinger,\n  Fuchang Liu, Wangmeng Zuo, Ruigang Yang", "title": "Identity Preserving Face Completion for Large Ocular Region Occlusion", "comments": "12 pages,9 figures", "journal-ref": "The British Machine Vision Conference (BMVC) 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel deep learning approach to synthesize complete face images\nin the presence of large ocular region occlusions. This is motivated by recent\nsurge of VR/AR displays that hinder face-to-face communications. Different from\nthe state-of-the-art face inpainting methods that have no control over the\nsynthesized content and can only handle frontal face pose, our approach can\nfaithfully recover the missing content under various head poses while\npreserving the identity. At the core of our method is a novel generative\nnetwork with dedicated constraints to regularize the synthesis process. To\npreserve the identity, our network takes an arbitrary occlusion-free image of\nthe target identity to infer the missing content, and its high-level CNN\nfeatures as an identity prior to regularize the searching space of generator.\nSince the input reference image may have a different pose, a pose map and a\nnovel pose discriminator are further adopted to supervise the learning of\nimplicit pose transformations. Our method is capable of generating coherent\nfacial inpainting with consistent identity over videos with large variations of\nhead motions. Experiments on both synthesized and real data demonstrate that\nour method greatly outperforms the state-of-the-art methods in terms of both\nsynthesis quality and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 18:13:16 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Zhao", "Yajie", ""], ["Chen", "Weikai", ""], ["Xing", "Jun", ""], ["Li", "Xiaoming", ""], ["Bessinger", "Zach", ""], ["Liu", "Fuchang", ""], ["Zuo", "Wangmeng", ""], ["Yang", "Ruigang", ""]]}, {"id": "1807.08776", "submitter": "Helisa Dhamo", "authors": "Helisa Dhamo, Keisuke Tateno, Iro Laina, Nassir Navab, Federico\n  Tombari", "title": "Peeking Behind Objects: Layered Depth Prediction from a Single Image", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2019.05.007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While conventional depth estimation can infer the geometry of a scene from a\nsingle RGB image, it fails to estimate scene regions that are occluded by\nforeground objects. This limits the use of depth prediction in augmented and\nvirtual reality applications, that aim at scene exploration by synthesizing the\nscene from a different vantage point, or at diminished reality. To address this\nissue, we shift the focus from conventional depth map prediction to the\nregression of a specific data representation called Layered Depth Image (LDI),\nwhich contains information about the occluded regions in the reference frame\nand can fill in occlusion gaps in case of small view changes. We propose a\nnovel approach based on Convolutional Neural Networks (CNNs) to jointly predict\ndepth maps and foreground separation masks used to condition Generative\nAdversarial Networks (GANs) for hallucinating plausible color and depths in the\ninitially occluded areas. We demonstrate the effectiveness of our approach for\nnovel scene view synthesis from a single image.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 18:23:53 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Dhamo", "Helisa", ""], ["Tateno", "Keisuke", ""], ["Laina", "Iro", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1807.08784", "submitter": "Tejas Sudharshan Mathai", "authors": "Tejas Sudharshan Mathai, Lingbo Jin, Vijay Gorantla, and John Galeotti", "title": "Fast Vessel Segmentation and Tracking in Ultra High-Frequency Ultrasound\n  Images", "comments": "Accepted for presentation at MICCAI 2018. 8 pages, and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra High Frequency Ultrasound (UHFUS) enables the visualization of highly\ndeformable small and medium vessels in the hand. Intricate vessel-based\nmeasurements, such as intimal wall thickness and vessel wall compliance,\nrequire sub-millimeter vessel tracking between B-scans. Our fast GPU-based\napproach combines the advantages of local phase analysis, a\ndistance-regularized level set, and an Extended Kalman Filter (EKF), to rapidly\nsegment and track the deforming vessel contour. We validated on 35 UHFUS\nsequences of vessels in the hand, and we show the transferability of the\napproach to 5 more diverse datasets acquired by a traditional High Frequency\nUltrasound (HFUS) machine. To the best of our knowledge, this is the first\nalgorithm capable of rapidly segmenting and tracking deformable vessel contours\nin 2D UHFUS images. It is also the fastest and most accurate system for 2D HFUS\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 18:54:31 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Mathai", "Tejas Sudharshan", ""], ["Jin", "Lingbo", ""], ["Gorantla", "Vijay", ""], ["Galeotti", "John", ""]]}, {"id": "1807.08844", "submitter": "Sebastien Motsch", "authors": "Adrien Motsch, Sebastien Motsch, and Thibaut Saguet", "title": "Lesion segmentation using U-Net network", "comments": "4 pages, ISIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explains the method used in the segmentation challenge (Task 1) in\nthe International Skin Imaging Collaboration's (ISIC) Skin Lesion Analysis\nTowards Melanoma Detection challenge held in 2018. We have trained a U-Net\nnetwork to perform the segmentation. The key elements for the training were\nfirst to adjust the loss function to incorporate unbalanced proportion of\nbackground and second to perform post-processing operation to adjust the\ncontour of the prediction.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 21:54:35 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Motsch", "Adrien", ""], ["Motsch", "Sebastien", ""], ["Saguet", "Thibaut", ""]]}, {"id": "1807.08865", "submitter": "Sameh Khamis", "authors": "Sameh Khamis, Sean Fanello, Christoph Rhemann, Adarsh Kowdle, Julien\n  Valentin, Shahram Izadi", "title": "StereoNet: Guided Hierarchical Refinement for Real-Time Edge-Aware Depth\n  Prediction", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents StereoNet, the first end-to-end deep architecture for\nreal-time stereo matching that runs at 60 fps on an NVidia Titan X, producing\nhigh-quality, edge-preserved, quantization-free disparity maps. A key insight\nof this paper is that the network achieves a sub-pixel matching precision than\nis a magnitude higher than those of traditional stereo matching approaches.\nThis allows us to achieve real-time performance by using a very low resolution\ncost volume that encodes all the information needed to achieve high disparity\nprecision. Spatial precision is achieved by employing a learned edge-aware\nupsampling function. Our model uses a Siamese network to extract features from\nthe left and right image. A first estimate of the disparity is computed in a\nvery low resolution cost volume, then hierarchically the model re-introduces\nhigh-frequency details through a learned upsampling function that uses compact\npixel-to-pixel refinement networks. Leveraging color input as a guide, this\nfunction is capable of producing high-quality edge-aware output. We achieve\ncompelling results on multiple benchmarks, showing how the proposed method\noffers extreme flexibility at an acceptable computational budget.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 00:45:36 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Khamis", "Sameh", ""], ["Fanello", "Sean", ""], ["Rhemann", "Christoph", ""], ["Kowdle", "Adarsh", ""], ["Valentin", "Julien", ""], ["Izadi", "Shahram", ""]]}, {"id": "1807.08881", "submitter": "Deepak Babu Sam", "authors": "Deepak Babu Sam, R. Venkatesh Babu", "title": "Top-Down Feedback for Crowd Counting Convolutional Neural Network", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting people in dense crowds is a demanding task even for humans. This is\nprimarily due to the large variability in appearance of people. Often people\nare only seen as a bunch of blobs. Occlusions, pose variations and background\nclutter further compound the difficulty. In this scenario, identifying a person\nrequires larger spatial context and semantics of the scene. But the current\nstate-of-the-art CNN regressors for crowd counting are feedforward and use only\nlimited spatial context to detect people. They look for local crowd patterns to\nregress the crowd density map, resulting in false predictions. Hence, we\npropose top-down feedback to correct the initial prediction of the CNN. Our\narchitecture consists of a bottom-up CNN along with a separate top-down CNN to\ngenerate feedback. The bottom-up network, which regresses the crowd density\nmap, has two columns of CNN with different receptive fields. Features from\nvarious layers of the bottom-up CNN are fed to the top-down network. The\nfeedback, thus generated, is applied on the lower layers of the bottom-up\nnetwork in the form of multiplicative gating. This masking weighs activations\nof the bottom-up network at spatial as well as feature levels to correct the\ndensity prediction. We evaluate the performance of our model on all major crowd\ndatasets and show the effectiveness of top-down feedback.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 02:16:14 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:42:11 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Sam", "Deepak Babu", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1807.08891", "submitter": "Simon Sun", "authors": "Yujie Wang, Simon Sun, Jahow Yu, Dr. Limin Yu", "title": "Skin Lesion Segmentation Using Atrous Convolution via DeepLab v3", "comments": "4 pages, 1 figure, ISIC challenge 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As melanoma diagnoses increase across the US, automated efforts to identify\nmalignant lesions become increasingly of interest to the research community.\nSegmentation of dermoscopic images is the first step in this process, thus\naccuracy is crucial. Although techniques utilizing convolutional neural\nnetworks have been used in the past for lesion segmentation, we present a\nsolution employing the recently published DeepLab 3, an atrous convolution\nmethod for image segmentation. Although the results produced by this run are\nnot ideal, with a mean Jaccard index of 0.498, we believe that with further\nadjustments and modifications to the compatibility with the DeepLab code and\nwith training on more powerful processing units, this method may achieve better\nresults in future trials.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 03:09:23 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Wang", "Yujie", ""], ["Sun", "Simon", ""], ["Yu", "Jahow", ""], ["Yu", "Dr. Limin", ""]]}, {"id": "1807.08894", "submitter": "Lin Shao", "authors": "Lin Shao, Ye Tian, Jeannette Bohg", "title": "ClusterNet: 3D Instance Segmentation in RGB-D Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for instance-level segmentation that uses RGB-D data as\ninput and provides detailed information about the location, geometry and number\nof individual objects in the scene. This level of understanding is fundamental\nfor autonomous robots. It enables safe and robust decision-making under the\nlarge uncertainty of the real-world. In our model, we propose to use the first\nand second order moments of the object occupancy function to represent an\nobject instance. We train an hourglass Deep Neural Network (DNN) where each\npixel in the output votes for the 3D position of the corresponding object\ncenter and for the object's size and pose. The final instance segmentation is\nachieved through clustering in the space of moments. The object-centric\ntraining loss is defined on the output of the clustering. Our method\noutperforms the state-of-the-art instance segmentation method on our\nsynthesized dataset. We show that our method generalizes well on real-world\ndata achieving visually better segmentation results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 03:42:53 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 05:23:11 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Shao", "Lin", ""], ["Tian", "Ye", ""], ["Bohg", "Jeannette", ""]]}, {"id": "1807.08902", "submitter": "Xiaolin Zhang", "authors": "Xiaolin Zhang, Yunchao Wei, Guoliang Kang, Yi Yang, Thomas Huang", "title": "Self-produced Guidance for Weakly-supervised Object Localization", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised methods usually generate localization results based on\nattention maps produced by classification networks. However, the attention maps\nexhibit the most discriminative parts of the object which are small and sparse.\nWe propose to generate Self-produced Guidance (SPG) masks which separate the\nforeground, the object of interest, from the background to provide the\nclassification networks with spatial correlation information of pixels. A\nstagewise approach is proposed to incorporate high confident object regions to\nlearn the SPG masks. The high confident regions within attention maps are\nutilized to progressively learn the SPG masks. The masks are then used as an\nauxiliary pixel-level supervision to facilitate the training of classification\nnetworks. Extensive experiments on ILSVRC demonstrate that SPG is effective in\nproducing high-quality object localizations maps. Particularly, the proposed\nSPG achieves the Top-1 localization error rate of 43.83% on the ILSVRC\nvalidation set, which is a new state-of-the-art error rate.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 04:35:44 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 20:22:34 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Zhang", "Xiaolin", ""], ["Wei", "Yunchao", ""], ["Kang", "Guoliang", ""], ["Yang", "Yi", ""], ["Huang", "Thomas", ""]]}, {"id": "1807.08917", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Panchromatic Sharpening of Remote Sensing Images Using a Multi-scale\n  Approach", "comments": "11 pages, 7 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ideal fusion method preserves the Spectral information in fused image and\nadds spatial information to it with no spectral distortion. Recently wavelet\nkalman filter method is proposed which uses ARSIS concept to fuses MS and PAN\nimages. This method is applied in a multiscale version, i.e. the variable index\nis scale instead of time. With the aim of fusion we present a more detailed\nstudy on this model and discuss about rationality of its assumptions such as\nfirst order markov model and Gaussian distribution of the posterior density.\nFinally, we propose a method using wavelet Kalman Particle filter to improve\nthe spectral and spatial quality of the fused image. We show that our model is\nmore consistent with natural MS and PAN images. Visual and statistical analyzes\nshow that the proposed algorithm clearly improves the fusion quality in terms\nof: correlation coefficient, ERGAS, UIQI, and Q4; compared to other methods\nincluding IHS, HMP, PCA, A`trous, udWI, udWPC, Adaptive IHS, Improved Adaptive\nPCA and wavelet kalman filter.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:02:19 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1807.08920", "submitter": "Yang Hu Dr.", "authors": "Yang Hu, Guihua Wen, Mingnan Luo, Dan Dai, Jiajiong Ma, Zhiwen Yu", "title": "Competitive Inner-Imaging Squeeze and Excitation for Residual Network", "comments": "Code is available at\n  https://github.com/scut-aitcm/Competitive-Inner-Imaging-SENet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual networks, which use a residual unit to supplement the identity\nmappings, enable very deep convolutional architecture to operate well, however,\nthe residual architecture has been proved to be diverse and redundant, which\nmay leads to low-efficient modeling. In this work, we propose a competitive\nsqueeze-excitation (SE) mechanism for the residual network. Re-scaling the\nvalue for each channel in this structure will be determined by the residual and\nidentity mappings jointly, and this design enables us to expand the meaning of\nchannel relationship modeling in residual blocks. Modeling of the competition\nbetween residual and identity mappings cause the identity flow to control the\ncomplement of the residual feature maps for itself. Furthermore, we design a\nnovel inner-imaging competitive SE block to shrink the consumption and re-image\nthe global features of intermediate network structure, by using the\ninner-imaging mechanism, we can model the channel-wise relations with\nconvolution in spatial. We carry out experiments on the CIFAR, SVHN, and\nImageNet datasets, and the proposed method can challenge state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:13:25 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 14:55:37 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 16:52:45 GMT"}, {"version": "v4", "created": "Sun, 23 Dec 2018 02:56:45 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hu", "Yang", ""], ["Wen", "Guihua", ""], ["Luo", "Mingnan", ""], ["Dai", "Dan", ""], ["Ma", "Jiajiong", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1807.08931", "submitter": "Andrew Spek", "authors": "Andrew Spek and Thanuja Dharmasiri and Tom Drummond", "title": "CReaM: Condensed Real-time Models for Depth Prediction using\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the resurgence of CNNs the robotic vision community has developed a\nrange of algorithms that perform classification, semantic segmentation and\nstructure prediction (depths, normals, surface curvature) using neural\nnetworks. While some of these models achieve state-of-the art results and super\nhuman level performance, deploying these models in a time critical robotic\nenvironment remains an ongoing challenge. Real-time frameworks are of paramount\nimportance to build a robotic society where humans and robots integrate\nseamlessly. To this end, we present a novel real-time structure prediction\nframework that predicts depth at 30fps on an NVIDIA-TX2. At the time of\nwriting, this is the first piece of work to showcase such a capability on a\nmobile platform. We also demonstrate with extensive experiments that neural\nnetworks with very large model capacities can be leveraged in order to train\naccurate condensed model architectures in a \"from teacher to student\" style\nknowledge transfer.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 07:16:54 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Spek", "Andrew", ""], ["Dharmasiri", "Thanuja", ""], ["Drummond", "Tom", ""]]}, {"id": "1807.08935", "submitter": "Jana Kemnitz", "authors": "Jana Kemnitz, Christian F. Baumgartner, Wolfgang Wirth, Felix\n  Eckstein, Sebastian K. Eder, and Ender Konukoglu", "title": "Combining Heterogeneously Labeled Datasets For Training Segmentation\n  Networks", "comments": "Accepted for presentation at 9th International Conference on Machine\n  Learning in Medical Imaging (MLMI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate segmentation of medical images is an important step towards\nanalyzing and tracking disease related morphological alterations in the\nanatomy. Convolutional neural networks (CNNs) have recently emerged as a\npowerful tool for many segmentation tasks in medical imaging. The performance\nof CNNs strongly depends on the size of the training data and combining data\nfrom different sources is an effective strategy for obtaining larger training\ndatasets. However, this is often challenged by heterogeneous labeling of the\ndatasets. For instance, one of the dataset may be missing labels or a number of\nlabels may have been combined into a super label. In this work we propose a\ncost function which allows integration of multiple datasets with heterogeneous\nlabel subsets into a joint training. We evaluated the performance of this\nstrategy on thigh MR and a cardiac MR datasets in which we artificially merged\nlabels for half of the data. We found the proposed cost function substantially\noutperforms a naive masking approach, obtaining results very close to using the\nfull annotations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 07:43:23 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Kemnitz", "Jana", ""], ["Baumgartner", "Christian F.", ""], ["Wirth", "Wolfgang", ""], ["Eckstein", "Felix", ""], ["Eder", "Sebastian K.", ""], ["Konukoglu", "Ender", ""]]}, {"id": "1807.08942", "submitter": "Muktabh Mayank Srivastava", "authors": "Pratyush Kumar, Muktabh Mayank Srivastava", "title": "Example Mining for Incremental Learning in Medical Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Incremental Learning is well known machine learning approach wherein the\nweights of the learned model are dynamically and gradually updated to\ngeneralize on new unseen data without forgetting the existing knowledge.\nIncremental learning proves to be time as well as resource-efficient solution\nfor deployment of deep learning algorithms in real world as the model can\nautomatically and dynamically adapt to new data as and when annotated data\nbecomes available. The development and deployment of Computer Aided Diagnosis\n(CAD) tools in medical domain is another scenario, where incremental learning\nbecomes very crucial as collection and annotation of a comprehensive dataset\nspanning over multiple pathologies and imaging machines might take years.\nHowever, not much has so far been explored in this direction. In the current\nwork, we propose a robust and efficient method for incremental learning in\nmedical imaging domain. Our approach makes use of Hard Example Mining technique\n(which is commonly used as a solution to heavy class imbalance) to\nautomatically select a subset of dataset to fine-tune the existing network\nweights such that it adapts to new data while retaining existing knowledge. We\ndevelop our approach for incremental learning of our already under test model\nfor detecting dental caries. Further, we apply our approach to one publicly\navailable dataset and demonstrate that our approach reaches the accuracy of\ntraining on entire dataset at once, while availing the benefits of incremental\nlearning scenario.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 07:50:18 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Kumar", "Pratyush", ""], ["Srivastava", "Muktabh Mayank", ""]]}, {"id": "1807.08943", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Hyperspectral Images Classification Using Energy Profiles of Spatial and\n  Spectral Features", "comments": "7 pages, 3 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a spatial feature extraction method based on energy of\nthe features for classification of the hyperspectral data. A proposed\northogonal filter set extracts spatial features with maximum energy from the\nprincipal components and then, a profile is constructed based on these\nfeatures. The important characteristic of the proposed approach is that the\nfilter sets coefficients are extracted from statistical properties of data,\nthus they are more consistent with the type and texture of the remotely sensed\nimages compared with those of other filters such as Gabor. To assess the\nperformance of the proposed feature extraction method, the extracted features\nare fed into a support vector machine (SVM) classifier. Experiments on the\nwidely used hyperspectral images namely, Indian Pines, and Salinas data sets\nreveal that the proposed approach improves the classification results in\ncomparison with some recent spectral spatial classification methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 07:52:25 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1807.08948", "submitter": "Yao Zhang", "authors": "Jinyi Zou, Xiao Ma, Cheng Zhong and Yao Zhang", "title": "Dermoscopic Image Analysis for ISIC Challenge 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper reports the algorithms we used and the evaluation\nperformances for ISIC Challenge 2018. Our team participates in all the tasks in\nthis challenge. In lesion segmentation task, the pyramid scene parsing network\n(PSPNet) is modified to segment the lesions. In lesion attribute detection\ntask, the modified PSPNet is also adopted in a multi-label way. In disease\nclassification task, the DenseNet-169 is adopted for multi-class\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 08:12:54 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Zou", "Jinyi", ""], ["Ma", "Xiao", ""], ["Zhong", "Cheng", ""], ["Zhang", "Yao", ""]]}, {"id": "1807.08957", "submitter": "Vladyslav Usenko", "authors": "Vladyslav Usenko, Nikolaus Demmel and Daniel Cremers", "title": "The Double Sphere Camera Model", "comments": null, "journal-ref": null, "doi": "10.1109/3DV.2018.00069", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-based motion estimation and 3D reconstruction, which have numerous\napplications (e.g., autonomous driving, navigation systems for airborne devices\nand augmented reality) are receiving significant research attention. To\nincrease the accuracy and robustness, several researchers have recently\ndemonstrated the benefit of using large field-of-view cameras for such\napplications. In this paper, we provide an extensive review of existing models\nfor large field-of-view cameras. For each model we provide projection and\nunprojection functions and the subspace of points that result in valid\nprojection. Then, we propose the Double Sphere camera model that well fits with\nlarge field-of-view lenses, is computationally inexpensive and has a\nclosed-form inverse. We evaluate the model using a calibration dataset with\nseveral different lenses and compare the models using the metrics that are\nrelevant for Visual Odometry, i.e., reprojection error, as well as computation\ntime for projection and unprojection functions and their Jacobians. We also\nprovide qualitative results and discuss the performance of all models.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 08:42:00 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 16:37:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Usenko", "Vladyslav", ""], ["Demmel", "Nikolaus", ""], ["Cremers", "Daniel", ""]]}, {"id": "1807.09064", "submitter": "Xiaoguang Han", "authors": "Xiaoguang Han, Kangcheng Hou, Dong Du, Yuda Qiu, Yizhou Yu, Kun Zhou,\n  Shuguang Cui", "title": "CaricatureShop: Personalized and Photorealistic Caricature Sketching", "comments": "12 pages,16 figures,submitted to IEEE TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the first sketching system for interactively\npersonalized and photorealistic face caricaturing. Input an image of a human\nface, the users can create caricature photos by manipulating its facial feature\ncurves. Our system firstly performs exaggeration on the recovered 3D face model\naccording to the edited sketches, which is conducted by assigning the laplacian\nof each vertex a scaling factor. To construct the mapping between 2D sketches\nand a vertex-wise scaling field, a novel deep learning architecture is\ndeveloped. With the obtained 3D caricature model, two images are generated, one\nobtained by applying 2D warping guided by the underlying 3D mesh deformation\nand the other obtained by re-rendering the deformed 3D textured model. These\ntwo images are then seamlessly integrated to produce our final output. Due to\nthe severely stretching of meshes, the rendered texture is of blurry\nappearances. A deep learning approach is exploited to infer the missing details\nfor enhancing these blurry regions. Moreover, a relighting operation is\ninvented to further improve the photorealism of the result. Both quantitative\nand qualitative experiment results validated the efficiency of our sketching\nsystem and the superiority of our proposed techniques against existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:26:57 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Han", "Xiaoguang", ""], ["Hou", "Kangcheng", ""], ["Du", "Dong", ""], ["Qiu", "Yuda", ""], ["Yu", "Yizhou", ""], ["Zhou", "Kun", ""], ["Cui", "Shuguang", ""]]}, {"id": "1807.09072", "submitter": "Shihao Sun", "authors": "Shihao Sun, Lei Yang, Wenjie Liu, Ruirui Li", "title": "Feature Fusion through Multitask CNN for Large-scale Remote Sensing\n  Image Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Fully Convolutional Networks (FCN) has been widely used in\nvarious semantic segmentation tasks, including multi-modal remote sensing\nimagery. How to fuse multi-modal data to improve the segmentation performance\nhas always been a research hotspot. In this paper, a novel end-toend fully\nconvolutional neural network is proposed for semantic segmentation of natural\ncolor, infrared imagery and Digital Surface Models (DSM). It is based on a\nmodified DeepUNet and perform the segmentation in a multi-task way. The\nchannels are clustered into groups and processed on different task pipelines.\nAfter a series of segmentation and fusion, their shared features and private\nfeatures are successfully merged together. Experiment results show that the\nfeature fusion network is efficient. And our approach achieves good performance\nin ISPRS Semantic Labeling Contest (2D).\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:48:15 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Sun", "Shihao", ""], ["Yang", "Lei", ""], ["Liu", "Wenjie", ""], ["Li", "Ruirui", ""]]}, {"id": "1807.09075", "submitter": "Aditya Arun", "authors": "Aditya Arun, C.V. Jawahar, M. Pawan Kumar", "title": "Learning Human Poses from Actions", "comments": "Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of learning to estimate human pose in still images. In\norder to avoid the high cost of full supervision, we propose to use a diverse\ndata set, which consists of two types of annotations: (i) a small number of\nimages are labeled using the expensive ground-truth pose; and (ii) other images\nare labeled using the inexpensive action label. As action information helps\nnarrow down the pose of a human, we argue that this approach can help reduce\nthe cost of training without significantly affecting the accuracy. To\ndemonstrate this we design a probabilistic framework that employs two\ndistributions: (i) a conditional distribution to model the uncertainty over the\nhuman pose given the image and the action; and (ii) a prediction distribution,\nwhich provides the pose of an image without using any action information. We\njointly estimate the parameters of the two aforementioned distributions by\nminimizing their dissimilarity coefficient, as measured by a task-specific loss\nfunction. During both training and testing, we only require an efficient\nsampling strategy for both the aforementioned distributions. This allows us to\nuse deep probabilistic networks that are capable of providing accurate pose\nestimates for previously unseen images. Using the MPII data set, we show that\nour approach outperforms baseline methods that either do not use the diverse\nannotations or rely on pointwise estimates of the pose.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:58:58 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Arun", "Aditya", ""], ["Jawahar", "C. V.", ""], ["Kumar", "M. Pawan", ""]]}, {"id": "1807.09083", "submitter": "Ngoc Quang Nguyen", "authors": "Ngoc-Quang Nguyen", "title": "ISIC 2017 Skin Lesion Segmentation Using Deep Encoder-Decoder Network", "comments": "ISIC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our method and validation results for part 1 of the\nISBI Challenge 2018. Our algorithm makes use of deep encoder-decoder network\nand novel skin lesion data augmentation to segment the challenge objective.\nBesides, we also propose an effective testing strategy by applying multi-model\ncomparison.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 13:17:55 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Nguyen", "Ngoc-Quang", ""]]}, {"id": "1807.09123", "submitter": "Huajie Jiang", "authors": "Huajie Jiang, Ruiping Wang, Shiguang Shan, and Xilin Chen", "title": "Learning Class Prototypes via Structure Alignment for Zero-Shot\n  Recognition", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to recognize objects of novel classes without\nany training samples of specific classes, which is achieved by exploiting the\nsemantic information and auxiliary datasets. Recently most ZSL approaches focus\non learning visual-semantic embeddings to transfer knowledge from the auxiliary\ndatasets to the novel classes. However, few works study whether the semantic\ninformation is discriminative or not for the recognition task. To tackle such\nproblem, we propose a coupled dictionary learning approach to align the\nvisual-semantic structures using the class prototypes, where the discriminative\ninformation lying in the visual space is utilized to improve the less\ndiscriminative semantic space. Then, zero-shot recognition can be performed in\ndifferent spaces by the simple nearest neighbor approach using the learned\nclass prototypes. Extensive experiments on four benchmark datasets show the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:01:04 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Jiang", "Huajie", ""], ["Wang", "Ruiping", ""], ["Shan", "Shiguang", ""], ["Chen", "Xilin", ""]]}, {"id": "1807.09150", "submitter": "Yong Xia", "authors": "Yongsheng Pan and Yong Xia", "title": "Residual Network based Aggregation Model for Skin Lesion Classification", "comments": "ISIC2018 task3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recognize that the skin lesion diagnosis is an essential and challenging\nsub-task in Image classification, in which the Fisher vector (FV) encoding\nalgorithm and deep convolutional neural network (DCNN) are two of the most\nsuccessful techniques. Since the joint use of FV and DCNN has demonstrated\nproven success, the joint techniques could have discriminatory power on skin\nlesion diagnosis as well. To this hypothesis, we propose the aggregation\nalgorithm for skin lesion diagnosis that utilize the residual network to\nextract the local features and the Fisher vector method to aggregate the local\nfeatures to image-level representation. We applied our algorithm on the\nInternational Skin Imaging Collaboration 2018 (ISIC2018) challenge and only\nfocus on the third task, i.e., the disease classification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:33:48 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Pan", "Yongsheng", ""], ["Xia", "Yong", ""]]}, {"id": "1807.09151", "submitter": "Roman Khudorozhkov", "authors": "Roman Khudorozhkov, Alexander Koryagin, Alexey Kozhevin", "title": "Clearing noisy annotations for computed tomography imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the problems on the way to successful implementation of neural\nnetworks is the quality of annotation. For instance, different annotators can\nannotate images in a different way and very often their decisions do not match\nexactly and in extreme cases are even mutually exclusive which results in noisy\nannotations and, consequently, inaccurate predictions.\n  To avoid that problem in the task of computed tomography (CT) imaging\nsegmentation we propose a clearing algorithm for annotations. It consists of 3\nstages:\n  - annotators scoring, which assigns a higher confidence level to better\nannotators;\n  - nodules scoring, which assigns a higher confidence level to nodules\nconfirmed by good annotators;\n  - nodules merging, which aggregates annotations according to nodules\nconfidence.\n  In general, the algorithm can be applied to many different tasks (namely,\nbinary and multi-class semantic segmentation, and also with trivial adjustments\nto classification and regression) where there are several annotators labeling\neach image.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:42:57 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Khudorozhkov", "Roman", ""], ["Koryagin", "Alexander", ""], ["Kozhevin", "Alexey", ""]]}, {"id": "1807.09154", "submitter": "Santosh Vipparthi Kumar", "authors": "Monu Verma, Prafulla Saxena, Santosh. K. Vipparthi, Gridhari Singh", "title": "QUEST: Quadriletral Senary bit Pattern for Facial Expression Recognition", "comments": "7 pages, 7 tables, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expression has a significant role in analyzing human cognitive state.\nDeriving an accurate facial appearance representation is a critical task for an\nautomatic facial expression recognition application. This paper provides a new\nfeature descriptor named as Quadrilateral Senary bit Pattern for facial\nexpression recognition. The QUEST pattern encoded the intensity changes by\nemphasizing the relationship between neighboring and reference pixels by\ndividing them into two quadrilaterals in a local neighborhood. Thus, the\nresultant gradient edges reveal the transitional variation information, that\nimproves the classification rate by discriminating expression classes.\nMoreover, it also enhances the capability of the descriptor to deal with\nviewpoint variations and illumination changes. The trine relationship in a\nquadrilateral structure helps to extract the expressive edges and suppressing\nnoise elements to enhance the robustness to noisy conditions. The QUEST pattern\ngenerates a six-bit compact code, which improves the efficiency of the FER\nsystem with more discriminability. The effectiveness of the proposed method is\nevaluated by conducting several experiments on four benchmark datasets: MMI,\nGEMEP-FERA, OULU-CASIA, and ISED. The experimental results show better\nperformance of the proposed method as compared to existing state-art-the\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:39:48 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Verma", "Monu", ""], ["Saxena", "Prafulla", ""], ["Vipparthi", "Santosh. K.", ""], ["Singh", "Gridhari", ""]]}, {"id": "1807.09162", "submitter": "Sara Iodice", "authors": "Sara Iodice and Krystian Mikolajczyk", "title": "Partial Person Re-identification with Alignment and Hallucination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial person re-identification involves matching pedestrian frames where\nonly a part of a body is visible in corresponding images. This reflects\npractical CCTV surveillance scenario, where full person views are often not\navailable. Missing body parts make the comparison very challenging due to\nsignificant misalignment and varying scale of the views. We propose Partial\nMatching Net (PMN) that detects body joints, aligns partial views and\nhallucinates the missing parts based on the information present in the frame\nand a learned model of a person. The aligned and reconstructed views are then\ncombined into a joint representation and used for matching images. We evaluate\nour approach and compare to other methods on three different datasets,\ndemonstrating significant improvements.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:48:26 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Iodice", "Sara", ""], ["Mikolajczyk", "Krystian", ""]]}, {"id": "1807.09163", "submitter": "Anabik Pal Mr.", "authors": "Anabik Pal, Sounak Ray and Utpal Garain", "title": "Skin disease identification from dermoscopy images using deep\n  convolutional neural network", "comments": "Challenge Participation in ISIC 2018: Skin Lesion Analysis Towards\n  Melanoma Detection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a deep neural network based ensemble method is experimented\nfor automatic identification of skin disease from dermoscopic images. The\ndeveloped algorithm is applied on the task3 of the ISIC 2018 challenge dataset\n(Skin Lesion Analysis Towards Melanoma Detection).\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:48:57 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Pal", "Anabik", ""], ["Ray", "Sounak", ""], ["Garain", "Utpal", ""]]}, {"id": "1807.09169", "submitter": "Rania Briq", "authors": "Rania Briq, Michael Moeller, Juergen Gall", "title": "Convolutional Simplex Projection Network (CSPN) for Weakly Supervised\n  Semantic Segmentation", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly supervised semantic segmentation has been a subject of increased\ninterest due to the scarcity of fully annotated images. We introduce a new\napproach for solving weakly supervised semantic segmentation with deep\nConvolutional Neural Networks (CNNs). The method introduces a novel layer which\napplies simplex projection on the output of a neural network using area\nconstraints of class objects. The proposed method is general and can be\nseamlessly integrated into any CNN architecture. Moreover, the projection layer\nallows strongly supervised models to be adapted to weakly supervised models\neffortlessly by substituting ground truth labels. Our experiments have shown\nthat applying such an operation on the output of a CNN improves the accuracy of\nsemantic segmentation in a weakly supervised setting with image-level labels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:06:59 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Briq", "Rania", ""], ["Moeller", "Michael", ""], ["Gall", "Juergen", ""]]}, {"id": "1807.09190", "submitter": "Jonathon Luiten", "authors": "Jonathon Luiten, Paul Voigtlaender, Bastian Leibe", "title": "PReMVOS: Proposal-generation, Refinement and Merging for Video Object\n  Segmentation", "comments": "Accepted for publication in ACCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address semi-supervised video object segmentation, the task of\nautomatically generating accurate and consistent pixel masks for objects in a\nvideo sequence, given the first-frame ground truth annotations. Towards this\ngoal, we present the PReMVOS algorithm (Proposal-generation, Refinement and\nMerging for Video Object Segmentation). Our method separates this problem into\ntwo steps, first generating a set of accurate object segmentation mask\nproposals for each video frame and then selecting and merging these proposals\ninto accurate and temporally consistent pixel-wise object tracks over a video\nsequence in a way which is designed to specifically tackle the difficult\nchallenges involved with segmenting multiple objects across a video sequence.\nOur approach surpasses all previous state-of-the-art results on the DAVIS 2017\nvideo object segmentation benchmark with a J & F mean score of 71.6 on the\ntest-dev dataset, and achieves first place in both the DAVIS 2018 Video Object\nSegmentation Challenge and the YouTube-VOS 1st Large-scale Video Object\nSegmentation Challenge.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:42:45 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 17:35:06 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Luiten", "Jonathon", ""], ["Voigtlaender", "Paul", ""], ["Leibe", "Bastian", ""]]}, {"id": "1807.09192", "submitter": "Weidi Xie", "authors": "Weidi Xie and Andrew Zisserman", "title": "Multicolumn Networks for Face Recognition", "comments": "To appear in BMVC2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is set-based face recognition, i.e. to decide if\ntwo sets of images of a face are of the same person or not. Conventionally, the\nset-wise feature descriptor is computed as an average of the descriptors from\nindividual face images within the set. In this paper, we design a neural\nnetwork architecture that learns to aggregate based on both \"visual\" quality\n(resolution, illumination), and \"content\" quality (relative importance for\ndiscriminative classification). To this end, we propose a Multicolumn Network\n(MN) that takes a set of images (the number in the set can vary) as input, and\nlearns to compute a fix-sized feature descriptor for the entire set. To\nencourage high-quality representations, each individual input image is first\nweighted by its \"visual\" quality, determined by a self-quality assessment\nmodule, and followed by a dynamic recalibration based on \"content\" qualities\nrelative to the other images within the set. Both of these qualities are learnt\nimplicitly during training for set-wise classification. Comparing with the\nprevious state-of-the-art architectures trained with the same dataset\n(VGGFace2), our Multicolumn Networks show an improvement of between 2-6% on the\nIARPA IJB face recognition benchmarks, and exceed the state of the art for all\nmethods on these benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:45:58 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Xie", "Weidi", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1807.09200", "submitter": "Vithursan Thangarasa", "authors": "Vithursan Thangarasa, Graham W. Taylor", "title": "Self-Paced Learning with Adaptive Deep Visual Embeddings", "comments": "Published as a conference paper at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the most appropriate data examples to present a deep neural network\n(DNN) at different stages of training is an unsolved challenge. Though\npractitioners typically ignore this problem, a non-trivial data scheduling\nmethod may result in a significant improvement in both convergence and\ngeneralization performance. In this paper, we introduce Self-Paced Learning\nwith Adaptive Deep Visual Embeddings (SPL-ADVisE), a novel end-to-end training\nprotocol that unites self-paced learning (SPL) and deep metric learning (DML).\nWe leverage the Magnet Loss to train an embedding convolutional neural network\n(CNN) to learn a salient representation space. The student CNN classifier\ndynamically selects similar instance-level training examples to form a\nmini-batch, where the easiness from the cross-entropy loss and the true\ndiverseness of examples from the learned metric space serve as sample\nimportance priors. To demonstrate the effectiveness of SPL-ADVisE, we use deep\nCNN architectures for the task of supervised image classification on several\ncoarse- and fine-grained visual recognition datasets. Results show that, across\nall datasets, the proposed method converges faster and reaches a higher final\naccuracy than other SPL variants, particularly on fine-grained classes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:01:00 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Thangarasa", "Vithursan", ""], ["Taylor", "Graham W.", ""]]}, {"id": "1807.09202", "submitter": "Francesco Giannini", "authors": "Giuseppe Marra and Francesco Giannini and Michelangelo Diligenti and\n  Marco Gori", "title": "Constraint-Based Visual Generation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30508-6_45", "report-no": null, "categories": "cs.LG cs.CV cs.GR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years the systematic adoption of deep learning to visual\ngeneration has produced impressive results that, amongst others, definitely\nbenefit from the massive exploration of convolutional architectures. In this\npaper, we propose a general approach to visual generation that combines\nlearning capabilities with logic descriptions of the target to be generated.\nThe process of generation is regarded as a constrained satisfaction problem,\nwhere the constraints describe a set of properties that characterize the\ntarget. Interestingly, the constraints can also involve logic variables, while\nall of them are converted into real-valued functions by means of the t-norm\ntheory. We use deep architectures to model the involved variables, and propose\na computational scheme where the learning process carries out a satisfaction of\nthe constraints. We propose some examples in which the theory can naturally be\nused, including the modeling of GAN and auto-encoders, and report promising\nresults in problems with the generation of handwritten characters and face\ntransformations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 22:56:15 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 09:44:10 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 14:37:06 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Marra", "Giuseppe", ""], ["Giannini", "Francesco", ""], ["Diligenti", "Michelangelo", ""], ["Gori", "Marco", ""]]}, {"id": "1807.09207", "submitter": "Yujiang Wang", "authors": "Yujiang Wang, Bingnan Luo, Jie Shen, Maja Pantic", "title": "Face Mask Extraction in Video Sequence", "comments": "300VW-Mask dataset is available at:\n  https://github.com/mapleandfire/300VW-Mask", "journal-ref": "International Journal of Computer Vision, 127(6), 2019, 625-641", "doi": "10.1007/s11263-018-1130-2", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent development of deep network-based methods in semantic\nimage segmentation, we introduce an end-to-end trainable model for face mask\nextraction in video sequence. Comparing to landmark-based sparse face shape\nrepresentation, our method can produce the segmentation masks of individual\nfacial components, which can better reflect their detailed shape variations. By\nintegrating Convolutional LSTM (ConvLSTM) algorithm with Fully Convolutional\nNetworks (FCN), our new ConvLSTM-FCN model works on a per-sequence basis and\ntakes advantage of the temporal correlation in video clips. In addition, we\nalso propose a novel loss function, called Segmentation Loss, to directly\noptimise the Intersection over Union (IoU) performances. In practice, to\nfurther increase segmentation accuracy, one primary model and two additional\nmodels were trained to focus on the face, eyes, and mouth regions,\nrespectively. Our experiment shows the proposed method has achieved a 16.99%\nrelative improvement (from 54.50% to 63.76% mean IoU) over the baseline FCN\nmodel on the 300 Videos in the Wild (300VW) dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:09:32 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 15:08:30 GMT"}, {"version": "v3", "created": "Sat, 27 Feb 2021 17:42:27 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Yujiang", ""], ["Luo", "Bingnan", ""], ["Shen", "Jie", ""], ["Pantic", "Maja", ""]]}, {"id": "1807.09210", "submitter": "Kwang Hee Lee", "authors": "Kwang Hee Lee and Sang Wook Lee", "title": "Deterministic Fitting of Multiple Structures using Iterative MaxFS with\n  Inlier Scale Estimation and Subset Updating", "comments": "An extended version of our ICCV 2013 paper", "journal-ref": null, "doi": "10.1109/ICCV.2013.12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient deterministic hypothesis generation algorithm for\nrobust fitting of multiple structures based on the maximum feasible subsystem\n(MaxFS) framework. Despite its advantage, a global optimization method such as\nMaxFS has two main limitations for geometric model fitting. First, its\nperformance is much influenced by the user-specified inlier scale. Second, it\nis computationally inefficient for large data. The presented MaxFS-based\nalgorithm iteratively estimates model parameters and inlier scale and also\novercomes the second limitation by reducing data for the MaxFS problem. Further\nit generates hypotheses only with top-n ranked subsets based on matching scores\nand data fitting residuals. This reduction of data for the MaxFS problem makes\nthe algorithm computationally realistic. Our method, called iterative MaxFS\nwith inlier scale estimation and subset updating (IMaxFS-ISE-SU) in this paper,\nperforms hypothesis generation and fitting alternately until all of true\nstructures are found. The IMaxFS-ISE-SU algorithm generates substantially more\nreliable hypotheses than random sampling-based methods especially as\n(pseudo-)outlier ratios increase. Experimental results demonstrate that our\nmethod can generate more reliable and consistent hypotheses than random\nsampling-based methods for estimating multiple structures from data with many\noutliers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:16:41 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Lee", "Kwang Hee", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1807.09227", "submitter": "Moi Hoon Yap", "authors": "Manu Goyal and Jiahua Ng and Moi Hoon Yap", "title": "Multi-Class Lesion Diagnosis with Pixel-wise Classification Network", "comments": "6 pages, 4 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lesion diagnosis of skin lesions is a very challenging task due to high\ninter-class similarities and intra-class variations in terms of color, size,\nsite and appearance among different skin lesions. With the emergence of\ncomputer vision especially deep learning algorithms, lesion diagnosis is made\npossible using these algorithms trained on dermoscopic images. Usually, deep\nclassification networks are used for the lesion diagnosis to determine\ndifferent types of skin lesions. In this work, we used pixel-wise\nclassification network to provide lesion diagnosis rather than classification\nnetwork. We propose to use DeeplabV3+ for multi-class lesion diagnosis in\ndermoscopic images of Task 3 of ISIC Challenge 2018. We used various\npost-processing methods with DeeplabV3+ to determine the lesion diagnosis in\nthis challenge and submitted the test results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:45:49 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Goyal", "Manu", ""], ["Ng", "Jiahua", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1807.09232", "submitter": "Maria Camila Alvarez Trivino", "authors": "Maria Camila Alvarez Trivino (1), Jeremie Despraz (2), Jesus Alfonso\n  Lopez Sotelo (1), Carlos Andres Pena (2) ((1) Universidad Autonoma de\n  Occidente, (2) School of Business and Engineering Vaud (HEIG-VD))", "title": "Deep Learning on Retina Images as Screening Tool for Diagnostic Decision\n  Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, we developed a deep learning system applied to human retina\nimages for medical diagnostic decision support. The retina images were provided\nby EyePACS. These images were used in the framework of a Kaggle contest, whose\npurpose to identify diabetic retinopathy signs through an automatic detection\nsystem. Using as inspiration one of the solutions proposed in the contest, we\nimplemented a model that successfully detects diabetic retinopathy from retina\nimages. After a carefully designed preprocessing, the images were used as input\nto a deep convolutional neural network (CNN). The CNN performed a feature\nextraction process followed by a classification stage, which allowed the system\nto differentiate between healthy and ill patients using five categories. Our\nmodel was able to identify diabetic retinopathy in the patients with an\nagreement rate of 76.73% with respect to the medical expert's labels for the\ntest data.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 16:59:06 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Trivino", "Maria Camila Alvarez", ""], ["Despraz", "Jeremie", ""], ["Sotelo", "Jesus Alfonso Lopez", ""], ["Pena", "Carlos Andres", ""]]}, {"id": "1807.09244", "submitter": "Jiajun Wu", "authors": "David Zheng, Vinson Luo, Jiajun Wu, Joshua B. Tenenbaum", "title": "Unsupervised Learning of Latent Physical Properties Using\n  Perception-Prediction Networks", "comments": "UAI 2018 (oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for the completely unsupervised learning of latent\nobject properties from their interactions: the perception-prediction network\n(PPN). Consisting of a perception module that extracts representations of\nlatent object properties and a prediction module that uses those extracted\nproperties to simulate system dynamics, the PPN can be trained in an end-to-end\nfashion purely from samples of object dynamics. The representations of latent\nobject properties learned by PPNs not only are sufficient to accurately\nsimulate the dynamics of systems comprised of previously unseen objects, but\nalso can be translated directly into human-interpretable properties (e.g.,\nmass, coefficient of restitution) in an entirely unsupervised manner.\nCrucially, PPNs also generalize to novel scenarios: their gradient-based\ntraining can be applied to many dynamical systems and their graph-based\nstructure functions over systems comprised of different numbers of objects. Our\nresults demonstrate the efficacy of graph-based neural architectures in\nobject-centric inference and prediction tasks, and our model has the potential\nto discover relevant object properties in systems that are not yet well\nunderstood.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 17:28:27 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 19:03:07 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zheng", "David", ""], ["Luo", "Vinson", ""], ["Wu", "Jiajun", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1807.09245", "submitter": "Jiajun Wu", "authors": "Tianfan Xue, Jiajun Wu, Katherine L. Bouman, William T. Freeman", "title": "Visual Dynamics: Stochastic Future Generation via Layered Cross\n  Convolutional Networks", "comments": "Journal preprint of arXiv:1607.02586 (IEEE TPAMI, 2019). The first\n  two authors contributed equally to this work. Project page:\n  http://visualdynamics.csail.mit.edu", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), vol. 41, no. 9, pp. 2236-2250, 2019", "doi": "10.1109/TPAMI.2018.2854726", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of synthesizing a number of likely future frames from a\nsingle input image. In contrast to traditional methods that have tackled this\nproblem in a deterministic or non-parametric way, we propose to model future\nframes in a probabilistic manner. Our probabilistic model makes it possible for\nus to sample and synthesize many possible future frames from a single input\nimage. To synthesize realistic movement of objects, we propose a novel network\nstructure, namely a Cross Convolutional Network; this network encodes image and\nmotion information as feature maps and convolutional kernels, respectively. In\nexperiments, our model performs well on synthetic data, such as 2D shapes and\nanimated game sprites, and on real-world video frames. We present analyses of\nthe learned network representations, showing it is implicitly learning a\ncompact encoding of object appearance and motion. We also demonstrate a few of\nits applications, including visual analogy-making and video extrapolation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 17:28:31 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 19:17:56 GMT"}, {"version": "v3", "created": "Fri, 9 Aug 2019 23:11:54 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Xue", "Tianfan", ""], ["Wu", "Jiajun", ""], ["Bouman", "Katherine L.", ""], ["Freeman", "William T.", ""]]}, {"id": "1807.09251", "submitter": "Albert Pumarola", "authors": "Albert Pumarola and Antonio Agudo and Aleix M. Martinez and Alberto\n  Sanfeliu and Francesc Moreno-Noguer", "title": "GANimation: Anatomically-aware Facial Animation from a Single Image", "comments": "Accepted as oral at ECCV 2018. Code available at\n  https://github.com/albertpumarola/GANimation. Added minor updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Generative Adversarial Networks (GANs) have shown\nimpressive results for task of facial expression synthesis. The most successful\narchitecture is StarGAN, that conditions GANs generation process with images of\na specific domain, namely a set of images of persons sharing the same\nexpression. While effective, this approach can only generate a discrete number\nof expressions, determined by the content of the dataset. To address this\nlimitation, in this paper, we introduce a novel GAN conditioning scheme based\non Action Units (AU) annotations, which describes in a continuous manifold the\nanatomical facial movements defining a human expression. Our approach allows\ncontrolling the magnitude of activation of each AU and combine several of them.\nAdditionally, we propose a fully unsupervised strategy to train the model, that\nonly requires images annotated with their activated AUs, and exploit attention\nmechanisms that make our network robust to changing backgrounds and lighting\nconditions. Extensive evaluation show that our approach goes beyond competing\nconditional generators both in the capability to synthesize a much wider range\nof expressions ruled by anatomically feasible muscle movements, as in the\ncapacity of dealing with images in the wild.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 17:47:09 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 23:46:23 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Pumarola", "Albert", ""], ["Agudo", "Antonio", ""], ["Martinez", "Aleix M.", ""], ["Sanfeliu", "Alberto", ""], ["Moreno-Noguer", "Francesc", ""]]}, {"id": "1807.09259", "submitter": "Paul Henderson", "authors": "Paul Henderson, Vittorio Ferrari", "title": "Learning to Generate and Reconstruct 3D Meshes with only 2D Supervision", "comments": "BMVC 2018 (Oral). Differentiable renderer available at\n  https://github.com/pmh47/dirt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework tackling two problems: class-specific 3D\nreconstruction from a single image, and generation of new 3D shape samples.\nThese tasks have received considerable attention recently; however, existing\napproaches rely on 3D supervision, annotation of 2D images with keypoints or\nposes, and/or training with multiple views of each object instance. Our\nframework is very general: it can be trained in similar settings to these\nexisting approaches, while also supporting weaker supervision scenarios.\nImportantly, it can be trained purely from 2D images, without ground-truth pose\nannotations, and with a single view per instance. We employ meshes as an output\nrepresentation, instead of voxels used in most prior work. This allows us to\nexploit shading information during training, which previous 2D-supervised\nmethods cannot. Thus, our method can learn to generate and reconstruct concave\nobject classes. We evaluate our approach on synthetic data in various settings,\nshowing that (i) it learns to disentangle shape from pose; (ii) using shading\nin the loss improves performance; (iii) our model is comparable or superior to\nstate-of-the-art voxel-based approaches on quantitative metrics, while\nproducing results that are visually more pleasing; (iv) it still performs well\nwhen given supervision weaker than in prior works.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 17:54:51 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 18:00:21 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2018 18:59:50 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Henderson", "Paul", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1807.09298", "submitter": "Jundong Liu", "authors": "Zhewei Wang, Charles D. Smith and Jundong Liu", "title": "Ensemble of Multi-sized FCNs to Improve White Matter Lesion Segmentation", "comments": "Accepted to MLMI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a two-stage neural network solution for the\nchallenging task of white-matter lesion segmentation. To cope with the vast\nvari- ability in lesion sizes, we sample brain MR scans with patches at three\ndiffer- ent dimensions and feed them into separate fully convolutional neural\nnetworks (FCNs). In the second stage, we process large and small lesion\nseparately, and use ensemble-nets to combine the segmentation results generated\nfrom the FCNs. A novel activation function is adopted in the ensemble-nets to\nimprove the segmen- tation accuracy measured by Dice Similarity Coefficient.\nExperiments on MICCAI 2017 White Matter Hyperintensities (WMH) Segmentation\nChallenge data demonstrate that our two-stage-multi-sized FCN approach, as well\nas the new activation function, are effective in capturing white-matter lesions\nin MR images.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:29:20 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Wang", "Zhewei", ""], ["Smith", "Charles D.", ""], ["Liu", "Jundong", ""]]}, {"id": "1807.09303", "submitter": "Andreas Maier", "authors": "Shahab Zarei, Bernhard Stimpel, Christopher Syben, Andreas Maier", "title": "User Loss -- A Forced-Choice-Inspired Approach to Train Neural Networks\n  directly by User Interaction", "comments": "Accepted on BVM 2019; Extended ArXiv Version with additional figures\n  and details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether is it possible to train a neural\nnetwork directly from user inputs. We consider this approach to be highly\nrelevant for applications in which the point of optimality is not well-defined\nand user-dependent. Our application is medical image denoising which is\nessential in fluoroscopy imaging. In this field every user, i.e. physician, has\na different flavor and image quality needs to be tailored towards each\nindividual.\n  To address this important problem, we propose to construct a loss function\nderived from a forced-choice experiment. In order to make the learning problem\nfeasible, we operate in the domain of precision learning, i.e., we inspire the\nnetwork architecture by traditional signal processing methods in order to\nreduce the number of trainable parameters. The algorithm that was used for this\nis a Laplacian pyramid with only six trainable parameters.\n  In the experimental results, we demonstrate that two image experts who prefer\ndifferent filter characteristics between sharpness and de-noising can be\ncreated using our approach. Also models trained for a specific user perform\nbest on this users test data. This approach opens the way towards\nimplementation of direct user feedback in deep learning and is applicable for a\nwide range of application.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:43:36 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 13:39:21 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Zarei", "Shahab", ""], ["Stimpel", "Bernhard", ""], ["Syben", "Christopher", ""], ["Maier", "Andreas", ""]]}, {"id": "1807.09304", "submitter": "Christopher Choi", "authors": "Christopher L. Choi, Jason Rebello, Leonid Koppel, Pranav Ganti, Arun\n  Das, and Steven L. Waslander", "title": "Encoderless Gimbal Calibration of Dynamic Multi-Camera Clusters", "comments": "ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dynamic Camera Clusters (DCCs) are multi-camera systems where one or more\ncameras are mounted on actuated mechanisms such as a gimbal. Existing methods\nfor DCC calibration rely on joint angle measurements to resolve the\ntime-varying transformation between the dynamic and static camera. This\ninformation is usually provided by motor encoders, however, joint angle\nmeasurements are not always readily available on off-the-shelf mechanisms. In\nthis paper, we present an encoderless approach for DCC calibration which\nsimultaneously estimates the kinematic parameters of the transformation chain\nas well as the unknown joint angles. We also demonstrate the integration of an\nencoderless gimbal mechanism with a state-of-the art VIO algorithm, and show\nthe extensions required in order to perform simultaneous online estimation of\nthe joint angles and vehicle localization state. The proposed calibration\napproach is validated both in simulation and on a physical DCC composed of a\n2-DOF gimbal mounted on a UAV. Finally, we show the experimental results of the\ncalibrated mechanism integrated into the OKVIS VIO package, and demonstrate\nsuccessful online joint angle estimation while maintaining localization\naccuracy that is comparable to a standard static multi-camera configuration.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:45:52 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Choi", "Christopher L.", ""], ["Rebello", "Jason", ""], ["Koppel", "Leonid", ""], ["Ganti", "Pranav", ""], ["Das", "Arun", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1807.09312", "submitter": "Alexander Kuvaev", "authors": "Alexander Kuvaev, Roman Khudorozhkov", "title": "A Simple Probabilistic Model for Uncertainty Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article focuses on determining the predictive uncertainty of a model on\nthe example of atrial fibrillation detection problem by a single-lead ECG\nsignal. To this end, the model predicts parameters of the beta distribution\nover class probabilities instead of these probabilities themselves. It was\nshown that the described approach allows to detect atypical recordings and\nsignificantly improve the quality of the algorithm on confident predictions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:14:29 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Kuvaev", "Alexander", ""], ["Khudorozhkov", "Roman", ""]]}, {"id": "1807.09324", "submitter": "Sagnik Majumder", "authors": "Sagnik Majumder, C. von der Malsburg, Aashish Richhariya, Surekha\n  Bhanot", "title": "Handwritten Digit Recognition by Elastic Matching", "comments": "8 pages, 1 figure, 1 table, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple model of MNIST handwritten digit recognition is presented here. The\nmodel is an adaptation of a previous theory of face recognition. It realizes\ntranslation and rotation invariance in a principled way instead of being based\non extensive learning from large masses of sample data. The presented\nrecognition rates fall short of other publications, but due to its\ninspectability and conceptual and numerical simplicity, our system commends\nitself as a basis for further development.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 19:59:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Majumder", "Sagnik", ""], ["von der Malsburg", "C.", ""], ["Richhariya", "Aashish", ""], ["Bhanot", "Surekha", ""]]}, {"id": "1807.09341", "submitter": "Thanard Kurutach", "authors": "Thanard Kurutach, Aviv Tamar, Ge Yang, Stuart Russell, Pieter Abbeel", "title": "Learning Plannable Representations with Causal InfoGAN", "comments": "ICML / IJCAI / AAMAS 2018 Workshop on Planning and Learning (PAL-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep generative models have been shown to 'imagine'\nconvincing high-dimensional observations such as images, audio, and even video,\nlearning directly from raw data. In this work, we ask how to imagine\ngoal-directed visual plans -- a plausible sequence of observations that\ntransition a dynamical system from its current configuration to a desired goal\nstate, which can later be used as a reference trajectory for control. We focus\non systems with high-dimensional observations, such as images, and propose an\napproach that naturally combines representation learning and planning. Our\nframework learns a generative model of sequential observations, where the\ngenerative process is induced by a transition in a low-dimensional planning\nmodel, and an additional noise. By maximizing the mutual information between\nthe generated observations and the transition in the planning model, we obtain\na low-dimensional representation that best explains the causal nature of the\ndata. We structure the planning model to be compatible with efficient planning\nalgorithms, and we propose several such models based on either discrete or\ncontinuous states. Finally, to generate a visual plan, we project the current\nand goal observations onto their respective states in the planning model, plan\na trajectory, and then use the generative model to transform the trajectory to\na sequence of observations. We demonstrate our method on imagining plausible\nvisual plans of rope manipulation.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 20:46:05 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Kurutach", "Thanard", ""], ["Tamar", "Aviv", ""], ["Yang", "Ge", ""], ["Russell", "Stuart", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1807.09372", "submitter": "Niclas Zeller", "authors": "Niclas Zeller and Franz Quint and Uwe Stilla", "title": "A Synchronized Stereo and Plenoptic Visual Odometry Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dataset to evaluate monocular, stereo, and plenoptic camera\nbased visual odometry algorithms. The dataset comprises a set of synchronized\nimage sequences recorded by a micro lens array (MLA) based plenoptic camera and\na stereo camera system. For this, the stereo cameras and the plenoptic camera\nwere assembled on a common hand-held platform. All sequences are recorded in a\nvery large loop, where beginning and end show the same scene. Therefore, the\ntracking accuracy of a visual odometry algorithm can be measured from the drift\nbetween beginning and end of the sequence. For both, the plenoptic camera and\nthe stereo system, we supply full intrinsic camera models, as well as\nvignetting data. The dataset consists of 11 sequences which were recorded in\nchallenging indoor and outdoor scenarios. We present, by way of example, the\nresults achieved by state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 21:59:01 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 20:43:35 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zeller", "Niclas", ""], ["Quint", "Franz", ""], ["Stilla", "Uwe", ""]]}, {"id": "1807.09380", "submitter": "Jue Wang", "authors": "Jue Wang and Anoop Cherian", "title": "Contrastive Video Representation Learning via Adversarial Perturbations", "comments": "Revised version of ECCV 2018 Paper: Learning Discriminative Video\n  Representations Using Adversarial Perturbations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adversarial perturbations are noise-like patterns that can subtly change the\ndata, while failing an otherwise accurate classifier. In this paper, we propose\nto use such perturbations within a novel contrastive learning setup to build\nnegative samples, which are then used to produce improved video\nrepresentations. To this end, given a well-trained deep model for per-frame\nvideo recognition, we first generate adversarial noise adapted to this model.\nPositive and negative bags are produced using the original data features from\nthe full video sequence and their perturbed counterparts, respectively. Unlike\nthe classic contrastive learning methods, we develop a binary classification\nproblem that learns a set of discriminative hyperplanes -- as a subspace --\nthat will separate the two bags from each other. This subspace is then used as\na descriptor for the video, dubbed \\emph{discriminative subspace pooling}. As\nthe perturbed features belong to data classes that are likely to be confused\nwith the original features, the discriminative subspace will characterize parts\nof the feature space that are more representative of the original data, and\nthus may provide robust video representations. To learn such descriptors, we\nformulate a subspace learning objective on the Stiefel manifold and resort to\nRiemannian optimization methods for solving it efficiently. We provide\nexperiments on several video datasets and demonstrate state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 22:46:42 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 13:46:24 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 00:03:53 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Wang", "Jue", ""], ["Cherian", "Anoop", ""]]}, {"id": "1807.09384", "submitter": "Aysegul Dundar", "authors": "Aysegul Dundar, Ming-Yu Liu, Ting-Chun Wang, John Zedlewski, Jan Kautz", "title": "Domain Stylization: A Strong, Simple Baseline for Synthetic to Real\n  Image Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have largely failed to effectively utilize synthetic\ndata when applied to real images due to the covariate shift problem. In this\npaper, we show that by applying a straightforward modification to an existing\nphotorealistic style transfer algorithm, we achieve state-of-the-art\nsynthetic-to-real domain adaptation results. We conduct extensive experimental\nvalidations on four synthetic-to-real tasks for semantic segmentation and\nobject detection, and show that our approach exceeds the performance of any\ncurrent state-of-the-art GAN-based image translation approach as measured by\nsegmentation and object detection metrics. Furthermore we offer a distance\nbased analysis of our method which shows a dramatic reduction in Frechet\nInception distance between the source and target domains, offering a\nquantitative metric that demonstrates the effectiveness of our algorithm in\nbridging the synthetic-to-real gap.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 23:06:49 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Dundar", "Aysegul", ""], ["Liu", "Ming-Yu", ""], ["Wang", "Ting-Chun", ""], ["Zedlewski", "John", ""], ["Kautz", "Jan", ""]]}, {"id": "1807.09388", "submitter": "Kai Xu", "authors": "Kai Xu, Zhikang Zhang, Fengbo Ren", "title": "LAPRAN: A Scalable Laplacian Pyramid Reconstructive Adversarial Network\n  for Flexible Compressive Sensing Reconstruction", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the single-image compressive sensing (CS) and\nreconstruction problem. We propose a scalable Laplacian pyramid reconstructive\nadversarial network (LAPRAN) that enables high-fidelity, flexible and fast CS\nimages reconstruction. LAPRAN progressively reconstructs an image following the\nconcept of Laplacian pyramid through multiple stages of reconstructive\nadversarial networks (RANs). At each pyramid level, CS measurements are fused\nwith a contextual latent vector to generate a high-frequency image residual.\nConsequently, LAPRAN can produce hierarchies of reconstructed images and each\nwith an incremental resolution and improved quality. The scalable pyramid\nstructure of LAPRAN enables high-fidelity CS reconstruction with a flexible\nresolution that is adaptive to a wide range of compression ratios (CRs), which\nis infeasible with existing methods. Experimental results on multiple public\ndatasets show that LAPRAN offers an average 7.47dB and 5.98dB PSNR, and an\naverage 57.93% and 33.20% SSIM improvement compared to model-based and\ndata-driven baselines, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 23:28:17 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 18:45:27 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 18:36:57 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Xu", "Kai", ""], ["Zhang", "Zhikang", ""], ["Ren", "Fengbo", ""]]}, {"id": "1807.09408", "submitter": "Kwang Hee Lee", "authors": "Kwang Hee Lee, Chanki Yu and Sang Wook Lee", "title": "Deterministic Hypothesis Generation for Robust Fitting of Multiple\n  Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm for generating robust and consistent hypotheses\nfor multiple-structure model fitting. Most of the existing methods utilize\nrandom sampling which produce varying results especially when outlier ratio is\nhigh. For a structure where a model is fitted, the inliers of other structures\nare regarded as outliers when multiple structures are present. Global\noptimization has recently been investigated to provide stable and unique\nsolutions, but the computational cost of the algorithms is prohibitively high\nfor most image data with reasonable sizes. The algorithm presented in this\npaper uses a maximum feasible subsystem (MaxFS) algorithm to generate\nconsistent initial hypotheses only from partial datasets in spatially\noverlapping local image regions. Our assumption is that each genuine structure\nwill exist as a dominant structure in at least one of the local regions. To\nrefine initial hypotheses estimated from partial datasets and to remove\nresidual tolerance dependency of the MaxFS algorithm, iterative re-weighted L1\n(IRL1) minimization is performed for all the image data. Initial weights of\nIRL1 framework are determined from the initial hypotheses generated in local\nregions. Our approach is significantly more efficient than those that use only\nglobal optimization for all the image data. Experimental results demonstrate\nthat the presented method can generate more reliable and consistent hypotheses\nthan random-sampling methods for estimating single and multiple structures from\ndata with a large amount of outliers. We clearly expose the influence of\nalgorithm parameter settings on the results in our experiments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 01:28:28 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Lee", "Kwang Hee", ""], ["Yu", "Chanki", ""], ["Lee", "Sang Wook", ""]]}, {"id": "1807.09413", "submitter": "Zi Jian Yew", "authors": "Zi Jian Yew and Gim Hee Lee", "title": "3DFeat-Net: Weakly Supervised Local 3D Features for Point Cloud\n  Registration", "comments": "17 pages, 6 figures. Accepted in ECCV 2018", "journal-ref": null, "doi": "10.1007/978-3-030-01267-0_37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the 3DFeat-Net which learns both 3D feature\ndetector and descriptor for point cloud matching using weak supervision. Unlike\nmany existing works, we do not require manual annotation of matching point\nclusters. Instead, we leverage on alignment and attention mechanisms to learn\nfeature correspondences from GPS/INS tagged 3D point clouds without explicitly\nspecifying them. We create training and benchmark outdoor Lidar datasets, and\nexperiments show that 3DFeat-Net obtains state-of-the-art performance on these\ngravity-aligned datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 01:56:21 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Yew", "Zi Jian", ""], ["Lee", "Gim Hee", ""]]}, {"id": "1807.09418", "submitter": "Junnan Li Dr", "authors": "Junnan Li, Yongkang Wong, Qi Zhao, Mohan S. Kankanhalli", "title": "Video Storytelling: Textual Summaries for Events", "comments": "Published in IEEE Transactions on Multimedia", "journal-ref": "J. Li, Y. Wong, Q. Zhao and M. S. Kankanhalli, \"Video\n  Storytelling: Textual Summaries for Events,\" in IEEE Transactions on\n  Multimedia, 2019", "doi": "10.1109/TMM.2019.2930041", "report-no": null, "categories": "cs.MM cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridging vision and natural language is a longstanding goal in computer\nvision and multimedia research. While earlier works focus on generating a\nsingle-sentence description for visual content, recent works have studied\nparagraph generation. In this work, we introduce the problem of video\nstorytelling, which aims at generating coherent and succinct stories for long\nvideos. Video storytelling introduces new challenges, mainly due to the\ndiversity of the story and the length and complexity of the video. We propose\nnovel methods to address the challenges. First, we propose a context-aware\nframework for multimodal embedding learning, where we design a Residual\nBidirectional Recurrent Neural Network to leverage contextual information from\npast and future. Second, we propose a Narrator model to discover the underlying\nstoryline. The Narrator is formulated as a reinforcement learning agent which\nis trained by directly optimizing the textual metric of the generated story. We\nevaluate our method on the Video Story dataset, a new dataset that we have\ncollected to enable the study. We compare our method with multiple\nstate-of-the-art baselines, and show that our method achieves better\nperformance, in terms of quantitative measures and user study.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 02:43:19 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 10:21:46 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 12:39:48 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Li", "Junnan", ""], ["Wong", "Yongkang", ""], ["Zhao", "Qi", ""], ["Kankanhalli", "Mohan S.", ""]]}, {"id": "1807.09430", "submitter": "Md Amirul Islam", "authors": "Md Amirul Islam, Mahmoud Kalash, and Neil D.B. Bruce", "title": "Semantics Meet Saliency: Exploring Domain Affinity and Models for\n  Dual-Task Prediction", "comments": "BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much research has examined models for prediction of semantic labels or\ninstances including dense pixel-wise prediction. The problem of predicting\nsalient objects or regions of an image has also been examined in a similar\nlight. With that said, there is an apparent relationship between these two\nproblem domains in that the composition of a scene and associated semantic\ncategories is certain to play into what is deemed salient. In this paper, we\nexplore the relationship between these two problem domains. This is carried out\nin constructing deep neural networks that perform both predictions together\nalbeit with different configurations for flow of conceptual information related\nto each distinct problem. This is accompanied by a detailed analysis of object\nco-occurrences that shed light on dataset bias and semantic precedence specific\nto individual categories.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 04:07:17 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Islam", "Md Amirul", ""], ["Kalash", "Mahmoud", ""], ["Bruce", "Neil D. B.", ""]]}, {"id": "1807.09434", "submitter": "Boeun Kim", "authors": "Boeun Kim, Young Han Lee, Hyedong Jung and Choongsang Cho", "title": "Distinctive-attribute Extraction for Image Captioning", "comments": "14 main pages, 4 supplementary pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image captioning, an open research issue, has been evolved with the progress\nof deep neural networks. Convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs) are employed to compute image features and generate\nnatural language descriptions in the research. In previous works, a caption\ninvolving semantic description can be generated by applying additional\ninformation into the RNNs. In this approach, we propose a distinctive-attribute\nextraction (DaE) which explicitly encourages significant meanings to generate\nan accurate caption describing the overall meaning of the image with their\nunique situation. Specifically, the captions of training images are analyzed by\nterm frequency-inverse document frequency (TF-IDF), and the analyzed semantic\ninformation is trained to extract distinctive-attributes for inferring\ncaptions. The proposed scheme is evaluated on a challenge data, and it improves\nan objective performance while describing images in more detail.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 04:34:17 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Kim", "Boeun", ""], ["Lee", "Young Han", ""], ["Jung", "Hyedong", ""], ["Cho", "Choongsang", ""]]}, {"id": "1807.09436", "submitter": "Zhipeng Cai", "authors": "Zhipeng Cai, Tat-Jun Chin, Huu Le, David Suter", "title": "Deterministic consensus maximization with biconvex programming", "comments": "European Conference on Computer Vision (ECCV) 2018, oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus maximization is one of the most widely used robust fitting\nparadigms in computer vision, and the development of algorithms for consensus\nmaximization is an active research topic. In this paper, we propose an\nefficient deterministic optimization algorithm for consensus maximization.\nGiven an initial solution, our method conducts a deterministic search that\nforcibly increases the consensus of the initial solution. We show how each\niteration of the update can be formulated as an instance of biconvex\nprogramming, which we solve efficiently using a novel biconvex optimization\nalgorithm. In contrast to our algorithm, previous consensus improvement\ntechniques rely on random sampling or relaxations of the objective function,\nwhich reduce their ability to significantly improve the initial consensus. In\nfact, on challenging instances, the previous techniques may even return a worse\noff solution. Comprehensive experiments show that our algorithm can\nconsistently and greatly improve the quality of the initial solution, without\nsubstantial cost.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 04:54:57 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 08:27:53 GMT"}, {"version": "v3", "created": "Mon, 3 Dec 2018 01:59:01 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Cai", "Zhipeng", ""], ["Chin", "Tat-Jun", ""], ["Le", "Huu", ""], ["Suter", "David", ""]]}, {"id": "1807.09441", "submitter": "Xingang Pan", "authors": "Xingang Pan, Ping Luo, Jianping Shi, Xiaoou Tang", "title": "Two at Once: Enhancing Learning and Generalization Capacities via\n  IBN-Net", "comments": "Accepted for publication at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) have achieved great successes in many\ncomputer vision problems. Unlike existing works that designed CNN architectures\nto improve performance on a single task of a single domain and not\ngeneralizable, we present IBN-Net, a novel convolutional architecture, which\nremarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as\nwell as its generalization capacity on another domain (e.g. GTA5) without\nfinetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch\nNormalization (BN) as building blocks, and can be wrapped into many advanced\ndeep networks to improve their performances. This work has three key\ncontributions. (1) By delving into IN and BN, we disclose that IN learns\nfeatures that are invariant to appearance changes, such as colors, styles, and\nvirtuality/reality, while BN is essential for preserving content related\ninformation. (2) IBN-Net can be applied to many advanced deep architectures,\nsuch as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their\nperformance without increasing computational cost. (3) When applying the\ntrained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves\ncomparable improvements as domain adaptation methods, even without using data\nfrom the target domain. With IBN-Net, we won the 1st place on the WAD 2018\nChallenge Drivable Area track, with an mIoU of 86.18%.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 05:51:15 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:03:23 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 03:31:11 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Pan", "Xingang", ""], ["Luo", "Ping", ""], ["Shi", "Jianping", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1807.09480", "submitter": "Marco Cannici", "authors": "Marco Cannici, Marco Ciccone, Andrea Romanoni, Matteo Matteucci", "title": "Attention Mechanisms for Object Recognition with Event-Based Cameras", "comments": "WACV2019 camera-ready submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-based cameras are neuromorphic sensors capable of efficiently encoding\nvisual information in the form of sparse sequences of events. Being\nbiologically inspired, they are commonly used to exploit some of the\ncomputational and power consumption benefits of biological vision. In this\npaper we focus on a specific feature of vision: visual attention. We propose\ntwo attentive models for event based vision: an algorithm that tracks events\nactivity within the field of view to locate regions of interest and a\nfully-differentiable attention procedure based on DRAW neural model. We\nhighlight the strengths and weaknesses of the proposed methods on four\ndatasets, the Shifted N-MNIST, Shifted MNIST-DVS, CIFAR10-DVS and N-Caltech101\ncollections, using the Phased LSTM recognition network as a baseline reference\nmodel obtaining improvements in terms of both translation and scale invariance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 08:39:25 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 16:29:21 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Cannici", "Marco", ""], ["Ciccone", "Marco", ""], ["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1807.09499", "submitter": "Konstantin Shmelkov", "authors": "Konstantin Shmelkov, Cordelia Schmid, Karteek Alahari", "title": "How good is my GAN?", "comments": "Accepted to ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are one of the most popular methods\nfor generating images today. While impressive results have been validated by\nvisual inspection, a number of quantitative criteria have emerged only\nrecently. We argue here that the existing ones are insufficient and need to be\nin adequation with the task at hand. In this paper we introduce two measures\nbased on image classification---GAN-train and GAN-test, which approximate the\nrecall (diversity) and precision (quality of the image) of GANs respectively.\nWe evaluate a number of recent GAN approaches based on these two measures and\ndemonstrate a clear difference in performance. Furthermore, we observe that the\nincreasing difficulty of the dataset, from CIFAR10 over CIFAR100 to ImageNet,\nshows an inverse correlation with the quality of the GANs, as clearly evident\nfrom our measures.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 09:31:17 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Shmelkov", "Konstantin", ""], ["Schmid", "Cordelia", ""], ["Alahari", "Karteek", ""]]}, {"id": "1807.09528", "submitter": "Xiaofei Du Ms", "authors": "Hsueh-Fu Lu and Xiaofei Du and Ping-Lin Chang", "title": "Toward Scale-Invariance and Position-Sensitive Region Proposal Networks", "comments": "22 pages, 10 figures, accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately localising object proposals is an important precondition for high\ndetection rate for the state-of-the-art object detection frameworks. The\naccuracy of an object detection method has been shown highly related to the\naverage recall (AR) of the proposals. In this work, we propose an advanced\nobject proposal network in favour of translation-invariance for objectness\nclassification, translation-variance for bounding box regression, large\neffective receptive fields for capturing global context and scale-invariance\nfor dealing with a range of object sizes from extremely small to large. The\ndesign of the network architecture aims to be simple while being effective and\nwith real time performance. Without bells and whistles the proposed object\nproposal network significantly improves the AR at 1,000 proposals by $35\\%$ and\n$45\\%$ on PASCAL VOC and COCO dataset respectively and has a fast inference\ntime of 44.8 ms for input image size of $640^{2}$. Empirical studies have also\nshown that the proposed method is class-agnostic to be generalised for general\nobject proposal.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 11:01:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Lu", "Hsueh-Fu", ""], ["Du", "Xiaofei", ""], ["Chang", "Ping-Lin", ""]]}, {"id": "1807.09532", "submitter": "Qi Chen", "authors": "Qi Chen, Lei Wang, Yifan Wu, Guangming Wu, Zhiling Guo, Steven L.\n  Waslander", "title": "Aerial Imagery for Roof Segmentation: A Large-Scale Dataset towards\n  Automatic Mapping of Buildings", "comments": "arXiv admin note: This version has been removed as the user did not\n  have the right to agree to the license at the time of submission", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2018.11.011", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  arXiv admin note: This version has been removed as the user did not have the\nright to agree to the license at the time of submission\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 11:23:45 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 07:58:23 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Chen", "Qi", ""], ["Wang", "Lei", ""], ["Wu", "Yifan", ""], ["Wu", "Guangming", ""], ["Guo", "Zhiling", ""], ["Waslander", "Steven L.", ""]]}, {"id": "1807.09534", "submitter": "Ufuk Can Bi\\c{c}ici", "authors": "Ufuk Can Bi\\c{c}ici, Cem Keskin, Lale Akarun", "title": "Conditional Information Gain Networks", "comments": "ICPR 2018 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models owe their representational power to the high\nnumber of learnable parameters. It is often infeasible to run these largely\nparametrized deep models in limited resource environments, like mobile phones.\nNetwork models employing conditional computing are able to reduce computational\nrequirements while achieving high representational power, with their ability to\nmodel hierarchies. We propose Conditional Information Gain Networks, which\nallow the feed forward deep neural networks to execute conditionally, skipping\nparts of the model based on the sample and the decision mechanisms inserted in\nthe architecture. These decision mechanisms are trained using cost functions\nbased on differentiable Information Gain, inspired by the training procedures\nof decision trees. These information gain based decision mechanisms are\ndifferentiable and can be trained end-to-end using a unified framework with a\ngeneral cost function, covering both classification and decision losses. We\ntest the effectiveness of the proposed method on MNIST and recently introduced\nFashion MNIST datasets and show that our information gain based conditional\nexecution approach can achieve better or comparable classification results\nusing significantly fewer parameters, compared to standard convolutional neural\nnetwork baselines.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 11:26:46 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Bi\u00e7ici", "Ufuk Can", ""], ["Keskin", "Cem", ""], ["Akarun", "Lale", ""]]}, {"id": "1807.09536", "submitter": "Francisco M. Castro", "authors": "Francisco M. Castro, Manuel J. Mar\\'in-Jim\\'enez, Nicol\\'as Guil,\n  Cordelia Schmid, Karteek Alahari", "title": "End-to-End Incremental Learning", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning approaches have stood out in recent years due to their\nstate-of-the-art results, they continue to suffer from catastrophic forgetting,\na dramatic decrease in overall performance when training with new classes added\nincrementally. This is due to current neural network architectures requiring\nthe entire dataset, consisting of all the samples from the old as well as the\nnew classes, to update the model -a requirement that becomes easily\nunsustainable as the number of classes grows. We address this issue with our\napproach to learn deep neural networks incrementally, using new data and only a\nsmall exemplar set corresponding to samples from the old classes. This is based\non a loss composed of a distillation measure to retain the knowledge acquired\nfrom the old classes, and a cross-entropy loss to learn the new classes. Our\nincremental training is achieved while keeping the entire framework end-to-end,\ni.e., learning the data representation and the classifier jointly, unlike\nrecent methods with no such guarantees. We evaluate our method extensively on\nthe CIFAR-100 and ImageNet (ILSVRC 2012) image classification datasets, and\nshow state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 11:38:25 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 07:51:16 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Castro", "Francisco M.", ""], ["Mar\u00edn-Jim\u00e9nez", "Manuel J.", ""], ["Guil", "Nicol\u00e1s", ""], ["Schmid", "Cordelia", ""], ["Alahari", "Karteek", ""]]}, {"id": "1807.09546", "submitter": "Michael Ying Yang", "authors": "Zhenchao Zhang, Markus Gerke, George Vosselman, Michael Ying Yang", "title": "Patch-based Evaluation of Dense Image Matching Quality", "comments": "16 pages", "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation, 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Airborne laser scanning and photogrammetry are two main techniques to obtain\n3D data representing the object surface. Due to the high cost of laser\nscanning, we want to explore the potential of using point clouds derived by\ndense image matching (DIM), as effective alternatives to laser scanning data.\nWe present a framework to evaluate point clouds from dense image matching and\nderived Digital Surface Models (DSM) based on automatically extracted sample\npatches. Dense matching error and noise level are evaluated quantitatively at\nboth the local level and whole block level. Experiments show that the optimal\nvertical accuracy achieved by dense matching is as follows: the mean offset to\nthe reference data is 0.1 Ground Sampling Distance (GSD); the maximum offset\ngoes up to 1.0 GSD. When additional oblique images are used in dense matching,\nthe mean deviation, the variation of mean deviation and the level of random\nnoise all get improved. We also detect a bias between the point cloud and DSM\nfrom a single photogrammetric workflow. This framework also allows to reveal\ninhomogeneity in the distribution of the dense matching errors due to\nover-fitted BBA network. Meanwhile, suggestions are given on the\nphotogrammetric quality control.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 12:17:40 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhang", "Zhenchao", ""], ["Gerke", "Markus", ""], ["Vosselman", "George", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1807.09562", "submitter": "Michael Ying Yang", "authors": "Zhenchao Zhang, George Vosselman, Markus Gerke, Devis Tuia, Michael\n  Ying Yang", "title": "Change Detection between Multimodal Remote Sensing Data Using Siamese\n  CNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting topographic changes in the urban environment has always been an\nimportant task for urban planning and monitoring. In practice, remote sensing\ndata are often available in different modalities and at different time epochs.\nChange detection between multimodal data can be very challenging since the data\nshow different characteristics. Given 3D laser scanning point clouds and 2D\nimagery from different epochs, this paper presents a framework to detect\nbuilding and tree changes. First, the 2D and 3D data are transformed to image\npatches, respectively. A Siamese CNN is then employed to detect candidate\nchanges between the two epochs. Finally, the candidate patch-based changes are\ngrouped and verified as individual object changes. Experiments on the urban\ndata show that 86.4\\% of patch pairs can be correctly classified by the model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 13:00:42 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhang", "Zhenchao", ""], ["Vosselman", "George", ""], ["Gerke", "Markus", ""], ["Tuia", "Devis", ""], ["Yang", "Michael Ying", ""]]}, {"id": "1807.09601", "submitter": "Wei Ke", "authors": "Chang Liu and Wei Ke and Fei Qin and Qixiang Ye", "title": "Linear Span Network for Object Skeleton Detection", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust object skeleton detection requires to explore rich representative\nvisual features and effective feature fusion strategies. In this paper, we\nfirst re-visit the implementation of HED, the essential principle of which can\nbe ideally described with a linear reconstruction model. Hinted by this, we\nformalize a Linear Span framework, and propose Linear Span Network (LSN)\nmodified by Linear Span Units (LSUs), which minimize the reconstruction error\nof convolutional network. LSN further utilizes subspace linear span beside the\nfeature linear span to increase the independence of convolutional features and\nthe efficiency of feature integration, which enlarges the capability of fitting\ncomplex ground-truth. As a result, LSN can effectively suppress the cluttered\nbackgrounds and reconstruct object skeletons. Experimental results validate the\nstate-of-the-art performance of the proposed LSN.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 13:45:29 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Liu", "Chang", ""], ["Ke", "Wei", ""], ["Qin", "Fei", ""], ["Ye", "Qixiang", ""]]}, {"id": "1807.09607", "submitter": "Feng Gu", "authors": "Feng Gu, Nikolay Burlutskiy, Mats Andersson and Lena Kajland Wilen", "title": "Multi-Resolution Networks for Semantic Segmentation in Whole Slide\n  Images", "comments": "Accepted by MICCAI COMPAY 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital pathology provides an excellent opportunity for applying fully\nconvolutional networks (FCNs) to tasks, such as semantic segmentation of whole\nslide images (WSIs). However, standard FCNs face challenges with respect to\nmulti-resolution, inherited from the pyramid arrangement of WSIs. As a result,\nnetworks specifically designed to learn and aggregate information at different\nlevels are desired. In this paper, we propose two novel multi-resolution\nnetworks based on the popular `U-Net' architecture, which are evaluated on a\nbenchmark dataset for binary semantic segmentation in WSIs. The proposed\nmethods outperform the U-Net, demonstrating superior learning and\ngeneralization capabilities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 13:54:11 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Gu", "Feng", ""], ["Burlutskiy", "Nikolay", ""], ["Andersson", "Mats", ""], ["Wilen", "Lena Kajland", ""]]}, {"id": "1807.09610", "submitter": "Hamid Shahdoosti", "authors": "Hamid Reza Shahdoosti", "title": "Improved Adaptive Brovey as a New Method for Image Fusion", "comments": "6 pages, 3 figures, journal paper. arXiv admin note: text overlap\n  with arXiv:1701.01996, arXiv:1807.08917", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ideal fusion method preserves the Spectral information in fused image and\nadds spatial information to it with no spectral distortion. Among the existing\nfusion algorithms, the contourlet-based fusion method is the most frequently\ndiscussed one in recent publications, because the contourlet has the ability to\ncapture and link the point of discontinuities to form a linear structure. The\nBrovey is a popular pan-sharpening method owing to its efficiency and high\nspatial resolution. This method can be explained by mathematical model of\noptical remote sensing sensors. This study presents a new fusion approach that\nintegrates the advantages of both the Brovey and the cotourlet techniques to\nreduce the color distortion of fusion results. Visual and statistical analyzes\nshow that the proposed algorithm clearly improves the merging quality in terms\nof: correlation coefficient, ERGAS, UIQI, and Q4; compared to fusion methods\nincluding IHS, PCA, Adaptive IHS, and Improved Adaptive PCA.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:35:55 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Shahdoosti", "Hamid Reza", ""]]}, {"id": "1807.09619", "submitter": "Paulo Guilherme De Lima Freire", "authors": "Paulo G. L. Freire, Ricardo J. Ferrari", "title": "Multiple sclerosis lesion enhancement and white matter region estimation\n  using hyperintensities in FLAIR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple sclerosis (MS) is a demyelinating disease that affects more than 2\nmillion people worldwide. The most used imaging technique to help in its\ndiagnosis and follow-up is magnetic resonance imaging (MRI). Fluid Attenuated\nInversion Recovery (FLAIR) images are usually acquired in the context of MS\nbecause lesions often appear hyperintense in this particular image weight,\nmaking it easier for physicians to identify them. Though lesions have a bright\nintensity profile, it may overlap with white matter (WM) and gray matter (GM)\ntissues, posing difficulties to be accurately segmented. In this sense, we\npropose a lesion enhancement technique to dim down WM and GM regions and\nhighlight hyperintensities, making them much more distinguishable than other\ntissues. We applied our technique to the ISBI 2015 MS Lesion Segmentation\nChallenge and took the average gray level intensity of MS lesions, WM and GM on\nFLAIR and enhanced images. The lesion intensity profile in FLAIR was on average\n25% and 19% brighter than white matter and gray matter, respectively;\ncomparatively, the same profile in our enhanced images was on average 444% and\n264% brighter. Such results mean a significant improvement on the intensity\ndistinction among these three clusters, which may come as aid both for experts\nand automated techniques. Moreover, a byproduct of our proposal is that the\nenhancement can be used to automatically estimate a mask encompassing WM and MS\nlesions, which may be useful for brain tissue volume assessment and improve MS\nlesion segmentation accuracy in future works.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 14:06:03 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Freire", "Paulo G. L.", ""], ["Ferrari", "Ricardo J.", ""]]}, {"id": "1807.09620", "submitter": "Nikolaos Zioulis Mr.", "authors": "Nikolaos Zioulis and Antonis Karakottas and Dimitrios Zarpalas and\n  Petros Daras", "title": "OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas", "comments": "Pre-print to appear in ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent work on depth estimation up to now has only focused on projective\nimages ignoring 360 content which is now increasingly and more easily produced.\nWe show that monocular depth estimation models trained on traditional images\nproduce sub-optimal results on omnidirectional images, showcasing the need for\ntraining directly on 360 datasets, which however, are hard to acquire. In this\nwork, we circumvent the challenges associated with acquiring high quality 360\ndatasets with ground truth depth annotations, by re-using recently released\nlarge scale 3D datasets and re-purposing them to 360 via rendering. This\ndataset, which is considerably larger than similar projective datasets, is\npublicly offered to the community to enable future research in this direction.\nWe use this dataset to learn in an end-to-end fashion the task of depth\nestimation from 360 images. We show promising results in our synthesized data\nas well as in unseen realistic images.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 14:06:10 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zioulis", "Nikolaos", ""], ["Karakottas", "Antonis", ""], ["Zarpalas", "Dimitrios", ""], ["Daras", "Petros", ""]]}, {"id": "1807.09627", "submitter": "Carolina Raposo", "authors": "Carolina Raposo, Cristovao Sousa, Luis Ribeiro, Rui Melo, Joao P.\n  Barreto, Joao Oliveira, Pedro Marques and Fernando Fonseca", "title": "Video-based computer aided arthroscopy for patient specific\n  reconstruction of the Anterior Cruciate Ligament", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Anterior Cruciate Ligament (ACL) tear is a common medical condition that\nis treated using arthroscopy by pulling a tissue graft through a tunnel opened\nwith a drill. The correct anatomical position and orientation of this tunnel is\ncrucial for knee stability, and drilling an adequate bone tunnel is the most\ntechnically challenging part of the procedure. This paper presents, for the\nfirst time, a guidance system based solely on intra-operative video for guiding\nthe drilling of the tunnel. Our solution uses small, easily recognizable visual\nmarkers that are attached to the bone and tools for estimating their relative\npose. A recent registration algorithm is employed for aligning a pre-operative\nimage of the patient's anatomy with a set of contours reconstructed by touching\nthe bone surface with an instrumented tool. Experimental validation using\nex-vivo data shows that the method enables the accurate registration of the\npre-operative model with the bone, providing useful information for guiding the\nsurgeon during the medical procedure.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 14:22:38 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Raposo", "Carolina", ""], ["Sousa", "Cristovao", ""], ["Ribeiro", "Luis", ""], ["Melo", "Rui", ""], ["Barreto", "Joao P.", ""], ["Oliveira", "Joao", ""], ["Marques", "Pedro", ""], ["Fonseca", "Fernando", ""]]}, {"id": "1807.09659", "submitter": "Qianli Liao", "authors": "Qianli Liao, Brando Miranda, Andrzej Banburski, Jack Hidary, Tomaso\n  Poggio", "title": "A Surprising Linear Relationship Predicts Test Performance in Deep\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two networks with the same training loss on a dataset, when would they\nhave drastically different test losses and errors? Better understanding of this\nquestion of generalization may improve practical applications of deep networks.\nIn this paper we show that with cross-entropy loss it is surprisingly simple to\ninduce significantly different generalization performances for two networks\nthat have the same architecture, the same meta parameters and the same training\nerror: one can either pretrain the networks with different levels of\n\"corrupted\" data or simply initialize the networks with weights of different\nGaussian standard deviations. A corollary of recent theoretical results on\noverfitting shows that these effects are due to an intrinsic problem of\nmeasuring test performance with a cross-entropy/exponential-type loss, which\ncan be decomposed into two components both minimized by SGD -- one of which is\nnot related to expected classification performance. However, if we factor out\nthis component of the loss, a linear relationship emerges between training and\ntest losses. Under this transformation, classical generalization bounds are\nsurprisingly tight: the empirical/training loss is very close to the\nexpected/test loss. Furthermore, the empirical relation between classification\nerror and normalized cross-entropy loss seem to be approximately monotonic\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 15:20:02 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Liao", "Qianli", ""], ["Miranda", "Brando", ""], ["Banburski", "Andrzej", ""], ["Hidary", "Jack", ""], ["Poggio", "Tomaso", ""]]}, {"id": "1807.09664", "submitter": "Khimya Khetarpal", "authors": "Khimya Khetarpal, Doina Precup", "title": "Attend Before you Act: Leveraging human visual attention for continual\n  learning", "comments": "Lifelong Learning: A Reinforcement Learning Approach (LLARLA)\n  Workshop, ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans perform a task, such as playing a game, they selectively pay\nattention to certain parts of the visual input, gathering relevant information\nand sequentially combining it to build a representation from the sensory data.\nIn this work, we explore leveraging where humans look in an image as an\nimplicit indication of what is salient for decision making. We build on top of\nthe UNREAL architecture in DeepMind Lab's 3D navigation maze environment. We\ntrain the agent both with original images and foveated images, which were\ngenerated by overlaying the original images with saliency maps generated using\na real-time spectral residual technique. We investigate the effectiveness of\nthis approach in transfer learning by measuring performance in the context of\nnoise in the environment.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 15:23:44 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Khetarpal", "Khimya", ""], ["Precup", "Doina", ""]]}, {"id": "1807.09666", "submitter": "Matthieu Ospici", "authors": "Matthieu Ospici, Antoine Cecchi", "title": "Person re-identification across different datasets with multi-task\n  learning", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to tackle the re-identification problem. This\nis a challenging problem due to the large variation of pose, illumination or\ncamera view. More and more datasets are available to train machine learning\nmodels for person re-identification. These datasets vary in conditions: cameras\nnumbers, camera positions, location, season, in size, i.e. number of images,\nnumber of different identities. Finally in labeling: there are datasets\nannotated with attributes while others are not. To deal with this variety of\ndatasets we present in this paper an approach to take information from\ndifferent datasets to build a system which performs well on all of them. Our\nmodel is based on a Convolutional Neural Network (CNN) and trained using\nmultitask learning. Several losses are used to extract the different\ninformation available in the different datasets. Our main task is learned with\na classification loss. To reduce the intra-class variation we experiment with\nthe center loss. Our paper ends with a performance evaluation in which we\ndiscuss the influence of the different losses on the global re-identification\nperformance. We show that with our method, we are able to build a system that\nperforms well on different datasets and simultaneously extracts attributes. We\nalso show that our system outperforms recent re-identification works on two\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 15:27:32 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ospici", "Matthieu", ""], ["Cecchi", "Antoine", ""]]}, {"id": "1807.09685", "submitter": "Lisa Anne Hendricks", "authors": "Lisa Anne Hendricks, Ronghang Hu, Trevor Darrell, Zeynep Akata", "title": "Grounding Visual Explanations", "comments": "Accepted to ECCV 2018", "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing visual explanation generating agents learn to fluently justify a\nclass prediction. However, they may mention visual attributes which reflect a\nstrong class prior, although the evidence may not actually be in the image.\nThis is particularly concerning as ultimately such agents fail in building\ntrust with human users. To overcome this limitation, we propose a phrase-critic\nmodel to refine generated candidate explanations augmented with flipped phrases\nwhich we use as negative examples while training. At inference time, our\nphrase-critic model takes an image and a candidate explanation as input and\noutputs a score indicating how well the candidate explanation is grounded in\nthe image. Our explainable AI agent is capable of providing counter arguments\nfor an alternative prediction, i.e. counterfactuals, along with explanations\nthat justify the correct classification decisions. Our model improves the\ntextual explanation quality of fine-grained classification decisions on the CUB\ndataset by mentioning phrases that are grounded in the image. Moreover, on the\nFOIL tasks, our agent detects when there is a mistake in the sentence, grounds\nthe incorrect phrase and corrects it significantly better than other models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:03:35 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 04:53:30 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Hendricks", "Lisa Anne", ""], ["Hu", "Ronghang", ""], ["Darrell", "Trevor", ""], ["Akata", "Zeynep", ""]]}, {"id": "1807.09700", "submitter": "Denis Feurer", "authors": "Denis Feurer and Fabrice Vinatier", "title": "The Time-SIFT method : detecting 3-D changes from archival\n  photogrammetric analysis with almost exclusively image information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Archival aerial imagery is a source of worldwide very high resolution data\nfor documenting paste 3-D changes. However, external information is required so\nthat accurate 3-D models can be computed from archival aerial imagery. In this\nresearch, we propose and test a new method, termed Time-SIFT (Scale Invariant\nFeature Transform), which allows for computing coherent multi-temporal Digital\nElevation Models (DEMs) with almost exclusively image information. This method\nis based on the invariance properties of the SIFT-like methods which are at the\nroot of the Structure from Motion (SfM) algorithms. On a test site of 170 km2,\nwe applied SfM algorithms to a unique image block with all the images of four\ndifferent dates covering forty years. We compared this method to more classical\nmethods based on the use of affordable additional data such as ground control\npoints collected in recent orthophotos. We did extensive tests to determine\nwhich processing choices were most impacting on the final result. With these\ntests, we aimed at evaluating the potential of the proposed Time-SIFT method\nfor the detection and mapping of 3-D changes. Our study showed that the\nTime-SIFT method was the prime criteria that allowed for computing informative\nDEMs of difference with almost exclusively image information and limited\nphotogrammetric expertise and human intervention. Due to the fact that the\nproposed Time-SIFT method can be automatically applied with exclusively image\ninformation, our results pave the way to a systematic processing of the\narchival aerial imagery on very large spatio-temporal windows, and should hence\ngreatly help the unlocking of archival aerial imagery for the documenting of\npast 3-D changes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:28:05 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Feurer", "Denis", ""], ["Vinatier", "Fabrice", ""]]}, {"id": "1807.09713", "submitter": "Guillermo Gallego", "authors": "Daniel Gehrig, Henri Rebecq, Guillermo Gallego, Davide Scaramuzza", "title": "Asynchronous, Photometric Feature Tracking using Events and Frames", "comments": "22 pages, 15 figures, Video: https://youtu.be/A7UfeUnG6c4", "journal-ref": "European Conference on Computer Vision (ECCV), Munich, 2018", "doi": "10.1007/978-3-030-01258-8_46", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that leverages the complementarity of event cameras and\nstandard cameras to track visual features with low-latency. Event cameras are\nnovel sensors that output pixel-level brightness changes, called \"events\". They\noffer significant advantages over standard cameras, namely a very high dynamic\nrange, no motion blur, and a latency in the order of microseconds. However,\nbecause the same scene pattern can produce different events depending on the\nmotion direction, establishing event correspondences across time is\nchallenging. By contrast, standard cameras provide intensity measurements\n(frames) that do not depend on motion direction. Our method extracts features\non frames and subsequently tracks them asynchronously using events, thereby\nexploiting the best of both types of data: the frames provide a photometric\nrepresentation that does not depend on motion direction and the events provide\nlow-latency updates. In contrast to previous works, which are based on\nheuristics, this is the first principled method that uses raw intensity\nmeasurements directly, based on a generative event model within a\nmaximum-likelihood framework. As a result, our method produces feature tracks\nthat are both more accurate (subpixel accuracy) and longer than the state of\nthe art, across a wide variety of scenes.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:40:05 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Gehrig", "Daniel", ""], ["Rebecq", "Henri", ""], ["Gallego", "Guillermo", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "1807.09715", "submitter": "Charles Ringer", "authors": "Charles Ringer and Mihalis A. Nicolaou", "title": "Deep Unsupervised Multi-View Detection of Video Game Stream Highlights", "comments": "Foundation of Digital Games 2018, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatic highlight-detection in video game\nstreams. Currently, the vast majority of highlight-detection systems for games\nare triggered by the occurrence of hard-coded game events (e.g., score change,\nend-game), while most advanced tools and techniques are based on detection of\nhighlights via visual analysis of game footage. We argue that in the context of\ngame streaming, events that may constitute highlights are not only dependent on\ngame footage, but also on social signals that are conveyed by the streamer\nduring the play session (e.g., when interacting with viewers, or when\ncommenting and reacting to the game). In this light, we present a multi-view\nunsupervised deep learning methodology for novelty-based highlight detection.\nThe method jointly analyses both game footage and social signals such as the\nplayers facial expressions and speech, and shows promising results for\ngenerating highlights on streams of popular games such as Player Unknown's\nBattlegrounds.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:41:12 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ringer", "Charles", ""], ["Nicolaou", "Mihalis A.", ""]]}, {"id": "1807.09755", "submitter": "Yijun Li", "authors": "Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, Ming-Hsuan Yang", "title": "Flow-Grounded Spatial-Temporal Video Prediction from Still Images", "comments": "Accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video prediction methods mainly rely on observing multiple\nhistorical frames or focus on predicting the next one-frame. In this work, we\nstudy the problem of generating consecutive multiple future frames by observing\none single still image only. We formulate the multi-frame prediction task as a\nmultiple time step flow (multi-flow) prediction phase followed by a\nflow-to-frame synthesis phase. The multi-flow prediction is modeled in a\nvariational probabilistic manner with spatial-temporal relationships learned\nthrough 3D convolutions. The flow-to-frame synthesis is modeled as a generative\nprocess in order to keep the predicted results lying closer to the manifold\nshape of real video sequence. Such a two-phase design prevents the model from\ndirectly looking at the high-dimensional pixel space of the frame sequence and\nis demonstrated to be more effective in predicting better and diverse results.\nExtensive experimental results on videos with different types of motion show\nthat the proposed algorithm performs favorably against existing methods in\nterms of quality, diversity and human perceptual evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 17:56:33 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 04:35:38 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Li", "Yijun", ""], ["Fang", "Chen", ""], ["Yang", "Jimei", ""], ["Wang", "Zhaowen", ""], ["Lu", "Xin", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1807.09810", "submitter": "Abhimanyu Dubey", "authors": "Abhimanyu Dubey, Moitreya Chatterjee, Narendra Ahuja", "title": "Coreset-Based Neural Network Compression", "comments": "Camera-Ready version for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Convolutional Neural Network (CNN) compression algorithm\nbased on coreset representations of filters. We exploit the redundancies extant\nin the space of CNN weights and neuronal activations (across samples) in order\nto obtain compression. Our method requires no retraining, is easy to implement,\nand obtains state-of-the-art compression performance across a wide variety of\nCNN architectures. Coupled with quantization and Huffman coding, we create\nnetworks that provide AlexNet-like accuracy, with a memory footprint that is\n$832\\times$ smaller than the original AlexNet, while also introducing\nsignificant reductions in inference time as well. Additionally these compressed\nnetworks when fine-tuned, successfully generalize to other domains as well.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 18:26:49 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Dubey", "Abhimanyu", ""], ["Chatterjee", "Moitreya", ""], ["Ahuja", "Narendra", ""]]}, {"id": "1807.09828", "submitter": "Arno Solin", "authors": "Santiago Cort\\'es, Arno Solin, Esa Rahtu, Juho Kannala", "title": "ADVIO: An authentic dataset for visual-inertial odometry", "comments": "To appear in European Conference on Computer Vision (ECCV)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of realistic and open benchmarking datasets for pedestrian\nvisual-inertial odometry has made it hard to pinpoint differences in published\nmethods. Existing datasets either lack a full six degree-of-freedom\nground-truth or are limited to small spaces with optical tracking systems. We\ntake advantage of advances in pure inertial navigation, and develop a set of\nversatile and challenging real-world computer vision benchmark sets for\nvisual-inertial odometry. For this purpose, we have built a test rig equipped\nwith an iPhone, a Google Pixel Android phone, and a Google Tango device. We\nprovide a wide range of raw sensor data that is accessible on almost any\nmodern-day smartphone together with a high-quality ground-truth track. We also\ncompare resulting visual-inertial tracks from Google Tango, ARCore, and Apple\nARKit with two recent methods published in academic forums. The data sets cover\nboth indoor and outdoor cases, with stairs, escalators, elevators, office\nenvironments, a shopping mall, and metro station.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 19:13:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Cort\u00e9s", "Santiago", ""], ["Solin", "Arno", ""], ["Rahtu", "Esa", ""], ["Kannala", "Juho", ""]]}, {"id": "1807.09834", "submitter": "Jo\\~ao Borrego", "authors": "Jo\\~ao Borrego, Atabak Dehban, Rui Figueiredo, Plinio Moreno,\n  Alexandre Bernardino and Jos\\'e Santos-Victor", "title": "Applying Domain Randomization to Synthetic Data for Object Category\n  Detection", "comments": "17 pages, 9 figures. Under review for ACCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep learning-based object detection techniques have\nrevolutionized their applicability in several fields. However, since these\nmethods rely on unwieldy and large amounts of data, a common practice is to\ndownload models pre-trained on standard datasets and fine-tune them for\nspecific application domains with a small set of domain relevant images. In\nthis work, we show that using synthetic datasets that are not necessarily\nphoto-realistic can be a better alternative to simply fine-tune pre-trained\nnetworks. Specifically, our results show an impressive 25% improvement in the\nmAP metric over a fine-tuning baseline when only about 200 labelled images are\navailable to train. Finally, an ablation study of our results is presented to\ndelineate the individual contribution of different components in the\nrandomization pipeline.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:08:57 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Borrego", "Jo\u00e3o", ""], ["Dehban", "Atabak", ""], ["Figueiredo", "Rui", ""], ["Moreno", "Plinio", ""], ["Bernardino", "Alexandre", ""], ["Santos-Victor", "Jos\u00e9", ""]]}, {"id": "1807.09848", "submitter": "Ahmet Iscen", "authors": "Ahmet Iscen, Ondrej Chum", "title": "Local Orthogonal-Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses approximate nearest neighbor search applied in the domain\nof large-scale image retrieval. Within the group testing framework we propose\nan efficient off-line construction of the search structures. The linear-time\ncomplexity orthogonal grouping increases the probability that at most one\nelement from each group is matching to a given query. Non-maxima suppression\nwith each group efficiently reduces the number of false positive results at no\nextra cost. Unlike in other well-performing approaches, all processing is\nlocal, fast, and suitable to process data in batches and in parallel. We\nexperimentally show that the proposed method achieves search accuracy of the\nexhaustive search with significant reduction in the search complexity. The\nmethod can be naturally combined with existing embedding methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 20:49:34 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 11:03:43 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Iscen", "Ahmet", ""], ["Chum", "Ondrej", ""]]}, {"id": "1807.09856", "submitter": "Issam Hadj Laradji", "authors": "Issam H. Laradji, Negar Rostamzadeh, Pedro O. Pinheiro, David Vazquez,\n  Mark Schmidt", "title": "Where are the Blobs: Counting by Localization with Point Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object counting is an important task in computer vision due to its growing\ndemand in applications such as surveillance, traffic monitoring, and counting\neveryday objects. State-of-the-art methods use regression-based optimization\nwhere they explicitly learn to count the objects of interest. These often\nperform better than detection-based methods that need to learn the more\ndifficult task of predicting the location, size, and shape of each object.\nHowever, we propose a detection-based method that does not need to estimate the\nsize and shape of the objects and that outperforms regression-based methods.\nOur contributions are three-fold: (1) we propose a novel loss function that\nencourages the network to output a single blob per object instance using\npoint-level annotations only; (2) we design two methods for splitting large\npredicted blobs between object instances; and (3) we show that our method\nachieves new state-of-the-art results on several challenging datasets including\nthe Pascal VOC and the Penguins dataset. Our method even outperforms those that\nuse stronger supervision such as depth features, multi-point annotations, and\nbounding-box labels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 21:00:09 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Laradji", "Issam H.", ""], ["Rostamzadeh", "Negar", ""], ["Pinheiro", "Pedro O.", ""], ["Vazquez", "David", ""], ["Schmidt", "Mark", ""]]}, {"id": "1807.09882", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Adriana Kovashka", "title": "Persuasive Faces: Generating Faces in Advertisements", "comments": null, "journal-ref": "In British Machine Vision Conference (BMVC), Newcastle upon Tyne,\n  UK, September 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the visual variability of objects across different\nad categories, i.e. what causes an advertisement to be visually persuasive. We\nfocus on modeling and generating faces which appear to come from different\ntypes of ads. For example, if faces in beauty ads tend to be women wearing\nlipstick, a generative model should portray this distinct visual appearance.\nTraining generative models which capture such category-specific differences is\nchallenging because of the highly diverse appearance of faces in ads and the\nrelatively limited amount of available training data. To address these\nproblems, we propose a conditional variational autoencoder which makes use of\npredicted semantic attributes and facial expressions as a supervisory signal\nwhen training. We show how our model can be used to produce visually distinct\nfaces which appear to be from a fixed ad topic category. Our human studies and\nquantitative and qualitative experiments confirm that our method greatly\noutperforms a variety of baselines, including two variations of a\nstate-of-the-art generative adversarial network, for transforming faces to be\nmore ad-category appropriate. Finally, we show preliminary generation results\nfor other types of objects, conditioned on an ad topic.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:21:53 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Thomas", "Christopher", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1807.09915", "submitter": "Qi Zheng", "authors": "Chaojian Yu, Xinyi Zhao, Qi Zheng, Peng Zhang, Xinge You", "title": "Hierarchical Bilinear Pooling for Fine-Grained Visual Recognition", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fine-grained visual recognition is challenging because it highly relies on\nthe modeling of various semantic parts and fine-grained feature learning.\nBilinear pooling based models have been shown to be effective at fine-grained\nrecognition, while most previous approaches neglect the fact that inter-layer\npart feature interaction and fine-grained feature learning are mutually\ncorrelated and can reinforce each other. In this paper, we present a novel\nmodel to address these issues. First, a cross-layer bilinear pooling approach\nis proposed to capture the inter-layer part feature relations, which results in\nsuperior performance compared with other bilinear pooling based approaches.\nSecond, we propose a novel hierarchical bilinear pooling framework to integrate\nmultiple cross-layer bilinear features to enhance their representation\ncapability. Our formulation is intuitive, efficient and achieves\nstate-of-the-art results on the widely used fine-grained recognition datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 01:46:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Yu", "Chaojian", ""], ["Zhao", "Xinyi", ""], ["Zheng", "Qi", ""], ["Zhang", "Peng", ""], ["You", "Xinge", ""]]}, {"id": "1807.09930", "submitter": "Jun Xu", "authors": "Jun Xu, Mengyang Yu, Ling Shao, Wangmeng Zuo, Deyu Meng, Lei Zhang,\n  David Zhang", "title": "Scaled Simplex Representation for Subspace Clustering", "comments": "Accepted by IEEE Transactions on Cybernetics. 13 pages, 9 figures, 10\n  tables. Code can be found at https://github.com/csjunxu/SSRSC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-expressive property of data points, i.e., each data point can be\nlinearly represented by the other data points in the same subspace, has proven\neffective in leading subspace clustering methods. Most self-expressive methods\nusually construct a feasible affinity matrix from a coefficient matrix,\nobtained by solving an optimization problem. However, the negative entries in\nthe coefficient matrix are forced to be positive when constructing the affinity\nmatrix via exponentiation, absolute symmetrization, or squaring operations.\nThis consequently damages the inherent correlations among the data. Besides,\nthe affine constraint used in these methods is not flexible enough for\npractical applications. To overcome these problems, in this paper, we introduce\na scaled simplex representation (SSR) for subspace clustering problem.\nSpecifically, the non-negative constraint is used to make the coefficient\nmatrix physically meaningful, and the coefficient vector is constrained to be\nsummed up to a scalar s<1 to make it more discriminative. The proposed SSR\nbased subspace clustering (SSRSC) model is reformulated as a linear\nequality-constrained problem, which is solved efficiently under the alternating\ndirection method of multipliers framework. Experiments on benchmark datasets\ndemonstrate that the proposed SSRSC algorithm is very efficient and outperforms\nstate-of-the-art subspace clustering methods on accuracy. The code can be found\nat https://github.com/csjunxu/SSRSC.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 02:51:27 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 17:23:48 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 12:23:37 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Xu", "Jun", ""], ["Yu", "Mengyang", ""], ["Shao", "Ling", ""], ["Zuo", "Wangmeng", ""], ["Meng", "Deyu", ""], ["Zhang", "Lei", ""], ["Zhang", "David", ""]]}, {"id": "1807.09937", "submitter": "Jiren Zhu", "authors": "Jiren Zhu, Russell Kaplan, Justin Johnson and Li Fei-Fei", "title": "HiDDeN: Hiding Data With Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that deep neural networks are highly sensitive to tiny\nperturbations of input images, giving rise to adversarial examples. Though this\nproperty is usually considered a weakness of learned models, we explore whether\nit can be beneficial. We find that neural networks can learn to use invisible\nperturbations to encode a rich amount of useful information. In fact, one can\nexploit this capability for the task of data hiding. We jointly train encoder\nand decoder networks, where given an input message and cover image, the encoder\nproduces a visually indistinguishable encoded image, from which the decoder can\nrecover the original message. We show that these encodings are competitive with\nexisting data hiding algorithms, and further that they can be made robust to\nnoise: our models learn to reconstruct hidden information in an encoded image\ndespite the presence of Gaussian blurring, pixel-wise dropout, cropping, and\nJPEG compression. Even though JPEG is non-differentiable, we show that a robust\nmodel can be trained using differentiable approximations. Finally, we\ndemonstrate that adversarial training improves the visual quality of encoded\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 03:25:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhu", "Jiren", ""], ["Kaplan", "Russell", ""], ["Johnson", "Justin", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1807.09940", "submitter": "Shuhan Chen", "authors": "Shuhan Chen, Xiuli Tan, Ben Wang, Xuelong Hu", "title": "Reverse Attention for Salient Object Detection", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefit from the quick development of deep learning techniques, salient\nobject detection has achieved remarkable progresses recently. However, there\nstill exists following two major challenges that hinder its application in\nembedded devices, low resolution output and heavy model weight. To this end,\nthis paper presents an accurate yet compact deep network for efficient salient\nobject detection. More specifically, given a coarse saliency prediction in the\ndeepest layer, we first employ residual learning to learn side-output residual\nfeatures for saliency refinement, which can be achieved with very limited\nconvolutional parameters while keep accuracy. Secondly, we further propose\nreverse attention to guide such side-output residual learning in a top-down\nmanner. By erasing the current predicted salient regions from side-output\nfeatures, the network can eventually explore the missing object parts and\ndetails which results in high resolution and accuracy. Experiments on six\nbenchmark datasets demonstrate that the proposed approach compares favorably\nagainst state-of-the-art methods, and with advantages in terms of simplicity,\nefficiency (45 FPS) and model size (81 MB).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 03:30:57 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 14:46:46 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Chen", "Shuhan", ""], ["Tan", "Xiuli", ""], ["Wang", "Ben", ""], ["Hu", "Xuelong", ""]]}, {"id": "1807.09946", "submitter": "Avanti Shrikumar", "authors": "Avanti Shrikumar, Jocelin Su, Anshul Kundaje", "title": "Computationally Efficient Measures of Internal Neuron Importance", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of assigning importance to individual neurons in a network is\nof interest when interpreting deep learning models. In recent work, Dhamdhere\net al. proposed Total Conductance, a \"natural refinement of Integrated\nGradients\" for attributing importance to internal neurons. Unfortunately, the\nauthors found that calculating conductance in tensorflow required the addition\nof several custom gradient operators and did not scale well. In this work, we\nshow that the formula for Total Conductance is mathematically equivalent to\nPath Integrated Gradients computed on a hidden layer in the network. We provide\na scalable implementation of Total Conductance using standard tensorflow\ngradient operators that we call Neuron Integrated Gradients. We compare Neuron\nIntegrated Gradients to DeepLIFT, a pre-existing computationally efficient\napproach that is applicable to calculating internal neuron importance. We find\nthat DeepLIFT produces strong empirical results and is faster to compute, but\nbecause it lacks the theoretical properties of Neuron Integrated Gradients, it\nmay not always be preferred in practice. Colab notebook reproducing results:\nhttp://bit.ly/neuronintegratedgradients\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 03:47:45 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Shrikumar", "Avanti", ""], ["Su", "Jocelin", ""], ["Kundaje", "Anshul", ""]]}, {"id": "1807.09951", "submitter": "Long Zhao", "authors": "Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, Dimitris Metaxas", "title": "Learning to Forecast and Refine Residual Motion for Image-to-Video\n  Generation", "comments": "17 pages, 8 figures, 4 tables, accepted by ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of image-to-video translation, where an input image\nis translated into an output video containing motions of a single object.\nRecent methods for such problems typically train transformation networks to\ngenerate future frames conditioned on the structure sequence. Parallel work has\nshown that short high-quality motions can be generated by spatiotemporal\ngenerative networks that leverage temporal knowledge from the training data. We\ncombine the benefits of both approaches and propose a two-stage generation\nframework where videos are generated from structures and then refined by\ntemporal signals. To model motions more efficiently, we train networks to learn\nresidual motion between the current and future frames, which avoids learning\nmotion-irrelevant details. We conduct extensive experiments on two\nimage-to-video translation tasks: facial expression retargeting and human pose\nforecasting. Superior results over the state-of-the-art methods on both tasks\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 04:42:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhao", "Long", ""], ["Peng", "Xi", ""], ["Tian", "Yu", ""], ["Kapadia", "Mubbasir", ""], ["Metaxas", "Dimitris", ""]]}, {"id": "1807.09954", "submitter": "Wei He", "authors": "Wei He and Naoto Yokoya", "title": "Multi-temporal Sentinel-1 and -2 Data Fusion for Optical Image\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the optical image simulation from a synthetic\naperture radar (SAR) data using deep learning based methods. Two models, i.e.,\noptical image simulation directly from the SAR data and from multi-temporal\nSARoptical data, are proposed to testify the possibilities. The deep learning\nbased methods that we chose to achieve the models are a convolutional neural\nnetwork (CNN) with a residual architecture and a conditional generative\nadversarial network (cGAN). We validate our models using the Sentinel-1 and -2\ndatasets. The experiments demonstrate that the model with multi-temporal\nSAR-optical data can successfully simulate the optical image, meanwhile, the\nmodel with simple SAR data as input failed. The optical image simulation\nresults indicate the possibility of SARoptical information blending for the\nsubsequent applications such as large-scale cloud removal, and optical data\ntemporal superresolution. We also investigate the sensitivity of the proposed\nmodels against the training samples, and reveal possible future directions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 04:51:02 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["He", "Wei", ""], ["Yokoya", "Naoto", ""]]}, {"id": "1807.09956", "submitter": "Yu Jiang", "authors": "Yu Jiang, Vivek Natarajan, Xinlei Chen, Marcus Rohrbach, Dhruv Batra,\n  Devi Parikh", "title": "Pythia v0.1: the Winning Entry to the VQA Challenge 2018", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes Pythia v0.1, the winning entry from Facebook AI\nResearch (FAIR)'s A-STAR team to the VQA Challenge 2018.\n  Our starting point is a modular re-implementation of the bottom-up top-down\n(up-down) model. We demonstrate that by making subtle but important changes to\nthe model architecture and the learning rate schedule, fine-tuning image\nfeatures, and adding data augmentation, we can significantly improve the\nperformance of the up-down model on VQA v2.0 dataset -- from 65.67% to 70.22%.\n  Furthermore, by using a diverse ensemble of models trained with different\nfeatures and on different datasets, we are able to significantly improve over\nthe 'standard' way of ensembling (i.e. same model with different random seeds)\nby 1.31%. Overall, we achieve 72.27% on the test-std split of the VQA v2.0\ndataset. Our code in its entirety (training, evaluation, data-augmentation,\nensembling) and pre-trained models are publicly available at:\nhttps://github.com/facebookresearch/pythia\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 04:57:43 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 17:31:54 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Jiang", "Yu", ""], ["Natarajan", "Vivek", ""], ["Chen", "Xinlei", ""], ["Rohrbach", "Marcus", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1807.09958", "submitter": "Bo Dai", "authors": "Bo Dai, Deming Ye, and Dahua Lin", "title": "Rethinking the Form of Latent States in Image Captioning", "comments": "ECCV 2018, first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNNs and their variants have been widely adopted for image captioning. In\nRNNs, the production of a caption is driven by a sequence of latent states.\nExisting captioning models usually represent latent states as vectors, taking\nthis practice for granted. We rethink this choice and study an alternative\nformulation, namely using two-dimensional maps to encode latent states. This is\nmotivated by the curiosity about a question: how the spatial structures in the\nlatent states affect the resultant captions? Our study on MSCOCO and Flickr30k\nleads to two significant observations. First, the formulation with 2D states is\ngenerally more effective in captioning, consistently achieving higher\nperformance with comparable parameter sizes. Second, 2D states preserve spatial\nlocality. Taking advantage of this, we visually reveal the internal dynamics in\nthe process of caption generation, as well as the connections between input\nvisual domain and output linguistic domain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 05:26:15 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Dai", "Bo", ""], ["Ye", "Deming", ""], ["Lin", "Dahua", ""]]}, {"id": "1807.09959", "submitter": "Viresh Ranjan", "authors": "Viresh Ranjan, Hieu Le, Minh Hoai", "title": "Iterative Crowd Counting", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we tackle the problem of crowd counting in images. We present a\nConvolutional Neural Network (CNN) based density estimation approach to solve\nthis problem. Predicting a high resolution density map in one go is a\nchallenging task. Hence, we present a two branch CNN architecture for\ngenerating high resolution density maps, where the first branch generates a low\nresolution density map, and the second branch incorporates the low resolution\nprediction and feature maps from the first branch to generate a high resolution\ndensity map. We also propose a multi-stage extension of our approach where each\nstage in the pipeline utilizes the predictions from all the previous stages.\nEmpirical comparison with the previous state-of-the-art crowd counting methods\nshows that our method achieves the lowest mean absolute error on three\nchallenging crowd counting benchmarks: Shanghaitech, WorldExpo'10, and UCF\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 05:26:46 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Ranjan", "Viresh", ""], ["Le", "Hieu", ""], ["Hoai", "Minh", ""]]}, {"id": "1807.09968", "submitter": "Yaojie Liu", "authors": "Amin Jourabloo and Yaojie Liu and Xiaoming Liu", "title": "Face De-Spoofing: Anti-Spoofing via Noise Modeling", "comments": "To appear in ECCV 2018. The first two authors contributed equally to\n  this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prior face anti-spoofing works develop discriminative models for\nrecognizing the subtle differences between live and spoof faces. Those\napproaches often regard the image as an indivisible unit, and process it\nholistically, without explicit modeling of the spoofing process. In this work,\nmotivated by the noise modeling and denoising algorithms, we identify a new\nproblem of face de-spoofing, for the purpose of anti-spoofing: inversely\ndecomposing a spoof face into a spoof noise and a live face, and then utilizing\nthe spoof noise for classification. A CNN architecture with proper constraints\nand supervisions is proposed to overcome the problem of having no ground truth\nfor the decomposition. We evaluate the proposed method on multiple face\nanti-spoofing databases. The results show promising improvements due to our\nspoof noise modeling. Moreover, the estimated spoof noise provides a\nvisualization which helps to understand the added spoof noise by each spoof\nmedium.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 06:21:12 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Jourabloo", "Amin", ""], ["Liu", "Yaojie", ""], ["Liu", "Xiaoming", ""]]}, {"id": "1807.09970", "submitter": "Pedro Miraldo", "authors": "Pedro Miraldo, Tiago Dias, and Srikumar Ramalingam", "title": "A Minimal Closed-Form Solution for Multi-Perspective Pose Estimation\n  using Points and Lines", "comments": "22 pages, 6 figures", "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a minimal solution for pose estimation using both points and lines\nfor a multi-perspective camera. In this paper, we treat the multi-perspective\ncamera as a collection of rigidly attached perspective cameras. These type of\nimaging devices are useful for several computer vision applications that\nrequire a large coverage such as surveillance, self-driving cars, and\nmotion-capture studios. While prior methods have considered the cases using\nsolely points or lines, the hybrid case involving both points and lines has not\nbeen solved for multi-perspective cameras. We present the solutions for two\ncases. In the first case, we are given 2D to 3D correspondences for two points\nand one line. In the later case, we are given 2D to 3D correspondences for one\npoint and two lines. We show that the solution for the case of two points and\none line can be formulated as a fourth degree equation. This is interesting\nbecause we can get a closed-form solution and thereby achieve high\ncomputational efficiency. The later case involving two lines and one point can\nbe mapped to an eighth degree equation. We show simulations and real\nexperiments to demonstrate the advantages and benefits over existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 06:30:40 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Miraldo", "Pedro", ""], ["Dias", "Tiago", ""], ["Ramalingam", "Srikumar", ""]]}, {"id": "1807.09972", "submitter": "Zimeng Zhou", "authors": "Miaopeng Li, Zimeng Zhou, Jie Li and Xinguo Liu", "title": "Bottom-up Pose Estimation of Multiple Person with Bounding Box\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new method for multi-person pose estimation which\ncombines the traditional bottom-up and the top-down methods. Specifically, we\nperform the network feed-forwarding in a bottom-up manner, and then parse the\nposes with bounding box constraints in a top-down manner. In contrast to the\nprevious top-down methods, our method is robust to bounding box shift and\ntightness. We extract features from an original image by a residual network and\ntrain the network to learn both the confidence maps of joints and the\nconnection relationships between joints. During testing, the predicted\nconfidence maps, the connection relationships and the bounding boxes are used\nto parse the poses of all persons. The experimental results showed that our\nmethod learns more accurate human poses especially in challenging situations\nand gains better time performance, compared with the bottom-up and the top-down\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 06:33:32 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Li", "Miaopeng", ""], ["Zhou", "Zimeng", ""], ["Li", "Jie", ""], ["Liu", "Xinguo", ""]]}, {"id": "1807.09975", "submitter": "Yantao Shen", "authors": "Yantao Shen, Hongsheng Li, Shuai Yi, Dapeng Chen, Xiaogang Wang", "title": "Person Re-identification with Deep Similarity-Guided Graph Neural\n  Network", "comments": "accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The person re-identification task requires to robustly estimate visual\nsimilarities between person images. However, existing person re-identification\nmodels mostly estimate the similarities of different image pairs of probe and\ngallery images independently while ignores the relationship information between\ndifferent probe-gallery pairs. As a result, the similarity estimation of some\nhard samples might not be accurate. In this paper, we propose a novel deep\nlearning framework, named Similarity-Guided Graph Neural Network (SGGNN) to\novercome such limitations. Given a probe image and several gallery images,\nSGGNN creates a graph to represent the pairwise relationships between\nprobe-gallery pairs (nodes) and utilizes such relationships to update the\nprobe-gallery relation features in an end-to-end manner. Accurate similarity\nestimation can be achieved by using such updated probe-gallery relation\nfeatures for prediction. The input features for nodes on the graph are the\nrelation features of different probe-gallery image pairs. The probe-gallery\nrelation feature updating is then performed by the messages passing in SGGNN,\nwhich takes other nodes' information into account for similarity estimation.\nDifferent from conventional GNN approaches, SGGNN learns the edge weights with\nrich labels of gallery instance pairs directly, which provides relation fusion\nmore precise information. The effectiveness of our proposed method is validated\non three public person re-identification datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 06:56:51 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Shen", "Yantao", ""], ["Li", "Hongsheng", ""], ["Yi", "Shuai", ""], ["Chen", "Dapeng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1807.09986", "submitter": "Wenhao Jiang", "authors": "Wenhao Jiang, Lin Ma, Yu-Gang Jiang, Wei Liu, Tong Zhang", "title": "Recurrent Fusion Network for Image Captioning", "comments": "ECCV-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, much advance has been made in image captioning, and an\nencoder-decoder framework has been adopted by all the state-of-the-art models.\nUnder this framework, an input image is encoded by a convolutional neural\nnetwork (CNN) and then translated into natural language with a recurrent neural\nnetwork (RNN). The existing models counting on this framework merely employ one\nkind of CNNs, e.g., ResNet or Inception-X, which describe image contents from\nonly one specific view point. Thus, the semantic meaning of an input image\ncannot be comprehensively understood, which restricts the performance of\ncaptioning. In this paper, in order to exploit the complementary information\nfrom multiple encoders, we propose a novel Recurrent Fusion Network (RFNet) for\ntackling image captioning. The fusion process in our model can exploit the\ninteractions among the outputs of the image encoders and then generate new\ncompact yet informative representations for the decoder. Experiments on the\nMSCOCO dataset demonstrate the effectiveness of our proposed RFNet, which sets\na new state-of-the-art for image captioning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 07:25:06 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 11:21:09 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 03:42:15 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jiang", "Wenhao", ""], ["Ma", "Lin", ""], ["Jiang", "Yu-Gang", ""], ["Liu", "Wei", ""], ["Zhang", "Tong", ""]]}, {"id": "1807.09993", "submitter": "Deepak Babu Sam", "authors": "Deepak Babu Sam, Neeraj N Sajjan, R. Venkatesh Babu", "title": "Divide and Grow: Capturing Huge Diversity in Crowd Images with\n  Incrementally Growing CNN", "comments": "CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated counting of people in crowd images is a challenging task. The major\ndifficulty stems from the large diversity in the way people appear in crowds.\nIn fact, features available for crowd discrimination largely depend on the\ncrowd density to the extent that people are only seen as blobs in a highly\ndense scene. We tackle this problem with a growing CNN which can progressively\nincrease its capacity to account for the wide variability seen in crowd scenes.\nOur model starts from a base CNN density regressor, which is trained in\nequivalence on all types of crowd images. In order to adapt with the huge\ndiversity, we create two child regressors which are exact copies of the base\nCNN. A differential training procedure divides the dataset into two clusters\nand fine-tunes the child networks on their respective specialties.\nConsequently, without any hand-crafted criteria for forming specialties, the\nchild regressors become experts on certain types of crowds. The child networks\nare again split recursively, creating two experts at every division. This\nhierarchical training leads to a CNN tree, where the child regressors are more\nfine experts than any of their parents. The leaf nodes are taken as the final\nexperts and a classifier network is then trained to predict the correct\nspecialty for a given test image patch. The proposed model achieves higher\ncount accuracy on major crowd datasets. Further, we analyse the characteristics\nof specialties mined automatically by our method.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 07:52:17 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Sam", "Deepak Babu", ""], ["Sajjan", "Neeraj N", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1807.09995", "submitter": "Alex Zyner", "authors": "Alex Zyner, Stewart Worrall, Eduardo Nebot", "title": "Naturalistic Driver Intention and Path Prediction using Recurrent Neural\n  Networks", "comments": "Submitted to IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the intentions of drivers at intersections is a critical\ncomponent for autonomous vehicles. Urban intersections that do not have traffic\nsignals are a common epicentre of highly variable vehicle movement and\ninteractions. We present a method for predicting driver intent at urban\nintersections through multi-modal trajectory prediction with uncertainty. Our\nmethod is based on recurrent neural networks combined with a mixture density\nnetwork output layer. To consolidate the multi-modal nature of the output\nprobability distribution, we introduce a clustering algorithm that extracts the\nset of possible paths that exist in the prediction output, and ranks them\naccording to likelihood. To verify the method's performance and\ngeneralizability, we present a real-world dataset that consists of over 23,000\nvehicles traversing five different intersections, collected using a vehicle\nmounted Lidar based tracking system. An array of metrics is used to demonstrate\nthe performance of the model against several baselines.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 07:57:13 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zyner", "Alex", ""], ["Worrall", "Stewart", ""], ["Nebot", "Eduardo", ""]]}, {"id": "1807.09999", "submitter": "Andrea Romanoni", "authors": "Andrea Romanoni and Matteo Matteucci", "title": "A Data-driven Prior on Facet Orientation for Semantic Mesh Labeling", "comments": "Accepted at 3DV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh labeling is the key problem of classifying the facets of a 3D mesh with\na label among a set of possible ones. State-of-the-art methods model mesh\nlabeling as a Markov Random Field over the facets. These algorithms map image\nsegmentations to the mesh by minimizing an energy function that comprises a\ndata term, a smoothness terms, and class-specific priors. The latter favor a\nlabeling with respect to another depending on the orientation of the facet\nnormals. In this paper we propose a novel energy term that acts as a prior, but\ndoes not require any prior knowledge about the scene nor scene-specific\nrelationship among classes. It bootstraps from a coarse mapping of the 2D\nsegmentations on the mesh, and it favors the facets to be labeled according to\nthe statistics of the mesh normals in their neighborhood. We tested our\napproach against five different datasets and, even if we do not inject prior\nknowledge, our method adapts to the data and overcomes the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 08:09:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Romanoni", "Andrea", ""], ["Matteucci", "Matteo", ""]]}, {"id": "1807.10002", "submitter": "Seonwook Park", "authors": "Seonwook Park and Adrian Spurr and Otmar Hilliges", "title": "Deep Pictorial Gaze Estimation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01261-8_44", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating human gaze from natural eye images only is a challenging task.\nGaze direction can be defined by the pupil- and the eyeball center where the\nlatter is unobservable in 2D images. Hence, achieving highly accurate gaze\nestimates is an ill-posed problem. In this paper, we introduce a novel deep\nneural network architecture specifically designed for the task of gaze\nestimation from single eye input. Instead of directly regressing two angles for\nthe pitch and yaw of the eyeball, we regress to an intermediate pictorial\nrepresentation which in turn simplifies the task of 3D gaze direction\nestimation. Our quantitative and qualitative results show that our approach\nachieves higher accuracies than the state-of-the-art and is robust to variation\nin gaze, head pose and image quality.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 08:14:46 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Park", "Seonwook", ""], ["Spurr", "Adrian", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1807.10007", "submitter": "Victor Kulikov", "authors": "Victor Kulikov and Victor Yurchenko and Victor Lempitsky", "title": "Instance Segmentation by Deep Coloring", "comments": "10 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new and, arguably, a very simple reduction of instance\nsegmentation to semantic segmentation. This reduction allows to train\nfeed-forward non-recurrent deep instance segmentation systems in an end-to-end\nfashion using architectures that have been proposed for semantic segmentation.\nOur approach proceeds by introducing a fixed number of labels (colors) and then\ndynamically assigning object instances to those labels during training\n(coloring). A standard semantic segmentation objective is then used to train a\nnetwork that can color previously unseen images. At test time, individual\nobject instances can be recovered from the output of the trained convolutional\nnetwork using simple connected component analysis. In the experimental\nvalidation, the coloring approach is shown to be capable of solving diverse\ninstance segmentation tasks arising in autonomous driving (the Cityscapes\nbenchmark), plant phenotyping (the CVPPP leaf segmentation challenge), and\nhigh-throughput microscopy image analysis.\n  The source code is publicly available:\nhttps://github.com/kulikovv/DeepColoring.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 08:20:15 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Kulikov", "Victor", ""], ["Yurchenko", "Victor", ""], ["Lempitsky", "Victor", ""]]}, {"id": "1807.10018", "submitter": "Yilei Xiong", "authors": "Yilei Xiong, Bo Dai, Dahua Lin", "title": "Move Forward and Tell: A Progressive Generator of Video Descriptions", "comments": "Accepted by ECCV 2018", "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient framework that can generate a coherent paragraph to\ndescribe a given video. Previous works on video captioning usually focus on\nvideo clips. They typically treat an entire video as a whole and generate the\ncaption conditioned on a single embedding. On the contrary, we consider videos\nwith rich temporal structures and aim to generate paragraph descriptions that\ncan preserve the story flow while being coherent and concise. Towards this\ngoal, we propose a new approach, which produces a descriptive paragraph by\nassembling temporally localized descriptions. Given a video, it selects a\nsequence of distinctive clips and generates sentences thereon in a coherent\nmanner. Particularly, the selection of clips and the production of sentences\nare done jointly and progressively driven by a recurrent network -- what to\ndescribe next depends on what have been said before. Here, the recurrent\nnetwork is learned via self-critical sequence training with both sentence-level\nand paragraph-level rewards. On the ActivityNet Captions dataset, our method\ndemonstrated the capability of generating high-quality paragraph descriptions\nfor videos. Compared to those by other methods, the descriptions produced by\nour method are often more relevant, more coherent, and more concise.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 08:57:24 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Xiong", "Yilei", ""], ["Dai", "Bo", ""], ["Lin", "Dahua", ""]]}, {"id": "1807.10027", "submitter": "Janka Hatvani", "authors": "Janka Hatvani, Adrian Basarab, Jean-Yves Tourneret, Mikl\\'os Gy\\\"ongy,\n  Denis Kouam\\'e", "title": "A Tensor Factorization Method for 3D Super-Resolution with Application\n  to Dental CT", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/TMI.2018.2883517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Available super-resolution techniques for 3D images are either\ncomputationally inefficient prior-knowledge-based iterative techniques or deep\nlearning methods which require a large database of known low- and\nhigh-resolution image pairs. A recently introduced tensor-factorization-based\napproach offers a fast solution without the use of known image pairs or strict\nprior assumptions. In this article this factorization framework is investigated\nfor single image resolution enhancement with an off-line estimate of the system\npoint spread function. The technique is applied to 3D cone beam computed\ntomography for dental image resolution enhancement. To demonstrate the\nefficiency of our method, it is compared to a recent state-of-the-art iterative\ntechnique using low-rank and total variation regularizations. In contrast to\nthis comparative technique, the proposed reconstruction technique gives a\n2-order-of-magnitude improvement in running time -- 2 minutes compared to 2\nhours for a dental volume of 282$\\times$266$\\times$392 voxels. Furthermore, it\nalso offers slightly improved quantitative results (peak signal-to-noise ratio,\nsegmentation quality). Another advantage of the presented technique is the low\nnumber of hyperparameters. As demonstrated in this paper, the framework is not\nsensitive to small changes of its parameters, proposing an ease of use.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:24:23 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hatvani", "Janka", ""], ["Basarab", "Adrian", ""], ["Tourneret", "Jean-Yves", ""], ["Gy\u00f6ngy", "Mikl\u00f3s", ""], ["Kouam\u00e9", "Denis", ""]]}, {"id": "1807.10029", "submitter": "Jiaolong Yang", "authors": "Dongqing Zhang, Jiaolong Yang, Dongqiangzi Ye and Gang Hua", "title": "LQ-Nets: Learned Quantization for Highly Accurate and Compact Deep\n  Neural Networks", "comments": "ECCV'18 (European Conference on Computer Vision); Main paper + suppl.\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although weight and activation quantization is an effective approach for Deep\nNeural Network (DNN) compression and has a lot of potentials to increase\ninference speed leveraging bit-operations, there is still a noticeable gap in\nterms of prediction accuracy between the quantized model and the full-precision\nmodel. To address this gap, we propose to jointly train a quantized,\nbit-operation-compatible DNN and its associated quantizers, as opposed to using\nfixed, handcrafted quantization schemes such as uniform or logarithmic\nquantization. Our method for learning the quantizers applies to both network\nweights and activations with arbitrary-bit precision, and our quantizers are\neasy to train. The comprehensive experiments on CIFAR-10 and ImageNet datasets\nshow that our method works consistently well for various network structures\nsuch as AlexNet, VGG-Net, GoogLeNet, ResNet, and DenseNet, surpassing previous\nquantization methods in terms of accuracy by an appreciable margin. Code\navailable at https://github.com/Microsoft/LQ-Nets\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:26:39 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhang", "Dongqing", ""], ["Yang", "Jiaolong", ""], ["Ye", "Dongqiangzi", ""], ["Hua", "Gang", ""]]}, {"id": "1807.10037", "submitter": "Seungeui Lee", "authors": "Myunggi Lee, Seungeui Lee, Sungjoon Son, Gyutae Park, Nojun Kwak", "title": "Motion Feature Network: Fixed Motion Filter for Action Recognition", "comments": "ECCV 2018, 14 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal representations in frame sequences play an important role in\nthe task of action recognition. Previously, a method of using optical flow as a\ntemporal information in combination with a set of RGB images that contain\nspatial information has shown great performance enhancement in the action\nrecognition tasks. However, it has an expensive computational cost and requires\ntwo-stream (RGB and optical flow) framework. In this paper, we propose MFNet\n(Motion Feature Network) containing motion blocks which make it possible to\nencode spatio-temporal information between adjacent frames in a unified network\nthat can be trained end-to-end. The motion block can be attached to any\nexisting CNN-based action recognition frameworks with only a small additional\ncost. We evaluated our network on two of the action recognition datasets\n(Jester and Something-Something) and achieved competitive performances for both\ndatasets by training the networks from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:45:36 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 15:19:29 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Lee", "Myunggi", ""], ["Lee", "Seungeui", ""], ["Son", "Sungjoon", ""], ["Park", "Gyutae", ""], ["Kwak", "Nojun", ""]]}, {"id": "1807.10066", "submitter": "Rohit Girdhar", "authors": "Rohit Girdhar, Jo\\~ao Carreira, Carl Doersch and Andrew Zisserman", "title": "A Better Baseline for AVA", "comments": "ActivityNet Workshop (AVA Challenge), CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple baseline for action localization on the AVA dataset.\nThe model builds upon the Faster R-CNN bounding box detection framework,\nadapted to operate on pure spatiotemporal features - in our case produced\nexclusively by an I3D model pretrained on Kinetics. This model obtains 21.9%\naverage AP on the validation set of AVA v2.1, up from 14.5% for the best RGB\nspatiotemporal model used in the original AVA paper (which was pretrained on\nKinetics and ImageNet), and up from 11.3 of the publicly available baseline\nusing a ResNet101 image feature extractor, that was pretrained on ImageNet. Our\nfinal model obtains 22.8%/21.9% mAP on the val/test sets and outperforms all\nsubmissions to the AVA challenge at CVPR 2018.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 11:11:25 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Girdhar", "Rohit", ""], ["Carreira", "Jo\u00e3o", ""], ["Doersch", "Carl", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1807.10073", "submitter": "Seong Hun Lee", "authors": "Seong Hun Lee, Javier Civera", "title": "Loosely-Coupled Semi-Direct Monocular SLAM", "comments": "Accepted for publication in IEEE Robotics and Automation Letters.\n  Watch video demo at: https://youtu.be/j7WnU7ZpZ8c", "journal-ref": null, "doi": "10.1109/LRA.2018.2889156", "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semi-direct approach for monocular simultaneous\nlocalization and mapping (SLAM) that combines the complementary strengths of\ndirect and feature-based methods. The proposed pipeline loosely couples direct\nodometry and feature-based SLAM to perform three levels of parallel\noptimizations: (1) photometric bundle adjustment (BA) that jointly optimizes\nthe local structure and motion, (2) geometric BA that refines keyframe poses\nand associated feature map points, and (3) pose graph optimization to achieve\nglobal map consistency in the presence of loop closures. This is achieved in\nreal-time by limiting the feature-based operations to marginalized keyframes\nfrom the direct odometry module. Exhaustive evaluation on two benchmark\ndatasets demonstrates that our system outperforms the state-of-the-art\nmonocular odometry and SLAM systems in terms of overall accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 11:35:34 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 10:20:56 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 18:50:31 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Lee", "Seong Hun", ""], ["Civera", "Javier", ""]]}, {"id": "1807.10088", "submitter": "Sebastian Lutz", "authors": "Sebastian Lutz, Konstantinos Amplianitis, Aljosa Smolic", "title": "AlphaGAN: Generative adversarial networks for natural image matting", "comments": "Accepted at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first generative adversarial network (GAN) for natural image\nmatting. Our novel generator network is trained to predict visually appealing\nalphas with the addition of the adversarial loss from the discriminator that is\ntrained to classify well-composited images. Further, we improve existing\nencoder-decoder architectures to better deal with the spatial localization\nissues inherited in convolutional neural networks (CNN) by using dilated\nconvolutions to capture global context information without downscaling feature\nmaps and losing spatial information. We present state-of-the-art results on the\nalphamatting online benchmark for the gradient error and give comparable\nresults in others. Our method is particularly well suited for fine structures\nlike hair, which is of great importance in practical matting applications, e.g.\nin film/TV production.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 12:17:22 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Lutz", "Sebastian", ""], ["Amplianitis", "Konstantinos", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1807.10097", "submitter": "Chunhua Shen", "authors": "Ruoxi Deng, Chunhua Shen, Shengjun Liu, Huibing Wang, Xinru Liu", "title": "Learning to predict crisp boundaries", "comments": "Accepted to European Conf. Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent methods for boundary or edge detection built on Deep Convolutional\nNeural Networks (CNNs) typically suffer from the issue of predicted edges being\nthick and need post-processing to obtain crisp boundaries. Highly imbalanced\ncategories of boundary versus background in training data is one of main\nreasons for the above problem. In this work, the aim is to make CNNs produce\nsharp boundaries without post-processing. We introduce a novel loss for\nboundary detection, which is very effective for classifying imbalanced data and\nallows CNNs to produce crisp boundaries. Moreover, we propose an end-to-end\nnetwork which adopts the bottom-up/top-down architecture to tackle the task.\nThe proposed network effectively leverages hierarchical features and produces\npixel-accurate boundary mask, which is critical to reconstruct the edge map.\nOur experiments illustrate that directly making crisp prediction not only\npromotes the visual results of CNNs, but also achieves better results against\nthe state-of-the-art on the BSDS500 dataset (ODS F-score of .815) and the NYU\nDepth dataset (ODS F-score of .762).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 12:40:36 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Deng", "Ruoxi", ""], ["Shen", "Chunhua", ""], ["Liu", "Shengjun", ""], ["Wang", "Huibing", ""], ["Liu", "Xinru", ""]]}, {"id": "1807.10108", "submitter": "Saumik Bhattacharya", "authors": "Prasun Roy, Subhankar Ghosh, Saumik Bhattacharya and Umapada Pal", "title": "Effects of Degradations on Deep Neural Network Architectures", "comments": "Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, image classification methods based on capsules (groups of neurons)\nand a novel dynamic routing protocol are proposed. The methods show promising\nperformances than the state-of-the-art CNN-based models in some of the existing\ndatasets. However, the behavior of capsule-based models and CNN-based models\nare largely unknown in presence of noise. So it is important to study the\nperformance of these models under various noises. In this paper, we demonstrate\nthe effect of image degradations on deep neural network architectures for image\nclassification task. We select six widely used CNN architectures to analyse\ntheir performances for image classification task on datasets of various\ndistortions. Our work has three main contributions: 1) we observe the effects\nof degradations on different CNN models; 2) accordingly, we propose a network\nsetup that can enhance the robustness of any CNN architecture for certain\ndegradations, and 3) we propose a new capsule network that achieves high\nrecognition accuracy. To the best of our knowledge, this is the first study on\nthe performance of CapsuleNet (CapsNet) and other state-of-the-art CNN\narchitectures under different types of image degradations. Also, our datasets\nand source code are available publicly to the researchers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 13:20:57 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 07:10:27 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 12:03:44 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2019 14:30:08 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Roy", "Prasun", ""], ["Ghosh", "Subhankar", ""], ["Bhattacharya", "Saumik", ""], ["Pal", "Umapada", ""]]}, {"id": "1807.10111", "submitter": "Skand Vishwanath Peri", "authors": "Apoorva Sikka, Skand Vishwanath Peri, Deepti.R.Bathula", "title": "MRI to FDG-PET: Cross-Modal Synthesis Using 3D U-Net For Multi-Modal\n  Alzheimer's Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies suggest that combined analysis of Magnetic resonance\nimaging~(MRI) that measures brain atrophy and positron emission\ntomography~(PET) that quantifies hypo-metabolism provides improved accuracy in\ndiagnosing Alzheimer's disease. However, such techniques are limited by the\navailability of corresponding scans of each modality. Current work focuses on a\ncross-modal approach to estimate FDG-PET scans for the given MR scans using a\n3D U-Net architecture. The use of the complete MR image instead of a local\npatch based approach helps in capturing non-local and non-linear correlations\nbetween MRI and PET modalities. The quality of the estimated PET scans is\nmeasured using quantitative metrics such as MAE, PSNR and SSIM. The efficacy of\nthe proposed method is evaluated in the context of Alzheimer's disease\nclassification. The accuracy using only MRI is 70.18% while joint\nclassification using synthesized PET and MRI is 74.43% with a p-value of\n$0.06$. The significant improvement in diagnosis demonstrates the utility of\nthe synthesized PET scans for multi-modal analysis.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 13:27:48 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 15:36:41 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sikka", "Apoorva", ""], ["Peri", "Skand Vishwanath", ""], ["Bathula", "Deepti. R.", ""]]}, {"id": "1807.10120", "submitter": "Abdolrahman Khoshrou", "authors": "Abdolrahman Khoshrou, Andre B. Dorsman and Eric. J. Pauwels", "title": "SVD-based Visualisation and Approximation for Time Series Data in Smart\n  Energy Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ISGTEurope.2017.8260303", "report-no": null, "categories": "physics.soc-ph cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time series in smart energy systems exhibit two different timescales. On\nthe one hand there are patterns linked to daily human activities. On the other\nhand, there are relatively slow trends linked to seasonal variations. In this\npaper we interpret these time series as matrices, to be visualized as images.\nThis approach has two advantages: First of all, interpreting such time series\nas images enables one to visually integrate across the image and makes it\ntherefore easier to spot subtle or faint features. Second, the matrix\ninterpretation also grants elucidation of the underlying structure using\nwell-established matrix decomposition methods. We will illustrate both these\naspects for data obtained from the German day-ahead market.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:06:48 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Khoshrou", "Abdolrahman", ""], ["Dorsman", "Andre B.", ""], ["Pauwels", "Eric. J.", ""]]}, {"id": "1807.10129", "submitter": "Andrew Fitzgibbon", "authors": "Filip \\v{S}rajer, Zuzana Kukelova, Andrew Fitzgibbon", "title": "A Benchmark of Selected Algorithmic Differentiation Tools on Some\n  Problems in Computer Vision and Machine Learning", "comments": "Previous versions of this article appeared at AD2016---7th\n  International Conference on Algorithmic Differentiation, and in Optimization\n  Methods and Software, Taylor and Francis, Feb 2018 (online)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic differentiation (AD) allows exact computation of derivatives\ngiven only an implementation of an objective function. Although many AD tools\nare available, a proper and efficient implementation of AD methods is not\nstraightforward. The existing tools are often too different to allow for a\ngeneral test suite. In this paper, we compare fifteen ways of computing\nderivatives including eleven automatic differentiation tools implementing\nvarious methods and written in various languages (C++, F#, MATLAB, Julia and\nPython), two symbolic differentiation tools, finite differences, and\nhand-derived computation.\n  We look at three objective functions from computer vision and machine\nlearning. These objectives are for the most part simple, in the sense that no\niterative loops are involved, and conditional statements are encapsulated in\nfunctions such as {\\tt abs} or {\\tt logsumexp}. However, it is important for\nthe success of algorithmic differentiation that such `simple' objective\nfunctions are handled efficiently, as so many problems in computer vision and\nmachine learning are of this form.\n  Of course, our results depend on programmer skill, and familiarity with the\ntools. However, we contend that this paper presents an important datapoint: a\nskilled programmer devoting roughly a week to each tool produced the timings we\npresent. We have made our implementations available as open source to allow the\ncommunity to replicate and update these benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 13:42:30 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["\u0160rajer", "Filip", ""], ["Kukelova", "Zuzana", ""], ["Fitzgibbon", "Andrew", ""]]}, {"id": "1807.10160", "submitter": "Gui-Song Xia", "authors": "Fudong Wang and Nan Xue and Yipeng Zhang and Xiang Bai and Gui-Song\n  Xia", "title": "Adaptively Transforming Graph Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many graph matching methods that incorporate pairwise constraint\nand that can be formulated as a quadratic assignment problem (QAP) have been\nproposed. Although these methods demonstrate promising results for the graph\nmatching problem, they have high complexity in space or time. In this paper, we\nintroduce an adaptively transforming graph matching (ATGM) method from the\nperspective of functional representation. More precisely, under a\ntransformation formulation, we aim to match two graphs by minimizing the\ndiscrepancy between the original graph and the transformed graph. With a linear\nrepresentation map of the transformation, the pairwise edge attributes of\ngraphs are explicitly represented by unary node attributes, which enables us to\nreduce the space and time complexity significantly. Due to an efficient\nFrank-Wolfe method-based optimization strategy, we can handle graphs with\nhundreds and thousands of nodes within an acceptable amount of time. Meanwhile,\nbecause transformation map can preserve graph structures, a domain\nadaptation-based strategy is proposed to remove the outliers. The experimental\nresults demonstrate that our proposed method outperforms the state-of-the-art\ngraph matching algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 14:12:40 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Wang", "Fudong", ""], ["Xue", "Nan", ""], ["Zhang", "Yipeng", ""], ["Bai", "Xiang", ""], ["Xia", "Gui-Song", ""]]}, {"id": "1807.10162", "submitter": "Rajendra Nagar", "authors": "Rajendra Nagar and Shanmuganathan Raman", "title": "Fast and Accurate Intrinsic Symmetry Detection", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": "10.1007/978-3-030-01246-5_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision and graphics, various types of symmetries are extensively\nstudied since symmetry present in objects is a fundamental cue for\nunderstanding the shape and the structure of objects. In this work, we detect\nthe intrinsic reflective symmetry in triangle meshes where we have to find the\nintrinsically symmetric point for each point of the shape. We establish\ncorrespondences between functions defined on the shapes by extending the\nfunctional map framework and then recover the point-to-point correspondences.\nPrevious approaches using the functional map for this task find the functional\ncorrespondences matrix by solving a non-linear optimization problem which makes\nthem slow. In this work, we propose a closed form solution for this matrix\nwhich makes our approach faster. We find the closed-form solution based on our\nfollowing results. If the given shape is intrinsically symmetric, then the\nshortest length geodesic between two intrinsically symmetric points is also\nintrinsically symmetric. If an eigenfunction of the Laplace-Beltrami operator\nfor the given shape is an even (odd) function, then its restriction on the\nshortest length geodesic between two intrinsically symmetric points is also an\neven (odd) function. The sign of a low-frequency eigenfunction is the same on\nthe neighboring points. Our method is invariant to the ordering of the\neigenfunctions and has the least time complexity. We achieve the best\nperformance on the SCAPE dataset and comparable performance with the\nstate-of-the-art methods on the TOSCA dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 14:18:36 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 04:04:55 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 05:52:52 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 04:09:38 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Nagar", "Rajendra", ""], ["Raman", "Shanmuganathan", ""]]}, {"id": "1807.10165", "submitter": "Zongwei Zhou", "authors": "Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, Jianming\n  Liang", "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation", "comments": "8 pages, 3 figures, 3 tables, accepted by 4th Deep Learning in\n  Medical Image Analysis (DLMIA) Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present UNet++, a new, more powerful architecture for\nmedical image segmentation. Our architecture is essentially a deeply-supervised\nencoder-decoder network where the encoder and decoder sub-networks are\nconnected through a series of nested, dense skip pathways. The re-designed skip\npathways aim at reducing the semantic gap between the feature maps of the\nencoder and decoder sub-networks. We argue that the optimizer would deal with\nan easier learning task when the feature maps from the decoder and encoder\nnetworks are semantically similar. We have evaluated UNet++ in comparison with\nU-Net and wide U-Net architectures across multiple medical image segmentation\ntasks: nodule segmentation in the low-dose CT scans of chest, nuclei\nsegmentation in the microscopy images, liver segmentation in abdominal CT\nscans, and polyp segmentation in colonoscopy videos. Our experiments\ndemonstrate that UNet++ with deep supervision achieves an average IoU gain of\n3.9 and 3.4 points over U-Net and wide U-Net, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 04:08:21 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Zhou", "Zongwei", ""], ["Siddiquee", "Md Mahfuzur Rahman", ""], ["Tajbakhsh", "Nima", ""], ["Liang", "Jianming", ""]]}, {"id": "1807.10174", "submitter": "Varun Jampani", "authors": "Varun Jampani and Deqing Sun and Ming-Yu Liu and Ming-Hsuan Yang and\n  Jan Kautz", "title": "Superpixel Sampling Networks", "comments": "ECCV2018. Project URL: https://varunjampani.github.io/ssn/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superpixels provide an efficient low/mid-level representation of image data,\nwhich greatly reduces the number of image primitives for subsequent vision\ntasks. Existing superpixel algorithms are not differentiable, making them\ndifficult to integrate into otherwise end-to-end trainable deep neural\nnetworks. We develop a new differentiable model for superpixel sampling that\nleverages deep networks for learning superpixel segmentation. The resulting\n\"Superpixel Sampling Network\" (SSN) is end-to-end trainable, which allows\nlearning task-specific superpixels with flexible loss functions and has fast\nruntime. Extensive experimental analysis indicates that SSNs not only\noutperform existing superpixel algorithms on traditional segmentation\nbenchmarks, but can also learn superpixels for other tasks. In addition, SSNs\ncan be easily integrated into downstream deep networks resulting in performance\nimprovements.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 14:48:22 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Jampani", "Varun", ""], ["Sun", "Deqing", ""], ["Liu", "Ming-Yu", ""], ["Yang", "Ming-Hsuan", ""], ["Kautz", "Jan", ""]]}, {"id": "1807.10194", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Raymond Chan, Carola-Bibiane Schonlieb, Gabriele Steidl,\n  Tieyong Zeng", "title": "Linkage between piecewise constant Mumford-Shah model and ROF model and\n  its virtue in image segmentation", "comments": "31 pages", "journal-ref": "SIAM Journal on Scientific Computing, 41(6):B1310-B1340, 2019", "doi": null, "report-no": null, "categories": "math.NA cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The piecewise constant Mumford-Shah (PCMS) model and the Rudin-Osher-Fatemi\n(ROF) model are two important variational models in image segmentation and\nimage restoration, respectively. In this paper, we explore a linkage between\nthese models. We prove that for the two-phase segmentation problem a partial\nminimizer of the PCMS model can be obtained by thresholding the minimizer of\nthe ROF model. A similar linkage is still valid for multiphase segmentation\nunder specific assumptions. Thus it opens a new segmentation paradigm: image\nsegmentation can be done via image restoration plus thresholding. This new\nparadigm, which circumvents the innate non-convex property of the PCMS model,\ntherefore improves the segmentation performance in both efficiency (much faster\nthan state-of-the-art methods based on PCMS model, particularly when the phase\nnumber is high) and effectiveness (producing segmentation results with better\nquality) due to the flexibility of the ROF model in tackling degraded images,\nsuch as noisy images, blurry images or images with information loss. As a\nby-product of the new paradigm, we derive a novel segmentation method, called\nthresholded-ROF (T-ROF) method, to illustrate the virtue of managing image\nsegmentation through image restoration techniques. The convergence of the T-ROF\nmethod is proved, and elaborate experimental results and comparisons are\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 15:28:12 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 00:34:31 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Cai", "Xiaohao", ""], ["Chan", "Raymond", ""], ["Schonlieb", "Carola-Bibiane", ""], ["Steidl", "Gabriele", ""], ["Zeng", "Tieyong", ""]]}, {"id": "1807.10201", "submitter": "Artsiom Sanakoyeu", "authors": "Artsiom Sanakoyeu and Dmytro Kotovenko and Sabine Lang and Bj\\\"orn\n  Ommer", "title": "A Style-Aware Content Loss for Real-time HD Style Transfer", "comments": "Accepted at ECCV18 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, style transfer has received a lot of attention. While much of this\nresearch has aimed at speeding up processing, the approaches are still lacking\nfrom a principled, art historical standpoint: a style is more than just a\nsingle image or an artist, but previous work is limited to only a single\ninstance of a style or shows no benefit from more images. Moreover, previous\nwork has relied on a direct comparison of art in the domain of RGB images or on\nCNNs pre-trained on ImageNet, which requires millions of labeled object\nbounding boxes and can introduce an extra bias, since it has been assembled\nwithout artistic consideration. To circumvent these issues, we propose a\nstyle-aware content loss, which is trained jointly with a deep encoder-decoder\nnetwork for real-time, high-resolution stylization of images and videos. We\npropose a quantitative measure for evaluating the quality of a stylized image\nand also have art historians rank patches from our approach against those from\nprevious work. These and our qualitative results ranging from small image\npatches to megapixel stylistic images and videos show that our approach better\ncaptures the subtle nature in which a style affects content.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 15:39:59 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 07:31:00 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sanakoyeu", "Artsiom", ""], ["Kotovenko", "Dmytro", ""], ["Lang", "Sabine", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1807.10215", "submitter": "Jen-Tang Lu", "authors": "Jen-Tang Lu, Stefano Pedemonte, Bernardo Bizzo, Sean Doyle, Katherine\n  P. Andriole, Mark H. Michalski, R. Gilberto Gonzalez, Stuart R. Pomerantz", "title": "DeepSPINE: Automated Lumbar Vertebral Segmentation, Disc-level\n  Designation, and Spinal Stenosis Grading Using Deep Learning", "comments": "Accepted as spotlight talk at Machine Learning for Healthcare (MLHC)\n  2018. Supplementary Video: https://bit.ly/DeepSPINE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high prevalence of spinal stenosis results in a large volume of MRI\nimaging, yet interpretation can be time-consuming with high inter-reader\nvariability even among the most specialized radiologists. In this paper, we\ndevelop an efficient methodology to leverage the subject-matter-expertise\nstored in large-scale archival reporting and image data for a deep-learning\napproach to fully-automated lumbar spinal stenosis grading. Specifically, we\nintroduce three major contributions: (1) a natural-language-processing scheme\nto extract level-by-level ground-truth labels from free-text radiology reports\nfor the various types and grades of spinal stenosis (2) accurate vertebral\nsegmentation and disc-level localization using a U-Net architecture combined\nwith a spine-curve fitting method, and (3) a multi-input, multi-task, and\nmulti-class convolutional neural network to perform central canal and foraminal\nstenosis grading on both axial and sagittal imaging series inputs with the\nextracted report-derived labels applied to corresponding imaging level\nsegments. This study uses a large dataset of 22796 disc-levels extracted from\n4075 patients. We achieve state-of-the-art performance on lumbar spinal\nstenosis classification and expect the technique will increase both radiology\nworkflow efficiency and the perceived value of radiology reports for referring\nclinicians and patients.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 15:59:49 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Lu", "Jen-Tang", ""], ["Pedemonte", "Stefano", ""], ["Bizzo", "Bernardo", ""], ["Doyle", "Sean", ""], ["Andriole", "Katherine P.", ""], ["Michalski", "Mark H.", ""], ["Gonzalez", "R. Gilberto", ""], ["Pomerantz", "Stuart R.", ""]]}, {"id": "1807.10221", "submitter": "Tete Xiao", "authors": "Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun", "title": "Unified Perceptual Parsing for Scene Understanding", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans recognize the visual world at multiple levels: we effortlessly\ncategorize scenes and detect objects inside, while also identifying the\ntextures and surfaces of the objects along with their different compositional\nparts. In this paper, we study a new task called Unified Perceptual Parsing,\nwhich requires the machine vision systems to recognize as many visual concepts\nas possible from a given image. A multi-task framework called UPerNet and a\ntraining strategy are developed to learn from heterogeneous image annotations.\nWe benchmark our framework on Unified Perceptual Parsing and show that it is\nable to effectively segment a wide range of concepts from images. The trained\nnetworks are further applied to discover visual knowledge in natural scenes.\nModels are available at \\url{https://github.com/CSAILVision/unifiedparsing}.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 16:13:49 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Xiao", "Tete", ""], ["Liu", "Yingcheng", ""], ["Zhou", "Bolei", ""], ["Jiang", "Yuning", ""], ["Sun", "Jian", ""]]}, {"id": "1807.10225", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G\n  Schwarz, Matthew L Senjem, Jeffrey L Gunter, Katherine Andriole, Mark\n  Michalski", "title": "Medical Image Synthesis for Data Augmentation and Anonymization using\n  Generative Adversarial Networks", "comments": "Accepted for 2018 Workshop on Simulation and Synthesis in Medical\n  Imaging - SASHIMI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data diversity is critical to success when training deep learning models.\nMedical imaging data sets are often imbalanced as pathologic findings are\ngenerally rare, which introduces significant challenges when training deep\nlearning models. In this work, we propose a method to generate synthetic\nabnormal MRI images with brain tumors by training a generative adversarial\nnetwork using two publicly available data sets of brain MRI. We demonstrate two\nunique benefits that the synthetic images provide. First, we illustrate\nimproved performance on tumor segmentation by leveraging the synthetic images\nas a form of data augmentation. Second, we demonstrate the value of generative\nmodels as an anonymization tool, achieving comparable tumor segmentation\nresults when trained on the synthetic data versus when trained on real subject\ndata. Together, these results offer a potential solution to two of the largest\nchallenges facing machine learning in medical imaging, namely the small\nincidence of pathological findings, and the restrictions around sharing of\npatient data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 16:25:18 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:11:23 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Tenenholtz", "Neil A", ""], ["Rogers", "Jameson K", ""], ["Schwarz", "Christopher G", ""], ["Senjem", "Matthew L", ""], ["Gunter", "Jeffrey L", ""], ["Andriole", "Katherine", ""], ["Michalski", "Mark", ""]]}, {"id": "1807.10254", "submitter": "Gabriela Csurka", "authors": "Gabriela Csurka, Christopher R. Dance and Martin Humenberger", "title": "From handcrafted to deep local features", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an overview of the evolution of local features from\nhandcrafted to deep-learning-based methods, followed by a discussion of several\nbenchmarks and papers evaluating such local features. Our investigations are\nmotivated by 3D reconstruction problems, where the precise location of the\nfeatures is important. As we describe these methods, we highlight and explain\nthe challenges of feature extraction and potential ways to overcome them. We\nfirst present handcrafted methods, followed by methods based on classical\nmachine learning and finally we discuss methods based on deep-learning. This\nlargely chronologically-ordered presentation will help the reader to fully\nunderstand the topic of image and region description in order to make best use\nof it in modern computer vision applications. In particular, understanding\nhandcrafted methods and their motivation can help to understand modern\napproaches and how machine learning is used to improve the results. We also\nprovide references to most of the relevant literature and code.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:28:28 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 10:57:58 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 13:52:49 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Csurka", "Gabriela", ""], ["Dance", "Christopher R.", ""], ["Humenberger", "Martin", ""]]}, {"id": "1807.10264", "submitter": "Shubham Tulsiani", "authors": "Shubham Tulsiani, Richard Tucker, Noah Snavely", "title": "Layer-structured 3D Scene Inference via View Synthesis", "comments": "Project url: http://shubhtuls.github.io/lsi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to infer a layer-structured 3D representation of a\nscene from a single input image. This allows us to infer not only the depth of\nthe visible pixels, but also to capture the texture and depth for content in\nthe scene that is not directly visible. We overcome the challenge posed by the\nlack of direct supervision by instead leveraging a more naturally available\nmulti-view supervisory signal. Our insight is to use view synthesis as a proxy\ntask: we enforce that our representation (inferred from a single image), when\nrendered from a novel perspective, matches the true observed image. We present\na learning framework that operationalizes this insight using a new,\ndifferentiable novel view renderer. We provide qualitative and quantitative\nvalidation of our approach in two different settings, and demonstrate that we\ncan learn to capture the hidden aspects of a scene.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:44:09 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Tulsiani", "Shubham", ""], ["Tucker", "Richard", ""], ["Snavely", "Noah", ""]]}, {"id": "1807.10267", "submitter": "Anurag Ranjan", "authors": "Anurag Ranjan and Timo Bolkart and Soubhik Sanyal and Michael J. Black", "title": "Generating 3D faces using Convolutional Mesh Autoencoders", "comments": null, "journal-ref": "European Conference on Computer Vision 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned 3D representations of human faces are useful for computer vision\nproblems such as 3D face tracking and reconstruction from images, as well as\ngraphics applications such as character generation and animation. Traditional\nmodels learn a latent representation of a face using linear subspaces or\nhigher-order tensor generalizations. Due to this linearity, they can not\ncapture extreme deformations and non-linear expressions. To address this, we\nintroduce a versatile model that learns a non-linear representation of a face\nusing spectral convolutions on a mesh surface. We introduce mesh sampling\noperations that enable a hierarchical mesh representation that captures\nnon-linear variations in shape and expression at multiple scales within the\nmodel. In a variational setting, our model samples diverse realistic 3D faces\nfrom a multivariate Gaussian distribution. Our training data consists of 20,466\nmeshes of extreme expressions captured over 12 different subjects. Despite\nlimited training data, our trained model outperforms state-of-the-art face\nmodels with 50% lower reconstruction error, while using 75% fewer parameters.\nWe also show that, replacing the expression space of an existing\nstate-of-the-art face model with our autoencoder, achieves a lower\nreconstruction error. Our data, model and code are available at\nhttp://github.com/anuragranj/coma\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:53:50 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 14:15:05 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 20:13:00 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Ranjan", "Anurag", ""], ["Bolkart", "Timo", ""], ["Sanyal", "Soubhik", ""], ["Black", "Michael J.", ""]]}, {"id": "1807.10272", "submitter": "Anish Athalye", "authors": "Logan Engstrom, Andrew Ilyas, Anish Athalye", "title": "Evaluating and Understanding the Robustness of Adversarial Logit Pairing", "comments": "NeurIPS SECML 2018. Source code at\n  https://github.com/labsix/adversarial-logit-pairing-analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate the robustness of Adversarial Logit Pairing, a recently proposed\ndefense against adversarial examples. We find that a network trained with\nAdversarial Logit Pairing achieves 0.6% accuracy in the threat model in which\nthe defense is considered. We provide a brief overview of the defense and the\nthreat models/claims considered, as well as a discussion of the methodology and\nresults of our attack, which may offer insights into the reasons underlying the\nvulnerability of ALP to adversarial attack.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:58:26 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 19:07:57 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Engstrom", "Logan", ""], ["Ilyas", "Andrew", ""], ["Athalye", "Anish", ""]]}, {"id": "1807.10278", "submitter": "Hao Yan", "authors": "Hao Yan, Kamran Paynabar, Massimo Pacella", "title": "Structured Point Cloud Data Analysis via Regularized Tensor Regression\n  for Process Modeling and Optimization", "comments": "Technometrics, accepted", "journal-ref": "Technometrics 61.3 (2019): 385-395", "doi": "10.1080/00401706.2018.1529628", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced 3D metrology technologies such as Coordinate Measuring Machine (CMM)\nand laser 3D scanners have facilitated the collection of massive point cloud\ndata, beneficial for process monitoring, control and optimization. However, due\nto their high dimensionality and structure complexity, modeling and analysis of\npoint clouds are still a challenge. In this paper, we utilize multilinear\nalgebra techniques and propose a set of tensor regression approaches to model\nthe variational patterns of point clouds and to link them to process variables.\nThe performance of the proposed methods is evaluated through simulations and a\nreal case study of turning process optimization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 02:57:49 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 15:46:11 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 08:33:17 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yan", "Hao", ""], ["Paynabar", "Kamran", ""], ["Pacella", "Massimo", ""]]}, {"id": "1807.10303", "submitter": "Joris Gu\\'erin", "authors": "Joris Gu\\'erin, Olivier Gibaru, Eric Nyiri, St\\'ephane Thiery and\n  Byron Boots", "title": "Semantically Meaningful View Selection", "comments": "6 pages double columns, 5 figures, 3 tables, Accepted for\n  presentation at IROS 2018, Madrid, Spain (46% acceptance)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An understanding of the nature of objects could help robots to solve both\nhigh-level abstract tasks and improve performance at lower-level concrete\ntasks. Although deep learning has facilitated progress in image understanding,\na robot's performance in problems like object recognition often depends on the\nangle from which the object is observed. Traditionally, robot sorting tasks\nrely on a fixed top-down view of an object. By changing its viewing angle, a\nrobot can select a more semantically informative view leading to better\nperformance for object recognition. In this paper, we introduce the problem of\nsemantic view selection, which seeks to find good camera poses to gain semantic\nknowledge about an observed object. We propose a conceptual formulation of the\nproblem, together with a solvable relaxation based on clustering. We then\npresent a new image dataset consisting of around 10k images representing\nvarious views of 144 objects under different poses. Finally we use this dataset\nto propose a first solution to the problem by training a neural network to\npredict a \"semantic score\" from a top view image and camera pose. The views\npredicted to have higher scores are then shown to provide better clustering\nresults than fixed top-down views.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 18:17:19 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Gu\u00e9rin", "Joris", ""], ["Gibaru", "Olivier", ""], ["Nyiri", "Eric", ""], ["Thiery", "St\u00e9phane", ""], ["Boots", "Byron", ""]]}, {"id": "1807.10335", "submitter": "Siddharth Krishna Kumar", "authors": "Siddharth Krishna Kumar", "title": "A general metric for identifying adversarial images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that a determined adversary can fool a neural network by\nmaking imperceptible adversarial perturbations to an image. Recent studies have\nshown that these perturbations can be detected even without information about\nthe neural network if the strategy taken by the adversary is known beforehand.\nUnfortunately, these studies suffer from the generalization limitation -- the\ndetection method has to be recalibrated every time the adversary changes his\nstrategy. In this study, we attempt to overcome the generalization limitation\nby deriving a metric which reliably identifies adversarial images even when the\napproach taken by the adversary is unknown. Our metric leverages key\ndifferences between the spectra of clean and adversarial images when an image\nis treated as a matrix. Our metric is able to detect adversarial images across\ndifferent datasets and attack strategies without any additional re-calibration.\nIn addition, our approach provides geometric insights into several unanswered\nquestions about adversarial perturbations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 19:29:37 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kumar", "Siddharth Krishna", ""]]}, {"id": "1807.10336", "submitter": "Anil Sharma", "authors": "Anil Sharma, Prabhat Kumar, Saket Anand, Sanjit K. Kaul", "title": "A Reinforcement Learning Approach to Target Tracking in a Camera Network", "comments": "The current version has a fault in the experiments section so we\n  would like to withdraw the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Target tracking in a camera network is an important task for surveillance and\nscene understanding. The task is challenging due to disjoint views and\nillumination variation in different cameras. In this direction, many\ngraph-based methods were proposed using appearance-based features. However, the\nappearance information fades with high illumination variation in the different\ncamera FOVs. We, in this paper, use spatial and temporal information as the\nstate of the target to learn a policy that predicts the next camera given the\ncurrent state. The policy is trained using Q-learning and it does not assume\nany information about the topology of the camera network. We will show that the\npolicy learns the camera network topology. We demonstrate the performance of\nthe proposed method on the NLPR MCT dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 19:37:57 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:03:06 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Sharma", "Anil", ""], ["Kumar", "Prabhat", ""], ["Anand", "Saket", ""], ["Kaul", "Sanjit K.", ""]]}, {"id": "1807.10376", "submitter": "Qi Guo", "authors": "Qi Guo, Iuri Frosio, Orazio Gallo, Todd Zickler, Jan Kautz", "title": "Tackling 3D ToF Artifacts Through Learning and the FLAT Dataset", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene motion, multiple reflections, and sensor noise introduce artifacts in\nthe depth reconstruction performed by time-of-flight cameras. We propose a\ntwo-stage, deep-learning approach to address all of these sources of artifacts\nsimultaneously. We also introduce FLAT, a synthetic dataset of 2000 ToF\nmeasurements that capture all of these nonidealities, and allows to simulate\ndifferent camera hardware. Using the Kinect 2 camera as a baseline, we show\nimproved reconstruction errors over state-of-the-art methods, on both simulated\nand real data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 21:43:32 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Guo", "Qi", ""], ["Frosio", "Iuri", ""], ["Gallo", "Orazio", ""], ["Zickler", "Todd", ""], ["Kautz", "Jan", ""]]}, {"id": "1807.10378", "submitter": "Yanchao Yang", "authors": "Yanchao Yang and Stefano Soatto", "title": "Conditional Prior Networks for Optical Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical computation of optical flow involves generic priors (regularizers)\nthat capture rudimentary statistics of images, but not long-range correlations\nor semantics. On the other hand, fully supervised methods learn the regularity\nin the annotated data, without explicit regularization and with the risk of\noverfitting. We seek to learn richer priors on the set of possible flows that\nare statistically compatible with an image. Once the prior is learned in a\nsupervised fashion, one can easily learn the full map to infer optical flow\ndirectly from two or more images, without any need for (additional)\nsupervision. We introduce a novel architecture, called Conditional Prior\nNetwork (CPN), and show how to train it to yield a conditional prior. When used\nin conjunction with a simple optical flow architecture, the CPN beats all\nvariational methods and all unsupervised learning-based ones using the same\ndata term. It performs comparably to fully supervised ones, that however are\nfine-tuned to a particular dataset. Our method, on the other hand, performs\nwell even when transferred between datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 21:49:36 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Yang", "Yanchao", ""], ["Soatto", "Stefano", ""]]}, {"id": "1807.10400", "submitter": "Anirudh Som", "authors": "Anirudh Som, Kowshik Thopalli, Karthikeyan Natesan Ramamurthy, Vinay\n  Venkataraman, Ankita Shukla, Pavan Turaga", "title": "Perturbation Robust Representations of Topological Persistence Diagrams", "comments": "19 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological methods for data analysis present opportunities for enforcing\ncertain invariances of broad interest in computer vision, including view-point\nin activity analysis, articulation in shape analysis, and measurement\ninvariance in non-linear dynamical modeling. The increasing success of these\nmethods is attributed to the complementary information that topology provides,\nas well as availability of tools for computing topological summaries such as\npersistence diagrams. However, persistence diagrams are multi-sets of points\nand hence it is not straightforward to fuse them with features used for\ncontemporary machine learning tools like deep-nets. In this paper we present\ntheoretically well-grounded approaches to develop novel perturbation robust\ntopological representations, with the long-term view of making them amenable to\nfusion with contemporary learning architectures. We term the proposed\nrepresentation as Perturbed Topological Signatures, which live on a Grassmann\nmanifold and hence can be efficiently used in machine learning pipelines. We\nexplore the use of the proposed descriptor on three applications: 3D shape\nanalysis, view-invariant activity analysis, and non-linear dynamical modeling.\nWe show favorable results in both high-level recognition performance and\ntime-complexity when compared to other baseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 23:39:52 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Som", "Anirudh", ""], ["Thopalli", "Kowshik", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Venkataraman", "Vinay", ""], ["Shukla", "Ankita", ""], ["Turaga", "Pavan", ""]]}, {"id": "1807.10413", "submitter": "Ulrich Viereck", "authors": "Ulrich Viereck, Xingchao Peng, Kate Saenko, Robert Platt", "title": "Adapting control policies from simulation to reality using a pairwise\n  loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to domain transfer based on a pairwise loss\nfunction that helps transfer control policies learned in simulation onto a real\nrobot. We explore the idea in the context of a 'category level' manipulation\ntask where a control policy is learned that enables a robot to perform a mating\ntask involving novel objects. We explore the case where depth images are used\nas the main form of sensor input. Our experimental results demonstrate that\nproposed method consistently outperforms baseline methods that train only in\nsimulation or that combine real and simulated data in a naive way.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 01:54:08 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 19:42:23 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Viereck", "Ulrich", ""], ["Peng", "Xingchao", ""], ["Saenko", "Kate", ""], ["Platt", "Robert", ""]]}, {"id": "1807.10417", "submitter": "Weitong Zhang", "authors": "Weitong Zhang", "title": "Characters Detection on Namecard with faster RCNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply Faster R-CNN to the detection of characters in namecard, in order to\nsolve the problem of a small amount of data and the inbalance between different\nclass, we designed the data augmentation and the 'fake' data generalizer to\ngenerate more data for the training of network. Without using data\naugmentation, the average IoU in correct samples could be no less than 80% and\nthe mAP result of 80% was also achieved with Faster R-CNN. By applying the data\naugmentation, the variance of mAP is decreased and both of the IoU and mAP\nscore has increased a little.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 02:17:58 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Zhang", "Weitong", ""]]}, {"id": "1807.10418", "submitter": "Sujoy Paul", "authors": "Sujoy Paul, Sourya Roy, Amit K Roy-Chowdhury", "title": "W-TALC: Weakly-supervised Temporal Activity Localization and\n  Classification", "comments": "Accepted at European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most activity localization methods in the literature suffer from the burden\nof frame-wise annotation requirement. Learning from weak labels may be a\npotential solution towards reducing such manual labeling effort. Recent years\nhave witnessed a substantial influx of tagged videos on the Internet, which can\nserve as a rich source of weakly-supervised training data. Specifically, the\ncorrelations between videos with similar tags can be utilized to temporally\nlocalize the activities. Towards this goal, we present W-TALC, a\nWeakly-supervised Temporal Activity Localization and Classification framework\nusing only video-level labels. The proposed network can be divided into two\nsub-networks, namely the Two-Stream based feature extractor network and a\nweakly-supervised module, which we learn by optimizing two complimentary loss\nfunctions. Qualitative and quantitative results on two challenging datasets -\nThumos14 and ActivityNet1.2, demonstrate that the proposed method is able to\ndetect activities at a fine granularity and achieve better performance than\ncurrent state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 02:31:49 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 01:13:46 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 17:02:27 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Paul", "Sujoy", ""], ["Roy", "Sourya", ""], ["Roy-Chowdhury", "Amit K", ""]]}, {"id": "1807.10421", "submitter": "Haoyi Wang", "authors": "Haoyi Wang, Xingjie Wei, Victor Sanchez, Chang-Tsun Li", "title": "Fusion Network for Face-based Age Estimation", "comments": "ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been applied to age-related research\nas the core framework. Although faces are composed of numerous facial\nattributes, most works with CNNs still consider a face as a typical object and\ndo not pay enough attention to facial regions that carry age-specific feature\nfor this particular task. In this paper, we propose a novel CNN architecture\ncalled Fusion Network (FusionNet) to tackle the age estimation problem. Apart\nfrom the whole face image, the FusionNet successively takes several\nage-specific facial patches as part of the input to emphasize the age-specific\nfeatures. Through experiments, we show that the FusionNet significantly\noutperforms other state-of-the-art models on the MORPH II benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 03:22:10 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Wang", "Haoyi", ""], ["Wei", "Xingjie", ""], ["Sanchez", "Victor", ""], ["Li", "Chang-Tsun", ""]]}, {"id": "1807.10437", "submitter": "Eunji Chong", "authors": "Eunji Chong, Nataniel Ruiz, Yongxin Wang, Yun Zhang, Agata Rozga,\n  James Rehg", "title": "Connecting Gaze, Scene, and Attention: Generalized Attention Estimation\n  via Joint Modeling of Gaze and Scene Saliency", "comments": "Appears in: European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenging problem of estimating the general visual\nattention of people in images. Our proposed method is designed to work across\nmultiple naturalistic social scenarios and provides a full picture of the\nsubject's attention and gaze. In contrast, earlier works on gaze and attention\nestimation have focused on constrained problems in more specific contexts. In\nparticular, our model explicitly represents the gaze direction and handles\nout-of-frame gaze targets. We leverage three different datasets using a\nmulti-task learning approach. We evaluate our method on widely used benchmarks\nfor single-tasks such as gaze angle estimation and attention-within-an-image,\nas well as on the new challenging task of generalized visual attention\nprediction. In addition, we have created extended annotations for the MMDB and\nGazeFollow datasets which are used in our experiments, which we will publicly\nrelease.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 05:25:52 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Chong", "Eunji", ""], ["Ruiz", "Nataniel", ""], ["Wang", "Yongxin", ""], ["Zhang", "Yun", ""], ["Rozga", "Agata", ""], ["Rehg", "James", ""]]}, {"id": "1807.10441", "submitter": "Spandan Madan", "authors": "Spandan Madan (1), Zoya Bylinskii (1), Matthew Tancik (1), Adri\\`a\n  Recasens (1), Kimberli Zhong (1), Sami Alsheikh (1), Hanspeter Pfister (2),\n  Aude Oliva (1), Fredo Durand (1) ((1) Massachusetts Institute of Technology,\n  (2) Harvard University)", "title": "Synthetically Trained Icon Proposals for Parsing and Summarizing\n  Infographics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used in news, business, and educational media, infographics are\nhandcrafted to effectively communicate messages about complex and often\nabstract topics including `ways to conserve the environment' and `understanding\nthe financial crisis'. Composed of stylistically and semantically diverse\nvisual and textual elements, infographics pose new challenges for computer\nvision. While automatic text extraction works well on infographics, computer\nvision approaches trained on natural images fail to identify the stand-alone\nvisual elements in infographics, or `icons'. To bridge this representation gap,\nwe propose a synthetic data generation strategy: we augment background patches\nin infographics from our Visually29K dataset with Internet-scraped icons which\nwe use as training data for an icon proposal mechanism. On a test set of 1K\nannotated infographics, icons are located with 38% precision and 34% recall\n(the best model trained with natural images achieves 14% precision and 7%\nrecall). Combining our icon proposals with icon classification and text\nextraction, we present a multi-modal summarization application. Our application\ntakes an infographic as input and automatically produces text tags and visual\nhashtags that are textually and visually representative of the infographic's\ntopics respectively.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 05:33:09 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Madan", "Spandan", ""], ["Bylinskii", "Zoya", ""], ["Tancik", "Matthew", ""], ["Recasens", "Adri\u00e0", ""], ["Zhong", "Kimberli", ""], ["Alsheikh", "Sami", ""], ["Pfister", "Hanspeter", ""], ["Oliva", "Aude", ""], ["Durand", "Fredo", ""]]}, {"id": "1807.10466", "submitter": "Nikolay Burlutskiy", "authors": "Nikolay Burlutskiy, Feng Gu, Lena Kajland Wilen, Max Backman, Patrick\n  Micke", "title": "A Deep Learning Framework for Automatic Diagnosis in Lung Cancer", "comments": "Presented as a poster at Medical Imaging with Deep Learning (MIDL) in\n  Amsterdam, 4-6th July 2018 (http://midl.amsterdam/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a deep learning framework that helps to automatically identify\nand segment lung cancer areas in patients' tissue specimens. The study was\nbased on a cohort of lung cancer patients operated at the Uppsala University\nHospital. The tissues were reviewed by lung pathologists and then the cores\nwere compiled to tissue micro-arrays (TMAs). For experiments, hematoxylin-eosin\nstained slides from 712 patients were scanned and then manually annotated. Then\nthese scans and annotations were used to train segmentation models of the\ndeveloped framework. The performance of the developed deep learning framework\nwas evaluated on fully annotated TMA cores from 178 patients reaching\npixel-wise precision of 0.80 and recall of 0.86. Finally, publicly available\nStanford TMA cores were used to demonstrate high performance of the framework\nqualitatively.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:32:46 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Burlutskiy", "Nikolay", ""], ["Gu", "Feng", ""], ["Wilen", "Lena Kajland", ""], ["Backman", "Max", ""], ["Micke", "Patrick", ""]]}, {"id": "1807.10482", "submitter": "Xiang Li", "authors": "Xiang Li, Ancong Wu and Wei-Shi Zheng", "title": "Adversarial Open-World Person Re-Identification", "comments": "17 pages, 3 figures, Accepted by European Conference on Computer\n  Vision 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical real-world application of re-id, a watch-list (gallery set) of a\nhandful of target people (e.g. suspects) to track around a large volume of\nnon-target people are demanded across camera views, and this is called the\nopen-world person re-id. Different from conventional (closed-world) person\nre-id, a large portion of probe samples are not from target people in the\nopen-world setting. And, it always happens that a non-target person would look\nsimilar to a target one and therefore would seriously challenge a re-id system.\nIn this work, we introduce a deep open-world group-based person re-id model\nbased on adversarial learning to alleviate the attack problem caused by similar\nnon-target people. The main idea is learning to attack feature extractor on the\ntarget people by using GAN to generate very target-like images (imposters), and\nin the meantime the model will make the feature extractor learn to tolerate the\nattack by discriminative learning so as to realize group-based verification.\nThe framework we proposed is called the adversarial open-world person\nre-identification, and this is realized by our Adversarial PersonNet (APN) that\njointly learns a generator, a person discriminator, a target discriminator and\na feature extractor, where the feature extractor and target discriminator share\nthe same weights so as to makes the feature extractor learn to tolerate the\nattack by imposters for better group-based verification. While open-world\nperson re-id is challenging, we show for the first time that the\nadversarial-based approach helps stabilize person re-id system under imposter\nattack more effectively.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 08:15:48 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 13:43:22 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 05:15:05 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Li", "Xiang", ""], ["Wu", "Ancong", ""], ["Zheng", "Wei-Shi", ""]]}, {"id": "1807.10487", "submitter": "Karthik Desingh", "authors": "Karthik Desingh, Anthony Opipari, Odest Chadwicke Jenkins", "title": "Pull Message Passing for Nonparametric Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a \"pull\" approach to approximate products of Gaussian mixtures\nwithin message updates for Nonparametric Belief Propagation (NBP) inference.\nExisting NBP methods often represent messages between continuous-valued latent\nvariables as Gaussian mixture models. To avoid computational intractability in\nloopy graphs, NBP necessitates an approximation of the product of such\nmixtures. Sampling-based product approximations have shown effectiveness for\nNBP inference. However, such approximations used within the traditional \"push\"\nmessage update procedures quickly become computationally prohibitive for\nmulti-modal distributions over high-dimensional variables. In contrast, we\npropose a \"pull\" method, as the Pull Message Passing for Nonparametric Belief\npropagation (PMPNBP) algorithm, and demonstrate its viability for efficient\ninference. We report results using an experiment from an existing NBP method,\nPAMPAS, for inferring the pose of an articulated structure in clutter. Results\nfrom this illustrative problem found PMPNBP has a greater ability to\nefficiently scale the number of components in its mixtures and, consequently,\nimprove inference accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 08:24:55 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Desingh", "Karthik", ""], ["Opipari", "Anthony", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1807.10510", "submitter": "Qingqiu Huang", "authors": "Qingqiu Huang, Wentao Liu, Dahua Lin", "title": "Person Search in Videos with One Portrait Through Visual and Temporal\n  Links", "comments": "European Conference on Computer Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world applications, e.g. law enforcement and video retrieval, one\noften needs to search a certain person in long videos with just one portrait.\nThis is much more challenging than the conventional settings for person\nre-identification, as the search may need to be carried out in the environments\ndifferent from where the portrait was taken. In this paper, we aim to tackle\nthis challenge and propose a novel framework, which takes into account the\nidentity invariance along a tracklet, thus allowing person identities to be\npropagated via both the visual and the temporal links. We also develop a novel\nscheme called Progressive Propagation via Competitive Consensus, which\nsignificantly improves the reliability of the propagation process. To promote\nthe study of person search, we construct a large-scale benchmark, which\ncontains 127K manually annotated tracklets from 192 movies. Experiments show\nthat our approach remarkably outperforms mainstream person re-id methods,\nraising the mAP from 42.16% to 62.27%.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 09:39:28 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Huang", "Qingqiu", ""], ["Liu", "Wentao", ""], ["Lin", "Dahua", ""]]}, {"id": "1807.10517", "submitter": "Emanuele Rodol\\`a", "authors": "Riccardo Marin, Simone Melzi, Emanuele Rodol\\`a, Umberto Castellani", "title": "FARM: Functional Automatic Registration Method for 3D Human Bodies", "comments": "Under submission to CGF", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for non-rigid registration of 3D human shapes. Our\nproposed pipeline builds upon a given parametric model of the human, and makes\nuse of the functional map representation for encoding and inferring shape maps\nthroughout the registration process. This combination endows our method with\nrobustness to a large variety of nuisances observed in practical settings,\nincluding non-isometric transformations, downsampling, topological noise, and\nocclusions; further, the pipeline can be applied invariably across different\nshape representations (e.g. meshes and point clouds), and in the presence of\n(even dramatic) missing parts such as those arising in real-world depth sensing\napplications. We showcase our method on a selection of challenging tasks,\ndemonstrating results in line with, or even surpassing, state-of-the-art\nmethods in the respective areas.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 09:53:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Marin", "Riccardo", ""], ["Melzi", "Simone", ""], ["Rodol\u00e0", "Emanuele", ""], ["Castellani", "Umberto", ""]]}, {"id": "1807.10520", "submitter": "Anjith George", "authors": "Anjith George, Aurobinda Routray", "title": "ESCaF: Pupil Centre Localization Algorithm with Candidate Filtering", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for accurate localization of pupil centre is essential for gaze\ntracking in real world conditions. Most of the algorithms fail in real world\nconditions like illumination variations, contact lenses, glasses, eye makeup,\nmotion blur, noise, etc. We propose a new algorithm which improves the\ndetection rate in real world conditions. The proposed algorithm uses both edges\nas well as intensity information along with a candidate filtering approach to\nidentify the best pupil candidate. A simple tracking scheme has also been added\nwhich improves the processing speed. The algorithm has been evaluated in\nLabelled Pupil in the Wild (LPW) dataset, largest in its class which contains\nreal world conditions. The proposed algorithm outperformed the state of the art\nalgorithms while achieving real-time performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 10:08:11 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["George", "Anjith", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1807.10547", "submitter": "Haitian Zheng", "authors": "Haitian Zheng, Mengqi Ji, Haoqian Wang, Yebin Liu, Lu Fang", "title": "CrossNet: An End-to-end Reference-based Super Resolution Network using\n  Cross-scale Warping", "comments": "To be appeared in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reference-based Super-resolution (RefSR) super-resolves a low-resolution\n(LR) image given an external high-resolution (HR) reference image, where the\nreference image and LR image share similar viewpoint but with significant\nresolution gap x8. Existing RefSR methods work in a cascaded way such as patch\nmatching followed by synthesis pipeline with two independently defined\nobjective functions, leading to the inter-patch misalignment, grid effect and\ninefficient optimization. To resolve these issues, we present CrossNet, an\nend-to-end and fully-convolutional deep neural network using cross-scale\nwarping. Our network contains image encoders, cross-scale warping layers, and\nfusion decoder: the encoder serves to extract multi-scale features from both\nthe LR and the reference images; the cross-scale warping layers spatially\naligns the reference feature map with the LR feature map; the decoder finally\naggregates feature maps from both domains to synthesize the HR output. Using\ncross-scale warping, our network is able to perform spatial alignment at\npixel-level in an end-to-end fashion, which improves the existing schemes both\nin precision (around 2dB-4dB) and efficiency (more than 100 times faster).\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:15:40 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Zheng", "Haitian", ""], ["Ji", "Mengqi", ""], ["Wang", "Haoqian", ""], ["Liu", "Yebin", ""], ["Fang", "Lu", ""]]}, {"id": "1807.10550", "submitter": "A. Sophia Koepke", "authors": "Olivia Wiles, A. Sophia Koepke, Andrew Zisserman", "title": "X2Face: A network for controlling face generation by using images,\n  audio, and pose codes", "comments": "To appear in ECCV 2018. Accompanying video:\n  http://www.robots.ox.ac.uk/~vgg/research/unsup_learn_watch_faces/x2face.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is a neural network model that controls the pose\nand expression of a given face, using another face or modality (e.g. audio).\nThis model can then be used for lightweight, sophisticated video and image\nediting.\n  We make the following three contributions. First, we introduce a network,\nX2Face, that can control a source face (specified by one or more frames) using\nanother face in a driving frame to produce a generated frame with the identity\nof the source frame but the pose and expression of the face in the driving\nframe. Second, we propose a method for training the network fully\nself-supervised using a large collection of video data. Third, we show that the\ngeneration process can be driven by other modalities, such as audio or pose\ncodes, without any further training of the network.\n  The generation results for driving a face with another face are compared to\nstate-of-the-art self-supervised/supervised methods. We show that our approach\nis more robust than other methods, as it makes fewer assumptions about the\ninput data. We also show examples of using our framework for video face\nediting.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:31:16 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Wiles", "Olivia", ""], ["Koepke", "A. Sophia", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1807.10552", "submitter": "Yongxiang Huang", "authors": "Yongxiang Huang and Albert Chi-shing Chung", "title": "Improving High Resolution Histology Image Classification with Deep\n  Spatial Fusion Network", "comments": "8 pages, MICCAI workshop preceedings", "journal-ref": "Computational Pathology and Ophthalmic Medical Image Analysis.\n  Springer, Cham, 2018. 19-26", "doi": "10.1007/978-3-030-00949-6_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Histology imaging is an essential diagnosis method to finalize the grade and\nstage of cancer of different tissues, especially for breast cancer diagnosis.\nSpecialists often disagree on the final diagnosis on biopsy tissue due to the\ncomplex morphological variety. Although convolutional neural networks (CNN)\nhave advantages in extracting discriminative features in image classification,\ndirectly training a CNN on high resolution histology images is computationally\ninfeasible currently. Besides, inconsistent discriminative features often\ndistribute over the whole histology image, which incurs challenges in\npatch-based CNN classification method. In this paper, we propose a novel\narchitecture for automatic classification of high resolution histology images.\nFirst, an adapted residual network is employed to explore hierarchical features\nwithout attenuation. Second, we develop a robust deep fusion network to utilize\nthe spatial relationship between patches and learn to correct the prediction\nbias generated from inconsistent discriminative feature distribution. The\nproposed method is evaluated using 10-fold cross-validation on 400 high\nresolution breast histology images with balanced labels and reports 95%\naccuracy on 4-class classification and 98.5% accuracy, 99.6% AUC on 2-class\nclassification (carcinoma and non-carcinoma), which substantially outperforms\nprevious methods and close to pathologist performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:34:34 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Huang", "Yongxiang", ""], ["Chung", "Albert Chi-shing", ""]]}, {"id": "1807.10561", "submitter": "Mickey Li Mr", "authors": "Mickey Li, Noyan Songur, Pavel Orlov, Stefan Leutenegger, A Aldo\n  Faisal", "title": "Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction\n  from ego-centric eye-tracker videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating the physical environment is essential for a complete\nunderstanding of human behavior in unconstrained every-day tasks. This is\nespecially important in ego-centric tasks where obtaining 3 dimensional\ninformation is both limiting and challenging with the current 2D video analysis\nmethods proving insufficient. Here we demonstrate a proof-of-concept system\nwhich provides real-time 3D mapping and semantic labeling of the local\nenvironment from an ego-centric RGB-D video-stream with 3D gaze point\nestimation from head mounted eye tracking glasses. We augment existing work in\nSemantic Simultaneous Localization And Mapping (Semantic SLAM) with collected\ngaze vectors. Our system can then find and track objects both inside and\noutside the user field-of-view in 3D from multiple perspectives with reasonable\naccuracy. We validate our concept by producing a semantic map from images of\nthe NYUv2 dataset while simultaneously estimating gaze position and gaze\nclasses from recorded gaze data of the dataset images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:51:36 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Li", "Mickey", ""], ["Songur", "Noyan", ""], ["Orlov", "Pavel", ""], ["Leutenegger", "Stefan", ""], ["Faisal", "A Aldo", ""]]}, {"id": "1807.10565", "submitter": "Odysseas Zisimopoulos", "authors": "Odysseas Zisimopoulos, Evangello Flouty, Imanol Luengo, Petros\n  Giataganas, Jean Nehme, Andre Chow, and Danail Stoyanov", "title": "DeepPhase: Surgical Phase Recognition in CATARACTS Videos", "comments": "8 pages, 3 figures, 1 table, MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated surgical workflow analysis and understanding can assist surgeons to\nstandardize procedures and enhance post-surgical assessment and indexing, as\nwell as, interventional monitoring. Computer-assisted interventional (CAI)\nsystems based on video can perform workflow estimation through surgical\ninstruments' recognition while linking them to an ontology of procedural\nphases. In this work, we adopt a deep learning paradigm to detect surgical\ninstruments in cataract surgery videos which in turn feed a surgical phase\ninference recurrent network that encodes temporal aspects of phase steps within\nthe phase classification. Our models present comparable to state-of-the-art\nresults for surgical tool detection and phase recognition with accuracies of 99\nand 78% respectively.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:50:03 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Zisimopoulos", "Odysseas", ""], ["Flouty", "Evangello", ""], ["Luengo", "Imanol", ""], ["Giataganas", "Petros", ""], ["Nehme", "Jean", ""], ["Chow", "Andre", ""], ["Stoyanov", "Danail", ""]]}, {"id": "1807.10568", "submitter": "Aboul Ella Hassanien Abo", "authors": "Chintan Bhatt, Aboul-ella Hassanien, Nirav Alpesh Shah, Jaydeep Thik", "title": "Barqi Breed Sheep Weight Estimation based on Neural Network with\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision is a very powerful method for understanding the contents from\nthe images. We tried to utilize this powerful technology to make the difficult\ntask of estimating sheep weights quick and accurate. It has enabled us to\nminimize the human involvement in measuring weight of the sheep. We are using a\nnovel approach for segmentation and neural network based regression model for\nachieving better results for the task of estimating sheep weight.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 16:20:44 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Bhatt", "Chintan", ""], ["Hassanien", "Aboul-ella", ""], ["Shah", "Nirav Alpesh", ""], ["Thik", "Jaydeep", ""]]}, {"id": "1807.10569", "submitter": "Gerald Friedland", "authors": "Gerald Friedland, Jingkang Wang, Ruoxi Jia, Bo Li", "title": "The Helmholtz Method: Using Perceptual Compression to Reduce Machine\n  Learning Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a fundamental answer to a frequently asked question in\nmultimedia computing and machine learning: Do artifacts from perceptual\ncompression contribute to error in the machine learning process and if so, how\nmuch? Our approach to the problem is a reinterpretation of the Helmholtz Free\nEnergy formula from physics to explain the relationship between content and\nnoise when using sensors (such as cameras or microphones) to capture multimedia\ndata. The reinterpretation allows a bit-measurement of the noise contained in\nimages, audio, and video by combining a classifier with perceptual compression,\nsuch as JPEG or MP3. Our experiments on CIFAR-10 as well as Fraunhofer's\nIDMT-SMT-Audio-Effects dataset indicate that, at the right quality level,\nperceptual compression is actually not harmful but contributes to a significant\nreduction of complexity of the machine learning process. That is, our noise\nquantification method can be used to speed up the training of deep learning\nclassifiers significantly while maintaining, or sometimes even improving,\noverall classification accuracy. Moreover, our results provide insights into\nthe reasons for the success of deep learning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 01:49:50 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Friedland", "Gerald", ""], ["Wang", "Jingkang", ""], ["Jia", "Ruoxi", ""], ["Li", "Bo", ""]]}, {"id": "1807.10570", "submitter": "Pedram Ghazi", "authors": "Pedram Ghazi, Antti P. Happonen, Jani Boutellier, and Heikki Huttunen", "title": "Embedded Implementation of a Deep Learning Smile Detector", "comments": "This work has been submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the real time deployment of deep learning algorithms\nin low resource computational environments. As the use case, we compare the\naccuracy and speed of neural networks for smile detection using different\nneural network architectures and their system level implementation on NVidia\nJetson embedded platform. We also propose an asynchronous multithreading scheme\nfor parallelizing the pipeline. Within this framework, we experimentally\ncompare thirteen widely used network topologies. The experiments show that low\ncomplexity architectures can achieve almost equal performance as larger ones,\nwith a fraction of computation required.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 07:37:37 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ghazi", "Pedram", ""], ["Happonen", "Antti P.", ""], ["Boutellier", "Jani", ""], ["Huttunen", "Heikki", ""]]}, {"id": "1807.10571", "submitter": "Jun Cheng", "authors": "Jun Cheng", "title": "Sparse Range-constrained Learning and Its Application for Medical Image\n  Grading", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2018.2851607", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse learning has been shown to be effective in solving many real-world\nproblems. Finding sparse representations is a fundamentally important topic in\nmany fields of science including signal processing, computer vision, genome\nstudy and medical imaging. One important issue in applying sparse\nrepresentation is to find the basis to represent the data,especially in\ncomputer vision and medical imaging where the data is not necessary incoherent.\nIn medical imaging, clinicians often grade the severity or measure the risk\nscore of a disease based on images. This process is referred to as medical\nimage grading. Manual grading of the disease severity or risk score is often\nused. However, it is tedious, subjective and expensive. Sparse learning has\nbeen used for automatic grading of medical images for different diseases. In\nthe grading, we usually begin with one step to find a sparse representation of\nthe testing image using a set of reference images or atoms from the dictionary.\nThen in the second step, the selected atoms are used as references to compute\nthe grades of the testing images. Since the two steps are conducted\nsequentially, the objective function in the first step is not necessarily\noptimized for the second step. In this paper, we propose a novel sparse\nrange-constrained learning(SRCL)algorithm for medical image grading.Different\nfrom most of existing sparse learning algorithms, SRCL integrates the objective\nof finding a sparse representation and that of grading the image into one\nfunction. It aims to find a sparse representation of the testing image based on\natoms that are most similar in both the data or feature representation and the\nmedical grading scores. We apply the new proposed SRCL to CDR computation and\ncataract grading. Experimental results show that the proposed method is able to\nimprove the accuracy in cup-to-disc ratio computation and cataract grading.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 06:59:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Cheng", "Jun", ""]]}, {"id": "1807.10572", "submitter": "Hongyu Li", "authors": "Tianqi Han, Zhihui Fu, and Hongyu Li", "title": "Two-Layer Mixture Network Ensemble for Apparel Attributes Classification", "comments": "To be published in Proc. of AIFT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing apparel attributes has recently drawn great interest in the\ncomputer vision community. Methods based on various deep neural networks have\nbeen proposed for image classification, which could be applied to apparel\nattributes recognition. An interesting problem raised is how to ensemble these\nmethods to further improve the accuracy. In this paper, we propose a two-layer\nmixture framework for ensemble different networks. In the first layer of this\nframework, two types of ensemble learning methods, bagging and boosting, are\nseparately applied. Different from traditional methods, our bagging process\nmakes use of the whole training set, not random subsets, to train each model in\nthe ensemble, where several differentiated deep networks are used to promote\nmodel variance. To avoid the bias of small-scale samples, the second layer only\nadopts bagging to mix the results obtained with bagging and boosting in the\nfirst layer. Experimental results demonstrate that the proposed mixture\nframework outperforms any individual network model or either independent\nensemble method in apparel attributes classification.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 10:16:27 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Han", "Tianqi", ""], ["Fu", "Zhihui", ""], ["Li", "Hongyu", ""]]}, {"id": "1807.10573", "submitter": "Pan Wei", "authors": "Pan Wei, Lucas Cagle, Tasmia Reza, John Ball and James Gafford", "title": "LiDAR and Camera Detection Fusion in a Real Time Industrial Multi-Sensor\n  Collision Avoidance System", "comments": "34 pages", "journal-ref": "MDPI journal Electronics, 7(6), 84, May, 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision avoidance is a critical task in many applications, such as ADAS\n(advanced driver-assistance systems), industrial automation and robotics. In an\nindustrial automation setting, certain areas should be off limits to an\nautomated vehicle for protection of people and high-valued assets. These areas\ncan be quarantined by mapping (e.g., GPS) or via beacons that delineate a\nno-entry area. We propose a delineation method where the industrial vehicle\nutilizes a LiDAR {(Light Detection and Ranging)} and a single color camera to\ndetect passive beacons and model-predictive control to stop the vehicle from\nentering a restricted space. The beacons are standard orange traffic cones with\na highly reflective vertical pole attached. The LiDAR can readily detect these\nbeacons, but suffers from false positives due to other reflective surfaces such\nas worker safety vests. Herein, we put forth a method for reducing false\npositive detection from the LiDAR by projecting the beacons in the camera\nimagery via a deep learning method and validating the detection using a neural\nnetwork-learned projection from the camera to the LiDAR space. Experimental\ndata collected at Mississippi State University's Center for Advanced Vehicular\nSystems (CAVS) shows the effectiveness of the proposed system in keeping the\ntrue detection while mitigating false positives.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 16:55:09 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Wei", "Pan", ""], ["Cagle", "Lucas", ""], ["Reza", "Tasmia", ""], ["Ball", "John", ""], ["Gafford", "James", ""]]}, {"id": "1807.10574", "submitter": "Pan Wei", "authors": "John E. Ball, Pan Wei", "title": "Deep Learning Hyperspectral Image Classification Using Multiple\n  Class-based Denoising Autoencoders, Mixed Pixel Training Augmentation, and\n  Morphological Operations", "comments": null, "journal-ref": "IGARSS, June 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein, we present a system for hyperspectral image segmentation that\nutilizes multiple class--based denoising autoencoders which are efficiently\ntrained. Moreover, we present a novel hyperspectral data augmentation method\nfor labelled HSI data using linear mixtures of pixels from each class, which\nhelps the system with edge pixels which are almost always mixed pixels.\nFinally, we utilize a deep neural network and morphological hole-filling to\nprovide robust image classification. Results run on the Salinas dataset verify\nthe high performance of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 23:49:30 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Ball", "John E.", ""], ["Wei", "Pan", ""]]}, {"id": "1807.10575", "submitter": "Yingruo Fan", "authors": "Yingruo Fan, Jacqueline C.K. Lam and Victor O.K. Li", "title": "Multi-Region Ensemble Convolutional Neural Network for Facial Expression\n  Recognition", "comments": "10pages, 5 figures, Accepted by ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions play an important role in conveying the emotional states\nof human beings. Recently, deep learning approaches have been applied to image\nrecognition field due to the discriminative power of Convolutional Neural\nNetwork (CNN). In this paper, we first propose a novel Multi-Region Ensemble\nCNN (MRE-CNN) framework for facial expression recognition, which aims to\nenhance the learning power of CNN models by capturing both the global and the\nlocal features from multiple human face sub-regions. Second, the weighted\nprediction scores from each sub-network are aggregated to produce the final\nprediction of high accuracy. Third, we investigate the effects of different\nsub-regions of the whole face on facial expression recognition. Our proposed\nmethod is evaluated based on two well-known publicly available facial\nexpression databases: AFEW 7.0 and RAF-DB, and has been shown to achieve the\nstate-of-the-art recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:50:35 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Fan", "Yingruo", ""], ["Lam", "Jacqueline C. K.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1807.10576", "submitter": "Dario Zanca", "authors": "Dario Zanca and Marco Gori", "title": "Visual Attention driven by Convolutional Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The understanding of where humans look in a scene is a problem of great\ninterest in visual perception and computer vision. When eye-tracking devices\nare not a viable option, models of human attention can be used to predict\nfixations. In this paper we give two contribution. First, we show a model of\nvisual attention that is simply based on deep convolutional neural networks\ntrained for object classification tasks. A method for visualizing saliency maps\nis defined which is evaluated in a saliency prediction task. Second, we\nintegrate the information of these maps with a bottom-up differential model of\neye-movements to simulate visual attention scanpaths. Results on saliency\nprediction and scores of similarity with human scanpaths demonstrate the\neffectiveness of this model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:02:54 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Zanca", "Dario", ""], ["Gori", "Marco", ""]]}, {"id": "1807.10577", "submitter": "Jiang Su", "authors": "Jiang Su and Nicholas J. Fraser and Giulio Gambardella and Michaela\n  Blott and Gianluca Durelli and David B. Thomas and Philip Leong and Peter Y.\n  K. Cheung", "title": "Accuracy to Throughput Trade-offs for Reduced Precision Neural Networks\n  on Reconfigurable Logic", "comments": "Accepted by ARC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern CNN are typically based on floating point linear algebra based\nimplementations. Recently, reduced precision NN have been gaining popularity as\nthey require significantly less memory and computational resources compared to\nfloating point. This is particularly important in power constrained compute\nenvironments. However, in many cases a reduction in precision comes at a small\ncost to the accuracy of the resultant network. In this work, we investigate the\naccuracy-throughput trade-off for various parameter precision applied to\ndifferent types of NN models. We firstly propose a quantization training\nstrategy that allows reduced precision NN inference with a lower memory\nfootprint and competitive model accuracy. Then, we quantitatively formulate the\nrelationship between data representation and hardware efficiency. Our\nexperiments finally provide insightful observation. For example, one of our\ntests show 32-bit floating point is more hardware efficient than 1-bit\nparameters to achieve 99% MNIST accuracy. In general, 2-bit and 4-bit fixed\npoint parameters show better hardware trade-off on small-scale datasets like\nMNIST and CIFAR-10 while 4-bit provide the best trade-off in large-scale tasks\nlike AlexNet on ImageNet dataset within our tested problem domain.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:44:00 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Su", "Jiang", ""], ["Fraser", "Nicholas J.", ""], ["Gambardella", "Giulio", ""], ["Blott", "Michaela", ""], ["Durelli", "Gianluca", ""], ["Thomas", "David B.", ""], ["Leong", "Philip", ""], ["Cheung", "Peter Y. K.", ""]]}, {"id": "1807.10580", "submitter": "Zhijie Fang", "authors": "Zhijie Fang and Antonio M. L\\'opez", "title": "Is the Pedestrian going to Cross? Answering by 2D Pose Estimation", "comments": "This is a paper presented in IEEE Intelligent Vehicles Symposium\n  (IEEE IV 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our recent work suggests that, thanks to nowadays powerful CNNs, image-based\n2D pose estimation is a promising cue for determining pedestrian intentions\nsuch as crossing the road in the path of the ego-vehicle, stopping before\nentering the road, and starting to walk or bending towards the road. This\nstatement is based on the results obtained on non-naturalistic sequences\n(Daimler dataset), i.e. in sequences choreographed specifically for performing\nthe study. Fortunately, a new publicly available dataset (JAAD) has appeared\nrecently to allow developing methods for detecting pedestrian intentions in\nnaturalistic driving conditions; more specifically, for addressing the relevant\nquestion is the pedestrian going to cross? Accordingly, in this paper we use\nJAAD to assess the usefulness of 2D pose estimation for answering such a\nquestion. We combine CNN-based pedestrian detection, tracking and pose\nestimation to predict the crossing action from monocular images. Overall, the\nproposed pipeline provides new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 17:57:54 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Fang", "Zhijie", ""], ["L\u00f3pez", "Antonio M.", ""]]}, {"id": "1807.10581", "submitter": "Heung-Il Suk", "authors": "Bum-Chae Kim, Jun-Sik Choi, Heung-Il Suk", "title": "Multi-Scale Gradual Integration CNN for False Positive Reduction in\n  Pulmonary Nodule Detection", "comments": "11 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lung cancer is a global and dangerous disease, and its early detection is\ncrucial to reducing the risks of mortality. In this regard, it has been of\ngreat interest in developing a computer-aided system for pulmonary nodules\ndetection as early as possible on thoracic CT scans. In general, a nodule\ndetection system involves two steps: (i) candidate nodule detection at a high\nsensitivity, which captures many false positives and (ii) false positive\nreduction from candidates. However, due to the high variation of nodule\nmorphological characteristics and the possibility of mistaking them for\nneighboring organs, candidate nodule detection remains a challenge. In this\nstudy, we propose a novel Multi-scale Gradual Integration Convolutional Neural\nNetwork (MGI-CNN), designed with three main strategies: (1) to use multi-scale\ninputs with different levels of contextual information, (2) to use abstract\ninformation inherent in different input scales with gradual integration, and\n(3) to learn multi-stream feature integration in an end-to-end manner. To\nverify the efficacy of the proposed network, we conducted exhaustive\nexperiments on the LUNA16 challenge datasets by comparing the performance of\nthe proposed method with state-of-the-art methods in the literature. On two\ncandidate subsets of the LUNA16 dataset, i.e., V1 and V2, our method achieved\nan average CPM of 0.908 (V1) and 0.942 (V2), outperforming comparable methods\nby a large margin. Our MGI-CNN is implemented in Python using TensorFlow and\nthe source code is available from 'https://github.com/ku-milab/MGICNN.'\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 10:08:12 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kim", "Bum-Chae", ""], ["Choi", "Jun-Sik", ""], ["Suk", "Heung-Il", ""]]}, {"id": "1807.10583", "submitter": "Bishesh Khanal", "authors": "Bishesh Khanal, Alberto Gomez, Nicolas Toussaint, Steven McDonagh,\n  Veronika Zimmer, Emily Skelton, Jacqueline Matthew, Daniel Grzech, Robert\n  Wright, Chandni Gupta, Benjamin Hou, Daniel Rueckert, Julia A.Schnabel, and\n  Bernhard Kainz", "title": "EchoFusion: Tracking and Reconstruction of Objects in 4D Freehand\n  Ultrasound Imaging without External Trackers", "comments": "MICCAI Workshop on Perinatal, Preterm and Paediatric Image analysis\n  (PIPPI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) is the most widely used fetal imaging technique. However, US\nimages have limited capture range, and suffer from view dependent artefacts\nsuch as acoustic shadows. Compounding of overlapping 3D US acquisitions into a\nhigh-resolution volume can extend the field of view and remove image artefacts,\nwhich is useful for retrospective analysis including population based studies.\nHowever, such volume reconstructions require information about relative\ntransformations between probe positions from which the individual volumes were\nacquired. In prenatal US scans, the fetus can move independently from the\nmother, making external trackers such as electromagnetic or optical tracking\nunable to track the motion between probe position and the moving fetus. We\nprovide a novel methodology for image-based tracking and volume reconstruction\nby combining recent advances in deep learning and simultaneous localisation and\nmapping (SLAM). Tracking semantics are established through the use of a\nResidual 3D U-Net and the output is fed to the SLAM algorithm. As a proof of\nconcept, experiments are conducted on US volumes taken from a whole body fetal\nphantom, and from the heads of real fetuses. For the fetal head segmentation,\nwe also introduce a novel weak annotation approach to minimise the required\nmanual effort for ground truth annotation. We evaluate our method\nqualitatively, and quantitatively with respect to tissue discrimination\naccuracy and tracking robustness.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 12:07:50 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Khanal", "Bishesh", ""], ["Gomez", "Alberto", ""], ["Toussaint", "Nicolas", ""], ["McDonagh", "Steven", ""], ["Zimmer", "Veronika", ""], ["Skelton", "Emily", ""], ["Matthew", "Jacqueline", ""], ["Grzech", "Daniel", ""], ["Wright", "Robert", ""], ["Gupta", "Chandni", ""], ["Hou", "Benjamin", ""], ["Rueckert", "Daniel", ""], ["Schnabel", "Julia A.", ""], ["Kainz", "Bernhard", ""]]}, {"id": "1807.10584", "submitter": "Kristoffer Wickstr{\\o}m", "authors": "Kristoffer Wickstr{\\o}m, Michael Kampffmeyer and Robert Jenssen", "title": "Uncertainty and Interpretability in Convolutional Neural Networks for\n  Semantic Segmentation of Colorectal Polyps", "comments": "To appear in IEEE MLSP 2018", "journal-ref": null, "doi": "10.1016/j.media.2019.101619", "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) are propelling advances in a range of\ndifferent computer vision tasks such as object detection and object\nsegmentation. Their success has motivated research in applications of such\nmodels for medical image analysis. If CNN-based models are to be helpful in a\nmedical context, they need to be precise, interpretable, and uncertainty in\npredictions must be well understood. In this paper, we develop and evaluate\nrecent advances in uncertainty estimation and model interpretability in the\ncontext of semantic segmentation of polyps from colonoscopy images. We evaluate\nand enhance several architectures of Fully Convolutional Networks (FCNs) for\nsemantic segmentation of colorectal polyps and provide a comparison between\nthese models. Our highest performing model achieves a 76.06\\% mean IOU accuracy\non the EndoScene dataset, a considerable improvement over the previous\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:01:01 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wickstr\u00f8m", "Kristoffer", ""], ["Kampffmeyer", "Michael", ""], ["Jenssen", "Robert", ""]]}, {"id": "1807.10585", "submitter": "Xavier Suau Cuadros", "authors": "Xavier Suau, Luca Zappella and Nicholas Apostoloff", "title": "Filter Distillation for Network Compression", "comments": "10 pages, 3 figures, Deep neural network compression, spectral\n  analysis, machine learning", "journal-ref": "WACV 2020", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Principal Filter Analysis (PFA), an easy to use\nand effective method for neural network compression. PFA exploits the\ncorrelation between filter responses within network layers to recommend a\nsmaller network that maintain as much as possible the accuracy of the full\nmodel. We propose two algorithms: the first allows users to target compression\nto specific network property, such as number of trainable variable (footprint),\nand produces a compressed model that satisfies the requested property while\npreserving the maximum amount of spectral energy in the responses of each\nlayer, while the second is a parameter-free heuristic that selects the\ncompression used at each layer by trying to mimic an ideal set of uncorrelated\nresponses. Since PFA compresses networks based on the correlation of their\nresponses we show in our experiments that it gains the additional flexibility\nof adapting each architecture to a specific domain while compressing. PFA is\nevaluated against several architectures and datasets, and shows considerable\ncompression rates without compromising accuracy, e.g., for VGG-16 on CIFAR-10,\nCIFAR-100 and ImageNet, PFA achieves a compression rate of 8x, 3x, and 1.4x\nwith an accuracy gain of 0.4%, 1.4% points, and 2.4% respectively. Our tests\nshow that PFA is competitive with state-of-the-art approaches while removing\nadoption barriers thanks to its practical implementation, intuitive philosophy\nand ease of use.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 23:36:11 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 16:37:14 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 21:33:41 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 13:43:48 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Suau", "Xavier", ""], ["Zappella", "Luca", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "1807.10586", "submitter": "KitIan Kou", "authors": "Wenshan Bi Dong Cheng Wankai Liu and Kit Ian Kou", "title": "A Robust Color Edge Detection Algorithm Based on Quaternion Hardy Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a robust filter called quaternion Hardy filter (QHF) for\ncolor image edge detection. The QHF can be capable of color edge feature\nenhancement and noise resistance. It is flexible to use QHF by selecting\nsuitable parameters to handle different levels of noise. In particular, the\nquaternion analytic signal, which is an effective tool in color image\nprocessing, can also be produced by quaternion Hardy filtering with specific\nparameters. Based on the QHF and the improved Di Zenzo gradient operator, a\nnovel color edge detection algorithm is proposed. Importantly, it can be\nefficiently implemented by using the fast discrete quaternion Fourier transform\ntechnique. From the experimental results, we conclude that the minimum PSNR\nimprovement rate is 2.3% and minimum SSIM improvement rate is 30.2% on the\nDataset 3. The experiments demonstrate that the proposed algorithm outperforms\nseveral widely used algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:02:35 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 09:44:10 GMT"}, {"version": "v3", "created": "Sat, 2 Jan 2021 12:45:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Wenshan Bi Dong Cheng Wankai", ""], ["Kou", "Kit Ian", ""]]}, {"id": "1807.10587", "submitter": "Gabriel Kreiman", "authors": "Mengmi Zhang, Jiashi Feng, Keng Teck Ma, Joo Hwee Lim, Qi Zhao,\n  Gabriel Kreiman", "title": "Finding any Waldo: zero-shot invariant and efficient visual search", "comments": "Number of figures: 6 Number of supplementary figures: 14", "journal-ref": null, "doi": "10.1038/s41467-018-06217-x", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for a target object in a cluttered scene constitutes a fundamental\nchallenge in daily vision. Visual search must be selective enough to\ndiscriminate the target from distractors, invariant to changes in the\nappearance of the target, efficient to avoid exhaustive exploration of the\nimage, and must generalize to locate novel target objects with zero-shot\ntraining. Previous work has focused on searching for perfect matches of a\ntarget after extensive category-specific training. Here we show for the first\ntime that humans can efficiently and invariantly search for natural objects in\ncomplex scenes. To gain insight into the mechanisms that guide visual search,\nwe propose a biologically inspired computational model that can locate targets\nwithout exhaustive sampling and generalize to novel objects. The model provides\nan approximation to the mechanisms integrating bottom-up and top-down signals\nduring search in natural scenes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 01:17:34 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhang", "Mengmi", ""], ["Feng", "Jiashi", ""], ["Ma", "Keng Teck", ""], ["Lim", "Joo Hwee", ""], ["Zhao", "Qi", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1807.10588", "submitter": "Mikael Agn", "authors": "Mikael Agn (1), Per Munck af Rosensch\\\"old (2), Oula Puonti (3),\n  Michael J. Lundemann (4), Laura Mancini (5 and 6), Anastasia Papadaki (5 and\n  6), Steffi Thust (5 and 6), John Ashburner (7), Ian Law (8), Koen Van Leemput\n  (1 and 9) ((1) Department of Applied Mathematics and Computer Science,\n  Technical University of Denmark, Denmark, (2) Radiation Physics, Department\n  of Hematology, Oncology and Radiation Physics, Sk{\\aa}ne University Hospital,\n  Lund, Sweden, (3) Danish Research Centre for Magnetic Resonance, Copenhagen\n  University Hospital Hvidovre, Denmark, (4) Department of Oncology, Copenhagen\n  University Hospital Rigshospitalet, Denmark, (5) Neuroradiological Academic\n  Unit, Department of Brain Repair and Rehabilitation, UCL Institute of\n  Neurology, University College London, UK, (6) Lysholm Department of\n  Neuroradiology, National Hospital for Neurology and Neurosurgery, UCLH NHS\n  Foundation Trust, UK, (7) Wellcome Centre for Human Neuroimaging, UCL\n  Institute of Neurology, University College London, UK, (8) Department of\n  Clinical Physiology, Nuclear Medicine and PET, Copenhagen University Hospital\n  Rigshospitalet, Denmark, (9) Athinoula A. Martinos Center for Biomedical\n  Imaging, Massachusetts General Hospital, Harvard Medical School, USA)", "title": "A Modality-Adaptive Method for Segmenting Brain Tumors and\n  Organs-at-Risk in Radiation Therapy Planning", "comments": "corrected one reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for simultaneously segmenting brain tumors\nand an extensive set of organs-at-risk for radiation therapy planning of\nglioblastomas. The method combines a contrast-adaptive generative model for\nwhole-brain segmentation with a new spatial regularization model of tumor shape\nusing convolutional restricted Boltzmann machines. We demonstrate\nexperimentally that the method is able to adapt to image acquisitions that\ndiffer substantially from any available training data, ensuring its\napplicability across treatment sites; that its tumor segmentation accuracy is\ncomparable to that of the current state of the art; and that it captures most\norgans-at-risk sufficiently well for radiation therapy planning purposes. The\nproposed method may be a valuable step towards automating the delineation of\nbrain tumors and organs-at-risk in glioblastoma patients undergoing radiation\ntherapy.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 20:16:00 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 19:41:38 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Agn", "Mikael", "", "5 and 6"], ["Rosensch\u00f6ld", "Per Munck af", "", "5 and 6"], ["Puonti", "Oula", "", "5 and 6"], ["Lundemann", "Michael J.", "", "5 and 6"], ["Mancini", "Laura", "", "5 and 6"], ["Papadaki", "Anastasia", "", "5 and\n  6"], ["Thust", "Steffi", "", "5 and 6"], ["Ashburner", "John", "", "1 and 9"], ["Law", "Ian", "", "1 and 9"], ["Van Leemput", "Koen", "", "1 and 9"]]}, {"id": "1807.10589", "submitter": "Santiago Cadena", "authors": "Santiago A. Cadena, Marissa A. Weis, Leon A. Gatys, Matthias Bethge,\n  Alexander S. Ecker", "title": "Diverse feature visualizations reveal invariances in early layers of\n  deep neural networks", "comments": "Accepted for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing features in deep neural networks (DNNs) can help understanding\ntheir computations. Many previous studies aimed to visualize the selectivity of\nindividual units by finding meaningful images that maximize their activation.\nHowever, comparably little attention has been paid to visualizing to what image\ntransformations units in DNNs are invariant. Here we propose a method to\ndiscover invariances in the responses of hidden layer units of deep neural\nnetworks. Our approach is based on simultaneously searching for a batch of\nimages that strongly activate a unit while at the same time being as distinct\nfrom each other as possible. We find that even early convolutional layers in\nVGG-19 exhibit various forms of response invariance: near-perfect phase\ninvariance in some units and invariance to local diffeomorphic transformations\nin others. At the same time, we uncover representational differences with\nResNet-50 in its corresponding layers. We conclude that invariance\ntransformations are a major computational component learned by DNNs and we\nprovide a systematic method to study them.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 13:15:08 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Cadena", "Santiago A.", ""], ["Weis", "Marissa A.", ""], ["Gatys", "Leon A.", ""], ["Bethge", "Matthias", ""], ["Ecker", "Alexander S.", ""]]}, {"id": "1807.10590", "submitter": "Wen Heng", "authors": "Wen Heng, Shuchang Zhou, Tingting Jiang", "title": "Harmonic Adversarial Attack Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks find perturbations that can fool models into\nmisclassifying images. Previous works had successes in generating\nnoisy/edge-rich adversarial perturbations, at the cost of degradation of image\nquality. Such perturbations, even when they are small in scale, are usually\neasily spottable by human vision. In contrast, we propose Harmonic Adversar-\nial Attack Methods (HAAM), that generates edge-free perturbations by using\nharmonic functions. The property of edge-free guarantees that the generated\nadversarial images can still preserve visual quality, even when perturbations\nare of large magnitudes. Experiments also show that adversaries generated by\nHAAM often have higher rates of success when transferring between models. In\naddition, we find harmonic perturbations can simulate natural phenomena like\nnatural lighting and shadows. It would then be possible to help find corner\ncases for given models, as a first step to improving them.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 10:09:37 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 03:58:43 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Heng", "Wen", ""], ["Zhou", "Shuchang", ""], ["Jiang", "Tingting", ""]]}, {"id": "1807.10591", "submitter": "Sergey Rodionov", "authors": "Alexey Potapov, Sergey Rodionov, Hugo Latapie, Enzo Fenoglio", "title": "Metric Embedding Autoencoders for Unsupervised Cross-Dataset Transfer\n  Learning", "comments": "ICANN 2018 (The 27th International Conference on Artificial Neural\n  Networks) proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-dataset transfer learning is an important problem in person\nre-identification (Re-ID). Unfortunately, not too many deep transfer Re-ID\nmodels exist for realistic settings of practical Re-ID systems. We propose a\npurely deep transfer Re-ID model consisting of a deep convolutional neural\nnetwork and an autoencoder. The latent code is divided into metric embedding\nand nuisance variables. We then utilize an unsupervised training method that\ndoes not rely on co-training with non-deep models. Our experiments show\nimprovements over both the baseline and competitors' transfer learning models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 09:59:34 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Potapov", "Alexey", ""], ["Rodionov", "Sergey", ""], ["Latapie", "Hugo", ""], ["Fenoglio", "Enzo", ""]]}, {"id": "1807.10596", "submitter": "Joowan Kim", "authors": "Joowan Kim, Younggun Cho and Ayoung Kim", "title": "Generic Camera Attribute Control using Bayesian Optimization", "comments": "8 pages, 11 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cameras are the most widely exploited sensor in both robotics and computer\nvision communities. Despite their popularity, two dominant attributes (i.e.,\ngain and exposure time) have been determined empirically and images are\ncaptured in very passive manner. In this paper, we present an active and\ngeneric camera attribute control scheme using Bayesian optimization. We extend\nfrom our previous work [1] in two aspects. First, we propose a method that\njointly controls camera gain and exposure time. Secondly, to speed up the\nBayesian optimization process, we introduce image synthesis using the camera\nresponse function (CRF). These synthesized images allowed us to diminish the\nimage acquisition time during the Bayesian optimization phase, substantially\nimproving overall control performance. The proposed method is validated both in\nan indoor and an outdoor environment where light condition rapidly changes.\nSupplementary material is available at https://youtu.be/XTYR_Mih3OU .\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 09:09:27 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 09:17:53 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 10:43:34 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Kim", "Joowan", ""], ["Cho", "Younggun", ""], ["Kim", "Ayoung", ""]]}, {"id": "1807.10597", "submitter": "Benjamin Au", "authors": "Benjamin Au, Uri Shaham, Sanket Dhruva, Georgios Bouras, Ecaterina\n  Cristea, Alexandra Lansky MD, Andreas Coppi, Fred Warner, Shu-Xia Li, Harlan\n  Krumholz", "title": "Automated Characterization of Stenosis in Invasive Coronary Angiography\n  Images with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of a coronary stenosis and its severity in current clinical\nworkflow is typically accomplished manually via physician visual assessment\n(PVA) during invasive coronary angiography. While PVA has shown large\ninter-rater variability, the more reliable and accurate alternative of\nQuantitative Coronary Angiography (QCA) is challenging to perform in real-time\ndue to the busy workflow in cardiac catheterization laboratories. We propose a\ndeep learning approach based on Convolutional Neural Networks (CNN) that\nautomatically characterizes and analyzes coronary stenoses in real-time by\nautomating clinical tasks performed during QCA. Our deep learning methods for\nlocalization, segmentation and classification of stenosis in still-frame\ninvasive coronary angiography (ICA) images of the right coronary artery (RCA)\nachieve performance of 72.7% localization accuracy, 0.704 dice coefficient and\n0.825 C-statistic in each respective task. Integrated in an end-to-end\napproach, our model's performance shows statistically significant improvement\nin false discovery rate over the current standard in real-time clinical\nstenosis assessment, PVA. To the best of the authors' knowledge, this is the\nfirst time an automated machine learning system has been developed that can\nimplement tasks performed in QCA, and the first time an automated machine\nlearning system has demonstrated significant improvement over the current\nclinical standard for rapid RCA stenosis analysis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 17:57:06 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Au", "Benjamin", ""], ["Shaham", "Uri", ""], ["Dhruva", "Sanket", ""], ["Bouras", "Georgios", ""], ["Cristea", "Ecaterina", ""], ["MD", "Alexandra Lansky", ""], ["Coppi", "Andreas", ""], ["Warner", "Fred", ""], ["Li", "Shu-Xia", ""], ["Krumholz", "Harlan", ""]]}, {"id": "1807.10598", "submitter": "Gil Shomron", "authors": "Gil Shomron and Uri Weiser", "title": "Spatial Correlation and Value Prediction in Convolutional Neural\n  Networks", "comments": "This paper has been accepted to IEEE Computer Architecture Letters\n  (https://ieeexplore.ieee.org/document/8594568)", "journal-ref": null, "doi": "10.1109/LCA.2018.2890236", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks (CNNs) are a widely used form of deep neural\nnetworks, introducing state-of-the-art results for different problems such as\nimage classification, computer vision tasks, and speech recognition. However,\nCNNs are compute intensive, requiring billions of multiply-accumulate (MAC)\noperations per input. To reduce the number of MACs in CNNs, we propose a value\nprediction method that exploits the spatial correlation of zero-valued\nactivations within the CNN output feature maps, thereby saving convolution\noperations. Our method reduces the number of MAC operations by 30.4%, averaged\non three modern CNNs for ImageNet, with top-1 accuracy degradation of 1.7%, and\ntop-5 accuracy degradation of 1.1%.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 14:56:30 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 20:10:32 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Shomron", "Gil", ""], ["Weiser", "Uri", ""]]}, {"id": "1807.10600", "submitter": "Yue Zhang", "authors": "Yue Zhang, Wanli Chen, Yifan Chen and Xiaoying Tang", "title": "A post-processing method to improve the white matter hyperintensity\n  segmentation accuracy for randomly-initialized U-net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  White matter hyperintensity (WMH) is commonly found in elder individuals and\nappears to be associated with brain diseases. U-net is a convolutional network\nthat has been widely used for biomedical image segmentation. Recently, U-net\nhas been successfully applied to WMH segmentation. Random initialization is\nusally used to initialize the model weights in the U-net. However, the model\nmay coverage to different local optima with different randomly initialized\nweights. We find a combination of thresholding and averaging the outputs of\nU-nets with different random initializations can largely improve the WMH\nsegmentation accuracy. Based on this observation, we propose a post-processing\ntechnique concerning the way how averaging and thresholding are conducted.\nSpecifically, we first transfer the score maps from three U-nets to binary\nmasks via thresholding and then average those binary masks to obtain the final\nWMH segmentation. Both quantitative analysis (via the Dice similarity\ncoefficient) and qualitative analysis (via visual examinations) reveal the\nsuperior performance of the proposed method. This post-processing technique is\nindependent of the model used. As such, it can also be applied to situations\nwhere other deep learning models are employed, especially when random\ninitialization is adopted and pre-training is unavailable.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 12:45:45 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Zhang", "Yue", ""], ["Chen", "Wanli", ""], ["Chen", "Yifan", ""], ["Tang", "Xiaoying", ""]]}, {"id": "1807.10602", "submitter": "Ramanarayan Mohanty", "authors": "Ramanarayan Mohanty, S L Happy, Nilesh Suthar, and Aurobinda Routray", "title": "A Trace Lasso Regularized L1-norm Graph Cut for Highly Correlated Noisy\n  Hyperspectral Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes an adaptive trace lasso regularized L1-norm based graph\ncut method for dimensionality reduction of Hyperspectral images, called as\n`Trace Lasso-L1 Graph Cut' (TL-L1GC). The underlying idea of this method is to\ngenerate the optimal projection matrix by considering both the sparsity as well\nas the correlation of the data samples. The conventional L2-norm used in the\nobjective function is sensitive to noise and outliers. Therefore, in this work\nL1-norm is utilized as a robust alternative to L2-norm. Besides, for further\nimprovement of the results, we use a penalty function of trace lasso with the\nL1GC method. It adaptively balances the L2-norm and L1-norm simultaneously by\nconsidering the data correlation along with the sparsity. We obtain the optimal\nprojection matrix by maximizing the ratio of between-class dispersion to\nwithin-class dispersion using L1-norm with trace lasso as the penalty.\nFurthermore, an iterative procedure for this TL-L1GC method is proposed to\nsolve the optimization function. The effectiveness of this proposed method is\nevaluated on two benchmark HSI datasets.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 07:44:56 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Mohanty", "Ramanarayan", ""], ["Happy", "S L", ""], ["Suthar", "Nilesh", ""], ["Routray", "Aurobinda", ""]]}, {"id": "1807.10603", "submitter": "Youngjoo Kim", "authors": "Youngjoo Kim, Peng Wang, Yifei Zhu, and Lyudmila Mihaylova", "title": "A Capsule Network for Traffic Speed Prediction in Complex Road Networks", "comments": "To be presented in 2018 Sensor Data Fusion: Trends, Solutions,\n  Applications (SDF), 10 Oct 2018, in Bonn, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a deep learning approach for traffic flow prediction in\ncomplex road networks. Traffic flow data from induction loop sensors are\nessentially a time series, which is also spatially related to traffic in\ndifferent road segments. The spatio-temporal traffic data can be converted into\nan image where the traffic data are expressed in a 3D space with respect to\nspace and time axes. Although convolutional neural networks (CNNs) have been\nshowing surprising performance in understanding images, they have a major\ndrawback. In the max pooling operation, CNNs are losing important information\nby locally taking the highest activation values. The inter-relationship in\ntraffic data measured by sparsely located sensors in different time intervals\nshould not be neglected in order to obtain accurate predictions. Thus, we\npropose a neural network with capsules that replaces max pooling by dynamic\nrouting. This is the first approach that employs the capsule network on a time\nseries forecasting problem, to our best knowledge. Moreover, an experiment on\nreal traffic speed data measured in the Santander city of Spain demonstrates\nthe proposed method outperforms the state-of-the-art method based on a CNN by\n13.1% in terms of root mean squared error.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 10:40:22 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 15:32:01 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Kim", "Youngjoo", ""], ["Wang", "Peng", ""], ["Zhu", "Yifei", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1807.10609", "submitter": "Ariel Ruiz-Garcia", "authors": "Yahaya Isah Shehu, Ariel Ruiz-Garcia, Vasile Palade, Anne James", "title": "Sokoto Coventry Fingerprint Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the Sokoto Coventry Fingerprint Dataset (SOCOFing), a\nbiometric fingerprint database designed for academic research purposes.\nSOCOFing is made up of 6,000 fingerprint images from 600 African subjects.\nSOCOFing contains unique attributes such as labels for gender, hand and finger\nname as well as synthetically altered versions with three different levels of\nalteration for obliteration, central rotation, and z-cut. The dataset is freely\navailable for noncommercial research purposes at:\nhttps://www.kaggle.com/ruizgara/socofing\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 13:14:11 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Shehu", "Yahaya Isah", ""], ["Ruiz-Garcia", "Ariel", ""], ["Palade", "Vasile", ""], ["James", "Anne", ""]]}, {"id": "1807.10610", "submitter": "Weiwen Wu", "authors": "Weiwen Wu, Fenglin Liu, Yanbo Zhang, Qian Wang and Hengyong Yu", "title": "Non-local Low-rank Cube-based Tensor Factorization for Spectral CT\n  Reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2018.2878226", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral computed tomography (CT) reconstructs material-dependent attenuation\nimages with the projections of multiple narrow energy windows, it is meaningful\nfor material identification and decomposition. Unfortunately, the multi-energy\nprojection dataset always contains strong complicated noise and result in the\nprojections has a lower signal-noise-ratio (SNR). Very recently, the\nspatial-spectral cube matching frame (SSCMF) was proposed to explore the\nnon-local spatial-spectrum similarities for spectral CT. The method constructs\nsuch a group by clustering up a series of non-local spatial-spectrum cubes. The\nsmall size of spatial patch for such a group make SSCMF fails to encode the\nsparsity and low-rank properties. In addition, the hard-thresholding and\ncollaboration filtering operation in the SSCMF are also rough to recover the\nimage features and spatial edges. While for all steps are operated on 4-D\ngroup, we may not afford such huge computational and memory load in practical.\nTo avoid the above limitation and further improve image quality, we first\nformulate a non-local cube-based tensor instead of the group to encode the\nsparsity and low-rank properties. Then, as a new regularizer,\nKronecker-Basis-Representation (KBR) tensor factorization is employed into a\nbasic spectral CT reconstruction model to enhance the ability of extracting\nimage features and protecting spatial edges, generating the non-local low-rank\ncube-based tensor factorization (NLCTF) method. Finally, the split-Bregman\nstrategy is adopted to solve the NLCTF model. Both numerical simulations and\nrealistic preclinical mouse studies are performed to validate and assess the\nNLCTF algorithm. The results show that the NLCTF method outperforms the other\ncompetitors.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:15:03 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 02:31:47 GMT"}, {"version": "v3", "created": "Sun, 28 Oct 2018 05:31:46 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Wu", "Weiwen", ""], ["Liu", "Fenglin", ""], ["Zhang", "Yanbo", ""], ["Wang", "Qian", ""], ["Yu", "Hengyong", ""]]}, {"id": "1807.10614", "submitter": "Huibing Wang", "authors": "Huibing Wang and Lin Feng and Adong Kong and Bo Jin", "title": "Multi-view Reconstructive Preserving Embedding for Dimension Reduction", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of feature extraction technique, one sample always can\nbe represented by multiple features which locate in high-dimensional space.\nMultiple features can re ect various perspectives of one same sample, so there\nmust be compatible and complementary information among the multiple views.\nTherefore, it's natural to integrate multiple features together to obtain\nbetter performance. However, most multi-view dimension reduction methods cannot\nhandle multiple features from nonlinear space with high dimensions. To address\nthis problem, we propose a novel multi-view dimension reduction method named\nMulti-view Reconstructive Preserving Embedding (MRPE) in this paper. MRPE\nreconstructs each sample by utilizing its k nearest neighbors. The similarities\nbetween each sample and its neighbors are primely mapped into lower-dimensional\nspace in order to preserve the underlying neighborhood structure of the\noriginal manifold. MRPE fully exploits correlations between each sample and its\nneighbors from multiple views by linear reconstruction. Furthermore, MRPE\nconstructs an optimization problem and derives an iterative procedure to obtain\nthe low-dimensional embedding. Various evaluations based on the applications of\ndocument classification, face recognition and image retrieval demonstrate the\neffectiveness of our proposed approach on multi-view dimension reduction.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 06:42:58 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Wang", "Huibing", ""], ["Feng", "Lin", ""], ["Kong", "Adong", ""], ["Jin", "Bo", ""]]}, {"id": "1807.10638", "submitter": "Xavier-Lewis Palmer", "authors": "Darlington Ahiale Akogo and Xavier-Lewis Palmer", "title": "End-to-End Learning via a Convolutional Neural Network for Cancer Cell\n  Line Classification", "comments": "arXiv admin note: text overlap with arXiv:1805.08702", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Computer Vision for automated analysis of cells and tissues usually include\nextracting features from images before analyzing such features via various\nMachine Learning and Machine Vision algorithms. We developed a Convolutional\nNeural Network model that classifies MDA-MB-468 and MCF7 breast cancer cells\nvia brightfield microscopy images without the need of any prior feature\nextraction. Our 6-layer Convolutional Neural Network is directly trained,\nvalidated and tested on 1,241 images of MDA-MB-468 and MCF7 breast cancer cell\nline in an end-to-end fashion, allowing a system to distinguish between\ndifferent cancer cell types. The model takes in as input imaged breast cancer\ncell line and then outputs the cell line type (MDA-MB-468 or MCF7) as predicted\nprobabilities between the two classes. Our model scored a 99% Accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:45:22 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Akogo", "Darlington Ahiale", ""], ["Palmer", "Xavier-Lewis", ""]]}, {"id": "1807.10641", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Fuchun Sun, Wenchang Zhang, Jianhua Chen and Chunfang Liu", "title": "Multimodal Classification with Deep Convolutional-Recurrent Neural\n  Networks for Electroencephalography", "comments": "10 pages, 6 figures", "journal-ref": "Neural Information Processing. 2017:767-776", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) has become the most significant input signal for\nbrain computer interface (BCI) based systems. However, it is very difficult to\nobtain satisfactory classification accuracy due to traditional methods can not\nfully exploit multimodal information. Herein, we propose a novel approach to\nmodeling cognitive events from EEG data by reducing it to a video\nclassification problem, which is designed to preserve the multimodal\ninformation of EEG. In addition, optical flow is introduced to represent the\nvariant information of EEG. We train a deep neural network (DNN) with\nconvolutional neural network (CNN) and recurrent neural network (RNN) for the\nEEG classification task by using EEG video and optical flow. The experiments\ndemonstrate that our approach has many advantages, such as more robustness and\nmore accuracy in EEG classification tasks. According to our approach, we\ndesigned a mixed BCI-based rehabilitation support system to help stroke\npatients perform some basic operations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 03:33:43 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Tan", "Chuanqi", ""], ["Sun", "Fuchun", ""], ["Zhang", "Wenchang", ""], ["Chen", "Jianhua", ""], ["Liu", "Chunfang", ""]]}, {"id": "1807.10653", "submitter": "Esther Puyol-Anton Miss", "authors": "Esther Puyol-Anton, Bram Ruijsink, Helene Langet, Mathieu De Craene,\n  Paolo Piro, Julia A. Schnabel, and Andrew P. King", "title": "Learning associations between clinical information and motion-based\n  descriptors using a large scale MR-derived cardiac motion atlas", "comments": "2018 International Workshop on Statistical Atlases and Computational\n  Modeling of the Heart", "journal-ref": null, "doi": "10.1007/978-3-030-12029-0_11", "report-no": null, "categories": "cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large scale databases containing imaging and non-imaging\ndata, such as the UK Biobank, represents an opportunity to improve our\nunderstanding of healthy and diseased bodily function. Cardiac motion atlases\nprovide a space of reference in which the motion fields of a cohort of subjects\ncan be directly compared. In this work, a cardiac motion atlas is built from\ncine MR data from the UK Biobank (~ 6000 subjects). Two automated quality\ncontrol strategies are proposed to reject subjects with insufficient image\nquality. Based on the atlas, three dimensionality reduction algorithms are\nevaluated to learn data-driven cardiac motion descriptors, and statistical\nmethods used to study the association between these descriptors and non-imaging\ndata. Results show a positive correlation between the atlas motion descriptors\nand body fat percentage, basal metabolic rate, hypertension, smoking status and\nalcohol intake frequency. The proposed method outperforms the ability to\nidentify changes in cardiac function due to these known cardiovascular risk\nfactors compared to ejection fraction, the most commonly used descriptor of\ncardiac function. In conclusion, this work represents a framework for further\ninvestigation of the factors influencing cardiac health.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 14:33:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Puyol-Anton", "Esther", ""], ["Ruijsink", "Bram", ""], ["Langet", "Helene", ""], ["De Craene", "Mathieu", ""], ["Piro", "Paolo", ""], ["Schnabel", "Julia A.", ""], ["King", "Andrew P.", ""]]}, {"id": "1807.10657", "submitter": "Takao Yamanaka", "authors": "Taiki Oyama and Takao Yamanaka", "title": "Influence of Image Classification Accuracy on Saliency Map Estimation", "comments": "CAAI Transactions on Intelligence Technology, accepted in 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Saliency map estimation in computer vision aims to estimate the locations\nwhere people gaze in images. Since people tend to look at objects in images,\nthe parameters of the model pretrained on ImageNet for image classification are\nuseful for the saliency map estimation. However, there is no research on the\nrelationship between the image classification accuracy and the performance of\nthe saliency map estimation. In this paper, it is shown that there is a strong\ncorrelation between image classification accuracy and saliency map estimation\naccuracy. We also investigated the effective architecture based on multi scale\nimages and the upsampling layers to refine the saliency-map resolution. Our\nmodel achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003\ndatasets. In the MIT Saliency Benchmark, our model achieved the best\nperformance in some metrics and competitive results in the other metrics.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 14:36:58 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Oyama", "Taiki", ""], ["Yamanaka", "Takao", ""]]}, {"id": "1807.10706", "submitter": "Humam Alwassel", "authors": "Humam Alwassel, Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem", "title": "Diagnosing Error in Temporal Action Detectors", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent progress in video understanding and the continuous rate of\nimprovement in temporal action localization throughout the years, it is still\nunclear how far (or close?) we are to solving the problem. To this end, we\nintroduce a new diagnostic tool to analyze the performance of temporal action\ndetectors in videos and compare different methods beyond a single scalar\nmetric. We exemplify the use of our tool by analyzing the performance of the\ntop rewarded entries in the latest ActivityNet action localization challenge.\nOur analysis shows that the most impactful areas to work on are: strategies to\nbetter handle temporal context around the instances, improving the robustness\nw.r.t. the instance absolute and relative size, and strategies to reduce the\nlocalization errors. Moreover, our experimental analysis finds the lack of\nagreement among annotator is not a major roadblock to attain progress in the\nfield. Our diagnostic tool is publicly available to keep fueling the minds of\nother researchers with additional insights about their algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:13:22 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Alwassel", "Humam", ""], ["Heilbron", "Fabian Caba", ""], ["Escorcia", "Victor", ""], ["Ghanem", "Bernard", ""]]}, {"id": "1807.10711", "submitter": "Manu Goyal", "authors": "Manu Goyal, Saeed Hassanpour and Moi Hoon Yap", "title": "Region of Interest Detection in Dermoscopic Images for Natural\n  Data-augmentation", "comments": "Natural Augmentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of medical imaging research, there is a great interest\nin the automated detection of skin lesions with computer algorithms. The\nstate-of-the-art datasets for skin lesions are often accompanied with very\nlimited amount of ground truth labeling as it is laborious and expensive. The\nRegion Of Interest (ROI) detection is vital to locate the lesion accurately and\nmust be robust to subtle features of different skin lesion types. In this work,\nwe propose the use of two object localization meta-architectures for end-to-end\nROI skin lesion detection in dermoscopic images. We trained the\nFaster-RCNN-InceptionV2 and SSD-InceptionV2 on the ISBI-2017 training dataset\nand evaluated their performance on the ISBI-2017 testing set, PH2 and HAM10000\ndatasets. Since there was no earlier work in ROI detection for skin lesion with\nCNNs, we compared the performance of skin localization methods with the\nstate-of-the-art segmentation method. The localization methods proved superior\nto the segmentation method in ROI detection on skin lesion datasets. In\naddition, based on the detected ROI, an automated natural data-augmentation\nmethod is proposed and used as pre-processing in the lesion diagnosis and\nsegmentation task. To further demonstrate the potential of our work, we\ndeveloped a real-time smart-phone application for automated skin lesions\ndetection.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:29:11 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 18:39:42 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Goyal", "Manu", ""], ["Hassanpour", "Saeed", ""], ["Yap", "Moi Hoon", ""]]}, {"id": "1807.10712", "submitter": "Samuel Albanie", "authors": "David Novotny, Samuel Albanie, Diane Larlus, Andrea Vedaldi", "title": "Semi-convolutional Operators for Instance Segmentation", "comments": "Accepted as a conference paper at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection and instance segmentation are dominated by region-based\nmethods such as Mask RCNN. However, there is a growing interest in reducing\nthese problems to pixel labeling tasks, as the latter could be more efficient,\ncould be integrated seamlessly in image-to-image network architectures as used\nin many other tasks, and could be more accurate for objects that are not well\napproximated by bounding boxes. In this paper we show theoretically and\nempirically that constructing dense pixel embeddings that can separate object\ninstances cannot be easily achieved using convolutional operators. At the same\ntime, we show that simple modifications, which we call semi-convolutional, have\na much better chance of succeeding at this task. We use the latter to show a\nconnection to Hough voting as well as to a variant of the bilateral kernel that\nis spatially steered by a convolutional network. We demonstrate that these\noperators can also be used to improve approaches such as Mask RCNN,\ndemonstrating better segmentation of complex biological shapes and PASCAL VOC\ncategories than achievable by Mask RCNN alone.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:29:53 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Novotny", "David", ""], ["Albanie", "Samuel", ""], ["Larlus", "Diane", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1807.10726", "submitter": "Bowen Zhang", "authors": "Bowen Zhang, Xifan Zhang, Fan Cheng, Deli Zhao", "title": "Few Shot Learning with Simplex", "comments": "There is still room for model improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made remarkable achievement in many fields. However,\nlearning the parameters of neural networks usually demands a large amount of\nlabeled data. The algorithms of deep learning, therefore, encounter\ndifficulties when applied to supervised learning where only little data are\navailable. This specific task is called few-shot learning. To address it, we\npropose a novel algorithm for few-shot learning using discrete geometry, in the\nsense that the samples in a class are modeled as a reduced simplex. The volume\nof the simplex is used for the measurement of class scatter. During testing,\ncombined with the test sample and the points in the class, a new simplex is\nformed. Then the similarity between the test sample and the class can be\nquantized with the ratio of volumes of the new simplex to the original class\nsimplex. Moreover, we present an approach to constructing simplices using local\nregions of feature maps yielded by convolutional neural networks. Experiments\non Omniglot and miniImageNet verify the effectiveness of our simplex algorithm\non few-shot learning.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:52:57 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 00:59:36 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhang", "Bowen", ""], ["Zhang", "Xifan", ""], ["Cheng", "Fan", ""], ["Zhao", "Deli", ""]]}, {"id": "1807.10731", "submitter": "John Ashburner PhD", "authors": "John Ashburner, Mikael Brudfors, Kevin Bronik, Yael Balbastre", "title": "An Algorithm for Learning Shape and Appearance Models without\n  Annotations", "comments": "61 pages, 16 figures (some downsampled by a factor of 4), submitted\n  to MedIA", "journal-ref": null, "doi": "10.1016/j.media.2019.04.008", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a framework for automatically learning shape and\nappearance models for medical (and certain other) images. It is based on the\nidea that having a more accurate shape and appearance model leads to more\naccurate image registration, which in turn leads to a more accurate shape and\nappearance model. This leads naturally to an iterative scheme, which is based\non a probabilistic generative model that is fit using Gauss-Newton updates\nwithin an EM-like framework. It was developed with the aim of enabling\ndistributed privacy-preserving analysis of brain image data, such that shared\ninformation (shape and appearance basis functions) may be passed across sites,\nwhereas latent variables that encode individual images remain secure within\neach site. These latent variables are proposed as features for\nprivacy-preserving data mining applications.\n  The approach is demonstrated qualitatively on the KDEF dataset of 2D face\nimages, showing that it can align images that traditionally require shape and\nappearance models trained using manually annotated data (manually defined\nlandmarks etc.). It is applied to MNIST dataset of handwritten digits to show\nits potential for machine learning applications, particularly when training\ndata is limited. The model is able to handle ``missing data'', which allows it\nto be cross-validated according to how well it can predict left-out voxels. The\nsuitability of the derived features for classifying individuals into patient\ngroups was assessed by applying it to a dataset of over 1,900 segmented\nT1-weighted MR images, which included images from the COBRE and ABIDE datasets.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 16:59:22 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Ashburner", "John", ""], ["Brudfors", "Mikael", ""], ["Bronik", "Kevin", ""], ["Balbastre", "Yael", ""]]}, {"id": "1807.10744", "submitter": "Amir Rasouli", "authors": "Amir Rasouli, Pablo Lanillos, Gordon Cheng and John K. Tsotsos", "title": "Attention-based Active Visual Search for Mobile Robots", "comments": null, "journal-ref": null, "doi": "10.1007/s10514-019-09882-z", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an active visual search model for finding objects in unknown\nenvironments. The proposed algorithm guides the robot towards the sought object\nusing the relevant stimuli provided by the visual sensors. Existing search\nstrategies are either purely reactive or use simplified sensor models that do\nnot exploit all the visual information available. In this paper, we propose a\nnew model that actively extracts visual information via visual attention\ntechniques and, in conjunction with a non-myopic decision-making algorithm,\nleads the robot to search more relevant areas of the environment. The attention\nmodule couples both top-down and bottom-up attention models enabling the robot\nto search regions with higher importance first.\n  The proposed algorithm is evaluated on a mobile robot platform in a 3D\nsimulated environment. The results indicate that the use of visual attention\nsignificantly improves search, but the degree of improvement depends on the\nnature of the task and the complexity of the environment. In our experiments,\nwe found that performance enhancements of up to 42\\% in structured and 38\\% in\nhighly unstructured cluttered environments can be achieved using visual\nattention mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 17:48:02 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Rasouli", "Amir", ""], ["Lanillos", "Pablo", ""], ["Cheng", "Gordon", ""], ["Tsotsos", "John K.", ""]]}, {"id": "1807.10755", "submitter": "Victor Lorena De Farias Souza", "authors": "Victor L. F. Souza, Adriano L. I. Oliveira, and Robert Sabourin", "title": "A writer-independent approach for offline signature verification using\n  deep convolutional neural networks features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of features extracted using a deep convolutional neural network (CNN)\ncombined with a writer-dependent (WD) SVM classifier resulted in significant\nimprovement in performance of handwritten signature verification (HSV) when\ncompared to the previous state-of-the-art methods. In this work it is\ninvestigated whether the use of these CNN features provide good results in a\nwriter-independent (WI) HSV context, based on the dichotomy transformation\ncombined with the use of an SVM writer-independent classifier. The experiments\nperformed in the Brazilian and GPDS datasets show that (i) the proposed\napproach outperformed other WI-HSV methods from the literature, (ii) in the\nglobal threshold scenario, the proposed approach was able to outperform the\nwriter-dependent method with CNN features in the Brazilian dataset, (iii) in an\nuser threshold scenario, the results are similar to those obtained by the\nwriter-dependent method with CNN features.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 22:09:45 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Souza", "Victor L. F.", ""], ["Oliveira", "Adriano L. I.", ""], ["Sabourin", "Robert", ""]]}, {"id": "1807.10756", "submitter": "Woochan Hwang", "authors": "Sejin Park, Woochan Hwang, Kyu Hwan Jung, Joon Beom Seo, Namkug Kim", "title": "False Positive Reduction by Actively Mining Negative Samples for\n  Pulmonary Nodule Detection in Chest Radiographs", "comments": "Presented at the 2nd SIIM C-MIMI(SIIM Conference on Machine\n  Intelligence in Medical Imaging)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating large quantities of quality labeled data in medical imaging is\nvery time consuming and expensive. The performance of supervised algorithms for\nvarious tasks on imaging has improved drastically over the years, however the\navailability of data to train these algorithms have become one of the main\nbottlenecks for implementation. To address this, we propose a semi-supervised\nlearning method where pseudo-negative labels from unlabeled data are used to\nfurther refine the performance of a pulmonary nodule detection network in chest\nradiographs. After training with the proposed network, the false positive rate\nwas reduced to 0.1266 from 0.4864 while maintaining sensitivity at 0.89.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 23:29:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Park", "Sejin", ""], ["Hwang", "Woochan", ""], ["Jung", "Kyu Hwan", ""], ["Seo", "Joon Beom", ""], ["Kim", "Namkug", ""]]}, {"id": "1807.10757", "submitter": "Veronica Corona", "authors": "Veronica Corona, Jan Lellmann, Peter Nestor, Carola-Bibiane\n  Schoenlieb, Julio Acosta-Cabronero", "title": "A multi-contrast MRI approach to thalamus segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thalamic alterations are relevant to many neurological disorders including\nAlzheimer's disease, Parkinson's disease and multiple sclerosis. Routine\ninterventions to improve symptom severity in movement disorders, for example,\noften consist of surgery or deep brain stimulation to diencephalic nuclei.\nTherefore, accurate delineation of grey matter thalamic subregions is of the\nupmost clinical importance. MRI is highly appropriate for structural\nsegmentation as it provides different views of the anatomy from a single\nscanning session. Though with several contrasts potentially available, it is\nalso of increasing importance to develop new image segmentation techniques that\ncan operate multi-spectrally. We hereby propose a new segmentation method for\nuse with multi-modality data, which we evaluated for automated segmentation of\nmajor thalamic subnuclear groups using T1-, T2*-weighted and quantitative\nsusceptibility mapping (QSM) information. The proposed method consists of four\nsteps: highly iterative image co-registration, manual segmentation on the\naverage training-data template, supervised learning for pattern recognition,\nand a final convex optimisation step imposing further spatial constraints to\nrefine the solution. This led to solutions in greater agreement with manual\nsegmentation than the standard Morel atlas based approach. Furthermore, we show\nthat the multi-contrast approach boosts segmentation performances. We then\ninvestigated whether prior knowledge using the training-template contours could\nfurther improve convex segmentation accuracy and robustness, which led to\nhighly precise multi-contrast segmentations in single subjects. This approach\ncan be extended to most 3D imaging data types and any region of interest\ndiscernible in single scans or multi-subject templates.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 06:13:04 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Corona", "Veronica", ""], ["Lellmann", "Jan", ""], ["Nestor", "Peter", ""], ["Schoenlieb", "Carola-Bibiane", ""], ["Acosta-Cabronero", "Julio", ""]]}, {"id": "1807.10760", "submitter": "Jinming Duan", "authors": "Jinming Duan, Jo Schlemper, Wenjia Bai, Timothy J W Dawes, Ghalib\n  Bello, Georgia Doumou, Antonio De Marvao, Declan P O'Regan, Daniel Rueckert", "title": "Deep nested level sets: Fully automated segmentation of cardiac MR\n  images in patients with pulmonary hypertension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel and accurate optimisation method for\nsegmentation of cardiac MR (CMR) images in patients with pulmonary hypertension\n(PH). The proposed method explicitly takes into account the image features\nlearned from a deep neural network. To this end, we estimate simultaneous\nprobability maps over region and edge locations in CMR images using a fully\nconvolutional network. Due to the distinct morphology of the heart in patients\nwith PH, these probability maps can then be incorporated in a single nested\nlevel set optimisation framework to achieve multi-region segmentation with high\nefficiency. The proposed method uses an automatic way for level set\ninitialisation and thus the whole optimisation is fully automated. We\ndemonstrate that the proposed deep nested level set (DNLS) method outperforms\nexisting state-of-the-art methods for CMR segmentation in PH patients.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 22:38:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Duan", "Jinming", ""], ["Schlemper", "Jo", ""], ["Bai", "Wenjia", ""], ["Dawes", "Timothy J W", ""], ["Bello", "Ghalib", ""], ["Doumou", "Georgia", ""], ["De Marvao", "Antonio", ""], ["O'Regan", "Declan P", ""], ["Rueckert", "Daniel", ""]]}, {"id": "1807.10806", "submitter": "Hang Dong", "authors": "Xinyi Zhang, Hang Dong, Zhe Hu, Wei-Sheng Lai, Fei Wang, Ming-Hsuan\n  Yang", "title": "Gated Fusion Network for Joint Image Deblurring and Super-Resolution", "comments": "Accepted as an oral presentation at BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-image super-resolution is a fundamental task for vision applications\nto enhance the image quality with respect to spatial resolution. If the input\nimage contains degraded pixels, the artifacts caused by the degradation could\nbe amplified by super-resolution methods. Image blur is a common degradation\nsource. Images captured by moving or still cameras are inevitably affected by\nmotion blur due to relative movements between sensors and objects. In this\nwork, we focus on the super-resolution task with the presence of motion blur.\nWe propose a deep gated fusion convolution neural network to generate a clear\nhigh-resolution frame from a single natural image with severe blur. By\ndecomposing the feature extraction step into two task-independent streams, the\ndual-branch design can facilitate the training process by avoiding learning the\nmixed degradation all-in-one and thus enhance the final high-resolution\nprediction results. Extensive experiments demonstrate that our method generates\nsharper super-resolved images from low-resolution inputs with high\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 19:17:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Zhang", "Xinyi", ""], ["Dong", "Hang", ""], ["Hu", "Zhe", ""], ["Lai", "Wei-Sheng", ""], ["Wang", "Fei", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1807.10816", "submitter": "Ling Liang", "authors": "Ling Liang, Lei Deng, Yueling Zeng, Xing Hu, Yu Ji, Xin Ma, Guoqi Li\n  and Yuan Xie", "title": "Crossbar-aware neural network pruning", "comments": null, "journal-ref": "IEEE Access 6 (2018): 58324-58337", "doi": "10.1109/ACCESS.2018.2874823", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crossbar architecture based devices have been widely adopted in neural\nnetwork accelerators by taking advantage of the high efficiency on\nvector-matrix multiplication (VMM) operations. However, in the case of\nconvolutional neural networks (CNNs), the efficiency is compromised\ndramatically due to the large amounts of data reuse. Although some mapping\nmethods have been designed to achieve a balance between the execution\nthroughput and resource overhead, the resource consumption cost is still huge\nwhile maintaining the throughput.\n  Network pruning is a promising and widely studied leverage to shrink the\nmodel size. Whereas, previous work didn`t consider the crossbar architecture\nand the corresponding mapping method, which cannot be directly utilized by\ncrossbar-based neural network accelerators. Tightly combining the crossbar\nstructure and its mapping, this paper proposes a crossbar-aware pruning\nframework based on a formulated L0-norm constrained optimization problem.\nSpecifically, we design an L0-norm constrained gradient descent (LGD) with\nrelaxant probabilistic projection (RPP) to solve this problem. Two grains of\nsparsity are successfully achieved: i) intuitive crossbar-grain sparsity and\nii) column-grain sparsity with output recombination, based on which we further\npropose an input feature maps (FMs) reorder method to improve the model\naccuracy. We evaluate our crossbar-aware pruning framework on median-scale\nCIFAR10 dataset and large-scale ImageNet dataset with VGG and ResNet models.\nOur method is able to reduce the crossbar overhead by 44%-72% with little\naccuracy degradation. This work greatly saves the resource and the related\nenergy cost, which provides a new co-design solution for mapping CNNs onto\nvarious crossbar devices with significantly higher efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 21:08:35 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 23:13:32 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 03:08:39 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Liang", "Ling", ""], ["Deng", "Lei", ""], ["Zeng", "Yueling", ""], ["Hu", "Xing", ""], ["Ji", "Yu", ""], ["Ma", "Xin", ""], ["Li", "Guoqi", ""], ["Xie", "Yuan", ""]]}, {"id": "1807.10819", "submitter": "Andrew Jesson D", "authors": "Andrew Jesson, Nicolas Guizard, Sina Hamidi Ghalehjegh, Damien Goblot,\n  Florian Soudan, Nicolas Chapados", "title": "CASED: Curriculum Adaptive Sampling for Extreme Data Imbalance", "comments": "20th International Conference on Medical Image Computing and Computer\n  Assisted Intervention 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66179-7_73", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce CASED, a novel curriculum sampling algorithm that facilitates\nthe optimization of deep learning segmentation or detection models on data sets\nwith extreme class imbalance. We evaluate the CASED learning framework on the\ntask of lung nodule detection in chest CT. In contrast to two-stage solutions,\nwherein nodule candidates are first proposed by a segmentation model and\nrefined by a second detection stage, CASED improves the training of deep nodule\nsegmentation models (e.g. UNet) to the point where state of the art results are\nachieved using only a trivial detection stage. CASED improves the optimization\nof deep segmentation models by allowing them to first learn how to distinguish\nnodules from their immediate surroundings, while continuously adding a greater\nproportion of difficult-to-classify global context, until uniformly sampling\nfrom the empirical data distribution. Using CASED during training yields a\nminimalist proposal to the lung nodule detection problem that tops the LUNA16\nnodule detection benchmark with an average sensitivity score of 88.35%.\nFurthermore, we find that models trained using CASED are robust to nodule\nannotation quality by showing that comparable results can be achieved when only\na point and radius for each ground truth nodule are provided during training.\nFinally, the CASED learning framework makes no assumptions with regard to\nimaging modality or segmentation target and should generalize to other medical\nimaging problems where class imbalance is a persistent problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 20:10:11 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Jesson", "Andrew", ""], ["Guizard", "Nicolas", ""], ["Ghalehjegh", "Sina Hamidi", ""], ["Goblot", "Damien", ""], ["Soudan", "Florian", ""], ["Chapados", "Nicolas", ""]]}, {"id": "1807.10820", "submitter": "Ansgar Steland", "authors": "Evgenii Sovetkin, Ansgar Steland", "title": "Automatic Processing and Solar Cell Detection in Photovoltaic\n  Electroluminescence Images", "comments": null, "journal-ref": "Integrated Computer-Aided Engineering, vol. 26, no. 2, pp.\n  123-137, 2019", "doi": "10.3233/ICA-180588", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroluminescence (EL) imaging is a powerful and established technique for\nassessing the quality of photovoltaic (PV) modules, which consist of many\nelectrically connected solar cells arranged in a grid. The analysis of\nimperfect real-world images requires reliable methods for preprocessing,\ndetection and extraction of the cells. We propose several methods for those\ntasks, which, however, can be modified to related imaging problems where\nsimilar geometric objects need to be detected accurately. Allowing for images\ntaken under difficult outdoor conditions, we present methods to correct for\nrotation and perspective distortions. The next important step is the extraction\nof the solar cells of a PV module, for instance to pass them to a procedure to\ndetect and analyze defects on their surface. We propose a method based on\nspecialized Hough transforms, which allows to extract the cells even when the\nmodule is surrounded by disturbing background and a fast method based on\ncumulated sums (CUSUM) change detection to extract the cell area of single-cell\nmini-module, where the correction of perspective distortion is implicitly done.\nThe methods are highly automatized to allow for big data analyses. Their\napplication to a large database of EL images substantiates that the methods\nwork reliably on a large scale for real-world images. Simulations show that the\napproach achieves high accuracy, reliability and robustness. This even holds\nfor low contrast images as evaluated by comparing the simulated accuracy for a\nlow and a high contrast image.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 09:58:34 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Sovetkin", "Evgenii", ""], ["Steland", "Ansgar", ""]]}, {"id": "1807.10831", "submitter": "Kamlesh Pawar", "authors": "Kamlesh Pawar, Zhaolin Chen, N. Jon Shah, and Gary F. Egan", "title": "MoCoNet: Motion Correction in 3D MPRAGE images using a Convolutional\n  Neural Network approach", "comments": null, "journal-ref": null, "doi": "10.1002/nbm.4225", "report-no": null, "categories": "eess.IV cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The suppression of motion artefacts from MR images is a challenging\ntask. The purpose of this paper is to develop a standalone novel technique to\nsuppress motion artefacts from MR images using a data-driven deep learning\napproach. Methods: A deep learning convolutional neural network (CNN) was\ndeveloped to remove motion artefacts in brain MR images. A CNN was trained on\nsimulated motion corrupted images to identify and suppress artefacts due to the\nmotion. The network was an encoder-decoder CNN architecture where the encoder\ndecomposed the motion corrupted images into a set of feature maps. The feature\nmaps were then combined by the decoder network to generate a motion-corrected\nimage. The network was tested on an unseen simulated dataset and an\nexperimental, motion corrupted in vivo brain dataset. Results: The trained\nnetwork was able to suppress the motion artefacts in the simulated motion\ncorrupted images, and the mean percentage error in the motion corrected images\nwas 2.69 % with a standard deviation of 0.95 %. The network was able to\neffectively suppress the motion artefacts from the experimental dataset,\ndemonstrating the generalisation capability of the trained network. Conclusion:\nA novel and generic motion correction technique has been developed that can\nsuppress motion artefacts from motion corrupted MR images. The proposed\ntechnique is a standalone post-processing method that does not interfere with\ndata acquisition or reconstruction parameters, thus making it suitable for a\nmultitude of MR sequences.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 09:24:54 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Pawar", "Kamlesh", ""], ["Chen", "Zhaolin", ""], ["Shah", "N. Jon", ""], ["Egan", "Gary F.", ""]]}, {"id": "1807.10839", "submitter": "Snehashis Roy", "authors": "Snehashis Roy, John A. Butman, Leighton Chan, Dzung L. Pham", "title": "TBI Contusion Segmentation from MRI using Convolutional Neural Networks", "comments": "https://ieeexplore.ieee.org/abstract/document/8363545/, IEEE 15th\n  International Symposium on Biomedical Imaging (ISBI 2018)", "journal-ref": null, "doi": "10.1109/ISBI.2018.8363545", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traumatic brain injury (TBI) is caused by a sudden trauma to the head that\nmay result in hematomas and contusions and can lead to stroke or chronic\ndisability. An accurate quantification of the lesion volumes and their\nlocations is essential to understand the pathophysiology of TBI and its\nprogression. In this paper, we propose a fully convolutional neural network\n(CNN) model to segment contusions and lesions from brain magnetic resonance\n(MR) images of patients with TBI. The CNN architecture proposed here was based\non a state of the art CNN architecture from Google, called Inception. Using a\n3-layer Inception network, lesions are segmented from multi-contrast MR images.\nWhen compared with two recent TBI lesion segmentation methods, one based on CNN\n(called DeepMedic) and another based on random forests, the proposed algorithm\nshowed improved segmentation accuracy on images of 18 patients with mild to\nsevere TBI. Using a leave-one-out cross validation, the proposed model achieved\na median Dice of 0.75, which was significantly better (p<0.01) than the two\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 21:56:05 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Roy", "Snehashis", ""], ["Butman", "John A.", ""], ["Chan", "Leighton", ""], ["Pham", "Dzung L.", ""]]}, {"id": "1807.10850", "submitter": "Snehashis Roy", "authors": "Snehashis Roy, John A. Butman, Dzung L. Pham", "title": "Synthesizing CT from Ultrashort Echo-Time MR Images via Convolutional\n  Neural Networks", "comments": "https://link.springer.com/chapter/10.1007/978-3-319-68127-6_3,\n  International Workshop on Simulation and Synthesis in Medical Imaging\n  (SASHIMI 2017)", "journal-ref": null, "doi": "10.1007/978-3-319-68127-6_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing popularity of PET-MR scanners in clinical applications,\nsynthesis of CT images from MR has been an important research topic. Accurate\nPET image reconstruction requires attenuation correction, which is based on the\nelectron density of tissues and can be obtained from CT images. While CT\nmeasures electron density information for x-ray photons, MR images convey\ninformation about the magnetic properties of tissues. Therefore, with the\nadvent of PET-MR systems, the attenuation coefficients need to be indirectly\nestimated from MR images. In this paper, we propose a fully convolutional\nneural network (CNN) based method to synthesize head CT from ultra-short\necho-time (UTE) dual-echo MR images. Unlike traditional $T_1$-w images which do\nnot have any bone signal, UTE images show some signal for bone, which makes it\na good candidate for MR to CT synthesis. A notable advantage of our approach is\nthat accurate results were achieved with a small training data set. Using an\natlas of a single CT and dual-echo UTE pair, we train a deep neural network\nmodel to learn the transform of MR intensities to CT using patches. We compared\nour CNN based model with a state-of-the-art registration based as well as a\nBayesian model based CT synthesis method, and showed that the proposed CNN\nmodel outperforms both of them. We also compared the proposed model when only\n$T_1$-w images are available instead of UTE, and show that UTE images produce\nbetter synthesis than using just $T_1$-w images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 22:43:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Roy", "Snehashis", ""], ["Butman", "John A.", ""], ["Pham", "Dzung L.", ""]]}, {"id": "1807.10889", "submitter": "Haoshu Fang", "authors": "Hao-Shu Fang, Jinkun Cao, Yu-Wing Tai, Cewu Lu", "title": "Pairwise Body-Part Attention for Recognizing Human-Object Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human-object interactions (HOI) recognition, conventional methods consider\nthe human body as a whole and pay a uniform attention to the entire body\nregion. They ignore the fact that normally, human interacts with an object by\nusing some parts of the body. In this paper, we argue that different body parts\nshould be paid with different attention in HOI recognition, and the\ncorrelations between different body parts should be further considered. This is\nbecause our body parts always work collaboratively. We propose a new pairwise\nbody-part attention model which can learn to focus on crucial parts, and their\ncorrelations for HOI recognition. A novel attention based feature selection\nmethod and a feature representation scheme that can capture pairwise\ncorrelations between body parts are introduced in the model. Our proposed\napproach achieved 4% improvement over the state-of-the-art results in HOI\nrecognition on the HICO dataset. We will make our model and source codes\npublicly available.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 04:37:18 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Fang", "Hao-Shu", ""], ["Cao", "Jinkun", ""], ["Tai", "Yu-Wing", ""], ["Lu", "Cewu", ""]]}, {"id": "1807.10908", "submitter": "T M Feroz Ali", "authors": "T M Feroz Ali, Subhasis Chaudhuri", "title": "Maximum Margin Metric Learning Over Discriminative Nullspace for Person\n  Re-identification", "comments": "Accepted for ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel metric learning framework called Nullspace\nKernel Maximum Margin Metric Learning (NK3ML) which efficiently addresses the\nsmall sample size (SSS) problem inherent in person re-identification and offers\na significant performance gain over existing state-of-the-art methods. Taking\nadvantage of the very high dimensionality of the feature space, the metric is\nlearned using a maximum margin criterion (MMC) over a discriminative nullspace\nwhere all training sample points of a given class map onto a single point,\nminimizing the within class scatter. A kernel version of MMC is used to obtain\na better between class separation. Extensive experiments on four challenging\nbenchmark datasets for person re-identification demonstrate that the proposed\nalgorithm outperforms all existing methods. We obtain 99.8% rank-1 accuracy on\nthe most widely accepted and challenging dataset VIPeR, compared to the\nprevious state of the art being only 63.92%.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 07:53:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ali", "T M Feroz", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1807.10915", "submitter": "Dan Xu", "authors": "Andrea Pilzer, Dan Xu, Mihai Marian Puscas, Elisa Ricci, Nicu Sebe", "title": "Unsupervised Adversarial Depth Estimation using Cycled Generative\n  Networks", "comments": "To appear in 3DV 2018. Code is available on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent deep monocular depth estimation approaches based on supervised\nregression have achieved remarkable performance, costly ground truth\nannotations are required during training. To cope with this issue, in this\npaper we present a novel unsupervised deep learning approach for predicting\ndepth maps and show that the depth estimation task can be effectively tackled\nwithin an adversarial learning framework. Specifically, we propose a deep\ngenerative network that learns to predict the correspondence field i.e. the\ndisparity map between two image views in a calibrated stereo camera setting.\nThe proposed architecture consists of two generative sub-networks jointly\ntrained with adversarial learning for reconstructing the disparity map and\norganized in a cycle such as to provide mutual constraints and supervision to\neach other. Extensive experiments on the publicly available datasets KITTI and\nCityscapes demonstrate the effectiveness of the proposed model and competitive\nresults with state of the art methods. The code and trained model are available\non https://github.com/andrea-pilzer/unsup-stereo-depthGAN.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 09:49:41 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Pilzer", "Andrea", ""], ["Xu", "Dan", ""], ["Puscas", "Mihai Marian", ""], ["Ricci", "Elisa", ""], ["Sebe", "Nicu", ""]]}, {"id": "1807.10916", "submitter": "Yabin Zhang", "authors": "Yabin Zhang, Hui Tang and Kui Jia", "title": "Fine-Grained Visual Categorization using Meta-Learning Optimization with\n  Sample Selection of Auxiliary Data", "comments": "Accepted to ECCV 2018, Source code available at:\n  https://github.com/YabinZhang1994/MetaFGNet, European Conference on Computer\n  Vision (ECCV), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine-grained visual categorization (FGVC) is challenging due in part to the\nfact that it is often difficult to acquire an enough number of training\nsamples. To employ large models for FGVC without suffering from overfitting,\nexisting methods usually adopt a strategy of pre-training the models using a\nrich set of auxiliary data, followed by fine-tuning on the target FGVC task.\nHowever, the objective of pre-training does not take the target task into\naccount, and consequently such obtained models are suboptimal for fine-tuning.\nTo address this issue, we propose in this paper a new deep FGVC model termed\nMetaFGNet. Training of MetaFGNet is based on a novel regularized meta-learning\nobjective, which aims to guide the learning of network parameters so that they\nare optimal for adapting to the target FGVC task. Based on MetaFGNet, we also\npropose a simple yet effective scheme for selecting more useful samples from\nthe auxiliary data. Experiments on benchmark FGVC datasets show the efficacy of\nour proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 09:54:54 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Zhang", "Yabin", ""], ["Tang", "Hui", ""], ["Jia", "Kui", ""]]}, {"id": "1807.10931", "submitter": "Daniel Ward", "authors": "Daniel Ward, Peyman Moghadam and Nicolas Hudson", "title": "Deep Leaf Segmentation Using Synthetic Data", "comments": "British Machine Vision Conference (BMVC) 2018 Proceedings. CVPPP\n  Workshop at BMVC 2018. Dataset available for download at:\n  https://research.csiro.au/robotics/databases/synthetic-arabidopsis-dataset/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated segmentation of individual leaves of a plant in an image is a\nprerequisite to measure more complex phenotypic traits in high-throughput\nphenotyping. Applying state-of-the-art machine learning approaches to tackle\nleaf instance segmentation requires a large amount of manually annotated\ntraining data. Currently, the benchmark datasets for leaf segmentation contain\nonly a few hundred labeled training images. In this paper, we propose a\nframework for leaf instance segmentation by augmenting real plant datasets with\ngenerated synthetic images of plants inspired by domain randomisation. We train\na state-of-the-art deep learning segmentation architecture (Mask-RCNN) with a\ncombination of real and synthetic images of Arabidopsis plants. Our proposed\napproach achieves 90% leaf segmentation score on the A1 test set outperforming\nthe-state-of-the-art approaches for the CVPPP Leaf Segmentation Challenge\n(LSC). Our approach also achieves 81% mean performance over all five test\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 13:05:04 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 00:51:41 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 23:50:41 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Ward", "Daniel", ""], ["Moghadam", "Peyman", ""], ["Hudson", "Nicolas", ""]]}, {"id": "1807.10936", "submitter": "Federico Paredes-Vall\\'es", "authors": "Federico Paredes-Vall\\'es, Kirk Y. W. Scheper, and Guido C. H. E. de\n  Croon", "title": "Unsupervised Learning of a Hierarchical Spiking Neural Network for\n  Optical Flow Estimation: From Events to Global Motion Perception", "comments": "14 pages, 14 figures", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2019", "doi": "10.1109/TPAMI.2019.2903179", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of spiking neural networks and event-based vision sensors\nholds the potential of highly efficient and high-bandwidth optical flow\nestimation. This paper presents the first hierarchical spiking architecture in\nwhich motion (direction and speed) selectivity emerges in an unsupervised\nfashion from the raw stimuli generated with an event-based camera. A novel\nadaptive neuron model and stable spike-timing-dependent plasticity formulation\nare at the core of this neural network governing its spike-based processing and\nlearning, respectively. After convergence, the neural architecture exhibits the\nmain properties of biological visual motion systems, namely feature extraction\nand local and global motion perception. Convolutional layers with input\nsynapses characterized by single and multiple transmission delays are employed\nfor feature and local motion perception, respectively; while global motion\nselectivity emerges in a final fully-connected layer. The proposed solution is\nvalidated using synthetic and real event sequences. Along with this paper, we\nprovide the cuSNN library, a framework that enables GPU-accelerated simulations\nof large-scale spiking neural networks. Source code and samples are available\nat https://github.com/tudelft/cuSNN.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 13:37:41 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 09:06:11 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Paredes-Vall\u00e9s", "Federico", ""], ["Scheper", "Kirk Y. W.", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "1807.10937", "submitter": "Muhammad Asad", "authors": "Muhammad Asad, Rilwan Basaru, S M Masudur Rahman Al Arif and Greg\n  Slabaugh", "title": "PROPEL: Probabilistic Parametric Regression Loss for Convolutional\n  Neural Networks", "comments": "11 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have enabled\nsignificant advancements to the state-of-the-art in computer vision. For\nclassification tasks, CNNs have widely employed probabilistic output and have\nshown the significance of providing additional confidence for predictions.\nHowever, such probabilistic methodologies are not widely applicable for\naddressing regression problems using CNNs, as regression involves learning\nunconstrained continuous and, in many cases, multi-variate target variables. We\npropose a PRObabilistic Parametric rEgression Loss (PROPEL) that facilitates\nCNNs to learn parameters of probability distributions for addressing\nprobabilistic regression problems. PROPEL is fully differentiable and, hence,\ncan be easily incorporated for end-to-end training of existing CNN regression\narchitectures using existing optimization algorithms. The proposed method is\nflexible as it enables learning complex unconstrained probabilities while being\ngeneralizable to higher dimensional multi-variate regression problems. We\nutilize a PROPEL-based CNN to address the problem of learning hand and head\norientation from uncalibrated color images. Our experimental validation and\ncomparison with existing CNN regression loss functions show that PROPEL\nimproves the accuracy of a CNN by enabling probabilistic regression, while\nsignificantly reducing required model parameters by $10 \\times$, resulting in\nimproved generalization as compared to the existing state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 13:41:44 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 21:09:41 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Asad", "Muhammad", ""], ["Basaru", "Rilwan", ""], ["Arif", "S M Masudur Rahman Al", ""], ["Slabaugh", "Greg", ""]]}, {"id": "1807.10972", "submitter": "Raghav Mehta", "authors": "Raghav Mehta, Tal Arbel", "title": "RS-Net: Regression-Segmentation 3D CNN for Synthesis of Full Resolution\n  Missing Brain MRI in the Presence of Tumours", "comments": "Accepted at Workshop on Simulation and Synthesis in Medical Imaging -\n  SASHIMI2018 held in conjunction with the 21st International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate synthesis of a full 3D MR image containing tumours from available\nMRI (e.g. to replace an image that is currently unavailable or corrupted) would\nprovide a clinician as well as downstream inference methods with important\ncomplementary information for disease analysis. In this paper, we present an\nend-to-end 3D convolution neural network that takes a set of acquired MR image\nsequences (e.g. T1, T2, T1ce) as input and concurrently performs (1) regression\nof the missing full resolution 3D MRI (e.g. FLAIR) and (2) segmentation of the\ntumour into subtypes (e.g. enhancement, core). The hypothesis is that this\nwould focus the network to perform accurate synthesis in the area of the\ntumour. Experiments on the BraTS 2015 and 2017 datasets [1] show that: (1) the\nproposed method gives better performance than state-of-the-art methods in terms\nof established global evaluation metrics (e.g. PSNR), (2) replacing real MR\nvolumes with the synthesized MRI does not lead to significant degradation in\ntumour and sub-structure segmentation accuracy. The system further provides\nuncertainty estimates based on Monte Carlo (MC) dropout [11] for the\nsynthesized volume at each voxel, permitting quantification of the system's\nconfidence in the output at each location.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 19:36:46 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Mehta", "Raghav", ""], ["Arbel", "Tal", ""]]}, {"id": "1807.10982", "submitter": "Chen Sun", "authors": "Chen Sun and Abhinav Shrivastava and Carl Vondrick and Kevin Murphy\n  and Rahul Sukthankar and Cordelia Schmid", "title": "Actor-Centric Relation Network", "comments": "ECCV 2018 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art approaches for spatio-temporal action localization\nrely on detections at the frame level and model temporal context with 3D\nConvNets. Here, we go one step further and model spatio-temporal relations to\ncapture the interactions between human actors, relevant objects and scene\nelements essential to differentiate similar human actions. Our approach is\nweakly supervised and mines the relevant elements automatically with an\nactor-centric relational network (ACRN). ACRN computes and accumulates\npair-wise relation information from actor and global scene features, and\ngenerates relation features for action classification. It is implemented as\nneural networks and can be trained jointly with an existing action detection\nsystem. We show that ACRN outperforms alternative approaches which capture\nrelation information, and that the proposed framework improves upon the\nstate-of-the-art performance on JHMDB and AVA. A visualization of the learned\nrelation features confirms that our approach is able to attend to the relevant\nrelations for each action.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 22:48:27 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sun", "Chen", ""], ["Shrivastava", "Abhinav", ""], ["Vondrick", "Carl", ""], ["Murphy", "Kevin", ""], ["Sukthankar", "Rahul", ""], ["Schmid", "Cordelia", ""]]}, {"id": "1807.10990", "submitter": "Chen Li", "authors": "Chen Li, Mai Xu, Xinzhe Du and Zulin Wang", "title": "Bridge the Gap Between VQA and Human Behavior on Omnidirectional Video:\n  A Large-Scale Dataset and a Deep Learning Model", "comments": "Accepted by ACM MM 2018", "journal-ref": null, "doi": "10.1145/3240508.3240581", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnidirectional video enables spherical stimuli with the $360 \\times 180^\n\\circ$ viewing range. Meanwhile, only the viewport region of omnidirectional\nvideo can be seen by the observer through head movement (HM), and an even\nsmaller region within the viewport can be clearly perceived through eye\nmovement (EM). Thus, the subjective quality of omnidirectional video may be\ncorrelated with HM and EM of human behavior. To fill in the gap between\nsubjective quality and human behavior, this paper proposes a large-scale visual\nquality assessment (VQA) dataset of omnidirectional video, called VQA-OV, which\ncollects 60 reference sequences and 540 impaired sequences. Our VQA-OV dataset\nprovides not only the subjective quality scores of sequences but also the HM\nand EM data of subjects. By mining our dataset, we find that the subjective\nquality of omnidirectional video is indeed related to HM and EM. Hence, we\ndevelop a deep learning model, which embeds HM and EM, for objective VQA on\nomnidirectional video. Experimental results show that our model significantly\nimproves the state-of-the-art performance of VQA on omnidirectional video.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 02:03:14 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Li", "Chen", ""], ["Xu", "Mai", ""], ["Du", "Xinzhe", ""], ["Wang", "Zulin", ""]]}, {"id": "1807.10993", "submitter": "Ramakrishna Prabhu", "authors": "Ramakrishna Prabhu, Xiaojing Yu, Zhangyang Wang, Ding Liu and Anxiao\n  (Andrew) Jiang", "title": "U-Finger: Multi-Scale Dilated Convolutional Network for Fingerprint\n  Image Denoising and Inpainting", "comments": "ECCV 2018 Track-3 Challenge Inpainting to denoise fingerprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the challenging problem of fingerprint image denoising and\ninpainting. To tackle the challenge of suppressing complicated artifacts (blur,\nbrightness, contrast, elastic transformation, occlusion, scratch, resolution,\nrotation, and so on) while preserving fine textures, we develop a multi-scale\nconvolutional network, termed U- Finger. Based on the domain expertise, we show\nthat the usage of dilated convolutions as well as the removal of padding have\nimportant positive impacts on the final restoration performance, in addition to\nmulti-scale cascaded feature modules. Our model achieves the overall ranking of\nNo.2 in the ECCV 2018 Chalearn LAP Inpainting Competition Track 3 (Fingerprint\nDenoising and Inpainting). Among all participating teams, we obtain the MSE of\n0.0231 (rank 2), PSNR 16.9688 dB (rank 2), and SSIM 0.8093 (rank 3) on the\nhold-out testing set.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 02:42:40 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 04:36:18 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Prabhu", "Ramakrishna", "", "Andrew"], ["Yu", "Xiaojing", "", "Andrew"], ["Wang", "Zhangyang", "", "Andrew"], ["Liu", "Ding", "", "Andrew"], ["Anxiao", "", "", "Andrew"], ["Jiang", "", ""]]}, {"id": "1807.11010", "submitter": "Santhosh Kumar Ramakrishnan", "authors": "Santhosh K. Ramakrishnan and Kristen Grauman", "title": "Sidekick Policy Learning for Active Visual Exploration", "comments": "26 pages, 13 figures, to appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an active visual exploration scenario, where an agent must\nintelligently select its camera motions to efficiently reconstruct the full\nenvironment from only a limited set of narrow field-of-view glimpses. While the\nagent has full observability of the environment during training, it has only\npartial observability once deployed, being constrained by what portions it has\nseen and what camera motions are permissible. We introduce sidekick policy\nlearning to capitalize on this imbalance of observability. The main idea is a\npreparatory learning phase that attempts simplified versions of the eventual\nexploration task, then guides the agent via reward shaping or initial policy\nsupervision. To support interpretation of the resulting policies, we also\ndevelop a novel policy visualization technique. Results on active visual\nexploration tasks with 360 scenes and 3D objects show that sidekicks\nconsistently improve performance and convergence rates over existing methods.\nCode, data and demos are available.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 06:32:42 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ramakrishnan", "Santhosh K.", ""], ["Grauman", "Kristen", ""]]}, {"id": "1807.11013", "submitter": "Yuxi Li", "authors": "Yuxi Li, Jiuwei Li, Weiyao Lin and Jianguo Li", "title": "Tiny-DSOD: Lightweight Object Detection for Resource-Restricted Usages", "comments": "12 pages, 3 figures, accepted by BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object detection has made great progress in the past few years along with the\ndevelopment of deep learning. However, most current object detection methods\nare resource hungry, which hinders their wide deployment to many resource\nrestricted usages such as usages on always-on devices, battery-powered low-end\ndevices, etc. This paper considers the resource and accuracy trade-off for\nresource-restricted usages during designing the whole object detection\nframework. Based on the deeply supervised object detection (DSOD) framework, we\npropose Tiny-DSOD dedicating to resource-restricted usages. Tiny-DSOD\nintroduces two innovative and ultra-efficient architecture blocks: depthwise\ndense block (DDB) based backbone and depthwise feature-pyramid-network (D-FPN)\nbased front-end. We conduct extensive experiments on three famous benchmarks\n(PASCAL VOC 2007, KITTI, and COCO), and compare Tiny-DSOD to the\nstate-of-the-art ultra-efficient object detection solutions such as Tiny-YOLO,\nMobileNet-SSD (v1 & v2), SqueezeDet, Pelee, etc. Results show that Tiny-DSOD\noutperforms these solutions in all the three metrics (parameter-size, FLOPs,\naccuracy) in each comparison. For instance, Tiny-DSOD achieves 72.1% mAP with\nonly 0.95M parameters and 1.06B FLOPs, which is by far the state-of-the-arts\nresult with such a low resource requirement.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 06:58:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Li", "Yuxi", ""], ["Li", "Jiuwei", ""], ["Lin", "Weiyao", ""], ["Li", "Jianguo", ""]]}, {"id": "1807.11034", "submitter": "Wei Dong", "authors": "Wei Dong, Qiuyuan Wang, Xin Wang, Hongbin Zha", "title": "PSDF Fusion: Probabilistic Signed Distance Function for On-the-fly 3D\n  Data Fusion and Scene Reconstruction", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel 3D spatial representation for data fusion and scene\nreconstruction. Probabilistic Signed Distance Function (Probabilistic SDF,\nPSDF) is proposed to depict uncertainties in the 3D space. It is modeled by a\njoint distribution describing SDF value and its inlier probability, reflecting\ninput data quality and surface geometry. A hybrid data structure involving\nvoxel, surfel, and mesh is designed to fully exploit the advantages of various\nprevalent 3D representations. Connected by PSDF, these components reasonably\ncooperate in a consistent frame- work. Given sequential depth measurements,\nPSDF can be incrementally refined with less ad hoc parametric Bayesian\nupdating. Supported by PSDF and the efficient 3D data representation,\nhigh-quality surfaces can be extracted on-the-fly, and in return contribute to\nreliable data fu- sion using the geometry information. Experiments demonstrate\nthat our system reconstructs scenes with higher model quality and lower\nredundancy, and runs faster than existing online mesh generation systems.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 10:10:12 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Dong", "Wei", ""], ["Wang", "Qiuyuan", ""], ["Wang", "Xin", ""], ["Zha", "Hongbin", ""]]}, {"id": "1807.11035", "submitter": "Ziming Wang", "authors": "Zi-Ming Wang, Gui-Song Xia, Yi-Peng Zhang", "title": "Texture Mixing by Interpolating Deep Statistics via Gaussian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, enthusiastic studies have devoted to texture synthesis using deep\nneural networks, because these networks excel at handling complex patterns in\nimages. In these models, second-order statistics, such as Gram matrix, are used\nto describe textures. Despite the fact that these model have achieved promising\nresults, the structure of their parametric space is still unclear,\nconsequently, it is difficult to use them to mix textures. This paper addresses\nthe texture mixing problem by using a Gaussian scheme to interpolate deep\nstatistics computed from deep neural networks. More precisely, we first reveal\nthat the statistics used in existing deep models can be unified using a\nstationary Gaussian scheme. We then present a novel algorithm to mix these\nstatistics by interpolating between Gaussian models using optimal transport. We\nfurther apply our scheme to Neural Style Transfer, where we can create mixed\nstyles. The experiments demonstrate that our method can achieve\nstate-of-the-art results. Because all the computations are implemented in\nclosed forms, our mixing algorithm adds only negligible time to the original\ntexture synthesis procedure.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 10:21:37 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wang", "Zi-Ming", ""], ["Xia", "Gui-Song", ""], ["Zhang", "Yi-Peng", ""]]}, {"id": "1807.11037", "submitter": "Wan-Ting Hsu", "authors": "Po-Yu Huang, Wan-Ting Hsu, Chun-Yueh Chiu, Ting-Fan Wu, Min Sun", "title": "Efficient Uncertainty Estimation for Semantic Segmentation in Videos", "comments": "16 pages. ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty estimation in deep learning becomes more important recently. A\ndeep learning model can't be applied in real applications if we don't know\nwhether the model is certain about the decision or not. Some literature\nproposes the Bayesian neural network which can estimate the uncertainty by\nMonte Carlo Dropout (MC dropout). However, MC dropout needs to forward the\nmodel $N$ times which results in $N$ times slower. For real-time applications\nsuch as a self-driving car system, which needs to obtain the prediction and the\nuncertainty as fast as possible, so that MC dropout becomes impractical. In\nthis work, we propose the region-based temporal aggregation (RTA) method which\nleverages the temporal information in videos to simulate the sampling\nprocedure. Our RTA method with Tiramisu backbone is 10x faster than the MC\ndropout with Tiramisu backbone ($N=5$). Furthermore, the uncertainty estimation\nobtained by our RTA method is comparable to MC dropout's uncertainty estimation\non pixel-level and frame-level metrics.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 10:38:00 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Huang", "Po-Yu", ""], ["Hsu", "Wan-Ting", ""], ["Chiu", "Chun-Yueh", ""], ["Wu", "Ting-Fan", ""], ["Sun", "Min", ""]]}, {"id": "1807.11042", "submitter": "Fu Xiong", "authors": "Fu Xiong, Yang Xiao, Zhiguo Cao, Kaicheng Gong, Zhiwen Fang, and Joey\n  Tianyi Zhou", "title": "Towards Good Practices on Building Effective CNN Baseline Model for\n  Person Re-identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is indeed a challenging visual recognition task due\nto the critical issues of human pose variation, human body occlusion, camera\nview variation, etc. To address this, most of the state-of-the-art approaches\nare proposed based on deep convolutional neural network (CNN), being leveraged\nby its strong feature learning power and classification boundary fitting\ncapacity. Although the vital role towards person re-identification, how to\nbuild effective CNN baseline model has not been well studied yet. To answer\nthis open question, we propose 3 good practices in this paper from the\nperspectives of adjusting CNN architecture and training procedure. In\nparticular, they are adding batch normalization after the global pooling layer,\nexecuting identity categorization directly using only one fully-connected, and\nusing Adam as optimizer. The extensive experiments on 3 widely-used benchmark\ndatasets demonstrate that, our propositions essentially facilitate the CNN\nbaseline model to achieve the state-of-the-art performance without any other\nhigh-level domain knowledge or low-level technical trick.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 11:38:33 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Xiong", "Fu", ""], ["Xiao", "Yang", ""], ["Cao", "Zhiguo", ""], ["Gong", "Kaicheng", ""], ["Fang", "Zhiwen", ""], ["Zhou", "Joey Tianyi", ""]]}, {"id": "1807.11071", "submitter": "Wangmeng Zuo", "authors": "Yingjie Yao, Xiaohe Wu, Lei Zhang, Shiguang Shan and Wangmeng Zuo", "title": "Joint Representation and Truncated Inference Learning for Correlation\n  Filter based Tracking", "comments": "16 pages, 3 figures, ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation filter (CF) based trackers generally include two modules, i.e.,\nfeature representation and on-line model adaptation. In existing off-line deep\nlearning models for CF trackers, the model adaptation usually is either\nabandoned or has closed-form solution to make it feasible to learn deep\nrepresentation in an end-to-end manner. However, such solutions fail to exploit\nthe advances in CF models, and cannot achieve competitive accuracy in\ncomparison with the state-of-the-art CF trackers. In this paper, we investigate\nthe joint learning of deep representation and model adaptation, where an\nupdater network is introduced for better tracking on future frame by taking\ncurrent frame representation, tracking result, and last CF tracker as input. By\nmodeling the representor as convolutional neural network (CNN), we truncate the\nalternating direction method of multipliers (ADMM) and interpret it as a deep\nnetwork of updater, resulting in our model for learning representation and\ntruncated inference (RTINet). Experiments demonstrate that our RTINet tracker\nachieves favorable tracking accuracy against the state-of-the-art trackers and\nits rapid version can run at a real-time speed of 24 fps. The code and\npre-trained models will be publicly available at\nhttps://github.com/tourmaline612/RTINet.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 15:24:51 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yao", "Yingjie", ""], ["Wu", "Xiaohe", ""], ["Zhang", "Lei", ""], ["Shan", "Shiguang", ""], ["Zuo", "Wangmeng", ""]]}, {"id": "1807.11078", "submitter": "Deyu Meng", "authors": "Wei Wei, Deyu Meng, Qian Zhao, Zongben Xu and Ying Wu", "title": "Semi-supervised Transfer Learning for Image Rain Removal", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single image rain removal is a typical inverse problem in computer vision.\nThe deep learning technique has been verified to be effective for this task and\nachieved state-of-the-art performance. However, previous deep learning methods\nneed to pre-collect a large set of image pairs with/without synthesized rain\nfor training, which tends to make the neural network be biased toward learning\nthe specific patterns of the synthesized rain, while be less able to generalize\nto real test samples whose rain types differ from those in the training data.\nTo this issue, this paper firstly proposes a semi-supervised learning paradigm\ntoward this task. Different from traditional deep learning methods which only\nuse supervised image pairs with/without synthesized rain, we further put real\nrainy images, without need of their clean ones, into the network training\nprocess. This is realized by elaborately formulating the residual between an\ninput rainy image and its expected network output (clear image without rain) as\na specific parametrized rain streaks distribution. The network is therefore\ntrained to adapt real unsupervised diverse rain types through transferring from\nthe supervised synthesized rain, and thus both the short-of-training-sample and\nbias-to-supervised-sample issues can be evidently alleviated. Experiments on\nsynthetic and real data verify the superiority of our model compared to the\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 16:31:40 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 04:30:54 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Wei", "Wei", ""], ["Meng", "Deyu", ""], ["Zhao", "Qian", ""], ["Xu", "Zongben", ""], ["Wu", "Ying", ""]]}, {"id": "1807.11079", "submitter": "Wayne Wu", "authors": "Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, Chen Change Loy", "title": "ReenactGAN: Learning to Reenact Faces via Boundary Transfer", "comments": "Accepted to ECCV 2018. Project page:\n  https://wywu.github.io/projects/ReenactGAN/ReenactGAN.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel learning-based framework for face reenactment. The\nproposed method, known as ReenactGAN, is capable of transferring facial\nmovements and expressions from monocular video input of an arbitrary person to\na target person. Instead of performing a direct transfer in the pixel space,\nwhich could result in structural artifacts, we first map the source face onto a\nboundary latent space. A transformer is subsequently used to adapt the boundary\nof source face to the boundary of target face. Finally, a target-specific\ndecoder is used to generate the reenacted target face. Thanks to the effective\nand reliable boundary-based transfer, our method can perform photo-realistic\nface reenactment. In addition, ReenactGAN is appealing in that the whole\nreenactment process is purely feed-forward, and thus the reenactment process\ncan run in real-time (30 FPS on one GTX 1080 GPU). Dataset and model will be\npublicly available at\nhttps://wywu.github.io/projects/ReenactGAN/ReenactGAN.html\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 16:35:15 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Wu", "Wayne", ""], ["Zhang", "Yunxuan", ""], ["Li", "Cheng", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1807.11080", "submitter": "Sharib Ali Dr.", "authors": "Sharib Ali, Martin Schober, Philipp Schl\\\"ome, Katrin Amunts, Markus\n  Axer, Karl Rohr", "title": "Towards ultra-high resolution 3D reconstruction of a whole rat brain\n  from 3D-PLI data", "comments": "9 pages, Accepted at 2nd International Workshop on Connectomics in\n  NeuroImaging (CNI), MICCAI'2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D reconstruction of the fiber connectivity of the rat brain at microscopic\nscale enables gaining detailed insight about the complex structural\norganization of the brain. We introduce a new method for registration and 3D\nreconstruction of high- and ultra-high resolution (64 $\\mu$m and 1.3 $\\mu$m\npixel size) histological images of a Wistar rat brain acquired by 3D polarized\nlight imaging (3D-PLI). Our method exploits multi-scale and multi-modal 3D-PLI\ndata up to cellular resolution. We propose a new feature transform-based\nsimilarity measure and a weighted regularization scheme for accurate and robust\nnon-rigid registration. To transform the 1.3 $\\mu$m ultra-high resolution data\nto the reference blockface images a feature-based registration method followed\nby a non-rigid registration is proposed. Our approach has been successfully\napplied to 278 histological sections of a rat brain and the performance has\nbeen quantitatively evaluated using manually placed landmarks by an expert.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 16:35:18 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ali", "Sharib", ""], ["Schober", "Martin", ""], ["Schl\u00f6me", "Philipp", ""], ["Amunts", "Katrin", ""], ["Axer", "Markus", ""], ["Rohr", "Karl", ""]]}, {"id": "1807.11089", "submitter": "Pramit Saha", "authors": "Pramit Saha, Praneeth Srungarapu and Sidney Fels", "title": "Towards Automatic Speech Identification from Vocal Tract Shape Dynamics\n  in Real-time MRI", "comments": "To appear in the INTERSPEECH 2018 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.CV cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vocal tract configurations play a vital role in generating distinguishable\nspeech sounds, by modulating the airflow and creating different resonant\ncavities in speech production. They contain abundant information that can be\nutilized to better understand the underlying speech production mechanism. As a\nstep towards automatic mapping of vocal tract shape geometry to acoustics, this\npaper employs effective video action recognition techniques, like Long-term\nRecurrent Convolutional Networks (LRCN) models, to identify different\nvowel-consonant-vowel (VCV) sequences from dynamic shaping of the vocal tract.\nSuch a model typically combines a CNN based deep hierarchical visual feature\nextractor with Recurrent Networks, that ideally makes the network\nspatio-temporally deep enough to learn the sequential dynamics of a short video\nclip for video classification tasks. We use a database consisting of 2D\nreal-time MRI of vocal tract shaping during VCV utterances by 17 speakers. The\ncomparative performances of this class of algorithms under various parameter\nsettings and for various classification tasks are discussed. Interestingly, the\nresults show a marked difference in the model performance in the context of\nspeech classification with respect to generic sequence or video classification\ntasks.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 17:36:08 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Saha", "Pramit", ""], ["Srungarapu", "Praneeth", ""], ["Fels", "Sidney", ""]]}, {"id": "1807.11091", "submitter": "Tianyun Zhang", "authors": "Tianyun Zhang, Shaokai Ye, Kaiqi Zhang, Xiaolong Ma, Ning Liu, Linfeng\n  Zhang, Jian Tang, Kaisheng Ma, Xue Lin, Makan Fardad and Yanzhi Wang", "title": "StructADMM: A Systematic, High-Efficiency Framework of Structured Weight\n  Pruning for DNNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weight pruning methods of DNNs have been demonstrated to achieve a good model\npruning rate without loss of accuracy, thereby alleviating the significant\ncomputation/storage requirements of large-scale DNNs. Structured weight pruning\nmethods have been proposed to overcome the limitation of irregular network\nstructure and demonstrated actual GPU acceleration. However, in prior work the\npruning rate (degree of sparsity) and GPU acceleration are limited (to less\nthan 50%) when accuracy needs to be maintained. In this work,we overcome these\nlimitations by proposing a unified, systematic framework of structured weight\npruning for DNNs. It is a framework that can be used to induce different types\nof structured sparsity, such as filter-wise, channel-wise, and shape-wise\nsparsity, as well non-structured sparsity. The proposed framework incorporates\nstochastic gradient descent with ADMM, and can be understood as a dynamic\nregularization method in which the regularization target is analytically\nupdated in each iteration. Without loss of accuracy on the AlexNet model, we\nachieve 2.58X and 3.65X average measured speedup on two GPUs, clearly\noutperforming the prior work. The average speedups reach 3.15X and 8.52X when\nallowing a moderate ac-curacy loss of 2%. In this case the model compression\nfor convolutional layers is 15.0X, corresponding to 11.93X measured CPU\nspeedup. Our experiments on ResNet model and on other data sets like UCF101 and\nCIFAR-10 demonstrate the consistently higher performance of our framework.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 18:07:04 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 18:52:21 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 02:37:46 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhang", "Tianyun", ""], ["Ye", "Shaokai", ""], ["Zhang", "Kaiqi", ""], ["Ma", "Xiaolong", ""], ["Liu", "Ning", ""], ["Zhang", "Linfeng", ""], ["Tang", "Jian", ""], ["Ma", "Kaisheng", ""], ["Lin", "Xue", ""], ["Fardad", "Makan", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1807.11113", "submitter": "Nanqing Dong", "authors": "Nanqing Dong, Michael Kampffmeyer, Xiaodan Liang, Zeya Wang, Wei Dai,\n  Eric P. Xing", "title": "Reinforced Auto-Zoom Net: Towards Accurate and Fast Breast Cancer\n  Segmentation in Whole-slide Images", "comments": "Accepted by MICCAI 2018 Workshop on Deep Learning in Medical Image\n  Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional neural networks have led to significant breakthroughs in the\ndomain of medical image analysis. However, the task of breast cancer\nsegmentation in whole-slide images (WSIs) is still underexplored. WSIs are\nlarge histopathological images with extremely high resolution. Constrained by\nthe hardware and field of view, using high-magnification patches can slow down\nthe inference process and using low-magnification patches can cause the loss of\ninformation. In this paper, we aim to achieve two seemingly conflicting goals\nfor breast cancer segmentation: accurate and fast prediction. We propose a\nsimple yet efficient framework Reinforced Auto-Zoom Net (RAZN) to tackle this\ntask. Motivated by the zoom-in operation of a pathologist using a digital\nmicroscope, RAZN learns a policy network to decide whether zooming is required\nin a given region of interest. Because the zoom-in action is selective, RAZN is\nrobust to unbalanced and noisy ground truth labels and can efficiently reduce\noverfitting. We evaluate our method on a public breast cancer dataset. RAZN\noutperforms both single-scale and multi-scale baseline approaches, achieving\nbetter accuracy at low inference cost.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 21:45:35 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Dong", "Nanqing", ""], ["Kampffmeyer", "Michael", ""], ["Liang", "Xiaodan", ""], ["Wang", "Zeya", ""], ["Dai", "Wei", ""], ["Xing", "Eric P.", ""]]}, {"id": "1807.11122", "submitter": "Keren Ye", "authors": "Keren Ye, Kyle Buettner, Adriana Kovashka", "title": "Story Understanding in Video Advertisements", "comments": "To appear, Proceedings of the British Machine Vision Conference\n  (BMVC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to resonate with the viewers, many video advertisements explore\ncreative narrative techniques such as \"Freytag's pyramid\" where a story begins\nwith exposition, followed by rising action, then climax, concluding with\ndenouement. In the dramatic structure of ads in particular, climax depends on\nchanges in sentiment. We dedicate our study to understand the dynamic structure\nof video ads automatically. To achieve this, we first crowdsource climax\nannotations on 1,149 videos from the Video Ads Dataset, which already provides\nsentiment annotations. We then use both unsupervised and supervised methods to\npredict the climax. Based on the predicted peak, the low-level visual and audio\ncues, and semantically meaningful context features, we build a sentiment\nprediction model that outperforms the current state-of-the-art model of\nsentiment prediction in video ads by 25%. In our ablation study, we show that\nusing our context features, and modeling dynamics with an LSTM, are both\ncrucial factors for improved performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 23:15:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ye", "Keren", ""], ["Buettner", "Kyle", ""], ["Kovashka", "Adriana", ""]]}, {"id": "1807.11130", "submitter": "Xiaohan Fei", "authors": "Xiaohan Fei, Alex Wong, Stefano Soatto", "title": "Geo-Supervised Visual Depth Prediction", "comments": "ICRA 2019, RA-L 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using global orientation from inertial measurements, and the bias\nit induces on the shape of objects populating the scene, to inform visual 3D\nreconstruction. We test the effect of using the resulting prior in depth\nprediction from a single image, where the normal vectors to surfaces of objects\nof certain classes tend to align with gravity or be orthogonal to it. Adding\nsuch a prior to baseline methods for monocular depth prediction yields\nimprovements beyond the state-of-the-art and illustrates the power of gravity\nas a supervisory signal.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 00:31:42 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 21:55:53 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 22:49:09 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2019 20:49:52 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Fei", "Xiaohan", ""], ["Wong", "Alex", ""], ["Soatto", "Stefano", ""]]}, {"id": "1807.11147", "submitter": "Yuchao Dai Dr.", "authors": "Xiang Guo and Yuchao Dai", "title": "Occluded Joints Recovery in 3D Human Pose Estimation based on Distance\n  Matrix", "comments": "Accepted by ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit the recent progress in single image 3D human pose estimation due to\nthe convolutional neural network, it is still challenging to handle real\nscenarios such as highly occluded scenes. In this paper, we propose to address\nthe problem of single image 3D human pose estimation with occluded measurements\nby exploiting the Euclidean distance matrix (EDM). Specifically, we present two\napproaches based on EDM, which could effectively handle occluded joints in 2D\nimages. The first approach is based on 2D-to-2D distance matrix regression\nachieved by a simple CNN architecture. The second approach is based on sparse\ncoding along with a learned over-complete dictionary. Experiments on the\nHuman3.6M dataset show the excellent performance of these two approaches in\nrecovering occluded observations and demonstrate the improvements in accuracy\nfor 3D human pose estimation with occluded joints.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:32:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Guo", "Xiang", ""], ["Dai", "Yuchao", ""]]}, {"id": "1807.11152", "submitter": "Ceyuan Yang", "authors": "Ceyuan Yang, Zhe Wang, Xinge Zhu, Chen Huang, Jianping Shi, Dahua Lin", "title": "Pose Guided Human Video Generation", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the emergence of Generative Adversarial Networks, video synthesis has\nwitnessed exceptional breakthroughs. However, existing methods lack a proper\nrepresentation to explicitly control the dynamics in videos. Human pose, on the\nother hand, can represent motion patterns intrinsically and interpretably, and\nimpose the geometric constraints regardless of appearance. In this paper, we\npropose a pose guided method to synthesize human videos in a disentangled way:\nplausible motion prediction and coherent appearance generation. In the first\nstage, a Pose Sequence Generative Adversarial Network (PSGAN) learns in an\nadversarial manner to yield pose sequences conditioned on the class label. In\nthe second stage, a Semantic Consistent Generative Adversarial Network (SCGAN)\ngenerates video frames from the poses while preserving coherent appearances in\nthe input image. By enforcing semantic consistency between the generated and\nground-truth poses at a high feature level, our SCGAN is robust to noisy or\nabnormal poses. Extensive experiments on both human action and human face\ndatasets manifest the superiority of the proposed method over other\nstate-of-the-arts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:46:43 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yang", "Ceyuan", ""], ["Wang", "Zhe", ""], ["Zhu", "Xinge", ""], ["Huang", "Chen", ""], ["Shi", "Jianping", ""], ["Lin", "Dahua", ""]]}, {"id": "1807.11156", "submitter": "ShihChung Lo Ph.D.", "authors": "ShihChung B. Lo, Matthew T. Freedman, and Seong K. Mun", "title": "Transformationally Identical and Invariant Convolutional Neural Networks\n  by Combining Symmetric Operations or Input Vectors", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformationally invariant processors constructed by transformed input\nvectors or operators have been suggested and applied to many applications. In\nthis study, transformationally identical processing based on combining results\nof all sub-processes with corresponding transformations at one of the\nprocessing steps or at the beginning step were found to be equivalent for a\ngiven condition. This property can be applied to most convolutional neural\nnetwork (CNN) systems. Specifically, a transformationally identical CNN can be\nconstructed by arranging internally symmetric operations in parallel with the\nsame transformation family that includes a flatten layer with weights sharing\namong their corresponding transformation elements. Other transformationally\nidentical CNNs can be constructed by averaging transformed input vectors of the\nfamily at the input layer followed by an ordinary CNN process or by a set of\nsymmetric operations. Interestingly, we found that both types of\ntransformationally identical CNN systems are mathematically equivalent by\neither applying an averaging operation to corresponding elements of all\nsub-channels before the activation function or without using a non-linear\nactivation function.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 03:14:50 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 13:32:14 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2018 10:32:55 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Lo", "ShihChung B.", ""], ["Freedman", "Matthew T.", ""], ["Mun", "Seong K.", ""]]}, {"id": "1807.11158", "submitter": "Tianyu Guo", "authors": "Tianyu Guo, Chang Xu, Shiyi He, Boxin Shi, Chao Xu, and Dacheng Tao", "title": "Robust Student Network Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks bring in impressive accuracy in various applications,\nbut the success often relies on the heavy network architecture. Taking\nwell-trained heavy networks as teachers, classical teacher-student learning\nparadigm aims to learn a student network that is lightweight yet accurate. In\nthis way, a portable student network with significantly fewer parameters can\nachieve a considerable accuracy which is comparable to that of teacher network.\nHowever, beyond accuracy, robustness of the learned student network against\nperturbation is also essential for practical uses. Existing teacher-student\nlearning frameworks mainly focus on accuracy and compression ratios, but ignore\nthe robustness. In this paper, we make the student network produce more\nconfident predictions with the help of the teacher network, and analyze the\nlower bound of the perturbation that will destroy the confidence of the student\nnetwork. Two important objectives regarding prediction scores and gradients of\nexamples are developed to maximize this lower bound, so as to enhance the\nrobustness of the student network without sacrificing the performance.\nExperiments on benchmark datasets demonstrate the efficiency of the proposed\napproach to learn robust student networks which have satisfying accuracy and\ncompact sizes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 03:27:04 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 01:01:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Guo", "Tianyu", ""], ["Xu", "Chang", ""], ["He", "Shiyi", ""], ["Shi", "Boxin", ""], ["Xu", "Chao", ""], ["Tao", "Dacheng", ""]]}, {"id": "1807.11164", "submitter": "Xiangyu Zhang", "authors": "Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, Jian Sun", "title": "ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture\n  Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, the neural network architecture design is mostly guided by the\n\\emph{indirect} metric of computation complexity, i.e., FLOPs. However, the\n\\emph{direct} metric, e.g., speed, also depends on the other factors such as\nmemory access cost and platform characterics. Thus, this work proposes to\nevaluate the direct metric on the target platform, beyond only considering\nFLOPs. Based on a series of controlled experiments, this work derives several\npractical \\emph{guidelines} for efficient network design. Accordingly, a new\narchitecture is presented, called \\emph{ShuffleNet V2}. Comprehensive ablation\nexperiments verify that our model is the state-of-the-art in terms of speed and\naccuracy tradeoff.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 04:18:25 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ma", "Ningning", ""], ["Zhang", "Xiangyu", ""], ["Zheng", "Hai-Tao", ""], ["Sun", "Jian", ""]]}, {"id": "1807.11176", "submitter": "Huseyin Coskun", "authors": "Huseyin Coskun, David Joseph Tan, Sailesh Conjeti, Nassir Navab, and\n  Federico Tombari", "title": "Human Motion Analysis with Deep Metric Learning", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effectively measuring the similarity between two human motions is necessary\nfor several computer vision tasks such as gait analysis, person identi-\nfication and action retrieval. Nevertheless, we believe that traditional\napproaches such as L2 distance or Dynamic Time Warping based on hand-crafted\nlocal pose metrics fail to appropriately capture the semantic relationship\nacross motions and, as such, are not suitable for being employed as metrics\nwithin these tasks. This work addresses this limitation by means of a\ntriplet-based deep metric learning specifically tailored to deal with human\nmotion data, in particular with the prob- lem of varying input size and\ncomputationally expensive hard negative mining due to motion pair alignment.\nSpecifically, we propose (1) a novel metric learn- ing objective based on a\ntriplet architecture and Maximum Mean Discrepancy; as well as, (2) a novel deep\narchitecture based on attentive recurrent neural networks. One benefit of our\nobjective function is that it enforces a better separation within the learned\nembedding space of the different motion categories by means of the associated\ndistribution moments. At the same time, our attentive recurrent neural network\nallows processing varying input sizes to a fixed size of embedding while\nlearning to focus on those motion parts that are semantically distinctive. Our\nex- periments on two different datasets demonstrate significant improvements\nover conventional human motion metrics.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 05:12:09 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 00:13:42 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Coskun", "Huseyin", ""], ["Tan", "David Joseph", ""], ["Conjeti", "Sailesh", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1807.11178", "submitter": "Yantao Shen", "authors": "Yantao Shen, Hongsheng Li, Tong Xiao, Shuai Yi, Dapeng Chen, Xiaogang\n  Wang", "title": "Deep Group-shuffling Random Walk for Person Re-identification", "comments": "CVPR 2018 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims at finding a person of interest in an image\ngallery by comparing the probe image of this person with all the gallery\nimages. It is generally treated as a retrieval problem, where the affinities\nbetween the probe image and gallery images (P2G affinities) are used to rank\nthe retrieved gallery images. However, most existing methods only consider P2G\naffinities but ignore the affinities between all the gallery images (G2G\naffinity). Some frameworks incorporated G2G affinities into the testing\nprocess, which is not end-to-end trainable for deep neural networks. In this\npaper, we propose a novel group-shuffling random walk network for fully\nutilizing the affinity information between gallery images in both the training\nand testing processes. The proposed approach aims at end-to-end refining the\nP2G affinities based on G2G affinity information with a simple yet effective\nmatrix operation, which can be integrated into deep neural networks. Feature\ngrouping and group shuffle are also proposed to apply rich supervisions for\nlearning better person features. The proposed approach outperforms\nstate-of-the-art methods on the Market-1501, CUHK03, and DukeMTMC datasets by\nlarge margins, which demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 05:35:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Shen", "Yantao", ""], ["Li", "Hongsheng", ""], ["Xiao", "Tong", ""], ["Yi", "Shuai", ""], ["Chen", "Dapeng", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1807.11182", "submitter": "Yantao Shen", "authors": "Yantao Shen, Tong Xiao, Hongsheng Li, Shuai Yi, Xiaogang Wang", "title": "End-to-End Deep Kronecker-Product Matching for Person Re-identification", "comments": "CVPR 2018 poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification aims to robustly measure similarities between person\nimages. The significant variation of person poses and viewing angles challenges\nfor accurate person re-identification. The spatial layout and correspondences\nbetween query person images are vital information for tackling this problem but\nare ignored by most state-of-the-art methods. In this paper, we propose a novel\nKronecker Product Matching module to match feature maps of different persons in\nan end-to-end trainable deep neural network. A novel feature soft warping\nscheme is designed for aligning the feature maps based on matching results,\nwhich is shown to be crucial for achieving superior accuracy. The multi-scale\nfeatures based on hourglass-like networks and self-residual attention are also\nexploited to further boost the re-identification performance. The proposed\napproach outperforms state-of-the-art methods on the Market-1501, CUHK03, and\nDukeMTMC datasets, which demonstrates the effectiveness and generalization\nability of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 05:56:47 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Shen", "Yantao", ""], ["Xiao", "Tong", ""], ["Li", "Hongsheng", ""], ["Yi", "Shuai", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1807.11195", "submitter": "Yunpeng Chen", "authors": "Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, Jiashi\n  Feng", "title": "Multi-Fiber Networks for Video Recognition", "comments": "ECCV 2018, Code is on GitHub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to reduce the computational cost of spatio-temporal\ndeep neural networks, making them run as fast as their 2D counterparts while\npreserving state-of-the-art accuracy on video recognition benchmarks. To this\nend, we present the novel Multi-Fiber architecture that slices a complex neural\nnetwork into an ensemble of lightweight networks or fibers that run through the\nnetwork. To facilitate information flow between fibers we further incorporate\nmultiplexer modules and end up with an architecture that reduces the\ncomputational cost of 3D networks by an order of magnitude, while increasing\nrecognition performance at the same time. Extensive experimental results show\nthat our multi-fiber architecture significantly boosts the efficiency of\nexisting convolution networks for both image and video recognition tasks,\nachieving state-of-the-art performance on UCF-101, HMDB-51 and Kinetics\ndatasets. Our proposed model requires over 9x and 13x less computations than\nthe I3D and R(2+1)D models, respectively, yet providing higher accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 07:08:29 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 18:40:43 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 07:17:44 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Yunpeng", ""], ["Kalantidis", "Yannis", ""], ["Li", "Jianshu", ""], ["Yan", "Shuicheng", ""], ["Feng", "Jiashi", ""]]}, {"id": "1807.11206", "submitter": "Rui Yu", "authors": "Rui Yu, Zhiyong Dou, Song Bai, Zhaoxiang Zhang, Yongchao Xu, Xiang Bai", "title": "Hard-Aware Point-to-Set Deep Metric for Person Re-identification", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification (re-ID) is a highly challenging task due to large\nvariations of pose, viewpoint, illumination, and occlusion. Deep metric\nlearning provides a satisfactory solution to person re-ID by training a deep\nnetwork under supervision of metric loss, e.g., triplet loss. However, the\nperformance of deep metric learning is greatly limited by traditional sampling\nmethods. To solve this problem, we propose a Hard-Aware Point-to-Set (HAP2S)\nloss with a soft hard-mining scheme. Based on the point-to-set triplet loss\nframework, the HAP2S loss adaptively assigns greater weights to harder samples.\nSeveral advantageous properties are observed when compared with other\nstate-of-the-art loss functions: 1) Accuracy: HAP2S loss consistently achieves\nhigher re-ID accuracies than other alternatives on three large-scale benchmark\ndatasets; 2) Robustness: HAP2S loss is more robust to outliers than other\nlosses; 3) Flexibility: HAP2S loss does not rely on a specific weight function,\ni.e., different instantiations of HAP2S loss are equally effective. 4)\nGenerality: In addition to person re-ID, we apply the proposed method to\ngeneric deep metric learning benchmarks including CUB-200-2011 and Cars196, and\nalso achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 07:41:34 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Yu", "Rui", ""], ["Dou", "Zhiyong", ""], ["Bai", "Song", ""], ["Zhang", "Zhaoxiang", ""], ["Xu", "Yongchao", ""], ["Bai", "Xiang", ""]]}, {"id": "1807.11212", "submitter": "Julien Tierny", "authors": "Guillaume Favelier, Noura Faraj, Brian Summa, Julien Tierny", "title": "Persistence Atlas for Critical Point Variability in Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": "2344815-v2", "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach for the visualization and analysis of the\nspatial variability of features of interest represented by critical points in\nensemble data. Our framework, called Persistence Atlas, enables the\nvisualization of the dominant spatial patterns of critical points, along with\nstatistics regarding their occurrence in the ensemble. The persistence atlas\nrepresents in the geometrical domain each dominant pattern in the form of a\nconfidence map for the appearance of critical points. As a by-product, our\nmethod also provides 2-dimensional layouts of the entire ensemble, highlighting\nthe main trends at a global level. Our approach is based on the new notion of\nPersistence Map, a measure of the geometrical density in critical points which\nleverages the robustness to noise of topological persistence to better\nemphasize salient features. We show how to leverage spectral embedding to\nrepresent the ensemble members as points in a low-dimensional Euclidean space,\nwhere distances between points measure the dissimilarities between critical\npoint layouts and where statistical tasks, such as clustering, can be easily\ncarried out. Further, we show how the notion of mandatory critical point can be\nleveraged to evaluate for each cluster confidence regions for the appearance of\ncritical points. Most of the steps of this framework can be trivially\nparallelized and we show how to efficiently implement them. Extensive\nexperiments demonstrate the relevance of our approach. The accuracy of the\nconfidence regions provided by the persistence atlas is quantitatively\nevaluated and compared to a baseline strategy using an off-the-shelf clustering\napproach. We illustrate the importance of the persistence atlas in a variety of\nreal-life datasets, where clear trends in feature layouts are identified and\nanalyzed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 07:46:27 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Favelier", "Guillaume", ""], ["Faraj", "Noura", ""], ["Summa", "Brian", ""], ["Tierny", "Julien", ""]]}, {"id": "1807.11215", "submitter": "Corentin Kervadec", "authors": "Corentin Kervadec, Valentin Vielzeuf, St\\'ephane Pateux, Alexis\n  Lechervy, Fr\\'ed\\'eric Jurie", "title": "CAKE: Compact and Accurate K-dimensional representation of Emotion", "comments": null, "journal-ref": "Image Analysis for Human Facial and Activity Recognition (BMVC\n  Workshop), Sep 2018, Newcastle, United Kingdom.\n  http://juz-dev.myweb.port.ac.uk/BMVCWorkshop/index.html", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous models describing the human emotional states have been built by the\npsychology community. Alongside, Deep Neural Networks (DNN) are reaching\nexcellent performances and are becoming interesting features extraction tools\nin many computer vision tasks.Inspired by works from the psychology community,\nwe first study the link between the compact two-dimensional representation of\nthe emotion known as arousal-valence, and discrete emotion classes (e.g. anger,\nhappiness, sadness, etc.) used in the computer vision community. It enables to\nassess the benefits -- in terms of discrete emotion inference -- of adding an\nextra dimension to arousal-valence (usually named dominance). Building on these\nobservations, we propose CAKE, a 3-dimensional representation of emotion\nlearned in a multi-domain fashion, achieving accurate emotion recognition on\nseveral public datasets. Moreover, we visualize how emotions boundaries are\norganized inside DNN representations and show that DNNs are implicitly learning\narousal-valence-like descriptions of emotions. Finally, we use the CAKE\nrepresentation to compare the quality of the annotations of different public\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:03:09 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 08:07:32 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Kervadec", "Corentin", ""], ["Vielzeuf", "Valentin", ""], ["Pateux", "St\u00e9phane", ""], ["Lechervy", "Alexis", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1807.11226", "submitter": "Sai Bi", "authors": "Sai Bi, Nima Khademi Kalantari, Ravi Ramamoorthi", "title": "Deep Hybrid Real and Synthetic Training for Intrinsic Decomposition", "comments": "Accepted to EGSR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic image decomposition is the process of separating the reflectance\nand shading layers of an image, which is a challenging and underdetermined\nproblem. In this paper, we propose to systematically address this problem using\na deep convolutional neural network (CNN). Although deep learning (DL) has been\nrecently used to handle this application, the current DL methods train the\nnetwork only on synthetic images as obtaining ground truth reflectance and\nshading for real images is difficult. Therefore, these methods fail to produce\nreasonable results on real images and often perform worse than the non-DL\ntechniques. We overcome this limitation by proposing a novel hybrid approach to\ntrain our network on both synthetic and real images. Specifically, in addition\nto directly supervising the network using synthetic images, we train the\nnetwork by enforcing it to produce the same reflectance for a pair of images of\nthe same real-world scene with different illuminations. Furthermore, we improve\nthe results by incorporating a bilateral solver layer into our system during\nboth training and test stages. Experimental results show that our approach\nproduces better results than the state-of-the-art DL and non-DL methods on\nvarious synthetic and real datasets both visually and numerically.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:25:54 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Bi", "Sai", ""], ["Kalantari", "Nima Khademi", ""], ["Ramamoorthi", "Ravi", ""]]}, {"id": "1807.11228", "submitter": "Mikhail Belyaev", "authors": "Yaroslav Shmulev and Mikhail Belyaev", "title": "Predicting Conversion of Mild Cognitive Impairments to Alzheimer's\n  Disease and Exploring Impact of Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, a lot of scientific efforts are concentrated on the diagnosis of\nAlzheimer's Disease (AD) applying deep learning methods to neuroimaging data.\nEven for 2017, there were published more than a hundred papers dedicated to AD\ndiagnosis, whereas only a few works considered a problem of mild cognitive\nimpairments (MCI) conversion to the AD. However, the conversion prediction is\nan important problem since approximately 15% of patients with MCI converges to\nthe AD every year. In the current work, we are focusing on the conversion\nprediction using brain Magnetic Resonance Imaging and clinical data, such as\ndemographics, cognitive assessments, genetic, and biochemical markers. First of\nall, we applied state-of-the-art deep learning algorithms on the neuroimaging\ndata and compared these results with two machine learning algorithms that we\nfit using the clinical data. As a result, the models trained on the clinical\ndata outperform the deep learning algorithms applied to the MR images. To\nexplore the impact of neuroimaging further, we trained a deep feed-forward\nembedding using similarity learning with Histogram loss on all available MRIs\nand obtained 64-dimensional vector representation of neuroimaging data. The use\nof learned representation from the deep embedding allowed to increase the\nquality of prediction based on the neuroimaging. Finally, the current results\non this dataset show that the neuroimaging does affect conversion prediction,\nhowever, cannot noticeably increase the quality of the prediction. The best\nresults of predicting MCI-to-AD conversion are provided by XGBoost algorithm\ntrained on the clinical and embedding data. The resulting accuracy is 0.76 +-\n0.01 and the area under the ROC curve - 0.86 +- 0.01.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:39:47 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Shmulev", "Yaroslav", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1807.11234", "submitter": "Jeffrey Ede BSc MPhys", "authors": "Jeffrey M. Ede", "title": "Improving Electron Micrograph Signal-to-Noise with an Atrous\n  Convolutional Encoder-Decoder", "comments": "15 pages, 10 figures, 1 table", "journal-ref": "Ultramicroscopy 202 (2019), pp.18-25", "doi": "10.1016/j.ultramic.2019.03.017", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an atrous convolutional encoder-decoder trained to denoise\n512$\\times$512 crops from electron micrographs. It consists of a modified\nXception backbone, atrous convoltional spatial pyramid pooling module and a\nmulti-stage decoder. Our neural network was trained end-to-end to remove\nPoisson noise applied to low-dose ($\\ll$ 300 counts ppx) micrographs created\nfrom a new dataset of 17267 2048$\\times$2048 high-dose ($>$ 2500 counts ppx)\nmicrographs and then fine-tuned for ordinary doses (200-2500 counts ppx). Its\nperformance is benchmarked against bilateral, non-local means, total variation,\nwavelet, Wiener and other restoration methods with their default parameters.\nOur network outperforms their best mean squared error and structural similarity\nindex performances by 24.6% and 9.6% for low doses and by 43.7% and 5.5% for\nordinary doses. In both cases, our network's mean squared error has the lowest\nvariance. Source code and links to our new high-quality dataset and trained\nnetwork have been made publicly available at\nhttps://github.com/Jeffrey-Ede/Electron-Micrograph-Denoiser\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:48:32 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 10:37:59 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ede", "Jeffrey M.", ""]]}, {"id": "1807.11236", "submitter": "Liu Yongcheng", "authors": "Yongcheng Liu, Bin Fan, Lingfeng Wang, Jun Bai, Shiming Xiang,\n  Chunhong Pan", "title": "Semantic Labeling in Very High Resolution Images via a Self-Cascaded\n  Convolutional Neural Network", "comments": "accepted by ISPRS Journal of Photogrammetry and Remote Senseing 2017", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2017.12.007", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic labeling for very high resolution (VHR) images in urban areas, is of\nsignificant importance in a wide range of remote sensing applications. However,\nmany confusing manmade objects and intricate fine-structured objects make it\nvery difficult to obtain both coherent and accurate labeling results. For this\nchallenging task, we propose a novel deep model with convolutional neural\nnetworks (CNNs), i.e., an end-to-end self-cascaded network (ScasNet).\nSpecifically, for confusing manmade objects, ScasNet improves the labeling\ncoherence with sequential global-to-local contexts aggregation. Technically,\nmulti-scale contexts are captured on the output of a CNN encoder, and then they\nare successively aggregated in a self-cascaded manner. Meanwhile, for\nfine-structured objects, ScasNet boosts the labeling accuracy with a\ncoarse-to-fine refinement strategy. It progressively refines the target objects\nusing the low-level features learned by CNN's shallow layers. In addition, to\ncorrect the latent fitting residual caused by multi-feature fusion inside\nScasNet, a dedicated residual correction scheme is proposed. It greatly\nimproves the effectiveness of ScasNet. Extensive experimental results on three\npublic datasets, including two challenging benchmarks, show that ScasNet\nachieves the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:49:25 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Liu", "Yongcheng", ""], ["Fan", "Bin", ""], ["Wang", "Lingfeng", ""], ["Bai", "Jun", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""]]}, {"id": "1807.11245", "submitter": "Yuansheng Hua", "authors": "Yuansheng Hua, Lichao Mou, Xiao Xiang Zhu", "title": "Recurrently Exploring Class-wise Attention in A Hybrid Convolutional and\n  Bidirectional LSTM Network for Multi-label Aerial Image Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.isprsjprs.2019.01.015", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial image classification is of great significance in remote sensing\ncommunity, and many researches have been conducted over the past few years.\nAmong these studies, most of them focus on categorizing an image into one\nsemantic label, while in the real world, an aerial image is often associated\nwith multiple labels, e.g., multiple object-level labels in our case. Besides,\na comprehensive picture of present objects in a given high resolution aerial\nimage can provide more in-depth understanding of the studied region. For these\nreasons, aerial image multi-label classification has been attracting increasing\nattention. However, one common limitation shared by existing methods in the\ncommunity is that the co-occurrence relationship of various classes, so called\nclass dependency, is underexplored and leads to an inconsiderate decision. In\nthis paper, we propose a novel end-to-end network, namely class-wise\nattention-based convolutional and bidirectional LSTM network (CA-Conv-BiLSTM),\nfor this task. The proposed network consists of three indispensable components:\n1) a feature extraction module, 2) a class attention learning layer, and 3) a\nbidirectional LSTM-based sub-network. Particularly, the feature extraction\nmodule is designed for extracting fine-grained semantic feature maps, while the\nclass attention learning layer aims at capturing discriminative class-specific\nfeatures. As the most important part, the bidirectional LSTM-based sub-network\nmodels the underlying class dependency in both directions and produce\nstructured multiple object labels. Experimental results on UCM multi-label\ndataset and DFC15 multi-label dataset validate the effectiveness of our model\nquantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 09:14:15 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 16:08:31 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Hua", "Yuansheng", ""], ["Mou", "Lichao", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "1807.11249", "submitter": "Hermann Blum", "authors": "Hermann Blum, Abel Gawel, Roland Siegwart, Cesar Cadena", "title": "Modular Sensor Fusion for Semantic Segmentation", "comments": "preprint of a paper presented at the IEEE International Conference on\n  Intelligent Robots and Systems 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor fusion is a fundamental process in robotic systems as it extends the\nperceptual range and increases robustness in real-world operations. Current\nmulti-sensor deep learning based semantic segmentation approaches do not\nprovide robustness to under-performing classes in one modality, or require a\nspecific architecture with access to the full aligned multi-sensor training\ndata. In this work, we analyze statistical fusion approaches for semantic\nsegmentation that overcome these drawbacks while keeping a competitive\nperformance. The studied approaches are modular by construction, allowing to\nhave different training sets per modality and only a much smaller subset is\nneeded to calibrate the statistical models. We evaluate a range of statistical\nfusion approaches and report their performance against state-of-the-art\nbaselines on both real-world and simulated data. In our experiments, the\napproach improves performance in IoU over the best single modality segmentation\nresults by up to 5%. We make all implementations and configurations publicly\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 09:27:14 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Blum", "Hermann", ""], ["Gawel", "Abel", ""], ["Siegwart", "Roland", ""], ["Cadena", "Cesar", ""]]}, {"id": "1807.11254", "submitter": "Bo Peng", "authors": "Bo Peng, Wenming Tan, Zheyang Li, Shun Zhang, Di Xie, Shiliang Pu", "title": "Extreme Network Compression via Filter Group Approximation", "comments": "Accepted by ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel decomposition method based on filter group\napproximation, which can significantly reduce the redundancy of deep\nconvolutional neural networks (CNNs) while maintaining the majority of feature\nrepresentation. Unlike other low-rank decomposition algorithms which operate on\nspatial or channel dimension of filters, our proposed method mainly focuses on\nexploiting the filter group structure for each layer. For several commonly used\nCNN models, including VGG and ResNet, our method can reduce over 80%\nfloating-point operations (FLOPs) with less accuracy drop than state-of-the-art\nmethods on various image classification datasets. Besides, experiments\ndemonstrate that our method is conducive to alleviating degeneracy of the\ncompressed network, which hurts the convergence and performance of the network.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 09:42:05 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 05:03:55 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Peng", "Bo", ""], ["Tan", "Wenming", ""], ["Li", "Zheyang", ""], ["Zhang", "Shun", ""], ["Xie", "Di", ""], ["Pu", "Shiliang", ""]]}, {"id": "1807.11272", "submitter": "Katar\\'ina T\\'othov\\'a", "authors": "Katar\\'ina T\\'othov\\'a, Sarah Parisot, Matthew C. H. Lee, Esther\n  Puyol-Ant\\'on, Lisa M. Koch, Andrew P. King, Ender Konukoglu, and Marc\n  Pollefeys", "title": "Uncertainty Quantification in CNN-Based Surface Prediction Using Shape\n  Priors", "comments": "Accepted to ShapeMI MICCAI 2018: Workshop on Shape in Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Surface reconstruction is a vital tool in a wide range of areas of medical\nimage analysis and clinical research. Despite the fact that many methods have\nproposed solutions to the reconstruction problem, most, due to their\ndeterministic nature, do not directly address the issue of quantifying\nuncertainty associated with their predictions. We remedy this by proposing a\nnovel probabilistic deep learning approach capable of simultaneous surface\nreconstruction and associated uncertainty prediction. The method incorporates\nprior shape information in the form of a principal component analysis (PCA)\nmodel. Experiments using the UK Biobank data show that our probabilistic\napproach outperforms an analogous deterministic PCA-based method in the task of\n2D organ delineation and quantifies uncertainty by formulating distributions\nover predicted surface vertex positions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 10:24:26 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["T\u00f3thov\u00e1", "Katar\u00edna", ""], ["Parisot", "Sarah", ""], ["Lee", "Matthew C. H.", ""], ["Puyol-Ant\u00f3n", "Esther", ""], ["Koch", "Lisa M.", ""], ["King", "Andrew P.", ""], ["Konukoglu", "Ender", ""], ["Pollefeys", "Marc", ""]]}, {"id": "1807.11279", "submitter": "Evgeniy Martyushev", "authors": "Evgeniy Martyushev", "title": "Self-Calibration of Cameras with Euclidean Image Plane in Case of Two\n  Views and Known Relative Rotation Angle", "comments": "13 pages, 7 eps-figures", "journal-ref": "ECCV 2018. Lecture Notes in Computer Science, vol 11208. Springer", "doi": "10.1007/978-3-030-01225-0_26", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal calibration of a pinhole camera is given by five parameters that\nare combined into an upper-triangular $3\\times 3$ calibration matrix. If the\nskew parameter is zero and the aspect ratio is equal to one, then the camera is\nsaid to have Euclidean image plane. In this paper, we propose a non-iterative\nself-calibration algorithm for a camera with Euclidean image plane in case the\nremaining three internal parameters --- the focal length and the principal\npoint coordinates --- are fixed but unknown. The algorithm requires a set of $N\n\\geq 7$ point correspondences in two views and also the measured relative\nrotation angle between the views. We show that the problem generically has six\nsolutions (including complex ones).\n  The algorithm has been implemented and tested both on synthetic data and on\npublicly available real dataset. The experiments demonstrate that the method is\ncorrect, numerically stable and robust.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 10:48:56 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Martyushev", "Evgeniy", ""]]}, {"id": "1807.11293", "submitter": "Uta B\\\"uchler", "authors": "Uta B\\\"uchler, Biagio Brattoli, Bj\\\"orn Ommer", "title": "Improving Spatiotemporal Self-Supervision by Deep Reinforcement Learning", "comments": "Accepted for publication at ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning of convolutional neural networks can harness large\namounts of cheap unlabeled data to train powerful feature representations. As\nsurrogate task, we jointly address ordering of visual data in the spatial and\ntemporal domain. The permutations of training samples, which are at the core of\nself-supervision by ordering, have so far been sampled randomly from a fixed\npreselected set. Based on deep reinforcement learning we propose a sampling\npolicy that adapts to the state of the network, which is being trained.\nTherefore, new permutations are sampled according to their expected utility for\nupdating the convolutional feature representation. Experimental evaluation on\nunsupervised and transfer learning tasks demonstrates competitive performance\non standard benchmarks for image and video classification and nearest neighbor\nretrieval.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 11:26:49 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["B\u00fcchler", "Uta", ""], ["Brattoli", "Biagio", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "1807.11332", "submitter": "Gurkirt Singh", "authors": "Valentina Fontana, Gurkirt Singh, Stephen Akrigg, Manuele Di Maio,\n  Suman Saha, Fabio Cuzzolin", "title": "Action Detection from a Robot-Car Perspective", "comments": "intial version, more to come - soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the new Road Event and Activity Detection (READ) dataset, designed\nand created from an autonomous vehicle perspective to take action detection\nchallenges to autonomous driving. READ will give scholars in computer vision,\nsmart cars and machine learning at large the opportunity to conduct research\ninto exciting new problems such as understanding complex (road) activities,\ndiscerning the behaviour of sentient agents, and predicting both the label and\nthe location of future actions and events, with the final goal of supporting\nautonomous decision making.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 13:11:21 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Fontana", "Valentina", ""], ["Singh", "Gurkirt", ""], ["Akrigg", "Stephen", ""], ["Di Maio", "Manuele", ""], ["Saha", "Suman", ""], ["Cuzzolin", "Fabio", ""]]}, {"id": "1807.11334", "submitter": "Liangchen Song", "authors": "Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang\n  Huang and Xinggang Wang", "title": "Unsupervised Domain Adaptive Re-Identification: Theory and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of unsupervised domain adaptive re-identification\n(re-ID) which is an active topic in computer vision but lacks a theoretical\nfoundation. We first extend existing unsupervised domain adaptive\nclassification theories to re-ID tasks. Concretely, we introduce some\nassumptions on the extracted feature space and then derive several loss\nfunctions guided by these assumptions. To optimize them, a novel self-training\nscheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively\nmakes guesses for unlabeled target data based on an encoder and trains the\nencoder based on the guessed labels. Extensive experiments on unsupervised\ndomain adaptive person re-ID and vehicle re-ID tasks with comparisons to the\nstate-of-the-arts confirm the effectiveness of the proposed theories and\nself-training framework. Our code is available at\n\\url{https://github.com/LcDog/DomainAdaptiveReID}.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 13:18:31 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Song", "Liangchen", ""], ["Wang", "Cheng", ""], ["Zhang", "Lefei", ""], ["Du", "Bo", ""], ["Zhang", "Qian", ""], ["Huang", "Chang", ""], ["Wang", "Xinggang", ""]]}, {"id": "1807.11348", "submitter": "Zhenhua Feng", "authors": "Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler", "title": "Learning Adaptive Discriminative Correlation Filters via Temporal\n  Consistency Preserving Spatial Feature Selection for Robust Visual Tracking", "comments": null, "journal-ref": "IEEE Transactions on Image Processing, 2019", "doi": "10.1109/TIP.2019.2919201", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With efficient appearance learning models, Discriminative Correlation Filter\n(DCF) has been proven to be very successful in recent video object tracking\nbenchmarks and competitions. However, the existing DCF paradigm suffers from\ntwo major issues, i.e., spatial boundary effect and temporal filter\ndegradation. To mitigate these challenges, we propose a new DCF-based tracking\nmethod. The key innovations of the proposed method include adaptive spatial\nfeature selection and temporal consistent constraints, with which the new\ntracker enables joint spatial-temporal filter learning in a lower dimensional\ndiscriminative manifold. More specifically, we apply structured spatial\nsparsity constraints to multi-channel filers. Consequently, the process of\nlearning spatial filters can be approximated by the lasso regularisation. To\nencourage temporal consistency, the filter model is restricted to lie around\nits historical value and updated locally to preserve the global structure in\nthe manifold. Last, a unified optimisation framework is proposed to jointly\nselect temporal consistency preserving spatial features and learn\ndiscriminative filters with the augmented Lagrangian method. Qualitative and\nquantitative evaluations have been conducted on a number of well-known\nbenchmarking datasets such as OTB2013, OTB50, OTB100, Temple-Colour, UAV123 and\nVOT2018. The experimental results demonstrate the superiority of the proposed\nmethod over the state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 13:46:46 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 16:00:22 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 16:49:16 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Xu", "Tianyang", ""], ["Feng", "Zhen-Hua", ""], ["Wu", "Xiao-Jun", ""], ["Kittler", "Josef", ""]]}, {"id": "1807.11368", "submitter": "Ben Glocker", "authors": "Vanya V. Valindria, Ioannis Lavdas, Juan Cerrolaza, Eric O. Aboagye,\n  Andrea G. Rockall, Daniel Rueckert, Ben Glocker", "title": "Small Organ Segmentation in Whole-body MRI using a Two-stage FCN and\n  Weighting Schemes", "comments": "Accepted at the MICCAI Workshop on Machine Learning in Medical\n  Imaging (MLMI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and robust segmentation of small organs in whole-body MRI is\ndifficult due to anatomical variation and class imbalance. Recent deep network\nbased approaches have demonstrated promising performance on abdominal\nmulti-organ segmentations. However, the performance on small organs is still\nsuboptimal as these occupy only small regions of the whole-body volumes with\nunclear boundaries and variable shapes. A coarse-to-fine, hierarchical strategy\nis a common approach to alleviate this problem, however, this might miss useful\ncontextual information. We propose a two-stage approach with weighting schemes\nbased on auto-context and spatial atlas priors. Our experiments show that the\nproposed approach can boost the segmentation accuracy of multiple small organs\nin whole-body MRI scans.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 14:35:02 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Valindria", "Vanya V.", ""], ["Lavdas", "Ioannis", ""], ["Cerrolaza", "Juan", ""], ["Aboagye", "Eric O.", ""], ["Rockall", "Andrea G.", ""], ["Rueckert", "Daniel", ""], ["Glocker", "Ben", ""]]}, {"id": "1807.11389", "submitter": "Shuhang Gu", "authors": "Shuhang Gu, Radu Timofte, Luc Van Gool", "title": "Multi-bin Trainable Linear Unit for Fast Image Restoration Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous advances in image restoration tasks such as denoising and\nsuper-resolution have been achieved using neural networks. Such approaches\ngenerally employ very deep architectures, large number of parameters, large\nreceptive fields and high nonlinear modeling capacity. In order to obtain\nefficient and fast image restoration networks one should improve upon the above\nmentioned requirements.\n  In this paper we propose a novel activation function, the multi-bin trainable\nlinear unit (MTLU), for increasing the nonlinear modeling capacity together\nwith lighter and shallower networks. We validate the proposed fast image\nrestoration networks for image denoising (FDnet) and super-resolution (FSRnet)\non standard benchmarks. We achieve large improvements in both memory and\nruntime over current state-of-the-art for comparable or better PSNR accuracies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 15:11:47 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Gu", "Shuhang", ""], ["Timofte", "Radu", ""], ["Van Gool", "Luc", ""]]}, {"id": "1807.11411", "submitter": "Asli Genctav", "authors": "Asli Genctav and Sibel Tari", "title": "A Non-structural Representation Scheme for Articulated Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For representing articulated shapes, as an alternative to the structured\nmodels based on graphs representing part hierarchy, we propose a pixel-based\ndistinctness measure. Its spatial distribution yields a partitioning of the\nshape into a set of regions each of which is represented via size normalized\nprobability distribution of the distinctness. Without imposing any structural\nrelation among parts, pairwise shape similarity is formulated as the cost of an\noptimal assignment between respective regions. The matching is performed via\nHungarian algorithm permitting some unmatched regions. The proposed similarity\nmeasure is employed in the context of clustering a set of shapes. The\nclustering results obtained on three articulated shape datasets show that our\nmethod performs comparable to state of the art methods utilizing component\ngraphs or trees even though we are not explicitly modeling component relations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:04:49 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Genctav", "Asli", ""], ["Tari", "Sibel", ""]]}, {"id": "1807.11433", "submitter": "Vivek Kumar Singh", "authors": "Vivek Kumar Singh, Hatem A. Rashwan, Adel Saleh, Farhan Akram, Md\n  Mostafa Kamal Sarker, Nidhi Pandey, Saddam Abdulwahab", "title": "REFUGE CHALLENGE 2018-Task 2:Deep Optic Disc and Cup Segmentation in\n  Fundus Images Using U-Net and Multi-scale Feature Matching Networks", "comments": "EYE REFUGE CHALLENGE 2018, submitted 7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an optic disc and cup segmentation method is proposed using\nU-Net followed by a multi-scale feature matching network. The proposed method\ntargets task 2 of the REFUGE challenge 2018. In order to solve the segmentation\nproblem of task 2, we firstly crop the input image using single shot multibox\ndetector (SSD). The cropped image is then passed to an encoder-decoder network\nwith skip connections also known as generator. Afterwards, both the ground\ntruth and generated images are fed to a convolution neural network (CNN) to\nextract their multi-level features. A dice loss function is then used to match\nthe features of the two images by minimizing the error at each layer. The\naggregation of error from each layer is back-propagated through the generator\nnetwork to enforce it to generate a segmented image closer to the ground truth.\nThe CNN network improves the performance of the generator network without\nincreasing the complexity of the model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:43:25 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Singh", "Vivek Kumar", ""], ["Rashwan", "Hatem A.", ""], ["Saleh", "Adel", ""], ["Akram", "Farhan", ""], ["Sarker", "Md Mostafa Kamal", ""], ["Pandey", "Nidhi", ""], ["Abdulwahab", "Saddam", ""]]}, {"id": "1807.11436", "submitter": "Yu-Ting Chen", "authors": "Yu-Ting Chen, Wen-Yen Chang, Hai-Lun Lu, Tingfan Wu, Min Sun", "title": "Leveraging Motion Priors in Videos for Improving Human Segmentation", "comments": "ECCV2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite many advances in deep-learning based semantic segmentation,\nperformance drop due to distribution mismatch is often encountered in the real\nworld. Recently, a few domain adaptation and active learning approaches have\nbeen proposed to mitigate the performance drop. However, very little attention\nhas been made toward leveraging information in videos which are naturally\ncaptured in most camera systems. In this work, we propose to leverage \"motion\nprior\" in videos for improving human segmentation in a weakly-supervised active\nlearning setting. By extracting motion information using optical flow in\nvideos, we can extract candidate foreground motion segments (referred to as\nmotion prior) potentially corresponding to human segments. We propose to learn\na memory-network-based policy model to select strong candidate segments\n(referred to as strong motion prior) through reinforcement learning. The\nselected segments have high precision and are directly used to finetune the\nmodel. In a newly collected surveillance camera dataset and a publicly\navailable UrbanStreet dataset, our proposed method improves the performance of\nhuman segmentation across multiple scenes and modalities (i.e., RGB to Infrared\n(IR)). Last but not least, our method is empirically complementary to existing\ndomain adaptation approaches such that additional performance gain is achieved\nby combining our weakly-supervised active learning approach with domain\nadaptation approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:52:04 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Chen", "Yu-Ting", ""], ["Chang", "Wen-Yen", ""], ["Lu", "Hai-Lun", ""], ["Wu", "Tingfan", ""], ["Sun", "Min", ""]]}, {"id": "1807.11440", "submitter": "Weidi Xie", "authors": "Weidi Xie, Li Shen and Andrew Zisserman", "title": "Comparator Networks", "comments": "To appear in ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is set-based verification, e.g. to decide if two\nsets of images of a face are of the same person or not. The traditional\napproach to this problem is to learn to generate a feature vector per image,\naggregate them into one vector to represent the set, and then compute the\ncosine similarity between sets. Instead, we design a neural network\narchitecture that can directly learn set-wise verification. Our contributions\nare: (i) We propose a Deep Comparator Network (DCN) that can ingest a pair of\nsets (each may contain a variable number of images) as inputs, and compute a\nsimilarity between the pair--this involves attending to multiple discriminative\nlocal regions (landmarks), and comparing local descriptors between pairs of\nfaces; (ii) To encourage high-quality representations for each set, internal\ncompetition is introduced for recalibration based on the landmark score; (iii)\nInspired by image retrieval, a novel hard sample mining regime is proposed to\ncontrol the sampling process, such that the DCN is complementary to the\nstandard image classification models. Evaluations on the IARPA Janus face\nrecognition benchmarks show that the comparator networks outperform the\nprevious state-of-the-art results by a large margin.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 16:54:21 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Xie", "Weidi", ""], ["Shen", "Li", ""], ["Zisserman", "Andrew", ""]]}, {"id": "1807.11455", "submitter": "Nicolas Dobigeon", "authors": "Yanna Cruz Cavalcanti, Thomas Oberlin, Nicolas Dobigeon, C\\'edric\n  F\\'evotte, Simon Stute, Maria-Joao Ribeiro, Clovis Tauber", "title": "Factor analysis of dynamic PET images: beyond Gaussian noise", "comments": "This manuscript has been accepted for publication in IEEE Trans.\n  Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis has proven to be a relevant tool for extracting tissue\ntime-activity curves (TACs) in dynamic PET images, since it allows for an\nunsupervised analysis of the data. Reliable and interpretable results are\npossible only if considered with respect to suitable noise statistics. However,\nthe noise in reconstructed dynamic PET images is very difficult to\ncharacterize, despite the Poissonian nature of the count-rates. Rather than\nexplicitly modeling the noise distribution, this work proposes to study the\nrelevance of several divergence measures to be used within a factor analysis\nframework. To this end, the $\\beta$-divergence, widely used in other\napplicative domains, is considered to design the data-fitting term involved in\nthree different factor models. The performances of the resulting algorithms are\nevaluated for different values of $\\beta$, in a range covering Gaussian,\nPoissonian and Gamma-distributed noises. The results obtained on two different\ntypes of synthetic images and one real image show the interest of applying\nnon-standard values of $\\beta$ to improve factor analysis.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 17:23:50 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 13:58:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Cavalcanti", "Yanna Cruz", ""], ["Oberlin", "Thomas", ""], ["Dobigeon", "Nicolas", ""], ["F\u00e9votte", "C\u00e9dric", ""], ["Stute", "Simon", ""], ["Ribeiro", "Maria-Joao", ""], ["Tauber", "Clovis", ""]]}, {"id": "1807.11458", "submitter": "Adrian Bulat", "authors": "Adrian Bulat, Jing Yang, Georgios Tzimiropoulos", "title": "To learn image super-resolution, use a GAN to learn how to do image\n  degradation first", "comments": "Accepted to ECCV18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is on image and face super-resolution. The vast majority of prior\nwork for this problem focus on how to increase the resolution of low-resolution\nimages which are artificially generated by simple bilinear down-sampling (or in\na few cases by blurring followed by down-sampling).We show that such methods\nfail to produce good results when applied to real-world low-resolution, low\nquality images. To circumvent this problem, we propose a two-stage process\nwhich firstly trains a High-to-Low Generative Adversarial Network (GAN) to\nlearn how to degrade and downsample high-resolution images requiring, during\ntraining, only unpaired high and low-resolution images. Once this is achieved,\nthe output of this network is used to train a Low-to-High GAN for image\nsuper-resolution using this time paired low- and high-resolution images. Our\nmain result is that this network can be now used to efectively increase the\nquality of real-world low-resolution images. We have applied the proposed\npipeline for the problem of face super-resolution where we report large\nimprovement over baselines and prior work although the proposed method is\npotentially applicable to other object categories.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 17:28:39 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Bulat", "Adrian", ""], ["Yang", "Jing", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "1807.11459", "submitter": "Parijat Dube", "authors": "Parijat Dube, Bishwaranjan Bhattacharjee, Elisabeth Petit-Bois,\n  Matthew Hill", "title": "Improving Transferability of Deep Neural Networks", "comments": "15 pages, 11 figures, 2 tables, Workshop on Domain Adaptation for\n  Visual Understanding (Joint IJCAI/ECAI/AAMAS/ICML 2018 Workshop) Keywords:\n  deep learning, transfer learning, finetuning, deep neural network,\n  experimental", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from small amounts of labeled data is a challenge in the area of\ndeep learning. This is currently addressed by Transfer Learning where one\nlearns the small data set as a transfer task from a larger source dataset.\nTransfer Learning can deliver higher accuracy if the hyperparameters and source\ndataset are chosen well. One of the important parameters is the learning rate\nfor the layers of the neural network. We show through experiments on the\nImageNet22k and Oxford Flowers datasets that improvements in accuracy in range\nof 127% can be obtained by proper choice of learning rates. We also show that\nthe images/label parameter for a dataset can potentially be used to determine\noptimal learning rates for the layers to get the best overall accuracy. We\nadditionally validate this method on a sample of real-world image\nclassification tasks from a public visual recognition API.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 17:34:24 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Dube", "Parijat", ""], ["Bhattacharjee", "Bishwaranjan", ""], ["Petit-Bois", "Elisabeth", ""], ["Hill", "Matthew", ""]]}, {"id": "1807.11473", "submitter": "Karim Ahmed", "authors": "Karim Ahmed and Lorenzo Torresani", "title": "MaskConnect: Connectivity Learning by Gradient Descent", "comments": "ECCV 2018. arXiv admin note: substantial text overlap with\n  arXiv:1709.09582", "journal-ref": "ECCV 2018", "doi": "10.1007/978-3-030-01228-1_22", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep networks have recently emerged as the model of choice for many\ncomputer vision problems, in order to yield good results they often require\ntime-consuming architecture search. To combat the complexity of design choices,\nprior work has adopted the principle of modularized design which consists in\ndefining the network in terms of a composition of topologically identical or\nsimilar building blocks (a.k.a. modules). This reduces architecture search to\nthe problem of determining the number of modules to compose and how to connect\nsuch modules. Again, for reasons of design complexity and training cost,\nprevious approaches have relied on simple rules of connectivity, e.g.,\nconnecting each module to only the immediately preceding module or perhaps to\nall of the previous ones. Such simple connectivity rules are unlikely to yield\nthe optimal architecture for the given problem.\n  In this work we remove these predefined choices and propose an algorithm to\nlearn the connections between modules in the network. Instead of being chosen a\npriori by the human designer, the connectivity is learned simultaneously with\nthe weights of the network by optimizing the loss function of the end task\nusing a modified version of gradient descent. We demonstrate our connectivity\nlearning method on the problem of multi-class image classification using two\npopular architectures: ResNet and ResNeXt. Experiments on four different\ndatasets show that connectivity learning using our approach yields consistently\nhigher accuracy compared to relying on traditional predefined rules of\nconnectivity. Furthermore, in certain settings it leads to significant savings\nin number of parameters.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 00:41:53 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ahmed", "Karim", ""], ["Torresani", "Lorenzo", ""]]}, {"id": "1807.11534", "submitter": "Jack Spencer", "authors": "Jack Spencer", "title": "A Restricted-Domain Dual Formulation for Two-Phase Image Segmentation", "comments": null, "journal-ref": "Irish Machine Vision and Image Processing Conference Proceedings,\n  pp. 139-146, 2017", "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two-phase image segmentation, convex relaxation has allowed global\nminimisers to be computed for a variety of data fitting terms. Many efficient\napproaches exist to compute a solution quickly. However, we consider whether\nthe nature of the data fitting in this formulation allows for reasonable\nassumptions to be made about the solution that can improve the computational\nperformance further. In particular, we employ a well known dual formulation of\nthis problem and solve the corresponding equations in a restricted domain. We\npresent experimental results that explore the dependence of the solution on\nthis restriction and quantify imrovements in the computational performance.\nThis approach can be extended to analogous methods simply and could provide an\nefficient alternative for problems of this type.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:15:38 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Spencer", "Jack", ""]]}, {"id": "1807.11541", "submitter": "Raphael Memmesheimer", "authors": "Raphael Memmesheimer, Ivanna Mykhalchyshyna, Viktor Seib, Nick\n  Theisen, Dietrich Paulus", "title": "Markerless Visual Robot Programming by Demonstration", "comments": "6 pages, 5 figures, 3rd BAILAR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach for learning to imitate human behavior\non a semantic level by markerless visual observation. We analyze a set of\nspatial constraints on human pose data extracted using convolutional pose\nmachines and object informations extracted from 2D image sequences. A scene\nanalysis, based on an ontology of objects and affordances, is combined with\ncontinuous human pose estimation and spatial object relations. Using a set of\nconstraints we associate the observed human actions with a set of executable\nrobot commands. We demonstrate our approach in a kitchen task, where the robot\nlearns to prepare a meal.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:33:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Memmesheimer", "Raphael", ""], ["Mykhalchyshyna", "Ivanna", ""], ["Seib", "Viktor", ""], ["Theisen", "Nick", ""], ["Paulus", "Dietrich", ""]]}, {"id": "1807.11546", "submitter": "Jinkyu Kim", "authors": "Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, Zeynep Akata", "title": "Textual Explanations for Self-Driving Vehicles", "comments": "Accepted to ECCV 2018", "journal-ref": "European Conference on Computer Vision (ECCV), 2018", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural perception and control networks have become key components of\nself-driving vehicles. User acceptance is likely to benefit from\neasy-to-interpret textual explanations which allow end-users to understand what\ntriggered a particular behavior. Explanations may be triggered by the neural\ncontroller, namely introspective explanations, or informed by the neural\ncontroller's output, namely rationalizations. We propose a new approach to\nintrospective explanations which consists of two parts. First, we use a visual\n(spatial) attention model to train a convolutional network end-to-end from\nimages to the vehicle control commands, i.e., acceleration and change of\ncourse. The controller's attention identifies image regions that potentially\ninfluence the network's output. Second, we use an attention-based video-to-text\nmodel to produce textual explanations of model actions. The attention maps of\ncontroller and explanation model are aligned so that explanations are grounded\nin the parts of the scene that mattered to the controller. We explore two\napproaches to attention alignment, strong- and weak-alignment. Finally, we\nexplore a version of our model that generates rationalizations, and compare\nwith introspective explanations on the same video segments. We evaluate these\nmodels on a novel driving dataset with ground-truth human explanations, the\nBerkeley DeepDrive eXplanation (BDD-X) dataset. Code is available at\nhttps://github.com/JinkyuKimUCB/explainable-deep-driving.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:38:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Kim", "Jinkyu", ""], ["Rohrbach", "Anna", ""], ["Darrell", "Trevor", ""], ["Canny", "John", ""], ["Akata", "Zeynep", ""]]}, {"id": "1807.11573", "submitter": "Pan Wei", "authors": "John E. Ball, Derek T. Anderson, Pan Wei", "title": "State-of-the-art and gaps for deep learning on limited training data in\n  remote sensing", "comments": "arXiv admin note: text overlap with arXiv:1709.00308", "journal-ref": "IGARSS June 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning usually requires big data, with respect to both volume and\nvariety. However, most remote sensing applications only have limited training\ndata, of which a small subset is labeled. Herein, we review three\nstate-of-the-art approaches in deep learning to combat this challenge. The\nfirst topic is transfer learning, in which some aspects of one domain, e.g.,\nfeatures, are transferred to another domain. The next is unsupervised learning,\ne.g., autoencoders, which operate on unlabeled data. The last is generative\nadversarial networks, which can generate realistic looking data that can fool\nthe likes of both a deep learning network and human. The aim of this article is\nto raise awareness of this dilemma, to direct the reader to existing work and\nto highlight current gaps that need solving.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 23:44:50 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ball", "John E.", ""], ["Anderson", "Derek T.", ""], ["Wei", "Pan", ""]]}, {"id": "1807.11575", "submitter": "Jiawei Mo", "authors": "Jiawei Mo, Junaed Sattar", "title": "SafeDrive: Enhancing Lane Appearance for Autonomous and Assisted Driving\n  Under Limited Visibility", "comments": "arXiv admin note: text overlap with arXiv:1701.08449", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous detection of lane markers improves road safety, and purely visual\ntracking is desirable for widespread vehicle compatibility and reducing sensor\nintrusion, cost, and energy consumption. However, visual approaches are often\nineffective because of a number of factors; e.g., occlusion, poor weather\nconditions, and paint wear-off. We present an approach to enhance lane marker\nappearance for assisted and autonomous driving, particularly under poor\nvisibility. Our method, named SafeDrive, attempts to improve visual lane\ndetection approaches in drastically degraded visual conditions. SafeDrive finds\nlane markers in alternate imagery of the road at the vehicle's location and\nreconstructs a sparse 3D model of the surroundings. By estimating the geometric\nrelationship between this 3D model and the current view, the lane markers are\nprojected onto the visual scene; any lane detection algorithm can be\nsubsequently used to detect lanes in the resulting image. SafeDrive does not\nrequire additional sensors other than vision and location data. We demonstrate\nthe effectiveness of our approach on a number of test cases obtained from\nactual driving data recorded in urban settings.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 02:20:08 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mo", "Jiawei", ""], ["Sattar", "Junaed", ""]]}, {"id": "1807.11583", "submitter": "Thomas Cherico Wanger Dr.", "authors": "Thomas Cherico Wanger, Peter Frohn", "title": "Testing the Efficient Network TRaining (ENTR) Hypothesis: initially\n  reducing training image size makes Convolutional Neural Network training for\n  image recognition tasks more efficient", "comments": "12 pages, 5 figures, 1 table +++ Keywords: Image recognition,\n  Efficient Network Training hypothesis, image size increase, network\n  efficiency, ResNet models, Google Colaboratory, free cloud GPU, material\n  science, geoscience, environmental science, convolutional neural networks,\n  regularization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) for image recognition tasks are seeing\nrapid advances in the available architectures and how networks are trained\nbased on large computational infrastructure and standard datasets with millions\nof images. In contrast, performance and time constraints for example, of small\ndevices and free cloud GPUs necessitate efficient network training (i.e.,\nhighest accuracy in the shortest inference time possible), often on small\ndatasets. Here, we hypothesize that initially decreasing image size during\ntraining makes the training process more efficient, because pre-shaping weights\nwith small images and later utilizing these weights with larger images reduces\ninitial network parameters and total inference time. We test this Efficient\nNetwork TRaining (ENTR) Hypothesis by training pre-trained Residual Network\n(ResNet) models (ResNet18, 34, & 50) on three small datasets (steel\nmicrostructures, bee images, and geographic aerial images) with a free cloud\nGPU. Based on three training regimes of i) not, ii) gradually or iii) in one\nstep increasing image size over the training process, we show that initially\nreducing image size increases training efficiency consistently across datasets\nand networks. We interpret these results mechanistically in the framework of\nregularization theory. Support for the ENTR hypothesis is an important\ncontribution, because network efficiency improvements for image recognition\ntasks are needed for practical applications. In the future, it will be exciting\nto see how the ENTR hypothesis holds for large standard datasets like ImageNet\nor CIFAR, to better understand the underlying mechanisms, and how these results\ncompare to other fields such as structural learning.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 21:10:25 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Wanger", "Thomas Cherico", ""], ["Frohn", "Peter", ""]]}, {"id": "1807.11590", "submitter": "Jiayuan Mao", "authors": "Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, Yuning Jiang", "title": "Acquisition of Localization Confidence for Accurate Object Detection", "comments": "Accepted to European Conference on Computer Vision (ECCV) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern CNN-based object detectors rely on bounding box regression and\nnon-maximum suppression to localize objects. While the probabilities for class\nlabels naturally reflect classification confidence, localization confidence is\nabsent. This makes properly localized bounding boxes degenerate during\niterative regression or even suppressed during NMS. In the paper we propose\nIoU-Net learning to predict the IoU between each detected bounding box and the\nmatched ground-truth. The network acquires this confidence of localization,\nwhich improves the NMS procedure by preserving accurately localized bounding\nboxes. Furthermore, an optimization-based bounding box refinement method is\nproposed, where the predicted IoU is formulated as the objective. Extensive\nexperiments on the MS-COCO dataset show the effectiveness of IoU-Net, as well\nas its compatibility with and adaptivity to several state-of-the-art object\ndetectors.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 21:36:20 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jiang", "Borui", ""], ["Luo", "Ruixuan", ""], ["Mao", "Jiayuan", ""], ["Xiao", "Tete", ""], ["Jiang", "Yuning", ""]]}, {"id": "1807.11598", "submitter": "Amod Jog", "authors": "Amod Jog and Bruce Fischl", "title": "Pulse Sequence Resilient Fast Brain Segmentation", "comments": "Accepted at MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate automatic segmentation of brain anatomy from\n$T_1$-weighted~($T_1$-w) magnetic resonance images~(MRI) has been a\ncomputationally intensive bottleneck in neuroimaging pipelines, with\nstate-of-the-art results obtained by unsupervised intensity modeling-based\nmethods and multi-atlas registration and label fusion. With the advent of\npowerful supervised convolutional neural networks~(CNN)-based learning\nalgorithms, it is now possible to produce a high quality brain segmentation\nwithin seconds. However, the very supervised nature of these methods makes it\ndifficult to generalize them on data different from what they have been trained\non. Modern neuroimaging studies are necessarily multi-center initiatives with a\nwide variety of acquisition protocols. Despite stringent protocol harmonization\npractices, it is not possible to standardize the whole gamut of MRI imaging\nparameters across scanners, field strengths, receive coils etc., that affect\nimage contrast. In this paper we propose a CNN-based segmentation algorithm\nthat, in addition to being highly accurate and fast, is also resilient to\nvariation in the input $T_1$-w acquisition. Our approach relies on building\napproximate forward models of $T_1$-w pulse sequences that produce a typical\ntest image. We use the forward models to augment the training data with test\ndata specific training examples. These augmented data can be used to update\nand/or build a more robust segmentation model that is more attuned to the test\ndata imaging properties. Our method generates highly accurate, state-of-the-art\nsegmentation results~(overall Dice overlap=0.94), within seconds and is\nconsistent across a wide-range of protocols.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 22:28:43 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Jog", "Amod", ""], ["Fischl", "Bruce", ""]]}, {"id": "1807.11599", "submitter": "Johan \\\"Ofverstedt", "authors": "Johan \\\"Ofverstedt, Joakim Lindblad, and Nata\\v{s}a Sladoje", "title": "Fast and Robust Symmetric Image Registration Based on Distances\n  Combining Intensity and Spatial Information", "comments": "14 pages, 4 tables, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2019.2899947", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensity-based image registration approaches rely on similarity measures to\nguide the search for geometric correspondences with high affinity between\nimages. The properties of the used measure are vital for the robustness and\naccuracy of the registration. In this study a symmetric, intensity\ninterpolation-free, affine registration framework based on a combination of\nintensity and spatial information is proposed. The excellent performance of the\nframework is demonstrated on a combination of synthetic tests, recovering known\ntransformations in the presence of noise, and real applications in biomedical\nand medical image registration, for both 2D and 3D images. The method exhibits\ngreater robustness and higher accuracy than similarity measures in common use,\nwhen inserted into a standard gradient-based registration framework available\nas part of the open source Insight Segmentation and Registration Toolkit (ITK).\nThe method is also empirically shown to have a low computational cost, making\nit practical for real applications. Source code is available.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 22:32:00 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:49:28 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["\u00d6fverstedt", "Johan", ""], ["Lindblad", "Joakim", ""], ["Sladoje", "Nata\u0161a", ""]]}, {"id": "1807.11626", "submitter": "Mingxing Tan", "authors": "Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler,\n  Andrew Howard, Quoc V. Le", "title": "MnasNet: Platform-Aware Neural Architecture Search for Mobile", "comments": "Published in CVPR 2019", "journal-ref": "CVPR 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing convolutional neural networks (CNN) for mobile devices is\nchallenging because mobile models need to be small and fast, yet still\naccurate. Although significant efforts have been dedicated to design and\nimprove mobile CNNs on all dimensions, it is very difficult to manually balance\nthese trade-offs when there are so many architectural possibilities to\nconsider. In this paper, we propose an automated mobile neural architecture\nsearch (MNAS) approach, which explicitly incorporate model latency into the\nmain objective so that the search can identify a model that achieves a good\ntrade-off between accuracy and latency. Unlike previous work, where latency is\nconsidered via another, often inaccurate proxy (e.g., FLOPS), our approach\ndirectly measures real-world inference latency by executing the model on mobile\nphones. To further strike the right balance between flexibility and search\nspace size, we propose a novel factorized hierarchical search space that\nencourages layer diversity throughout the network. Experimental results show\nthat our approach consistently outperforms state-of-the-art mobile CNN models\nacross multiple vision tasks. On the ImageNet classification task, our MnasNet\nachieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8x\nfaster than MobileNetV2 [29] with 0.5% higher accuracy and 2.3x faster than\nNASNet [36] with 1.2% higher accuracy. Our MnasNet also achieves better mAP\nquality than MobileNets for COCO object detection. Code is at\nhttps://github.com/tensorflow/tpu/tree/master/models/official/mnasnet\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 01:34:21 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 17:10:45 GMT"}, {"version": "v3", "created": "Wed, 29 May 2019 01:30:05 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tan", "Mingxing", ""], ["Chen", "Bo", ""], ["Pang", "Ruoming", ""], ["Vasudevan", "Vijay", ""], ["Sandler", "Mark", ""], ["Howard", "Andrew", ""], ["Le", "Quoc V.", ""]]}, {"id": "1807.11637", "submitter": "Jiahao Pang", "authors": "Jin Zeng, Jiahao Pang, Wenxiu Sun, Gene Cheung", "title": "Deep Graph Laplacian Regularization for Robust Denoising of Real Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in deep learning have revolutionized the paradigm of\nimage restoration. However, its applications on real image denoising are still\nlimited, due to its sensitivity to training data and the complex nature of real\nimage noise. In this work, we combine the robustness merit of model-based\napproaches and the learning power of data-driven approaches for real image\ndenoising. Specifically, by integrating graph Laplacian regularization as a\ntrainable module into a deep learning framework, we are less susceptible to\noverfitting than pure CNN-based approaches, achieving higher robustness to\nsmall datasets and cross-domain denoising. First, a sparse neighborhood graph\nis built from the output of a convolutional neural network (CNN). Then the\nimage is restored by solving an unconstrained quadratic programming problem,\nusing a corresponding graph Laplacian regularizer as a prior term. The proposed\nrestoration pipeline is fully differentiable and hence can be end-to-end\ntrained. Experimental results demonstrate that our work is less prone to\noverfitting given small training data. It is also endowed with strong\ncross-domain generalization power, outperforming the state-of-the-art\napproaches by a remarkable margin.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:44:34 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 09:36:58 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 12:41:25 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Zeng", "Jin", ""], ["Pang", "Jiahao", ""], ["Sun", "Wenxiu", ""], ["Cheung", "Gene", ""]]}, {"id": "1807.11643", "submitter": "Sifeng He", "authors": "Sifeng He and Bahram Jalali", "title": "Brain MRI Image Super Resolution using Phase Stretch Transform and\n  Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hallucination-free and computationally efficient algorithm for enhancing\nthe resolution of brain MRI images is demonstrated.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:51:21 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["He", "Sifeng", ""], ["Jalali", "Bahram", ""]]}, {"id": "1807.11649", "submitter": "Fei Wang", "authors": "Fei Wang, Liren Chen, Cheng Li, Shiyao Huang, Yanjie Chen, Chen Qian,\n  Chen Change Loy", "title": "The Devil of Face Recognition is in the Noise", "comments": "accepted to ECCV'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing scale of face recognition datasets empowers us to train strong\nconvolutional networks for face recognition. While a variety of architectures\nand loss functions have been devised, we still have a limited understanding of\nthe source and consequence of label noise inherent in existing datasets. We\nmake the following contributions: 1) We contribute cleaned subsets of popular\nface databases, i.e., MegaFace and MS-Celeb-1M datasets, and build a new\nlarge-scale noise-controlled IMDb-Face dataset. 2) With the original datasets\nand cleaned subsets, we profile and analyze label noise properties of MegaFace\nand MS-Celeb-1M. We show that a few orders more samples are needed to achieve\nthe same accuracy yielded by a clean subset. 3) We study the association\nbetween different types of noise, i.e., label flips and outliers, with the\naccuracy of face recognition models. 4) We investigate ways to improve data\ncleanliness, including a comprehensive user study on the influence of data\nlabeling strategies to annotation accuracy. The IMDb-Face dataset has been\nreleased on https://github.com/fwang91/IMDb-Face.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 03:43:11 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Wang", "Fei", ""], ["Chen", "Liren", ""], ["Li", "Cheng", ""], ["Huang", "Shiyao", ""], ["Chen", "Yanjie", ""], ["Qian", "Chen", ""], ["Loy", "Chen Change", ""]]}, {"id": "1807.11674", "submitter": "Roshanak Zakizadeh", "authors": "Roshanak Zakizadeh, Michele Sasdelli, Yu Qian, Eduard Vazquez", "title": "Improving the Annotation of DeepFashion Images for Fine-grained\n  Attribute Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DeepFashion is a widely used clothing dataset with 50 categories and more\nthan overall 200k images where each image is annotated with fine-grained\nattributes. This dataset is often used for clothes recognition and although it\nprovides comprehensive annotations, the attributes distribution is unbalanced\nand repetitive specially for training fine-grained attribute recognition\nmodels. In this work, we tailored DeepFashion for fine-grained attribute\nrecognition task by focusing on each category separately. After selecting\ncategories with sufficient number of images for training, we remove very scarce\nattributes and merge the duplicate ones in each category, then we clean the\ndataset based on the new list of attributes. We use a bilinear convolutional\nneural network with pairwise ranking loss function for multi-label fine-grained\nattribute recognition and show that the new annotations improve the results for\nsuch a task. The detailed annotations for each of the selected categories are\nprovided for public use.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 06:03:00 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zakizadeh", "Roshanak", ""], ["Sasdelli", "Michele", ""], ["Qian", "Yu", ""], ["Vazquez", "Eduard", ""]]}, {"id": "1807.11677", "submitter": "Saad Ullah Akram", "authors": "Saad Ullah Akram, Talha Qaiser, Simon Graham, Juho Kannala, Janne\n  Heikkil\\\"a, Nasir Rajpoot", "title": "Leveraging Unlabeled Whole-Slide-Images for Mitosis Detection", "comments": "Accepted for MICCAI COMPAY 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitosis count is an important biomarker for prognosis of various cancers. At\npresent, pathologists typically perform manual counting on a few selected\nregions of interest in breast whole-slide-images (WSIs) of patient biopsies.\nThis task is very time-consuming, tedious and subjective. Automated mitosis\ndetection methods have made great advances in recent years. However, these\nmethods require exhaustive labeling of a large number of selected regions of\ninterest. This task is very expensive because expert pathologists are needed\nfor reliable and accurate annotations. In this paper, we present a\nsemi-supervised mitosis detection method which is designed to leverage a large\nnumber of unlabeled breast cancer WSIs. As a result, our method capitalizes on\nthe growing number of digitized histology images, without relying on exhaustive\nannotations, subsequently improving mitosis detection. Our method first learns\na mitosis detector from labeled data, uses this detector to mine additional\nmitosis samples from unlabeled WSIs, and then trains the final model using this\nlarger and diverse set of mitosis samples. The use of unlabeled data improves\nF1-score by $\\sim$5\\% compared to our best performing fully-supervised model on\nthe TUPAC validation set. Our submission (single model) to TUPAC challenge\nranks highly on the leaderboard with an F1-score of 0.64.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 06:19:19 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Akram", "Saad Ullah", ""], ["Qaiser", "Talha", ""], ["Graham", "Simon", ""], ["Kannala", "Juho", ""], ["Heikkil\u00e4", "Janne", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1807.11688", "submitter": "Skand Vishwanath Peri", "authors": "Jatin Garg, Skand Vishwanath Peri, Himanshu Tolani, Narayanan C\n  Krishnan", "title": "Deep Cross Modal Learning for Caricature Verification and\n  Identification(CaVINet)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from different modalities is a challenging task. In this paper, we\nlook at the challenging problem of cross modal face verification and\nrecognition between caricature and visual image modalities. Caricature have\nexaggerations of facial features of a person. Due to the significant variations\nin the caricatures, building vision models for recognizing and verifying data\nfrom this modality is an extremely challenging task. Visual images with\nsignificantly lesser amount of distortions can act as a bridge for the analysis\nof caricature modality. We introduce a publicly available large\nCaricature-VIsual dataset [CaVI] with images from both the modalities that\ncaptures the rich variations in the caricature of an identity. This paper\npresents the first cross modal architecture that handles extreme distortions of\ncaricatures using a deep learning network that learns similar representations\nacross the modalities. We use two convolutional networks along with\ntransformations that are subjected to orthogonality constraints to capture the\nshared and modality specific representations. In contrast to prior research,\nour approach neither depends on manually extracted facial landmarks for\nlearning the representations, nor on the identities of the person for\nperforming verification. The learned shared representation achieves 91%\naccuracy for verifying unseen images and 75% accuracy on unseen identities.\nFurther, recognizing the identity in the image by knowledge transfer using a\ncombination of shared and modality specific representations, resulted in an\nunprecedented performance of 85% rank-1 accuracy for caricatures and 95% rank-1\naccuracy for visual images.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 07:19:14 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Garg", "Jatin", ""], ["Peri", "Skand Vishwanath", ""], ["Tolani", "Himanshu", ""], ["Krishnan", "Narayanan C", ""]]}, {"id": "1807.11699", "submitter": "Guorun Yang", "authors": "Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, Jiaya Jia", "title": "SegStereo: Exploiting Semantic Information for Disparity Estimation", "comments": "Accepted to ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Disparity estimation for binocular stereo images finds a wide range of\napplications. Traditional algorithms may fail on featureless regions, which\ncould be handled by high-level clues such as semantic segments. In this paper,\nwe suggest that appropriate incorporation of semantic cues can greatly rectify\nprediction in commonly-used disparity estimation frameworks. Our method\nconducts semantic feature embedding and regularizes semantic cues as the loss\nterm to improve learning disparity. Our unified model SegStereo employs\nsemantic features from segmentation and introduces semantic softmax loss, which\nhelps improve the prediction accuracy of disparity maps. The semantic cues work\nwell in both unsupervised and supervised manners. SegStereo achieves\nstate-of-the-art results on KITTI Stereo benchmark and produces decent\nprediction on both CityScapes and FlyingThings3D datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:24:36 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Yang", "Guorun", ""], ["Zhao", "Hengshuang", ""], ["Shi", "Jianping", ""], ["Deng", "Zhidong", ""], ["Jia", "Jiaya", ""]]}, {"id": "1807.11706", "submitter": "Risheng Liu", "authors": "Risheng Liu, Yi He, Shichao Cheng, Xin Fan, Zhongxuan Luo", "title": "Learning Collaborative Generation Correction Modules for Blind Image\n  Deblurring and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind image deblurring plays a very important role in many vision and\nmultimedia applications. Most existing works tend to introduce complex priors\nto estimate the sharp image structures for blur kernel estimation. However, it\nhas been verified that directly optimizing these models is challenging and easy\nto fall into degenerate solutions. Although several experience-based heuristic\ninference strategies, including trained networks and designed iterations, have\nbeen developed, it is still hard to obtain theoretically guaranteed accurate\nsolutions. In this work, a collaborative learning framework is established to\naddress the above issues. Specifically, we first design two modules, named\nGenerator and Corrector, to extract the intrinsic image structures from the\ndata-driven and knowledge-based perspectives, respectively. By introducing a\ncollaborative methodology to cascade these modules, we can strictly prove the\nconvergence of our image propagations to a deblurring-related optimal solution.\nAs a nontrivial byproduct, we also apply the proposed method to address other\nrelated tasks, such as image interpolation and edge-preserved smoothing. Plenty\nof experiments demonstrate that our method can outperform the state-of-the-art\napproaches on both synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 08:55:11 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Liu", "Risheng", ""], ["He", "Yi", ""], ["Cheng", "Shichao", ""], ["Fan", "Xin", ""], ["Luo", "Zhongxuan", ""]]}, {"id": "1807.11719", "submitter": "Shaobo Min", "authors": "Shaobo Min, Xuejin Chen, Zheng-Jun Zha, Feng Wu, Yongdong Zhang", "title": "A Two-Stream Mutual Attention Network for Semi-supervised Biomedical\n  Segmentation with Noisy Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\begin{abstract} Learning-based methods suffer from a deficiency of clean\nannotations, especially in biomedical segmentation. Although many\nsemi-supervised methods have been proposed to provide extra training data,\nautomatically generated labels are usually too noisy to retrain models\neffectively. In this paper, we propose a Two-Stream Mutual Attention Network\n(TSMAN) that weakens the influence of back-propagated gradients caused by\nincorrect labels, thereby rendering the network robust to unclean data. The\nproposed TSMAN consists of two sub-networks that are connected by three types\nof attention models in different layers. The target of each attention model is\nto indicate potentially incorrect gradients in a certain layer for both\nsub-networks by analyzing their inferred features using the same input. In\norder to achieve this purpose, the attention models are designed based on the\npropagation analysis of noisy gradients at different layers. This allows the\nattention models to effectively discover incorrect labels and weaken their\ninfluence during the parameter updating process. By exchanging multi-level\nfeatures within the two-stream architecture, the effects of noisy labels in\neach sub-network are reduced by decreasing the updating gradients. Furthermore,\na hierarchical distillation is developed to provide more reliable pseudo labels\nfor unlabelded data, which further boosts the performance of our retrained\nTSMAN. The experiments using both the HVSMR 2016 and BRATS 2015 benchmarks\ndemonstrate that our semi-supervised learning framework surpasses the\nstate-of-the-art fully-supervised results.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:34:16 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 08:56:31 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 13:21:52 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Min", "Shaobo", ""], ["Chen", "Xuejin", ""], ["Zha", "Zheng-Jun", ""], ["Wu", "Feng", ""], ["Zhang", "Yongdong", ""]]}, {"id": "1807.11720", "submitter": "Dasom Seo", "authors": "Dasom Seo, Kanghan Oh, Il-Seok Oh", "title": "Regional Multi-scale Approach for Visually Pleasing Explanations of Deep\n  Neural Networks", "comments": "9 pages, 5 figures, submitted on NIPS 2018", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2963055", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many methods to interpret and visualize deep neural network\npredictions have been proposed and significant progress has been made. However,\na more class-discriminative and visually pleasing explanation is required.\nThus, this paper proposes a region-based approach that estimates feature\nimportance in terms of appropriately segmented regions. By fusing the saliency\nmaps generated from multi-scale segmentations, a more class-discriminative and\nvisually pleasing map is obtained. We incorporate this regional multi-scale\nconcept into a prediction difference method that is model-agnostic. An input\nimage is segmented in several scales using the super-pixel method, and\nexclusion of a region is simulated by sampling a normal distribution\nconstructed using the boundary prior. The experimental results demonstrate that\nthe regional multi-scale method produces much more class-discriminative and\nvisually pleasing saliency maps.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:37:39 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 09:01:35 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Seo", "Dasom", ""], ["Oh", "Kanghan", ""], ["Oh", "Il-Seok", ""]]}, {"id": "1807.11724", "submitter": "Ashish Mishra", "authors": "Sasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra, Anurag\n  Mittal", "title": "A Zero-Shot Framework for Sketch-based Image Retrieval", "comments": "Accepted in ECCV 2018, Munich Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Sketch-based image retrieval (SBIR) is the task of retrieving images from a\nnatural image database that correspond to a given hand-drawn sketch. Ideally,\nan SBIR model should learn to associate components in the sketch (say, feet,\ntail, etc.) with the corresponding components in the image having similar shape\ncharacteristics. However, current evaluation methods simply focus only on\ncoarse-grained evaluation where the focus is on retrieving images which belong\nto the same class as the sketch but not necessarily having the same shape\ncharacteristics as in the sketch. As a result, existing methods simply learn to\nassociate sketches with classes seen during training and hence fail to\ngeneralize to unseen classes. In this paper, we propose a new benchmark for\nzero-shot SBIR where the model is evaluated in novel classes that are not seen\nduring training. We show through extensive experiments that existing models for\nSBIR that are trained in a discriminative setting learn only class specific\nmappings and fail to generalize to the proposed zero-shot setting. To\ncircumvent this, we propose a generative approach for the SBIR task by\nproposing deep conditional generative models that take the sketch as an input\nand fill the missing information stochastically. Experiments on this new\nbenchmark created from the \"Sketchy\" dataset, which is a large-scale database\nof sketch-photo pairs demonstrate that the performance of these generative\nmodels is significantly better than several state-of-the-art approaches in the\nproposed zero-shot framework of the coarse-grained SBIR task.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 09:42:16 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Yelamarthi", "Sasi Kiran", ""], ["Reddy", "Shiva Krishna", ""], ["Mishra", "Ashish", ""], ["Mittal", "Anurag", ""]]}, {"id": "1807.11745", "submitter": "Thomas Kruezer", "authors": "Jahanzaib Shabbir and Thomas Kruezer", "title": "Deep Visual Odometry Methods for Mobile Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology has made navigation in 3D real time possible and this has made\npossible what seemed impossible. This paper explores the aspect of deep visual\nodometry methods for mobile robots. Visual odometry has been instrumental in\nmaking this navigation successful. Noticeable challenges in mobile robots\nincluding the inability to attain Simultaneous Localization and Mapping have\nbeen solved by visual odometry through its cameras which are suitable for human\nenvironments. More intuitive, precise and accurate detection have been made\npossible by visual odometry in mobile robots. Another challenge in the mobile\nrobot world is the 3D map reconstruction for exploration. A dense map in mobile\nrobots can facilitate for localization and more accurate findings.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:20:19 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Shabbir", "Jahanzaib", ""], ["Kruezer", "Thomas", ""]]}, {"id": "1807.11766", "submitter": "Filippo Maria Bianchi", "authors": "Luigi T. Luppino, Filippo M. Bianchi, Gabriele Moser, Stian N.\n  Anfinsen", "title": "Remote sensing image regression for heterogeneous change detection", "comments": "Accepted to Machine Learning for Signal Processing 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change detection in heterogeneous multitemporal satellite images is an\nemerging topic in remote sensing. In this paper we propose a framework, based\non image regression, to perform change detection in heterogeneous multitemporal\nsatellite images, which has become a main topic in remote sensing. Our method\nlearns a transformation to map the first image to the domain of the other\nimage, and vice versa. Four regression methods are selected to carry out the\ntransformation: Gaussian processes, support vector machines, random forests,\nand a recently proposed kernel regression method called homogeneous pixel\ntransformation. To evaluate not only potentials and limitations of our\nframework, but also the pros and cons of each regression method, we perform\nexperiments on two data sets. The results indicates that random forests achieve\ngood performance, are fast and robust to hyperparameters, whereas the\nhomogeneous pixel transformation method can achieve better accuracy at the cost\nof a higher complexity.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 11:28:52 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Luppino", "Luigi T.", ""], ["Bianchi", "Filippo M.", ""], ["Moser", "Gabriele", ""], ["Anfinsen", "Stian N.", ""]]}, {"id": "1807.11783", "submitter": "Diego Marcos", "authors": "Diego Marcos, Benjamin Kellenberger, Sylvain Lobry and Devis Tuia", "title": "Scale equivariance in CNNs with vector fields", "comments": "ICML/FAIM 2018 workshop on Towards learning with limited labels:\n  Equivariance, Invariance, and Beyond (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of injecting local scale equivariance into Convolutional\nNeural Networks. This is done by applying each convolutional filter at multiple\nscales. The output is a vector field encoding for the maximally activating\nscale and the scale itself, which is further processed by the following\nconvolutional layers. This allows all the intermediate representations to be\nlocally scale equivariant. We show that this improves the performance of the\nmodel by over $20\\%$ in the scale equivariant task of regressing the scaling\nfactor applied to randomly scaled MNIST digits. Furthermore, we find it also\nuseful for scale invariant tasks, such as the actual classification of randomly\nscaled digits. This highlights the usefulness of allowing for a compact\nrepresentation that can also learn relationships between different local scales\nby keeping internal scale equivariance.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 12:14:45 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Marcos", "Diego", ""], ["Kellenberger", "Benjamin", ""], ["Lobry", "Sylvain", ""], ["Tuia", "Devis", ""]]}, {"id": "1807.11794", "submitter": "Swathikiran Sudhakaran", "authors": "Swathikiran Sudhakaran and Oswald Lanz", "title": "Attention is All We Need: Nailing Down Object-centric Attention for\n  Egocentric Activity Recognition", "comments": "Accepted to BMVC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an end-to-end trainable deep neural network model\nfor egocentric activity recognition. Our model is built on the observation that\negocentric activities are highly characterized by the objects and their\nlocations in the video. Based on this, we develop a spatial attention mechanism\nthat enables the network to attend to regions containing objects that are\ncorrelated with the activity under consideration. We learn highly specialized\nattention maps for each frame using class-specific activations from a CNN\npre-trained for generic image recognition, and use them for spatio-temporal\nencoding of the video with a convolutional LSTM. Our model is trained in a\nweakly supervised setting using raw video-level activity-class labels.\nNonetheless, on standard egocentric activity benchmarks our model surpasses by\nup to +6% points recognition accuracy the currently best performing method that\nleverages hand segmentation and object location strong supervision for\ntraining. We visually analyze attention maps generated by the network,\nrevealing that the network successfully identifies the relevant objects present\nin the video frames which may explain the strong recognition performance. We\nalso discuss an extensive ablation analysis regarding the design choices.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 12:54:06 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Sudhakaran", "Swathikiran", ""], ["Lanz", "Oswald", ""]]}, {"id": "1807.11805", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris and Francesc X. Prenafeta-Bold\\'u", "title": "Disaster Monitoring using Unmanned Aerial Vehicles and Deep Learning", "comments": "Disaster Management for Resilience and Public Safety Workshop, Proc.\n  of EnviroInfo 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of disasters is crucial for mitigating their effects on the\nenvironment and human population, and can be facilitated by the use of unmanned\naerial vehicles (UAV), equipped with camera sensors that produce aerial photos\nof the areas of interest. A modern technique for recognition of events based on\naerial photos is deep learning. In this paper, we present the state of the art\nwork related to the use of deep learning techniques for disaster\nidentification. We demonstrate the potential of this technique in identifying\ndisasters with high accuracy, by means of a relatively simple deep learning\nmodel. Based on a dataset of 544 images (containing disaster images such as\nfires, earthquakes, collapsed buildings, tsunami and flooding, as well as\nnon-disaster scenes), our results show an accuracy of 91% achieved, indicating\nthat deep learning, combined with UAV equipped with camera sensors, have the\npotential to predict disasters with high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 13:24:31 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 09:29:37 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Kamilaris", "Andreas", ""], ["Prenafeta-Bold\u00fa", "Francesc X.", ""]]}, {"id": "1807.11809", "submitter": "Andreas Kamilaris", "authors": "Andreas Kamilaris and Francesc X. Prenafeta-Boldu", "title": "Deep learning in agriculture: A survey", "comments": null, "journal-ref": "Computers and Electronics in Agriculture International Journal,\n  2018", "doi": "10.1016/j.compag.2018.02.016", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning constitutes a recent, modern technique for image processing and\ndata analysis, with promising results and large potential. As deep learning has\nbeen successfully applied in various domains, it has recently entered also the\ndomain of agriculture. In this paper, we perform a survey of 40 research\nefforts that employ deep learning techniques, applied to various agricultural\nand food production challenges. We examine the particular agricultural problems\nunder study, the specific models and frameworks employed, the sources, nature\nand pre-processing of data used, and the overall performance achieved according\nto the metrics used at each work under study. Moreover, we study comparisons of\ndeep learning with other existing popular techniques, in respect to differences\nin classification or regression performance. Our findings indicate that deep\nlearning provides high accuracy, outperforming existing commonly used image\nprocessing techniques.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 13:30:03 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Kamilaris", "Andreas", ""], ["Prenafeta-Boldu", "Francesc X.", ""]]}, {"id": "1807.11857", "submitter": "Anil Baslamisli", "authors": "Anil S. Baslamisli, Thomas T. Groenestege, Partha Das, Hoang-An Le,\n  Sezer Karaoglu, Theo Gevers", "title": "Joint Learning of Intrinsic Images and Semantic Segmentation", "comments": "ECCV 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of outdoor scenes is problematic when there are\nvariations in imaging conditions. It is known that albedo (reflectance) is\ninvariant to all kinds of illumination effects. Thus, using reflectance images\nfor semantic segmentation task can be favorable. Additionally, not only\nsegmentation may benefit from reflectance, but also segmentation may be useful\nfor reflectance computation. Therefore, in this paper, the tasks of semantic\nsegmentation and intrinsic image decomposition are considered as a combined\nprocess by exploring their mutual relationship in a joint fashion. To that end,\nwe propose a supervised end-to-end CNN architecture to jointly learn intrinsic\nimage decomposition and semantic segmentation. We analyze the gains of\naddressing those two problems jointly. Moreover, new cascade CNN architectures\nfor intrinsic-for-segmentation and segmentation-for-intrinsic are proposed as\nsingle tasks. Furthermore, a dataset of 35K synthetic images of natural\nenvironments is created with corresponding albedo and shading (intrinsics), as\nwell as semantic labels (segmentation) assigned to each object/scene. The\nexperiments show that joint learning of intrinsic image decomposition and\nsemantic segmentation is beneficial for both tasks for natural scenes. Dataset\nand models are available at: https://ivi.fnwi.uva.nl/cv/intrinseg\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:08:25 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Baslamisli", "Anil S.", ""], ["Groenestege", "Thomas T.", ""], ["Das", "Partha", ""], ["Le", "Hoang-An", ""], ["Karaoglu", "Sezer", ""], ["Gevers", "Theo", ""]]}, {"id": "1807.11886", "submitter": "Qijie Zhao", "authors": "Qijie Zhao, Feng Ni, Yang Song, Yongtao Wang, Zhi Tang", "title": "Deep Dual Pyramid Network for Barcode Segmentation using Barcode-30k\n  Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital signs(such as barcode or QR code) are widely used in our daily life,\nand for many applications, we need to localize them on images. However,\ndifficult cases such as targets with small scales, half-occlusion, shape\ndeformation and large illumination changes cause challenges for conventional\nmethods. In this paper, we address this problem by producing a large-scale\ndataset and adopting a deep learning based semantic segmentation approach.\nSpecifically, a synthesizing method was proposed to generate well-annotated\nimages containing barcode and QR code labels, which contributes to largely\ndecrease the annotation time. Through the synthesis strategy, we introduce a\ndataset that contains 30000 images with Barcode and QR code - Barcode-30k.\nMoreover, we further propose a dual pyramid structure based segmentation\nnetwork - BarcodeNet, which is mainly formed with two novel modules, Prior\nPyramid Pooling Module(P3M) and Pyramid Refine Module(PRM). We validate the\neffectiveness of BarcodeNet on the proposed synthetic dataset, and it yields\nthe result of mIoU accuracy 95.36\\% on validation set. Additional segmentation\nresults of real images have shown that accurate segmentation performance is\nachieved.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 15:59:11 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhao", "Qijie", ""], ["Ni", "Feng", ""], ["Song", "Yang", ""], ["Wang", "Yongtao", ""], ["Tang", "Zhi", ""]]}, {"id": "1807.11888", "submitter": "Youness Mansar", "authors": "Youness Mansar", "title": "Deep End-to-end Fingerprint Denoising and Inpainting", "comments": "Winning solution to the Chalearn LAP In-painting Competition Track 3\n  / Accepted in the 2018 Chalearn Looking at People Satellite Workshop ECCV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes our winning solution for the Chalearn LAP In-painting\nCompetition Track 3 - Fingerprint Denoising and In-painting. The objective of\nthis competition is to reduce noise, remove the background pattern and replace\nmissing parts of fingerprint images in order to simplify the verification made\nby humans or third-party software. In this paper, we use a U-Net like CNN model\nthat performs all those steps end-to-end after being trained on the competition\ndata in a fully supervised way. This architecture and training procedure\nachieved the best results on all three metrics of the competition.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 16:01:23 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 09:23:47 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 14:28:28 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Mansar", "Youness", ""]]}, {"id": "1807.11916", "submitter": "Michael Andrews", "authors": "Michael Andrews, Manfred Paulini, Sergei Gleyzer, Barnabas Poczos", "title": "End-to-End Physics Event Classification with CMS Open Data: Applying\n  Image-Based Deep Learning to Detector Data for the Direct Classification of\n  Collision Events at the LHC", "comments": "14 pages, 5 figures; v3: published version", "journal-ref": "Comput Softw Big Sci 4, 6 (2020)", "doi": "10.1007/s41781-020-00038-8", "report-no": null, "categories": "physics.data-an cs.CV cs.LG hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the construction of novel end-to-end image-based\nclassifiers that directly leverage low-level simulated detector data to\ndiscriminate signal and background processes in pp collision events at the\nLarge Hadron Collider at CERN. To better understand what end-to-end classifiers\nare capable of learning from the data and to address a number of associated\nchallenges, we distinguish the decay of the standard model Higgs boson into two\nphotons from its leading background sources using high-fidelity simulated CMS\nOpen Data. We demonstrate the ability of end-to-end classifiers to learn from\nthe angular distribution of the photons recorded as electromagnetic showers,\ntheir intrinsic shapes, and the energy of their constituent hits, even when the\nunderlying particles are not fully resolved, delivering a clear advantage in\nsuch cases over purely kinematics-based classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 16:52:07 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 21:17:04 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2020 23:14:06 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Andrews", "Michael", ""], ["Paulini", "Manfred", ""], ["Gleyzer", "Sergei", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1807.11926", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Gabriel Kreiman", "title": "What am I Searching for: Zero-shot Target Identity Inference in Visual\n  Search", "comments": "Accepted for presentation at EPIC@CVPR2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we infer intentions from a person's actions? As an example problem, here\nwe consider how to decipher what a person is searching for by decoding their\neye movement behavior. We conducted two psychophysics experiments where we\nmonitored eye movements while subjects searched for a target object. We defined\nthe fixations falling on non-target objects as \"error fixations\". Using those\nerror fixations, we developed a model (InferNet) to infer what the target was.\nInferNet uses a pre-trained convolutional neural network to extract features\nfrom the error fixations and computes a similarity map between the error\nfixations and all locations across the search image. The model consolidates the\nsimilarity maps across layers and integrates these maps across all error\nfixations. InferNet successfully identifies the subject's goal and outperforms\ncompetitive null models, even without any object-specific training on the\ninference task.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 17:15:11 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 01:17:22 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Zhang", "Mengmi", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1807.11929", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Keng Teck Ma, Shih-Cheng Yen, Joo Hwee Lim, Qi Zhao, and\n  Jiashi Feng", "title": "Egocentric Spatial Memory", "comments": "8 pages, 6 figures, accepted in IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric spatial memory (ESM) defines a memory system with encoding,\nstoring, recognizing and recalling the spatial information about the\nenvironment from an egocentric perspective. We introduce an integrated deep\nneural network architecture for modeling ESM. It learns to estimate the\noccupancy state of the world and progressively construct top-down 2D global\nmaps from egocentric views in a spatially extended environment. During the\nexploration, our proposed ESM model updates belief of the global map based on\nlocal observations using a recurrent neural network. It also augments the local\nmapping with a novel external memory to encode and store latent representations\nof the visited places over long-term exploration in large environments which\nenables agents to perform place recognition and hence, loop closure. Our\nproposed ESM network contributes in the following aspects: (1) without feature\nengineering, our model predicts free space based on egocentric views\nefficiently in an end-to-end manner; (2) different from other deep\nlearning-based mapping system, ESMN deals with continuous actions and states\nwhich is vitally important for robotic control in real applications. In the\nexperiments, we demonstrate its accurate and robust global mapping capacities\nin 3D virtual mazes and realistic indoor environments by comparing with several\ncompetitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 17:27:19 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhang", "Mengmi", ""], ["Ma", "Keng Teck", ""], ["Yen", "Shih-Cheng", ""], ["Lim", "Joo Hwee", ""], ["Zhao", "Qi", ""], ["Feng", "Jiashi", ""]]}, {"id": "1807.11936", "submitter": "Vahid Mirjalili Dr", "authors": "Vahid Mirjalili, Sebastian Raschka, Arun Ross", "title": "Gender Privacy: An Ensemble of Semi Adversarial Networks for Confounding\n  Arbitrary Gender Classifiers", "comments": "Published in Proc. of IEEE 9th International Conference on\n  Biometrics: Theory, Applications and Systems (BTAS), (Los Angeles, CA),\n  October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has proposed the use of Semi Adversarial Networks (SAN) for\nimparting privacy to face images. SANs are convolutional autoencoders that\nperturb face images such that the perturbed images cannot be reliably used by\nan attribute classifier (e.g., a gender classifier) but can still be used by a\nface matcher for matching purposes. However, the generalizability of SANs\nacross multiple arbitrary gender classifiers has not been demonstrated in the\nliterature. In this work, we tackle the generalization issue by designing an\nensemble SAN model that generates a diverse set of perturbed outputs for a\ngiven input face image. This is accomplished by enforcing diversity among the\nindividual models in the ensemble through the use of different data\naugmentation techniques. The goal is to ensure that at least one of the\nperturbed output faces will confound an arbitrary, previously unseen gender\nclassifier. Extensive experiments using different unseen gender classifiers and\nface matchers are performed to demonstrate the efficacy of the proposed\nparadigm in imparting gender privacy to face images.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 17:53:07 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mirjalili", "Vahid", ""], ["Raschka", "Sebastian", ""], ["Ross", "Arun", ""]]}]