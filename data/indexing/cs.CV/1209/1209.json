[{"id": "1209.0053", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Moumita Pal, Achintya Das", "title": "A Session Based Blind Watermarking Technique within the NROI of Retinal\n  Fundus Images for Authentication Using DWT, Spread Spectrum and Harris Corner\n  Detection", "comments": "9 pages, 10 figures", "journal-ref": "International Journal of Modern Engineering Research\n  (IJMER),Vol.2, Issue.3,May-June 2012 pp-749-757,ISSN: 2249-6645", "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Retinal Fundus Images helps to detect various ophthalmic diseases by\ndetecting morphological changes in optical cup, optical disc and macula.\nPresent work proposes a method for the authentication of medical images based\non Discrete Wavelet Transformation (DWT) and Spread Spectrum. Proper selection\nof the Non Region of Interest (NROI) for watermarking is crucial, as the area\nunder concern has to be the least required portion conveying any medical\ninformation. Proposed method discusses both the selection of least impact area\nand the blind watermarking technique. Watermark is embedded within the\nHigh-High (HH) sub band. During embedding, watermarked image is dispersed\nwithin the band using a pseudo random sequence and a Session key. Watermarked\nimage is extracted using the session key and the size of the image. In this\napproach the generated watermarked image having an acceptable level of\nimperceptibility and distortion is compared to the Original retinal image based\non Peak Signal to Noise Ratio (PSNR) and correlation value.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 04:17:39 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Dey", "Nilanjan", ""], ["Pal", "Moumita", ""], ["Das", "Achintya", ""]]}, {"id": "1209.0196", "submitter": "Roberto Herrera", "authors": "Roberto H. Herrera and Mirko Van der Baan", "title": "Short-time homomorphic wavelet estimation", "comments": "13 pages, 5 figures. 2012 J. Geophys. Eng. 9 674", "journal-ref": "J. Geophys. Eng. 9 674, 2012", "doi": "10.1088/1742-2132/9/6/674", "report-no": null, "categories": "physics.geo-ph cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful wavelet estimation is an essential step for seismic methods like\nimpedance inversion, analysis of amplitude variations with offset and full\nwaveform inversion. Homomorphic deconvolution has long intrigued as a\npotentially elegant solution to the wavelet estimation problem. Yet a\nsuccessful implementation has proven difficult. Associated disadvantages like\nphase unwrapping and restrictions of sparsity in the reflectivity function\nlimit its application. We explore short-time homomorphic wavelet estimation as\na combination of the classical homomorphic analysis and log-spectral averaging.\nThe introduced method of log-spectral averaging using a short-term Fourier\ntransform increases the number of sample points, thus reducing estimation\nvariances. We apply the developed method on synthetic and real data examples\nand demonstrate good performance.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2012 17:57:05 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2012 21:40:20 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2013 06:59:53 GMT"}], "update_date": "2013-01-10", "authors_parsed": [["Herrera", "Roberto H.", ""], ["Van der Baan", "Mirko", ""]]}, {"id": "1209.0654", "submitter": "Laurent Jacques", "authors": "Adriana Gonzalez, Laurent Jacques, Christophe De Vleeschouwer,\n  Philippe Antoine", "title": "Compressive Optical Deflectometric Tomography: A Constrained\n  Total-Variation Minimization Approach", "comments": "37 pages, 51 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical Deflectometric Tomography (ODT) provides an accurate characterization\nof transparent materials whose complex surfaces present a real challenge for\nmanufacture and control. In ODT, the refractive index map (RIM) of a\ntransparent object is reconstructed by measuring light deflection under\nmultiple orientations. We show that this imaging modality can be made\n\"compressive\", i.e., a correct RIM reconstruction is achievable with far less\nobservations than required by traditional Filtered Back Projection (FBP)\nmethods. Assuming a cartoon-shape RIM model, this reconstruction is driven by\nminimizing the map Total-Variation under a fidelity constraint with the\navailable observations. Moreover, two other realistic assumptions are added to\nimprove the stability of our approach: the map positivity and a frontier\ncondition. Numerically, our method relies on an accurate ODT sensing model and\non a primal-dual minimization scheme, including easily the sensing operator and\nthe proposed RIM constraints. We conclude this paper by demonstrating the power\nof our method on synthetic and experimental data under various compressive\nscenarios. In particular, the compressiveness of the stabilized ODT problem is\ndemonstrated by observing a typical gain of 20 dB compared to FBP at only 5% of\n360 incident light angles for moderately noisy sensing.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 14:13:40 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2013 15:39:01 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Gonzalez", "Adriana", ""], ["Jacques", "Laurent", ""], ["De Vleeschouwer", "Christophe", ""], ["Antoine", "Philippe", ""]]}, {"id": "1209.0841", "submitter": "Xi Peng", "authors": "Xi Peng, Zhiding Yu, Huajin Tang, Zhang Yi", "title": "Constructing the L2-Graph for Robust Subspace Learning and Subspace\n  Clustering", "comments": null, "journal-ref": "IEEE Trans. on Cybernetics, vol. 47, no. 4, pp. 1053-1066, Apr.\n  2017", "doi": "10.1109/TCYB.2016.2536752", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the framework of graph-based learning, the key to robust subspace\nclustering and subspace learning is to obtain a good similarity graph that\neliminates the effects of errors and retains only connections between the data\npoints from the same subspace (i.e., intra-subspace data points). Recent works\nachieve good performance by modeling errors into their objective functions to\nremove the errors from the inputs. However, these approaches face the\nlimitations that the structure of errors should be known prior and a complex\nconvex problem must be solved. In this paper, we present a novel method to\neliminate the effects of the errors from the projection space (representation)\nrather than from the input space. We first prove that $\\ell_1$-, $\\ell_2$-,\n$\\ell_{\\infty}$-, and nuclear-norm based linear projection spaces share the\nproperty of Intra-subspace Projection Dominance (IPD), i.e., the coefficients\nover intra-subspace data points are larger than those over inter-subspace data\npoints. Based on this property, we introduce a method to construct a sparse\nsimilarity graph, called L2-Graph. The subspace clustering and subspace\nlearning algorithms are developed upon L2-Graph. Experiments show that L2-Graph\nalgorithms outperform the state-of-the-art methods for feature extraction,\nimage clustering, and motion segmentation in terms of accuracy, robustness, and\ntime efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 01:36:11 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2012 01:52:37 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2012 09:41:24 GMT"}, {"version": "v4", "created": "Sun, 11 Nov 2012 06:17:34 GMT"}, {"version": "v5", "created": "Tue, 5 Mar 2013 06:58:31 GMT"}, {"version": "v6", "created": "Tue, 17 Sep 2013 02:42:04 GMT"}, {"version": "v7", "created": "Sat, 17 Jan 2015 12:00:47 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Peng", "Xi", ""], ["Yu", "Zhiding", ""], ["Tang", "Huajin", ""], ["Yi", "Zhang", ""]]}, {"id": "1209.0999", "submitter": "Anna Vilanova", "authors": "Anna Vilanova, Bernhard Preim, Roy van Pelt, Rocco Gasteiger, Mathias\n  Neugebauer, and Thomas Wischgoll", "title": "Visual Exploration of Simulated and Measured Blood Flow", "comments": "20 pages book chapter of Dagstuhl Seminar 09251 \"Scientific\n  Visualization 2011\" book http://www.dagstuhl.de/09251", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphology of cardiovascular tissue is influenced by the unsteady behavior of\nthe blood flow and vice versa. Therefore, the pathogenesis of several\ncardiovascular diseases is directly affected by the blood-flow dynamics.\nUnderstanding flow behavior is of vital importance to understand the\ncardiovascular system and potentially harbors a considerable value for both\ndiagnosis and risk assessment. The analysis of hemodynamic characteristics\ninvolves qualitative and quantitative inspection of the blood-flow field.\nVisualization plays an important role in the qualitative exploration, as well\nas the definition of relevant quantitative measures and its validation. There\nare two main approaches to obtain information about the blood flow: simulation\nby computational fluid dynamics, and in-vivo measurements. Although research on\nblood flow simulation has been performed for decades, many open problems remain\nconcerning accuracy and patient-specific solutions. Possibilities for real\nmeasurement of blood flow have recently increased considerably by new\ndevelopments in magnetic resonance imaging which enable the acquisition of 3D\nquantitative measurements of blood-flow velocity fields. This chapter presents\nthe visualization challenges for both simulation and real measurements of\nunsteady blood-flow fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 14:47:55 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Vilanova", "Anna", ""], ["Preim", "Bernhard", ""], ["van Pelt", "Roy", ""], ["Gasteiger", "Rocco", ""], ["Neugebauer", "Mathias", ""], ["Wischgoll", "Thomas", ""]]}, {"id": "1209.1048", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Paramartha Dutta, Hiranmay Saha", "title": "Performance Analysis Of Neuro Genetic Algorithm Applied On Detecting\n  Proportion Of Components In Manhole Gas Mixture", "comments": "16 pages,11 figures", "journal-ref": null, "doi": "10.5121/ijaia.2012.3406", "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents performance analysis of a real valued neuro genetic\nalgorithm applied for the detection of proportion of the gases found in manhole\ngas mixture. The neural network (NN) trained using genetic algorithm (GA) leads\nto concept of neuro genetic algorithm, which is used for implementing an\nintelligent sensory system for the detection of component gases present in\nmanhole gas mixture Usually a manhole gas mixture contains several toxic gases\nlike Hydrogen Sulfide, Ammonia, Methane, Carbon Dioxide, Nitrogen Oxide, and\nCarbon Monoxide. A semiconductor based gas sensor array used for sensing\nmanhole gas components is an integral part of the proposed intelligent system.\nIt consists of many sensor elements, where each sensor element is responsible\nfor sensing particular gas component. Multiple sensors of different gases used\nfor detecting gas mixture of multiple gases, results in cross-sensitivity. The\ncross-sensitivity is a major issue and the problem is viewed as pattern\nrecognition problem. The objective of this article is to present performance\nanalysis of the real valued neuro genetic algorithm which is applied for\nmultiple gas detection.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2012 04:29:47 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Dutta", "Paramartha", ""], ["Saha", "Hiranmay", ""]]}, {"id": "1209.1125", "submitter": "Jamel Slimi", "authors": "Jamel Slimi, Anis Ben Ammar and Adel M. Alimi", "title": "Video Data Visualization System: Semantic Classification And\n  Personalization", "comments": "graphics", "journal-ref": null, "doi": "10.5121/ijcga.2012.2201", "report-no": null, "categories": "cs.IR cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper an intelligent video data visualization tool, based\non semantic classification, for retrieving and exploring a large scale corpus\nof videos. Our work is based on semantic classification resulting from semantic\nanalysis of video. The obtained classes will be projected in the visualization\nspace. The graph is represented by nodes and edges, the nodes are the keyframes\nof video documents and the edges are the relation between documents and the\nclasses of documents. Finally, we construct the user's profile, based on the\ninteraction with the system, to render the system more adequate to its\nreferences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2012 21:28:32 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Slimi", "Jamel", ""], ["Ammar", "Anis Ben", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1209.1181", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Anamitra Bardhan Roy, Moumita Pal, Achintya Das", "title": "FCM Based Blood Vessel Segmentation Method for Retinal Images", "comments": "5 pages,3figures", "journal-ref": "International Journal of Computer Science and Network\n  (IJCSN),Volume 1, Issue 3, June 2012,ISSN 2277-5420", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation of blood vessels in retinal images provides early diagnosis of\ndiseases like glaucoma, diabetic retinopathy and macular degeneration. Among\nthese diseases occurrence of Glaucoma is most frequent and has serious ocular\nconsequences that can even lead to blindness, if it is not detected early. The\nclinical criteria for the diagnosis of glaucoma include intraocular pressure\nmeasurement, optic nerve head evaluation, retinal nerve fiber layer and visual\nfield defects. This form of blood vessel segmentation helps in early detection\nfor ophthalmic diseases, and potentially reduces the risk of blindness. The\nlow-contrast images at the retina owing to narrow blood vessels of the retina\nare difficult to extract. These low contrast images are, however useful in\nrevealing certain systemic diseases. Motivated by the goals of improving\ndetection of such vessels, this present work proposes an algorithm for\nsegmentation of blood vessels and compares the results between expert\nophthalmologist hand-drawn ground-truths and segmented image(i.e. the output of\nthe present work).Sensitivity, specificity, positive predictive value (PPV),\npositive likelihood ratio (PLR) and accuracy are used to evaluate overall\nperformance.It is found that this work segments blood vessels successfully with\nsensitivity, specificity, PPV, PLR and accuracy of 99.62%, 54.66%, 95.08%,\n219.72 and 95.03%, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 05:12:53 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Dey", "Nilanjan", ""], ["Roy", "Anamitra Bardhan", ""], ["Pal", "Moumita", ""], ["Das", "Achintya", ""]]}, {"id": "1209.1224", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Achintya Das, Sheli Sinha Chaudhuri", "title": "Wavelet Based Normal and Abnormal Heart Sound Identification using\n  Spectrogram Analysis", "comments": "7 pages, 13 figures", "journal-ref": "International Journal of Computer Science & Engineering Technology\n  (IJCSET), Vol. 3 No. 6 June 2012, ISSN : 2229-3345", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present work proposes a computer-aided normal and abnormal heart sound\nidentification based on Discrete Wavelet Transform (DWT), it being useful for\ntele-diagnosis of heart diseases. Due to the presence of Cumulative Frequency\ncomponents in the spectrogram, DWT is applied on the spectro-gram up to n level\nto extract the features from the individual approximation components. One\ndimensional feature vector is obtained by evaluating the Row Mean of the\napproximation components of these spectrograms. For this present approach, the\nset of spectrograms has been considered as the database, rather than raw sound\nsamples. Minimum Euclidean distance is computed between feature vector of the\ntest sample and the feature vectors of the stored samples to identify the heart\nsound. By applying this algorithm, almost 82% of accuracy was achieved.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2012 08:37:44 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Dey", "Nilanjan", ""], ["Das", "Achintya", ""], ["Chaudhuri", "Sheli Sinha", ""]]}, {"id": "1209.1558", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Pradipti Nandi, Nilanjana Barman, Debolina Das,\n  Subhabrata Chakraborty", "title": "A Comparative Study between Moravec and Harris Corner Detection of Noisy\n  Images Using Adaptive Wavelet Thresholding Technique", "comments": "8 pages, 13 figures", "journal-ref": "International Journal of Engineering Research and Applications\n  (IJERA) Vol. 2, Issue 1, Jan-Feb 2012, pp.599-606", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a comparative study between Moravec and Harris Corner Detection\nhas been done for obtaining features required to track and recognize objects\nwithin a noisy image. Corner detection of noisy images is a challenging task in\nimage processing. Natural images often get corrupted by noise during\nacquisition and transmission. As Corner detection of these noisy images does\nnot provide desired results, hence de-noising is required. Adaptive wavelet\nthresholding approach is applied for the same.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 14:52:02 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Dey", "Nilanjan", ""], ["Nandi", "Pradipti", ""], ["Barman", "Nilanjana", ""], ["Das", "Debolina", ""], ["Chakraborty", "Subhabrata", ""]]}, {"id": "1209.1563", "submitter": "Nilanjan  Dey", "authors": "Sayantan Mukhopadhyay, Shouvik Biswas, Anamitra Bardhan Roy, Nilanjan\n  Dey", "title": "Wavelet Based QRS Complex Detection of ECG Signal", "comments": "5 pages, 8 figures, ISSN: 2248-9622", "journal-ref": "Journal of Engineering Research and Applications (IJERA) Vol. 2,\n  Issue 3, 2012, pp.2361-2365", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Electrocardiogram (ECG) is a sensitive diagnostic tool that is used to\ndetect various cardiovascular diseases by measuring and recording the\nelectrical activity of the heart in exquisite detail. A wide range of heart\ncondition is determined by thorough examination of the features of the ECG\nreport. Automatic extraction of time plane features is important for\nidentification of vital cardiac diseases. This paper presents a\nmulti-resolution wavelet transform based system for detection 'P', 'Q', 'R',\n'S', 'T' peaks complex from original ECG signal. 'R-R' time lapse is an\nimportant minutia of the ECG signal that corresponds to the heartbeat of the\nconcerned person. Abrupt increase in height of the 'R' wave or changes in the\nmeasurement of the 'R-R' denote various anomalies of human heart. Similarly\n'P-P', 'Q-Q', 'S-S', 'T-T' also corresponds to different anomalies of heart and\ntheir peak amplitude also envisages other cardiac diseases. In this proposed\nmethod the 'PQRST' peaks are marked and stored over the entire signal and the\ntime interval between two consecutive 'R' peaks and other peaks interval are\nmeasured to detect anomalies in behavior of heart, if any. The peaks are\nachieved by the composition of Daubeheissub bands wavelet of original ECG\nsignal. The accuracy of the 'PQRST' complex detection and interval measurement\nis achieved up to 100% with high exactitude by processing and thresholding the\noriginal ECG signal.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2012 15:05:57 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Mukhopadhyay", "Sayantan", ""], ["Biswas", "Shouvik", ""], ["Roy", "Anamitra Bardhan", ""], ["Dey", "Nilanjan", ""]]}, {"id": "1209.1759", "submitter": "Yani Ioannou", "authors": "Yani Ioannou, Babak Taati, Robin Harrap, Michael Greenspan", "title": "Difference of Normals as a Multi-Scale Operator in Unorganized Point\n  Clouds", "comments": "To be published in proceedings of 3DIMPVT 2012", "journal-ref": "Proceedings of the 2012 Second International Conference on 3D\n  Imaging, Modeling, Processing, Visualization & Transmission (3DIMPVT)", "doi": "10.1109/3DIMPVT.2012.12", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel multi-scale operator for unorganized 3D point clouds is introduced.\nThe Difference of Normals (DoN) provides a computationally efficient,\nmulti-scale approach to processing large unorganized 3D point clouds. The\napplication of DoN in the multi-scale filtering of two different real-world\noutdoor urban LIDAR scene datasets is quantitatively and qualitatively\ndemonstrated. In both datasets the DoN operator is shown to segment large 3D\npoint clouds into scale-salient clusters, such as cars, people, and lamp posts\ntowards applications in semi-automatic annotation, and as a pre-processing step\nin automatic object recognition. The application of the operator to\nsegmentation is evaluated on a large public dataset of outdoor LIDAR scenes\nwith ground truth annotations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 22:43:28 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Ioannou", "Yani", ""], ["Taati", "Babak", ""], ["Harrap", "Robin", ""], ["Greenspan", "Michael", ""]]}, {"id": "1209.1788", "submitter": "Alejandro Frery", "authors": "Elsa E. Moschetti, M. Gabriela Palacio, Mery Picco, Oscar H. Bustos,\n  Alejandro C. Frery", "title": "On the Use of Lee's Protocol for Speckle-Reducing Techniques", "comments": "http://www.laar.uns.edu.ar", "journal-ref": "Latin American Applied Research, volume 36, number 2, pages\n  115-121, 2006", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two new MAP (Maximum a Posteriori) filters for speckle\nnoise reduction and a Monte Carlo procedure for the assessment of their\nperformance. In order to quantitatively evaluate the results obtained using\nthese new filters, with respect to classical ones, a Monte Carlo extension of\nLee's protocol is proposed. This extension of the protocol shows that its\noriginal version leads to inconsistencies that hamper its use as a general\nprocedure for filter assessment. Some solutions for these inconsistencies are\nproposed, and a consistent comparison of speckle-reducing filters is provided.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 10:30:08 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Moschetti", "Elsa E.", ""], ["Palacio", "M. Gabriela", ""], ["Picco", "Mery", ""], ["Bustos", "Oscar H.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1209.1826", "submitter": "Kinjal Basu", "authors": "Kinjal Basu and Debapriya Sengupta", "title": "A spatio-spectral hybridization for edge preservation and noisy image\n  restoration via local parametric mixtures and Lagrangian relaxation", "comments": "29 Pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a fully unsupervised statistical method for edge\npreserving image restoration and compression using a spatial decomposition\nscheme. Smoothed maximum likelihood is used for local estimation of edge pixels\nfrom mixture parametric models of local templates. For the complementary smooth\npart the traditional L2-variational problem is solved in the Fourier domain\nwith Thin Plate Spline (TPS) regularization. It is well known that naive\nFourier compression of the whole image fails to restore a piece-wise smooth\nnoisy image satisfactorily due to Gibbs phenomenon. Images are interpreted as\nrelative frequency histograms of samples from bi-variate densities where the\nsample sizes might be unknown. The set of discontinuities is assumed to be\ncompletely unsupervised Lebesgue-null, compact subset of the plane in the\ncontinuous formulation of the problem. Proposed spatial decomposition uses a\nwidely used topological concept, partition of unity. The decision on edge pixel\nneighborhoods are made based on the multiple testing procedure of Holms.\nStatistical summary of the final output is decomposed into two layers of\ninformation extraction, one for the subset of edge pixels and the other for the\nsmooth region. Robustness is also demonstrated by applying the technique on\nnoisy degradation of clean images.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2012 18:23:21 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Basu", "Kinjal", ""], ["Sengupta", "Debapriya", ""]]}, {"id": "1209.1960", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi, Hassan A. Kingravi, Patricio A. Vela", "title": "A Comparative Study of Efficient Initialization Methods for the K-Means\n  Clustering Algorithm", "comments": "17 pages, 1 figure, 7 tables", "journal-ref": "Expert Systems with Applications 40 (2013) 200-210", "doi": "10.1016/j.eswa.2012.07.021", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is undoubtedly the most widely used partitional clustering algorithm.\nUnfortunately, due to its gradient descent nature, this algorithm is highly\nsensitive to the initial placement of the cluster centers. Numerous\ninitialization methods have been proposed to address this problem. In this\npaper, we first present an overview of these methods with an emphasis on their\ncomputational efficiency. We then compare eight commonly used linear time\ncomplexity initialization methods on a large and diverse collection of data\nsets using various performance criteria. Finally, we analyze the experimental\nresults using non-parametric statistical tests and provide recommendations for\npractitioners. We demonstrate that popular initialization methods often perform\npoorly and that there are in fact strong alternatives to these methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 12:22:06 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""], ["Vela", "Patricio A.", ""]]}, {"id": "1209.2082", "submitter": "Guangcan Liu", "authors": "Guangcan Liu and Shiyu Chang and Yi Ma", "title": "Blind Image Deblurring by Spectral Properties of Convolution Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of recovering a sharp version of a given\nblurry image when the blur kernel is unknown. Previous methods often introduce\nan image-independent regularizer (such as Gaussian or sparse priors) on the\ndesired blur kernel. We shall show that the blurry image itself encodes rich\ninformation about the blur kernel. Such information can be found through\nanalyzing and comparing how the spectrum of an image as a convolution operator\nchanges before and after blurring. Our analysis leads to an effective convex\nregularizer on the blur kernel which depends only on the given blurry image. We\nshow that the minimizer of this regularizer guarantees to give good\napproximation to the blur kernel if the original image is sharp enough. By\ncombining this powerful regularizer with conventional image deblurring\ntechniques, we show how we could significantly improve the deblurring results\nthrough simulations and experiments on real images. In addition, our analysis\nand experiments help explaining a widely accepted doctrine; that is, the edges\nare good features for deblurring.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2012 18:19:36 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 00:34:29 GMT"}, {"version": "v3", "created": "Tue, 22 Apr 2014 17:36:34 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Liu", "Guangcan", ""], ["Chang", "Shiyu", ""], ["Ma", "Yi", ""]]}, {"id": "1209.2295", "submitter": "Davide Eynard", "authors": "Davide Eynard, Klaus Glashoff, Michael M. Bronstein, Alexander M.\n  Bronstein", "title": "Multimodal diffusion geometry by joint diagonalization of Laplacians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct an extension of diffusion geometry to multiple modalities\nthrough joint approximate diagonalization of Laplacian matrices. This naturally\nextends classical data analysis tools based on spectral geometry, such as\ndiffusion maps and spectral clustering. We provide several synthetic and real\nexamples of manifold learning, retrieval, and clustering demonstrating that the\njoint diffusion geometry frequently better captures the inherent structure of\nmulti-modal data. We also show that many previous attempts to construct\nmultimodal spectral clustering can be seen as particular cases of joint\napproximate diagonalization of the Laplacians.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 12:01:08 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2012 07:31:14 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Eynard", "Davide", ""], ["Glashoff", "Klaus", ""], ["Bronstein", "Michael M.", ""], ["Bronstein", "Alexander M.", ""]]}, {"id": "1209.2515", "submitter": "Rehna V J", "authors": "V. J. Rehna and M. K. Jeya Kumar", "title": "Wavelet Based Image Coding Schemes : A Recent Survey", "comments": "18 pages, 7 figures, journal", "journal-ref": "International Journal on Soft Computing (IJSC) Vol.3, No.3, August\n  2012, 101-118", "doi": "10.5121/ijsc.2012.3308", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of new and powerful algorithms have been developed for image\ncompression over the years. Among them the wavelet-based image compression\nschemes have gained much popularity due to their overlapping nature which\nreduces the blocking artifacts that are common phenomena in JPEG compression\nand multiresolution character which leads to superior energy compaction with\nhigh quality reconstructed images. This paper provides a detailed survey on\nsome of the popular wavelet coding techniques such as the Embedded Zerotree\nWavelet (EZW) coding, Set Partitioning in Hierarchical Tree (SPIHT) coding, the\nSet Partitioned Embedded Block (SPECK) Coder, and the Embedded Block Coding\nwith Optimized Truncation (EBCOT) algorithm. Other wavelet-based coding\ntechniques like the Wavelet Difference Reduction (WDR) and the Adaptive Scanned\nWavelet Difference Reduction (ASWDR) algorithms, the Space Frequency\nQuantization (SFQ) algorithm, the Embedded Predictive Wavelet Image Coder\n(EPWIC), Compression with Reversible Embedded Wavelet (CREW), the Stack-Run\n(SR) coding and the recent Geometric Wavelet (GW) coding are also discussed.\nBased on the review, recommendations and discussions are presented for\nalgorithm development and implementation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 08:08:50 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Rehna", "V. J.", ""], ["Kumar", "M. K. Jeya", ""]]}, {"id": "1209.2657", "submitter": "Laura Rebollo-Neira", "authors": "Laura Rebollo-Neira and James Bowley", "title": "Sparse Representation of Astronomical Images", "comments": "Software to implement the approach is available on\n  http://www.nonlinear-approx.info/examples/node1.html", "journal-ref": null, "doi": "10.1364/JOSAA.30.000758", "report-no": null, "categories": "math-ph cs.CV math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation of astronomical images is discussed. It is shown that a\nsignificant gain in sparsity is achieved when particular mixed dictionaries are\nused for approximating these types of images with greedy selection strategies.\nExperiments are conducted to confirm: i)Effectiveness at producing sparse\nrepresentations. ii)Competitiveness, with respect to the time required to\nprocess large images.The latter is a consequence of the suitability of the\nproposed dictionaries for approximating images in partitions of small\nblocks.This feature makes it possible to apply the effective greedy selection\ntechnique Orthogonal Matching Pursuit, up to some block size. For blocks\nexceeding that size a refinement of the original Matching Pursuit approach is\nconsidered. The resulting method is termed Self Projected Matching Pursuit,\nbecause is shown to be effective for implementing, via Matching Pursuit itself,\nthe optional back-projection intermediate steps in that approach.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 16:38:39 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Rebollo-Neira", "Laura", ""], ["Bowley", "James", ""]]}, {"id": "1209.2696", "submitter": "Eugenio Culurciello Eugenio Culurciello", "authors": "Aysegul Dundar, Jonghoon Jin and Eugenio Culurciello", "title": "Visual Tracking with Similarity Matching Ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to visual tracking: Similarity Matching\nRatio (SMR). The traditional approach of tracking is minimizing some measures\nof the difference between the template and a patch from the frame. This\napproach is vulnerable to outliers and drastic appearance changes and an\nextensive study is focusing on making the approach more tolerant to them.\nHowever, this often results in longer, corrective algo- rithms which do not\nsolve the original problem. This paper proposes a novel approach to the\ndefinition of the tracking problems, SMR, which turns the differences into a\nprobability measure. Only pixel differences below a threshold count towards\ndeciding the match, the rest are ignored. This approach makes the SMR tracker\nrobust to outliers and points that dramaticaly change appearance. The SMR\ntracker is tested on challenging video sequences and achieved state-of-the-art\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 19:32:36 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Dundar", "Aysegul", ""], ["Jin", "Jonghoon", ""], ["Culurciello", "Eugenio", ""]]}, {"id": "1209.2816", "submitter": "Padmavathi S", "authors": "S. Padmavathi, B. Priyalakshmi. Dr. K. P. Soman", "title": "Hirarchical Digital Image Inpainting Using Wavelets", "comments": "8 pages, 9 figures", "journal-ref": "Signal & Image Processing : An International Journal (SIPIJ)\n  Vol.3, No.4, August 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpainting is the technique of reconstructing unknown or damaged portions of\nan image in a visually plausible way. Inpainting algorithm automatically fills\nthe damaged region in an image using the information available in undamaged\nregion. Propagation of structure and texture information becomes a challenge as\nthe size of damaged area increases. In this paper, a hierarchical inpainting\nalgorithm using wavelets is proposed. The hierarchical method tries to keep the\nmask size smaller while wavelets help in handling the high pass structure\ninformation and low pass texture information separately. The performance of the\nproposed algorithm is tested using different factors. The results of our\nalgorithm are compared with existing methods such as interpolation, diffusion\nand exemplar techniques.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 08:40:17 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Padmavathi", "S.", ""], ["Soman", "B. Priyalakshmi. Dr. K. P.", ""]]}, {"id": "1209.2903", "submitter": "Nilanjan  Dey", "authors": "Nilanjan Dey, Pradipti Nandi, Nilanjana Barman", "title": "A Novel Approach of Harris Corner Detection of Noisy Images using\n  Adaptive Wavelet Thresholding Technique", "comments": "5 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1209.1558", "journal-ref": "International Journal of Computer Science & Technology(IJCST) Vol.\n  2, ISSUE 4, OCT. - DEC. 2011", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a method of corner detection for obtaining features\nwhich is required to track and recognize objects within a noisy image. Corner\ndetection of noisy images is a challenging task in image processing. Natural\nimages often get corrupted by noise during acquisition and transmission. Though\nCorner detection of these noisy images does not provide desired results, hence\nde-noising is required. Adaptive wavelet thresholding approach is applied for\nthe same.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2012 14:15:16 GMT"}], "update_date": "2012-09-14", "authors_parsed": [["Dey", "Nilanjan", ""], ["Nandi", "Pradipti", ""], ["Barman", "Nilanjana", ""]]}, {"id": "1209.3113", "submitter": "Baran Tander", "authors": "Baran Tander, Atilla \\\"Ozmen, Murat Ba\\c{s}kan", "title": "Detection and Classification of Viewer Age Range Smart Signs at TV\n  Broadcast", "comments": "11 pages, 13 figures, 2 tables", "journal-ref": null, "doi": "10.5121/sipij.2012.3410", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the identification and classification of Viewer Age Range\nSmart Signs, designed by the Radio and Television Supreme Council of Turkey, to\ngive age range information for the TV viewers, are realized. Therefore, the\nautomatic detection at the broadcast will be possible, enabling the\nmanufacturing of TV receivers which are sensible to these signs. The most\nimportant step at this process is the pattern recognition. Since the symbols\nthat must be identified are circular, various circle detection techniques can\nbe employed. In our study, first, two different circle segmentation methods for\nstill images are analyzed, their advantages and drawbacks are discussed. A\npopular neural network structure called Multilayer Perceptron is employed for\nthe classification. Afterwards, the same procedures are carried out for\nstreaming video. All of the steps depicted above are realized on a standard PC.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 07:52:09 GMT"}], "update_date": "2012-09-17", "authors_parsed": [["Tander", "Baran", ""], ["\u00d6zmen", "Atilla", ""], ["Ba\u015fkan", "Murat", ""]]}, {"id": "1209.3318", "submitter": "Stamatios Lefkimmiatis", "authors": "Stamatios Lefkimmiatis, John Paul Ward, and Michael Unser", "title": "Hessian Schatten-Norm Regularization for Linear Inverse Problems", "comments": "15 pages double-column format. This manuscript will appear in IEEE\n  Transactions on Image Processing", "journal-ref": "IEEE Trans. Image Process. 22 (2013), no. 5, 1873--1888", "doi": "10.1109/TIP.2013.2237919", "report-no": null, "categories": "math.OC cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel family of invariant, convex, and non-quadratic\nfunctionals that we employ to derive regularized solutions of ill-posed linear\ninverse imaging problems. The proposed regularizers involve the Schatten norms\nof the Hessian matrix, computed at every pixel of the image. They can be viewed\nas second-order extensions of the popular total-variation (TV) semi-norm since\nthey satisfy the same invariance properties. Meanwhile, by taking advantage of\nsecond-order derivatives, they avoid the staircase effect, a common artifact of\nTV-based reconstructions, and perform well for a wide range of applications. To\nsolve the corresponding optimization problems, we propose an algorithm that is\nbased on a primal-dual formulation. A fundamental ingredient of this algorithm\nis the projection of matrices onto Schatten norm balls of arbitrary radius.\nThis operation is performed efficiently based on a direct link we provide\nbetween vector projections onto $\\ell_q$ norm balls and matrix projections onto\nSchatten norm balls. Finally, we demonstrate the effectiveness of the proposed\nmethods through experimental results on several inverse imaging problems with\nreal and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2012 20:31:06 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2013 19:27:48 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2013 08:02:27 GMT"}], "update_date": "2014-02-20", "authors_parsed": [["Lefkimmiatis", "Stamatios", ""], ["Ward", "John Paul", ""], ["Unser", "Michael", ""]]}, {"id": "1209.3433", "submitter": "Salah A. Aly", "authors": "Hossam M. Zawbaa, Salah A. Aly, Adnan A. Gutub", "title": "A Hajj And Umrah Location Classification System For Video Crowded Scenes", "comments": "9 pages, 10 figures, 2 tables, 3 algirthms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new automatic system for classifying ritual locations in\ndiverse Hajj and Umrah video scenes is investigated. This challenging subject\nhas mostly been ignored in the past due to several problems one of which is the\nlack of realistic annotated video datasets. HUER Dataset is defined to model\nsix different Hajj and Umrah ritual locations[26].\n  The proposed Hajj and Umrah ritual location classifying system consists of\nfour main phases: Preprocessing, segmentation, feature extraction, and location\nclassification phases. The shot boundary detection and background/foregroud\nsegmentation algorithms are applied to prepare the input video scenes into the\nKNN, ANN, and SVM classifiers. The system improves the state of art results on\nHajj and Umrah location classifications, and successfully recognizes the six\nHajj rituals with more than 90% accuracy. The various demonstrated experiments\nshow the promising results.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2012 20:57:51 GMT"}], "update_date": "2012-09-18", "authors_parsed": [["Zawbaa", "Hossam M.", ""], ["Aly", "Salah A.", ""], ["Gutub", "Adnan A.", ""]]}, {"id": "1209.4233", "submitter": "Laurent Najman", "authors": "Roland Levillain (LIGM, LRDE), Thierry G\\'eraud (LRDE), Laurent Najman\n  (LIGM)", "title": "Writing Reusable Digital Geometry Algorithms in a Generic Image\n  Processing Framework", "comments": "Workshop on Applications of Discrete Geometry and Mathematical\n  Morphology, Istanb : France (2010)", "journal-ref": null, "doi": "10.1007/978-3-642-32313-3_10", "report-no": null, "categories": "cs.MS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Geometry software should reflect the generality of the underlying\nmathe- matics: mapping the latter to the former requires genericity. By\ndesigning generic solutions, one can effectively reuse digital geometry data\nstructures and algorithms. We propose an image processing framework focused on\nthe Generic Programming paradigm in which an algorithm on the paper can be\nturned into a single code, written once and usable with various input types.\nThis approach enables users to design and implement new methods at a lower\ncost, try cross-domain experiments and help generalize results\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 15:17:10 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Levillain", "Roland", "", "LIGM, LRDE"], ["G\u00e9raud", "Thierry", "", "LRDE"], ["Najman", "Laurent", "", "LIGM"]]}, {"id": "1209.4317", "submitter": "Haichao Zhang Haichao Zhang", "authors": "Haichao Zhang, David Wipf and Yanning Zhang", "title": "Image Super-Resolution via Sparse Bayesian Modeling of Natural Images", "comments": "8 figures, 29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image super-resolution (SR) is one of the long-standing and active topics in\nimage processing community. A large body of works for image super resolution\nformulate the problem with Bayesian modeling techniques and then obtain its\nMaximum-A-Posteriori (MAP) solution, which actually boils down to a regularized\nregression task over separable regularization term. Although straightforward,\nthis approach cannot exploit the full potential offered by the probabilistic\nmodeling, as only the posterior mode is sought. Also, the separable property of\nthe regularization term can not capture any correlations between the sparse\ncoefficients, which sacrifices much on its modeling accuracy. We propose a\nBayesian image SR algorithm via sparse modeling of natural images. The sparsity\nproperty of the latent high resolution image is exploited by introducing latent\nvariables into the high-order Markov Random Field (MRF) which capture the\ncontent adaptive variance by pixel-wise adaptation. The high-resolution image\nis estimated via Empirical Bayesian estimation scheme, which is substantially\nfaster than our previous approach based on Markov Chain Monte Carlo sampling\n[1]. It is shown that the actual cost function for the proposed approach\nactually incorporates a non-factorial regularization term over the sparse\ncoefficients. Experimental results indicate that the proposed method can\ngenerate competitive or better results than \\emph{state-of-the-art} SR\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 18:02:41 GMT"}], "update_date": "2012-09-20", "authors_parsed": [["Zhang", "Haichao", ""], ["Wipf", "David", ""], ["Zhang", "Yanning", ""]]}, {"id": "1209.4419", "submitter": "Chao Wang", "authors": "Chao Wang", "title": "Head Frontal-View Identification Using Extended LLE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic head frontal-view identification is challenging due to appearance\nvariations caused by pose changes, especially without any training samples. In\nthis paper, we present an unsupervised algorithm for identifying frontal view\namong multiple facial images under various yaw poses (derived from the same\nperson). Our approach is based on Locally Linear Embedding (LLE), with the\nassumption that with yaw pose being the only variable, the facial images should\nlie in a smooth and low dimensional manifold. We horizontally flip the facial\nimages and present two K-nearest neighbor protocols for the original images and\nthe flipped images, respectively. In the proposed extended LLE, for any facial\nimage (original or flipped one), we search (1) the Ko nearest neighbors among\nthe original facial images and (2) the Kf nearest neighbors among the flipped\nfacial images to construct the same neighborhood graph. The extended LLE\neliminates the differences (because of background, face position and scale in\nthe whole image and some asymmetry of left-right face) between the original\nfacial image and the flipped facial image at the same yaw pose so that the\nflipped facial images can be used effectively. Our approach does not need any\ntraining samples as prior information. The experimental results show that the\nfrontal view of head can be identified reliably around the lowest point of the\npose manifold for multiple facial images, especially the cropped facial images\n(little background and centered face).\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 04:15:39 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Wang", "Chao", ""]]}, {"id": "1209.4420", "submitter": "Chao Wang", "authors": "Lan-Ting LI", "title": "An Efficient Color Face Verification Based on 2-Directional\n  2-Dimensional Feature Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel and uniform framework for face verification is presented in this\npaper. First of all, a 2-directional 2-dimensional feature extraction method is\nadopted to extract client-specific template - 2D discrimant projection matrix.\nThen the face skin color information is utilized as an additive feature to\nenhance decision making strategy that makes use of not only 2D grey feature but\nalso 2D skin color feature. A fusion decision of both is applied to experiment\nthe performance on the XM2VTS database according to Lausanne protocol.\nExperimental results show that the framework achieves high verification\naccuracy and verification speed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 04:20:40 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["LI", "Lan-Ting", ""]]}, {"id": "1209.4850", "submitter": "Shanshan Huang", "authors": "Mireille Boutin, Shanshan Huang", "title": "The Pascal Triangle of a Discrete Image: Definition, Properties and\n  Application to Shape Analysis", "comments": null, "journal-ref": "SIGMA 9 (2013), 031, 25 pages", "doi": "10.3842/SIGMA.2013.031", "report-no": null, "categories": "math-ph cs.CV math.MP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We define the Pascal triangle of a discrete (gray scale) image as a pyramidal\narrangement of complex-valued moments and we explore its geometric\nsignificance. In particular, we show that the entries of row k of this triangle\ncorrespond to the Fourier series coefficients of the moment of order k of the\nRadon transform of the image. Group actions on the plane can be naturally\nprolonged onto the entries of the Pascal triangle. We study the prolongation of\nsome common group actions, such as rotations and reflections, and we propose\nsimple tests for detecting equivalences and self-equivalences under these group\nactions. The motivating application of this work is the problem of\ncharacterizing the geometry of objects on images, for example by detecting\napproximate symmetries.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2012 15:49:14 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2013 05:51:09 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Boutin", "Mireille", ""], ["Huang", "Shanshan", ""]]}, {"id": "1209.5039", "submitter": "Jaswinder  Dilawari Singh", "authors": "Jaswinder Singh Dilawari, Ravinder Khanna", "title": "Creation of Digital Test Form for Prepress Department", "comments": "5 Pages,4 Figures", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 10, No. 9, September 2012 (IJCSIS) International Journal of\n  Computer Science and Information Security, Vol. 10, No. 9, September 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main problem in colour management in prepress department is lack of\navailability of literature on colour management and knowledge gap between\nprepress department and press department. So a digital test from has been\ncreated by Adobe Photoshop to analyse the ICC profile and to create a new\nprofile and this analysed data is used to study about various grey scale of RGB\nand CMYK images. That helps in conversion of image from RGB to CMYK in prepress\ndepartment.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 07:52:01 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Dilawari", "Jaswinder Singh", ""], ["Khanna", "Ravinder", ""]]}, {"id": "1209.5040", "submitter": "Jaswinder  Dilawari Singh", "authors": "Jaswinder Singh Dilawari, Ravinder Khanna", "title": "Image Classification and Optimized Image Reproduction", "comments": "5 Pages, 9 Figures", "journal-ref": "International Journal of Computer Science Issues,volume 9,Issue 5\n  ,Septmeber 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By taking into account the properties and limitations of the human visual\nsystem, images can be more efficiently compressed, colors more accurately\nreproduced, prints better rendered. To show all these advantages in this paper\nnew adapted color charts have been created based on technical and visual image\ncategory analysis. A number of tests have been carried out using extreme images\nwith their key information strictly in dark and light areas. It was shown that\nthe image categorization using the adapted color charts improves the analysis\nof relevant image information with regard to both the image gradation and the\ndetail reproduction. The images with key information in hi-key areas were also\ntest printed using the adapted color charts.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 08:11:27 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Dilawari", "Jaswinder Singh", ""], ["Khanna", "Ravinder", ""]]}, {"id": "1209.5041", "submitter": "Jaswinder  Dilawari Singh", "authors": "Jaswinder Singh Dilawari, Ravinder Khanna", "title": "An Implementation of Computer Graphics as Prepress Image Enhancement\n  Process", "comments": "4 Pages,8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of a printed product involves three stages: prepress, the\nprinting process (press) itself, and finishing (post press). There are various\ntypes of equipments (printers, scanners) and various qualities image are\npresent in the market. These give different color rendering each time during\nreproduction. So, a color key tool has been developed keeping Color Management\nScheme (CMS) in mind so that during reproduction no color rendering takes place\nirrespective of use of any device and resolution level has also been improved.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 09:15:20 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Dilawari", "Jaswinder Singh", ""], ["Khanna", "Ravinder", ""]]}, {"id": "1209.5111", "submitter": "James Bergstra", "authors": "J. Bergstra and D. Yamins and D. D. Cox", "title": "Making a Science of Model Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision algorithms depend on a variety of parameter choices and\nsettings that are typically hand-tuned in the course of evaluating the\nalgorithm. While such parameter tuning is often presented as being incidental\nto the algorithm, correctly setting these parameter choices is frequently\ncritical to evaluating a method's full potential. Compounding matters, these\nparameters often must be re-tuned when the algorithm is applied to a new\nproblem domain, and the tuning process itself often depends on personal\nexperience and intuition in ways that are hard to describe. Since the\nperformance of a given technique depends on both the fundamental quality of the\nalgorithm and the details of its tuning, it can be difficult to determine\nwhether a given technique is genuinely better, or simply better tuned.\n  In this work, we propose a meta-modeling approach to support automated hyper\nparameter optimization, with the goal of providing practical tools to replace\nhand-tuning with a reproducible and unbiased optimization process. Our approach\nis to expose the underlying expression graph of how a performance metric (e.g.\nclassification accuracy on validation examples) is computed from parameters\nthat govern not only how individual processing steps are applied, but even\nwhich processing steps are included. A hyper parameter optimization algorithm\ntransforms this graph into a program for optimizing that performance metric.\nOur approach yields state of the art results on three disparate computer vision\nproblems: a face-matching verification task (LFW), a face identification task\n(PubFig83) and an object recognition task (CIFAR-10), using a single algorithm.\nMore broadly, we argue that the formalization of a meta-model supports more\nobjective, reproducible, and quantitative evaluation of computer vision\nalgorithms, and that it can serve as a valuable tool for guiding algorithm\ndevelopment.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2012 20:59:41 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Bergstra", "J.", ""], ["Yamins", "D.", ""], ["Cox", "D. D.", ""]]}, {"id": "1209.5245", "submitter": "Tarek Behi Mr", "authors": "Tarek Behi, Najet Arous and Noureddine Ellouze", "title": "Spike Timing Dependent Competitive Learning in Recurrent Self Organizing\n  Pulsed Neural Networks Case Study: Phoneme and Word Recognition", "comments": "10 pages, 15 tables", "journal-ref": "International Journal of Computer Science Issues, Vol. 9, Issue 4,\n  No 2, (2012)328-337", "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaptic plasticity seems to be a capital aspect of the dynamics of neural\nnetworks. It is about the physiological modifications of the synapse, which\nhave like consequence a variation of the value of the synaptic weight. The\ninformation encoding is based on the precise timing of single spike events that\nis based on the relative timing of the pre- and post-synaptic spikes, local\nsynapse competitions within a single neuron and global competition via lateral\nconnections. In order to classify temporal sequences, we present in this paper\nhow to use a local hebbian learning, spike-timing dependent plasticity for\nunsupervised competitive learning, preserving self-organizing maps of spiking\nneurons. In fact we present three variants of self-organizing maps (SOM) with\nspike-timing dependent Hebbian learning rule, the Leaky Integrators Neurons\n(LIN), the Spiking_SOM and the recurrent Spiking_SOM (RSSOM) models. The case\nstudy of the proposed SOM variants is phoneme classification and word\nrecognition in continuous speech and speaker independent.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 12:28:16 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Behi", "Tarek", ""], ["Arous", "Najet", ""], ["Ellouze", "Noureddine", ""]]}, {"id": "1209.5417", "submitter": "Hesam Ekhtiyar", "authors": "Hesam Ekhtiyar, Mehdi Sheida, Somaye Sobati Moghadam", "title": "Model based neuro-fuzzy ASR on Texas processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an algorithm for recognizing speech has been proposed. The\nrecognized speech is used to execute related commands which use the MFCC and\ntwo kind of classifiers, first one uses MLP and second one uses fuzzy inference\nsystem as a classifier. The experimental results demonstrate the high gain and\nefficiency of the proposed algorithm. We have implemented this system based on\ngraphical design and tested on a fix point digital signal processor (DSP) of\n600 MHz, with reference DM6437-EVM of Texas instrument.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 20:47:27 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Ekhtiyar", "Hesam", ""], ["Sheida", "Mehdi", ""], ["Moghadam", "Somaye Sobati", ""]]}, {"id": "1209.5494", "submitter": "Nafiza Saidin", "authors": "Nafiza Saidin, Harsa Amylia Mat Sakim, Umi Kalthum Ngah and Ibrahim\n  Lutfi Shuaib", "title": "Segmentation of Breast Regions in Mammogram Based on Density: A Review", "comments": "9 pages, 2 figures,IJCSI International Journal of Computer Science\n  Issues, Vol. 9, Issue 4, No 2, July 2012", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 9,\n  Issue 4, No 2, July 2012", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is to review approaches for segmentation of breast\nregions in mammograms according to breast density. Studies based on density\nhave been undertaken because of the relationship between breast cancer and\ndensity. Breast cancer usually occurs in the fibroglandular area of breast\ntissue, which appears bright on mammograms and is described as breast density.\nMost of the studies are focused on the classification methods for glandular\ntissue detection. Others highlighted on the segmentation methods for\nfibroglandular tissue, while few researchers performed segmentation of the\nbreast anatomical regions based on density. There have also been works on the\nsegmentation of other specific parts of breast regions such as either detection\nof nipple position, skin-air interface or pectoral muscles. The problems on the\nevaluation performance of the segmentation results in relation to ground truth\nare also discussed in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 04:58:55 GMT"}], "update_date": "2012-09-26", "authors_parsed": [["Saidin", "Nafiza", ""], ["Sakim", "Harsa Amylia Mat", ""], ["Ngah", "Umi Kalthum", ""], ["Shuaib", "Ibrahim Lutfi", ""]]}, {"id": "1209.5756", "submitter": "Souli Sameh", "authors": "Sameh Souli, Zied Lachiri", "title": "Environmental Sounds Spectrogram Classification using Log-Gabor Filters\n  and Multiclass Support Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper presents novel approaches for efficient feature extraction using\nenvironmental sound magnitude spectrogram. We propose approach based on the\nvisual domain. This approach included three methods. The first method is based\non extraction for each spectrogram a single log-Gabor filter followed by mutual\ninformation procedure. In the second method, the spectrogram is passed by the\nsame steps of the first method but with an averaged bank of 12 log-Gabor\nfilter. The third method consists of spectrogram segmentation into three\npatches, and after that for each spectrogram patch we applied the second\nmethod. The classification results prove that the second method is the most\nefficient in our environmental sound classification system.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2012 20:11:23 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Souli", "Sameh", ""], ["Lachiri", "Zied", ""]]}, {"id": "1209.5826", "submitter": "Jorg Peters", "authors": "Jorg Peters", "title": "Refinability of splines from lattice Voronoi cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Splines can be constructed by convolving the indicator function of the\nVoronoi cell of a lattice. This paper presents simple criteria that imply that\nonly a small subset of such spline families can be refined: essentially the\nwell-known box splines and tensor-product splines. Among the many non-refinable\nconstructions are hex-splines and their generalization to non-Cartesian\nlattices. An example shows how non-refinable splines can exhibit increased\napproximation error upon refinement of the lattice.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 04:04:29 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Peters", "Jorg", ""]]}, {"id": "1209.5982", "submitter": "Robert Templeman", "authors": "Robert Templeman, Zahid Rahman, David Crandall, Apu Kapadia", "title": "PlaceRaider: Virtual Theft in Physical Spaces with Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smartphones become more pervasive, they are increasingly targeted by\nmalware. At the same time, each new generation of smartphone features\nincreasingly powerful onboard sensor suites. A new strain of sensor malware has\nbeen developing that leverages these sensors to steal information from the\nphysical environment (e.g., researchers have recently demonstrated how malware\ncan listen for spoken credit card numbers through the microphone, or feel\nkeystroke vibrations using the accelerometer). Yet the possibilities of what\nmalware can see through a camera have been understudied. This paper introduces\na novel visual malware called PlaceRaider, which allows remote attackers to\nengage in remote reconnaissance and what we call virtual theft. Through\ncompletely opportunistic use of the camera on the phone and other sensors,\nPlaceRaider constructs rich, three dimensional models of indoor environments.\nRemote burglars can thus download the physical space, study the environment\ncarefully, and steal virtual objects from the environment (such as financial\ndocuments, information on computer monitors, and personally identifiable\ninformation). Through two human subject studies we demonstrate the\neffectiveness of using mobile devices as powerful surveillance and virtual\ntheft platforms, and we suggest several possible defenses against visual\nmalware.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 15:56:07 GMT"}], "update_date": "2012-09-27", "authors_parsed": [["Templeman", "Robert", ""], ["Rahman", "Zahid", ""], ["Crandall", "David", ""], ["Kapadia", "Apu", ""]]}, {"id": "1209.6037", "submitter": "Jaswinder  Dilawari Singh", "authors": "Jaswinder Singh Dilawari, Ravinder Khanna", "title": "Reproduction of Images by Gamut Mapping and Creation of New Test Charts\n  in Prepress Process", "comments": "5 Pages,10 Figures; International Journal of Scientific and\n  Engineering Research,Volume 3, Issue 10, October 2012 Edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of digital images the problem of keeping picture\nvisualization uniformity arises because each printing or scanning device has\nits own color chart. So, universal color profiles are made by ICC to bring\nuniformity in various types of devices. Keeping that color profile in mind\nvarious new color charts are created and calibrated with the help of standard\nIT8 test charts available in the market. The main objective to color\nreproduction is to produce the identical picture at device output. For that\nprinciples for gamut mapping has been designed\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 19:25:56 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Dilawari", "Jaswinder Singh", ""], ["Khanna", "Ravinder", ""]]}, {"id": "1209.6151", "submitter": "Thai Le", "authors": "Thai Hoang Le, Truong Nhat Vo", "title": "Face Alignment Using Active Shape Model And Support Vector Machine", "comments": "11 pages and 11 figures", "journal-ref": "International Journal of Biometrics and Bioinformatics, 2011,\n  Volume (4): Issue (6), pp. 224-234", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Active Shape Model (ASM) is one of the most popular local texture models\nfor face alignment. It applies in many fields such as locating facial features\nin the image, face synthesis, etc. However, the experimental results show that\nthe accuracy of the classical ASM for some applications is not high. This paper\nsuggests some improvements on the classical ASM to increase the performance of\nthe model in the application: face alignment. Four of our major improvements\ninclude: i) building a model combining Sobel filter and the 2-D profile in\nsearching face in image; ii) applying Canny algorithm for the enhancement edge\non image; iii) Support Vector Machine (SVM) is used to classify landmarks on\nface, in order to determine exactly location of these landmarks support for\nASM; iv)automatically adjust 2-D profile in the multi-level model based on the\nsize of the input image. The experimental results on Caltech face database and\nTechnical University of Denmark database (imm_face) show that our proposed\nimprovement leads to far better performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 07:58:10 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Le", "Thai Hoang", ""], ["Vo", "Truong Nhat", ""]]}, {"id": "1209.6189", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin, Valentina E. Balas, Iulia M. Motoc", "title": "The Biometric Menagerie - A Fuzzy and Inconsistent Concept", "comments": "5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24\n  Aug 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proves that in iris recognition, the concepts of sheep, goats,\nlambs and wolves - as proposed by Doddington and Yager in the so-called\nBiometric Menagerie, are at most fuzzy and at least not quite well defined.\nThey depend not only on the users or on their biometric templates, but also on\nthe parameters that calibrate the iris recognition system. This paper shows\nthat, in the case of iris recognition, the extensions of these concepts have\nvery unsharp and unstable (non-stationary) boundaries. The membership of a user\nto these categories is more often expressed as a degree (as a fuzzy value)\nrather than as a crisp value. Moreover, they are defined by fuzzy Sugeno rules\ninstead of classical (crisp) definitions. For these reasons, we said that the\nBiometric Menagerie proposed by Doddington and Yager could be at most a fuzzy\nconcept of biometry, but even this status is conditioned by improving its\ndefinition. All of these facts are confirmed experimentally in a series of 12\nexhaustive iris recognition tests undertaken for University of Bath Iris Image\nDatabase while using three different iris code dimensions (256x16, 128x8 and\n64x4), two different iris texture encoders (Log-Gabor and Haar-Hilbert) and two\ndifferent types of safety models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 11:24:28 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""], ["Balas", "Valentina E.", ""], ["Motoc", "Iulia M.", ""]]}, {"id": "1209.6190", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Iulia M. Motoc, Cristina M. Noaica, Robert Badea, Claudiu G. Ghica", "title": "Noise Influence on the Fuzzy-Linguistic Partitioning of Iris Code Space", "comments": "5th Int. Conf. on Soft Computing and Applications (Szeged, HU), 22-24\n  Aug 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyses the set of iris codes stored or used in an iris\nrecognition system as an f-granular space. The f-granulation is given by\nidentifying in the iris code space the extensions of the fuzzy concepts wolves,\ngoats, lambs and sheep (previously introduced by Doddington as 'animals' of the\nbiometric menagerie) - which together form a partitioning of the iris code\nspace. The main question here is how objective (stable / stationary) this\npartitioning is when the iris segments are subject to noisy acquisition. In\norder to prove that the f-granulation of iris code space with respect to the\nfuzzy concepts that define the biometric menagerie is unstable in noisy\nconditions (is sensitive to noise), three types of noise (localvar, motion\nblur, salt and pepper) have been alternatively added to the iris segments\nextracted from University of Bath Iris Image Database. The results of 180\nexhaustive (all-to-all) iris recognition tests are presented and commented\nhere.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 11:31:25 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Motoc", "Iulia M.", ""], ["Noaica", "Cristina M.", ""], ["Badea", "Robert", ""], ["Ghica", "Claudiu G.", ""]]}, {"id": "1209.6204", "submitter": "Mikhail Kharinov Vyacheslavovich", "authors": "M. Kharinov", "title": "Reclassification formula that provides to surpass K-means method", "comments": "10 pages, 2 figures, 13 formulas", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a formula for the reclassification of multidimensional\ndata points (columns of real numbers, \"objects\", \"vectors\", etc.). This formula\ndescribes the change in the total squared error caused by reclassification of\ndata points from one cluster into another and prompts the way to calculate the\nsequence of optimal partitions, which are characterized by a minimum value of\nthe total squared error E (weighted sum of within-class variance,\nwithin-cluster sum of squares WCSS etc.), i.e. the sum of squared distances\nfrom each data point to its cluster center. At that source data points are\ntreated with repetitions allowed, and resulting clusters from different\npartitions, in general case, overlap each other. The final partitions are\ncharacterized by \"equilibrium\" stability with respect to the reclassification\nof the data points, where the term \"stability\" means that any prescribed\nreclassification of data points does not increase the total squared error E. It\nis important that conventional K-means method, in general case, provides\ngeneration of instable partitions with overstated values of the total squared\nerror E. The proposed method, based on the formula of reclassification, is more\nefficient than K-means method owing to converting of any partition into stable\none, as well as involving into the process of reclassification of certain sets\nof data points, in contrast to the classification of individual data points\naccording to K-means method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 12:24:05 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Kharinov", "M.", ""]]}, {"id": "1209.6299", "submitter": "Jason Williams", "authors": "Jason L. Williams and Roslyn A. Lau", "title": "Approximate evaluation of marginal association probabilities with belief\n  propagation", "comments": "http://dx.doi.org/10.1109/TAES.2014.120568. appears in IEEE\n  Transactions on Aerospace and Electronic Systems, vol. 50, no. 4, October\n  2014", "journal-ref": "IEEE Transactions on Aerospace and Electronic Systems, vol 50, no\n  4, pp 2942-2959, October 2014", "doi": "10.1109/TAES.2014.120568", "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association, the problem of reasoning over correspondence between\ntargets and measurements, is a fundamental problem in tracking. This paper\npresents a graphical model formulation of data association and applies an\napproximate inference method, belief propagation (BP), to obtain estimates of\nmarginal association probabilities. We prove that BP is guaranteed to converge,\nand bound the number of iterations necessary. Experiments reveal a favourable\ncomparison to prior methods in terms of accuracy and computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 06:45:25 GMT"}, {"version": "v2", "created": "Thu, 20 Nov 2014 00:47:06 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Williams", "Jason L.", ""], ["Lau", "Roslyn A.", ""]]}, {"id": "1209.6491", "submitter": "Alan Brunton", "authors": "Alan Brunton, Augusto Salazar, Timo Bolkart and Stefanie Wuhrer", "title": "Review of Statistical Shape Spaces for 3D Data with Comparative Analysis\n  for Human Faces", "comments": "revised literature review, improved experiments, statistical models\n  and code published", "journal-ref": "Computer Vision and Image Understanding, 128, pp. 1-17, 2014", "doi": "10.1016/j.cviu.2014.05.005", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With systems for acquiring 3D surface data being evermore commonplace, it has\nbecome important to reliably extract specific shapes from the acquired data. In\nthe presence of noise and occlusions, this can be done through the use of\nstatistical shape models, which are learned from databases of clean examples of\nthe shape in question. In this paper, we review, analyze and compare different\nstatistical models: from those that analyze the variation in geometry globally\nto those that analyze the variation in geometry locally. We first review how\ndifferent types of models have been used in the literature, then proceed to\ndefine the models and analyze them theoretically, in terms of both their\nstatistical and computational aspects. We then perform extensive experimental\ncomparison on the task of model fitting, and give intuition about which type of\nmodel is better for a few applications. Due to the wide availability of\ndatabases of high-quality data, we use the human face as the specific shape we\nwish to extract from corrupted data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:48:59 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 12:01:19 GMT"}, {"version": "v3", "created": "Sun, 4 May 2014 11:56:01 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Brunton", "Alan", ""], ["Salazar", "Augusto", ""], ["Bolkart", "Timo", ""], ["Wuhrer", "Stefanie", ""]]}, {"id": "1209.6525", "submitter": "Marcelo Fiori", "authors": "Marcelo Fiori, Pablo Mus\\'e, Guillermo Sapiro", "title": "A Complete System for Candidate Polyps Detection in Virtual Colonoscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer tomographic colonography, combined with computer-aided detection, is\na promising emerging technique for colonic polyp analysis. We present a\ncomplete pipeline for polyp detection, starting with a simple colon\nsegmentation technique that enhances polyps, followed by an adaptive-scale\ncandidate polyp delineation and classification based on new texture and\ngeometric features that consider both the information in the candidate polyp\nlocation and its immediate surrounding area. The proposed system is tested with\nground truth data, including flat and small polyps which are hard to detect\neven with optical colonoscopy. For polyps larger than 6mm in size we achieve\n100% sensitivity with just 0.9 false positives per case, and for polyps larger\nthan 3mm in size we achieve 93% sensitivity with 2.8 false positives per case.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 14:09:30 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Fiori", "Marcelo", ""], ["Mus\u00e9", "Pablo", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1209.6560", "submitter": "Alex Bronstein", "authors": "J. Pokrass, A. M. Bronstein, M. M. Bronstein, P. Sprechmann, G. Sapiro", "title": "Sparse Modeling of Intrinsic Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel sparse modeling approach to non-rigid shape matching using\nonly the ability to detect repeatable regions. As the input to our algorithm,\nwe are given only two sets of regions in two shapes; no descriptors are\nprovided so the correspondence between the regions is not know, nor we know how\nmany regions correspond in the two shapes. We show that even with such scarce\ninformation, it is possible to establish very accurate correspondence between\nthe shapes by using methods from the field of sparse modeling, being this, the\nfirst non-trivial use of sparse models in shape correspondence. We formulate\nthe problem of permuted sparse coding, in which we solve simultaneously for an\nunknown permutation ordering the regions on two shapes and for an unknown\ncorrespondence in functional representation. We also propose a robust variant\ncapable of handling incomplete matches. Numerically, the problem is solved\nefficiently by alternating the solution of a linear assignment and a sparse\ncoding problem. The proposed methods are evaluated qualitatively and\nquantitatively on standard benchmarks containing both synthetic and scanned\nobjects.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 16:05:37 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Pokrass", "J.", ""], ["Bronstein", "A. M.", ""], ["Bronstein", "M. M.", ""], ["Sprechmann", "P.", ""], ["Sapiro", "G.", ""]]}]